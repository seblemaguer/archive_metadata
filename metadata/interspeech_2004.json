{
 "title": "Interspeech 2004",
 "location": "Jeju Island, Korea",
 "startDate": "4/10/2004",
 "endDate": "8/10/2004",
 "chair": "General Chair: Dae Hee Youn",
 "conf": "Interspeech",
 "year": "2004",
 "name": "interspeech_2004",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2004",
 "date": "4-8 October 2004",
 "booklet": "interspeech_2004.pdf",
 "papers": {
  "lee04_interspeech": {
   "authors": [
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "From decoding-driven to detection-based paradigms for automatic speech recognition",
   "original": "i04_p2",
   "page_count": 5,
   "order": 1,
   "p1": "paper P2",
   "pn": "",
   "abstract": [
    "We present a detection-based automatic speech recognition (ASR) paradigm that is capable of integrating both the knowledge sources accumulated in the speech science community and the modeling techniques established in the speech processing community. By exploring this new framework, we expect that researchers in the Interspeech community can collaboratively contribute to developing next generation algorithms that have the potential to surpass current capabilities, and go beyond the limitations of the state-of-the-art ASR technologies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-1"
  },
  "lee04b_interspeech": {
   "authors": [
    [
     "Hyun-Bok",
     "Lee"
    ]
   ],
   "title": "In search of a universal phonetic alphabet - theory and application of an organic visible speech-",
   "original": "i04_p3",
   "page_count": 9,
   "order": 2,
   "p1": "paper P3",
   "pn": "",
   "abstract": [
    "Phonetic symbols have an important role to play in phonetics, linguistics, language teaching, speech pathology and speech sciences in general, and linguists and phoneticians have tried to devise appropriate phonetic alphabets. Notable among them are Sweet, Bell, Jespersen, Pike, etc. The most successful and popular phonetic alphabet today is no doubt the International Phonetic Alphabet. The International Korean Phonetic Alphabet (IKPA for short) is a system of phonetic symbols that has been devised by the author on the basis of the articulatory phonetic(organic) principles exploited by the Korean King Sejong in creating the Korean alphabet of 28 letters in 1443. The Korean alphabet is not merely a phonetic alphabet of arbitrary nature but a highly sophisticated system consisting of sets of interrelated organic phonetic symbols, each set representing either the shape of the organs of speech, i.e. lips, tooth and velar etc. or their articulatory action. The Korean alphabet is, in a true sense of the word, a set of phonetic symbols designed to represent the organic visible speech of the human being. The author has applied the organic phonetic principles much more extensively and systematically in devising IKPA than the King had done. Consequently the IKPA symbols are just as systematic, scientific, easy to learn and memorize as the Korean alphabet, quite unlike the IPA counterparts which, having been derived mainly from Roman and Greek letters, are mostly unsystematic and arbitrary. The IKPA symbols visualize or mirror the actual speech organs or their action and thus tell us exactly what sort of an articulatory action is involved in producing sounds. It is in this sense that the IKPA deserves to be called a \"Universal Visible Speech\", which is to be shared by all.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-2"
  },
  "vaissiere04_interspeech": {
   "authors": [
    [
     "Jacqueline",
     "Vaissière"
    ]
   ],
   "title": "From X-ray or MRU data to sounds through articulatory synthesis: towards an integrated view of the speech communication process",
   "original": "i04_p4",
   "page_count": 1,
   "order": 3,
   "p1": "paper P4",
   "pn": "",
   "abstract": [
    "This tutorial presents an integrated method to simulate the transfer from X-ray (or MRI) data to acoustics and finally to sounds. It illustrates the necessity of an articulatory model (hereby Maedas model) so as to: Construct realistic stimuli (sounds that human beings could really produce) for psychoacoustic experiments. \"hear\" what kind of sounds the vocal tract of a man or a woman, of a new-born or a monkey could produce and inversely, what vocal shapes could produce a sound with given acoustic characteristics. Study the correlation between the observed subtle articulatory and acoustic differences and the choices of preferred prototypes in the realisation and perception of the same API symbol by native speakers of different languages. Modelise vowels and consonants in context, and differentiate between transitional gestures which are necessary in a co-articulation process, but not essential in order to differentiate phonemes. Simulate the acoustic and perceptual consequences of the articulatory deformation realized by the singers (e.g. singing formant), or in case of pathological voices.\n",
    "Emphasis is put on the work done in our laboratory, and more generally by different teams in France (Grenoble, Aix-en-Provence, Strasbourg and Nancy).\n",
    ""
   ]
  },
  "balakrishnan04_interspeech": {
   "authors": [
    [
     "Sreeram",
     "Balakrishnan"
    ],
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Vaibhava",
     "Goe"
    ]
   ],
   "title": "Stochastic gradient adaptation of front-end parameters",
   "original": "i04_0001",
   "page_count": 4,
   "order": 4,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "This paper examines how any parameter in the typical front end of a speech recognizer, can be rapidly and inexpensively adapted with usage. It focusses on firstly demonstrating that effective adaptation can be accomplished using low CPU/Memory cost stochastic gradient descent methods, secondly showing that adaptation can be done at time scales small enough to make it effective with just a single utterance, and lastly showing that using a prior on the parameter significantly improves adaptation performance on small amounts of data. It extends previous work on stochastic gradient descent implementation of fMLLR and work on adapting any parameter in the front-end chain using general 2nd order opimization techniques. The framework for general stochastic gradient descent of any frontend parameter with a prior is presented, along with practical techniques to improve convergence. In addition the methods for obtaining the alignment at small time intervals before the end of the utterance are presented. Finally it shown that experimentally online causal adaptation can result in a 5-15% WER reduction across a variety of problems sets and noise conditions, even with just 1 or 2 utterances of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-3"
  },
  "raux04_interspeech": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ],
    [
     "Rita",
     "Singh"
    ]
   ],
   "title": "Maximum - likelihod adaptation of semi-continuous HMMs by latent variable decomposition of state distributions",
   "original": "i04_0005",
   "page_count": 4,
   "order": 5,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "Compared to fully-continuous HMMs, semi-continuous HMMs are more compact in size, require less data to train well and result in comparable recognition performance with much faster decoding speeds. Nevertheless, the use of semi-continuous HMMs in large vocabulary speech recognition systems has declined considerably in recent years. A significant factor that has contributed this is that systems that use semi-continuous HMMs cannot be easily adapted to new acoustic (environmental or speaker) conditions. While maximum likelihood (ML) adaptation techniques have been very successful for continuous density HMMs, these have not worked to a usable degree for semi-continuous HMMs. This paper presents a new framework for supervised and unsupervised ML adaptation of semi-continuous HMMs, built upon the paradigm of probabilistic latent semantic analysis. Experiments with a specific implementation developed under this framework demonstrate its effectiveness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-4"
  },
  "huang04_interspeech": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Tao",
     "Chen"
    ],
    [
     "Eric",
     "Chang"
    ]
   ],
   "title": "Transformation and combination of hiden Markov models for speaker selection training",
   "original": "i04_0009",
   "page_count": 4,
   "order": 6,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "This paper presents a 3-stage adaptation framework based on speaker selection training. First a subset of cohort speakers is selected for test speaker using Gaussian mixture model, which is more reliable given very limited adaptation data. Then cohort models are linearly transformed closer to each test speaker. Finally the adapted model for the test speaker is obtained by combining these transformed models. Combination weights as well as bias items are adaptively learned from adaptation data. Experiments showed that model transformation before combination would improve the robustness of the scheme. With only 30s of adaptation data, about 14.9% relative error rate reduction is achieved on a large vocabulary continuous speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-5"
  },
  "mak04_interspeech": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Roger",
     "Hsiao"
    ]
   ],
   "title": "Improving eigenspace-based MLLR adaptation by kernel PCA",
   "original": "i04_0013",
   "page_count": 4,
   "order": 7,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "Eigenspace-based MLLR (EMLLR) adaptation has been shown effective for fast speaker adaptation. It applies the basic idea of eigenvoice adaptation, and derives a small set of eigenmatrices using principal component analysis (PCA). The MLLR adaptation transformation of a new speaker is then a linear combination of the eigenmatrices. In this paper, we investigate the use of kernel PCA to find the eigenmatrices in the kernel-induced high dimensional feature space so as to exploit possible nonlinearity in the transformation supervector space. In addition, composite kernel is used to preserve the row information in the transformation supervector which, otherwise, will be lost during the mapping to the kernel-induced feature space. We call our new method kernel eigenspace-based MLLR (KEMLLR) adaptation. On a RM adaptation task, we find that KEMLLR adaptation may reduce the word error rate of a speakerindependent model by 11%, and outperforms MLLR and EMLLR adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-6"
  },
  "chatzichrisafis04_interspeech": {
   "authors": [
    [
     "Nikos",
     "Chatzichrisafis"
    ],
    [
     "Vasilios",
     "Digalakis"
    ],
    [
     "Vasilios",
     "Diakoloukas"
    ],
    [
     "Costas",
     "Harizakis"
    ]
   ],
   "title": "Rapid acoustic model development using Gaussian mixture clustering and language adaptation",
   "original": "i04_0017",
   "page_count": 4,
   "order": 8,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "This work presents techniques for improved cross-language transfer of speech recognition systems to new, previously undeveloped, languages. Such techniques are particularly useful for target languages where minimal amounts of training data are available. We describe a novel method to produce a language-independent system by combining acoustic models from a number of source languages. This intermediate language-independent acoustic model is used to bootstrap a target-language system by applying language adaptation. For our experiments we use acoustic models of seven source languages to develop a target Greek acoustic model. We show that our technique significantly outperforms a system trained from scratch when less than 8 hours of read speech is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-7"
  },
  "visweswariah04_interspeech": {
   "authors": [
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Ramesh",
     "Gopinath"
    ]
   ],
   "title": "Adaptation of front end parameters in a speech recognizer",
   "original": "i04_0021",
   "page_count": 4,
   "order": 9,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "In this paper we consider the problem of adapting parameters of the algorithm used for extraction of features. Typical speech recognition systems use a sequence of modules to extract features which are then used for recognition. We present a method to adapt the parameters in these modules under a variety of criteria, e.g maximum likelihood, maximum mutual information. This method works under the assumption that the functions that the modules implement are differentiable with respect to their inputs and parameters. We use this framework to optimize a linear transform preceding the linear discriminant analysis (LDA) matrix and show that it gives significantly better performance than a linear transform after the LDA matrix with small amounts of data. We show that linear transforms can be estimated by directly optimizing likelihood or the MMI objective without using auxiliary functions. We also apply the method to optimize the Mel bins, and the compression power in a system that uses power law compression.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-8"
  },
  "giuliani04_interspeech": {
   "authors": [
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "Speaker normalization through constrained MLLR based transforms",
   "original": "i04_2893",
   "page_count": 4,
   "order": 10,
   "p1": "2893",
   "pn": "2896",
   "abstract": [
    "In this paper, a novel speaker normalization method is presented and compared to a well known vocal tract length normalization method. With this method, acoustic observations of training and testing speakers are mapped into a normalized acoustic space through speaker-specific transformations with the aim of reducing inter-speaker acoustic variability. For each speaker, an affine transformation is estimated with the goal of reducing the mismatch between the acoustic data of the speaker and a set of target hidden Markov models. This transformation is estimated through constrained maximum likelihood linear regression and then applied to map the acoustic observations of the speaker into the normalized acoustic space. Recognition experiments made use of two corpora, the first one consisting of adults' speech, the second one consisting of children's speech. Performing training and recognition with normalized data resulted in a consistent reduction of the word error rate with respect to the baseline systems trained on unnormalized data. In addition, the novel method always performed better than the reference vocal tract length normalization method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-9"
  },
  "mu04_interspeech": {
   "authors": [
    [
     "Xiangyu",
     "Mu"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Multi-layer structure MLLR adaptation algorithm with subspace regression classes and tying",
   "original": "i04_2897",
   "page_count": 4,
   "order": 11,
   "p1": "2897",
   "pn": "2900",
   "abstract": [
    "MLLR is a parameter transformation technique for both speaker and environment adaptation. When the amount of adaptation data is scarce, it is necessary to do adaptation with regression classes. In this paper, we present a rapid MLLR adaptation algorithm, which is called Multi-layer structure MLLR adaptation with subspace regression classes and tying (SRCMLR). The method groups the Gaussians on a finer acoustic subspace level. The motivation is that clustering at subspaces of lower dimensions results in lower distortion, and there are fewer parameters to be estimated for the subsequent MLLR transformation matrix. On the other hand, the multi-layer structure generates a regression class dynamically for each subspace using the outcome of the former MLLR transformation. By using the transform structure, computation load in performing transformation is much reduced. Experiments in large vocabulary mandarin speech recognition show the advantages of SRCMLLR over the traditional MLLR while the amount adaptation data is scarce.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-10"
  },
  "stemmer04_interspeech": {
   "authors": [
    [
     "Georg",
     "Stemmer"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Adaptation in the pronunciation space for non-native speech recognition",
   "original": "i04_2901",
   "page_count": 4,
   "order": 12,
   "p1": "2901",
   "pn": "2904",
   "abstract": [
    "We introduce a new technique to improve the recognition of non-native speech. The underlying assumption is that for each non-native pronunciation of a speech sound, there is at least one sound in the target language that has a similar native pronunciation. The adaptation is performed by HMM interpolation between adequate native acoustic models. The interpolation partners are determined automatically in a data-driven manner. Our experiments show that this technique is suitable for both the off-line adaptation to a whole group of speakers as well as for the unsupervised online adaptation to a single speaker. Results are given both for spontaneous non-native English speech as well as for a set of read non-native German utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-11"
  },
  "wang04_interspeech": {
   "authors": [
    [
     "Xuechuan",
     "Wang"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust ASR model adaptation by feature-based statistical data mapping",
   "original": "i04_2905",
   "page_count": 4,
   "order": 13,
   "p1": "2905",
   "pn": "2908",
   "abstract": [
    "Automatic speech recognition (ASR) model adaptation is important to many real-life ASR applications due to the variability of speech. The differences of speaker, bandwidth, context, channel and et al. between speech databases of initial ASR models and application data can be major obstacles to the effectiveness of ASR models. ASR models, therefore, need to be adapted to the application environments. Maximum Likelihood Linear Regression (MLLR) is a popular model-based method mainly used for speaker adaptation. This paper proposes a feature-based statistical Data Mapping (SDM) approach, which is more flexible than MLLR in various applications, such as different bandwidth and context. Experimental results on the TIMIT database show that ASR models adapted by the SDM approach have improved accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-12"
  },
  "han04_interspeech": {
   "authors": [
    [
     "Zhaobing",
     "Han"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A novel target-driven generalized JMAP adaptation algorithm",
   "original": "i04_2909",
   "page_count": 4,
   "order": 14,
   "p1": "2909",
   "pn": "2912",
   "abstract": [
    "Adapting the parameters of a statistical speaker independent continuous speech recognizer to the speaker can significantly improve the recognition performance and robustness of the system. In this paper, we propose a novel target-driven speaker adaptation method, Generalized Joint Maximum a Posteriori (GJMAP), which extends and improves the previous successful method JMAP. GJMAP partitions the HMM parameters with respect to the adaptation data, using the priori phonetic knowledge. The generation of regression class trees is dynamically constructed on the target-driven principle in order to obtain the maximum increase of the auxiliary function. An off-line adaptation experiment on large vocabulary continuous speech recognition is carried out. The experimental results show GJMAP has more advantages than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-13"
  },
  "mak04b_interspeech": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Simon",
     "Ho"
    ],
    [
     "James T.",
     "Kwok"
    ]
   ],
   "title": "Speedup of kernel eigenvoice speaker adaptation by embedded kernel PCA",
   "original": "i04_2913",
   "page_count": 4,
   "order": 15,
   "p1": "2913",
   "pn": "2916",
   "abstract": [
    "Recently, we proposed an improvement to the eigenvoice (EV) speaker adaptation called kernel eigenvoice (KEV) speaker adaptation. In KEV adaptation, eigenvoices are computed using kernel PCA, and a new speaker's adapted model is implicitly computed in the kernel-induced feature space. Due to many online kernel evaluations, both adaptation and subsequent recognition of KEV adaptation are slower than EV adaptation. In this paper, we eliminate all online kernel computations by finding an approximate pre-image of the implicit adapted model found by KEV adaptation. Furthermore, the two steps of finding the implicit adapted model and its approximate pre-image are integrated by embedding the kernel PCA procedure in our new embedded kernel eigenvoice (eKEV) speaker adaptation method. When tested in an TIDIGITS task with less than 10s of adaptation speech, eKEV adaptation obtained a speedup of 6-14 times in adaptation and 136 times in recognition over KEV adaptation with 12-13% relative improvement in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-14"
  },
  "jeon04_interspeech": {
   "authors": [
    [
     "Hyung Bae",
     "Jeon"
    ],
    [
     "Dong Kook",
     "Kim"
    ]
   ],
   "title": "Maximum a posteriori eigenvoice speaker adaptation for Korean connected digit recognition",
   "original": "i04_2917",
   "page_count": 4,
   "order": 16,
   "p1": "2917",
   "pn": "2920",
   "abstract": [
    "In this paper, we present a maximum a posteriori (MAP) eigenvoice speaker adaptation approach to the self-adaptation system. The proposed MAP eigenvoice is developed by introducing a probability density for the eigenvoice coefficients such as MAPLR adaptation. And we make a self-adaptation system which is useful to public user, because user does not need to speak several sentences for adaptation. In self-adaptation system we use only one utterance that will be recognized, so we use eigenvoice adaptation with MAP criterion that is most robust adaptation algorithm for very small adaptation data. In a series of self-adaptation experiments on the Korean connected digit recognition task, we demonstrate that the proposed approach achieves a good performance for a very small amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-15"
  },
  "wang04b_interspeech": {
   "authors": [
    [
     "Wei",
     "Wang"
    ],
    [
     "Stephen",
     "Zahorian"
    ]
   ],
   "title": "Vocal tract normalization based on spectral warping",
   "original": "i04_2921",
   "page_count": 4,
   "order": 17,
   "p1": "2921",
   "pn": "2924",
   "abstract": [
    "Two techniques for speaker adaptation based on frequency scale modifications are described and evaluated. In one method, minimum mean square error matching is performed between a spectral template for each speaker to a \"typical speaker\" spectral template. One parameter, a warping factor, is used to control the spectral matching. In the second method, a neural network classifier is used to adjust the frequency warping factor for each speaker so as to maximize vowel classification performance for each speaker. A vowel classifier trained only with normalized female speech and tested only with normalized male speech, or vice versa, is nearly as accurate as when speaker genders are matched for training and testing, and the speech is not normalized. The improvement due to normalization is much smaller, if training and test data are matched. The normalization based on classification performance is superior to that based on minimizing mean square error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-16"
  },
  "tanaka04_interspeech": {
   "authors": [
    [
     "Koji",
     "Tanaka"
    ],
    [
     "Fuji",
     "Ren"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Satoru",
     "Tsuge"
    ]
   ],
   "title": "Acoustic model adaptation for coded speech using synthetic speech",
   "original": "i04_2925",
   "page_count": 4,
   "order": 18,
   "p1": "2925",
   "pn": "2928",
   "abstract": [
    "In this paper, we describe a novel acoustic model adaptation technique which generates \"speaker-independent\" HMM for the target environment. Recently, personal digital assistants like cellular phones are shifting to IP terminals. The encoding-decoding process utilized for transmitting over IP networks deteriorates the quality of speech data. This deterioration causes degradation in speech recognition performance. Acoustic model adaptations can improve recognition performance. However, the conventional adaptation methods usually require a large amount of adaptation data. The proposed method uses HMM-based speech synthesis to generate adaptation data from the acoustic model of HMM-based speech recognizer, and consequently does not require any speech data for adaptation. Experimental results on G.723.1 coded speech recognition show that the proposed method improves speech recognition performance. A relative word error rate reduction of approximately 12% was observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-17"
  },
  "suzuki04_interspeech": {
   "authors": [
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Hirokazu",
     "Ogasawara"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Yuichi",
     "Ohkawa"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Speaker adaptation method for CALL system using bilingual speakers' utterances",
   "original": "i04_2929",
   "page_count": 4,
   "order": 19,
   "p1": "2929",
   "pn": "2932",
   "abstract": [
    "Several CALL systems have two acoustic models to evaluate a learner's pronunciation. In order to achieve high performance for evaluation, speaker adaptation method is introduced in CALL system. It requires adaptation data of a target language, however, a learner cannot pronounce correctly. In this paper, we proposed two types of new speaker adaptation methods for CALL system. The new methods only require learner's utterance of the native language for adaptation. The first method is an algorithm to adapt acoustic models using bilingual's utterances. The speaker-independent acoustic models of native and target languages are adapted to a bilingual speaker once, then they are adapted to the learner again using the learner's speech of the native language. Phoneme recognition accuracy is about 5% higher than the baseline method. The second method is a training algorithm of an acoustic model. It can robustly train bilinguals' model from a few bilinguals' utterances. Phoneme recognition accuracy is about 10% higher than the baseline method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-18"
  },
  "watanabe04_interspeech": {
   "authors": [
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Acoustic model adaptation based on coarse/fine training of transfer vectors and its application to a speaker adaptation task",
   "original": "i04_2933",
   "page_count": 4,
   "order": 20,
   "p1": "2933",
   "pn": "2936",
   "abstract": [
    "In this paper, we propose a novel adaptation technique based on coarse/fine training of transfer vectors. We focus on transfer vector estimation of a Gaussian mean from an initial model to an adapted model. The transfer vector is decomposed into a direction vector and a scaling factor. By using tied-Gaussian class (coarse class) estimation for the direction vector, and by using individual Gaussian class (fine class) estimation for the scaling factor, we can obtain accurate transfer vectors with a small number of parameters. Simple training algorithms for transfer vector estimation are analytically derived using the variational Bayes, maximum a posteriori (MAP) and maximum likelihood methods. Speaker adaptation experiments show that our proposals clearly improve speech recognition performance for any amount of adaptation data, compared with conventional MAP adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-19"
  },
  "tsai04_interspeech": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Shih-Sian",
     "Cheng"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Speaker clustering of speech utterances using a voice characteristic reference space",
   "original": "i04_2937",
   "page_count": 4,
   "order": 21,
   "p1": "2937",
   "pn": "2940",
   "abstract": [
    "This paper presents an effective technique for clustering speech utterances based on their associated speaker. In attempts to determine which utterances are from the same speakers, a prerequisite is to measure the similarity of voice characteristics between utterances. Since the vast majority of existing methods evaluate the inter-utterance similarity by taking only the information from the spectrum-based features of utterance pairs into account, the resulting clusters may not be well relevant to speaker, but instead likely to the environmental conditions or other acoustic classes. To compensate for this shortcoming, this study proposes to project utterances from their spectrum-based feature representation onto a reference space trained to cover the generic voice characteristics inherently in all of the utterances to be clustered. The resultant projection vectors naturally reflect the relationships between all the utterances and are more robust against the interference from non-speaker factors. We exemplarily present three distinct implementations for reference space creation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-20"
  },
  "kim04_interspeech": {
   "authors": [
    [
     "Young Kuk",
     "Kim"
    ],
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Performance improvement of connected digit recognition using unsupervised fast speaker adaptation",
   "original": "i04_2941",
   "page_count": 4,
   "order": 22,
   "p1": "2941",
   "pn": "2944",
   "abstract": [
    "In this paper, we investigate unsupervised fast speaker adaptation based on eigenvoice to improve the performance of Korean connected digit recognition over the telephone channel. In addition, utterance verification is introduced into speaker adaptation to examine whether input utterance is appropriate to adaptation or not. Performance evaluation showed that the proposed method yielded performance improvements. We obtained 18%-22% string error reduction by the N-best-based fast speaker adaptation method with utterance verification using Support Vector Machine.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-21"
  },
  "kim04b_interspeech": {
   "authors": [
    [
     "Hyung Soon",
     "Kim"
    ],
    [
     "Hwa Jeon",
     "Song"
    ]
   ],
   "title": "Simultaneous estimation of weights of eigenvoices and bias compensation vector for rapid speaker adaptation",
   "original": "i04_2945",
   "page_count": 4,
   "order": 23,
   "p1": "2945",
   "pn": "2948",
   "abstract": [
    "Eigenvoice based speaker adaptation method is known to be very effective tool for rapid speaker adaptation. Stochastic matching approach is also known as a powerful method to reduce the mismatch between training and test environments. In this paper, we simultaneously applied two methods for speaker adaptation and environment compensation space based on the eigenvoice adaptation framework. In experiments for vocabulary-independent word recognition task with supervised mode adaptation, the proposed method shows higher performance improvement than conventional eigenvoice adaptation method with a small adaptation data. We obtained 19~30% relative improvement with only single adaptation utterance and obtained 37% relative improvement with 50 adaptation utterances by proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-22"
  },
  "wolfel04_interspeech": {
   "authors": [
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Speaker dependent model order selection of spectral envelopes",
   "original": "i04_2949",
   "page_count": 4,
   "order": 24,
   "p1": "2949",
   "pn": "2952",
   "abstract": [
    "This work introduces a maximum-likelihood based model order (MO) selection technique for spectral envelopes to apply speaker dependent adaptation in the feature-space similar to vocal tract length normalization. Speech recognition systems based on spectral envelopes are using a fixed MO for the underlying linear parametric model. Using a fixed MO over different speakers or channels might not be optimal. To address this problem we investigated the use of warped and scaled minimum variance distortionless response spectral estimation techniques with speaker dependent MOs based on a maximum-likelihood criteria. Comparing experimental results on the Translanguage English Database we can show an improvement by 1,9% relative compared to the word error rate by the fixed MO and 3,5% relative to the traditional Mel-frequency cepstral coefficients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-23"
  },
  "bocchieri04_interspeech": {
   "authors": [
    [
     "Enrico",
     "Bocchieri"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Murat",
     "Saraclar"
    ]
   ],
   "title": "Methods for task adaptation of acoustic models with limited transcribed in-domain data",
   "original": "i04_2953",
   "page_count": 4,
   "order": 25,
   "p1": "2953",
   "pn": "2956",
   "abstract": [
    "Application specific acoustic models provide the best recognition accuracy, but they are expensive to train, because they require the transcription of large amount of in-domain speech. This paper focuses on the acoustic model estimation given limited in-domain transcribed speech data, and large amounts of transcribed out-of-domain data. First, we evaluate several combinations of known methods to optimize the adaptation/training of acoustic models on the limited in-domain speech data. Then, we propose Gaussian sharing to combine in-domain models with out-of-domain models, and a data generation process to simulate the presence of more speakers in the in-domain data. In a spoken language dialog application, we contrast our methods against an upper accuracy bound of 69.1% (model trained on many in-domain data) and a lower bound of 60.8% (no in-domain data). Using only 2 hours of in-domain speech for model estimation, we improve the accuracy by 5.1% (to 65.9%) over the lower bound.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-24"
  },
  "fujii04_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Fujii"
    ],
    [
     "Tetsuya",
     "Ishikawa"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Tomoyosi",
     "Akiba"
    ]
   ],
   "title": "Unsupervised topic adaptation for lecture speech retrieval",
   "original": "i04_2957",
   "page_count": 4,
   "order": 26,
   "p1": "2957",
   "pn": "2960",
   "abstract": [
    "We are developing a cross-media information retrieval system, in which users can view specific segments of lecture videos by submitting text queries. To produce a text index, the audio track is extracted from a lecture video and a transcription is generated by automatic speech recognition. In this paper, to improve the quality of our retrieval system, we extensively investigate the effects of adapting acoustic and language models on speech recognition. We perform an MLLR-based method to adapt an acoustic model. To obtain a corpus for language model adaptation, we use the textbook for a target lecture to search a Web collection for the pages associated with the lecture topic. We show the effectiveness of our method by means of experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-25"
  },
  "liu04_interspeech": {
   "authors": [
    [
     "Haibin",
     "Liu"
    ],
    [
     "Zhenyang",
     "Wu"
    ]
   ],
   "title": "Mean and covariance adaptation based on minimum classification error linear regression for continuous density HMMs",
   "original": "i04_2961",
   "page_count": 4,
   "order": 27,
   "p1": "2961",
   "pn": "2964",
   "abstract": [
    "The performance of speech recognition system will be significantly deteriorated because of the mismatches between training and testing conditions. This paper addresses the problem and proposes an algorithm to adapt the mean and covariance of HMM simultaneously within the minimum classification error linear regression (MCELR) framework. Rather than estimating the transformation parameters using maximum likelihood estimation (MLE) or maximum a posteriori, we proposed to use minimum classification error (MCE) as the estimation criterion. The proposed algorithm, called IMCELR (Improved MCELR), has been evaluated on a Chinese digit recognition tasks based on continuous density HMM. The experiments show that the proposed algorithm is more efficient than maximum likelihood linear regression with the same amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-26"
  },
  "nagino04_interspeech": {
   "authors": [
    [
     "Goshu",
     "Nagino"
    ],
    [
     "Makoto",
     "Shozakai"
    ]
   ],
   "title": "Design of ready-made acoustic model library by two-dimensional visualization of acoustic space",
   "original": "i04_2965",
   "page_count": 4,
   "order": 28,
   "p1": "2965",
   "pn": "2968",
   "abstract": [
    "This paper proposes the technique enabling a design of ready-made library composed of high performance and small size acoustic models utilizing the method of visualizing multiple HMM acoustic models onto two-dimensional space (the COSMOS method: aCOustic Space Map Of Sound), and providing one of these models without overburdening users. The acoustic space (as expressed in multi-dimensional future parameters) is partitioned into zones on twodimensional space, allowing for the creation of highly precise acoustic models through the generation of acoustic models for respective zones of the acoustic space. A set of these acoustic models is called an acoustic model library. In an experiment of this paper, a plotted map (called the COSMOS map) featuring a total of 145 male speakers speaking in various styles was generated utilizing the COSMOS method. Through the COSMOS map, the distribution of each speaking styles and the relationship between the positioning of the speaker on the COSMOS map and the speech-recognition performance were analyzed, thereby demonstrating the effectiveness of the COSMOS method in the analysis of acoustic space. The COSMOS map was then partitioned into concentric acoustic space zones to produce acoustic models representing each acoustic space zones. By selecting the acoustic model providing maximum likelihood score effectively using voice samples consisting of 5 words, the acoustic model, even if expressed in single Gaussian distribution, showed high performance comparable to speaker-independent acoustic model (called SI-model) expressed in 16 mixture Gaussian distributions. Furthermore, the acoustic model showed performance higher than SI-model adapted with voice samples of 30 words by the MLLR [2] method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-27"
  },
  "gauvain04_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Abdel",
     "Messaoudi"
    ],
    [
     "Holger",
     "Schwenk"
    ]
   ],
   "title": "Language recognition using phone latices",
   "original": "i04_0025",
   "page_count": 4,
   "order": 29,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "This paper proposes a new phone lattice based method for automatic language recognition from speech data. By using phone lattices some approximations usually made by language identification (LID) systems relying on phonotactic constraints to simplify the training and decoding processes can be avoided. We demonstrate the use of phone lattices both in training and testing significantly improves the accuracy of a phonotactically based LID system. Performance is further enhanced by using a neural network to combine the results of multiple phone recognizers. Using three phone recognizers with context independent phone models, the system achieves an equal error rate of 2.7% on the Eval03 NIST detection test (30s segment, primary condition) with an overall decoding process that runs faster than real-time (0.5xRT).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-28"
  },
  "huckvale04_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "ACCDIST: a metric for comparing speakers' accents",
   "original": "i04_0029",
   "page_count": 4,
   "order": 30,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "This paper introduces a new metric for the quantitative assessment of the similarity of speakers' accents. The ACCDIST metric is based on the correlation of inter-segment distance tables across speakers or groups. Basing the metric on segment similarity within a speaker ensures that it is sensitive to the speaker's pronunciation system rather than to his or her voice characteristics. The metric is shown to have an error rate of only 11% on the accent classification of speakers into 14 English regional accents of the British Isles, half the error rate of a metric based on spectral information directly. The metric may also be useful for cluster analysis of accent groups.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-29"
  },
  "levit04_interspeech": {
   "authors": [
    [
     "Michael",
     "Levit"
    ],
    [
     "Allen",
     "Gorin"
    ],
    [
     "Patrick",
     "Haffner"
    ],
    [
     "Hiyan",
     "Alshawi"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Aspects of named entity processing",
   "original": "i04_0033",
   "page_count": 4,
   "order": 31,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "In this paper we investigate the utility of three aspects of named entity processing: detection, localization and value extraction. We corroborate this task categorization by providing examples of practical applications for each of these subtasks. We also suggest methods for tackling these subtasks, giving particular attention to working with speech data. We employ Support Vector Machines to solve the detection task and show how localization and value extraction can successfully be dealt with using a combination of grammar-based and statistical methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-30"
  },
  "crego04_interspeech": {
   "authors": [
    [
     "Josep M.",
     "Crego"
    ],
    [
     "José B.",
     "Marino"
    ],
    [
     "Adria de",
     "Gispert"
    ]
   ],
   "title": "Finite-state-based and phrase-based statistical machine translation",
   "original": "i04_0037",
   "page_count": 4,
   "order": 32,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "This paper shows the common framework that underlies the translation systems based on phrases or driven by finite state transducers, and summarizes a first comparison between them. In both approaches the translation process is based on pairs of source and target strings of words (segments) related by word alignment. Their main difference comes from the statistical modeling of the translation context. The experimental study has been carried out on an English/Spanish version of the VERBMOBIL corpus. Under the constrain of a monotone composition of translated segments to generate the target sentence, the finite state based translation outperforms the phrase based counterpart.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-31"
  },
  "schultz04_interspeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Szu-Chen",
     "Jou"
    ],
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Shirin",
     "Saleem"
    ]
   ],
   "title": "Using word latice information for a tighter coupling in speech translation systems",
   "original": "i04_0041",
   "page_count": 4,
   "order": 33,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "In this paper we present first experiments towards a tighter coupling between Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT) to improve the overall performance of our speech translation system. In coventional speech translation systems, the recognizer outputs a single hypothesis which is then translated by the SMT system. This approach has the limitation of being largely dependent on the word error rate of the first best hypothesis. The word error rate is typically lowered by generating many alternative hypotheses in the form of a word lattice. The information in the word lattice and the scores from the recognizer can be used by the translation system to obtain better performance. In our experiments, by switching from the single best hypotheses to word lattices as the interface between ASR and SMT, and by introducing weighted acoustic scores in the translation system, the overall performance was increased by 16.22%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-32"
  },
  "misu04_interspeech": {
   "authors": [
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kazunori",
     "Komatani"
    ]
   ],
   "title": "Confirmation strategy for document retrieval systems with spoken dialog interface",
   "original": "i04_0045",
   "page_count": 4,
   "order": 34,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "Adequate confirmation is indispensable in spoken dialog systems to eliminate misunderstandings caused by speech recognition errors. Spoken language also inherently includes redundant expressions such as disfluency and out-of-domain phrases, which do not contribute to task achievement. It is easy to define a set of keywords to be confirmed for conventional database query tasks, but not straightforward in general document retrieval tasks. In this paper, we propose two statistical measures for identifying portions to be confirmed. A relevance score (RS) represents matching degree with the document set. A significance score (SS) detects portions that consequently affect the retrieval results. With these measures, the system can generate confirmation prior to and posterior to the retrieval, respectively. The strategy is implemented and evaluated with retrieval from software support knowledge base of 40K entries. It is shown that the proposed strategy using the two measures is more efficient than using the conventional confidence measure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-33"
  },
  "lee04c_interspeech": {
   "authors": [
    [
     "Shi-Wook",
     "Lee"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ]
   ],
   "title": "Multilayer subword units for open-vocabulary spoken document retrieval",
   "original": "i04_1553",
   "page_count": 4,
   "order": 35,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "This paper presents the developed open-vocabulary spoken document retrieval system including the newly proposed subphonetic segment(SPS) unit and combining multilayer subword units. There are two principal approaches to the task of Spoken Document Retrieval(SDR), the subword-based approach and the word-based approach. An inevitable problem of this approach is the fact that the vocabulary size is limited. An alternative approach is to perform retrieval by subword-based transcriptions resulted from a subword recognizer. Subword-based SDR has the advantages that the recognizer is less expensive and open-vocabulary retrieval is possible, because the recognition component is not bound to any vocabulary. Our approach to SDR is based on a subword recognizer which initially transforms the spoken documents into subword sequences. From the experimental evaluation on the Japanese retrieval we confirmed that using the proposed SPS unit and the combination of multilayer subword units is effective for open-vocabulary spoken document retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-34"
  },
  "itoh04_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ]
   ],
   "title": "An efficient partial matching algorithm toward speech retrieval by speech",
   "original": "i04_1557",
   "page_count": 4,
   "order": 36,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "This paper proposes a new efficient partial matching algorithm, called Island Driven Partial Matching (IDPM) based on Continuous Dynamic Programming (CDP), to realize flexible retrieval from a speech database by query speech. IDPM enables detecting the sections in the speech database which match partial sections of the query speech efficiently. IDPM applies CDP to short and constant length of unit reference patterns, which are composed of the query speech, and finds the best matching island sections in the speech database. Arbitrary lengths of similar sections are detected by only checking those islands sections. Some experiments were conducted for conversational speech and the results showed IDPM enables the fast matching between arbitrary sections of the reference pattern and the input speech without declining the performance in detecting similar sections compared with our former method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-35"
  },
  "sedogbo04_interspeech": {
   "authors": [
    [
     "Celestin",
     "Sedogbo"
    ],
    [
     "Sebastien",
     "Herry"
    ],
    [
     "Bruno",
     "Gas"
    ],
    [
     "Jean Luc",
     "Zarader"
    ]
   ],
   "title": "Language detection by neural discrimination",
   "original": "i04_1561",
   "page_count": 4,
   "order": 37,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "We present in this paper a new method of language detection, which allows detection on telephone speech with short sentences (3 seconds). The modeling based on neural networks allows to model any language without acoustic decomposition of the speech signal. We applied our system for a detection task over 11 languages contained in the OGI MLTS corpus with 3 seconds duration signals. The results obtained with our design allow to detect languages with an average competitive rate of 77% for a real time application running on a P4 1.7GHz based platform.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-36"
  },
  "cordoba04_interspeech": {
   "authors": [
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Valentin",
     "Sama"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Luis F.",
     "D'Haro"
    ],
    [
     "Fernando",
     "Fernandez"
    ]
   ],
   "title": "Language identification techniques based on full recognition in an air traffic control task",
   "original": "i04_1565",
   "page_count": 4,
   "order": 38,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "Automatic language identification has become an important issue in recent years in speech recognition systems. In this paper, we present the work done in language identification for an air traffic control speech recognizer for continuous speech. The system is able to distinguish between Spanish and English. We present several language identification techniques based on full recognition that improve the baseline results obtained using the most commonly known \"PPRLM\" technique. We have in our database some task specific critical problems for language identification like non native speakers, extremely spontaneous speech or Spanish-English mix in the same sentence. We confirm that PPRLM is quite sensible to those problems and that a technique based on a Bayesian classifier is the one with the best performance in spite of its higher computational cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-37"
  },
  "hansen04_interspeech": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Rongqing",
     "Huang"
    ],
    [
     "Ayako",
     "Ikeno"
    ]
   ],
   "title": "Dialect analysis and modeling for automatic classification",
   "original": "i04_1569",
   "page_count": 4,
   "order": 39,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "In this paper, we present our recent work in the analysis and modeling of speech under dialect. Dialect and accent significantly influence automatic speech recognition performance, and therefore it is critical to detect and classify non-native speech. In this study, we consider three areas that include: (i) prosodic structure (normalized f0, syllable rate, and sentence duration), (ii) phoneme acoustic space modeling and sub-word classification, and (iii) word-level based modeling using large vocabulary data. The corpora used in this study include: the NATO N-4 corpus (2 accents, 2 dialects of English), TIMIT (7 dialect regions), and American and British English versions of the WSJ corpus. These corpora were selected because the contained audio material from specific dialects/accents of English (N- 4), were phonetically balanced and organized across U.S. (TIMIT), or contained significant amounts of read audio material from distinct dialects (WSJ). The results show that significant changes occur at the prosodic, phoneme space, and word levels for dialect analysis, and that effective dialect classification can be achieved using processing strategies from each domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-38"
  },
  "ferragne04_interspeech": {
   "authors": [
    [
     "Emmanuel",
     "Ferragne"
    ],
    [
     "Francois",
     "Pellegrino"
    ]
   ],
   "title": "Rhythm in read british English: interdialect variability",
   "original": "i04_1573",
   "page_count": 4,
   "order": 40,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "Duration features have been thought to be the most obvious correlates of speech rhythm. Previous studies have shown that they can be used to distinguish among some world's languages. This paper investigates to what extent the methods employed in these studies can be applied to the dialects of British English. We have tested whether a set of variables derived from automatically extracted duration measurements constitute reliable predictors that could be used for automatic dialect identification. Preliminary results show that the automatic procedure, combined with the high interspeaker variability, yield overlapping rather than crisp dialectal categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-39"
  },
  "fung04_interspeech": {
   "authors": [
    [
     "Pascale",
     "Fung"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Yongsheng",
     "Yang"
    ],
    [
     "Yihai",
     "Shen"
    ],
    [
     "Dekai",
     "Wu"
    ]
   ],
   "title": "A grammar-based Chinese to English speech translation system for portable devices",
   "original": "i04_1577",
   "page_count": 4,
   "order": 41,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "Portable devices such as PDA phones and smart phones are increasingly popular. Many of these devices already have voice dialing capability. The next step is to offer more powerful personal-assistant features such as speech translation. In this paper, we propose a system that can translate speech commands in Chinese into English, in real-time, on small, portable devices with limited memory and computational power. We address the various computational and platform issues of speech recognition and translation on portable devices. We propose fixed-point computation, discrete front-end speech features, bi-phone acoustic models, grammar-based speech decoding, and unambiguous inversion transduction grammars for transfer-based translation. As a result, our speech translation system requires only 500k memory and a 200MHz CPU.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-40"
  },
  "tur04_interspeech": {
   "authors": [
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "Cost-sensitive call classification",
   "original": "i04_1581",
   "page_count": 4,
   "order": 42,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "We present an efficient and effective method which extends the Boosting family of classifiers to allow the weighted classes. Typically classifiers do not treat individual classes separately. For most real world applications, this is not the case, not all classes have the same importance. The accuracy of a particular class can be more critical than others. In this paper we extend the mathematical formulation for Boosting to weigh the classes differently during training. We have evaluated this method for call classification in AT&T spoken language understanding system. Our results indicate significant improvements in the \"important\" classes without a significant loss in the overall performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-41"
  },
  "kurimo04_interspeech": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Ville",
     "Turunen"
    ],
    [
     "Inger",
     "Ekman"
    ]
   ],
   "title": "An evaluation of a spoken document retrieval baseline system in finish",
   "original": "i04_1585",
   "page_count": 4,
   "order": 43,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "This paper presents a baseline spoken document retrieval system in Finnish. Due to its agglutinative structure, Finnish speech can not be adequately transcribed using the standard large vocabulary continuous speech recognition approaches. The definition of a sufficient lexicon and the training of the statistical language models are difficult, because the words appear transformed by many inflections and compounds. In this work we apply a recently developed unlimited vocabulary speech recognition system that allows the use of n-gram language models based on morpheme-like subword units discovered in an unsupervised manner. In addition to word-based indexing, we also propose an indexing based on the subword units provided directly by our speech recognizer. In an initial evaluation of newsreading in Finnish, we obtained a fairly low recognition error rate and average document retrieval precisions close to that from human reference transcripts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-42"
  },
  "jiang04_interspeech": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Pengfei",
     "Liu"
    ],
    [
     "Imed",
     "Zitouni"
    ]
   ],
   "title": "Discriminative training of naive Bayes classifiers for natural language call routing",
   "original": "i04_1589",
   "page_count": 4,
   "order": 44,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "In this paper, we propose to use a discriminative training method to improve naive Bayes classifiers(NBC) in context of natural language call routing. As opposed to the traditional maximum likelihood estimation, all conditional probabilties in Naive Bayes classifers are estimated discriminatively based on the minimum classification error criterion. A smoothed classification error rate in training set is formulated as an objective function and the generalized probabilistic descent method is used to minimize the objective function with respect to all conditional probabilities in NBCs. Two versions of NBC are used in this work. In the first version all NBCs corresponding to various destinations use the same word feature set while destination-dependent feature set is chosen for each destination in the second version. Experimental results on a banking call routing task show that the discriminative training method can achieve up to about 30% error reduction over our best ML-trained system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-43"
  },
  "moreau04_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Moreau"
    ],
    [
     "Hyoung-Gook",
     "Kim"
    ],
    [
     "Thomas",
     "Sikora"
    ]
   ],
   "title": "Phonetic confusion based document expansion for spoken document retrieval",
   "original": "i04_1593",
   "page_count": 4,
   "order": 45,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "This paper presents a phone-based approach of spoken document retrieval (SDR), developed in the framework of the emerging MPEG-7 standard. We describe an indexing and retrieval system that uses phonetic information only. The retrieval method is based on the vector space IR model, using phone N-grams as indexing terms. We propose a technique to expand the representation of documents by means of phone confusion probabilities in order to improve the retrieval performance. This method is tested on a collection of short German spoken documents, using 10 city names as queries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-44"
  },
  "chung04_interspeech": {
   "authors": [
    [
     "Euisok",
     "Chung"
    ],
    [
     "Soojong",
     "Lim"
    ],
    [
     "Yi-Gyu",
     "Hwang"
    ],
    [
     "Myung-Gil",
     "Jang"
    ]
   ],
   "title": "Hybrid named entity recognition for question-answering system",
   "original": "i04_1597",
   "page_count": 4,
   "order": 46,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "Named entity recognition is important in a sophisticated information service such as Question-Answering and Text-Mining since most of the answer type and text mining unit depend on the named entity. Korean named entity recognition is difficult since each word of named entity has not specific features such as the capitalizing feature of English which represents named entity distinctly. In addition, since named entities are usually unknown words, the result of the morphological analyzer would make an error. Considering these problems this paper proposes hybrid named entity recognition system for Question-Answering which is constructed on people domain of encyclopedia. Finally, the experiment shows the soundness of proposed hybrid NER system which consists of determining word features, syllable based NER, pattern-rule based NER and statistical NER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-45"
  },
  "ajmera04_interspeech": {
   "authors": [
    [
     "Jitendra",
     "Ajmera"
    ],
    [
     "Iain",
     "McCowan"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "An online audio indexing system",
   "original": "i04_1601",
   "page_count": 4,
   "order": 47,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "This paper presents overview of an online audio indexing system, which creates a searchable index of speech content embedded in digitized audio files. This system is based on our recently proposed offline audio segmentation techniques. As the data arrives continuously, the system first finds boundaries of the acoustically homogenous segments. Next, each of these segments is classified as speech, music or mixture classes, where mixtures are defined as regions where speech and other non-speech sounds are present simultaneously and noticeably. The speech segments are then clustered together to provide consistent speaker labels. The speech and mixture segments are converted to text via an ASR system. The resulting words are time-stamped together with other metadata information (speaker identity, speech confidence score) in an XML file to rapidly identify and access target segments. In this paper, we analyze the performance at each stage of this audio indexing system and also compare it with the performance of the corresponding offline modules.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-46"
  },
  "sanders04_interspeech": {
   "authors": [
    [
     "Eric",
     "Sanders"
    ],
    [
     "Febe de",
     "Wet"
    ]
   ],
   "title": "Histogram normalisation and the recognition of names and ontology words in the MUMIS project",
   "original": "i04_1605",
   "page_count": 4,
   "order": 48,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "The automatic transcription of German football commentaries and the analysis thereof are described. Histogram normalisation was used to improve the transcription of the very noisy data. The recognition of player names and ontology words was also investigated, since these are of crucial importance for the information retrieval task for which the transcriptions were used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-47"
  },
  "amaral04_interspeech": {
   "authors": [
    [
     "Rui",
     "Amaral"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Improving the topic indexation and segmentation modules of a media watch system",
   "original": "i04_1609",
   "page_count": 4,
   "order": 49,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "This paper describes on-going work related to the topic segmentation and indexation module of an alert system for selective dissemination of multimedia information. This system was submitted in the past year to a field trial which exposed a number of issues that should be dealt with in order to improve its performance. Some of our efforts involved the use of multiple topics, confidence measures and named entity extraction. This paper discusses these approaches and the corresponding results which, unfortunately, are still affected by the limited amount of topic-annotated training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-48"
  },
  "barkatdefradas04_interspeech": {
   "authors": [
    [
     "Melissa",
     "Barkat-Defradas"
    ],
    [
     "Rym",
     "Hamdi"
    ],
    [
     "Emmanuel",
     "Ferragne"
    ],
    [
     "Francois",
     "Pellegrino"
    ]
   ],
   "title": "Speech timing and rhythmic structure in arabic dialects: a comparison of two approaches",
   "original": "i04_1613",
   "page_count": 4,
   "order": 50,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "This paper raises questions about the discrete or continuous nature of rhythm classes. Within this framework, our study investigates speech rhythm in the different Arabic dialects that have been constantly described as stress-timed compared with other languages belonging to different rhythm categories. Preliminary evidence from perceptual experiments revealed that listeners use speech rhythm cues to distinguish speakers of Arabic from North Africa from those of the Middle East. In an attempt to elucidate the reasons for this perceptual discrimination, an acoustic investigation based on duration measurement was carried out (i.e. percentages of vocalic intervals (%V) and the standard deviation of consonantal intervals (DC)). This experiment reveals that despite their rhythmic differences, all Arabic dialects still cluster around stress-timed languages exhibiting a different distribution from languages belonging to other rhythm categories such as French and Catalan. Besides, our study suggests that there is no such thing as clear-cut rhythm classes but rather overlapping categories. As a means of comparison, we also used Pairwise Variability Indices so as to validate the reliability of our findings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-49"
  },
  "wang04c_interspeech": {
   "authors": [
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Shih-sian",
     "Cheng"
    ]
   ],
   "title": "METRIC-SEQDAC: a hybrid approach for audio segmentation",
   "original": "i04_1617",
   "page_count": 4,
   "order": 51,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "This paper presents a hybrid approach for audio segmentation, in which the metric-based segmentation with long sliding windows is applied first to segment an audio stream into shorter sub-segments, and then the divide-and-conquer segmentation is applied to a fixed-length window that slides from the beginning to the end of each sub-segment to sequentially detect the remaining acoustic changes. The experimental results on five one-hour broadcast news shows show that our approach outperforms the existing metric-based and model-selection-based approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-50"
  },
  "kuo04_interspeech": {
   "authors": [
    [
     "Jen-Wei",
     "Kuo"
    ],
    [
     "Yao-Min",
     "Huang"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-min",
     "Wang"
    ]
   ],
   "title": "Statistical Chinese spoken document retrieval using latent topical information",
   "original": "i04_1621",
   "page_count": 4,
   "order": 52,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "Information retrieval which aims to provide people with easy access to all kinds of information is now becoming more and more emphasized. However, most approaches to information retrieval are primarily based on literal term matching and operate in a deterministic manner. Thus their performance is often limited due to the problems of vocabulary mismatch and not able to be steadily improved through use. In order to overcome these drawbacks as well as to enhance the retrieval performance, in this paper we explore the use of topical mixture model for statistical Chinese spoken document retrieval. Various kinds of model structures and learning approaches were extensively investigated. In addition, the retrieval capabilities were verified by comparison with the conventional vector space model and latent semantic indexing model, as well as our previously presented HMM/N-gram retrieval model. The experiments were performed on the TDT-2 Chinese collection. Very encouraging retrieval performance was obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-51"
  },
  "masahiko04_interspeech": {
   "authors": [
    [
     "Matsushita",
     "Masahiko"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Takehito",
     "Utsuro"
    ]
   ],
   "title": "Keyword recognition and extraction by multiple-LVCSRs with 60,000 words in speech-driven WEB retrieval task",
   "original": "i04_1625",
   "page_count": 4,
   "order": 53,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "This paper presents speech-driven Web retrieval models which accepts spoken search topics (queries) in the NTCIR-3 Web retrieval task. We experimentally evaluate the techniques of combining outputs of multiple LVCSR models with a language model(LM) with a 60,000 vocabulary size in recognition of spoken queries. As model combination techniques, we use the SVM learning. We show that the techniques of multiple LVCSR model combination can achieve improvement both in speech recognition and retrieval accuracies in speech-driven text retrieval. Comparing with the etrieval accuracies when a LM with a 20,000/60,000 vocabulary size is used in LVCSRs, the LM that has larger size of the vocabulary improves also retrieval accuracies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-52"
  },
  "zhang04_interspeech": {
   "authors": [
    [
     "Ruiqiang",
     "Zhang"
    ],
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Taro",
     "Watanabe"
    ],
    [
     "Eiichiro",
     "Sumita"
    ],
    [
     "Wai-Kit",
     "Lo"
    ]
   ],
   "title": "Improved spoken language translation using n-best speech recognition hypotheses",
   "original": "i04_1629",
   "page_count": 4,
   "order": 54,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "We intended to demonstrate the effect of using N-best speech recognition hypotheses for improving speech translation performance. A log-linear model, which integrated features from speech recognition and statistical machine translation, was used to rescore the translation candidates. Model parameters were estimated by optimizing an objectively measurable but subjectively relevant translation quality metric. Experimental results have shown that the proposed N-best approach improved translation quality over the conventional single-best approach. The improvements were confirmed consistently by several automatic translation evaluation metrics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-53"
  },
  "wong04_interspeech": {
   "authors": [
    [
     "Kakeung",
     "Wong"
    ],
    [
     "Man-hung",
     "Siu"
    ]
   ],
   "title": "Automatic language identification using discrete hidden Markov model",
   "original": "i04_1633",
   "page_count": 4,
   "order": 55,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "In the recent automatic language identification research, phonotactic approach has been studied in which all training utterances are passed through a tokenizer in order to get phonetic sequences to train the language model of different languages. The true transcription of the utterances was totally ignored. However, information in the transcription may process important discriminating power for language identification. In this paper, we propose to use discrete hidden Markov model that takes account of the potential error patterns of the acoustic tokenizer and incorporates the transcription of the utterances in the language model training. Furthermore, with the DHMM approach, LID using multiple phonetic tokenizers can simply be considered as using a multi-dimensional features to the DHMM allowing the making of joint decision earlier in the process. A system employing this approach produces 59.00% and 68.33% accuracy on 10-sec and 45-sec speech respectively on recognizing a close set of six languages in the OGI telephone speech corpus while the phonotactic approach gives 57.00% and 77.50% recognition accuracy on 10-sec and 45-sec speech when the phone recognizer uses threestate and three-mixture HMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-54"
  },
  "zhou04_interspeech": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Daniel",
     "Dechelotte"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Two-way speech-to-speech translation on handheld devices",
   "original": "i04_1637",
   "page_count": 4,
   "order": 56,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "This paper presents a two-way speech translation system that is completely hosted on an off-the-shelf handheld device. Specifically, this end-to-end system includes an HMM-based large vocabulary continuous speech recognizer (LVCSR) for both English and Chinese using statistical n-grams, a two-way translation system between English and Chinese, and, a multi-lingual speech synthesis system that outputs speech in the target language. This paper describes the system development and the functionality of the major components, focusing on the optimization efforts employed to achieve real time results on such a limited platform.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-55"
  },
  "blanchon04_interspeech": {
   "authors": [
    [
     "Hervé",
     "Blanchon"
    ]
   ],
   "title": "HLT modules scalability within the NESPOLE! project",
   "original": "i04_1641",
   "page_count": 4,
   "order": 57,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "The spoken dialogue translation project NESPOLE! proposed two showcases in order to focus on two important issues: scalability-namely, the capability of a system to progressively handle larger portions of a given domain - and cross-domain portability. Those concerns were rather new when the project was proposed. ShowCase-1 dealt with limited tourism, while ShowCase-2 consisted of ShowCase-2a on extended tourism (thus focusing on scalability) and ShowCase-2b on a medical domain (thus focusing on cross-domain portability). In this article, we address the issue of scalability for the French analysis and generation modules in the tourism domain. We discuss ShC-1 and ShC-2a evaluations results and evaluate our progress.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-56"
  },
  "kim04c_interspeech": {
   "authors": [
    [
     "Midam",
     "Kim"
    ]
   ],
   "title": "Correlation between VOT and F0 in the perception of Korean stops and affricates",
   "original": "i04_0049",
   "page_count": 4,
   "order": 58,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "This research examines the trading correlation between VOT and F0 in the production and perception of the three-way distinction of Korean stops and affricates, namely, lenis, aspirated, and fortis, in the word-initial position. For this research, I conducted production and perception tests. For the production test, two female and two male native speakers of Seoul Korean recorded a monosyllabic word list including /ka, kha, k*a, pa, pha, p*a, ta, tha, t*a, ca, cha, c*a/ 15 times in random order. On VOT-F0 plains, the results showed that lenis, aspirated and fortis were discriminated with the two cues of VOT and F0, without overlapping. The results of a MANOVA test showed that there is a significant difference in lenis, aspirated, and fortis with correlation between VOT and F0 (p<0.001). In the perception test, the stimuli were made by manipulating the sound files recorded in the production test in such a way that F0 values were heightened or lowered at 10 Hz intervals, fixing VOT values. 14 subjects (seven females and seven males) participated in the perception test. The results showed that more than 94% of all fortis stimuli were not influenced by F0 changes, and that VOT and F0 values at the lenis-aspirated boundary showed strong negative correlation (r=<-0.923). From these results, I concluded that: 1) Lenis, aspirated, and fortis of Korean word-initial consonants are distinguished with the correlation of VOT and F0; 2) F0 does not function as an acoustic cue in the perception of Korean fortis; and 3) There is a phonetic trade-off between VOT and F0 in the distinction of Korean lenis and aspirated stops and affricates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-57"
  },
  "noiray04_interspeech": {
   "authors": [
    [
     "Aude",
     "Noiray"
    ],
    [
     "Lucie",
     "Menard"
    ],
    [
     "Marie-Agnes",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "The development of anticipatory labial coarticulation in French: a pionering study",
   "original": "i04_0053",
   "page_count": 4,
   "order": 59,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "This article reports an experimental study initiated on labial anticipatory coarticulation in the framework of a description of motor control development. Four French children between 4 and 8 years old have been audio-visually recorded, uttering [iCny] puppets names, in which Cn corresponds to a varying number of intervocalic consonants. A kinetic lip area function has been obtained via the ICP tracking system (Lallouache [1]) in order to describe the anticipatory movements of vocalic targets. As several concurrent models have been evaluated to account for anticipation in adults but very few for children, and since the more robust has proved for French to be the Movement Expansion Model (MEM; Abry & Lallouache [2], [3], [4], Abry et al. [5]) we adopted this framework for testing the anticipatory motor behaviour in children.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-58"
  },
  "hunt04_interspeech": {
   "authors": [
    [
     "Melvyn John",
     "Hunt"
    ]
   ],
   "title": "Speech recognition, sylabification and statistical phonetics",
   "original": "i04_0057",
   "page_count": 4,
   "order": 60,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The classical approach in phonetics of careful observation of individual utterances can, this paper contends, be usefully augmented with automatic statistical analyses of large amounts of speech. Such analyses, using methods derived from speech recognition, are shown to quantify several known phonetic phenomena, most of which require syllable structure to be taken into account, and reveal some apparently new phenomena. Practical speech recognition normally ignores syllable structure. This paper presents quantitative evidence that prevocalic and postvocalic consonants behave differently. It points out some ways in which current speech recognition can be improved by taking syllable boundaries into account.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-59"
  },
  "tian04_interspeech": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ]
   ],
   "title": "Data-driven approaches for automatic detection of syllable boundaries",
   "original": "i04_0061",
   "page_count": 4,
   "order": 61,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "Syllabification is an essential component of many speech and language processing systems. The development of automatic speech recognizers frequently requires working with subword units such as syllables. More importantly, syllabification is an inevitable part of speech synthesis system. In this paper we present data-driven approaches to supervised learning and automatic detection of syllable boundaries. The generalization capability of the learning is investigated on the assignment of syllable boundaries to phoneme sequence representation in English. A rule-based self-correction algorithm is also proposed to automatically correct some syllabification errors. We conducted a series of experiments and the neural network approach is clearly better in terms of generalization performance and complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-60"
  },
  "cutler04_interspeech": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ],
    [
     "Nuria",
     "Sebastian-Galles"
    ]
   ],
   "title": "Phonemic repertoire and similarity within the vocabulary",
   "original": "i04_0065",
   "page_count": 4,
   "order": 62,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "Language-specific differences in the size and distribution of the phonemic repertoire can have implications for the task facing listeners in recognising spoken words. A language with more phonemes will allow shorter words and reduced embedding of short words within longer ones, decreasing the potential for spurious lexical competitors to be activated by speech signals. We demonstrate that this is the case via comparative analyses of the vocabularies of English and Spanish. A language which uses suprasegmental as well as segmental contrasts, however, can substantially reduce the extent of spurious embedding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-61"
  },
  "maskey04_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Alan",
     "Black"
    ],
    [
     "Laura",
     "Tomokiya"
    ]
   ],
   "title": "Boostrapping phonetic lexicons for new languages",
   "original": "i04_0069",
   "page_count": 4,
   "order": 63,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "Although phonetic lexicons are critical for many speech applications, the process of building one for a new language can take a significant amount of time and effort. We present a bootstrapping algorithm to build phonetic lexicons for new languages. Our method relies on a large amount of unlabeled text, a small set of 'seed words' with their phonetic transcription, and the proficiency of a native speaker in correctly inspecting the generated pronunciations of the words. The method proceeds by automatically building Letter-to-Sound (LTS) rules from a small set of the most commonly occurring words in a large corpus of a given language. These LTS rules are retrained as new words are added to the lexicon in an Active Learning step. This procedure is repeated until we have a lexicon that can predict the pronunciation of any word in the target language with the accuracy desired. We tested our approach for three languages: English, German and Nepali.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-62"
  },
  "broersma04_interspeech": {
   "authors": [
    [
     "Mirjam",
     "Broersma"
    ],
    [
     "K. Marieke",
     "Kolkman"
    ]
   ],
   "title": "Lexical representation of non-native phonemes",
   "original": "i04_1241",
   "page_count": 4,
   "order": 64,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "This study investigates whether the inaccurate processing of non-native phonemes leads to a not native-like representation of word forms containing these phonemes. Dutch and English listeners' processing of two English vowels and four plosives was studied in a phoneme monitoring experiment. The processing of difficult to identify non-native phonemes was compared to the processing of easy to identify ones. One of the vowels was difficult and the other easy to identify for Dutch listeners. The plosives were easy in word-initial and word-medial position and difficult to identify in word-final position for Dutch listeners. Lexical mediation was found to play a similar role for Dutch and English listeners, and there were no differences in the amount of lexical mediation for 'difficult' and 'easy' phonemes for Dutch listeners. This suggests that the inaccurate processing of non-native phonemes does not necessarily lead to a not native-like representation of word forms containing these phonemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-63"
  },
  "lee04d_interspeech": {
   "authors": [
    [
     "Jong-Pyo",
     "Lee"
    ],
    [
     "Tae-Yeoub",
     "Jang"
    ]
   ],
   "title": "A comparative study on the production of inter-stress intervals of English speech by English native speakers and Korean speakers",
   "original": "i04_1245",
   "page_count": 4,
   "order": 65,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "This study attempts to compare the Inter-Stress Interval (ISI) patterns of English between the native speakers of English and Korean. One of the invariable results of the experiments about English speech rhythm has been that the strict concept of isochronism did not seem to exist at least in the surface phonetic level. However, the remarkable difference shown from the production experiment of the present study suggests that distinction in language rhythms, especially between English and Korean, be apparent. While the English native speakers and the proficient Korean speakers of English consistently produce rather shorter increase in ISI duration as the number of unstressed syllables located between target stressed syllables increased, the non-proficient Korean speakers of English produce a little longer one. The position of an ISI in a sentence does not seem at the moment to affect critically the duration of the ISI.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-64"
  },
  "murano04_interspeech": {
   "authors": [
    [
     "Emi Zuiki",
     "Murano"
    ],
    [
     "Mihoko",
     "Teshigawara"
    ]
   ],
   "title": "Articulatory correlates of voice qualities of god guys and bad guys in Japanese anime: an MRI study",
   "original": "i04_1249",
   "page_count": 4,
   "order": 66,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "This paper examines the articulatory correlates of the Hero and Villain Voice Types, which were auditorily identified in a separate study on cartoon voices, using the MRI technique. The MRI images were in agreement with the previous auditory analysis results; the major difference between articulatory postures of heroes and those of villains and between two villainous voice types was found in the supraglottal states and the pharyngeal cavity. Auditory analysis can be as valid as any other analysis method depending on the level of training in a commonly accepted system such as Laver's framework for voice quality description. However, the MRI technique also allowed us to see what would not be observed otherwise, e.g., larynx height, pharyngeal cavity, vocal tract length, and the position of the hyoid bone. Auditory and physiological methods should be used in combination in order to further our understanding of the larynx and the pharynx.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-65"
  },
  "dusan04_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "Effects of phonetic contexts on the duration of phonetic segments in fluent read speech",
   "original": "i04_1253",
   "page_count": 4,
   "order": 67,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "Coarticulation is an important phenomenon that affects the realization of phonetic segments. The effects of coarticulation are prominent in both spectral and temporal domains. Various durational effects of phonetic contexts on the adjacent phonetic segments have been previously reported based on individual distinctive features (e.g., voiced stops lengthen and unvoiced stops shorten the preceding vowels) or specific contexts (e.g., both /s/ and /p/ are shorter in a /sp/ cluster). This paper presents a comprehensive method for analyzing the phonetic context effects of all phonetic segments on the duration of their preceding or succeeding adjacent phonetic segments in fluent read speech, using the TIMIT American English corpus. Statistical methods are employed to analyze the variations in mean durations of all phonetic segments as functions of preceding or succeeding phonetic identities. 99% confidence intervals for the mean durations are also presented to reveal which pairs of phonetic contexts present statistically significant differences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-66"
  },
  "fang04_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fang"
    ]
   ],
   "title": "A study on nasal coda los in continuous speech",
   "original": "i04_1257",
   "page_count": 4,
   "order": 68,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "In this study, statistical analysis is used to investigate nasal coda loss in spoken Standard Chinese. In order to find out the factors that influence nasal coda loss, we take into account the segmental and supra-segmental features and their interactions. We find out that articulation manner, post nasal coda boundary and tone influenced the nasal coda loss significantly, the interaction between tone and stress, tone and post boundary, articulation manner and tone are significant too.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-67"
  },
  "jian04_interspeech": {
   "authors": [
    [
     "Hua-Li",
     "Jian"
    ]
   ],
   "title": "An improved pair-wise variability index for comparing the timing characteristics of speech",
   "original": "i04_1261",
   "page_count": 4,
   "order": 69,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "The pair-wise variability index has become a useful and widely used tool for comparing syllable timing of speech. In this paper we present an improved pair-wise variability index based on median instead of means that can more strongly amplify and reveal the differences in the timing characteristics of two datasets. Further, it places less stringent requirements on the pre-processing of the measurements, and it is therefore more robust to outliers in the dataset. The effectiveness of the new measure is demonstrated through an example where the measure is applied to data based on American English speech and Taiwan English speech. The results obtained with the improved pair-wise variability index are compared to those of the standard pair-wise variability index and the rhythm ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-68"
  },
  "jian04b_interspeech": {
   "authors": [
    [
     "Hua-Li",
     "Jian"
    ]
   ],
   "title": "An acoustic study of speech rhythm in taiwan English",
   "original": "i04_1265",
   "page_count": 4,
   "order": 70,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "American English and Taiwan English have been found to exhibit different rhythmic patterns. American English is often described as a stress-timed language, while it has been suggested that Taiwan English is a syllable-timed language. This paper addresses the differences between these two varieties of English through an acoustic study. In particular the F1/F2 formant space is investigated. The results show that reduced vowels in American English are more concentrated in the F1/F2 formant space than reduced vowels in Taiwan English, which are more dispersed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-69"
  },
  "kim04d_interspeech": {
   "authors": [
    [
     "Sung-A",
     "Kim"
    ]
   ],
   "title": "Language specific phonetic rules: evidence from domain-initial strengthening",
   "original": "i04_1269",
   "page_count": 4,
   "order": 71,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "This paper investigates the domain-initial strengthening in English and Hamkyeong Korean. Although many languages are known to display domain-initial strengthening (Byrd 2000, Dilley, Shattuck- Hufnagel & Ostendorf 1996), it is yet unclear whether initial-syllable vowels preceded by consonants undergo it as well. This study presents the result of an experimental study of initial syllables in English and Hamkyeong Korean. Durations of initial-syllable vowels were compared to those of second vowels in real-word tokens for both languages. Hamkyeong Korean, like English, tuned out to strengthen the domain-initial consonants. With regard to vowel durations, we found no significant of prosodic effect in English. On the other hand, Hamkyeong Korean showed significant differences between durations of initial and non-initial vowels in the higher prosodic domains. The findings in the study are theoretically important as they reveal that the potentially-universal phenomenon of initial strengthening is subject to language specific variations in its implementation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-70"
  },
  "park04_interspeech": {
   "authors": [
    [
     "Hansang",
     "Park"
    ]
   ],
   "title": "Spectral characteristics of the release bursts in Korean alveolar stops",
   "original": "i04_1273",
   "page_count": 4,
   "order": 72,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "This study investigates spectral characteristics of the release bursts in Korean alveolar stops in terms of intensity, center of gravity, and skewness of the spectra of the release burst across phonation types and speakers. The results showed that there was no significant difference in intensity, center of gravity, or skewness across phonation types but a significant difference across speakers. This means that difference in phonation type does not lead to any significant difference in the spectra of the release burst. This study suggests that difference in the spectral characteristics of the release burst across phonation types can be ignored in speech synthesis or speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-71"
  },
  "son04_interspeech": {
   "authors": [
    [
     "Rob Van",
     "Son"
    ],
    [
     "Olga",
     "Bolotova"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Mietta",
     "Lennes"
    ]
   ],
   "title": "Frequency effects on vowel reduction in three typologically different languages (dutch, finish, Russian)",
   "original": "i04_1277",
   "page_count": 4,
   "order": 73,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "As a result of the cooperation in the Intas 915 project, annotated speech corpora have become available in three different languages for both read and spontaneous speech of some 4-5 male and 4-5 female speakers per language (6-10 minutes per speaker). These data have been used to study the effects of redundancy on acoustic vowel reduction, in terms of vowel duration, F1-F2 distance to a virtual target of reduction, spectral center of gravity, and vowel intensity. It was shown that in all three (typologically different) languages vowel redundancy increases acoustic reduction in the same way. The reduction of redundant vowels seems to be a language universal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-72"
  },
  "abresch04_interspeech": {
   "authors": [
    [
     "Julia",
     "Abresch"
    ],
    [
     "Stefan",
     "Breuer"
    ]
   ],
   "title": "Assessment of non-native phones in anglicisms by German listeners",
   "original": "i04_1281",
   "page_count": 4,
   "order": 74,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "By means of a pair comparison test, preferences of German native speakers for English or German sounds in spoken anglicisms were investigated. The collected data can be used as a reference point, which of the English xenophones have to be integrated into a German TTS system to allow for an appropriate pronunciation of anglicisms in German.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-73"
  },
  "kim04e_interspeech": {
   "authors": [
    [
     "Sunhee",
     "Kim"
    ]
   ],
   "title": "Phonology of exceptions for for Korean grapheme-to-phoneme conversion",
   "original": "i04_1285",
   "page_count": 4,
   "order": 75,
   "p1": "1285",
   "pn": "1289",
   "abstract": [
    "Being an essential part of a Korean speech recognition system and a Text-To-Speech (TTS) system, a Korean Grapheme-to-Phoneme conversion system is generally composed of a set of regular rules and an exceptions dictionary [1, 2, 3]. The exceptions have been recorded in the dictionary in a simple and random manner, whereas the researches on the regular rules have been actively progressed. This paper presents a systematic description of the exceptions for a Grapheme-to-Phoneme conversion system based on the analysis of entries of a lexical dictionary [4] from the phonological point of view, showing that the exceptions are related with certain limited phonological phenomena.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-74"
  },
  "shigeyoshi04_interspeech": {
   "authors": [
    [
     "Kitazawa",
     "Shigeyoshi"
    ],
    [
     "Shinya",
     "Kiriyama"
    ]
   ],
   "title": "Acoustic and prosodic analysis of Japanese vowel-vowel hiatus with laryngeal effect",
   "original": "i04_1289",
   "page_count": 4,
   "order": 76,
   "p1": "1289",
   "pn": "1293",
   "abstract": [
    "We investigated V-V hiatus through J-ToBI labeling and listening to whole phrases to estimate degree of discontinuity and to determine the exact boundary between two phrases if possible. Appropriate boundaries were found in most cases as the maximum perceptual score. Using electroglottography (EGG) and spectrogram, the acoustic phonological feature of these V-V hiatus was phrase-initial glottalization and phrase final nasalization observable in EGG and spectrogram, as well as phrase-final lengthening and phrase-initial shortening. The test materials are taken from the \"Japanese MULTEXT\", consisting of a particle - vowel (36), adjective - vowel (5), and word - word (4).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-75"
  },
  "tsukada04_interspeech": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "A cross-linguistic acoustic comparison of unreleased word-final stops: Korean and Thai",
   "original": "i04_1293",
   "page_count": 4,
   "order": 77,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "This study compared acoustic characteristics of final stops in Korean and Thai. Word-final stops are phonetically realized as unreleased stops in these languages. Native speakers of Korean and Thai produced monosyllabic words ending with [p t k] in each of their native languages. Formant frequencies of /i a u/ at the vowel's offset were examined. In both languages, the place effect was significant and interacted with the vowel type. For non-front vowels (/a/ and /u/), F2 offset was highest before [t], while for the front vowel (/i/), it was highest before [k]. Preliminary results of a perception experiment with English-speaking listeners suggest that the absence of release bursts is most detrimental to the intelligibility of [k], least for [p] and intermediate for [t].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-76"
  },
  "cho04_interspeech": {
   "authors": [
    [
     "Taehong",
     "Cho"
    ],
    [
     "Elizabeth K.",
     "Johnson"
    ]
   ],
   "title": "Acoustic correlates of phrase-internal lexical boundaries in dutch",
   "original": "i04_1297",
   "page_count": 4,
   "order": 78,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "The aim of this study was to determine if Dutch speakers reliably signal phrase-internal lexical boundaries, and if so, how. Six speakers recorded 4 pairs of phonemically identical strong-weak-strong (SWS) strings with matching syllable boundaries but mismatching intended word boundaries (e.g. 'reis # pastei' versus 'reispas # tij', or more broadly C1V2(C)#C2V2(C)C3V3(C) vs. C1V2 (C)C2V2(C)#C3V3 (C)). An Analysis of Variance revealed 3 acoustic parameters that were significantly greater in S#WS items (C2 DURATION, RIME1 DURATION, C3 BURST AMPLITUDE) and 5 parameters that were significantly greater in the SW#S items (C2 VOT, C3 DURATION, RIME2 DURATION, RIME3 DURATION, and V2 AMPLITUDE). Additionally, center of gravity measurements suggested that the [s] to [t] coarticulation was greater in 'reis # pa[st]ei' versus 'reispa[s] # [t]ij'. Finally, a Logistic Regression Analysis revealed that the 3 parameters (RIME1 DURATION, RIME2 DURATION, and C3 DURATION) contributed most reliably to a S#WS versus SW#S classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-77"
  },
  "cho04b_interspeech": {
   "authors": [
    [
     "Taehong",
     "Cho"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Phonotactics vs. phonetic cues in native and non-native listening: dutch and Korean listeners' perception of dutch and English",
   "original": "i04_1301",
   "page_count": 4,
   "order": 79,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "We investigated how listeners of two unrelated languages, Dutch and Korean, process phonotactically legitimate and illegitimate sounds spoken in Dutch and American English. To Dutch listeners, unreleased word-final stops are phonotactically illegal because word-final stops in Dutch are generally released in isolation, but to Korean listeners, released final stops are illegal because word-final stops are never released in Korean. Two phoneme monitoring experiments showed a phonotactic effect: Dutch listeners detected released stops more rapidly than unreleased stops whereas the reverse was true for Korean listeners. Korean listeners with English stimuli detected released stops more accurately than unreleased stops, however, suggesting that acoustic-phonetic cues associated with released stops improve detection accuracy. We propose that in non-native speech perception, phonotactic legitimacy in the native language speeds up phoneme recognition, the richness of acoustic-phonetic cues improves listening accuracy, and familiarity with the non-native language modulates the relative influence of these two factors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-78"
  },
  "kaminskaia04_interspeech": {
   "authors": [
    [
     "Svetlana",
     "Kaminskaia"
    ],
    [
     "Francois",
     "Poire"
    ]
   ],
   "title": "Comparing intonation of two varieties of French using normalized F0 values",
   "original": "i04_1305",
   "page_count": 4,
   "order": 80,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "Normalized F0 values measured from a specific prosodic domain (the Accentual Phrase, AP) are used in order to compare readings from four female speakers of two varieties of French: two Canadians (Quebec City area) and two French (Vendee area). Comparisons are based on APs showing similar internal organization (contour, number of syllables, timing contour-to-text) and identical position within an utterance. Normalized F0 values show greater extra-variety variation than intra-variety for specific parts of the contours, namely the L tones of LLH continuation contour and the first L and H tones of LHL final contour. Perceptual experiments will be later conducted based on these findings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-79"
  },
  "oh04_interspeech": {
   "authors": [
    [
     "Mira",
     "Oh"
    ],
    [
     "Kee-Ho",
     "Kim"
    ]
   ],
   "title": "Phonetic realization of the suffix-suppressed accentual phrase in Korean",
   "original": "i04_1309",
   "page_count": 4,
   "order": 81,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "Suffixes can surface or can be suppressed depending on context in Korean. Shin (1982) argues that the case marker-marked phrases deliver new information, while the case marker-suppressed phrases given information. The tonal pattern of the Accentual Phrase in Korean, LHLH, is not specific to morphological constituents within the phrase but is a property of the phrase (Jun 1993). Given that prosody often distinguishes pragmatic meanings, this study aims to find the phonetic characteristics of the suffix-suppressed Accentual Phrase through a phonetic experiment. The results indicate that the case marker-marked and -suppressed phrases are realized differently with respect to the degree of AP-final rising and AP-final tone realization. Different phonetic realizations reflect that the suffix-suppressed AP functions differently from the suffix-marked AP in discourse.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-80"
  },
  "bunnell04_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "James",
     "Polikoff"
    ],
    [
     "Jane",
     "McNicholas"
    ]
   ],
   "title": "Spectral moment vs. bark cepstral analysis of children's word-initial voiceles stops",
   "original": "i04_1313",
   "page_count": 4,
   "order": 82,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "Spectral moments analysis has been shown to be effective in deriving acoustic features for classifying voiceless stop release bursts [1], and is an analysis method that has commonly been cited in the clinical phonetics literature dealing with children's disordered speech. In this study, we compared the classification of stops /p/, /t/, and /k/ based on spectral moments with classification based on an equal number of Bark Cepstrum coefficients. Utterance-initial /p/, /t/, and /k/ (1338 samples in all) were collected from a database of children's speech. Linear discriminant analysis (LDA) was used to classify the three stops based on four analysis frames from the initial 40 msec of each token. The best classification based on spectral moments used all four spectral moment features (computed from bark-scaled spectra) and all four time intervals and yielded 75.6% correct classification. The best classification based on Bark cepstrum yielded 83.4% correct also using four coefficients and four time frames.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-81"
  },
  "minematsu04_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Pronunciation assessment based upon the compatibility between a learner's pronunciation structure and the target language's lexical structure",
   "original": "i04_1317",
   "page_count": 4,
   "order": 83,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "Native-sounding vs. intelligible. This has been a controversial issue for a long time in language learning and many teachers claim that the intelligible pronunciation should be the goal of pronunciation training. What is the physical definition of the intelligibility? The current work shows a very good candidate answer to this question. The author proposed a new paradigm of observing speech acoustics based upon phonology, where all the kinds of speech events are viewed as an entire structure and this structure was shown to be mathematically invariant with any static non-linguistic features such as age, gender, size, shape, microphone, room, line, and so on. This acoustic structure is purely linguistic and the phoneme-level structure is regarded as the pronunciation structure of individual students. This structure is matched with another linguistic structure, the lexical structure of the target language, and degree of compatibility between the two different levels of structures is calculated, which is defined as the intelligibility in this work. To increase the intelligibility, different instructions should be prepared for different students because no two students are the same. The proposed method can show the order of phonemes to learn, which is appropriate to a student and different from that of the others.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-82"
  },
  "yoshida04_interspeech": {
   "authors": [
    [
     "Kenji",
     "Yoshida"
    ]
   ],
   "title": "Spread of high tone in akita Japanese",
   "original": "i04_1321",
   "page_count": 4,
   "order": 84,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "The present study explores the phonetic implementation of accentual H(igh) tone of the Akita dialect of Japanese, specifically its timing control. Lexical accent in the dialect is implemented as an eminence in F0. The conspicuous feature is that F0 does not fall sharply as in Tokyo Japanese. Two hypotheses are examined concerning the extent of the duration of the pitch summit. One is that the H lasts for some fixed period of time. The other is that it lasts until some prosodic edge appears. An experimental study was performed for two native speakers of Akita, with a speaker of Tokyo dialect. The comparison elucidates the conspicuous characteristics of the phonetic implementation of the lexical accent differing from Tokyo Japanese. The result grossly supports the latter hypothesis. It suggests that the H tone in the dialect is phonetically implemented so that it lasts until the end of a Intonational Phrase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-83"
  },
  "godinollorente04_interspeech": {
   "authors": [
    [
     "Juan-Ignacio",
     "Godino-Llorente"
    ],
    [
     "Victoria",
     "Rodellar-Biarge"
    ],
    [
     "Pedro",
     "Gomez-Vilda"
    ],
    [
     "Francisco",
     "Diaz-Perez"
    ],
    [
     "Agustin",
     "Alvarez-Marquina"
    ],
    [
     "Rafael",
     "Martinez-Olalla"
    ]
   ],
   "title": "Biomechanical parameter fingerprint in the mucosal wave power spectral density",
   "original": "i04_0073",
   "page_count": 4,
   "order": 85,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "The importance of mucosal wave detection and estimation has been stressed in literature regarding the automatic classification and recognition of larynx pathologies from voice records. Using a new estimation method of the mucosal wave correlate and simulation results from a 2-mass model of the vocal folds the present paper shows that the main fingerprints found in the power spectral density of the mucosal wave correlate are directly related to the biomechanics of the system. These findings open the door to the non-invasive estimation of biomechanical parameters of the vocal folds directly from voice records, thus easing the task of automatic pathologic classification and recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-84"
  },
  "jo04_interspeech": {
   "authors": [
    [
     "Cheolwoo",
     "Jo"
    ],
    [
     "Soo-Geon",
     "Wang"
    ],
    [
     "Byung-Gon",
     "Yang"
    ],
    [
     "Hyung-Soon",
     "Kim"
    ],
    [
     "Tao",
     "Li"
    ]
   ],
   "title": "Classification of pathological voice including severely noisy cases",
   "original": "i04_0077",
   "page_count": 4,
   "order": 86,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In this paper we tried to classify pathological voices from normal ones based on two different parameters, spectral slope and ratio of energies in harmonic and noise components (HNR), and artificial neural network (ANN). Voice data from normal peoples and patients were collected, then classified into three different categories (normal, relatively less noisy and severely noisy pathological data). The spectral slope and HNR were computed and used to classify severely noisy pathological voice from others first because of its much noise. Then artificial neural network was used as a classifier to discriminate the rest of data into normal and relatively less noisy pathological categories when common numerical parameters were used as inputs. And the classification results were evaluated by comparing the distribution characteristics of the spectral slope and HNR for all of data and analyzing the classification rates for the normal and relative less noisy pathological voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-85"
  },
  "fu04_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fu"
    ],
    [
     "Peter",
     "Murphy"
    ]
   ],
   "title": "A robust glottal source model estimation technique",
   "original": "i04_0081",
   "page_count": 4,
   "order": 87,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "This paper describes a robust glottal source estimation method based on a joint source-filter separation technique. In this method, the glottal flow derivative is modelled as the Liljencrants-Fant (LF) model and the vocal tract is described as a time-varying ARX model. Since the joint estimation problem is a multi-parameter nonlinear optimization procedure, we separate the optimization procedure into two passes. The first pass initializes the glottal source and vocal tract models providing robust initial parameters to the following joint optimization procedure. The joint estimation determines the accuracy of model estimation, which is implemented with a trust-region descent optimization algorithm. Experiments with synthetic and real voices show the proposed method is a robust glottal source parameter estimation method with a considerable degree of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-86"
  },
  "mori04_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Yasunori",
     "Kobayashi"
    ],
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Hajime",
     "Hirose"
    ],
    [
     "Noriko",
     "Kobayashi"
    ]
   ],
   "title": "F0 and formant frequency distribution of dysarthric speech - a comparative study",
   "original": "i04_0085",
   "page_count": 4,
   "order": 88,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "We are investigating acoustical analysis for dysarthric speech, which appears as a symptom of neurologic disease, in order to elucidate its physiological and acoustical mechanism, and to develop aids for diagnosis and training, etc. In this report, acoustical characteristics of various kinds of dysarthrias are measured. As a result, shrinking of the F0 range as well as vowel space are observed in dysarthric speech. Also, from the comparison of F0 range and vowel formant frequencies it is suggested that speech effort to produce wider F0 range can influence vowel quality as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-87"
  },
  "kawahara04_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Yumi",
     "Hirachi"
    ],
    [
     "Morise",
     "Masanori"
    ],
    [
     "Hideki",
     "Banno"
    ]
   ],
   "title": "Procedure \"senza vibrato\": a key component for morphing singing",
   "original": "i04_0089",
   "page_count": 4,
   "order": 89,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "A procedure to remove vibrato from singing voice was proposed to enable auditory morphing between musical performances played under different conditions. Analyses of singing samples in the RWCP-music database using a speech analysis, modification and synthesis system STRAIGHT provided necessary information to implement \"senza vibrato,\" the procedure that removes vibrato. A preliminary subjective evaluation for artificially adding and removing vibrato indicated that the proposed procedure effectively control perceived vibrato while preserving naturalness of the original singing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-88"
  },
  "manfredi04_interspeech": {
   "authors": [
    [
     "Claudia",
     "Manfredi"
    ],
    [
     "Giorgio",
     "Peretti"
    ],
    [
     "Laura",
     "Magnoni"
    ],
    [
     "Fabrizio",
     "Dori"
    ],
    [
     "Ernesto",
     "Iadanza"
    ]
   ],
   "title": "Thyroplastic medialisation in unilateral vocal fold paralysis: assessing voice quality recovering",
   "original": "i04_0093",
   "page_count": 4,
   "order": 90,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Medialization thyroplasty and endoscopic intracordal infusion of fat or heterologous materials are the treatments of choice for glottic incompetence, of both neurological and cicatricial origin. Functional evaluation after thyroplastic medialisation often based on several approaches, in order to assess effectiveness of the adopted technique. The most common analysis methods are: videolaryngostroboscopy (VLS), morphological aspects evaluation, GRBAS scale and (Voice Handicap Index), relative to perceptive and subjective voice analysis, and MDVP(R), that provides objective acoustic parameters. First results are presented here, obtained both with approaches and a new voice analysis tool, based on robust estimators for tracking fundamental frequency F0, noise formants. New indexes are also proposed, to easily quantify voice quality recovering. The proposed approach successfully applied to patients that underwent thyroplastic medialisation, and is suited for integrating the MDVP features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-89"
  },
  "kubin04_interspeech": {
   "authors": [
    [
     "Gernot",
     "Kubin"
    ],
    [
     "Martin",
     "Hagmueller"
    ]
   ],
   "title": "Voice enhancement of male speakers with laryngeal neoplasm",
   "original": "i04_0541",
   "page_count": 4,
   "order": 91,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "In this paper an approach is presented which is aimed to enhance disordered male voices. This approach aims at high-pitched voices with a severe degree of hoarseness. The goal of the study is to determine whether pitch modification combined with periodicity enhancement can improve the perceived quality of a disordered speech utterance. Signal manipulation is done pitch-synchronously, so, firstly pitch marks have to be detected. Then period enhancement is performed followed by a PSOLA based pitch modification step. Finally, the period enhancement is performed once more for the utterance with the lower pitch. Perceptual evaluations were performed by both professional speech and language pathologists and naive listeners to rate the subjective perceived enhancement of the voice. Results show that the modified voice has a reduced breathiness, whereas roughness seems not to be influenced by the processing. The most significant result from the naive listener test is a reduced perceived speaking effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-90"
  },
  "choi04_interspeech": {
   "authors": [
    [
     "Jong Min",
     "Choi"
    ],
    [
     "Myung-Whun",
     "Sung"
    ],
    [
     "Kwang Suk",
     "Park"
    ],
    [
     "Jeong-Hun",
     "Hah"
    ]
   ],
   "title": "A comparison of the perturbation analysis between PRAAT and computerize speech lab",
   "original": "i04_0545",
   "page_count": 4,
   "order": 92,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "Programs for the analysis of pathological voice data have been presented for the last few decades. Computerized Speech Lab (CSL) has been recognized as a standard in the field of pathological voice analysis. PRAAT is a new open program which is constantly being improved almost every week. CSL gives the \"Voice disorders database\" which was produced by Massachusetts Eye and Ear Infirmary. In this paper, we use PRAAT program for the analysis of voice data and compare the result of PRAAT with the result of CSL. We focus on the perturbation analysis on frequencies using several parameters like jitter, standard deviation of fundamental frequencies etc.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-91"
  },
  "ji04_interspeech": {
   "authors": [
    [
     "Ming",
     "Ji"
    ],
    [
     "Baochun",
     "Hou"
    ]
   ],
   "title": "Evaluation of universal compensation on Aurora 2 and 3 and beyond",
   "original": "i04_0097",
   "page_count": 4,
   "order": 93,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "A new method, namely Universal Compensation (UC), is introduced for speech recognition involving additive noise assuming no knowledge about the noise. The UC method involves a novel combination of the principle of multi-condition training and the principle of the missing-feature method. This combination makes the new method potentially capable of dealing with any additive noise - with arbitrary temporal-spectral characteristics - based only on clean speech training data and simulated noise data, without requiring knowledge about the noise. This paper describes the evaluation of the new method on Aurora 2 and 3 and further, on noise conditions unseen in the Aurora tasks. The results show that the new model assuming no knowledge of noise has performed equally well as the baseline models trained for the specific tasks. The new model has outperformed the baseline when there exists a mismatch between the training and testing conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-92"
  },
  "vanhamme04_interspeech": {
   "authors": [
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "PROSPECT features and their application to missing data techniques for robust speech recognition",
   "original": "i04_0101",
   "page_count": 4,
   "order": 94,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "Missing data theory has been applied to the problem of speech recognition in adverse environments. The resulting systems require acoustic models that are expressed in the spectral rather than in the cepstral domain, which leads to loss of accuracy. Cepstral Missing Data Techniques (CMDT) surmount this disadvantage, but require significantly more computation. In this paper, we study alternatives to the cepstral representation that lead to more efficient MDT systems. The proposed solution, PROSPECT features (Projected Spectra), can be interpreted as a novel speech representation, or as an approximation of the inverse covariance (precision) matrix of the Gaussian distributions modeling the log-spectra.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-93"
  },
  "vanhamme04b_interspeech": {
   "authors": [
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Patrick",
     "Wambacq"
    ],
    [
     "Veronique",
     "Stouten"
    ]
   ],
   "title": "Accounting for the uncertainty of speech estimates in the context of model-based feature enhancement",
   "original": "i04_0105",
   "page_count": 4,
   "order": 95,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "In this paper we present two techniques to cover the gap between the true and the estimated clean speech features in the context of Model-Based Feature Enhancement (MBFE) for noise robust speech recognition. While in the output of every feature enhancement algorithm some residual uncertainty remains, currently this information is mostly discarded. Firstly, we explain how the generation of not only a global MMSE-estimate of clean speech, but also several alternative (state-conditional) estimates are supplied to the back-end for recognition. Secondly, we explore the benefits of calculating the variance of the front-end estimate and incorporating this in the acoustic models of the recogniser. Experiments on the Aurora2 task confirmed the superior performance of the resulting system: an average increase in recognition accuracy from 85.65% to 88.50% was obtained for the clean training condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-94"
  },
  "hirsch04_interspeech": {
   "authors": [
    [
     "Hans-Guenter",
     "Hirsch"
    ],
    [
     "Harald",
     "Finster"
    ]
   ],
   "title": "Applying the Aurora feature extraction schemes to a phoneme based recognition task",
   "original": "i04_0109",
   "page_count": 4,
   "order": 96,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "The robustness of the ETSI-Aurora standardized feature extraction schemes is investigated for phoneme based recognition tasks of German speech data. The recognition tasks are an isolated command word recognition and the recognition of connected digits. The motivation of this work is the easy extensibility of a whole word recognition system by allowing also the recognition of phoneme based word HMMs. The recognition performance has been determined for different numbers of HMM states and different numbers of Gaussians per state. It turns out that fairly high recognition rates can be achieved also for noisy data when applying the second robust ETSI frontend.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-95"
  },
  "zhang04b_interspeech": {
   "authors": [
    [
     "Zhipeng",
     "Zhang"
    ],
    [
     "Tomoyuki",
     "Ohya"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Evaluation of tree-structured piecewise linear transformation-based noise adaptation on AURORA2 database",
   "original": "i04_0113",
   "page_count": 4,
   "order": 97,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This paper uses the AURORA2 task to investigate the performance of our proposed tree-structured piecewise linear transformation (PLT) noise adaptation. In our proposed method, an HMM that best matches the input speech is selected based on the likelihood maximization criterion by tracing a tree structured HMM space that is prepared in the training step, and the selected HMM is further adapted by linear transformation. Experimental results show that our method achieves a significant improvement for the AURORA2 database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-96"
  },
  "myrvoll04_interspeech": {
   "authors": [
    [
     "Tor Andre",
     "Myrvoll"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Online minimum mean square error filtering of noisy cepstral coefficients using a sequential EM algorithm",
   "original": "i04_0117",
   "page_count": 4,
   "order": 98,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "In this work we propose an online filtering algorithm that aims to alleviate the decrease we see in ASR performance when the speech is corrupted by additive noise. Using an initial estimate of the noise distribution, the algorithm updates the noise model on a frame synchronous basis. The minimum mean square error (MMSE) filtering is also performed at a frame per frame basis, using the most current noise model estimate at all times. The algorithm is compared to a batch version which uses several iterations of the EM-algorithm over the complete utterance to estimate the noise model, and it is demonstrated that the performance is as good or better at a fraction of the computational complexity when the noise is non-stationary.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-97"
  },
  "sasou04_interspeech": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Futoshi",
     "Asano"
    ]
   ],
   "title": "HMM-based feature compensation method: an evaluation using the AURORA2",
   "original": "i04_0121",
   "page_count": 4,
   "order": 99,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "In this paper, we describe an HMM-based feature-compensation method. The proposed method compensates for noise-corrupted features in the MFCC domain using the output probability density functions (pdf) of the Hidden Markov Models (HMM). In compensating the features, the output pdfs are adaptively weighted according to forward path probabilities. Because of this, the proposed method can minimize degradation of feature-compensation accuracy due to temporally changing noise environment. We evaluated the proposed method based on the AURORA2 database. All the experiments were conducted in a clean condition. The experiment results indicate that the proposed method, combined with cepstral mean subtraction, can achieve a word accuracy of 87.64%. We also show that the proposed method is useful in a transient pulse noise environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-98"
  },
  "wang04d_interspeech": {
   "authors": [
    [
     "Xuechuan",
     "Wang"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Noise adaptation for robust AURORA 2 noisy digit recognition using statistical data mapping",
   "original": "i04_0125",
   "page_count": 4,
   "order": 100,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "The mismatch between the system training and operating conditions often has negative influences on the automatic speech recognition (ASR) systems. Noise in the operating environments is commonly encountered. ASR model adaptation is an important way to enhance the system performance in noisy environments. This paper proposes a feature-based statistical data mapping (SDM) approach for robust noisy digit recognition. The recognition tasks are carried out on the AURORA 2 database. Compared to other model adaptation methods such as MLLR, the SDM approach has more robust performances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-99"
  },
  "shannon04_interspeech": {
   "authors": [
    [
     "Benjamin J.",
     "Shannon"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "MFCC computation from magnitude spectrum of higher lag autocorrelation coefficients for robust speech recognition",
   "original": "i04_0129",
   "page_count": 4,
   "order": 101,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Processing of the speech signal in the autocorrelation domain in the context of robust feature extraction is based on the following two properties: 1) pole preserving property (the poles of a given (original) signal are preserved in its autocorrelation function), and 2) noise separation property (the autocorrelation function of a noise signal is confined to lower lags, while the speech signal contribution is spread over all the lags in the autocorrelation function, thus providing a way to eliminate noise by discarding lower-lag autocorrelation coefficients). In this paper, we use these properties to derive robust features for automatic speech recognition. We compute the magnitude spectrum of the one-sided higher-lag autocorrelation sequence, process it through a Mel filter bank and parameterise it in terms of Mel Frequency Cepstral Coefficients (MFCCs). Since the proposed method combines autocorrelation domain processing with Mel filter bank analysis, we call the resulting MFCCs, Autocorrelation Mel Frequency Cepstral Coefficients (AMFCCs). Recognition experiments are conducted on the Aurora II database and it is found that the AMFCC representation performs as well as the MFCC representation in clean conditions and provides more robust performance in the presence of background noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-100"
  },
  "muhammad04_interspeech": {
   "authors": [
    [
     "Ghulam",
     "Muhammad"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Junsei",
     "Horikawa"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "A noise-robust feature extraction method based on pitch-synchronous ZCPA for ASR",
   "original": "i04_0133",
   "page_count": 4,
   "order": 102,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "In this paper, we propose a novel feature extraction method based on an auditory nervous system for robust automatic speech recognition (ASR). In the proposed method, a pitch-synchronous mechanism is embedded in ZCPA (Zero-Crossings Peak-Amplitudes), which has previously been shown to outperform the conventional features in the presence of noise. A noise-robust non-delayed pitch determination algorithm (PDA) is also developed. In the experiment, the proposed pitch-synchronous ZCPA (PS-ZCPA)was proved more robust than the original ZCPA method. Moreover, a simple noise subtraction (NS) method is also integrated in the proposed method and the performance was evaluated using the Aurora-2J database. The experimental results showed the superiority of the proposed PSZCPA method with NS over the PS-ZCPA method without NS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-101"
  },
  "segura04_interspeech": {
   "authors": [
    [
     "José Carlos",
     "Segura"
    ],
    [
     "Angel De la",
     "Torre"
    ],
    [
     "Javier",
     "Ramirez"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Carmen",
     "Benitez"
    ]
   ],
   "title": "Including uncertainty of speech observations in robust speech recognition",
   "original": "i04_0137",
   "page_count": 4,
   "order": 103,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "Noise compensation methods for speech recognition provide a cleaned version of the speech representation. Usually this version is the expected value of the speech parameters given the observed noisy speech and the noise statistic. A more realistic representation should include a probability distribution of the cleaned speech instead of its expected value in order to represent the uncertainty associated to the compensation process due to the variability of the noise process. Recently, the inclusion of the uncertainty in the recognition process has been studied. In this paper we have developed a noise compensation technique that incorporates the variance of the cleaned speech into the speech representation. The variance is estimated using a Wiener filter during the speech feature enhancement process. This way of including the uncertainty implies the modification of the decoding rule. Experimental results demonstrate a improvement of the performance in the recognition system (about 21% WER reduction) when uncertainty is included.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-102"
  },
  "yamada04_interspeech": {
   "authors": [
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Jiro",
     "Okada"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ]
   ],
   "title": "Integration of n-best recognition results obtained by multiple noise reduction algorithms",
   "original": "i04_0141",
   "page_count": 4,
   "order": 104,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "During the last decade, a number of noise reduction algorithms were proposed for realizing noise robust speech recognition. However, their effectiveness strongly depends on noise conditions. One way for solving this problem is to select an optimal algorithm every time before or after recognition process. This paper proposes a new method for integrating N-best recognition results obtained by multiple noise reduction algorithms. The proposed method selects the best recognition result by using a confidence measure based on a frame-normalized log likelihood score. To evaluate the performance of the proposed method, recognition experiments were performed on the AURORA-2J connected digit recognition task. These results confirmed that the proposed method is very effective in the high and middle SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-103"
  },
  "setiawan04_interspeech": {
   "authors": [
    [
     "Panji",
     "Setiawan"
    ],
    [
     "Sorel",
     "Stan"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "Revisiting some model-based and data-driven denoising algorithms in Aurora 2 context",
   "original": "i04_0145",
   "page_count": 4,
   "order": 105,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "In this paper we evaluate some model-based and data-driven algorithms for robust speech recognition in noise, using the experimental framework provided by ETSI Aurora 2. Specifically, we focus on statistical linear approximation (SLA), sequential interacting multiple models (S-IMM), and histogram normalization (HN). As the baseline for the feature extraction scheme we use the ETSI front-end. Recognition tests on a subset of Aurora 2 show that SLA is approximately 4% better than HN and that S-IMM is worse than HN by almost 3% in terms of absolute word accuracy. A comparison with the ETSI advanced front-end (AFE) is also presented. While none of these algorithms outperforms AFE, we identify the reasons why this might have happened and point out potential directions for improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-104"
  },
  "ding04_interspeech": {
   "authors": [
    [
     "Guo-Hong",
     "Ding"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Exploring high-performance speech recognition in noisy environments using high-order taylor series expansion",
   "original": "i04_0149",
   "page_count": 4,
   "order": 106,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "In this paper, high-order Taylor Series expansion is proposed to explore the most effective formulas of log-spectral compensation. The power feature, which is crucial to speech recognition in noisy environments and can't be compensated in usual feature compensation, is processed similarly to spectral subtraction. The modeling accuracy of speech log-spectral Gaussian Mixture Model (GMM) is also discussed and carefully treated. Experimental results show that the log-spectral compensation can greatly improve recognition performance in noisy environments and with the acoustic model trained using multi-condition data, the recognition performance is superior to that in matched conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-105"
  },
  "au04_interspeech": {
   "authors": [
    [
     "Wing-Hei",
     "Au"
    ],
    [
     "Man-Hung",
     "Siu"
    ]
   ],
   "title": "A robust training algorithm based on neighborhood information",
   "original": "i04_0153",
   "page_count": 4,
   "order": 107,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Robustness is an important issue in automatic speech recognition systems. When the testing conditions do not match the training condition or when there is insufficient training data, the performance of a system trained by maximum likelihood criterion may degrade significantly. Different robust algorithms were proposed especially for cases in which the mismatch condition is known or can be estimated from the test data. In many practical cases, however, the mismatch information may not be available. In this paper, we propose a robust training algorithm that does not make any assumption about the mismatch condition. Instead, it is based on a neighborhood concept that appropriately broaden the model distributions to increase the model robustness. The proposed algorithm is evaluated in the Aurora3 tasks which shows that the neighborhood approach can reduce the degradation in mismatch conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-106"
  },
  "lee04e_interspeech": {
   "authors": [
    [
     "Siu Wa",
     "Lee"
    ],
    [
     "Pak Chung",
     "Ching"
    ]
   ],
   "title": "In-phase feature induction: an effective compensation technique for robust speech recognition",
   "original": "i04_0157",
   "page_count": 4,
   "order": 108,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "The performance of most standard Automatic Speech Recognition (ASR) systems degrades severely under noisy environments, because they are usually trained with clean speech data. There exist several speech enhancement or compensation schemes that can improve the robustness of ASR to different levels. In this paper, an effective feature compensation method called In-phase Feature Induction (IFI) is proposed. It makes use of the phase relationship between the spectra of noisy input and the corresponding noise signal to accurately obtain the clean speech spectrum. Experimental results show that recognition systems with IFI compensation yield much higher accuracy rates under various noisy conditions compared with the Spectral Subtraction based method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-107"
  },
  "yeung04_interspeech": {
   "authors": [
    [
     "Siu-Kei Au",
     "Yeung"
    ],
    [
     "Man-Hung",
     "Siu"
    ]
   ],
   "title": "Improved performance of Aurora 4 using HTK and unsupervised MLLR adaptation",
   "original": "i04_0161",
   "page_count": 4,
   "order": 109,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "The introduction of Aurora 4 tasks provides a standard database and methodology for comparing the effectiveness of different robust algorithms on LVCSR. One important issue on Aurora 4 tasks is the computation time involved in evaluating different test conditions. In this paper we show that by employing HTK as the recognition frontend and backend on Aurora 4 tasks with the use of cepstral mean subtraction, 14% relative improvement is achieved on the baseline clean train tasks at a 82.5% time reduction in training time and 40% time reduction on decoding. Furthermore, we found that optimizing the model complexity can increase the recognition performance (in both computation time and accuracy). Accuracy can be further improved with the use of unsupervised MLLR adaptation on one or multiple sentences. The adaptation results show that most of the gain from adaptation comes from adapting to the environment instead of to the speaker. With the use of adaptation,the error rate is reduced from the baseline result of 69.6% to 40%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-108"
  },
  "tsai04b_interspeech": {
   "authors": [
    [
     "Shang-nien",
     "Tsai"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "A new feature extraction front-end for robust speech recognition using progressive histogram equalization and multi-eigenvector temporal filtering",
   "original": "i04_0165",
   "page_count": 4,
   "order": 110,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "In this paper, a new feature extraction front-end for robust speech recognition using progressive histogram equalization and multi-eigenvector temporal filtering is proposed. The progressive histogram equalization (PHEQ) performs the histogram equalization (HEQ) progressively with respect to a reference interval which moves with the present frame to be processed. The multi-eigenvector temporal filtering (m-eigen) uses the linear combination of m eigenvectors corresponding to the largest eigenvalues in the PCA-based temporal filtering approach. The very useful handling of two-stage Wiener filtering (2WF) and SNR-dependent waveform processing (SWP) are first applied to remove the noise and enhance the overall SNR. MFCC parameters are then extracted, followed by the progressive histogram equalization. The multi-eigenvector temporal filtering is finally performed to produce robust feature extraction for speech recognition. Extensive experiments with respect to AURORA2 database and testing conditions verified the effectiveness of each component here and showed that the proposed front-end gives better overall performance when compared to the Advanced Front-End recently announced by ETSI, especially under channel-mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-109"
  },
  "fugen04_interspeech": {
   "authors": [
    [
     "Christian",
     "Fügen"
    ],
    [
     "Hartwig",
     "Holzapfel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Tight coupling of speech recognition and dialog management - dialog-context dependent grammar weighting for speech recognition",
   "original": "i04_0169",
   "page_count": 4,
   "order": 111,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "In this paper we present our current work on a tight coupling of a speech recognizer with a dialog manager and our results by restricting the search space of our grammar based speech recognizer through the information given by the dialog manager. As a result of the tight coupling the same lingustic knowledge sources can be used in both, speech recognizer and dialog manager. Furthermore, the flexible context-free grammar implementation of our speech decoder Ibis allows weighting of specific rules at run-time to restrict the search space of the recognizer for the next decoding step. These rules are given by the dialog manager depending on the current dialog context. With this approach we were able to reduce the word error rate of user responses to system questions by 3.3% relative for close talking and 16.0% relative, when using distant speech input. The sentence error rates were reduced by 2.2%, 9.2% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-110"
  },
  "lee04f_interspeech": {
   "authors": [
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keisuke",
     "Nakamura"
    ],
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Noise robust real world spoken dialogue system using GMM based rejection of unintended inputs",
   "original": "i04_0173",
   "page_count": 4,
   "order": 112,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "To realize a robust spoken dialogue system for use in a real environment, the robust rejection of unintended inputs such as laughter, coughing, background speech and other noise based on GMM is implemented and examined on the basis of actual utterances. All the triggered inputs to a speech-oriented guidance system from 125 days of field tests in a public space are collected, and the occurrence of unintended inputs is investigated. GMM classifiers for voice categories (adult speech and child speech) and non-voice categories (laughter, coughing and other noises) are trained on the basis of the analysis result. The rejection performance of unintended speech was experimented on actual uncontrolled real inputs, and an EER of 3.32% was achieved by the 5-class GMM, which outperforms simple 2-class (voice / non-voice) GMM. The rejection of background speech using GMM is also investigated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-111"
  },
  "oshikawa04_interspeech": {
   "authors": [
    [
     "Hironori",
     "Oshikawa"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Speech interface for name input based on combination of recognition methods using syllable-based n-gram and word dictionary",
   "original": "i04_0177",
   "page_count": 4,
   "order": 113,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "We propose an interface for name input based on speech recognition using syllable-based N-gram and word dictionary. Name utterance is hard to recognize accurately because of the large vocabulary size, so the system uses continuous syllable recognition with syllable-based N-gram and isolated word recognition with a dictionary containing frequent words. User first utters a name and then chooses the correct word/syllables by pen touch from word/syllable candidates which were obtained from speech recognition. System displays word candidates, syllable sequence candidates and a syllable lattice on a touch panel and user can select a desired word from the candidates. We evaluated this interface. User could find the correct answer from word candidates or syllable sequence candidates at a rate of 82-86%, and could input correct name at a rate of 94-96% using syllable selection from the syllable lattice. Some subjects used this interface and felt that it was efficient and useful.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-112"
  },
  "zitouni04_interspeech": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Hui",
     "Jiang"
    ]
   ],
   "title": "Constrained minimization technique for topic identification using discriminative training and support vector machines",
   "original": "i04_0181",
   "page_count": 4,
   "order": 114,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "This paper describes the constrained minimization approach to combine multiple classifiers in order to improve classification accuracy. Since errors of individual classifiers in the ensemble should somehow be uncorrelated to yield higher classification accuracy, we propose a combination strategy where the combined classifier accuracy is a function of the correlation between classification errors of the individual classifiers. To obtain powerful single classifiers, different techniques are investigated including support vector machines and latent semantic indexing (LSI) matrix, which is a popular vector-space model. We also investigate discriminative training (DT) of the LSI matrix on constrained minimization approach. DT minimizes the classification error by increasing the score separation of the correct from competing documents. Experimental evaluation is carried out on a banking call routing and on switchboard databases with a set of 23 and 67 topics respectively. Results show that the combined classifier we propose outperforms the accuracy of individual baseline classifiers by 44%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-113"
  },
  "williams04_interspeech": {
   "authors": [
    [
     "Jason D.",
     "Williams"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Characterizing task-oriented dialog using a simulated ASR chanel",
   "original": "i04_0185",
   "page_count": 4,
   "order": 115,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "We describe a data collection consisting of task-oriented human-human conversations in a simulated ASR channel in which the WER is systematically varied. We find that users infrequently give a direct indication of having been misunderstood; levels of expert \"initiative\" increase with WER primarily due to increased grounding activity; and asking task-related questions appears to be a more successful repair strategy at moderate WER levels. A PARADISE analysis finds task completion most predictive of user satisfaction; efficiency is also important at lower WERs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-114"
  },
  "konashi04_interspeech": {
   "authors": [
    [
     "Takashi",
     "Konashi"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "A spoken dialog system based on automatic grammar generation and template-based weighting for autonomous mobile robots",
   "original": "i04_0189",
   "page_count": 4,
   "order": 116,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "We have been developing a spoken dialog system. Conventional spoken dialog systems need grammar descriptions and scripts of a dialog, that are difficult to develop. The system proposed in this paper is based on semantic frames, and the system generates the recognition grammar from the frames automatically. As the system requires only a frame-based description for a task of dialog, the system can be easily applied to different kinds of tasks. Moreover, the recognition accuracy is improved by sentence weighting based on phrase class template. We evaluated the system by experiments. The system reached the goal with 2.44 user's utterances in average.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-115"
  },
  "ito04_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Takanobu",
     "Oba"
    ],
    [
     "Takashi",
     "Konashi"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Noise adaptive spoken dialog system based on selection of multiple dialog strategies",
   "original": "i04_0193",
   "page_count": 4,
   "order": 117,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "Speech recognition under noisy environment is one of the hottest topic in the speech recognition research. In this paper, we propose a method to improve accuracy of spoken dialog system from a dialog strategy point of view. In the proposed method, the dialog system automatically changes its dialog strategy according to the estimated recognition accuracy in noisy environment in order to keep the performance of the system constant. In a noisy environment, the system restricts its grammar and vocabulary to improve recognition accuracy. On the other hand, the system accepts any utterance from a user in a noise-free environment. To realize this strategy, we investigated a method to avoid user's out-of-grammar utterances through an instruction given by the system to a user. Furthermore, we developed a method to estimate recognition accuracy from features extracted from noise signal. Finally, we constructed a proposed dialog system and confirmed its effectiveness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-116"
  },
  "hartikainen04_interspeech": {
   "authors": [
    [
     "Mikko",
     "Hartikainen"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Esa-Pekka",
     "Salonen"
    ],
    [
     "J. Adam",
     "Funk"
    ]
   ],
   "title": "Flexible dialogue management using distributed and dynamic dialogue control",
   "original": "i04_0197",
   "page_count": 4,
   "order": 118,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "In spoken dialogue applications dialogue management has conventionally been realized with a single monolithic dialogue manager implementing a comprehensive dialogue control model. We present a highly distributed system structure that enables the integration of different dialogue control approaches to handle spoken dialogues. With this structure it is possible to integrate multiple dialogue control models into a single working application in a flexible fashion. The main principle is that all the processing is distributed into many compact agents that focus on single tasks. The agents process the information independently and share all the information via shared information storage. We present the principles that enable such distribution. In addition, a multilingual example application, AthosMail, is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-117"
  },
  "houck04_interspeech": {
   "authors": [
    [
     "Keith",
     "Houck"
    ]
   ],
   "title": "Contextual revision in information seeking conversation systems",
   "original": "i04_0201",
   "page_count": 4,
   "order": 119,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "Contextual revision allows users to make incremental revisions to their previous query rather than providing a complete query each time. This is a powerful capability of conversation systems that can greatly reduce the word length of users' input, thereby improving speech recognition accuracy and simplifying NLU. We describe a novel method combining linguistic cues, semantic constraints, and data characteristics to address the ambiguities introduced by this construct.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-118"
  },
  "oneill04_interspeech": {
   "authors": [
    [
     "Ian",
     "O'Neill"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "Xingkun",
     "Liu"
    ],
    [
     "Michael",
     "McTear"
    ]
   ],
   "title": "Cross domain dialogue modelling: an object-based approach",
   "original": "i04_0205",
   "page_count": 4,
   "order": 120,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "Advanced spoken dialogue systems incorporate functionalities such as mixed-initiative and cross-domain dialogues. In this paper an object-based approach to cross domain dialogue modelling is described in which service agents representing primary transaction types and support agents representing tasks such as eliciting payment details are selected as appropriate by a domain spotter. The domain spotter also deals with user-led focus shifts and selects the agent that is best placed to respond to the user's utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-119"
  },
  "sagawa04_interspeech": {
   "authors": [
    [
     "Hirohiko",
     "Sagawa"
    ],
    [
     "Teruko",
     "Mitamura"
    ],
    [
     "Eric",
     "Nyberg"
    ]
   ],
   "title": "A comparison of confirmation styles for error handling in a speech dialog system",
   "original": "i04_0209",
   "page_count": 4,
   "order": 121,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "Speech recognition errors are inevitable in a speech dialog system. It is important to provide a dialog flow that allows the user to correct system errors and quickly return to the original dialog. This paper describes explicit, final and implicit confirmation styles that are implemented in the CAMMIA speech dialog system, and compares them from the viewpoint of usability. Our results show that a final confirmation with fewer confirmation turns is preferred by the user when there is no error in the dialog. On the other hand, an explicit confirmation is preferred when an error occurs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-120"
  },
  "yang04_interspeech": {
   "authors": [
    [
     "Fan",
     "Yang"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "Using computer simulation to compare two models of mixed-initiative",
   "original": "i04_0213",
   "page_count": 4,
   "order": 122,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "In this paper, we use computer simulation to better understand mixed-initiative dialogues. We compare two models of mixed-initiative: unrestricted initiative, where either participant can take over control at any point; and restricted initiative where one participant keeps control and the other plays a secondary role, but greater than what single-initiative allows. We find that restricted initiative results in similar solution quality as unrestricted, less communication effort, and similar or less reasoning effort. These results agree with our empirical studies on human-human dialogues, in which we find that participants seem to follow the restricted initiative model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-121"
  },
  "yang04b_interspeech": {
   "authors": [
    [
     "Fan",
     "Yang"
    ],
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Kristy",
     "Hollingshead"
    ]
   ],
   "title": "Towards understanding mixed-initiative in task-oriented dialogues",
   "original": "i04_0217",
   "page_count": 4,
   "order": 123,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "Mixed-initiative in dialogue does not mean that either speaker is free to take the lead in the conversation at any time. Rather, who can show initiative, and when, is restricted. In this paper, we define initiative and control as two levels of dialogue phenomena, and give further evidence for our theory of restricted initiative that control belongs to the initiator of a discourse segment. We show that the initiator and the non-initiator play different roles in terms of showing initiative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-122"
  },
  "wolf04_interspeech": {
   "authors": [
    [
     "Peter",
     "Wolf"
    ],
    [
     "Joseph",
     "Woelfel"
    ],
    [
     "Jan Van",
     "Gemert"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "David",
     "Wong"
    ]
   ],
   "title": "Spokenquery: an alternate approach to chosing items with speech",
   "original": "i04_0221",
   "page_count": 4,
   "order": 124,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "A majority of spoken user interfaces deal with the task of retrieving an element from a list. Conventionally, spoken UIs deal with such tasks through hierarchies of menus or dialogs, that navigate users through a series of steps, each of which present them with a limited set of choices. In a recent paper [3] we presented an alternative approach to such UIs, termed SpokenQuery, that recasts the problem of selection from lists as one of retrieval, and demonstrated that it could result in significantly lowered cognitive load on the user. In this paper, we examine various aspects of retrieval from spoken queries, and UIs based on such retrieval, and demonstrate that that in addition to reducing the cognitive load on the user, the system is effective for searching large databases, is robust to environment noise, and is effective as a UI.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-123"
  },
  "douglas04_interspeech": {
   "authors": [
    [
     "Shona",
     "Douglas"
    ],
    [
     "Deepak",
     "Agarwal"
    ],
    [
     "Tirso",
     "Alonso"
    ],
    [
     "Robert",
     "Bell"
    ],
    [
     "Mazin",
     "Rahim"
    ],
    [
     "Deborah F.",
     "Swayne"
    ],
    [
     "Chris",
     "Volinsky"
    ]
   ],
   "title": "Mining customer care dialogs for \"daily news\"",
   "original": "i04_0225",
   "page_count": 4,
   "order": 125,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "As real deployments of spoken dialog systems become more common, a wealth of information about their operation becomes available from their system logs. This paper describes the \"VoiceTone Daily News\" data mining tool for analyzing this information and presenting it in a readily comprehensible form suitable for use by either system designers or call center businesses. Relevant features are extracted from the logs of caller-system interactions and tracked by a trend analysis algorithm. Features that move outside their expected bounds on a given day generate headlines as part of a web site generated completely automatically from each day's logs. A \"drilldown\" facility allows headlines to be investigated all the way to viewing logs of individual interactions behind the headline and listening to the audio for individual turns. Some initial experiments with automated measures of dialog success are described as possible additional features to track.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-124"
  },
  "edlund04_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Gabriel",
     "Skantze"
    ],
    [
     "Rolf",
     "Carlson"
    ]
   ],
   "title": "Higgins - a spoken dialogue system for investigating error handling techniques",
   "original": "i04_0229",
   "page_count": 4,
   "order": 126,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "In this paper, an overview of the Higgins project and the research within the project is presented. The project incorporates studies of error handling for spoken dialogue systems on several levels, from processing to dialogue level. A domain in which a range of different error types can be studied has been chosen: pedestrian navigation and guiding. Several data collections within Higgins have been analysed along with data from Higgins' predecessor, the AdApt system. The error handling research issues in the project are presented in light of these analyses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-125"
  },
  "weng04_interspeech": {
   "authors": [
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Lawrence",
     "Cavedon"
    ],
    [
     "Badri",
     "Raghunathan"
    ],
    [
     "Danilo",
     "Mirkovic"
    ],
    [
     "Hua",
     "Cheng"
    ],
    [
     "Hauke",
     "Schmidt"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Rohit",
     "Mishra"
    ],
    [
     "Stanley",
     "Peters"
    ],
    [
     "Sandra",
     "Upson"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Carsten",
     "Bergmann"
    ],
    [
     "Lin",
     "Zhao"
    ]
   ],
   "title": "A conversational dialogue system for cognitively overloaded users",
   "original": "i04_0233",
   "page_count": 4,
   "order": 127,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "Spoken dialogue interfaces are gaining increased acceptance in a wide range of applications. Most current examples of such systems, however, rely on using restricted language and scripted dialogue interactions. We argue that speech interfaces in highly stressed or cognitively overloaded domains, i.e. those involving a user concentrating on other tasks, call for more flexible dialogue with robust, wide-coverage language understanding. We describe an initial effort at addressing flexible and rich dialogue in a system with a number of features, such as full spoken language understanding, a multi-threaded dialogue manager, dynamic update of information, and recognition of partial proper names.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-126"
  },
  "hanrieder04_interspeech": {
   "authors": [
    [
     "Gerhard",
     "Hanrieder"
    ],
    [
     "Stefan W.",
     "Hamerich"
    ]
   ],
   "title": "Modeling generic dialog applications for embedded systems",
   "original": "i04_0237",
   "page_count": 4,
   "order": 128,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "This paper presents an industrial point of view regarding dialog description languages for dialog applications on embedded devices. We present GDML, a dialog description language developed for embedding dialogs in speech control systems in cars. Furthermore we present the environment used for developing speech dialog applications with GDML. As GDML is already used in existent products for years, we have collected a lot of experiences with it. This enables us, since we also worked with VoiceXML finally to compare both approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-127"
  },
  "stuttle04_interspeech": {
   "authors": [
    [
     "Matthew N.",
     "Stuttle"
    ],
    [
     "Jason D.",
     "Williams"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "A framework for dialogue data collection with a simulated ASR channel",
   "original": "i04_0241",
   "page_count": 4,
   "order": 129,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "The application of machine learning methods to the dialogue management component of spoken dialogue systems is a growing research area. Whereas traditional methods use hand-crafted rules to specify a dialogue policy, machine learning techniques seek to learn dialogue behaviours from a corpus of training data. In this paper, we identify the properties of a corpus suitable for training machine- learning techniques, and propose a framework for collecting dialogue data. The approach is akin to a \"Wizard of Oz\" set-up with a \"wizard\" and a \"user\", but introduces several novel variations to simulate the ASR communication-channel. Specifically, a turn-taking model common in spoken dialogue system is used, and rather than hearing the user directly, the wizard sees simulated speech recognition results on a screen. The simulated recognition results are produced with an error-generation algorithm which allows the target WER to be adjusted. An evaluation of the algorithm is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-128"
  },
  "pan04_interspeech": {
   "authors": [
    [
     "Shimei",
     "Pan"
    ]
   ],
   "title": "A multi-layer conversation management approach for information seeking applications",
   "original": "i04_0245",
   "page_count": 4,
   "order": 130,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "The paper describes a new conversation management approach which may apply to a class of mixed initiative information seeking applications. It employs an application-independent conversation model inspired by the conversation theory of Grosz and Sidner [1]. Based on the model, we design a multi-layer conversation manager which employs instance-based learning (IBL) to determine conversation plans, while performs modularized realization of each conversation move in a conversation plan. Because we adopt IBL to determine conversation plans, new conversation behaviors are easy to incorporate. Our realization of conversation moves is also application-independent. We illustrate this method with a real estate application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-129"
  },
  "harris04_interspeech": {
   "authors": [
    [
     "Thomas Kevin",
     "Harris"
    ],
    [
     "Roni",
     "Rosenfeld"
    ]
   ],
   "title": "A universal speech interface for appliances",
   "original": "i04_0249",
   "page_count": 4,
   "order": 131,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "Can a single, universal speech interface look-and-feel be used to effectively control a wide variety of appliances? Can such an interface be automatically derived from a functional appliance specification? We built the Speech Graffiti Personal Universal Controller (SG-PUC), a universal interface and framework for human-appliance speech interaction, as a proof-of-concept. Its specification language and communications protocol effectively separate the SG-PUC from the appliances that it controls, enabling mobile and universal speech-based appliance control. To realize such an automatically derived dialog system, the controller employs a universal control language. The development of interfaces to numerous appliances and the results of user studies demonstrate the usefulness of the SG-PUC, indicating that high quality and low cost human-appliance speech interface can be largely appliance agnostic. This investigation also helps to validate the principles of Speech Graffiti as a speech interface paradigm, and provides a baseline for future studies in this area.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-130"
  },
  "hayashi04_interspeech": {
   "authors": [
    [
     "Keita",
     "Hayashi"
    ],
    [
     "Yuki",
     "Irie"
    ],
    [
     "Yukiko",
     "Yamaguchi"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ]
   ],
   "title": "Speech understanding, dialogue management and response generation in corpus-based spoken dialogue system",
   "original": "i04_0253",
   "page_count": 4,
   "order": 132,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "This paper presents construction of a spoken dialogue system using a large-scale spoken dialogue corpus with intention tags. In this system, all of main components, such as speech understanding, dialogue management, and response generation, are constructed with corpus-based methods. An evaluation experiment using a test set has shown that the performance of the corpus-based dialogue system is improved by adding examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-131"
  },
  "fernandez04_interspeech": {
   "authors": [
    [
     "Fernando",
     "Fernandez"
    ],
    [
     "Valentin",
     "Sama"
    ],
    [
     "Luis F.",
     "D'Haro"
    ],
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Juan Manuel",
     "Montero"
    ]
   ],
   "title": "Implementation of dialog applications in an open-source voiceXML platform",
   "original": "i04_0257",
   "page_count": 4,
   "order": 133,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In this paper, we study the approach followed to use the VoiceXML standard in a dialog system platform already available in our group. As VoiceXML interpreter we have chosen OpenVXI, an open source portable solution where we can make the modifications needed to adapt the solution to the characteristics of our recognition and synthesis modules; so we will emphasize the changes that we have had to make in such interpreter. Besides, we review some relevant modules in our platform and their capabilities, highlighting the use of standards in them, as SSML for the text-to-speech system and JSGF for the specification of grammars for recognition. Finally, we discuss several ideas regarding the limitations detected in VoiceXML.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-132"
  },
  "lau04_interspeech": {
   "authors": [
    [
     "Chun Wai",
     "Lau"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Helen Mei-Ling",
     "Meng"
    ],
    [
     "Yiu-Sang",
     "Moon"
    ],
    [
     "Yeung",
     "Yam"
    ]
   ],
   "title": "Fuzzy logic decision fusion in a multimodal biometric system",
   "original": "i04_0261",
   "page_count": 4,
   "order": 134,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "This paper presents a multi-biometric verification system that combines speaker verification, fingerprint verification with face identification. Their respective equal error rates (EER) are 4.3%, 5.1% and the range of (5.1% to 11.5%) for matched conditions in facial image capture. Fusion of the three by majority voting gave a relative improvement of 48% over speaker verification (i.e. the best-performing biometric). Fusion by weighted average scores produced a further relative improvement of 52%. We propose the use of fuzzy logic decision fusion, in order to account for external conditions that affect verification performance. Examples include recording conditions of utterances for speaker verification, lighting and facial expressions in face identification and finger placement and pressure for fingerprint verification. The fuzzy logic framework incorporates some external factors relating to face and fingerprint verification and achieved an additional improvement of 19%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-133"
  },
  "poller04_interspeech": {
   "authors": [
    [
     "Peter",
     "Poller"
    ],
    [
     "Norbert",
     "Reithinger"
    ]
   ],
   "title": "A state model for the realization of visual perceptive feedback in smartkom",
   "original": "i04_0265",
   "page_count": 4,
   "order": 135,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "User acceptance of multimodal dialog systems depends on the quality of the process feedback to the user during interaction. We show an approach that permits the system to present simultaneously (and synchronously to the ongoing processing) adequate detailed and adaptive immediate feedback in various ways. For example, in multimodal dialog systems corresponding graphical presentations and/or appropriate behaviour patterns of an animated agent during user input processing significantly contribute to a more natural and expressively more powerful system. Additionally, the approach provides sufficient information to be able to present the user detailed descriptive feedback on the stage of processing at which an error occurred. The approach is mainly based on a module working state model that is used to determine in real time general processing state information in terms of relatively fine grained global system working states. Evaluations of the overall system show that simultaneous processing feedback is highly appreciated by users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-134"
  },
  "iida04_interspeech": {
   "authors": [
    [
     "Akemi",
     "Iida"
    ],
    [
     "Yoshito",
     "Ueno"
    ],
    [
     "Ryohei",
     "Matsuura"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "A vector-based method for efficiently representing multivariate environmental information",
   "original": "i04_0269",
   "page_count": 4,
   "order": 136,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper outlines our approach for describing multivariate environmental information such as weather as it might be characterized by humans using connotations and delicate nuances. The key feature of our approach is the use of a vector-based method. First, phrases implicitly expressing environmental information are collected. Next the environmental conditions of that time are entered from the GUI. The collected phrases and the newly entered information are represented as vectors of keywords representing such information. By computing the similarity between the given condition and each candidate phrase, the phrase that best represents the given condition is chosen. Two experimental systems that work in Japanese have been developed. The first system selects the appropriate phrase to represent the weather implicitly with a minimum number of phrases and not by lining up measured values. The other system selects poetic phrases that most appropriately represent the environmental conditions, including weather information and emotional states. The result of our evaluation has shown that both systems successfully selected the suitable phrases for representing given environmental conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-135"
  },
  "toptsis04_interspeech": {
   "authors": [
    [
     "Ioannis",
     "Toptsis"
    ],
    [
     "Shuyin",
     "Li"
    ],
    [
     "Britta",
     "Wrede"
    ],
    [
     "Gernot A.",
     "Fink"
    ]
   ],
   "title": "A multi-modal dialog system for a mobile robot",
   "original": "i04_0273",
   "page_count": 4,
   "order": 137,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "A challenging domain for dialog systems is their use for the communication with robotic assistants. In contrast to the classical use of spoken language for information retrieval, on a mobile robot multi-modal dialogs and the dynamic interaction of the robot system with its environment have to be considered. In this paper we will present the dialog system developed for BIRON - the Bielefeld Robot Companion. The system is able to handle multi-modal dialogs by augmenting semantic interpretation structures derived from speech with hypotheses for additional modalities as e.g. speech-accompanying gestures. The architecture of the system is modular with the dialog manager being the central component. In order to be aware of the dynamic behavior of the robot itself, the possible states of the robot control system are integrated into the dialog model. We will present example interactions with BIRON from the \"home-tour\" scenario defined within the COGNIRON project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-136"
  },
  "bernsen04_interspeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ]
   ],
   "title": "Structured interview-based evaluation of spoken multimodal conversation with h.c. andersen",
   "original": "i04_0277",
   "page_count": 4,
   "order": 138,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "This paper presents evaluation results on system performance and interaction from the user test of the first prototype of a multimodal conversational system. The system enables spoken and gestural interaction with life-like fairytale author Hans Christian Andersen about his fairytales, life, study, etc. The evaluation is based on structured interviews with 18 target users after their conversations with the system in a controlled laboratory setting. The obtained results are encouraging.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-137"
  },
  "novak04_interspeech": {
   "authors": [
    [
     "Miroslav",
     "Novak"
    ],
    [
     "Vladimir",
     "Bergl"
    ]
   ],
   "title": "Memory efficient decoding graph compilation with wide cross-word acoustic context",
   "original": "i04_0281",
   "page_count": 4,
   "order": 139,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "We present an efficient technique for the compilation of static decoding graphs which can utilize full word of left cross-word context. We put an emphasis on memory efficiency, in particular to be able to deploy this technique on platforms with limited resources. The incremental application of the composition process efficiently produces a weighted finite state acceptor which is globally deterministic and minimized with the maximum memory need during the composition essentially the same as that needed for the final graph.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-138"
  },
  "zhang04c_interspeech": {
   "authors": [
    [
     "Dongbin",
     "Zhang"
    ],
    [
     "Limin",
     "Du"
    ]
   ],
   "title": "Dynamic beam pruning strategy using adaptive control",
   "original": "i04_0285",
   "page_count": 4,
   "order": 140,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "In large vocabulary continuous speech recognition, huge search space results in vast computational cost. While most pruning search strategies can reduce the computation, but the recognition rate often decreases. This paper aims to reduce the computation time without any sacrifice of the recognition rate. By means of the adaptive control theory, a novel pruning method is presented. It can automatically steer beam to make search space attain an expected size. In order to make the decoder more efficient, the average number of active model instances is used as the dynamic reference of the adaptive system. Compared with the baseline system with fixed beam and histogram pruning, the proposed method leads to a significant reduction in the computation time and a slight improvement in word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-139"
  },
  "hori04_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Fast on-the-fly composition for weighted finite-state transducers in 1.8 million-word vocabulary continuous speech recognition",
   "original": "i04_0289",
   "page_count": 4,
   "order": 141,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "This paper proposes a new on-the-fly composition algorithm for Weighted Finite-State Transducers (WFSTs) in large-vocabulary continuous-speech recognition. In general on-the-fly composition, two transducers are composed during decoding, and a Viterbi search is performed based on the composed search space. In this new method, a Viterbi search is performed based on the first of two transducers. The second transducer is only used to rescore the hypotheses generated during the search. Since this rescoring is very efficient, the total amount of computation is almost the same as when using only the first transducer. In a 30k-word vocabulary spontaneous speech transcription task, our proposed method significantly outperformed the general on-the-fly algorithm. Furthermore our method worked with small memory requirements, and the speed was slightly faster than that of decoding with a single fully composed and optimized WFST. Finally, we have achieved one-pass real-time speech recognition in an extremely large vocabulary of 1.8 million words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-140"
  },
  "yu04_interspeech": {
   "authors": [
    [
     "Peng",
     "Yu"
    ],
    [
     "Frank Torsten Bernd",
     "Seide"
    ]
   ],
   "title": "A hybrid word / phoneme-based approach for improved vocabulary-independent search in spontaneous speech",
   "original": "i04_0293",
   "page_count": 4,
   "order": 142,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "For efficient organization of speech recordings -- meetings, interviews, voice mails, and lectures -- the ability to search for spoken keywords is essential. Today, most spoken document retrieval systems use large-vocabulary recognition. For the above scenarios, such systems suffer from the unpredictable domain and out-of-vocabulary queries. In previous work, we presented phoneme-lattice based vocabulary-independent search of spontaneous speech. In this paper, we propose to combine word-based and phonetic search into a hybrid, and explore two ways: posterior combination (merging search results of a word and a phoneme based system) and prior combination (combining word and phoneme LMs / vocabularies to form a hybrid recognizer). Our best phonetic baseline (FOM 64%) is improved by the hybrid approach to FOM 73% (word-level LM matching test set domain) and 71% (domain mismatch case).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-141"
  },
  "smidl04_interspeech": {
   "authors": [
    [
     "Lubos",
     "Smidl"
    ],
    [
     "Ludek",
     "Müller"
    ]
   ],
   "title": "Keyword spotting for highly inflectional languages",
   "original": "i04_0297",
   "page_count": 4,
   "order": 143,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "This paper presents our new keyword spotting system taking advantage of both the filler model and the confidence measure measure approaches. The novelty is in a non-standard connection of the filler and the keyword models together with introduction of a new confidence measure based on a keyword normalized score. In detail the paper deals with a decision block. Two methods are introduced. The first is based on comparison a keyword normalized score with a predefined decision threshold. The second uses three-layer feedforward neural network for decision if the keyword was or was not spoken. Results from the both presented methods are compared with the large vocabulary continuous speech recognition system used for keyword spotting. Obviously, LVCSR using a proper language model can give better results. Besides, it has higher CPU and memory demands. Furthermore, in many common situations the spontaneous language is mostly unconstrained and includes OOV words (keywords) such as names of peoples, companies, places, products etc., so the availability of an appropriate language model is very problematic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-142"
  },
  "tendeau04_interspeech": {
   "authors": [
    [
     "Frédéric",
     "Tendeau"
    ]
   ],
   "title": "Optimizing an engine network that allows dynamic masking",
   "original": "i04_0301",
   "page_count": 4,
   "order": 144,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "We present a method to determinize and minimize speech recognition networks where any relevant sub-network can be switched on/off at runtime without recompilation. We show how the general algorithm for determinizing general weighted transducers can be applied for this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-143"
  },
  "ohtsuki04_interspeech": {
   "authors": [
    [
     "Katsutoshi",
     "Ohtsuki"
    ],
    [
     "Nobuaki",
     "Hiroshima"
    ],
    [
     "Yoshihiko",
     "Hayashi"
    ],
    [
     "Katsuji",
     "Bessho"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Topic structure extraction for meeting indexing",
   "original": "i04_0305",
   "page_count": 4,
   "order": 145,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This paper describes a system that automatically generates meeting minutes by extracting a topic hierarchy from a meeting's speech. The topic hierarchy is a tree structure whose nodes comprise a topic summary. The topic structure extraction process converts speech recognition results into a word conceptual vector sequence and divides the sequence into the topic segments (topic segmentation). It classifies the topic segments hierarchically (segment clustering). Experimental results show that for the transcription of a meeting, the proposed algorithm is useful. Experiments on the transcription of a televised debate showed that the proposed topic segmentation algorithm is superior to the conventional method using local word frequency vectors. We also discuss experiments on the speech recognition results for the televised debate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-144"
  },
  "rosset04_interspeech": {
   "authors": [
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Automatic detection of dialog acts based on multilevel information",
   "original": "i04_0309",
   "page_count": 4,
   "order": 146,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "This paper reports on our experience in the automatic detection of dialog acts in human-human spoken dialog corpora. Two hypotheses underlie this work: first, word position is important in identifying the dialog act; and second, there is a strong grammar constraining the sequence of dialog acts. A Memory Based Learning approach has been used to detect dialog acts. Experiments are carried out with a known number of utterances per speaker turn, and with a hypothesized number of utterances determined using a language model for automatic utterance boundary detection. In order to verify our first hypothesis, the model trained on a French corpus was tested on an English corpus for a similar task and on a French corpus from a different domain. A correct dialog act detection rate of 83% is obtained for the same domain and language conditions and about 75% for the cross-language and cross-domain conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-145"
  },
  "levow04_interspeech": {
   "authors": [
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Identifying local corrections in human-computer dialogue",
   "original": "i04_0313",
   "page_count": 4,
   "order": 147,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "Miscommunication in human-computer interaction is unavoidable, although speech recognition accuracy continues to improve. While prior research has emphasized identifying the corrective status of an utterance, we focus in this paper on identifying the point of local correction. Users of spoken language systems often do not use specific syntactic structures or cue phrases to identify corrective intent or corrected content; most commonly a valid utterance is simply repeated, possibly slightly reworded. However, users do exploit prosodic cues to signal both presence and location of a correction. Using utterances from the 2000 and 2001 Communicator evaluation data collections, we build a boosted classifier to automatically identify the point of local correction in a corrective utterance. Exploiting the within sentence rank of prosodic cues including pitch maximum, pitch range, and intensity maximum, we distinguish locally corrected elements from other elements at 85.5% accuracy, a nearly 50% reduction in error rate over a naive majority class assignment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-146"
  },
  "reichl04_interspeech": {
   "authors": [
    [
     "Peter",
     "Reichl"
    ],
    [
     "Florian",
     "Hammer"
    ]
   ],
   "title": "Hot discussion or frosty dialogue? towards a temperature metric for conversational interactivity",
   "original": "i04_0317",
   "page_count": 4,
   "order": 148,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "Despite of its evident importance, measuring interactivity as a central parameter for characterizing conversations has not experienced much attention so far. Starting with an axiomatic approach and using thermodynamic concepts, this paper proposes the so-called conversational temperature as a new interactivity metric. The resulting scalar parameter is easily to be determined from standard measurements with conversation traces. Several flavors of the method are introduced and investigated for different tasks through a variety of experiments, thus validating the efficiency of our proposal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-147"
  },
  "seneff04_interspeech": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Lee",
     "Hetherington"
    ],
    [
     "Grace",
     "Chung"
    ]
   ],
   "title": "A dynamic vocabulary spoken dialogue interface",
   "original": "i04_0321",
   "page_count": 4,
   "order": 149,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "Mixed-initiative spoken dialogue systems today generally allow users to query with a fixed vocabulary and grammar that is determined prior to run-time. This paper presents a spoken dialogue interface enhanced with a dynamic vocabulary capability. One or more word classes can be made dynamic in the speech recognizer and natural language (NL) grammar so that a context-specific vocabulary subset can be incorporated on-the-fly as the context of the dialogue changes, at each dialogue turn. Described is a restaurant information domain which continually updates the restaurant name class, given the dialogue context. The system can even augment the vocabulary during a single user query by utilizing context available within the same utterance in a second pass. We examine progress made to the speech recognizer, natural language parser and dialogue manager in order to support the dynamic vocabulary capability, and present preliminary experimental results conducted from simulated dialogue runs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-148"
  },
  "denecke04_interspeech": {
   "authors": [
    [
     "Matthias",
     "Denecke"
    ],
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Mikio",
     "Nakano"
    ]
   ],
   "title": "Learning dialogue policies using state aggregation in reinforcement learning",
   "original": "i04_0325",
   "page_count": 4,
   "order": 150,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "The learning of dialogue strategies in spoken dialogue systems using reinforcement learning is a promising approach to acquire robust dialogue strategies. However, the trade-off between available dialogue data and information in the dialogue state either forces information to be excluded from the state representations or requires large amount of training data. In this paper, we propose to use dynamic state aggregation to efficiently learn dialogue policies using less data. State aggregation reduces the size of the problem to be solved. Experimental results show that the proposed method converges faster and that in case of data sparseness, the proposed method is less sensitive to atypical training examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-149"
  },
  "shatzman04_interspeech": {
   "authors": [
    [
     "Keren B.",
     "Shatzman"
    ]
   ],
   "title": "Segmenting ambiguous phrases using phoneme duration",
   "original": "i04_0329",
   "page_count": 4,
   "order": 151,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "The results of an eye-tracking experiment are presented in which Dutch listeners' eye movements were monitored as they heard sentences and saw four pictured objects. Participants were instructed to click on the object mentioned in the sentence. In the critical sentences, a stop-initial target (e.g., \"pot\") was preceded by an [s], thus causing ambiguity regarding whether the sentence refers to a stop-initial or a cluster-initial word (e.g., \"spot\"). Participants made fewer fixations to the target pictures when the stop and the preceding [s] were cross-spliced from the cluster-initial word than when they were spliced from a different token of the sentence containing the stop-initial word. Acoustic analyses showed that the two versions differed in various measures, but only one of these - the duration of the [s] - correlated with the perceptual effect. Thus, in this context, the [s] duration information is an important factor guiding word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-150"
  },
  "sakamoto04_interspeech": {
   "authors": [
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Yo-iti",
     "Suzuki"
    ],
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Tadahisa",
     "Kondo"
    ],
    [
     "Naoki",
     "Iwaoka"
    ]
   ],
   "title": "A compensation method for word-familiarity difference with SNR control in intelligibility test",
   "original": "i04_0333",
   "page_count": 4,
   "order": 152,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "Isolated-monosyllable listening tests are widely used in Japan to assess personal hearing ability. However, word intelligibility tests could be more suitable for measuring hearing ability. We developed new word lists with controlled word familiarity and phonetic balance. Listening test results showed slight differences in intelligibility scores among word lists within the same word familiarity rank. We examined whether such variation can be compensated by changing the signal to noise ratio (SNR) of each word to equalize intelligibility scores of word lists in identical word familiarity ranks. Results of listening tests indicated that a 1.0 difference in word familiarity was approximately equivalent to a 2.0dB difference in SNR. Based on the results, we developed and tested a method of compensating for the difference caused by word familiarity by controlling SNR. We confirmed that this compensation method was effective in equalizing the intelligibility score of each word list.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-151"
  },
  "otake04_interspeech": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Yoko",
     "Sakamoto"
    ],
    [
     "Yasuyuki",
     "Konomi"
    ]
   ],
   "title": "Phoneme-based word activation in spoken-word recognition: evidence from Japanese school children",
   "original": "i04_0337",
   "page_count": 4,
   "order": 153,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "This paper has investigated whether phoneme-based word activation in spoken-word recognition can be observed in Japanese by examining Japanese school children who have no knowledge of Roman alphabet. Two experiments were conducted with Japanese third graders at Japanese elementary schools with a word reconstruction task in order to test whether they could access to phonemes to reconstruct the original words from partially distorted words. The position of the distortion included both word-initial and word-medial. The results in the two experiments have clearly shown that they could reconstruct original Japanese words from the phoneme- distorted words than the mora-distorted words much faster and more accurate. The findings suggest that Japanese school children who do not have Roman alphabet may activate word candidates on the basis of phonemes, just like Japanese adults in our earlier studies and that the phoneme-based word activation may be universal in spoken-word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-152"
  },
  "brahimi04_interspeech": {
   "authors": [
    [
     "Belynda",
     "Brahimi"
    ],
    [
     "Philippe Boula de",
     "Mareuil"
    ],
    [
     "Cedric",
     "Gendrot"
    ]
   ],
   "title": "Role of segmental and suprasegmental cues in the perception of maghrebian-acented French",
   "original": "i04_0341",
   "page_count": 4,
   "order": 154,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "The general objective of this study is to clear up the relative importance of prosody in the identification of a foreign accent. The methodology we propose,based on the prosody transplantation paradigm, can be applied to different languages or language varieties. Here, it is applied to Magrhrebian-accented French: we wanted to study what is perceived when the segmental and suprasegmental characteristics of Maghrebian and native French speakers are crossed. Results obtained with French listeners (accent degree rating task) and Algerian listeners (origin identification task) converge and suggest that the articulation of phonemes overrides prosody to account for Maghrebian accents in French.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-153"
  },
  "kato04_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Makiko",
     "Muto"
    ]
   ],
   "title": "Effect of speaking rate on the acceptability of change in segment duration",
   "original": "i04_0345",
   "page_count": 4,
   "order": 155,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "An acceptability of segment duration changes in different speaking rates was studied to know perceptual characteristics for designing an objective naturalness measure in speech synthesis. Based on a series of our studies on intra-phrase positional dependency of perceptual acceptability, where listeners were more sensitive to the phrase-initial segment duration than the phrase-final one, we designed perceptual experiments using speech at three rates (fast, normal and slow) with or without a carrier sentence. The duration of each vowel at different phrase positions was either lengthened or shortened from 10ms to 50 ms and listeners evaluated the acceptability of these changes. The results showed that the acceptability declined more rapidly as a speaking rate became faster and that the difference of acceptability declination between intra-phrase positions was consistent. These results serve as fundamental data of speaking-rate dependent acceptability characteristics for designing an objective temporal naturalness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-154"
  },
  "yoneyama04_interspeech": {
   "authors": [
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "A cross-linguistic study of diphthongs in spoken word processing in Japanese and English",
   "original": "i04_0349",
   "page_count": 4,
   "order": 156,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "The current study conducted three phoneme-detecting experiments in order to (1) provide further evidence for the Japanese listeners' sensitivity to moraic structure and (2) treatment of diphthongs by three groups of language users (Japanese listeners, English listeners and semi-bilingual Japanese speakers of English). Japanese natives showed sensitivity to the moraic structure even when the moraic vowels are the second part of diphthongs. The results further showed that English listeners treat diphthongs as single units, while Japanese listeners treat them as two separate units. The semi-bilingual Japanese speakers of English treat diphthongs as single units in English but as separate units in Japanese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-155"
  },
  "waibel04_interspeech": {
   "authors": [
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Speech translation: past, present and future",
   "original": "i04_0353",
   "page_count": 4,
   "order": 157,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "A decade after its first beginnings, the grand challenge of building automatic systems that translate speech has grown into an active research area, a focus for speech and language researchers worldwide. Workshops, conferences, sessions, journal issues are devoted to it, and research is supported by governments in Asia, Europe and the US. The problem has attracted much attention, as practical needs in an increasingly globalized world converge with scientific advances that bring possible solutions within reach. While great progress has been made, the problem is certainly not solved and much remains to be done. At a midway point along the way, we present this paper as a review of the past, of the successes so far, and as an attempt to chart a course for the future.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-156"
  },
  "kikui04_interspeech": {
   "authors": [
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Multilingual corpora for speech-to-speech translation research",
   "original": "i04_0357",
   "page_count": 4,
   "order": 158,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "Multilingual spoken language corpora are indispensable for developing new speech-to-speech machine translation (S2SMT) technologies. This paper first discusses characteristics that corpora for S2SMT should have, then surveys existing corpora. Finally, it compares these corpora focusing on relations between collected data and collection scheme including instructions given to speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-157"
  },
  "ney04_interspeech": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Statistical machine translation and its challenges",
   "original": "i04_0361",
   "page_count": 4,
   "order": 159,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "In addition to speech recognition and syntactic parsing, during the last 10 years, the statistical approach has found widespread use in machine translation of both written language and spoken language. In many comparative evaluations, the statistical approach was found to be competitive or superior to the existing conventional approaches. Since the first statistical approach was proposed at the end of the 80s, many attempts have been made to improve the state of the art. Like other natural language processing tasks, machine translation requires four major components: a decision rule, a set of probability models, a training criterion and an efficient generation of the target sentence. We will consider each of these four components in more detail and point out promising research directions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-158"
  },
  "lee04g_interspeech": {
   "authors": [
    [
     "John",
     "Lee"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Translingual grammar induction",
   "original": "i04_0365",
   "page_count": 4,
   "order": 160,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "We propose an induction algorithm to semi-automate grammar authoring in an interlingua-based machine translation framework. This algorithm uses a pre-existing one-way translation system from some other language to the target language as prior information to infer a grammar for the target language. We demonstrate the system's effectiveness by automatically inducing a Chinese grammar for a weather domain from its English counterpart, and showing that it can produce high-quality translation from Chinese back to English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-159"
  },
  "lee04h_interspeech": {
   "authors": [
    [
     "Youngjik",
     "Lee"
    ],
    [
     "Jun",
     "Park"
    ],
    [
     "Seung-Shin",
     "Oh"
    ]
   ],
   "title": "Usability considerations of speech-to-speech translation system",
   "original": "i04_0369",
   "page_count": 4,
   "order": 161,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "This paper describes a practical speech-to-speech translation system architecture for operation in face-to-face situations, from a system point of view. The current achievements in speech recognition and language translation technologies are still insufficient to build a real working speech-to-speech translation system, even in a limited domain of application. To build such a working speech-to-speech translation system, it would be necessary to consider a system point of view that includes system design concept, usability and human correction capability. As a possible solution to meet this requirement, we suggest a speech-to-speech translation system architecture that maximizes the keyword transfer rate, a performance measure which we introduce in this paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-160"
  },
  "lazzari04_interspeech": {
   "authors": [
    [
     "Gianni",
     "Lazzari"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "Chengqing",
     "Zong"
    ]
   ],
   "title": "Worldwide ongoing activities on multilingual speech to speech translation",
   "original": "i04_0373",
   "page_count": 4,
   "order": 162,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "This paper presents an overview of worldwide ongoing activities on Speech to Speech Translation. After a short introduction to the field and a summarization of the major projects and milestones, activities and projects ongoing in Asia, Europe and US are presented and described.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-161"
  },
  "fohr04_interspeech": {
   "authors": [
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "The automatic news transcription system: ANTS, some real time experiments",
   "original": "i04_0377",
   "page_count": 4,
   "order": 163,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "This paper presents the recent development of ANTS, the Automatic News Transcription System of LORIA. This system was designed in the framework of ESTER, the French broadcast radio news transcription task evaluation. After describing its different components and some segmentation and recognition results on the ESTER database, we present a number of experiments focusing on the real-time version of ANTS. We evaluate the system with different number of Gaussians, sizes of vocabulary and decoder settings. The non real time version of ANTS yields an overall error rate of 36 % that can be compared to 44.5 % for the real time version.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-162"
  },
  "ramabhadran04_interspeech": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Use of metadata to improve recognition of spontaneous speech and named entities",
   "original": "i04_0381",
   "page_count": 4,
   "order": 164,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "With improved recognition accuracies for LVCSR tasks, it has become possible to search large collections of spontaneous speech for a variety of information. The MALACH corpus of Holocaust testimonials is one such collection, in which we are interested in automatically transcribing and retrieving portions that are relevant to named entities such as people, places, and organizations. Since the testimonials were gathered from thousands of people in countries throughout Europe, an extremely large number of potential named entities are possible, and this causes a well-known dilemma: increasing the size of the vocabulary allows for more of these words to be recognized, but also increases confusability, and can harm recognition performance. However, the MALACH corpus, like many other collections, includes side information or metadata that can be exploited to provide prior information on exactly which named entities are likely to appear. This paper proposes a method that capitalizes on this prior information to reduce named-entity recognition errors by over 50% relative, and simultaneously decrease the overall word error rate by 7% relative. The metadata we use derives from a pre-interview questionaire that includes the names of friends, relatives, places visited, membership of organizations, synonyms of place names, and similar information. By augmenting the lexicon and language model with this information on a speakerby- speaker basis, we are able to exploit the textual information that is already available in the corpus to facilitate much improved speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-163"
  },
  "pylkkonen04_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkonen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Duration modeling techniques for continuous speech recognition",
   "original": "i04_0385",
   "page_count": 4,
   "order": 165,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "Phone durations play a significant part in the comprehension of speech. The duration information is still mostly disregarded in automatic speech recognizers due to the use of hidden Markov models (HMMs) which are deficient in modeling phone durations properly. Previous results have shown that using different approaches for explicit duration modeling have improved the isolated word recognition in English. However, a unified comparison between the methods has not been reported. In this paper three techniques for explicit duration modeling are compared and evaluated in a large vocabulary continuous speech recognition task. The target language was Finnish, in which phone durations are especially important for proper understanding. The results show that the choice of the duration modeling technique depends on the speed requirements of the recognizer. The best technique required a slightly longer running time than without an explicit duration model, but achieved an 8% relative improvement to the letter error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-164"
  },
  "alumae04_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumae"
    ]
   ],
   "title": "Large vocabulary continuous speech recognition for estonian using morpheme classes",
   "original": "i04_0389",
   "page_count": 4,
   "order": 166,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "This paper describes development of a large vocabulary continuous speaker independent speech recognition system for Estonian. Estonian is an agglutinative language and the number of different word forms is very large, in addition, the word order is relatively unconstrained. To achieve a good language coverage, we use pseudo-morphemes as basic units in a statistical trigram language model. To improve language model robustness, we automatically find morpheme classes and interpolate the morpheme model with the class-based model. The language model is trained on a newspaper corpus of 15 million word forms. Clustered triphones with multiple Gaussian mixture components are used for acoustic modeling. The system with interpolated morpheme language model is found to perform significantly better than the baseline word form trigram system in all areas. The word error rate of the best system is 27.3% which is a 10.0% absolute improvement over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-165"
  },
  "han04b_interspeech": {
   "authors": [
    [
     "Zhaobing",
     "Han"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Combining agglomerative and tree-based state clustering for high accuracy acoustic modeling",
   "original": "i04_0393",
   "page_count": 4,
   "order": 167,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "Robust estimate of a large number of parameters against the availability of training data is a crucial issue in triphone based continuous speech recognition. To cope with the issue, two major context-clustering methods, agglomerative (AGG) and tree-based (TB), have been widely studied. In this paper, we analyze two algorithms with respect to their advantages and disadvantages and introduce a novel combined method that takes advantage of each method to cluster and tie similar acoustic states for highly detailed acoustic models. In addition, we devise a two-level clustering approach for TB, which uses the tree-based state tying for rare acoustic phonetic events twice. For LVCSR, the experimental results showed the performance could be highly improved by using the proposed combined method, compared with those of using the popular TB method alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-166"
  },
  "wang04e_interspeech": {
   "authors": [
    [
     "William S-Y.",
     "Wang"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Parallel tone score association method for tone language speech recognition",
   "original": "i04_0397",
   "page_count": 4,
   "order": 168,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "Tone is an essential component for word formation in all tone languages. Substantial work has been done on using tone information to improve speech recognition of tone languages. In this paper, a new method, called Parallel Tone Score Association (PTSA), for effectively and efficiently using tone in speech recognition is proposed. Experimental results show that the relative character error rates are reduced by as much as 20.94% for Cantonese, and 20.49% for Mandarin compared with the recognition results without tone information. This relative reduction in error rates compares favorably with results reported for other recognition experiments on tone languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-167"
  },
  "zheng04_interspeech": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Effective acoustic modeling for rate-of-speech variation in large vocabulary conversational speech recognition",
   "original": "i04_0401",
   "page_count": 4,
   "order": 169,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "We investigate several variants of speech-rate-dependent acoustic models for large-vocabulary conversational speech recognition, in the framework of combining rate-specific models in decoding to compensate for speech rate variation. We study two basic approaches to combining rate-specific models: one combines models at the pronunciation level and the other at the HMM state level. Furthermore, we investigate the influence of different numbers of rate-of-speech classes and different parameter tying schemes. Experiments on the Switchboard database, using SRI's DECIPHER recognition system, show that rate-dependent acoustic modeling resulted in a 2% relative word error rate reduction over a rate-independent baseline, and that the pronunciation-level constraint, Gaussian sharing between rate-specific models, and a well-chosen number of rate-of-speech classes are all important for best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-168"
  },
  "ghadiyaram04_interspeech": {
   "authors": [
    [
     "G.L. Sarada",
     "Ghadiyaram"
    ],
    [
     "N. Hemalatha",
     "Nagarajan"
    ],
    [
     "T. Nagarajan",
     "Thangavelu"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Automatic transcription of continuous speech using unsupervised and incremental training",
   "original": "i04_0405",
   "page_count": 4,
   "order": 170,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "In [1], a novel approach is proposed for automatically segmenting and transcribing continuous speech signal without the use of manually annotated speech corpora. In this approach, the continuous speech signal is first automatically segmented into syllable-like units and similar syllable segments are grouped together using an unsupervised and incremental clustering technique. Separate models are generated for each cluster of syllable segments and labels are assigned to them. These syllable models are then used for recognition/transcription. Even though the results in [1] are quite promising, there are some problems in the clustering technique due to (i) the presence of silence segments at the beginning and end of syllable boundaries. (ii) fragmentation of syllables (iii) merging of syllables and (iv) poor initialization of syllable models. In this paper we specifically address these issues, make several refinements to the baseline system, which has resulted in a significant performance improvement of 8% over that of the baseline system described in [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-169"
  },
  "nouza04_interspeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Dana",
     "Nejedlova"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Jan",
     "Kolorenc"
    ]
   ],
   "title": "Very large vocabulary speech recognition system for automatic transcription of czech broadcast programs",
   "original": "i04_0409",
   "page_count": 4,
   "order": 171,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "This paper describes the first speech recognition system capable of transcribing a wide range of spoken broadcast programs in Czech language with the OOV rate being below 3 per cent. To achieve that level we had to a) create an optimized 200k word vocabulary with multiple text and pronunciation forms, b) extract an appropriate language model from a 300M word text corpus and c) develop an own decoder specially designed for the lexicon of that size. The system was tested on various types of broadcast programs with the following results: the Czech part of the European COST278 database of TV news (71.5 % accuracy rate on complete news streams, 82.7 % on their clean parts), radio news (80.2 %), read commentaries (78.6 %), broadcast debates (74.3 %) and recordings of the speeches given by state presidents (85.8 %).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-170"
  },
  "siohan04_interspeech": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Speech recognition error analysis on the English MALACH corpus",
   "original": "i04_0413",
   "page_count": 4,
   "order": 172,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "This paper presents an analysis of the word recognition error rate on an English subset of the MALACH corpus. The MALACH project is an NSF-funded research program related to the development of multilingual access to large audio archives. The archive is a large collection of testimonies from survivors of the Nazi Holocaust. This data has some unique characteristics such as elderly speech, noisy conditions, heavily accented speech. Hence, it is a challenging task for automatic speech recognition (ASR). This paper attempts to identify the factors affecting the ASR performance. It was found that the signal-to-noise ratio and syllable rate were two dominant factors in explaining the overall word error rate, while we observed no evidence of the impact of accent and speaker's age on the recognition performance. Based on this evidence, noise compensation experiments were carried out and led to a 1.1% absolute reduction of the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-171"
  },
  "zhang04d_interspeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "A frame level boosting training scheme for acoustic modeling",
   "original": "i04_0417",
   "page_count": 4,
   "order": 173,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "Conventional Boosting algorithms for acoustic modeling have two notable weaknesses. (1) The objective function aims to minimize utterance error rate, though the goal for most speech recognition systems is to reduce word error rate. (2) During Boosting training, an utterance is treated as a unit for re-sampling and each frame within the same utterance is assigned equal weight. Intuitively, the frames associated with a misclassified word should be given more emphasis than others. We propose a frame level Boosting training scheme that addresses these shortcomings and allows each frame to have a different weight. We describe a technique and provide experimental results for this approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-172"
  },
  "zhang04e_interspeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "Optimizing boosting with discriminative criteria",
   "original": "i04_0421",
   "page_count": 4,
   "order": 174,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "We describe the use of discriminative criteria to optimize Boosting based ensembles. Boosting algorithms may create hundreds of individual classifiers in order to fit the training data. However, this strategy is not feasible and necessary for complex classification problems, such as real-time continuous speech recognition, in which only the combination of a few of acoustic models is practical. How to improve the classification accuracy for small size of ensemble is the focus of this paper. Two discriminative criteria that attempt to minimize the true Bayes error rate are investigated. Improvements are observed over a variety of datasets including image and speech recognition, indicating the prospective utility of these two criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-173"
  },
  "xu04_interspeech": {
   "authors": [
    [
     "Xianghua",
     "Xu"
    ],
    [
     "Qiang",
     "Guo"
    ],
    [
     "Jie",
     "Zhu"
    ]
   ],
   "title": "Restructuring HMM states for speaker adaptation in Mandarin speech recognition",
   "original": "i04_0425",
   "page_count": 4,
   "order": 175,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "With the tendency of posterior probability taken into account, a state-restructuring method is proposed based on confusions between HMM states. In the method, HMM state is restructured by sharing Gaussian components with its related states and the re-estimation to the increased-parameters, i.e., the inter-state weights, is derived under the EM framework. Experiments are performed on speaker-independent large vocabulary continuous Mandarin speech recognition. The results show the state-restructured systems outperform the baseline system and the combining with MLLR adaptation can lead to consistent and significant improvement on recognition accuracy over MLLR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-174"
  },
  "matton04_interspeech": {
   "authors": [
    [
     "Mike",
     "Matton"
    ],
    [
     "Mathias De",
     "Wachter"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Ronald",
     "Cools"
    ]
   ],
   "title": "A discriminative locally weighted distance measure for speaker independent template based speech recognition",
   "original": "i04_0429",
   "page_count": 4,
   "order": 176,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "In template based speech recognition, there is a need for a high-performant distance measure between speech frames. Some well known metrics include the Euclidean and the Mahalanobis distance. The recent tendency is to perform a local scaling of the distance metric, defining a set of classes and computing a set of weights for each of these classes. Discriminative training approaches have already proven their usefulness in various domains including speech recognition. They have the well known characteristic of training the weights for all of the classes simultaneously, and not independently of each other. In this paper, a first attempt is made to incorporate a discriminative distance measure into template based speech recognition. We use a distance measure trained by a very intuitive discriminative criterion and show that it works very well, even beating the performance results of comparable HMM-based speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-175"
  },
  "itaya04_interspeech": {
   "authors": [
    [
     "Yohei",
     "Itaya"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Deterministic annealing EM algorithm in parameter estimation for acoustic model",
   "original": "i04_0433",
   "page_count": 4,
   "order": 177,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "This paper investigates the effectiveness of the DAEM (Deterministic Annealing EM) algorithm in acoustic modeling for speaker and speech recognition. Although the EM algorithm has been widely used to approximate the ML estimates, it has the problem of initialization dependence. To relax this problem, the DAEM algorithm has been proposed and confirmed the effectiveness in small tasks. In this paper, we applied the DAEM algorithm to speaker recognition based on GMMs and coutinuous speech recognition based on HMMs. Experimental results show that the DAEM algorithm can improve the recognition performance as compared to the ordinary EM algorithm with conventional initialization methods, especially in the flat start for continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-176"
  },
  "grezl04_interspeech": {
   "authors": [
    [
     "Frantisek",
     "Grezl"
    ],
    [
     "Martin",
     "Karafiat"
    ],
    [
     "Jan",
     "Cernocky"
    ]
   ],
   "title": "TRAP based features for LVCSR of meting data",
   "original": "i04_0437",
   "page_count": 4,
   "order": 178,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "This paper describes using temporal patterns (TRAPs) feature extraction in large vocabulary continuous speech recognition (LVCSR) of meeting data. Frequency differentiation and local operators are applied to critical-band speech spectrum. Tests are performed with HMM recognizer on ICSI meetings database. We show that TRAP features in with standard ones lead to improvement of word-error rate (WER).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-177"
  },
  "soong04_interspeech": {
   "authors": [
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Wai Kit",
     "Lo"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Optimal acoustic and language model weights for minimizing word verification errors",
   "original": "i04_0441",
   "page_count": 4,
   "order": 179,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "Generalized word posterior probability (GWPP), a confidence measure for verifying recognized words, needs to optimize acoustic and language model weights. In this study, we investigate the word verification error surface and use it to optimize these weights and the corresponding verification threshold in a development set. We test three different search algorithms for finding the optimal parameters, including: a full grid search, a gradient-based steepest descent search, and a downhill simplex search. The three search methods yield very similar solutions. Proper acoustic and language model weights, especially the ratio between them, changes with the relative importance (reliability) between the two knowledge sources. For a narrow beam width, the role of the acoustic model is less critical than language model in GWPP-based word verification, which is due to the noisier acoustic information maintained in a narrow beam. Using a large vocabulary continuous Japanese speech database (Basic Travel Expression Corpus), the largest relative improvement obtained is 33.2% for confidence error rate and 38.7% for a modified word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-178"
  },
  "sako04_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Sako"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Structuring of baseball live games based on speech recognition using task dependant knowledge",
   "original": "i04_0445",
   "page_count": 4,
   "order": 180,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "It is a difficult problem to recognize baseball live speech because the speech is rather fast, noisy and disfluent due to rephrasing, repetition, mistake and grammatical deviation caused by spontaneous speaking style. To solve these problems, we propose in this paper a speech recognition method of incorporating the baseball game knowledge such as counting of inning, out, strike and ball. Due to this task-dependent knowledge, the proposed method can effectively prevent speech recognition errors. This method is formalized in the framework of probability theory and implemented in the conventional speech decoding (Viterbi) algorithm. The experimental results showed that the proposed approach improved the structuring situation segmentation accuracy as well as keywords accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-179"
  },
  "zhou04b_interspeech": {
   "authors": [
    [
     "Zhengyu",
     "Zhou"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "A two-level schema for detecting recognition errors",
   "original": "i04_0449",
   "page_count": 4,
   "order": 181,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "This paper proposes a two-level schema for detecting recognition errors. Given the recognition hypothesis of an utterance, the first level in our schema applies an utterance classifier (UC) to decide if the hypothesis is error-free or erroneous. In the latter case, the utterance is passed on to the second level in our schema for further processing. A word classifier (WC) is applied to each of the word hypotheses in the utterance to decide whether or not it is a misrecognition. Hence the two-level schema can locate error-containing regions in the recognition hypotheses. These are the target regions to which we can apply more sophisticated and expensive language models for error correction as a next step. Experiments on Mandarin Chinese speech recognition showed that the UC has a detection error rate of 16.5% for misrecognized utterances; the WC has a detection error rate of 19.8% for erroneous word hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-180"
  },
  "choi04b_interspeech": {
   "authors": [
    [
     "In-Jeong",
     "Choi"
    ],
    [
     "Nam-Hoon",
     "Kim"
    ],
    [
     "Su Youn",
     "Yoon"
    ]
   ],
   "title": "Large vocabulary continuous speech recognition based on cross-morpheme phonetic information",
   "original": "i04_0453",
   "page_count": 4,
   "order": 182,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "In this paper, we present a novel method to regulate lexical connections among morpheme-based pronunciation lexicons for Korean large vocabulary continuous speech recognition (LVCSR) systems. A pronunciation dictionary plays an important role in subword-based LVCSR in that pronunciation variations such as coarticulation will deteriorate the performance of an LVCSR system if it is not well accounted for. In general, pronunciation variations are modeled by applying phonological variations with all possible phonemic contexts. In order to achieve high recognition performance, current speech recognition systems impose constraints among lexicons using both morphological and phonetic knowledge. This paper suggests a method both to refine pronunciation variations according to cross-morpheme phonetic information and to regulate the connections between pronunciation variants. This method effectively excludes improper connections between pronunciation lexicons, and thus the proposed method gave a 27% reduction in word error rate over the recognizer with conventional lexicons relatively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-181"
  },
  "ma04_interspeech": {
   "authors": [
    [
     "Changxue",
     "Ma"
    ]
   ],
   "title": "Automatic phonetic base form generation based on maximum context tree",
   "original": "i04_0457",
   "page_count": 4,
   "order": 183,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "To improve the performance and the usability of the speech recognition devices, it is necessary for most applications to allow users to enter new words or personalize words in the system vocabulary. The voice-tagging technique is a simple example of using speaker dependent spoken samples to generate baseform transcriptions of the spoken words. More sophisticated techniques can use both spoken samples and text versions of the new words to generate baseform transcriptions. In this paper, we propose a maximum context tree (MCT) based approach to the problem. Comparison is made to the common decision tree based method and Pronunciation by Analogy (PbA) approach. The new approach gives exact baseform transcription for in-vocabulary words and it shows better performance than decision tree. It performs significantly better than PbA approach with less memory usages. MCT uses the word segment probability rather than frequency count used in PbA. MCT uses the full context for the focus letter to overcome the some deficiencies in the PbA approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-182"
  },
  "hernandezabrego04_interspeech": {
   "authors": [
    [
     "Gustavo",
     "Hernandez-Abrego"
    ],
    [
     "Lex",
     "Olorenshaw"
    ],
    [
     "Raquel",
     "Tato"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "Dictionary refinements based on phonetic consensus and non-uniform pronunciation reduction",
   "original": "i04_1697",
   "page_count": 4,
   "order": 184,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "In this paper we present a procedure to refine the recognition dictionary based on a composite approach to prune the unneeded pronunciations. First, pruning is applied in a non-uniform manner according to the characteristics of each word. Even though this straightforward operation may produce high-quality dictionaries, it makes the refined dictionary heavily dependent on the data used in this process. For the words not observed in the data, we propose, in second place, to use Multiple Sequence Alignment techniques in order to find phonetic consensus among the pronunciation variants and select the worthy pronunciations that will represent the unobserved words. Experimental results show that our dictionary refining method helps to improve the recognition performance in two relevant aspects: it increases the recognition accuracy by reducing the cross-word confusibility and it improves the recognition speed by reducing the complexity of the search space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-183"
  },
  "messaoudi04_interspeech": {
   "authors": [
    [
     "Abdel.",
     "Messaoudi"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Transcription of arabic broadcast news",
   "original": "i04_1701",
   "page_count": 4,
   "order": 185,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "This paper describes recent research on transcribing Modern Standard Arabic broadcast news data. The Arabic language presents a number of challenges for speech recognition, arising in part from the significant differences in the spoken and written forms, in particular the conventional form of texts being non-vowelized. Arabic is a highly inflected language where articles and affixes are added to roots in order to change the word's meaning. A corpus of 50 hours of audio data from 7 television and radio sources and 200M words of newspaper texts were used to train the acoustic and language models. The transcription system based on these models and a vowelized dictionary obtains an average word error rate on a test set comprised of 12 hours of test data from 8 sources is about 18%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-184"
  },
  "shinozaki04_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Spontaneous speech recognition using a massively parallel decoder",
   "original": "i04_1705",
   "page_count": 4,
   "order": 186,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "Since spontaneous utterances include many variations, speaker- and task-independent general models do not work well. This paper proposes combining cluster-based language and acoustic models based on the framework of Massively Parallel Decoder (MPD). The MPD is a parallel decoder that has a large number of decoding units, in which each unit is assigned to each combination of element models. It runs efficiently on a parallel computer, and thus the turnaround time is comparable to conventional decoders using a single model and a processor. In the experiments conducted using lecture speeches from the Corpus of Spontaneous Japanese, two types of cluster models have been investigated: lecture-based cluster models and utterance-based cluster models. It has been confirmed that utterance-based cluster models give significantly lower recognition error rate than lecture-based cluster models in both language and acoustic modeling. It has also been shown that roughly 100 decoding units are enough in terms of recognition rate, and in the best setting, 12% reduction in word error rate was obtained in comparison with the conventional decoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-185"
  },
  "schultz04b_interspeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Yue",
     "Pan"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Christian",
     "Fügen"
    ]
   ],
   "title": "Issues in meeting transcription - the ISL meeting transcription system",
   "original": "i04_1709",
   "page_count": 4,
   "order": 187,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "This paper describes the Interactive Systems Lab's Meeting transcription system, which performs segmentation, speaker clustering as well as transcriptions of conversational meeting speech. The system described here was evaluated in NIST's RT-04S \"Meeting\" speech evaluation and reached the lowest word error rates for the distant microphone conditions. Also, we compare the performance of our Broadcast News and the most recent Switchboard system on the Meeting data and compare both with the newly-trained meeting recognizer. Furthermore, we investigate the effects of automatic segmentation on adaptation. Our best meeting system achieves a WER of 44.5% on the \"MDM\" condition in NIST's RT-04S evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-186"
  },
  "ohtsuki04b_interspeech": {
   "authors": [
    [
     "Katsutoshi",
     "Ohtsuki"
    ],
    [
     "Nobuaki",
     "Hiroshima"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Yoshihiko",
     "Hayashi"
    ]
   ],
   "title": "Multi-pass ASR using vocabulary expansion",
   "original": "i04_1713",
   "page_count": 4,
   "order": 188,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "Current ASR systems have to limit its vocabulary size depending on available memory size, expected processing time, and available text data for building a vocabulary and a language model. Although vocabularies of ASR systems are designed to achieve high coverage for expected input data, it can not be avoided that input data includes out-of-vocabulary (OOV) words that is OOV problem. In this paper, we propose dynamic vocabulary expansion using conceptual base and multi-pass speech recognition using the expanded vocabulary. Relevant words to content of input speech are extracted based on a speech recognition result obtained using a reference vocabulary. An expanded vocabulary that includes less OOV words is built by adding the extracted words to the reference vocabulary. The second recognition process is performed using the new vocabulary. The experimental results for broadcast news speech show the proposed method achieves 30% reduction of OOV rate and improve speech recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-187"
  },
  "doumpiotis04_interspeech": {
   "authors": [
    [
     "Vlasios",
     "Doumpiotis"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Pinched lattice minimum Bayes risk discriminative training for large vocabulary continuous speech recognition",
   "original": "i04_1717",
   "page_count": 4,
   "order": 189,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "Iterative estimation procedures that minimize empirical risk based on general loss functions such as the Levenshtein distance have been derived as extensions of the Extended Baum Welch algorithm. While reducing expected loss on training data is a desirable training criterion, these algorithms can be difficult to apply. They are unlike MMI estimation in that they require an explicit listing of the hypotheses to be considered and in complex problems such lists tend to be prohibitively large. To overcome this difficulty, modeling techniques originally developed to improve search efficiency in Minimum Bayes Risk decoding can be used to transform these estimation algorithms so that exact update, risk minimization procedures can be used for complex recognition problems. Experimental results in two large vocabulary speech recognition tasks show improvements over conventionally trained MMIE models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-188"
  },
  "shafran04_interspeech": {
   "authors": [
    [
     "Izhak",
     "Shafran"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Task-specific minimum Bayes-risk decoding using learned edit distance",
   "original": "i04_1945",
   "page_count": 4,
   "order": 190,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "This paper extends the minimum Bayes-risk framework to incorporate a loss function specific to the task and the ASR system. The errors are modeled as a noisy channel and the parameters are learned from the data. The resulting loss function is used in the risk criterion for decoding. Experiments on a large vocabulary conversational speech recognition system demonstrate significant gains of about 1% absolute over MAP hypothesis and about 0.6% absolute over untrained loss function. The approach is general enough to be applicable to other sequence recognition problems such as in Optical Character Recognition (OCR) and in analysis of biological sequences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-189"
  },
  "zhang04f_interspeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "Apply n-best list re-ranking to acoustic model combinations of boosting training",
   "original": "i04_1949",
   "page_count": 4,
   "order": 191,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "The object function for Boosting training method in acoustic modeling aims to reduce utterance level error rate. This is different from the most commonly used performance metric in speech recognition, word error rate. This paper proposes that the combination of N-best list re-ranking and ROVER can partly address this problem. In particular, model combination is applied to re-ranked hypotheses rather than to the original top-1 hypotheses and carried on word level. Improvement of system performance is observed in our experiments. In addition, we describe and evaluate a new confidence feature that measures the correctness of frame level decoding result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-190"
  },
  "kim04f_interspeech": {
   "authors": [
    [
     "D. Y.",
     "Kim"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "T.",
     "Hain"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Using VTLN for broadcast news transcription",
   "original": "i04_1953",
   "page_count": 4,
   "order": 192,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "Vocal tract length normalisation (VTLN) is a commonly used speaker normalisation approach. It is attractive compared to many normalisation schemes as it is typically dependent on only a single parameter, allowing the warp factors to be robustly calculated on little data. However, the scheme normally requires explicitly coding the data at multiple warp factors. Furthermore, it is only possible to approximate the Jacobian associated with the VTLN transformation. A new, simple, linear approximation to VTLN is described in this paper. This linear approximation allows the Jacobian to be exactly computed. It can also be highly efficient in terms of warp factor estimation and application of the warp factors. Both the linear and standard CUED VTLN schemes are evaluated in the 2003 BNE evaluation framework and found to yield similar performance. When used in system combination both VTLN schemes yielded slight gains over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-191"
  },
  "stolcke04_interspeech": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Scott",
     "Otterson"
    ],
    [
     "Barbara",
     "Peskin"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Dave",
     "Gelbart"
    ],
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Tuomo",
     "Pirinen"
    ]
   ],
   "title": "From switchboard to meetings: development of the 2004 ICSI-SRI-UW meeting recognition system",
   "original": "i04_1957",
   "page_count": 4,
   "order": 193,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "We describe the ICSI-SRI-UW team's entry in the Spring 2004 NIST Meeting Recognition Evaluation. The system was derived from SRI's 5xRT Conversational Telephone Speech (CTS) recognizer by adapting CTS acoustic and language models to the meeting domain, adding noise reduction and delay-sum array processing processing for farfield recognition, and postprocessing for cross-talk suppression. A modified MAP adaptation procedure was developed to make best use of discriminatively trained (MMIE) prior models. These meeting-specific changes yielded an overall 9% and 22% relative improvement as compared to the orignal CTS system, and 16% and 29% relative improvement as compared to our 2002 Meeting Evaluation system, for the individual-headset and multiple-distant microphones conditions, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-192"
  },
  "venkataraman04_interspeech": {
   "authors": [
    [
     "Anand",
     "Venkataraman"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Venkata Ramana Rao",
     "Gadde"
    ]
   ],
   "title": "An efficient repair procedure for quick transcriptions",
   "original": "i04_1961",
   "page_count": 4,
   "order": 194,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "We describe an efficient procedure for automatic repair of quickly transcribed speech. Quickly transcribed (QT) speech, typically closed captioned data from television brodcasts, usually has a significant number of deletions and misspellings, and has characteristic absence of disfluencies. Errors of this kind often throw an acoustic model training program out of alignment and make it hard for it to resynchronize. At best the erroneous utterance is discarded and does not benefit the training procedure. At worst, it could misalign and end up sabotaging the training data. The procedure we propose in this paper aims to {em cleanse/} such quick transcriptions so that they align better with the acoustic evidence and thus provide for better acoustic models. Results from comparing our transcripts with those from careful transcriptions on the same corpus, and from comparable state-of-the-art methods are also presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-193"
  },
  "qian04_interspeech": {
   "authors": [
    [
     "Yao",
     "Qian"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Tone information as a confidence measure for improving Cantonese LVCSR",
   "original": "i04_1965",
   "page_count": 4,
   "order": 195,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "Cantonese, a syllabically paced, southern Chinese dialect, is a tonal language. A Cantonese syllable can have up to 9 different tone patterns which are lexically important. In this paper after reviewing major approaches to incorporating tone information into a large vocabulary continuous speech recognition (LVCSR) system, we propose two schemes to employ the tone information as a confidence measure for improving Cantonese LVCSR performance: either by guiding the stack decoder to search word lattice or by rescoring the N-best strings. It is shown the proposed schemes yield 4.2% and 8.0% relative improvement of Chinese character error rate, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-194"
  },
  "due04_interspeech": {
   "authors": [
    [
     "Danielle",
     "Due"
    ]
   ],
   "title": "Temporal variables in parkinsonian speech",
   "original": "i04_0461",
   "page_count": 4,
   "order": 196,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "Temporal variables were detailed and compared in a text read by ten French subjects with mild to moderate Parkinson disease (PDS') and ten French healthy control subjects (CS'). Longer total pause times were found for PDS', correlated to higher numbers of pauses and longer mean pause duration. The typical relationship observed between the distributional scheme of pauses and syntactic structure of the text remained largely intact for PDS'. However, PDS' also made strikingly long and unexpected within-phrase and within-word pauses, most often associated with a dysfluency. Pause time and pattern were found to be highly variable across PDS', some of whom had a fast speech rate, others a slow one. The results can be viewed as manifestations of deficient timing of motor events which appears to be a strong pathomechanism in Parkinson disease.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-195"
  },
  "engwall04_interspeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Speaker adaptation of a three-dimensional tongue model",
   "original": "i04_0465",
   "page_count": 4,
   "order": 197,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "Magnetic Resonance Images of nine subjects have been collected to determine scaling factors that can adapt a 3D tongue model to new subjects. The aim is to define few and simple measures that will allow for an automatic, but accurate, scaling of the model. The scaling should be automatic in order to be useful in an application for articulation training, in which the model must replicate the user's articulators without involving the user in a complicated speaker adaptation. It should further be accurate enough to allow for correct acoustic-to-articulatory inversion. The evaluation shows that the defined scaling technique is able to estimate a tongue shape that was not included in the training with an accuracy of 1.5 mm in the midsagittal plane and 1.7 mm for the whole 3D tongue, based on four articulatory measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-196"
  },
  "cooper04_interspeech": {
   "authors": [
    [
     "Nicole",
     "Cooper"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Perception of non-native phonemes in noise",
   "original": "i04_0469",
   "page_count": 4,
   "order": 198,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "We report an investigation of the perception of American English phonemes by Dutch listeners proficient in English. Listeners identified either the consonant or the vowel in most possible English CV and VC syllables. The syllables were embedded in multispeaker babble at three signal-to-noise ratios (16 dB, 8 dB, and 0 dB). Effects of signal-to-noise ratio on vowel and consonant identification are discussed as a function of syllable position and of relationship to the native phoneme inventory. Comparison of the results with previously reported data from native listeners reveals that noise affected the responding of native and non-native listeners similarly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-197"
  },
  "kawahara04b_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Jiang",
     "Jin"
    ]
   ],
   "title": "Intelligibility of degraded speech from smeared STRAIGHT spectrum",
   "original": "i04_0473",
   "page_count": 4,
   "order": 199,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "Intelligibility of degraded speech sounds has been investigated based on a new signal processing technique using a high-quality vocoder, STRAIGHT. This enables us to manipulate essential speech parameters for vocal tract filtering and glottal excitation. We report that the effect of spectral smearing on the intelligibility of Japanese four- mora words as an initial study. Results reveal that the intelligibility decreases as the degree of smearing increases. We also investigated the relationship between the phonetic and word intelligibilities and found that the word identification score was predicted as the power function of the phonetic score when the power value was about 5. This implies that the mora structure and prosodic information such as F0, timing, and duration also play an important role in speech perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-198"
  },
  "kim04g_interspeech": {
   "authors": [
    [
     "Young-Ik",
     "Kim"
    ],
    [
     "Rhee Man",
     "Kil"
    ]
   ],
   "title": "Sound source localization based on zero-crosing peak-amplitude coding",
   "original": "i04_0477",
   "page_count": 4,
   "order": 200,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "This paper presents a new method of sound source localization based on zero-crossings generated from binaural filter-bank outputs. To detect the sound source direction, in the conventional methods the inter-aural time differences (ITDs) and the inter-aural intensity differences (IIDs) are estimated using the cross-correlations of neuronal firing rates. However, these methods require high computational complexity involved in the computation of cross-correlations and they suffer from inaccuracies in estimating the ITDs and IIDs, especially in a noisy multi-source environment. In this context, we propose a method using zero-crossing and peak-amplitudes (ZCPAs) of binaural filter-bank outputs for accurate and efficient estimation of ITDs and IIDs. The proposed method is able to provide accurate estimation of the sound source direction and robustness to noise while offering significantly less computational complexity compared to cross-correlation based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-199"
  },
  "sachiyo04_interspeech": {
   "authors": [
    [
     "Kajikawa",
     "Sachiyo"
    ],
    [
     "Fais",
     "Laurel"
    ],
    [
     "Amano",
     "Shigeaki"
    ],
    [
     "Werker",
     "Janet"
    ]
   ],
   "title": "Adult and infant sensitivity to phonotactic features in spoken Japanese",
   "original": "i04_0481",
   "page_count": 4,
   "order": 201,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "Japanese speakers perceive an epenthetic vowel between consonants in words, reflecting an adaptation to Japanese phonotactics. However, there are some contexts in which CC (successive consonant) clusters are acceptable in Japanese speech. This study explored how Japanese speakers perceive phoneme sequences according to Japanese phonotactics. In Experiment 1, adults rated goodness of nonsense words with CV and CC sequences as exemplars of Japanese words. The adults were sensitive to the legitimacy of vowel devoicing. They considered phoneme sequences following the Japanese phonotactics to be better than exceptional but possible sequences. Experiment 2 investigated 6-, 12-, and 18-month-old infants' sensitivity to phoneme changes in words. Infants of all age groups detected vowel changes (CVCC vs. CVCVCV) in vowel-devoicing contexts and only 18- month infants detected consonant changes (CVC vs. CVCC). The results indicate that native phonotactic constraints have a large effect on adult perception but a small effect on infant perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-200"
  },
  "green04_interspeech": {
   "authors": [
    [
     "Phil",
     "Green"
    ],
    [
     "James",
     "Carmichael"
    ]
   ],
   "title": "Revisiting dysarthria assessment intelligibility metrics",
   "original": "i04_0485",
   "page_count": 4,
   "order": 202,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "This study reports on the development of an automated isolated-word intelligibility metric system designed to improve the scoring consistency and reliability of the Frenchay Dysarthria Assessment Test (FDA). The proposed intelligibility measurements are based on the probabilistic likelihood scores derived from the forced alignment of the dysarthric speech to whole-word hidden Markov models (HMMs) trained on data from a variety of normal speakers. The hypothesis is that these probability scores are correlated to the decoding effort made by naive listeners when trying to comprehend dysarthric utterances. Initial results indicate that the scores returned from these composite measurements provide a more fine-grained assessment of a given dysarthric individual's oral communicative competence when compared with traditional \"right-or-wrong\" scoring of expert listeners' transcriptions of dysarthric speech samples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-201"
  },
  "ciocca04_interspeech": {
   "authors": [
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Tara L.",
     "Whitehill"
    ],
    [
     "Joan K.-Y.",
     "Ma"
    ]
   ],
   "title": "The effect of intonation on perception of Cantonese lexical tones",
   "original": "i04_0489",
   "page_count": 4,
   "order": 203,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "This study was to investigate the perception of intonation-induced changes in tone in Cantonese. Previous studies have showed that the F0 level and contour of tones are likely to be modified by different intonations. As perception of tones depends primarily on the fundamental frequency (F0) pattern, it is likely that these intonation-induced changes in F0 will affect listeners' perception. In the present study, speech materials of two different intonations (question and statement) with six tonal contrasts placed at two different positions (initial and final) were presented to ten listeners. The listeners were asked to identify the tone of the target word embedded in the stimuli. Results showed that tones overall were accurately perceived, except at the final position of questions, where tones 33, 21, 23 and 22 were confused with tone 25. The perceptual patterns of individual listeners were also analysed, with differences in tone identification strategies identified across listeners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-202"
  },
  "iseijaakkola04_interspeech": {
   "authors": [
    [
     "Toshiko",
     "Isei-Jaakkola"
    ]
   ],
   "title": "Maximum short quantity in Japanese and finish in two perception tests with F0 and db variants",
   "original": "i04_0493",
   "page_count": 4,
   "order": 204,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "Maximum short segments of quantity in 120 discrimination tests at the word level were compared between two- and three-choice alternatives and with production test results utilising the same syllable structures. The materials in the perception test were eight kinds of bisyllabic synthetic nonsense words with five different prosodic patterns. These structures were used in the production test. 28 Finnish and Japanese subjects participated in the two perception tests and three speakers of each language in the production test. The results showed that (1) the short vowels/consonants in the two perception tests were longer than in the production tests; (2) the word-structural differences had more effect than the prosodic conditional differences in both Finnish and Japanese in both tests; (3) the short segmental durations were shorter in Finnish in the three choices than in the two choices but the Japanese counterparts were the reverse; (4)there was a correlation in some cases between perception and production.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-203"
  },
  "alku04_interspeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Matti",
     "Airas"
    ],
    [
     "Brad",
     "Story"
    ]
   ],
   "title": "Evaluation of an inverse filtering technique using physical modeling of voice production",
   "original": "i04_0497",
   "page_count": 4,
   "order": 205,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "Glottal flows and sound pressure waveforms of four different fundamental frequencies were generated using a computational model of vocal fold vibration and acoustic wave propagation in order to evaluate the performance of an inverse filtering method. Four time-based parameters of the glottal flow were used in order to assess the accuracy of the inverse filtering technique. The results show that for most of the cases analyzed the relative error was less than 5 % when the time-based parameters extracted from the estimated glottal flows were compared to those obtained from the original flow waveforms produced by the physical model of the vocal fold vibration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-204"
  },
  "hsu04_interspeech": {
   "authors": [
    [
     "Hui-ju",
     "Hsu"
    ],
    [
     "Janice",
     "Fon"
    ]
   ],
   "title": "Positional and phonotactic effects on the realization of taiwan Mandarin tone 2",
   "original": "i04_0501",
   "page_count": 4,
   "order": 206,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "This study investigates how phonotactics and position affect the realization of Guoyu T2. 10 speakers read sentences containing syllables of different types in isolation and in various sentential positions. Results showed that N-initial syllables had relatively high onsets, turning points, and offsets while G-initial syllables had low values for all three. V-initial syllables had high turning points and offsets while obstruent-initial syllables had low offsets. In general, offsets were higher than onsets when syllables were read in isolation or sentence-finally. However, in sentence-initial and -medial positions, onsets were be higher in pitch than offsets. V-initial syllables had higher offsets than onsets regardless of positions. In terms of duration, the falling ratio was the highest sentence-medially, especially with L-initial and N-initial syllables. There was no difference in the degree of steepness between the falling and the rising portions of the tone, but female speakers in general showed steeper slopes than male.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-205"
  },
  "schnell04_interspeech": {
   "authors": [
    [
     "Karl",
     "Schnell"
    ],
    [
     "Arild",
     "Lacroix"
    ]
   ],
   "title": "Speech production based on lossy tube models: unit concatenation and sound transitions",
   "original": "i04_0505",
   "page_count": 4,
   "order": 207,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "Discrete time tube models describe the propagation of plane sound waves through the vocal tract. Therefore they are important for speech analysis and production. The standard lossless tube model is extended by introducing distributed frequency dependent losses. In comparison to a previous investigation it is shown how the lossy vocal tract model can be used for speech production. The vocal tract areas of the lossy model are estimated from speech signals by an optimization algorithm. With the aid of the obtained parameters resynthesis can be successfully performed. For a synthesis of new utterances analyzed diphones are concatenated by linear transitions of the vocal tract areas. Furthermore for a generation of sound transitions also a nonlinear area transition is discussed considering horizontal area movements and a component of a neutral vocal tract configuration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-206"
  },
  "yan04_interspeech": {
   "authors": [
    [
     "Qin",
     "Yan"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Dimitrios",
     "Rentzos"
    ],
    [
     "Ching-Hsiang",
     "Ho"
    ]
   ],
   "title": "Modelling and ranking of differences across formants of british, australian and american accents",
   "original": "i04_0509",
   "page_count": 4,
   "order": 208,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "The differences between formants of British, Australian and American English accents are analysed and ranked. An improved formant model based on linear prediction (LP) feature analysis and a two-dimensional(2D) hidden Markov model (HMM) of formants is employed for estimation of the formant frequencies and bandwidths of vowels and diphthongs. Comparative analysis of the formant trajectories, the formant target points and the bandwidth of the spectral resonance at formants of British, Australian and American accents are presented. British vowels and diphthongs have smaller formant bandwidth than Australian. A method for ranking the contribution of different formants in conveying an accent is proposed whereby formants are ranked according to the normalized distances between the formants across accents. The first two formants are considered more sensitive to accents than other formants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-207"
  },
  "kitamura04_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Satoru",
     "Fujita"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Hironori",
     "Nishimoto"
    ]
   ],
   "title": "An experimental method for measuring transfer functions of acoustic tubes",
   "original": "i04_0513",
   "page_count": 4,
   "order": 209,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "This work proposes an experimental method for direct measurement of transfer functions of acoustic tubes. The method obtains a pressure-to-velocity transfer function from measurement of input volume velocity and output pressures of a target tube. Steady sinusoidal waves from 100 Hz to 5 kHz with a 10-Hz increment were used as a source signal. Experimental results compared with transmission line simulations indicate the following: (1) transfer functions obtained from the measurements agree well with those from transmission line simulations; (2) differences between the resonant frequencies obtained from the measurements and simulations with a uniform tube are less than 2.6%. These results show conclusive evidence that the proposed method permits accurate measurements of transfer functions of acoustic tubes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-208"
  },
  "tsuji04_interspeech": {
   "authors": [
    [
     "Takuya",
     "Tsuji"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Kohei",
     "Wakamiya"
    ],
    [
     "Jiji",
     "Kim"
    ]
   ],
   "title": "Estimation of the vocal tract spectrum from articulatory movements using phoneme-dependent neural networks",
   "original": "i04_0517",
   "page_count": 4,
   "order": 210,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "This paper presents an estimation method of the vocal tract spectrum from articulatory movements. The method is based on the interpolation of spectra obtained by phoneme-dependent neural networks. Given the phonemic context and articulation timing corresponding to each phoneme, the proposed method first transforms articulator positions to phoneme-dependent spectra. Then the vocal tract spectrum is estimated by the interpolation of transformed spectra. This interpolation is based on the distance among the input articulator position and that of the preceding and succeeding phonemes. Also, a training procedure of the neural networks is presented while taking the spectral interpolation into account. Articulatory and acoustic data pairs collected by a simultaneous recording of articulator positions and speech were used as the training and test data. Finally, we showed an estimation result using the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-209"
  },
  "motoki04_interspeech": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Hiroki",
     "Matsuzaki"
    ]
   ],
   "title": "Computation of the acoustic characteristics of vocal-tract models with geometrical perturbation",
   "original": "i04_0521",
   "page_count": 4,
   "order": 211,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "This paper presents computational results of sound pressure distributions and transfer characteristics for a large number of vocal-tract models which contain geometrical perturbations. A small change of the vocal-tract shape is regarded as a geometrical perturbation of the axis position of each vocal-tract section. Computation of the acoustic field in the models are performed using higher-order modes. The results indicate that acoustic characteristics in the higher frequencies are highly sensitive to the small change of the vocal-tract shape.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-210"
  },
  "vijayalakshmi04_interspeech": {
   "authors": [
    [
     "P.",
     "Vijayalakshmi"
    ],
    [
     "M. Ramasubba",
     "Reddy"
    ]
   ],
   "title": "Analysis of hypernasality by synthesis",
   "original": "i04_0525",
   "page_count": 4,
   "order": 212,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "Speakers with velopharyngeal incompetence, produce hypernasal speech across voiced elements. During the production of vowels, due to the defective velopharyngeal mechanism, oral cavity gets coupled with the nasal cavity. This coupling may introduce resonances of nasal cavity in the resulting vowel sound. In the present study, an attempt is made to convert pure vowels in to its nasalized counterpart, by introducing nasal resonances in full/part, in addition to the oral tract resonances. Further, an analysis is done by varying the formant bandwidths of all the resonances. By considering nasalized vowel of normal speakers as reference, a perceptual experiment is conducted and validated. The speech data of 10 patients with cleft palate, who are expected to produce hypernasal speech were analyzed. The presence of nasal formants in their Linear Prediction spectra validated the above results. Presence of nasal formants in the hypernasal speech, may be a cue for detection of hypernasality. Another experiment is also conducted in the time domain, to verify whether the widening of formant bandwidth is due to the distribution of energy between oral and nasal cavities.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-211"
  },
  "kacha04_interspeech": {
   "authors": [
    [
     "Abdellah",
     "Kacha"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Frédéric",
     "Bettens"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Adaptive long-term predictive analysis of disordered speech",
   "original": "i04_0529",
   "page_count": 4,
   "order": 213,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "A time-varying long-term bi-directional linear predictive analysis is proposed in the context of the acoustic assessment of disordered speech. It is shown that performing a forward and backward longterm linear prediction of each speech sample and retaining the minimal overall prediction error as a cue of vocal dysperiodicity obtains a signal-to-noise ratio that correlates with the perceived degree of hoarseness. The coefficients of the time-varying long-term linear predictive model are estimated adaptively by means of a recursive least squares algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-212"
  },
  "jovicic04_interspeech": {
   "authors": [
    [
     "Slobodan",
     "Jovicic"
    ],
    [
     "Sandra",
     "Antesevic"
    ],
    [
     "Zoran",
     "Saric"
    ]
   ],
   "title": "Phoneme restoration in degraded speech communication",
   "original": "i04_0533",
   "page_count": 4,
   "order": 214,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "This paper presents the experimental results of investigation in a research on the phoneme restoration effects for the case of degraded speech communication. The first experiment examined the role of coarticulation in consonant restoration inside CV syllables. In the second experiment the phonological effects in initial consonant restoration around the words were demonstrated. The third experiment demonstrated consonant restoration effects inside the sentences either grammatically and semantically correct or incorrect. Finally, the fourth experiment showed an integral effects of coarticulation, phonology, syntax and semantics on phoneme restoration in spontaneous speech communication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-213"
  },
  "marinaki04_interspeech": {
   "authors": [
    [
     "Maria",
     "Marinaki"
    ],
    [
     "Constantine",
     "Kotropoulos"
    ],
    [
     "Ioannis",
     "Pitas"
    ],
    [
     "Nikolaos",
     "Maglaveras"
    ]
   ],
   "title": "Automatic detection of vocal fold paralysis and edema",
   "original": "i04_0537",
   "page_count": 4,
   "order": 215,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "In this paper we propose a combined scheme of linear prediction analysis for feature extraction along with linear projection methods for feature reduction followed by known pattern recognition methods on the purpose of discriminating between normal and pathological voice samples. Two different cases of speech under vocal fold pathology are examined: vocal fold paralysis and vocal fold edema. Three known classifiers are tested and compared in both cases, namely the Fisher linear discriminant, the K-nearest neighbor classifier, and the nearest mean classifier. The performance of each classifier is evaluated in terms of the probabilities of false alarm and detection or the receiver operating characteristic. The datasets used are part of a database of disordered speech developed by Massachusetts Eye and Ear Infirmary. The experimental results indicate that vocal fold paralysis and edema can easily be detected by any of the aforementioned classifiers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-214"
  },
  "minami04_interspeech": {
   "authors": [
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Erik",
     "McDermott"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "A theoretical analysis of speech recognition based on feature trajectory models",
   "original": "i04_0549",
   "page_count": 4,
   "order": 216,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "In previous work, we proposed a new speech recognition technique that generates a smooth speech trajectory from hidden Markov models (HMMs) by maximizing likelihood subject to the constraints that exist between static and dynamic speech features. This paper presents a theoretical analysis of this method. We show that the approach used to generate the smoothed trajectory is equivalent to a Kalman filter. This result demonstrates that there is a strong relationship between the dynamics of delta features (and delta-delta features) in HMM-based speech recognition and Kalman filter dynamics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-215"
  },
  "ou04_interspeech": {
   "authors": [
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Wang",
     "Zuoying"
    ]
   ],
   "title": "Discriminative combination of multiple linear predictions for speech recognition",
   "original": "i04_0553",
   "page_count": 4,
   "order": 217,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "In this paper, new analyses are provided for the problems of applying linear prediction (LP) HMMs in speech recognition. It is shown that, apart from simply aggregating all predictors in one LP, which produces inconsistent results, 'combination' provides another useful way to implement complex dependencies. A method by discriminative combination of multiple LPs (DCoLP) is proposed, with a component-LP selection heuristic. The resulting DCoLP model was tested on a speaker-independent, large vocabulary continuous speech recognition task, and showed improved performance over the standard HMM with comparable computation cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-216"
  },
  "gharavian04_interspeech": {
   "authors": [
    [
     "Davood",
     "Gharavian"
    ],
    [
     "Mohammad",
     "Ahadi"
    ]
   ],
   "title": "Use of formants in stressed and unstressed continuous speech recognition",
   "original": "i04_0557",
   "page_count": 4,
   "order": 218,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "Stress plays a crucial role in the understanding of speech by human listeners. However, automatic speech recognition results deteriorate in the presence of stress due to the change it causes in the speech parameters. Meanwhile, due to the vast diversity of the presence of stress in speech, a speech corpus that contains the majority of different stress conditions is difficult to obtain in real world. Therefore, other ways to improve stressed speech recognition performance have to be taken into account. In previous works, we have evaluated the effects of stress on several speech parameters such as phone durations, pitch and formant frequencies. In this paper, the use of formants in stressed speech recognition will be discussed. We have found that formants and their dynamics (slopes) are useful in improving speech recognition rates both in stressed and unstressed conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-217"
  },
  "markov04_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Integration of articulatory dynamic parameters in HMM/BN based speech recognition system",
   "original": "i04_0561",
   "page_count": 4,
   "order": 219,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "In this paper, we describe several approaches to integration of the articulatory dynamic parameters along with articulatory position data into HMM/BN model based automatic speech recognition system. This work is a continuation of our previous study, where we have successfully combined speech acoustic features in form of MFCC with articulatory position observations. Articulatory dynamic parameters are represented by velocity and acceleration coefficients. All these features are integrated using the HMM/BN acoustic model where each feature corresponds to different Bayesian Network variable. By changing the BN topology we can change the way articulatory and acoustic parameters are combined. The evaluation experiments showed that the effect of the articulatory dynamic features greatly depends on the BN structure and that careful data analysis is essential in gaining knowledge about the data dependencies. In comparison with conventional HMM system trained on acoustic data only, the HMM/BN system achieved significant improvement of the recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-218"
  },
  "alsteris04_interspeech": {
   "authors": [
    [
     "Leigh David",
     "Alsteris"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "ASR on speech reconstructed from short-time fourier phase spectra",
   "original": "i04_0565",
   "page_count": 4,
   "order": 220,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "In our earlier papers, we have measured human intelligibility of speech stimuli reconstructed either from the short-time magnitude spectra (magnitude-only stimuli) or the short-time phase spectra (phase-only stimuli) of a speech stimulus. We demonstrated that, even for small analysis window durations of 20-40 ms (of relevance to automatic speech recognition), the short-time phase spectrum can contribute to speech intelligibility as much as the short-time magnitude spectrum. In this paper, we perform automatic speech recognition on magnitude-only and phase-only stimuli. When employing an MFCC-based front-end, the recognition achieved for these phase-only stimuli is much worse than magnitude-only stimuli at small analysis window durations, which is not consistent with their corresponding human intelligibility results. This implies that the MFCC feature set is not capturing all of the discriminating information present in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-219"
  },
  "lieb04_interspeech": {
   "authors": [
    [
     "Robert",
     "Lieb"
    ],
    [
     "Tibor",
     "Fabian"
    ],
    [
     "Guenther",
     "Ruske"
    ],
    [
     "Matthias",
     "Thomae"
    ]
   ],
   "title": "Estimation of semantic confidences on lattice hierarchies",
   "original": "i04_0569",
   "page_count": 4,
   "order": 221,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "Inspired by the well-known method for confidence measure calculation via estimation of word posterior probabilities on the word graph, we devised a technique to estimate confidences on all levels of the hierarchically structured output of our one-stage decoder for interpretation of natural speech (ODINS). By constructing a nested lattice hierarchy, the generalized counterpart of the word graph, we estimate posterior probabilities for all nodes in the decoded semantic tree, namely for all contained semantic units and words. The obtained experimental results show that the tree node confidence measure performs significantly better than the confidence error base line, no matter if the evaluation is carried out on tree nodes representing semantic concepts, word classes, or words. Furthermore, the paper proposes possible applications of the tree node confidences to improve the grounding strategy of spoken dialog systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-220"
  },
  "fukumoto04_interspeech": {
   "authors": [
    [
     "Fumiyo",
     "Fukumoto"
    ],
    [
     "Yoshimi",
     "Suzuki"
    ]
   ],
   "title": "Learning subject drift for topic tracking",
   "original": "i04_0573",
   "page_count": 4,
   "order": 222,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "For topic tracking where data is collected over an extended period of time, the discussion of a topic, i.e. the subject in a story changes over time. This paper focuses on subject drift and presents a method for topic tracking on broadcast news stories to handle subject drift. The basic idea is to automatically extract the optimal positive training data of the target topic so as to include only the data which are sufficiently related to the current subject. The method was tested on the TDT1 and TDT2, and the results show the effectiveness of the method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-221"
  },
  "shriberg04_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Dustin",
     "Hillard"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Barbara",
     "Peskin"
    ],
    [
     "Mary",
     "Harper"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "The ICSI-SRI-UW metadata extraction system",
   "original": "i04_0577",
   "page_count": 4,
   "order": 223,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "Both human and automatic processing of speech require recognizing more than just the words. We describe a state-of-the-art system for automatic detection of \"metadata\" (information beyond the words) in both broadcast news and spontaneous telephone conversations, developed as part of the DARPA EARS Rich Transcription program. System tasks include sentence boundary detection, filler word detection, and detection/correction of disfluencies. To achieve best performance, we combine information from different types of language models (based on words, part-of-speech classes, and automatically induced classes) with information from a prosodic classifier. The prosodic classifier employs bagging and ensemble approaches to better estimate posterior probabilities. We use confusion networks to improve robustness to speech recognition errors. Most recently, we have investigated a maximum entropy approach for the sentence boundary detection task, yielding a gain over our standard HMM approach. We report results for these techniques on the official NIST Rich Transcription metadata tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-222"
  },
  "hasegawajohnson04_interspeech": {
   "authors": [
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Stephen",
     "Levinson"
    ],
    [
     "Tong",
     "Zhang"
    ]
   ],
   "title": "Automatic detection of contrast for speech understanding",
   "original": "i04_0581",
   "page_count": 4,
   "order": 224,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "Contrast is a very popular phenomenon in spoken language, and carries very important information to help understanding contents and structures of spoken language. In this paper, we propose an idea of automatic contrast detection as an effort for better speech understanding. We study the automatic tagging of three specific types of contrast: symmetric contrast, contrastive focus, and contrastive topic. We label the three types of contrasted words as contrast (C), and other words as noncontrast (C). The classification of contrast events is based on prosodic, spectral, and part-of-speech (POS) information sources. The integration of different knowledge sources is realized by a time-delay recursive neural network (TDRNN). The approach we proposed was testified on 235 spontaneous utterances consisting of 3500 words (samples). The contrast detection was speaker independent. The tests yielded an average of 87.9% classification rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-223"
  },
  "wang04f_interspeech": {
   "authors": [
    [
     "Nick Jui-Chang",
     "Wang"
    ],
    [
     "Jia-Lin",
     "Shen"
    ],
    [
     "Ching-Ho",
     "Tsai"
    ]
   ],
   "title": "Integrating layer concept inform ation into n-gram modeling for spoken language understanding",
   "original": "i04_0585",
   "page_count": 4,
   "order": 225,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "The paper presents a novel approach, integrating layer concept information into the trigram language model, to improve the understanding accuracy for spoken dialogue systems. With this approach, both the recognition accuracy and out-of-grammar problem can be largely improved. The concept error rate is therefore reduced. In the experiment using a real-world air-ticket information spoken dialogue system for Mandarin Chinese, a relative concept error rate reduction of 30% is achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-224"
  },
  "chen04_interspeech": {
   "authors": [
    [
     "Junyan",
     "Chen"
    ],
    [
     "Ji",
     "Wu"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "A robust understanding model for spoken dialogues",
   "original": "i04_0589",
   "page_count": 4,
   "order": 226,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "This paper presents a robust model for language understanding in spoken dialogue systems, in which the understanding problem is formulated as a three-stage process. In the first stage, semantic concepts included in the utterance are identified through a bottom-up chart parser to build a concept graph. Then, in the second stage, communicative goal of each candidate path searched from the concept graph is determined by a classifier based on latent semantic analysis. Finally, information of discourse history is introduced to guide the search in all hypotheses for the best result. Experiments with a test set composed of spontaneous utterances show that this model outperforms a rule-based model by 8.1% and 21.5% in goal and concept understanding respectively, which proves its advantage to robust spontaneous language understanding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-225"
  },
  "wutiwiwatchai04_interspeech": {
   "authors": [
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Belief-based nonlinear rescoring in Thai speech understanding",
   "original": "i04_2129",
   "page_count": 4,
   "order": 227,
   "p1": "2129",
   "pn": "2133",
   "abstract": [
    "This paper proposes an approach to improve speech understanding based on rescoring of N-best semantic hypotheses. In rescoring, probabilities produced by an understanding component are combined with additional probabilities derived from system beliefs. While a normal rescoring approach is to multiply or linearly interpolate with belief probabilities, this paper shows that probabilities from various sources are better combined using a nonlinear estimator. Using the proposed model together with a dialogue-state dependent semantic model shows a significant improvement when applying to a Thai interactive hotel reservation agent (TIRA), the first spoken dialogue system in Thai language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-226"
  },
  "itoh04b_interspeech": {
   "authors": [
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Yukihiro",
     "Itoh"
    ],
    [
     "Tatsuhiro",
     "Konishi"
    ]
   ],
   "title": "An understanding strategy based on plausibility score in recognition history using CSR confidence measure",
   "original": "i04_2133",
   "page_count": 4,
   "order": 228,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "Although car-navigation systems attract attention as one of spoken dialogue interfaces, recognition errors due to the influence of natural speech and surrounding noise may prevent a smooth dialogue and disappoint the user. Thus, this research aims at the construction of a dialogue system which can achieve a smooth dialogue and a high degree of user satisfaction. Our system performs language understanding and response generation by using the confidence measure(CM) based on continuous speech recognizer (CSR) and the recognition history. This paper shows the spoken language understanding technique in the dialogue system. The CM, together with the speech type and the recognition history, is used for generating an integrated score. The system realizes a spoken language understanding which is more plausible for a given dialogue. As the result of evaluation experiment, it was shown that our system is more efficient (more than 15%) than a language understanding technique which simply gives priority to the higher-rank hypothesis of a speech recognition result (n-best).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-227"
  },
  "jung04_interspeech": {
   "authors": [
    [
     "Sangkeun",
     "Jung"
    ],
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Speech recognition error correction using maximum entropy language model",
   "original": "i04_2137",
   "page_count": 4,
   "order": 229,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "A speech interface is often required in many application environments, such as telephone-based information retrieval, car navigation systems, and user-friendly interfaces, but the low speech recognition rate makes it difficult to extend its application to new fields. We propose a domain adaptation technique via error correction with a maximum entropy language model, which is a general and elegant framework to combine higher level linguistic knowledge. Our approach has the ability to correct both semantic and lexical errors in 1-best output from the black-box style speech recognizer, and can improve the performance of speech recognition and application system. Through extensive experiments using a speech-driven in-vehicle telematics information retrieval and spoken language understanding, we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented error correction approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-228"
  },
  "li04_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Juan",
     "Huerta"
    ]
   ],
   "title": "Discriminative training of compound-word based multinomial classifiers for speech routing",
   "original": "i04_2141",
   "page_count": 4,
   "order": 230,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "We describe a method for utterance classification for speech routing based on a discriminative training multinomial topic classifier. We propose the utilization of the n-norm a posteriori topic probability of the speech hypothesis as the objective function of a discriminative training step, and explore various ways to compute and maximize this function with respect to model parameters. To avoid obtaining negative probability estimates, we propose an alternative representation of the model parameterization. We utilize our approach in combination with a simple non-discriminative word detection algorithm based on Mutual Information and with a technique to identify the most salient phrases of the domain in order to alleviate the feature sparsity problem. We also explore the post-processing of classification results to further improve the classification performance via a decision tree model and a neural network. Overall, our discriminative trained multinomial system reduces the classification error rate up to 45% in an NLU task of financial transactions comparative to our baseline non-discriminative multinomial system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-229"
  },
  "eun04_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Eun"
    ],
    [
     "Changki",
     "Lee"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "An information extraction approach for spoken language understanding",
   "original": "i04_2145",
   "page_count": 4,
   "order": 231,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "This paper presents an Information Extraction (IE) approach for spoken language understanding. The goal in IE is to find proper values for pre-defined slots of given templates. IE for spoken language understanding proposes a concept spotting approach for spoken language because IE approach is interested in only predefined concept slots. In spite of this partial understanding, we can acquire necessary information for an application from the values of pre-defined slots because the slots are properly designed for speech understanding in a specific domain. Spoken language has so many recognition errors especially in a poor environment so it is more difficult to understand than textual language. Considering this fact, we attempt to understand the languages by concentrating on the specified information. In experiments on the car navigation domain, F-measure for concept spotting for textual input (WER 0%) and spoken input (WER 39%) are 96.33% and 78.30% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-230"
  },
  "horowitz04_interspeech": {
   "authors": [
    [
     "David",
     "Horowitz"
    ],
    [
     "Partha",
     "Lal"
    ],
    [
     "Pierce Gerard",
     "Buckley"
    ]
   ],
   "title": "A maximum entropy shallow functional parser for spoken language understanding",
   "original": "i04_2149",
   "page_count": 4,
   "order": 232,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "In this paper we investigate a maximum entropy approach to spoken language under standing. We compare this approach with a parser based on finite-state transducers. The parsers are evaluated on a corpus of utterances modelling human-computer interactions within a single domain. The corpus was annotated with task-oriented semantic categories to obtain a set of shallow functional parse trees. We focus our investigation on the quality of the structured concepts produced by each approach. A direct comparison shows that the maximum entropy parser achieves better performance than the finite-state parser, even with very limited training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-231"
  },
  "huang04b_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huang"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Mixture language models for call routing",
   "original": "i04_2153",
   "page_count": 4,
   "order": 233,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "Our goal is to extract information from a telephone call in order to route the call to one of a number of destinations. We assume that we do not know \"a priori\" the vocabulary used in the application and so we use phonetic recognition followed by identification of salient phone sequences. In previous work, we showed that using a separate language model during recognition for each route gave improved performance over using a single model. However, this technique decodes each utterance in terms of the salient sequences of each call route, which leads to insertion and substitution errors that degrade performance. In this paper, we introduce the use of mixture language models for speech recognition in the context of call route classification. The benefit of technique can has the efficiency of multiple language models to get accurate recognition on salient phoneme sequences; on the other hand, it can give help in classification, even if the size of some call routes have just 50~60 utterances. It avoids building HMMs for some salient phoneme sequences to decide whether it is correct of occurring in the utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-232"
  },
  "wu04_interspeech": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Jui-Feng",
     "Yeh"
    ],
    [
     "Ming-Jun",
     "Chen"
    ]
   ],
   "title": "Speech act identification using an ontology-based partial pattern tree",
   "original": "i04_2157",
   "page_count": 4,
   "order": 234,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "This paper presents an ontology-based partial pattern tree to identify the speech act in a spoken dialogue system. This study first extracts the key concepts in an application domain using latent semantic analysis. A partial pattern tree is used to deal with the ill-formed sentence problem in a spoken dialogue system. Concept expansion based on domain ontology is adopted to improve system performance. For performance evaluation, a medical dialogue system with multiple services, including registration information, clinic information and FAQ information, is implemented. Four performance measures were separately used for evaluation. The speech act identification rate achieves 86.2%. A Task Success Rate of 77% is obtained. The contextual appropriateness of the system response is 78.5%. Finally, the correct rate for FAQ retrieval is 82% with an improvement of 15% in comparison with the keyword-based vector space model. The results show the proposed ontology-based partial pattern tree is effective for dialogue management.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-233"
  },
  "wang04g_interspeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ]
   ],
   "title": "Creating speech recognition grammars from regular expressions for alphanumeric concepts",
   "original": "i04_2161",
   "page_count": 4,
   "order": 235,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "We propose a novel approach that enables the developers with little grammar authoring experience to construct high performance speech grammars for alphanumeric concepts, which are often needed in the more commonly used directed dialog systems in practice. A developer can simply write down a regular expression for the concept and the algorithm automatically constructs a W3C grammar with appropriate semantic interpretation tags. While the quality of the grammar is ultimately determined by the way in which the regular expression is written, the algorithm relieves the developers from the difficult tasks of optimizing grammar structures and assigning appropriate semantic interpretation tags, thus it greatly speeds up grammar development and reduces the requirement of expertise. Preliminary experimental results have shown that the grammar created with this approach consistently out-performed the general alphanumeric rules in the grammar library. In some cases the semantic error rates were cut by more than 50%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-234"
  },
  "trancoso04_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Paulo",
     "Araujo"
    ],
    [
     "Ceu",
     "Viana"
    ],
    [
     "Nuno",
     "Mamede"
    ]
   ],
   "title": "Poetry assistant",
   "original": "i04_2165",
   "page_count": 4,
   "order": 236,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "The ultimate goal of the poetry assistant currently under development in our lab is an application to be used either as a poetry game or as a teaching tool for both poetry and grammar, including the complex relationships between sound and meaning. Until now we focused on the automatic classification of poems and the suggestion of the ending word for a verse. The classification module is based on poetic concepts that take into account structure and metrics. The prediction module uses several criteria to select the ending word: the structural constraints of the poem, the grammatical category of the words, and the statistical language models obtained from a text corpus. The first version of the system, rather than being self-contained, is still based on the use of different heterogeneous modules. We are currently working on a second version based on a modular architecture that facilitates the reuse of the linguistic processing modules already developed within the lab.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-235"
  },
  "kitade04_interspeech": {
   "authors": [
    [
     "Tasuku",
     "Kitade"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ]
   ],
   "title": "Automatic extraction of key sentences from oral presentations using statistical measure based on discourse markers",
   "original": "i04_2169",
   "page_count": 4,
   "order": 237,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "Automatic extraction of key sentences from academic presentation speeches is addressed. The method makes use of the characteristic expressions used in initial utterances of sections, which are defined as discourse markers and derived in a totally unsupervised manner based on word statistics. The statistics of the discourse markers are then used to define the importance of the sentences. It is also combined with the conventional tf-idf measure of content words. Comprehensive evaluation using the Corpus of Spontaneous Japanese and a variety of experimental setups is presented in this paper. We carefully designed the evaluation scheme to be compared to human performance. The proposed method using the discourse markers shows consistent effectiveness in the key sentence extraction. Based on the indexing, we realize efficient browsing of lecture audio archives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-236"
  },
  "ohno04_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Ohno"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Robust dependency parsing of spontaneous Japanese speech and its evaluation",
   "original": "i04_2173",
   "page_count": 4,
   "order": 238,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "Spontaneously spoken Japanese includes a lot of grammatically ill-formed linguistic phenomena such as fillers, hesitations, inversions, and so on, which do not appear in written language. This paper proposes a method of robust dependency parsing using a large-scale spoken language corpus, and evaluates the availability and robustness of the method using spontaneously spoken dialogue sentences. By utilizing stochastic information about the appearance of ill-formed phenomena, the method can robustly parse spoken Japanese including fillers, inversions, or dependencies over utterance units. As a result of an experiment, the parsing accuracy provided 87.0%, and we confirmed that it is effective to utilize the location information of a bunsetsu, and the distance information between bunsetsus as stochastic information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-237"
  },
  "minker04_interspeech": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ],
    [
     "Dirk",
     "Buehler"
    ],
    [
     "Christiane",
     "Beuschel"
    ]
   ],
   "title": "Strategies for optimizing a stochastic spoken natural language parser",
   "original": "i04_2177",
   "page_count": 4,
   "order": 239,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "Strategies to enhance the semantic decoding accuracy of a stochastic parser are discussed and comparatively evaluated on the English Spontaneous Speech Task for multilingual appointment scheduling. The performance of the parser could be improved by subsequently adding valuable and removing redundant semantic information as well as by iteratively combining several decoding methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-238"
  },
  "lee04i_interspeech": {
   "authors": [
    [
     "Tzu-Lun",
     "Lee"
    ],
    [
     "Ya-Fang",
     "He"
    ],
    [
     "Yun-Ju",
     "Huang"
    ],
    [
     "Shu-Chuan",
     "Tseng"
    ],
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "Prolongation in spontaneous Mandarin",
   "original": "i04_2181",
   "page_count": 4,
   "order": 240,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "This paper presents a corpus-based study on prolongations in spontaneous Mandarin. Prolongations are mainly produced for hesitation, but also for emphasizing a discourse focus and signalling an explicit feedback. 786 prolongation occurrences are investigated in terms of the position, the part of speech and the segment and tone types. Prolongations are often found in word-final, phrase-final and utterance-medial positions. It is more likely to prolong in function words than in content words. However, in the case of monosyllabic words prolongations are more frequently found in function words, but in the remaining cases prolongations are more likely to be found in content words. Prolongations in transitive verbs, adverbs, nouns and particles show particularly high rates, while prolongations in intransitive verbs and aspectual adverbs are really rare. Especially, there is no prolonged adjective. Consonants are rarely prolonged in Mandarin and no particular effect is found for lexical tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-239"
  },
  "irie04_interspeech": {
   "authors": [
    [
     "Yuki",
     "Irie"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Yukiko",
     "Yamaguchi"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Speech intention understanding based on decision tree learning",
   "original": "i04_2185",
   "page_count": 4,
   "order": 241,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "This paper proposes a method of speech intention understanding based on a spoken dialogue corpus to which the intention tags are given. The intention tag expresses the task-dependent intention of the speaker, and therefore, the proper understanding enables a spoken dialogue system to take appropriate actions. We have tagged about 35000 utterances in the CIAIR in-car speech database. In our method, several decision trees for intention understanding are constructed. By constructing decision trees and using them at the same time, the strong amount of characteristic features related to intentions can be retrieved, and it can also be robustly coped with the diversity of the utterances. An experiment on inference of utterance intentions has shown 73.1% accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-240"
  },
  "banerjee04_interspeech": {
   "authors": [
    [
     "Satanjeev",
     "Banerjee"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "Using simple speech-based features to detect the state of a meeting and the roles of the meeting participants",
   "original": "i04_2189",
   "page_count": 4,
   "order": 242,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "We introduce a simple taxonomy of meeting states and participant roles. Our goal is to automatically detect the state of a meeting and the role of each meeting participant and to do so concurrent with a meeting. We trained a decision tree classifier that learns to detect these states and roles from simple speech--based features that are easy to compute automatically. This classifier detects meeting states 18% absolute more accurately than a random classifier, and detects participant roles 10% absolute more accurately than a majority classifier. The results imply that simple, easy to compute features can be used for this purpose.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-241"
  },
  "yildirim04_interspeech": {
   "authors": [
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Chul Min",
     "Lee"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Zhigang",
     "Deng"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "An acoustic study of emotions expressed in speech",
   "original": "i04_2193",
   "page_count": 4,
   "order": 243,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "In this study, we investigate acoustic properties of speech associated with four different emotions (sadness, anger, happiness, and neutral) intentionally expressed in speech by an actress. The aim is to obtain detailed acoustic knowledge on how speech is modulated when speaker's emotion changes from neutral to a certain emotional state. It is based on measurements of acoustic parameters related to speech prosody, vowel articulation and spectral energy distribution. Acoustic similarities and differences among the emotions are then explored with mutual information computation, multidimensional scaling, and comparison of acoustic likelihoods relative to the neutral emotion. In addition, acoustic separability of the emotions is tested using the discriminant analysis at the utterance level and the result is compared with human evaluation. Results show that happiness/anger and neutral/sadness share similar acoustic properties in this speaker. Speech associated with anger and happiness are characterized by longer utterance duration, shorter inter-word silence, higher pitch and energy values with wider ranges, showing the characteristics of exaggerated or hyperarticulated speech. The discriminant analysis indicates that within-group acoustic separability is relatively poor, suggesting that conventional acoustic parameters examined in this study are not effective in describing the emotions along the valance (or pleasure) dimension. It is noted that RMS energy, inter-word silence and speaking rate are useful in distinguishing sadness from others. Interestingly, the between-group difference in formant patterns seems better reflected in back vowels such as /a/ (/father/) than in the front vowels. Larger lip opening and/or more tongue constriction at the mid or rear part of the vocal tract could be underlying reasons.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-242"
  },
  "kawahara04c_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Ian Richard",
     "Lane"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Topic classification and verification modeling for out-of-domain utterance detection",
   "original": "i04_2197",
   "page_count": 4,
   "order": 244,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "The detection and handling of OOD (out-of-domain) user utterances are significant problems for spoken language systems. We approach these problems by applying an OOD detection framework, combining topic classification and in-domain verification. In this paper, we compare the performance of three topic classification modeling schemes: 1-vs-all, where a single classifier is trained for each topic; weighted 1-vs-all; and 1-vs-1, which combines multiple pair-wise classifiers. We also compare the performance of a linear discriminate verifier and nonlinear SVM-based verification. In an OOD detection task as a front-end for speech-to-speech translation, detection performance was comparable for all classification schemes, indicating that the simplest 1-vs-all approach is sufficient for this task. SVM-based in-domain verification was found to provide a significant reduction in detection errors compared to a linear discriminate model. However, when the training and testing scenarios differ, the SVM approach was not robust, while the linear discriminate model remained effective.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-243"
  },
  "park04b_interspeech": {
   "authors": [
    [
     "So-Young",
     "Park"
    ],
    [
     "Yong-Jae",
     "Kwak"
    ],
    [
     "Joon-Ho",
     "Lim"
    ],
    [
     "Hae-Chang",
     "Rim"
    ],
    [
     "Soo-Hong",
     "Kim"
    ]
   ],
   "title": "Partially lexicalized parsing model utilizing rich features",
   "original": "i04_2201",
   "page_count": 4,
   "order": 245,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "In this paper, we propose a partially lexicalized parsing model utilizing rich features to improve the parsing ability and reduce the parsing cost. In order to disambiguate parse trees effectively, it employs several useful features such as a syntactic label feature, a content feature, a functional feature, and a size feature. Besides, it is partially lexicalized so as to reduce the parsing cost closely connected with lexical information. Moreover, it is designed to be suitable for representing word order variation and constituent ellipsis in Korean sentences. Experimental results show that the proposed parsing model using more features performs better although it less depends on lexical information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-244"
  },
  "suzuki04b_interspeech": {
   "authors": [
    [
     "Yoshimi",
     "Suzuki"
    ],
    [
     "Fumiyo",
     "Fukumoto"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "Clustering similar nouns for selecting related news articles",
   "original": "i04_2205",
   "page_count": 4,
   "order": 246,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "In both written language and spoken language, we sometimes use differentwords in order to express the same thing. For instance, we sometimes use \"candidacy\" and \"running in an election\" as the same meaning. This causes that extracting semantically similar documents and event tracking are difficult. In order to solve the problem, we have to identify the words which are semantically similar to each other accurately. In this paper, we propose a method to extract the words which are semantically similar to each other accurately. Using the method, we extracted similar word pairs accurately on newspaper articles. We also extracted news articles which are related to a news article. Using similar word pair list by the method, we obtained better results than the results without similar word pair list. The results suggest that the method is useful for extracting related document, text tracking and so on.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-245"
  },
  "badino04_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "Chinese text word-segmentation considering semantic links among sentences",
   "original": "i04_2209",
   "page_count": 4,
   "order": 247,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "Segmentation of Chinese input text into words is a necessary step to realize a Mandarin Chinese text-to-speech. Several word-segmentation algorithms were developed in which linguistic information are combined with statistical ones or with heuristic rules. In this paper we investigate in the advantages that can arise when semantic relation among sentences is taken into account during the word segmentation process. The algorithm we purpose shows how this kind of semantic information could improve the performances of a word segmentation algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-246"
  },
  "lee04j_interspeech": {
   "authors": [
    [
     "Do-Gil",
     "Lee"
    ],
    [
     "Hae-Chang",
     "Rim"
    ]
   ],
   "title": "Syllable-based probabilistic morphological analysis model of Korean",
   "original": "i04_2213",
   "page_count": 4,
   "order": 248,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "In this paper, we present a syllable-based probabilistic morphological analysis model of Korean. While the previous morphological analyzers that regard morpheme as a processing unit, the model exploits syllable as a processing unit in order to endure the unknown word problem. Actually, it does not use any morpheme dictionary. In contract to the previous systems that depend on manually constructed linguistic knowledge, the proposed system can fully automatically acquire the linguistic knowledge from annotated corpora. Besides, without any modification, the system can be applied to other corpus having a different tagset and annotation guidelines. We describe the model and present experimental results on two corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-247"
  },
  "valente04_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Christian",
     "Wellekens"
    ]
   ],
   "title": "Scoring unknown speaker clustering : VB vs. BIC",
   "original": "i04_0593",
   "page_count": 4,
   "order": 249,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "This paper aims at comparing the Bayesian Information Criterion and the Variational Bayesian approach for scoring unknown multiple speaker clustering. Variational Bayesian learning is very effective method that allows parameter learning and model selection at the same time. The application we consider here consists in finding the optimal clustering in a conversation where the speaker number is not a priori known. Experiments are run on synthetic data and on the evaluation data set NIST-1996 HUB-4. VB learning achieves higher score in terms of average cluster purity and average speaker purity compared to ML/BIC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-248"
  },
  "jin04_interspeech": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Speaker segmentation and clustering in meetings",
   "original": "i04_0597",
   "page_count": 4,
   "order": 250,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "This paper describes the automatic speaker segmentation and clustering system for natural, multi-speaker meeting conversations based on multiple distant microphones. The system was evaluated in the NIST RT-04S Meeting Recognition Evaluation on the speaker diarization task and achieved speaker diarization performance of 28.17%. This system also aims to provide automatic speech segments and speaker grouping information for speech recognition, a necessary prerequisite for subsequent audio processing. A 44.5% word error rate was achieved for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-249"
  },
  "lamel04_interspeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Leonardo",
     "Canseco-Rodriguez"
    ]
   ],
   "title": "Speaker diarization from speech transcripts",
   "original": "i04_0601",
   "page_count": 4,
   "order": 251,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "The aim of this study is to investigate the use of the linguistic information present in the audio signal to structure broadcast news data, and in particular to associate speaker identities with audio segments. While speaker recognition has been an active area of research for many years, addressing the problem of identifying speakers in huge audio corpora is relatively recent and has been mainly concerned with speaker tracking. The speech transcriptions contain a wealth of linguistic information that is useful for speaker diarization. Patterns which can be used to identify the current, previous or next speaker have been developed based on the analysis of 150 hours of manually transcribed broadcast news data. Each pattern is associated with one or more rules. After validation on the training transcripts, these patterns and rules were tested on an independent data set containing transcripts of 10 hours of broadcasts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-250"
  },
  "miro04_interspeech": {
   "authors": [
    [
     "Xavier Anguera",
     "Miro"
    ],
    [
     "Javier Hernando",
     "Pericas"
    ]
   ],
   "title": "Evolutive speaker segmentation using a repository system",
   "original": "i04_0605",
   "page_count": 4,
   "order": 252,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "When performing blind speaker segmentation one of the main problems is not knowing how many speakers appear in a conversation and wether they appear once or more than once. In this paper, an iterative method, which is based on the Evolutive-HMM is presented. Two main improvements to this system are introduced. On one hand, a repository generic speaker is used to model all utterances and all speaker models are derived from this iteratively. Different normalization of the scores are applied to the repository and the speakers to emphasize speaker changes. On the other hand, in all cases we use Gaussian Mixture Models (GMM) for their flexibility compared to an HMM structure. This method has been successfully tested using multi-speaker speech sequences generated by concatenation of speech segments from Speecon.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-251"
  },
  "aronowitz04_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "David",
     "Burshtein"
    ],
    [
     "Amihood",
     "Amir"
    ]
   ],
   "title": "Speaker indexing in audio archives using test utterance Gaussian mixture modeling",
   "original": "i04_0609",
   "page_count": 4,
   "order": 253,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "Speaker Indexing has recently emerged as an important task due to the rapidly growing volume of audio archives. Current filtration techniques still suffer from problems both in accuracy and efficiency. The major reason for the drawbacks of existing solutions is the use of inaccurate anchor models. The contribution of this paper is two-fold. On the theoretical side, a new method is developed for simulating GMM scoring. This enables to fit a GMM not only to every target speaker but also to every test call, and then compute the likelihood of the test utterance using these GMMs instead of using the original data. The second, contribution of this paper is in harnessing this GMM simulation to achieve very efficient speaker indexing in terms of both search time and index size. Results on the SPIDRE corpus show that our approach maintains the accuracy of the conventional GMM algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-252"
  },
  "raux04b_interspeech": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ]
   ],
   "title": "Automated lexical adaptation and speaker clustering based on pronunciation habits for non-native speech recognition",
   "original": "i04_0613",
   "page_count": 4,
   "order": 254,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "This paper describes a method to improve speech recognition for non-native speech in a spoken dialogue system. Based on very general rules about possible vocalic substitutions, the frequency of occurrence of each substitution in different phonetic contexts is estimated on a small set of recordings. The most frequently observed substitutions are applied to the lexicon of the recognizer. Speakers in the training set are automatically clustered according to their preferred phonetic variants, and a specific lexicon is built for each cluster. Acoustic adaptation is also performed on each cluster. Experiments show that lexical adaptation provides a 16.4% relative WER reduction over acoustic adaptation alone. Lexical clustering can further reduce WER if the system can reliably select the cluster best matching each input utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-253"
  },
  "paliwal04_interspeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Stephen",
     "So"
    ]
   ],
   "title": "Scalable distributed speech recognition using multi-frame GMM-based block quantization",
   "original": "i04_0617",
   "page_count": 4,
   "order": 255,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "In this paper, we propose the use of the multi-frame Gaussian mixture model-based block quantizer for the coding of Mel frequency-warped cepstral coefficient (MFCC) features in distributed speech recognition (DSR) applications. This coding scheme exploits intraframe correlation via the Karhunen-Loeve transform (KLT) and interframe correlation via the joint processing of adjacent frames together with the computational simplicity of scalar quantization. The proposed coder is bit-rate scalable, which means that the bitrate can be adjusted without the need for re-training of the quantizers. Static parameters such as the probability density function (PDF) model and KLT orthogonal matrices are stored at the encoder and decoder and bit allocations are calculated 'on-the-fly' without intensive processing. This coding scheme is evaluated in this paper on the Aurora-2 database in a DSR framework. It is shown that this coding scheme achieves high recognition performance at lower bitrates, with a word error rate (WER) of 2.5% at 800 bps, which is less than 1% degradation from the baseline word recognition accuracy, and graceful degradation down to a WER of 7% at 300 bps.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-254"
  },
  "srinivasamurthy04_interspeech": {
   "authors": [
    [
     "Naveen",
     "Srinivasamurthy"
    ],
    [
     "Kyu Jeong",
     "Han"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Robust speech recognition over packet networks: an overview",
   "original": "i04_0621",
   "page_count": 4,
   "order": 256,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "Conventional circuit-switched networks are increasingly being replaced by packet-based networks for voice communication applications. Additionally, there has been an increased deployment of services supporting speech based interactions. These trends demand reliable transmission of speech data not just for playback but also to ensure acceptable automatic speech recognition (ASR) performance. In this paper, we present an overview of techniques that have been investigated to improve ASR performance against two major degradation factors in the context of packet networks: (1) information loss due to a low bit-rate codec and (2) packet loss due to channel (network) conditions. In addition, we highlight another key issue, packet loss rate, by showing ASR performance as a function of packet size and channel condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-255"
  },
  "eriksson04_interspeech": {
   "authors": [
    [
     "Thomas",
     "Eriksson"
    ],
    [
     "Samuel",
     "Kim"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Chungyong",
     "Lee"
    ]
   ],
   "title": "Theory for speaker recognition over IP",
   "original": "i04_0625",
   "page_count": 4,
   "order": 257,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "In this paper, we develop theory for speaker recognition, based on information theory. We show that the performance of a speaker recognition system is closely connected to the mutual information between features and speaker, and derive upper and lower bounds for the performance. We apply the theory to the case when the speech is coded and transmitted over a packet-based channel, in which packet losses occurs. The theory gives important insights in what methods can be used to improve the recognition performance, and what methods are meaningless.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-256"
  },
  "chou04_interspeech": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Feng",
     "Liu"
    ]
   ],
   "title": "Voice portal services in packet network and voIP environment",
   "original": "i04_0629",
   "page_count": 4,
   "order": 258,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "In this paper, we study the voice portal services in packet network and VoIP environment. An extensible VoIPTeleserver for VoIP in SIP (Session Initiation Protocol) environment is described. It is based on the concept of dialogue system and web convergence that separates the channel dependent media resources from the application creation environment. It supports XML based service applications for multiple channels including voice, DTMF, IM and chat over IP. Special attention is given to the adverse effect of delay, jitter and packet loss for voice portal services over IP. In particular, case studies of DTMF service in voice portal under adverse channel conditions are performed. The compounding effects of multiple channel impairments to DTMF in voice portal services over IP are revealed. The potential high error rate indicates that the data redundancy method as proposed in RFC 2198 is needed for DTMF in order to achieve reliable voice portal services over IP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-257"
  },
  "kabal04_interspeech": {
   "authors": [
    [
     "Peter",
     "Kabal"
    ],
    [
     "Colm",
     "Elliott"
    ]
   ],
   "title": "Synchronization of speaker selection for centralized tandem free voIP conferencing",
   "original": "i04_0633",
   "page_count": 4,
   "order": 259,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "Traditional teleconferencing uses a select-and-mix function at a centralized conferencing bridge. In VoIP environments, this mixing operation can lead to speech degradation when using high compression speech codecs due to tandem encodings and coding of multi-talker signals. A tandem-free architecture can eliminate tandem encodings and preserve speech quality. VoIP conference bridges must also consider the variable network delays experienced by different packetized voice streams. A synchronized speaker selection algorithm at the bridge can smooth out network delay variations and synchronize incoming voice streams. This provides a clean mapping of the N input packet streams to the M output streams representing selected speakers. This paper presents a synchronized speaker selection algorithm and evaluates its performance using a conference simulator. The synchronization process is shown to account for only a small part of the overall delay experienced by selected packets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-258"
  },
  "kataoka04_interspeech": {
   "authors": [
    [
     "Akitoshi",
     "Kataoka"
    ],
    [
     "Yusuke",
     "Hiwasaki"
    ],
    [
     "Toru",
     "Morinaga"
    ],
    [
     "Jotaro",
     "Ikedo"
    ]
   ],
   "title": "Measuring the perceived importance of time- and frequency-divided speech blocks for transmitting over packet networks",
   "original": "i04_0637",
   "page_count": 4,
   "order": 260,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "This paper presents a way to calculate the perceived importance of speech segments as a single value criterion, using a linear regression model. Unlike the commonly used voice activity detection (VAD) algorithms, this method allows us to obtain a finer priority granularity of speech segments. This can be used in conjunction with frequency scalable speech coding techniques and IP QoS techniques to achieve efficient and quality-controlled voice transmission. A simple linear regression model is used to calculate the estimated mean opinion score (MOS) of the various cases of missing speech segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-259"
  },
  "kim04h_interspeech": {
   "authors": [
    [
     "Moo Young",
     "Kim"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Comparison of transmitter - based packet-loss recovery techniques for voice transmission",
   "original": "i04_0641",
   "page_count": 4,
   "order": 261,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "To facilitate real-time voice communication through the Internet, forward error correction (FEC) and multiple description coding (MDC) can be used as low-delay packet-loss recovery techniques. We use both a Gilbert channel model and data obtained from real IP connections to compare the rate-distortion performance of different variants of FEC and MDC. Using identical overall rates with stringent delay constraints, we find that side-distortion optimized MDC generally performs better than Reed-Solomon based FEC. If the channel condition is known from feedback through the Real-Time Control Protocol (RTCP), then channel-optimized MDC can be used to exploit this information, resulting in significantly improved performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-260"
  },
  "jouvet04_interspeech": {
   "authors": [
    [
     "Denis",
     "Jouvet"
    ],
    [
     "Ronaldo",
     "Messina"
    ]
   ],
   "title": "Context dependent \"long units\" for speech recognition",
   "original": "i04_0645",
   "page_count": 4,
   "order": 262,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "It is expected that longer-than-phoneme units such as syllables or multi-phone units can deal with sources of performance degradation, such as pronunciation variation or coarticulation, better than phoneme-sized units like triphones. The possible number of contextual realizations of those \"long units\" (LU) is very high, causing an explosion of the number of parameters to be estimated. As the training data are limited, the usual solution is to share parameters between different units to improve parameter estimation. Another problem is how to provide a model for a unit (syllable/multi-phones) that was not present during training (unseen unit). In this paper we evaluate and compare syllable and automatically derived multi-phone units. We introduce a method called \"contextual factorization\" to share parameters between different models and we propose a figure of merit to decide which decomposition of an unseen syllable is the most appropriate. Performance is improved comparing to a triphone based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-261"
  },
  "yoshizawa04_interspeech": {
   "authors": [
    [
     "Shinichi",
     "Yoshizawa"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Rapid EM training based on model-integration",
   "original": "i04_0649",
   "page_count": 4,
   "order": 263,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "Recently, speech recognition technique has started being used in various products. In order to make a good acoustic model, usually a lot of training speech data is needed. However, due to the right of voice and privacy issues, it is not easy to collect a lot of training data. Statistical models which have been trained and transformed from speech data do not have the above mentioned problem, and are comparatively easy to obtain. Therefore, a training technique which can make an acoustic model by using statistical models as training data is preferred. This paper describes a new method called Rapid EM Training based on Model-Integration and shows that the proposed method is evaluated positively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-262"
  },
  "fohr04b_interspeech": {
   "authors": [
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Christophe",
     "Cerisara"
    ]
   ],
   "title": "Experiments on the accuracy of phone models and liaison processing in a French broadcast news transcription system",
   "original": "i04_0653",
   "page_count": 4,
   "order": 264,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "In the framework of ESTER, the recent French broadcast radio news transcription task evaluation, we have developed the first version of ANTS, the Automatic News Transcription System of LORIA. This paper describes the different components of the ANTS system and provides some first recognition results on the ESTER database. Then it presents several experiments carried out on this system to take into account the specificities of the French language: how accurate should the phones models be and how to deal with the problem of the liaisons between words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-263"
  },
  "silva04_interspeech": {
   "authors": [
    [
     "Jorge",
     "Silva"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "A statistical discrimination measure for hidden Markov models based on divergence",
   "original": "i04_0657",
   "page_count": 4,
   "order": 265,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "This paper proposes and evaluates a new statistical discrimination measure for hidden Markov models (HMMs) extending the notion of divergence, a measure of average discrimination information originally defined for two probability density functions. The Average Divergence Distance (ADD) is proposed as a statistical discrimination measure between two HMMs, considering the transient behavior of these models. We show the analytical formulation of this discrimination measure, and demonstrate that this quantity is well defined for a left-to-right HMM topology with final non-emitting state, a standard model for basic acoustic units in Automatic Speech Recognition (ASR). Using experiments based on this discrimination measure, it is shown that ADD is a coherent way to evaluate the discrimination dissimilarity between acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-264"
  },
  "stadermann04_interspeech": {
   "authors": [
    [
     "Jan",
     "Stadermann"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "A hybrid SVM/HMM acoustic modeling approach to automatic speech recognition",
   "original": "i04_0661",
   "page_count": 4,
   "order": 266,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "Acoustic models based on a NN/HMM framework have been used successfully on various recognition tasks for continuous speech recognition. Recently tied-posteriors have been introduced within this context. Here, we present an approach combining SVMs and HMMs using the tied-posteriors idea. One set of SVMs calculates class posterior probabilities and shares these probabilities among all HMMs. The number of SVMs is varied as well as the input context and the amount of training data. Applying a first implementation, results on the AURORA2 task show already a promising improvement of the word error rate compared to the baseline acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-265"
  },
  "knoblauch04_interspeech": {
   "authors": [
    [
     "Dirk",
     "Knoblauch"
    ]
   ],
   "title": "Data driven number-of-states selection in HMM topologies",
   "original": "i04_0665",
   "page_count": 4,
   "order": 267,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "In this paper we discuss a data driven approach to select better phone model topologies, in particular to decide on the number of states for linear left-right continuous HMMs. The novel approach is based on a conditional probabilistic viterbi path estimation and operates on forward-backward trained multiple parallel-path HMMs consisting of two different topologies. We compare this conditional probabilistic viterbi path estimation with systematic, statistical and knowledge based designs of different monophone based continuous HMM topologies and evaluate them in a LVSRS system with speech data from the German Verbmobil corpus. We obtained a 5.8% of WER reduction compared to a system with uniform three-state length topologies and a 1.8% WER reduction compared with results of a knowledge based approach with heterogeneous selection of emitting HMM states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-266"
  },
  "cho04c_interspeech": {
   "authors": [
    [
     "Youngkyu",
     "Cho"
    ],
    [
     "Sung-a",
     "Kim"
    ],
    [
     "Dongsuk",
     "Yook"
    ]
   ],
   "title": "Hybrid model using subspace distribution clustering hidden Markov models and semi-continuous hidden Markov models for embedded speech recognizers",
   "original": "i04_0669",
   "page_count": 4,
   "order": 268,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "Today's state-of-the-art speech recognition systems typically use continuous density hidden Markov models with mixture of Gaussian distributions. Such speech recognition systems have problems; they require too much memory to run, and are too slow for large vocabulary applications. Two approaches are proposed for the design of compact acoustic models, namely, subspace distribution clustering hidden Markov models and semi-continuous hidden Markov models. However, these models require also large memory to acquire high recognition accuracy. In this paper, we propose a new hybrid model using subspace distribution clustering hidden Markov model and semi-continuous hidden Markov model with the aim of achieving much more compact acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-267"
  },
  "olsen04_interspeech": {
   "authors": [
    [
     "Peder",
     "Olsen"
    ],
    [
     "Karthik",
     "Visweswariah"
    ]
   ],
   "title": "Fast clustering of Gaussians and the virtue of representing Gaussians in exponential model format",
   "original": "i04_0673",
   "page_count": 4,
   "order": 269,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "This paper aims to show the power and versatility of exponential models by focusing on exponential model representations of Gaussian Mixture Models (GMMs). In a recent series of papers by several authors, GMMs of varying structure and complexity have been considered. These GMMs can all be readily represented as exponential models and oftentimes favorably so. This paper shows how the exponential model representation can offer useful insight even in the case of diagonal and full covariance GMMs! The power of the exponential model is illustrated by proving the concavity of the log determinant function and also by discovering how to speed up diagonal covariance gaussian clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-268"
  },
  "livescu04_interspeech": {
   "authors": [
    [
     "Karen",
     "Livescu"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Feature-based pronunciation modeling with trainable asynchrony probabilities",
   "original": "i04_0677",
   "page_count": 4,
   "order": 270,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "We report on ongoing work on a pronunciation model based on explicit representation of the evolution of multiple linguistic feature streams. In this type of model, most pronunciation variation is viewed as the result of asynchrony between features and changes in feature values. We have implemented such a model using dynamic Bayesian networks. In this paper, we extend our previous work with a mechanism for learning feature asynchrony probabilities from data. We present experimental results on a word classification task using phonetic transcriptions of utterances from the Switchboard corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-269"
  },
  "kuo04b_interspeech": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Maximum entropy direct model as a unified model for acoustic modeling in speech recognition",
   "original": "i04_0681",
   "page_count": 4,
   "order": 271,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "Traditional statistical models for speech recognition have been dominated by generative models such as Hidden Markov Models (HMMs). We recently proposed a new framework for speech recognition using maximum entropy direct modeling, where the probability of a state or word sequence given an observation sequence is computed directly from the model. In contrast to HMMs, features can be non-independent, asynchronous, and overlapping. In this paper, we discuss how to make the computationally intensive training of such models feasible through parallelizing the IIS (Improved Iterative Scaling) algorithm. The direct model significantly outperforms traditional HMMs in word error rate when used as stand-alone acoustic models. Modest improvements over the best HMM system are seen when combined with HMM and language model scores. The maximum entropy model can potentially incorporate non-independent features such as acoustic phonetic features in a way that is robust to missing features due to mismatch between training and testing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-270"
  },
  "zhu04_interspeech": {
   "authors": [
    [
     "Yu",
     "Zhu"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Explicit duration modeling for Cantonese connected-digit recognition",
   "original": "i04_0685",
   "page_count": 4,
   "order": 272,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "This paper describes a study on using explicit duration models in hidden Markov model (HMM) based Cantonese connected-digit recognition. An HMM does not give explicit control to the temporal structure of speech. As a result, the recognition output may exhibit unreasonable duration pattern, which is often accompanied with the presence of recognition errors. We propose to use a duration model that models the relative duration of the tail part of a Cantonese digit, together with conventional word-level duration models. The duration models are integrated into the Viterbi search algorithm for speech recognition. Experimental results show that proposed method leads to substantial reduction of recognition errors, especially for slowly spoken utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-271"
  },
  "chan04_interspeech": {
   "authors": [
    [
     "Arthur",
     "Chan"
    ],
    [
     "Ravishankar",
     "Mosur"
    ],
    [
     "Alexander",
     "Rudnicky"
    ],
    [
     "Jahanzeb",
     "Sherwani"
    ]
   ],
   "title": "Four-layer categorization scheme of fast GMM computation techniques in large vocabulary continuous speech recognition systems",
   "original": "i04_0689",
   "page_count": 4,
   "order": 273,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "Large vocabulary continuous speech recognition systems are known to be computationally intensive. A major bottleneck is the Gaussian mixture model (GMM) computation and various techniques have been proposed to address this problem. We present a systematic study of fast GMM computation techniques. As there are a large number of these and it is impractical to exhaustively evaluate all of them, we first categorized techniques into four layers and selected representative ones to evaluate in each layer. Based on this framework of study, we provide a detailed analysis and comparison of GMM computation techniques from the four-layer perspective and explore two subtle practical issues, 1) how different techniques can be combined effectively and 2) how beam pruning will affect the performance of GMM computation techniques. All techniques are evaluated in the CMU Communicator domain. We also compare their performance with others reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-272"
  },
  "park04c_interspeech": {
   "authors": [
    [
     "Junho",
     "Park"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Compact acoustic model for embedded implementation",
   "original": "i04_0693",
   "page_count": 4,
   "order": 274,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "An acoustic model for an embedded speech recognition system must exhibit two desirable features; ability to minimize performance degradation in recognition while solving the memory problem under limited system resources. To cope with the challenges, we introduce the state-clustered tied-mixture (SCTM) HMM as an acoustic model optimization. The proposed SCTM modeling shows a significant improvement in recognition performance as well as a solution to sparse training data problem. Moreover, the state weight quantizing method achieves a drastic reduction in model size. In this paper, we describe the acoustic model optimization procedure for embedded speech recognition system and corresponding performance evaluation results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-273"
  },
  "jitsuhiro04_interspeech": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Increasing the mixture components of non-uniform HMM structures based on a variational Bayesian approach",
   "original": "i04_0697",
   "page_count": 4,
   "order": 275,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "We propose using the Variational Bayesian (VB) approach for automatically creating non-uniform, context-dependent HMM topologies. Although the Maximum Likelihood (ML) criterion is generally used to create HMM topologies, it has an over-fitting problem. Recently, to avoid this problem, the VB approach has been applied to create acoustic models for speech recognition. We introduce the VB approach to the Successive State Splitting (SSS) algorithm, which can create both contextual and temporal variations for HMMs. Experimental results show that the proposed method can automatically create a more efficient model than the original method. Furthermore, we evaluated a method to increase the number of mixture components by using the VB approach and considering temporal structures. The VB approach obtained the best performance with a smaller number of mixture components in comparison with that obtained by using ML based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-274"
  },
  "somervuo04_interspeech": {
   "authors": [
    [
     "Panu Juhani",
     "Somervuo"
    ]
   ],
   "title": "Comparison of ML, MAP, and VB based acoustic models in large vocabulary speech recognition",
   "original": "i04_0701",
   "page_count": 4,
   "order": 276,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "The present work compares three different methods for training acoustic models in a Finnish large vocabulary speech recognition system. The models are trained using the maximum likelihood (ML), maximum a posteriori (MAP), and variational Bayesian (VB) principle. The results show that when the model complexity is properly chosen, all three methods give similar performance. As the model complexity increases, the performance of ML based system starts to degrade whereas no overfitting is observed using MAP and VB based models. MAP gives slightly better recognition accuracy over VB but it cannot be used for model selection without auxiliary data. The advantage of VB is that it can be used for selecting a well performing model structure using only training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-275"
  },
  "macherey04_interspeech": {
   "authors": [
    [
     "Wolfgang",
     "Macherey"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Discriminative training with tied covariance matrices",
   "original": "i04_0705",
   "page_count": 4,
   "order": 277,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "Discriminative training techniques have proved to be a powerful method for improving large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models. Typically, the optimization of discriminative objective functions is done using the extended Baum algorithm. Since for continuous distributions no proof of fast and stable convergence is known up to now, parameter re-estimation depends on setting the iteration constants in the update rules heuristically, ensuring that the new variances are positive definite. In case of density specific variances this leads to a system of quadratic inequalities. However, if tied variances are used, the inequalities become more complicated and often the resulting constants are too large to be appropriate for discriminative training. In this paper we present an alternative approach to setting the iteration constants to alleviate this problem. First experimental results show that the new method leads to improved convergence speed and test set performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-276"
  },
  "diehl04_interspeech": {
   "authors": [
    [
     "Frank",
     "Diehl"
    ],
    [
     "Asuncion",
     "Moreno"
    ]
   ],
   "title": "Acoustic phonetic modeling using local codebook features",
   "original": "i04_0709",
   "page_count": 4,
   "order": 278,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "In this article we present an alternative method for defining the question set used for the induction of acoustic phonetic decision trees. The method is data driven and employs local similarities between the probability density functions of hidden Markov models. The method is shown to work at least as well as the standard method using question sets devised by human experts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-277"
  },
  "jung04b_interspeech": {
   "authors": [
    [
     "Gue Jun",
     "Jung"
    ],
    [
     "Su-Hyun",
     "Kim"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "An efficient codebook design in SDCHMM for mobile communication environments",
   "original": "i04_0713",
   "page_count": 4,
   "order": 279,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "Speech recognition systems require too much memory to run and are too slow for mass application. In order to overcome these constraints and make speech recognition systems suitable for mobile devices, we propose efficient codebook construction method for subspace distribution clustering hidden markov modeling (SDCHMM). The output probability of mixture Gaussians is more sensitive to quantization error of mean vectors than that of variance vectors. Therefore we propose a new subspace definition which minimizes quantization error of mean vectors first. Next, we split mixture Gaussians into mean and variance vectors and construct separate codebooks using modified Bhattacharyya distance measure. In experiments using RM database, proposed method decreases 24.5% relative word error rate compared with general SDCHMM without use of extra memory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-278"
  },
  "shozakai04_interspeech": {
   "authors": [
    [
     "Makoto",
     "Shozakai"
    ],
    [
     "Goshu",
     "Nagino"
    ]
   ],
   "title": "Analysis of speaking styles by two-dimensional visualization of aggregate of acoustic models",
   "original": "i04_0717",
   "page_count": 4,
   "order": 280,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "To ensure high enough recognition performance from the outset of usage of the speech recognition system, prior development of highly precise acoustic model library is necessary. The analysis of HMM acoustic models expressed with Gaussian distributions of multidimensional vectors is typically a difficult task. The COSMOS (aCOustic Space Map Of Sound) method featuring the visualization of distributions of the acoustic models in a two dimensional space by utilizing multidimensional scaling technique is proposed in order to support the analysis through capability of human visual perception. The effectiveness of the proposed technique is reviewed based on an analysis on speaking styles. The marginal region within the two-dimensional visual map(called COSMOS map) obtained by the proposed method the contains acoustic models with lower recognition performance. It is possible to improve recognition performance by dividing the marginal region into several smaller zones in which separate acoustic model is trained and provided to the speakers belonging to the same zone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-279"
  },
  "koo04_interspeech": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Ho-Hyun",
     "Jeon"
    ],
    [
     "Sang-Hong",
     "Lee"
    ]
   ],
   "title": "Context dependent phoneme duration modeling with tree-based state tying",
   "original": "i04_0721",
   "page_count": 4,
   "order": 281,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "In this paper, we propose phoneme duration modeling methods with tree-based state tying. Two kinds of phone duration modeling methods are suggested. The first is context independent phoneme duration model in which duration parameters are stored in each phone. The second is context dependent duration model in which duration parameters are stored in each state being shared by context dependent phone. We split duration parameters of each context dependent phoneme into three kinds of tied states estimated by tree-based clustering.Both HMM and duration parameters are stored in states tied for expressing all context dependent phones in a phone. The duration parameters of context dependent phoneme are automatically generated from state duration parameters in the initialization stage of recognition. Context dependent phoneme duration model is compared with context independent phoneme duration model as well as with no duration model.Experimental results demonstrate that duration information rejects OOT (out-of-task) words very well and that context dependent duration model yields the best performance among three methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-280"
  },
  "bridle04_interspeech": {
   "authors": [
    [
     "John Scott",
     "Bridle"
    ]
   ],
   "title": "Towards better understanding of the model implied by the use of dynamic features in HMMs",
   "original": "i04_0725",
   "page_count": 4,
   "order": 282,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "We examine a widely-used kind of Hidden Markov Model (HMM) in which dynamic features are included along with the direct measurements. We conclude that the generative model implied by the use of dynamic features is quite different from the conventional view and that such models are capable of providing a surprising amount of the sort of dynamics that we thought were necessary to describe the important properties of speech patterns. We suggest that one reason why several attempts to replace HMMs with models with explicit dynamics have failed is that the dynamics already implicit in standard HMMs are roughly equivalent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-281"
  },
  "li04b_interspeech": {
   "authors": [
    [
     "Jian-Feng",
     "Li"
    ],
    [
     "Guo-Ping",
     "Hu"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": "Chinese prosody phrase break prediction based on maximum entropy model",
   "original": "i04_0729",
   "page_count": 4,
   "order": 283,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "A maximum entropy based model for prosody phrase break prediction was proposed in this paper, and a comparison was conducted on large corpora between the new model and the decision tree based model which was the mainstream method for prosody phrase break prediction. The contribution of lexical information and influences of different cutoff values were also investigated. It was demonstrated that, utilizing the same information, maximum entropy based method made an improvement of 5.5% on F-Score over decision tree based method. Integrating lexical information, an improvement of 9.4% over decision tree was achieved. Using maximum entropy based method, to achieve the performance of traditional decision tree, 83% manual work could be saved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-282"
  },
  "sreenivasarao04_interspeech": {
   "authors": [
    [
     "Krothapalli",
     "Sreenivasa Rao"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Intonation modeling for indian languages",
   "original": "i04_0733",
   "page_count": 4,
   "order": 284,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "In this paper we study neural network models to capture intonation patterns of speech in Indian languages. We examine the performance of neural networks and support vector machines (SVM) for this purpose. Modeling the intonation pattern is the task of predicting the sequence of fundamental frequency (f0) values for the sequence of syllables in the given text. Analysis is performed on broadcast news data in the languages Hindi, Telugu and Tamil, in order to predict the 'f0' of syllables in these languages using neural network and SVM models. The input to both the models consists of a set of phonological, positional and contextual features extracted from the text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-283"
  },
  "zheng04b_interspeech": {
   "authors": [
    [
     "Yu",
     "Zheng"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ],
    [
     "Byeongchang",
     "Kim"
    ]
   ],
   "title": "Using multiple linguistic features for Mandarin phrase break prediction in maximum-entropy classification framework",
   "original": "i04_0737",
   "page_count": 4,
   "order": 285,
   "p1": "737",
   "pn": "",
   "abstract": [
    "We model Mandarin phrase break prediction as a classification problem with three level prosodic structures and apply conditional maximum entropy classification to this problem. We acquire multiple levels of linguistic knowledge from an annotated corpus to become well-integrated features for maximum entropy framework. Five kinds of features were used to represent various linguistic constraints including POS tag features, lexical features, phonetic features, length features, and distance features. Experiment results show that our method performs better than the previous methods and the conditional maximum entropy (ME) model is very effective for data sparseness problem in Mandarin phrase break prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-284"
  },
  "read04_interspeech": {
   "authors": [
    [
     "Ian",
     "Read"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Using part-of-speech for predicting phrase breaks",
   "original": "i04_0741",
   "page_count": 4,
   "order": 286,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "Predicting the location of phrase breaks within an utterance is an important task in text-to-speech synthesis, and can be done with reasonable accuracy using part-of-speech (POS) tags as features. However, it seems unlikely that the 40 or more different tags used by most taggers all contribute to this task, and in fact many may contribute noise. In this paper, we present an algorithm for reducing the standard Penn Treebank POS tag set for use in predicting phrase breaks. Using the best first search approach, the algorithm considers possible groupings of tags, searching the groupings that yield the highest overall performance. The reduced tag sets were evaluated by an n-gram model trained on POS sequences along with their associated juncture (break/non-break), the reduced tag set raised the model's performance on junctures correct from 90.38% to 92.43%, and reduced insertions from 2.89% to 1.83%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-285"
  },
  "escuderomancebo04_interspeech": {
   "authors": [
    [
     "David",
     "Escudero-Mancebo"
    ],
    [
     "Valentin",
     "Cardenoso-Payo"
    ]
   ],
   "title": "A proposal to quantitatively select the right intonation unit in data-driven intonation modeling",
   "original": "i04_0745",
   "page_count": 4,
   "order": 287,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "In this work, we provide a procedure for the systematic evaluation of the quantitative impact of the selection of the basic intonation unit for data-driven intonation modeling. Taking advantage of the corpus based modeling technique previously developed, we show how the number of prosodic features selected and the kind of basic unit determines the final prediction RMSE of the synthesized F0 profiles. This provides a means to fully characterize a given corpus and, also, a procedure to test how 'a priori' linguistic knowledge corresponds with real samples in that corpus. Although all the results shown are for Spanish, the methodology can be readily applied to any other similar language for which a similar annotated corpus is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-286"
  },
  "ni04_interspeech": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Formulating contextual tonal variations in Mandarin",
   "original": "i04_0749",
   "page_count": 4,
   "order": 288,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "This paper deals with both tone modeling and contextual tonal variations for formulating the latter in a parametric form based on a functional fundamental frequency (F0) model.The contextual tonal variations are measured in terms of F0 peak and valley targets from 1,560 Chinese polysyllabic words, taking into account two factors of contextual tones and target tone position. The speaker-dependent effect on these observed samples is eliminated by voice range normalization. In the formulation with tone modeling, the peak target of a tone is represented as its bias with respect to a natural declination line, while the gliding feature of the tone is expressed as the response time and amplitude of transition between its valley and peak targets. This paper presents the details of formulated parameters for tri-tone contexts and algorithms for converting tonal peak and valley targets into the model parameters for synthesizing F0 contours. The achievement is useful for synthesizing neutral intonation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-287"
  },
  "mouline04_interspeech": {
   "authors": [
    [
     "Salma",
     "Mouline"
    ],
    [
     "Olivier",
     "Boeffard"
    ],
    [
     "Paul",
     "Bagshaw"
    ]
   ],
   "title": "Automatic adaptation of the momel F0 stylisation algorithm to new corpora",
   "original": "i04_0753",
   "page_count": 4,
   "order": 289,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "The paper investigates the adaptability of the MoMel (Modelling of Melody) Algorithm, Hirst & Espesser 1983, to new corpora. A detailed overview of the MoMel algorithm and its parameters are presented. The generality of the default parameter values to new corpora is studied empirically. Two of the parameters, related to window durations, are discovered to be highly corpus dependant. The paper presents a significant reduction in the modelling error (r.m.s. from 8.45 to 6.19 Hz) by automatically adapting these two parameters to the new corpora. Despite this optimisation, the observed errors remain perceptually noticeable (JND of approximately 3-4 Hz) and future work needs to address this problem.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-288"
  },
  "aguero04_interspeech": {
   "authors": [
    [
     "Pablo Daniel",
     "Aguero"
    ],
    [
     "Klaus",
     "Wimmer"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Joint extraction and prediction of fujisaki's intonation model parameters",
   "original": "i04_0757",
   "page_count": 4,
   "order": 290,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "This paper presents a joint extraction and prediction framework for intonation modeling applied to Fujisaki's intonation model for text-to-speech conversion. Previous methods in the area extract the parameters of accent and phrase commands for each sentence. Then, these parameters are related to linguistic features for prediction. In our approach commands that share the same linguistic features are globally estimated. This approach intends to overcome some consistency problems of the extracted model parameters. The global nature of the parameter optimization avoids the interpolation step, which sometimes can produce a bias in the extracted parameters. Experimental results show that the higher consistency of the parameters result in a higher accuracy when the fundamental frequency contours are predicted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-289"
  },
  "zervas04_interspeech": {
   "authors": [
    [
     "Panagiotis",
     "Zervas"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ],
    [
     "George",
     "Kouroupetroglou"
    ],
    [
     "Gerasimos",
     "Xydas"
    ]
   ],
   "title": "Evaluation of corpus based tone prediction in mismatched environments for greek tts synthesis",
   "original": "i04_0761",
   "page_count": 4,
   "order": 291,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "One of the main aspects in Text-to-Speech (TtS) synthesis is the successful prediction of tonal events. In this work we deal with the evaluation of corpus-based models in operational environments other than the training ones. Two pitch accent frameworks derived by linguistically enriched speech data from a generic domain and a limited domain were initially evaluated by applying the 10-fold cross validation method. As a second step, we utilized the cross domains data validation. Due to the heterogeneity of the data, we further employed three machine learning approaches, CART, Naive Bayes and Bayesian networks. The results demonstrate that the limited domain models achieve in average 10% improved accuracy in self-domain evaluation, while the generic models preserve a their performance regardless the domain of application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-290"
  },
  "xiong04_interspeech": {
   "authors": [
    [
     "Ziyu",
     "Xiong"
    ],
    [
     "Juanwen",
     "Chen"
    ]
   ],
   "title": "The duration of pitch transition phase and its relative factors",
   "original": "i04_0765",
   "page_count": 4,
   "order": 292,
   "p1": "765",
   "pn": "768",
   "abstract": [
    "The pitch phase of a syllable (PPS) can be divided into a pitch transition phase (PTrP) and a pitch target phase (PTaP). To analyze the relationships between the duration of the PTrP and the pitch range of the PTrP, the pitch range of the PTaP, and the total duration of the PPS, 306 disyllable combinations were selected from a corpus to generalize a linear regression model, which can be applied to predict the duration of the PTrP. In this model, the duration of the PTrP has a positive relationship with the pitch range of the PTrP and the total duration of the PPS, while a negative relationship with the pitch range of the PTaP. Moreover, with the increase of the total duration of the PPS, the duration of the PTrP will increase faster. By using this linear regression model to predict the duration of the PTrP, the relative coefficient between predicted value and the metrical value is 0.853.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-291"
  },
  "hu04_interspeech": {
   "authors": [
    [
     "Yu",
     "Hu"
    ],
    [
     "Renhua",
     "Wang"
    ],
    [
     "Lu",
     "Sun"
    ]
   ],
   "title": "Polynomial regression model for duration prediction in Mandarin",
   "original": "i04_0769",
   "page_count": 4,
   "order": 293,
   "p1": "769",
   "pn": "772",
   "abstract": [
    "Duration modeling is to establish a mapping relationship between the prosodic environment and the segmental duration engendered in natural speech. In this paper, we first study the effect of prosodic features on segmental duration of neutral utterance in Mandarin by introducing a statistical concept---eta squared, then choose more forceful prosodic features and design interaction quantifying algorithm to study the interaction phenomenon among them, and finally determine the duration model using a polynomial and obtain the coefficients through nonlinear regression. Our research work indicates that 5 to 6 prosodic features might by and large assist a close and accurate mapping between prosodic environment and the perceived duration. Compared to Wagon tree method, this one has undeniable merits.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-292"
  },
  "tooher04_interspeech": {
   "authors": [
    [
     "Michelle",
     "Tooher"
    ],
    [
     "John G.",
     "McKenna"
    ]
   ],
   "title": "Prediction of the glottal LF parameters using regression trees",
   "original": "i04_0773",
   "page_count": 4,
   "order": 294,
   "p1": "773",
   "pn": "776",
   "abstract": [
    "The behaviour of the glottal Liljencrants-Fant (LF) parameters were studied across vowel, context, duration, stress and fundamental frequency. Using statistical analysis we attempted to account for the variation in the glottal parameters for a speaker and from the patterns observed create a model capable of predicting the parameters for a given utterance and fundamental frequency. The parameters varied, as expected, with fundamental frequency, and both prosodic and contextual information were statistically significant predictors. Regression trees were used to create models for each of the parameters and predictions of the LF parameters were calculated. The average relative percentage error of the predictions varied across parameters, and the study indicated that it is possible to make acceptable predictions of the LF parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-293"
  },
  "dellwo04_interspeech": {
   "authors": [
    [
     "Volker",
     "Dellwo"
    ],
    [
     "Bianca",
     "Aschenberner"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Jana",
     "Dancovicova"
    ],
    [
     "Ingmar",
     "Steiner"
    ]
   ],
   "title": "Bonntempo-corpus and bonntempo-tools: a database for the study of speech rhythm and rate",
   "original": "i04_0777",
   "page_count": 4,
   "order": 295,
   "p1": "777",
   "pn": "780",
   "abstract": [
    "Work is currently being carried out on a speech database constructed in order to study speech rhythm in connection with speech rate. The database, BonnTempo-Corpus, and the Praat based analysis tools, BonnTempo-Tools, are a powerful instrument for examining various aspects of recently proposed rhythm measures (e.g. %V, deltaC, nPVI, rPVI, etc.) in relation to speech rate among a wide range of languages and speakers. First observations pose new problems on traditionally not well classifiable languages like Czech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-294"
  },
  "gu04_interspeech": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis of F0 contours of Cantonese utterances based on the command-response model",
   "original": "i04_0781",
   "page_count": 4,
   "order": 296,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "As a major Chinese dialect, Cantonese is well known for its complex tone system. This paper applies the command-response model to represent the F0 contours of Cantonese speech. Analysis-by-Synthesis is conducted on both designed sentences and connected utterances, from which a set of appropriate tone command patterns is derived. By intrinsically incorporating the effects of tone coarticulation, word accentuation and phrase intonation, the model provides a high accuracy of approximation to the F0 contours of Cantonese, and hence serves as a much better tool to quantitatively describe the continuous F0 contours than the traditional tone scale notation system. The constraints in timing and amplitude of tone commands are also investigated, which can be used for rule-based synthesis of F0 contours.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-295"
  },
  "dohen04_interspeech": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Helene",
     "Loevenbruck"
    ]
   ],
   "title": "Pre-focal rephrasing, focal enhancement and postfocal deaccentuation in French",
   "original": "i04_0785",
   "page_count": 4,
   "order": 297,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "This study aims at better describing the acoustic correlates of contrastive focus in French. A corpus was recorded from a male native speaker of French. It consisted of sentences with a subject-verb-object (SVO) structure under four conditions: focus on each phrase (S,V,O) and broad focus. The focal, pre-focal and post-focal constituents were studied separately. The acoustic analysis showed that: a) the pitch of the focal constituent rises, b) that of the surrounding constituents decreases, c) the duration of the focal syllables and of the pre-focal syllable increases, the onset of the focal constituent increasing the most, d) the pre-focal sequence is rephrased, e) the post-focal sequence is deaccented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-296"
  },
  "nemala04_interspeech": {
   "authors": [
    [
     "Sridhar Krishna",
     "Nemala"
    ],
    [
     "Partha Pratim",
     "Talukdar"
    ],
    [
     "Kalika",
     "Bali"
    ],
    [
     "A. G.",
     "Ramakrishnan"
    ]
   ],
   "title": "Duration modeling for hindi text-to-speech synthesis system",
   "original": "i04_0789",
   "page_count": 4,
   "order": 298,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "This paper reports preliminary results of data-driven modeling of segmental (phoneme) duration for Hindi. Classification and Regression Tree (CART) based data-driven duration modeling for segmental duration prediction is presented. A number of features are considered and their usefulness and relative contribution for segmental duration prediction is assessed. Objective evaluation of the duration model, by root mean squared prediction error (RMSE) and correlation between actual and predicted durations, is performed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-297"
  },
  "krishna04_interspeech": {
   "authors": [
    [
     "Nemala Sridhar",
     "Krishna"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "A new prosodic phrasing model for indian language telugu",
   "original": "i04_0793",
   "page_count": 4,
   "order": 299,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "Prosodic phrasing is an important and more difficult a problem for Indian languages, as the Indian language scripts use very little or no punctuation. This paper reports a preliminary attempt on data-driven modeling of prosodic phrase boundary prediction for the Indian language Telugu. In an effort to identify meaningful features that affect the prosodic phrasing, a new feature, namely 'morpheme tag', is defined. A Classification and Regression Tree (CART) based data-driven phrasing model is developed for the prosodic phrase boundary prediction and the usefulness of the 'morpheme tag' feature is further demonstrated in an evaluation process. The phrasing model developed has been implemented in an Indian language Text-to-Speech synthesis system being developed within Festival framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-298"
  },
  "jokisch04_interspeech": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Michael",
     "Hofmann"
    ]
   ],
   "title": "Evolutionary optimization of an adaptive prosody model",
   "original": "i04_0797",
   "page_count": 4,
   "order": 300,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "The perceived quality of synthetic speech strongly depends on its prosodic naturalness. Departing from the syllable-based, adaptive prosody model IGM the authors surveyed a novel evolutionary approach to optimize the model structure itself and to finally improve the predicted prosodic contours. A German newsreader corpus has been trained using a feed forward neural network. In parallel, network and data configurations were automatically optimized using the Strength Pareto Evolutionary Algorithm (SPEA). Achieving similar prediction results as in the original IGM configuration, the evolutionary optimization reduces the network and parameter complexity. This optimization method may be helpful in the further development of resource-saving prosody modules, e.g., for use in embedded text-to-speech applications and it also eases the difficult introspection of prosodic rules which are automatically generated during training. Nevertheless, preliminary perceptive tests show no significant differences in comparison to synthetic stimuli based on prosodic contours predicted by the original model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-299"
  },
  "xydas04_interspeech": {
   "authors": [
    [
     "Gerasimos",
     "Xydas"
    ],
    [
     "Georgios",
     "Kouroupetroglou"
    ]
   ],
   "title": "An intonation model for embedded devices based on natural F0 samples",
   "original": "i04_0801",
   "page_count": 4,
   "order": 301,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "The evolution of hand-held devices has made possible the porting of high quality Text-to-Speech systems to embedded platforms. However, linguistic resources required to build natural-sounding prosody models still need to be scaled down, to meet the hardware specifications of the devices. In this work, we present a compact intonation model that brings together the naturalness of corpus based prosody modeling with the limited nature of the embedded TtS applications. A sampling process of 3 points per syllable over a small set of appropriately set up utterances is used as the tonal unit database. The sampled points are applied at synthesis time over an onset/offset syllabic structuring of phrases. The model requires less than 1KB of storage for modeling each prosodic phrase class. The application to the Greek language is being demonstrated utilizing only lexical stress information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-300"
  },
  "vesela04_interspeech": {
   "authors": [
    [
     "Katerina",
     "Vesela"
    ],
    [
     "Nino",
     "Peterek"
    ],
    [
     "Eva",
     "Hajicova"
    ]
   ],
   "title": "Prosodic characteristics of czech contrastive topic",
   "original": "i04_0805",
   "page_count": 4,
   "order": 302,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "It is the main motivation of our present study to identify some further criterion for the oppositions of topic, contrastive topic and, as the case may be, of focus in the prosodic characteristics of utterances. For this purpose, we have examined three sets of naturally occurring speech and analyzed them in order to find whether the character of intonation contours distinguishes between the three notions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-301"
  },
  "graciarena04_interspeech": {
   "authors": [
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Federico",
     "Cesari"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Greg",
     "Myers"
    ],
    [
     "Cregg",
     "Cowan"
    ],
    [
     "Victor",
     "Abrash"
    ]
   ],
   "title": "Combination of standard and throat microphones for robust speech recognition in highly noisy environments",
   "original": "i04_0809",
   "page_count": 4,
   "order": 303,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "We present a method to combine the standard and throat microphone signals for noise-robust speech recognition. Our approach is to extend the probabilistic optimum filter algorithm to estimate the standard microphone clean speech feature vectors from both microphones' noisy speech feature vectors. We tested the proposed approach in two noisy speech recognition tasks. In the first task we used a large vocabulary continuous speech recognition system and noisy speech both using artificially added noise and recorded inside an M1 tank. In the second task we used a real-time system and noisy speech recorded in a highly noisy environment, a HMMWV vehicle. We used noise-canceling and throat microphones. Because of the highly adverse conditions in this second task we propose an extension of the combined microphone approach, which takes into account the level of noise captured by the throat microphone. The combined microphone approach outperforms the single microphone approach in all the recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-302"
  },
  "demiroglu04_interspeech": {
   "authors": [
    [
     "Cenk",
     "Demiroglu"
    ],
    [
     "Anderson",
     "David"
    ]
   ],
   "title": "Noise robust digit recognition using a glottal radar sensor for voicing detection",
   "original": "i04_0813",
   "page_count": 4,
   "order": 304,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "A voicing feature is used in concatenation to MFCC features to increase the performance of digit recognition at both low and high SNRs. The problem of noise robust extraction of the voicing feature is solved by using the glottal electromagnetic sensor (GEMS). The GEMS device provides reliable voicing information at all SNRs and noise environments. It is shown that although the voicing feature increases the performance for the clean speech case, the relative improvement for the noisy case is significantly higher for a digit recognition task. Our results indicate that the GEMS device can solve the fundamental problem of extracting reliable voicing information in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-303"
  },
  "raub04_interspeech": {
   "authors": [
    [
     "Dominik",
     "Raub"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Matthias",
     "Wöfel"
    ]
   ],
   "title": "A cepstral domain maximum likelihod beamformer for speech recognition",
   "original": "i04_0817",
   "page_count": 4,
   "order": 305,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "Recent work by Seltzer indicates that classical approaches to beamforming, minimizing output power while enforcing a distortionless constraint, do not yield optimal results in terms of word error rate (WER) on speech recognition task. This problem can be traced back to the mismatch between the target criterion of classical adaptive beamformers, which is optimization of the signal to noise ratio, and the actual target criterion, which is the reduction of the recognizer's WER. Following an approach by Seltzer we therefore investigate the performance of an alternative error criterion, which attempts to optimize the beamformer weights, so as to improve the likelihoods along the recognizer's Viterbi path for each utterance. This criterion matches the goal of lower WERs more closely and therefore leads to better recognition results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-304"
  },
  "mochiki04_interspeech": {
   "authors": [
    [
     "Naoya",
     "Mochiki"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Toshiyuki",
     "Sekiya"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ]
   ],
   "title": "Recognition of three simultaneous utterance of speech by four-line directivity microphone mounted on head of robot",
   "original": "i04_0821",
   "page_count": 4,
   "order": 306,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "A sound source separation method using four-line directivity microphones mounted on a head of a robot is proposed and applied to speech recognition under existence of two disturbances of speech. Sound source separation methods using microphones mounted on robot heads generally used strict head-related transfer functions (HRTF). We propose a robust sound source separation that does not require an estimate of a strict HRTF. Our method takes advantage of a sound pressure difference with the robot head acting as a sound barrier. The enhancement of the difference in the target speech is performed by signal processing of three layers:two-line SAFIA, two-line Spectral Subtraction and their integration. The experimental results of three simultaneous utterance recognition with vocabulary of 20K show that the proposed method is effective in achieving 71% error reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-305"
  },
  "sagayama04_interspeech": {
   "authors": [
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Okajima",
     "Takashi"
    ],
    [
     "Kamamoto",
     "Yutaka"
    ],
    [
     "Nishimoto",
     "Takuya"
    ]
   ],
   "title": "Complex spectrum circle centroid for microphone-array-based noisy speech recognition",
   "original": "i04_0825",
   "page_count": 4,
   "order": 307,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "We propose a novel principle based on Complex Spectrum Circle Centroid (CSCC) for restoring complex spectrum of the target (speech) signal from multiple microphone input signals in a noisy environment. If noise arrives at multiple microphones with different time delays relative to the target signal, the observed noisy signals lie on a circle in the complex spectrum plane from which the target signal is restored by finding the centroid of the circle. Unlike most existing methods such as ICA, AMNOR and beamforming, this nonlinear method allows any type of noise including non-stationary, moving, signal-correlated, non-planar, and spoken noises, without identifying the noise direction and training parameters. In speech recognition experiments, the proposed method showed a word accuracy close to the clean speech recognition rate of 89.4% in the case of single noise, and from 0% with one microphone to 60.6% with 8 microphones in the case of 3 spoken noises.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-306"
  },
  "heck04_interspeech": {
   "authors": [
    [
     "Larry",
     "Heck"
    ],
    [
     "Mark",
     "Mao"
    ]
   ],
   "title": "Automatic speech recognition of co-channel speech: integrated speaker and speech recognition approach",
   "original": "i04_0829",
   "page_count": 4,
   "order": 308,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "This paper presents a novel Bayesian approach to the problem of co-channel speech. The problem is formulated as the joint maximization of the a posteriori probability of the word sequence and the target speaker given the observed speech signal. An efficient single-pass Viterbi search strategy is presented. Experimental results on over-the-telephone recognition of co-channel speech show a 45% reduction in word error rate of a 10-digit telephone number task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-307"
  },
  "marino04_interspeech": {
   "authors": [
    [
     "José B.",
     "Marino"
    ],
    [
     "Asuncion",
     "Moreno"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "A first experience on multilingual acoustic modeling of the languages spoken in morocco",
   "original": "i04_0833",
   "page_count": 4,
   "order": 309,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "The goal of this paper is to explore and describe the potential of multilingual acoustic models for automatic speech recognition of the languages spoken in Morocco. The basic experimental framework comes from the OrienTel project, mainly the sound inventory of the Arabic languages and the speech databases. Monolingual and multilingual automatic speech recognition systems for Modern Colloquial and Standard Arabic (MCA and MSA, respectively) and French languages are developed and evaluated, in order to envisage the phonetic exchange and similarity among the three languages. As a main result, it can be stated that a combined modeling of MSA and MCA or, even a trilingual design, does not harm the performance of the recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-308"
  },
  "caballero04_interspeech": {
   "authors": [
    [
     "Monica",
     "Caballero"
    ],
    [
     "Asuncion",
     "Moreno"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "Data driven multidialectal phone set for Spanish dialects",
   "original": "i04_0837",
   "page_count": 4,
   "order": 310,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "This paper addresses the use of a data-driven approach to determine a multidialectal phone set for an automatic speech recognition system for Spanish dialects. This approach is based on a decision tree clustering algorithm that tries to cluster contextual units of different dialects. This procedure avoids the definition of a global phonetic inventory and the previous study of similarity of sounds. The procedure is applied in Spanish as spoken in Spain, Colombia and Venezuela. Results show differences between phonemes that share the same SAMPA symbol in different dialects and also detect similarities between phonemes that are represented by different symbols in dialectal variants. Recognition results using this multidialectal approach overcome the monodialectal ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-309"
  },
  "oria04_interspeech": {
   "authors": [
    [
     "Daniela",
     "Oria"
    ],
    [
     "Akos",
     "Vetek"
    ]
   ],
   "title": "Multilingual e-mail text processing for speech synthesis",
   "original": "i04_0841",
   "page_count": 4,
   "order": 311,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "An integrated method of text pre-processing and language identification is introduced to deal with the problem of mixed-language e-mail messages in a speech-enabled e-mail reading system. Our method can confidently distinguish between the supported languages and switch between several TTS engines or languages to read the portions of the text in the appropriate language. This is achieved by making use of the combined information from a text pre-processor and a language identifier that relies on both statistical information and linguistic features indicative of a particular language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-310"
  },
  "romsdorfer04_interspeech": {
   "authors": [
    [
     "Harald",
     "Romsdorfer"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Multi-context rules for phonological processing in polyglot TTS synthesis",
   "original": "i04_0845",
   "page_count": 4,
   "order": 312,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "Polyglot text-to-speech synthesis, i.e. the synthesis of sentences containing one or more inclusions from other languages, primarily depends on an accurate morpho-syntactic analyzer for such mixed-lingual texts. From the output of this analyzer, the pronunciation can be derived by means of phonological transformations which are language-specific and depend on various contexts. In this paper a new rule formalism for such phonological transformations is presented, which complies also with the requirements of the mixed-lingual situation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-311"
  },
  "badino04b_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Badino"
    ],
    [
     "Claudia",
     "Barolo"
    ],
    [
     "Silvia",
     "Quazza"
    ]
   ],
   "title": "A general approach to TTS reading of mixed-language texts",
   "original": "i04_0849",
   "page_count": 4,
   "order": 313,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "The paper presents the Loquendo TTS approach to mixed-language speech synthesis, offering a range of options to face the various situations where texts may occur in different languages or embedding foreign phrases. The most challenging target is to make a monolingual TTS voice read a foreign language text. The adopted Foreign Pronunciation Strategy here discussed allows mixing phonetic transcriptions of different languages, relying on a Phoneme Mapping algorithm making foreign phoneme sequences pronounceable by monolingual voices. The algorithm extends previous solutions, obtaining a plausible approximated pronunciation. The method is efficient, language independent, entirely phonetics-based and it enables any Loquendo TTS voice to speak all the languages provided by the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-312"
  },
  "georgiou04_interspeech": {
   "authors": [
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Hooman Shirani",
     "Mehr"
    ]
   ],
   "title": "Context dependent statistical augmentation of persian transcripts",
   "original": "i04_0853",
   "page_count": 4,
   "order": 314,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "Persian language is transcribed in a lossy manner as it does not, as a rule, encode vowel information. This renders the use of the written script suboptimal for language models for speech applications or for statistical machine translation. It also causes the text-to-speech synthesis from a Persian script input to be a one-to-many operation. In our previous work, we introduced an augmented transcription scheme that eliminates the ambiguity present in the Arabic script. In this paper, we propose a method of generating the augmented transcription from the Arabic script by statistically decoding through all possibilities and choosing the maximum likelihood solution. We demonstrate that even with a small amount of initial bootstrap data, we can achieve a decoding precision of about 93% with no human intervention. The precision can be as high as 99.2% in a semi-automated mode where low confidence decisions are marked for human processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-313"
  },
  "demiroglu04b_interspeech": {
   "authors": [
    [
     "Cenk",
     "Demiroglu"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "A soft decision MMSE amplitude estimator as a noise preprocessor to speech coder s using a glottal sensor",
   "original": "i04_0857",
   "page_count": 4,
   "order": 315,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "A soft-decision Ephraim-Malah suppression rule based speech enhancement algorithm is proposed for intelligibility enhancement in parametric speech coders. A glottal sensor is used to improve the intelligibility of a baseline system that uses only the acoustic microphone. The objective measure test shows that the proposed system decreases the spectral distortion by 2-3 dB for most phonetic classes. Moreover, significant improvements in DRT scores of nasality and sibilation features are obtained compared to the baseline system when the noise suppression systems are concatenated with a MELP based speech coder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-314"
  },
  "hu04b_interspeech": {
   "authors": [
    [
     "Rongqiang",
     "Hu"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Single acoustic-channel speech enhancement based on glottal correlation using non-acoustic sensor",
   "original": "i04_0861",
   "page_count": 4,
   "order": 316,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "This paper describes a single acoustic-channel speech enhancement, utilizing an auxiliary non-acoustic sensor. Unlike classical algorithms, which make use of the knowledge from acoustic signal alone, the glottal correlation (GCORR) algorithm takes advantage of non-acoustic throat sensors such as the general electromagnetic motion sensor (GEMS). The non-acoustic sensor provides a measure of the glottal excitation function that is relatively immune to background acoustic noise. Thus, inspired by human speech production mechanisms, the GCORR algorithm extracts the desired speech signal from noisy acoustic mixture using statistical correlation between the speech and its excitation. The algorithm leads to a significant reduction of wide-band noise, even when the SNR is very low. The improvement in the quality of the speech is demonstrated in terms of an objective evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-315"
  },
  "zhang04g_interspeech": {
   "authors": [
    [
     "Xianxian",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Kathryn",
     "Arehart"
    ],
    [
     "Jessica",
     "Rossi-Katz"
    ]
   ],
   "title": "In-vehicle based speech processing for hearing impaired subjects",
   "original": "i04_0865",
   "page_count": 4,
   "order": 317,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "It is very important to help hearing impaired people to be able to do normal work as normal hearing people can do, such as driving a car. While there have been numerous studies in the field of speech enhancement for car noise environments, the majority of these studies have focused on noise reduction for normal hearing individuals. In this paper, we present recent results in the development of more effective speech capture and enhancement processing for wireless voice interaction between subjects with hearing loss in real car environments. We first present a data collection experiment for a proposed FM wireless transmission scenario using a 5-channel microphone array in the car, followed by several alternative speech enhancement algorithms. After formulating 6 different processing methods, we evaluate the performance by SegSNR improvement using data recorded in a moving car environment. Among the 6 processing configurations, the combined fixed/adaptive beam-forming (CFA-BF) obtains the highest level of SegSNR improvement by up to 2.65 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-316"
  },
  "srinivasan04_interspeech": {
   "authors": [
    [
     "Sriram",
     "Srinivasan"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Speech enhancement using adaptive time-domain segmentation",
   "original": "i04_0869",
   "page_count": 4,
   "order": 318,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "In this paper, we investigate the benefits of using an adaptive segmentation of the speech signal in speech enhancement. The adaptive segmentation scheme divides the signal into the longest segments within which stationarity is preserved, thus providing a good time-frequency resolution. The segmentation is performed with the help of an orthogonal library of local cosine bases using a computationally efficient tree-structured best-basis search. We show that such an adaptive segmentation results in improved speech enhancement compared to a fixed segmentation. The resulting enhanced speech is free from musical noise, without any additional smoothing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-317"
  },
  "nakatani04_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Masato",
     "Miyoshi"
    ],
    [
     "Parham S.",
     "Zolfaghari"
    ]
   ],
   "title": "Harmonicity based monaural speech dereverberation with time warping and F0 adaptive window",
   "original": "i04_0873",
   "page_count": 4,
   "order": 319,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "Although a number of dereverberation methods have been reported, dereverberation is still a challenging problem especially when a single microphone is used. To overcome this problem, we proposed a harmonicity based dereverberation method (HERB). HERB can blindly estimate the inverse filter of a room transfer function based on the harmonicity of speech signals and dereverberate the signals. However, HERB uses an imprecise assumption that hinders the dereverberation performance, that is, the fundamental frequency (F0) of a speech signal is assumed to be constant within a short time frame when extracting the features of harmonic components. In this paper, we combine HERB with time warping analysis and an F0 adaptive window to remove this bottleneck. This extension makes it possible to estimate harmonic components precisely even when their frequencies change rapidly. Experiments show that time warping analysis with an F0 adaptive window can effectively improve the dereverberation effect of HERB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-318"
  },
  "delcroix04_interspeech": {
   "authors": [
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Takafumi",
     "Hikichi"
    ],
    [
     "Masato",
     "Miyoshi"
    ]
   ],
   "title": "Dereverberation of speech signals based on linear prediction",
   "original": "i04_0877",
   "page_count": 4,
   "order": 320,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "This paper proposes an algorithm for the blind dereverberation of speech signals based on a two-channel linear prediction. Traditional dereverberation methods usually achieve good performance when the input signal is white noise. However, when dealing with colored signals generated by an autoregressive (AR) process such as speech, the generating AR process is deconvolved causing excessive whitening of the signal. This paper proposes a blind dereverberation algorithm that recovers speech signals suffering from deterioration due to the reverberation in a room. We overcome the whitening problem faced by traditional methods by estimating the generating AR process and applying this estimated AR process to the whitened signal. Simulation results show the great potential of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-319"
  },
  "campbell04_interspeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Perception of affect in speech - towards an automatic processing of paralinguistic information in spoken conversation",
   "original": "i04_0881",
   "page_count": 4,
   "order": 321,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "This paper reports a study of the perception of affective information in conversational speech utterances and shows that there are consistent differences in the acoustic features of same-word utterances that are perceived as having different discourse effects or displaying different affective states. We propose that rather than selecting one label to describe each utterance, a vector of activations across a range of features may be more appropriate. This finding complicates the representation of speech elements, but offers a more appropriate description of their attributes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-320"
  },
  "chateau04_interspeech": {
   "authors": [
    [
     "Noel",
     "Chateau"
    ],
    [
     "Valerie",
     "Maffiolo"
    ],
    [
     "Christophe",
     "Blouin"
    ]
   ],
   "title": "Analysis of emotional speech in voice mail messages: the influence of speakers' gender",
   "original": "i04_0885",
   "page_count": 4,
   "order": 322,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "This paper deals with the analysis of the emotional content of voice messages left on an answering machine, in the context of a customer care service. The 103 messages which constitute the corpus are split into 478 emotional phases. These phases are then used as material for a subjective test and a signal analysis. The relation between subjective data and acoustical data is then explored with a focus drawn on inter-subject agreements obtained in the subjective test and on speakers' gender. The results show that the acoustical parameters which are relevant to differentiate emotions for male speakers differ from those able to differentiate emotions for female speakers, but also depend of the inter-subject agreement, which governs the number of observations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-321"
  },
  "lee04k_interspeech": {
   "authors": [
    [
     "Chul Min",
     "Lee"
    ],
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Zhigang",
     "Deng"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Emotion recognition based on phoneme classes",
   "original": "i04_0889",
   "page_count": 4,
   "order": 323,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "Recognizing human emotions/attitudes from speech cues has gained increased attention recently. Most previous work has focused primarily on suprasegmental prosodic features calculated at the utterance level for modeling against details at the segmental phoneme level. Based on the hypothesis that different emotions have varying effects on the properties of the different speech sounds, this paper investigates the usefulness of phoneme-level modeling for the classification of emotional states from speech. Hidden Markov models (HMM) based on short-term spectral features are used for this purpose using data obtained from a recording of an actress' expressing 4 different emotional states - anger,happiness, neutral, and sadness. We designed and compared two sets of HMM classifiers: a generic set of \"emotional speech\" HMMs (one for each emotion) and a set of broad phonetic-class based HMMs for each emotion type considered. Five broad phonetic classes were used to explore the effect of emotional coloring on different phoneme classes, and it was found that spectral properties of vowel sounds were the best indicator of emotions in terms of the classification performance. The experiments also showed that the better performance can be obtained by using phoneme-class classifiers than generic \"emotional\" HMM classifier and classifiers based on global prosodic features. To see the complementary effect of the prosodic and spectral features, the two classifiers were combined at the decision level. The improvement was 0.55% in absolute (0.7% relatively) compared with the result from phoneme-class based HMM classifier.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-322"
  },
  "robinson04_interspeech": {
   "authors": [
    [
     "Peter",
     "Robinson"
    ],
    [
     "Tal Sobol",
     "Shikler"
    ]
   ],
   "title": "Visualizing dynamic features of expressions in speech",
   "original": "i04_0893",
   "page_count": 4,
   "order": 324,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "In this paper we examine some of the issues involved in analyzing the expression of emotions, mental states and attitudes in speech. We investigate timing issues that may affect the structure of future automated inference systems. These timing issues include tracking transitions and changes over time within expressions, existence of thresholds between expressions, and the necessity of tracking time-frequency features within an utterance. We also suggest an outline for a system that includes these features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-323"
  },
  "li04c_interspeech": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Haibo",
     "Wang"
    ]
   ],
   "title": "Friendly speech analysis and perception in standard Chinese",
   "original": "i04_0897",
   "page_count": 4,
   "order": 325,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "Previous analyses on friendly speech were made using dialogues without differentiating their linguistic functions. This paper reports an analysis on declarative and interrogative sentences respectively. Pitch and duration of prosodic words were statistically analyzed and compared concerning factors of their positions and stresses. Based on the acoustic investigation of friendly speech, tonal pitch and prosodic duration distributions were adjusted in synthesis and the synthesized stimuli were subjected to perception test. It was found that: (1) Friendliness of synthesized speech could be achieved via adjusting the perceptually distinctive acoustic parameters; (2) Tonal pitch is the most important cue for better expression of friendliness; (3) Only adjusting duration is no use for expressive friendly speech; (4) Interrogative sentences got higher perceptual results than declarative sentences; (5) A high boundary tone for interrogative sentence was usually used by speakers to express friendly speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-324"
  },
  "nichasaide04_interspeech": {
   "authors": [
    [
     "Ailbhe",
     "Ní Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Decomposing linguistic and affective components of phonatory quality",
   "original": "i04_0901",
   "page_count": 4,
   "order": 326,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "This paper is concerned with the role of phonatory quality in signalling affect. An overview of perception experiments is presented, which used synthetic stimuli with different phonatory qualities and f0 contours in order to explore the mapping of voice quality to affect as well as the way in which voice quality combines with f0. Results highlight the need for these phonetic parameters to be considered together. To identify the phonatory correlates of affect, we also need to understand the substrate of voice source variation, due to the linguistic content of utterances (prosodic and segmental) as well as to speaker specific characteristics. Illustrations of the former type of variation are presented, based on source parameterisation of inverse filtered data. A holistic analytic approach is advocated, which incorporates the main phonetic dimensions (voice quality, f0 and temporal parameters) and which integrates the affective dimension with the more linguistic dimension of prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-325"
  },
  "jiang04b_interspeech": {
   "authors": [
    [
     "Dan-Ning",
     "Jiang"
    ],
    [
     "Lian-Hong",
     "Cai"
    ]
   ],
   "title": "Classifying emotion in Chinese speech by decomposing prosodic features",
   "original": "i04_1325",
   "page_count": 4,
   "order": 327,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "Prosodic features have been proven important to discriminate between different speech emotions, but they also have a fundamental linguistic function. Variations caused by linguistic contexts act as noises in emotion classification and should be eliminated. The paper proposes a novel method to decompose the raw mixed prosodic features into features determined by linguistic contexts and those responsible for emotionality, and the latter are further used exclusively in emotion classification. In the method, features determined by linguistic contexts are first predicted based on the analysis of neutral speech through Generalized Regression Neural Network (GRNN), and Linear Discriminant Analysis (LDA) is then applied to accomplish the decomposition. Experiments on Chinese emotional speech have shown that the emotional features estimated through feature decomposition have a better discrimination between different emotions, and could achieve much higher classification accuracy than raw features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-326"
  },
  "yu04b_interspeech": {
   "authors": [
    [
     "Chen",
     "Yu"
    ],
    [
     "Paul",
     "Aoki"
    ],
    [
     "Allison",
     "Woodruff"
    ]
   ],
   "title": "Detecting user engagement in everyday conversations",
   "original": "i04_1329",
   "page_count": 4,
   "order": 328,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "This paper presents a novel application of speech emotion recognition: estimation of the level of conversational engagement between users of a voice communication system. We begin by using machine learning techniques, such as the support vector machine (SVM), to classify users' emotions as expressed in individual utterances. However, this alone fails to model the temporal and interactive aspects of conversational engagement. We therefore propose the use of a multilevel structure based on coupled hidden Markov models (CHMM) to estimate engagement levels in continuous natural speech. The first level is comprised of SVM-based classifiers that recognize emotional states, which could be discrete emotion types or arousal/valence levels. A high-level HMM then uses these emotional states as input, estimating users' engagement in conversation by decoding the internal states of the HMM. We report experimental results obtained by applying our algorithms to the LDC Emotional Prosody and CallFriend speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-327"
  },
  "fujisawa04_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fujisawa"
    ],
    [
     "Norman D.",
     "Cook"
    ]
   ],
   "title": "Identifying emotion in speech prosody using acoustical cues of harmony",
   "original": "i04_1333",
   "page_count": 4,
   "order": 329,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "We have studied the prosody of emotional speech using a psychoacoustical model of musical harmony (designed to explain the basic facts of the perception of pitch combinations: interval consonance/dissonance and chordal harmony/tension). For any voiced utterance, the model provides 3 quasi-musical measures: dissonance, tension, and harmonic modality of the pitches used. Modality is the most interesting, as it relates to the major and minor modes of traditional harmony theory and their characteristic positive and negative affect. In a study of emotional speech using 216 utterances, factor analysis showed that these measures are distinct from those obtained from basic statistics on the fundamental frequency of the voice (mean F0, range, rate of change, etc.). Moreover, there was a significant correlation between the major/minor modality measure and the positive/ negative affect of the utterance. We argue that, in addition to the traditional acoustical measures, a measure of multiple-pitch combinations, i.e., harmony, is essential for determining the affective character of the tone of voice in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-328"
  },
  "tao04_interspeech": {
   "authors": [
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Context based emotion detection from text input",
   "original": "i04_1337",
   "page_count": 4,
   "order": 330,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "Emotion detection was normally conducted from the viewpoint of prosody and articulation features. There is still an opening question on how to extract the emotion from the text input. To solve the problem, the paper generates an emotion estimation net (ESiN), which combines the content words and emotion functional words to estimate the final emotion output. In the paper, emotion functional words are also classified into emotional keyword, modifier word and metaphor word. To make more detailed word classification, some context information was analyzed. Both experiments and cross tests show that the method could generate the good results for emotion detection from text input.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-329"
  },
  "iwai04_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Iwai"
    ],
    [
     "Yoshikazu",
     "Yano"
    ],
    [
     "Shigeru",
     "Okuma"
    ]
   ],
   "title": "Complex emotion recognition system for a specific user using SOM based on prosodic features",
   "original": "i04_1341",
   "page_count": 4,
   "order": 331,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "This paper proposes complex emotion recognition system for a specific user, where complex emotion has mingled emotions. In order to show the differences between individuals, we use Self-Organizing Feature Map(SOM) for proposed system. Additionaly, in order that emotion recognition system expresses complex emotion, we propose new method for labeling. We verify proposed system using emotional speech database we recorded. As a result of verifying, this system could express fuzziness emotions such as anger with sadness, and this paper showed the effectiveness for emotion recognition to specify the user.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-330"
  },
  "cho04d_interspeech": {
   "authors": [
    [
     "Hoon-Young",
     "Cho"
    ],
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Te-Won",
     "Lee"
    ]
   ],
   "title": "Emotion verification for emotion detection and unknown emotion rejection",
   "original": "i04_1345",
   "page_count": 4,
   "order": 332,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper focuses on detection of a single emotion and verification of a specific emotion type in a test utterance. To utilize a probabilistic output of a classifier as well as to exploit various long term acoustic features, we built a probabilistic output SVM and applied several approximated log likelihood ratio tests for emotion verification. Experimental results on SUSAS and AIBO emotion database show that anger and sadness are easier emotions to be detected than boredom and happiness. Results also verifies the efficacy of applying log likelihood ratio with respect to neutral emotion as a measure for emotion verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-331"
  },
  "hirose04_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Improvement in corpus-based generation of F0 contours using generation process model for emotional speech synthesis",
   "original": "i04_1349",
   "page_count": 4,
   "order": 333,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "In our fully automatic corpus-based method of generating fundamental frequency (F0) contours for emotional speech synthesis, an improvement was realized related to the process of corpus preparation. The method assumes the generation process model and predicts its command parameters using binary regression trees with inputs of linguistic information of the sentence to be synthesized. Because of the model constraint, a certain quality is still kept in synthesized speech even if the prediction is done incorrectly. The speech corpus includes three types of emotional speech (anger, joy, sadness) and calm speech uttered by a female narrator. The command parameters necessary for the training (and testing) of the method were automatically extracted from speech using a program developed by the authors. Since the accuracy of the extraction largely affects the prediction performance, a constraint is newly applied on the position of phrase commands during the extraction. Also, since performance of phrase command prediction dominates the overall accuracy of generated F0 contours, the method was modified to predict phrase commands first. The mismatches between the predicted and target contours for angry speech were similar to those for calm speech. Synthesis of emotional speech was conducted with text inputs. The segmental features were handled by the HMM synthesis method and the phoneme durations are predicted in a similar corpus-based method. Perceptual experiment was conducted using the synthesized speech, and the result indicated that the anger could be well conveyed by the developed method. The result came worse for joy and sadness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-332"
  },
  "hegde04_interspeech": {
   "authors": [
    [
     "Rajesh Mahanand",
     "Hegde"
    ],
    [
     "Hema A.",
     "Murthy"
    ],
    [
     "Venkata Ramana Rao",
     "Gadde"
    ]
   ],
   "title": "Continuous speech recognition using joint features derived from the modified group delay function and MFCC",
   "original": "i04_0905",
   "page_count": 4,
   "order": 334,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "Feature extraction and selection for continuous speech recognition is a complex task. State of the art speech recognition systems use features that are derived by ignoring the Fourier transform phase. In our earlier studies we have shown the efficacy of The Modified Group Delay Feature (MODGDF) derived from the Fourier transform phase for phoneme, syllable and speaker recognition. In this paper we use the MODGDF and the popular MFCC derived from Fourier transform magnitude to compute joint features for continuous speech recognition of two Indian languages Tamil and Telugu. A novel method of segmentation of the continuous speech signal into syllable like units followed by isolated style recognition using HMMs is used. We further use an innovative technique which transforms the problem of detecting the correct string of syllabic units with maximum likelihood to finding an optimal state sequence locally. The recognition system does not use any language models. The MODGDF gave promising recognition performance for the two languages and compared well with the MFCC. Joint features derived using MODGDF and MFCC gave a 10.6% improvement for both Tamil and Telugu languages. The improvement reinforces the hypothesis that MODGDF captures complementary information to that of the MFCC and can be used along with the MFCC to capture the complete information in the speech signal at functional level and help in avoiding heavy auditory and language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-333"
  },
  "yu04c_interspeech": {
   "authors": [
    [
     "Hua",
     "Yu"
    ]
   ],
   "title": "Phase-space representation of speech",
   "original": "i04_0909",
   "page_count": 4,
   "order": 335,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "Speech production is essentially a nonlinear dynamic process. Motivated by ideas in dynamic system research, this paper seeks to recast the speech representation problem (front-end) as an attempt to reconstruct the phase space of the production process, or articulatory configurations. We point out that the use of the delta and double delta features, common in current ASR (Automatic Speech Recognition) systems, corresponds to time-delayed embedding, a technique in nonlinear time series analysis for phase space reconstruction. The traditional delta and double features also impose a suboptimal linear transform in the reconstructed space. We show that a significant improvement in recognition accuracy can be achieved by choosing the transform in a data-driven fashion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-334"
  },
  "murthy04_interspeech": {
   "authors": [
    [
     "Hema A.",
     "Murthy"
    ],
    [
     "Rajesh Mahanand",
     "Hegde"
    ],
    [
     "Venkata Ramana Rao",
     "Gadde"
    ]
   ],
   "title": "The modified group delay feature: a new spectral representation of speech",
   "original": "i04_0913",
   "page_count": 4,
   "order": 336,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "Automatic recognition of speech by machines begins with extraction of meaningful features from the speech signal. Conventional features like the MFCC are derived from the Fourier transform magnitude spectrum, while totally ignoring the phase spectrum. The importance of the Modified group delay feature (MODGDF) derived from the Fourier transform phase spectrum for speaker and phoneme recognition has been presented in our previous efforts. In this paper we try to analyse the feature theoretically and provide justifications in terms of de-correlation, robustness to convolutional and white noise, cluster structures, separability in lower dimensional space, task independence and class separability. The results of speaker identification and continuous speech recognition using the MODGDF as the front end are also presented. Joint features derived from the MODGDF and MFCC gave significant improvements in recognition performance for both speaker and continuous speech recognition tasks. Using the analytical results in the first half of the paper and the results of performance evaluation in the second half, the MODGDF is proposed as an alternative spectral representation of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-335"
  },
  "kwon04_interspeech": {
   "authors": [
    [
     "Oh-Wook",
     "Kwon"
    ],
    [
     "Te-Won",
     "Lee"
    ]
   ],
   "title": "ICA-based feature extraction for phoneme recognition",
   "original": "i04_0917",
   "page_count": 4,
   "order": 337,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "We propose a new scheme to reduce phase sensitivity in independent component analysis (ICA)-based feature extraction using an analytical description of the ICA-adapted basis functions. Furthermore, since the basis functions are not shift invariant, we extend the method to include a spectral-domain ICA stage that removes redundant time shift information. The performance of the new scheme is evaluated for TIMIT phoneme recognition and compared with the standard mel frequency cepstral coefficient (MFCC) feature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-336"
  },
  "zhu04b_interspeech": {
   "authors": [
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Barry",
     "Chen"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "On using MLP features in LVCSR",
   "original": "i04_0921",
   "page_count": 4,
   "order": 338,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "One of the major research thrusts in the speech group at ICSI is to use Multi-Layer Perceptron (MLP) based features in automatic speech recognition (ASR). This paper presents a study of three aspects of this effort: 1) the properties of the MLP features which make them useful, 2) incorporating MLP features together with PLP features in ASR, and 3) possible redundancy between MLP features and more conventional system refinements such as discriminative training and system combination. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. Recognition results show that MLP features can significantly improve recognition performance in large vocabulary continuous speech recognition (LVCSR) tasks for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even when discriminative training and system combination are used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-337"
  },
  "chen04b_interspeech": {
   "authors": [
    [
     "Barry",
     "Chen"
    ],
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Learning long-term temporal features in LVCSR using neural networks",
   "original": "i04_0925",
   "page_count": 4,
   "order": 339,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "Incorporating long-term (~500-1000 ms) temporal information using multi-layered perceptrons (MLPs) has improved performance on ASR tasks, especially when used to complement traditional shortterm (~25-100 ms) features. This paper further studies techniques for incorporating long-term temporal information in the acoustic model by presenting experiments showing: 1) that simply widening acoustic context by using more frames of full band speech energies as input to the MLP is suboptimal compared to a more constrained two-stage approach that first focuses on long-term temporal patterns in each critical band separately and then combines them, 2) that the best two-stage approach studied utilizes hidden activation values of MLPs trained on the log critical band energies (LCBEs) of 51 consecutive frames, and 3) that combining the best two-stage approach with conventional short-term features significantly reduces word error rates on the 2001 NIST Hub-5 conversational telephone speech (CTS) evaluation set with models trained using the Switchboard Corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-338"
  },
  "sreenivas04_interspeech": {
   "authors": [
    [
     "T. V.",
     "Sreenivas"
    ],
    [
     "G. V.",
     "Kiran"
    ],
    [
     "A. G.",
     "Krishna"
    ]
   ],
   "title": "Neural \"spike rate spectrum\" as a noise robust, speaker invariant feature for automatic speech recognition",
   "original": "i04_0929",
   "page_count": 4,
   "order": 340,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "A new feature set for ASR called Rate-Spectrum(RS) is proposed. RS is a spectral representation obtained using a computational auditory model. The feature is noise-robust and considerably speaker invariant. RS matches the smoothed log spectrum both in shape and dynamic range variation. DCT is used to reduce dimensionality. Comparison of the proposed features with MFCC is done using an Isolated word recognition experiment on the TI Digits database, for clean and noisy speech cases. For speakers seen during training, RS and RS-DCT outperform MFCC in noisy case while matching that of MFCC in the clean case. For unseen speakers, RS does better than MFCC in the clean case, RS-DCT outperforms MFCC in the noisy case. We have thus shown that the proposed feature for ASR is noise robust and speaker invariant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-339"
  },
  "nakatoh04_interspeech": {
   "authors": [
    [
     "Yoshihisa",
     "Nakatoh"
    ],
    [
     "Makoto",
     "Nishizaki"
    ],
    [
     "Shinichi",
     "Yoshizawa"
    ],
    [
     "Maki",
     "Yamada"
    ]
   ],
   "title": "An adaptive MEL-LPC analysis for speech recognition",
   "original": "i04_0933",
   "page_count": 4,
   "order": 341,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "A Mel-LPC analysis is effective in speech recognition because of their auditory like frequency resolution. However, the spectral resolution is equal at all the frequency band. We proposed an Adaptive Mel-LPC Analysis Method (AMLPC). In AMLPC, the spectral resolution is difference according to phoneme category (vowel, fricative, etc). First, power normalized 1st-order auto-correlation r[1] that represents the phoneme category is calculated from the input spectrum. Secondly, the frequency warped parameter \"alpha\" is calculated using r[1]. The alpha is the coefficient of all-pass filter in MLPC, and is used to control the frequency resolution of spectral envelope. Finally, the warped predictors are obtained by each alpha parameter. The recognition performance of cepstrum parameters obtained by AMLPC was compared with that of the standard MLPC cepstrum through speaker independent word recognition. The results show that the AMLPC cepstrum leads to the improvement of error rate about 10% for the standard MLPC cepstrum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-340"
  },
  "ishizuka04_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Improvement in robustness of speech feature extraction method using sub-band based periodicity and aperiodicity decomposition",
   "original": "i04_0937",
   "page_count": 4,
   "order": 342,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "This paper shows improvements in robustness of a speech feature extraction method using Sub-band based Periodicity and Aperiodicity DEcomposition or SPADE. With SPADE, the speech signal is divided into sub-band signals through bandpass filter banks, after which the sub-band signal is decomposed into its periodic and aperiodic features by the comb filter. The evaluation experiment conducted with AURORA-2J (Japanese AURORA-2) shows that SPADE degrades the performance under open-channel condition. To cope with this problem, in this paper we apply the cepstral mean normalization (CMN) to SPADE. The result shows that CMN greatly improves the performance not only for test data under the open-channel condition but also for data under the closed-channel condition. SPADE with CMN achieves an averaged word accuracy of 89.96 %, and an averaged WER reduction of 28.61 %. This word accuracy is better than that achieved by using MFCC with CMN.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-341"
  },
  "ishi04_interspeech": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ]
   ],
   "title": "A new acoustic measure for aspiration noise detection",
   "original": "i04_0941",
   "page_count": 4,
   "order": 343,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "In this paper, we propose a new acoustic measure for detecting aspiration noise in vowels. The measure is an index of synchronization between frequency bands around the first and third formants. The measure is based on the principle that the vocal tract responses to the glottal excitation are synchronized between these frequency bands when aspiration noise is absent, and uncorrelated otherwise. Evaluation results show that the proposed measure can be used together with spectral slope measures for automatic detection of aspiration noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-342"
  },
  "demuynck04_interspeech": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Oscar",
     "Garcia"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Synthesizing speech from speech recognition parameters",
   "original": "i04_0945",
   "page_count": 4,
   "order": 344,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "The merits of different signal preprocessing schemes for speech recognizers are usually assessed purely on the basis of the resulting recognition accuracy. Such benchmarks give a good indication as to whether one preprocessing is better than another, but little knowledge is acquired about why it is better or how it could be further improved. In order to gain more insight in the preprocessing, we seek to re-synthesize speech from speech recognition features. This way, we are able to pin-point some deficiencies in our current preprocessing scheme. Additional analysis of successful new preprocessing schemes may allow us one day to identify precisely those properties that are desirable in a feature set. Next to these purely scientific aims, the re-synthesis of speech from recognition features is of interest to thin-client speech applications, and as an alternative to the classical LPC source-filter model for speech manipulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-343"
  },
  "athineos04_interspeech": {
   "authors": [
    [
     "Marios",
     "Athineos"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Daniel P.W.",
     "Ellis"
    ]
   ],
   "title": "LP-TRAP: linear predictive temporal patterns",
   "original": "i04_0949",
   "page_count": 4,
   "order": 345,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "Auto-regressive modeling is applied for approximating the temporal evolution of spectral density in critical-band-sized sub-bands of a segment of speech signal. The generalized auto-correlation linear predictive technique allows for a compromise between fitting the peaks and the troughs of the Hilbert envelope of the signal in the sub-band. The cosine transform coefficients of the approximated sub-band envelopes, computed recursively from the all-pole polynomials, are used as inputs to a TRAP-based speech recognition system and are shown to improve recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-344"
  },
  "li04d_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Richard",
     "Stern"
    ]
   ],
   "title": "Parallel feature generation based on maximizing normalized acoustic likelihood",
   "original": "i04_0953",
   "page_count": 4,
   "order": 346,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "Combining information from parallel feature streams generally improves speech recognition accuracy. While many studies have attempted to determine the stage of the recognition system that provides best combination performance and the specific nature of how features are combined, relatively little attention has been paid to the design or selection of parallel feature sets when used in combination. In this paper we propose a new parallel feature generation algorithm based on the criterion of maximizing the normalized acoustic likelihood of the features after they are combined, which is closely related to the recognition accuracy obtained using the combination of these features. We use a gradient ascent procedure to manipulate the values of a set of transformation matrices through which individual features are passed before they are combined in a fashion that maximizes the normalized acoustic likelihood term after the features are combined. The function that combine the parallel features together is an intrinsic part of the optimization process. The use of the optimal linear transformation provides a relative decrease of 12.7 percent Word Error Rate on the DARPA Resource Management task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-345"
  },
  "wang04h_interspeech": {
   "authors": [
    [
     "Kun-Ching",
     "Wang"
    ]
   ],
   "title": "An adaptive band-partitioning spectral entropy based speech detection in realistic noisy environments",
   "original": "i04_0957",
   "page_count": 4,
   "order": 347,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "Generally, the feature parameters used for speech detection are highly sensitive to the environment. The performance of speech detection is severely degraded under realistic noisy environments since the characteristics of a speech signal cannot be fully expressed by those feature parameters. As a result, this study seeks the acoustic fingerprints of speech spectrogram as a robust feature to distinguish a speech from a non-speech, especially in adverse environments, and the fact that the frequency energies of difference types of noise are concentrated on different frequency bands, an ABSE-based speech detection algorithm is proposed to detect speech signals in adverse environments. Additionally, the ABSE-based algorithm is demonstrated to work in real-time with minimal processing delay. Experimental results indicate that the ABSE parameter is very effective for several SNRs and various noise conditions. Furthermore, the proposed ABSE-based algorithm outperforms other approaches and is reliable in a real car.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-346"
  },
  "ramirez04_interspeech": {
   "authors": [
    [
     "Javier",
     "Ramirez"
    ],
    [
     "José Carlos",
     "Segura"
    ],
    [
     "Carmen",
     "Benitez"
    ],
    [
     "Angel de la",
     "Torre"
    ],
    [
     "Antonio",
     "Rubio"
    ]
   ],
   "title": "Improved voice activity detection combining noise reduction and subband divergence measures",
   "original": "i04_0961",
   "page_count": 4,
   "order": 348,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "Currently, new trends in wireless communications are demanding reliable human-machine interaction in real-life environments. However, there are obstacles inhibiting automatic speech recognition systems (ASR) working in noisy environments. The main difficulty is the degradation suffered by ASR systems due to a mismatch between training and test conditions. This paper shows an improved voice activity detector (VAD) combining noise reduction and subband divergence estimation for improving the reliability of speech recognizers operating in noisy environments. The algorithm formulates the decision rule by measuring the divergence between the subband spectral magnitude of speech and noise using the Kullback-Leibler (KL) distance on the denoised signal. Experiments demonstrate a sustained advantage over different VAD methods including standard VADs such as G.729 and AMR, which are used as a reference, recently reported algorithms, and the VADs of the advanced frontend (AFE) for distributed speech recognition (DSR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-347"
  },
  "park04d_interspeech": {
   "authors": [
    [
     "Kiyoung",
     "Park"
    ],
    [
     "Changkyu",
     "Choi"
    ],
    [
     "Jeongsu",
     "Kim"
    ]
   ],
   "title": "Voice activity detection using global soft decision with mixture of Gaussian model",
   "original": "i04_0965",
   "page_count": 4,
   "order": 349,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "An improvement on the voice detection algorithm using global soft decision (GSD) is made in this paper. In GSD method, the speech and noise are modelled by the presumed probability density function, e.g. Gaussian pdf. We propose that the estimation and modelling of the signal is done in the domain of filterbank output which widely used in most speech processing applications. Since the output of filterbank is the weighted sum of outputs of several frequency bins, the signals can no longer be estimated using the Gaussian models but mixture of Gaussian models (GMM) in general. It is shown that the estimation of speech absence probability with GMM gives better performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-348"
  },
  "kemp04_interspeech": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Yin Hay",
     "Lam"
    ],
    [
     "Josep Maria Sola i",
     "Caros"
    ]
   ],
   "title": "Environmental robust features for speech detection",
   "original": "i04_0969",
   "page_count": 4,
   "order": 350,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "In this paper, two novel features, Line Spectrum Center Range and Line Spectrum Flux, both derived from Line Spectrum Frequencies, are proposed to detect the presence of speech in various acoustic environments. Evaluation results using Fischer Discriminant Analysis and Scatter Matrices indicated that the new features excel the state-of-the-art features. An environmental robust hybrid feature set including the proposed features, Normalized Energy Dynamic Range and Mel-Frequency Cepstrum Coefficients is further introduced. When evaluating the hybrid feature set on a Gaussian Mixture Model based classification engine, the results showed that the hybrid feature set outperformed Mel-Frequency Cepstrum Coefficients up to 49% in terms of relative frame error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-349"
  },
  "laskowski04_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Crosscorrelation-based multispeaker speech activity detection",
   "original": "i04_0973",
   "page_count": 4,
   "order": 351,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "We propose an algorithm for segmenting multispeaker meeting audio, recorded with personal channel microphones, into speech and non-speech intervals for each microphone's wearer. An algorithm of this type turns out to be necessary prior to subsequent audio processing because, in spite of close-talking microphones, the channels exhibit a high degree of crosstalk due to unbalanced calibration and small inter-speaker distance. The proposed algorithm is based on the shorttime crosscorrelation of all channel pairs. It requires no prior training and executes in one fifth real time on modern architectures. Using meeting audio collected at several sites, we present error rates for the segmentation task which do not appear correlated with microphone type or number of speakers. We also present the resulting improvement in speech recognition accuracy when segmentation is provided by this algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-350"
  },
  "tsai04c_interspeech": {
   "authors": [
    [
     "Shang-nien",
     "Tsai"
    ]
   ],
   "title": "Improved robustness of time-frequency principal components (TFPC) by synergy of methods in different domains",
   "original": "i04_0977",
   "page_count": 4,
   "order": 352,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "Our previously proposed integration of time-frequency principal components (TFPC) features and histogram equalization (HEQ) has improved the robustness of TFPC features under mismatched conditions. To further enhance the robustness of TFPC features, we herein propose to (a) replace HEQ with another feature normalization technique, progressive histogram equalization (PHEQ), (b) combine a spectral noise reduction method, two-stage Wiener filter, and (c) add a temporal robustness algorithm, SNR-dependent waveform processing, which enhances not only the overall SNR but also the periodicity of noisy speech waveform. Although with the same goal of reducing the performance gap caused by noise, these algorithms mentioned above do operate in different domains, and therefore their synergy significantly improves the robustness of TFPC features. Extensive experiments with respect to AURORA2 database are conducted to verify the effectiveness of each technique, and the overall feature extraction scheme gives a relative error reduction of 25.17 % over our previous proposal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-351"
  },
  "deng04_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Yu",
     "Dong"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "A quantitative model for formant dynamics and contextually assimilated reduction in fluent speech",
   "original": "i04_0981",
   "page_count": 4,
   "order": 353,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "A quantitative model of coarticulation is presented that accurately predicts formant dynamics in fluent speech using the prior information of resonance targets in the phone sequence, in absence of actual acoustic data. Realistic formant undershoot (reduction) and \"static\" sound confusion is produced naturally from the model for fast-rate speech in a contextually assimilated manner. The model developed is capable of resolving the confusion with dynamic speech specification. As a source of a-priori knowledge about the speech structure, the model is a central component of our Bayesian generative modeling approach to automatic recognition of conversational speech, where varying degrees of sound reduction abound due to the free-varying speaking style and rate. We present details of the model simulation that demonstrates quantitative effects of speaking rate and segment duration on the magnitude of reduction, agreeing closely with experimental measurement results in the acoustic-phonetic literature. The model simulation also gives quantitative effects of varying the \"stiffness\" parameter in the model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-352"
  },
  "kubin04b_interspeech": {
   "authors": [
    [
     "Gernot",
     "Kubin"
    ],
    [
     "Van Tuan",
     "Pham"
    ]
   ],
   "title": "DWT-based classification of acoustic-phonetic classes and phonetic units",
   "original": "i04_0985",
   "page_count": 4,
   "order": 354,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "In this paper, we describe a new algorithm based on the discrete wavelet transform (DWT) which uses a multi-threshold decision model (MTD model) to detect acoustic and phonetic classes (based on 10ms speech signal segments). The best thresholds of the model are found by using experimental pattern classification. Then a unit level interpolation technique is combined with the MTD model to classify phonetic units (based on sequences of 10ms segments). The results of the classifiers are compared and jointly adjusted by an interactive scheme (IS) in order to improve the performance of the algorithm. The algorithm is tested with the TIMIT database and compared with the SUB-CRA-based algorithm and other algorithms to demonstrate its effectiveness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-353"
  },
  "cho04e_interspeech": {
   "authors": [
    [
     "Yong-Choon",
     "Cho"
    ],
    [
     "Seungjin",
     "Choi"
    ]
   ],
   "title": "Learning nonnegative features of spectro-temporal sounds for classification",
   "original": "i04_0989",
   "page_count": 4,
   "order": 355,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "In this paper we present a method of sound classification which exploits a parts-based representation of spectro-temporal sounds, employing the nonnegative matrix factorization (NMF). We illustrate a new way of learning nonnegative features using a variant of NMF and show its useful behavior in the task of general sound classification with comparison to independent component analysis (ICA) which produces holistic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-354"
  },
  "chung04b_interspeech": {
   "authors": [
    [
     "Sungyup",
     "Chung"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "N-gram language modeling of Japanese using bunsetsu boundaries",
   "original": "i04_0993",
   "page_count": 4,
   "order": 356,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "A new scheme of N-gram language modeling was proposed for Japanese, where word N-grams were calculated separately for the two cases: crossing and not crossing bunsetsu boundaries. Here, bunsetsu is a basic grammatical (and pronunciation) unit of Japanese. Similar scheme using accent phrase boundaries instead of bunsetsu boundaries has already been proposed by the authors with a certain success, but it suffered from the training data shortage, because assignment of accent phrase boundaries requires a speech corpus. In contrast, bunsetsu boundaries can be detected automatically from a written text with a rather high accuracy using parsers. Experiments showed that perplexity reduction and word recognition rate improvement, especially in case of small training corpus, were possible by estimating bunsetsu boundaries from the history longer than N-1 words in the case of N-gram modeling and by selecting one from two types of models (crossing and not crossing bunsetsu boundaries) according to the estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-355"
  },
  "chen04c_interspeech": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "Dynamic language modeling for broadcast news",
   "original": "i04_0997",
   "page_count": 4,
   "order": 357,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "This paper describes some recent experiments on unsupervised language model adaptation for transcription of broadcast news data. In previous work, a framework for automatically selecting adaptation data using information retrieval techniques was proposed. This work extends the method and presents experimental results with unsupervised language model adaptation. Three primary aspects are considered: (1) the performance of 5 widely used LM adaptation methods using the same adaptation data is compared; (2) the influence of the temporal distance between the training and test data epoch on the adaptation efficiency is assessed; and (3) show-based language model adaptation is compared with story-based language model adaptation. Experiments have been carried out for broadcast news transcription in English and Mandarin Chinese. A relative word error rate reduction of 4.7% was obtained in English and a 5.6% relative character error rate reduction in Mandarin with story-based MDI adapation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-356"
  },
  "lyu04_interspeech": {
   "authors": [
    [
     "Ren-Yuan",
     "Lyu"
    ],
    [
     "Dau-Cheng",
     "Lyu"
    ],
    [
     "Min-Siong",
     "Liang"
    ],
    [
     "Min-Hong",
     "Wang"
    ],
    [
     "Yuang-Chin",
     "Chiang"
    ],
    [
     "Chun-Nan",
     "Hsu"
    ]
   ],
   "title": "A unified framework for large vocabulary speech recognition of mutually unintelligible Chinese \"regionalects\"",
   "original": "i04_1001",
   "page_count": 4,
   "order": 358,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "In this paper, a new approach is proposed for recognizing speech of mutually unintelligible spoken Chinese regionalects based on a unified three-layer framework and a one-stage searching strategy. This framework includes (1) a unified acoustic model for all the considered regionalects; (2) a multiple pronunciation lexicon constructed by both a rule-based and a data-driven approaches; (3) a one-stage searching network, whose nodes represent the Chinese characters with their multiple pronunciations. Unlike the traditional approaches, the new approach avoids searching the intermediate local optimal syllable sequences or lattices. Instead, by using the Chinese characters as the searching nodes, the new approach can search to find the globally optimal character sequences directly. This paper reports the experiments on two of the Chinese regionalects, i.e., Taiwanese and Mandarin. Results show that the unified framework can efficiently deal with the issues of multiple pronunciations of the spoken Chinese regionalects. The character error reduction rate is 34.1%, which is achieved by using the new approach compared with the traditional two-stage scheme. Furthermore, the new approach is shown more robust when dealing with the poor uttered speech database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-357"
  },
  "sluis04_interspeech": {
   "authors": [
    [
     "Ielka van der",
     "Sluis"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "The influence of target size and distance on the production of speech and gesture in multimodal referring expressions",
   "original": "i04_1005",
   "page_count": 4,
   "order": 359,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "In this paper we report on a production experiment for multimodal referring expressions. Subjects performed an object identification task in an interactive setting. 20 subjects participated and were asked if they could identify 30 countries on a world map on the wall. Subjects performed their tasks on two distances: close (10 subjects) and at a distance of 2.5 meters (10 subjects). The assumption is that these conditions yield precise and imprecise pointing gestures respectively. In addition we varied the 'size' of target objects (large or isolated objects versus small objects). This study resulted in a corpus of 600 multimodal referring expressions. A statistical analysis (ANOVA) revealed a main effect of distance (subjects adapt their language to the kind of pointing gesture) and also a main effect of target (smaller objects are more difficult to describe than large or isolated objects).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-358"
  },
  "gupta04_interspeech": {
   "authors": [
    [
     "Anurag Kumar",
     "Gupta"
    ],
    [
     "Tasos",
     "Anastasakos"
    ]
   ],
   "title": "Dynamic time windows for multimodal input fusion",
   "original": "i04_1009",
   "page_count": 4,
   "order": 360,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "Natural interaction in multimodal dialogue systems demands quick system response after the end of a user turn. The prediction of the end of user input at each multimodal dialog turn is complicated as users can interact through modalities in any order, and convey a variety of different messages to the system within the turn. Several multimodal interaction frameworks have used fixed-duration time windows to address this problem. We conducted a user study to evaluate the use of fixed-duration time windows and motivate further improvements. This paper describes a probabilistic method for computing an adaptive time window for multimodal input fusion. The goal is to adjust the time window dynamically depending on the user, task, and the number of multimodal inputs for each turn. Experimental results show that the resulting system has superior performance when compared to a system with fixed-duration time windows.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-359"
  },
  "lee04l_interspeech": {
   "authors": [
    [
     "Raymond H.",
     "Lee"
    ],
    [
     "Anurag Kumar",
     "Gupta"
    ]
   ],
   "title": "MICot : a tool for multimodal input data collection",
   "original": "i04_1013",
   "page_count": 4,
   "order": 361,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "In this paper, a multi-modal data collection tool called MICoT is described. We highlight the various design and implementation aspects that we consider to be important for MICoT. An example is given to illustrate the application of the tool to collect data for our research in multi-modal dialog system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-360"
  },
  "tadj04_interspeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Hicham",
     "Djenidi"
    ],
    [
     "Madjid",
     "Haouani"
    ],
    [
     "Amar",
     "Ramdane-Cherif"
    ],
    [
     "Nicole",
     "Levy"
    ]
   ],
   "title": "Simulating multimodal applications",
   "original": "i04_1017",
   "page_count": 4,
   "order": 362,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "A design methodology for multimodal-controlled application has been developed using Wizard-of-Oz simulations as the principal mechanism for evaluating and getting input for dialogue design. This methodology may enable multimodal application developers to support dialogues that are optimal with respect to naturalness, especially on a pragmatic level, given the technical restrictions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-361"
  },
  "pedersen04_interspeech": {
   "authors": [
    [
     "Jakob Schou",
     "Pedersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "A multimodal communication aid for global aphasia patients",
   "original": "i04_1021",
   "page_count": 4,
   "order": 363,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "This paper presents the basic rationale behind the development and testing of a multimodal communication aid especially designed for people suffering from global aphasia, and thus having severe expressive difficulties. The principle of the aid is to trigger patient associations by presenting various multimodal representations of communicative expressions. The aid can in this way be seen as a conceptual continuation of previous research within the field of communication aids based on uni-modal (pictorial) representations of communicative expressions. As patients suffering from global aphasia seldom have identical symptoms, the focus of this paper is placed on the development of a highly dedicated communication aid adaptive to the individual patients' needs. The paper investigates whether or not such a highly dedicated communication aid based on multimodal representations of communicative expressions can be used to support patients with global aphasia in communicating by means of short sentences with their surroundings. Only a limited evaluation is carried out, and as such no statistically significant results are obtained. The tests however indicate that the aid is capable of supporting a global aphasia patient in participating in conversations based on short sentences - which otherwise would be impossible without the use of a communication aid.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-362"
  },
  "yamamoto04_interspeech": {
   "authors": [
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Mis-recognized utterance detection using hierarchical language model",
   "original": "i04_1025",
   "page_count": 4,
   "order": 364,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "In this paper, a mis-recognized utterance detection and modification scheme is proposed to recover speech recognition errors in speech translation. In a speech recognition stage, mis-recognition is frequently observed. The most of mis-recognitions result from mismatch of acoustic models and out-of-vocabulary (OOV) words. To cope with both acoustic model mis-match and OOVs, we adopt a hierarchical language model to identify them. A hierarchical language model can generate both hypotheses with and without OOVs (or acoustic mis-matched words). Likelihood difference of these hypotheses is used as utterance confidence measure. To confirm the possibility of this scheme, as a first experiment, we have conducted speech recognition experiments and mis-recognized utterance detection. Experiment results showed 99% detection rate for utterances with OOVs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-363"
  },
  "moberg04_interspeech": {
   "authors": [
    [
     "Marko",
     "Moberg"
    ],
    [
     "Kimmo",
     "Parssinen"
    ],
    [
     "Juha",
     "Iso-Sipila"
    ]
   ],
   "title": "Cross-lingual phoneme mapping for multilingual synthesis systems",
   "original": "i04_1029",
   "page_count": 4,
   "order": 365,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "Development of a multilingual text-to-speech (TTS) system requires usually a lot of time, effort and language resources. The implementation tends to consume large amounts of memory as the number of supported languages increases. This paper proposes a simple method for quickly increasing the language portfolio of an existing TTS system with the minimal effort and memory consumption. The cross-lingual phoneme mapping modifies the phonetic transcription of a new language by presenting it with the phoneme set supported by an existing TTS system. The synthesis output of the mapped language is quite accurate phonetically but the intonation is based on the existing target language. The evaluations showed that the cross-lingual phoneme mapping could provide an adequate quality synthesis when the language portfolio must be rapidly expanded and the memory and the language resources are scarce. The method is best applied for the synthesis of short utterances including names and isolated words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-364"
  },
  "komatani04_interspeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Tsuyoshi",
     "Tasaki"
    ],
    [
     "Takeshi",
     "Yamaguchi"
    ]
   ],
   "title": "Robot motion control using listener's back-channels and head gesture information",
   "original": "i04_1033",
   "page_count": 4,
   "order": 366,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "A novel method is described for robot gestures and utterances during a dialogue based on the listener's understanding and interest, which are recognized from back-channels and head gestures. \"Back-channels\" are defined as sounds like \"uh-huh\" uttered by a listener during a dialogue, and \"head gestures\" are defined as nod and tilt motions of the listener's head. The back-channels are recognized using sound features such as power and fundamental frequency. The head gestures are recognized using the movement of the skin-color area and the optical flow data. Based on the estimated understanding and interest of the listener, the speed and size of robot motions are changed. This method was implemented in a humanoid robot called SIG2.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-365"
  },
  "sakti04_interspeech": {
   "authors": [
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Arry Akhmad",
     "Arman"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Paulus",
     "Hutagaol"
    ]
   ],
   "title": "Indonesian speech recognition for hearing and speaking impaired people",
   "original": "i04_1037",
   "page_count": 4,
   "order": 367,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "This paper outlines our efforts in developing Indonesian speech recognition for hearing and speaking impaired people. The lack of speech-enabling technology and research, as well as a shortage of data on the Indonesian language presents a major challenge for us to deal with. Difficulties arise in developing an Indonesian speech corpus since Indonesian is actually most people's second language after their own ethnic native language. Collecting all of the possible languages and dialects of the tribes recognized in Indonesia is still the biggest problem we face. In speech recognition, segmented utterances according to labels are usually used as a starting point for training speech models. This segmentation strategy is also one of the main issues. Initialization training utterances with flat segmentation would not give sufficient performance. Here, we used an English speech recognizer to set initial segmentation of Indonesian utterances. This method produced a significant improvement of up to 40% in performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-366"
  },
  "rashwan04_interspeech": {
   "authors": [
    [
     "Mohsen",
     "Rashwan"
    ]
   ],
   "title": "A two phase arabic language model for speech recognition and other language applications",
   "original": "i04_1041",
   "page_count": 4,
   "order": 368,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "A new language model for Arabic language for large vocabulary automatic speech recognition (ASR) is introduced. The derivative future of the Arabic word is quite useful in dividing the process into two phases. In phase-1 the fixed words, the prefix, the suffix and the form of the derivative words are determined through phase-1M-gram, of course, given the acoustical data. In phase 2 another M-gram is used to determine the roots of the derivative words. The idea was tested on 60 words (10 roots x 6 forms). Results are encouraging the idea, and more work is to follow to realize a complete large vocabulary ASR for Arabic language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-367"
  },
  "akita04_interspeech": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Language model adaptation based on PLSA of topics and speakers",
   "original": "i04_1045",
   "page_count": 4,
   "order": 369,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "We address an adaptation method of statistical language models to topics and speaker characteristics for automatic transcription of meetings and discussions. A baseline language model is a mixture of two models, which are trained with different corpora covering various topics and speakers, respectively. Then, probabilistic latent semantic analysis (PLSA) is performed on the same respective corpora and the initial ASR result to provide unigram probabilities conditioned on input speech. Finally, the baseline model is adapted by scaling N-gram probabilities with these unigram probabilities. For speaker adaptation purpose, we make use of spontaneous speech corpus (CSJ) in which a large number of speakers gave talks for given topics. Experimental evaluation with real discussions showed that both topic and speaker adaptation improved test-set perplexity and word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-368"
  },
  "dolfing04_interspeech": {
   "authors": [
    [
     "Hans J. G. A.",
     "Dolfing"
    ],
    [
     "Pierce Gerard",
     "Buckley"
    ],
    [
     "David",
     "Horowitz"
    ]
   ],
   "title": "Unified language modeling using finite-state transducers with first applications",
   "original": "i04_1049",
   "page_count": 4,
   "order": 370,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "In this paper, we investigate a weighted finite-state transducer approach to language modelling for speech recognition applications. We explore a unified framework to conversational speech recognition which combines the benefits of grammars, n-gram and class-based language models, with the flexibility of using dynamic data, and the potential for integrating semantics. Based on a virtual personal assistant application, we show first applications and recognition results of out-of-grammar handling and the integration of class-based, weighted, dynamic data into this framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-369"
  },
  "itou04_interspeech": {
   "authors": [
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Atsushi",
     "Fujii"
    ],
    [
     "Tomoyosi",
     "Akiba"
    ]
   ],
   "title": "Effects of language modeling on speech-driven question answering",
   "original": "i04_1053",
   "page_count": 4,
   "order": 371,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "We integrate automatic speech recognition (ASR) and question answering (QA) to realize a speech-driven QA system, and evaluate its performance. We adapt an N-gram language model to natural language questions, so that the input of our system can be recognized with a high accuracy. We target WH-questions which consist of the topic part and fixed phrase used to ask about something. We first produce a general N-gram model intended to recognize the topic and emphasize the counts of the N-grams that correspond to the fixed phrases. Given a transcription by the ASR engine, the QA engine extracts the answer candidates from target documents. We propose a passage retrieval method robust against recognition errors in the transcription. We use the QA test collection produced in NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness of our method by means of experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-370"
  },
  "sethy04_interspeech": {
   "authors": [
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Measuring convergence in language model estimation using relative entropy",
   "original": "i04_1057",
   "page_count": 4,
   "order": 372,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "Language models are generally estimated using smoothed counting techniques. These counting schemes can be viewed as non linear functions operating on a Bernoulli process which converge asymptotically to the true density. The rate at which these counting schemes converge to the true density is constrained by the traning data set available and the nature of the language model (LM) being estimated. In this paper we look at language model estimates as random variables and present a efficient relative entropy (R.E) based approach to study their convergence with increasing training data size. We present experimental results for language modeling in a generic LVCSR system and a medical domain dialogue task. We also present an efficient recursive R.E computation method which can be used as a LM distance measure for a number of tasks including LM clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-371"
  },
  "huang04c_interspeech": {
   "authors": [
    [
     "Rongqing",
     "Huang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "High-level feature weighted GMM network for audio stream classification",
   "original": "i04_1061",
   "page_count": 4,
   "order": 373,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "The problem of unsupervised audio classification continuous to be a challenging research problem which significantly impacts ASR and Spoken Document Retrieval (SDR) performance. This paper addresses novel advances in audio classification for speech recognition. A new algorithm is proposed for audio classification, which is based on Weighted GMM Network (WGN). Two new highlevel features: VSF (Variance of the Spectrum Flux) and VZCR (Variance of the Zero-Crossing Rate) are used to pre-classify the audio and supply weights to the output probabilities of the GMM networks. The classification is then implemented using weighted GMM networks. Evaluations on a standard data set --- DARPA Hub4 Broadcast News 1997evaluation data, shows that the WGN classification algorithm achieves over a 50% improvement versus the GMM network baseline algorithm. The WGN also obtains very satisfactory results on the more diverse and challenging NGSW (National Gallery of the Spoken Word) corpus. Classification based on segmentation method is also explored.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-372"
  },
  "zdansky04_interspeech": {
   "authors": [
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Petr",
     "David"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "An improved preprocessor for the automatic transcription of broadcast news audio stream",
   "original": "i04_1065",
   "page_count": 4,
   "order": 374,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "This paper deals with the preprocessing of the broadcast news (BN) audio stream for the automatic transcription purposes. The preprocessing consists of the automatic segmentation followed by the broad-class segment identification. The former is capable of detecting speaker and/or acoustic changes in the BN audio stream with the precision being 82.75%. The latter acts as a filter that removes non-speech parts. The performance of the proposed system was evaluated on the multi-lingual pan-European COST278 BN database containing data in 6 languages. The preprocessing and segmentation module operates in a near-real-time way, with the total delay of 12 seconds. Its practical functionality was evaluated on the Czech part of the BN database. The automatically segmented signal was directly sent to the large vocabulary continuous speech recognition system operating with a 200K-word Czech lexicon. The difference in performance between automatically and manually segmented BN streams was only minimal - 1.12%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-373"
  },
  "wang04i_interspeech": {
   "authors": [
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Chi-Han",
     "Huang"
    ]
   ],
   "title": "Speaker-and-environment change detection in broadcast news using the common component GMM-based divergence measure",
   "original": "i04_1069",
   "page_count": 4,
   "order": 375,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "In this paper, a GMM with common mixture components, referred to as the common component GMM (CCGMM), is proposed to be the signal model for calculating the diversity measure for the speaker-and-environment change detection in broadcast news signal. The use of GMM is to increase the accuracy of audio signal modeling while the use of common mixture components is to solve the complexity problem of parameter estimation and similarity measure evaluation. Experimental results on a TV broadcast news database showed that it outperformed a BIC-based method. A MDR of 21.9% with 16.0% FAR was achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-374"
  },
  "lahti04_interspeech": {
   "authors": [
    [
     "Tommi",
     "Lahti"
    ]
   ],
   "title": "Beginning of utterance detection algorithm for low complexity ASR engines",
   "original": "i04_1073",
   "page_count": 4,
   "order": 376,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "In this paper, a novel method for beginning of utterance detection is proposed for low complexity ASR systems. Assuming MFCC calculations in the ASR front-end, the additional computational load due to the algorithm is negligible. The algorithm makes use of the delay between the MFCC calculation and decoding process, which is typical in front-ends with feature normalization. The main steps of the algorithm involve LDA projection of MFCC features, mean calculation over the projected features, simple implicit SNR estimation and weighting of the decision statistics according to the estimate. Our experimental results show that high performance is obtained down to fairly low SNR conditions as the beginning of utterance detection starts to fail in a safe way at about 5 dB SNR. These properties make the algorithm an attractive choice for low complexity ASR engines.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-375"
  },
  "sukittanon04_interspeech": {
   "authors": [
    [
     "Somsak",
     "Sukittanon"
    ],
    [
     "Arun C.",
     "Surendran"
    ],
    [
     "John C.",
     "Platt"
    ],
    [
     "Chris J.C.",
     "Burges"
    ]
   ],
   "title": "Convolutional networks for speech detection",
   "original": "i04_1077",
   "page_count": 4,
   "order": 377,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "In this paper, we introduce a new framework for speech detection using convolutional networks. We propose a network architecture that can incorporate long and short-term temporal and spectral correlations of speech in the detection process. The proposed design is able to address many shortcomings of existing speech detectors in a unified new framework: First, it improves the robustness of the system to environmental variability while still being fast to evaluate. Second, it allows for a framework that is extendable to work under different time-scales for different applications. Finally, it is discriminative and produces reliable estimates of the probability of presence of speech in each frame for a wide variety of noise conditions. We propose that the inputs to the system be features that are measures of the true signal-to-noise ratio of a set of frequency bands of the signal. These can be easily and automatically generated by tracking the noise spectrum online. We present preliminary results on the AURORA database to demonstrate the effectiveness of the detector over conventional Gaussian detectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-376"
  },
  "gangashetty04_interspeech": {
   "authors": [
    [
     "Suryakanth V.",
     "Gangashetty"
    ],
    [
     "Chellu",
     "Chandra Sekhar"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Detection of vowel on set points in continuous speech using autoassociative neural network models",
   "original": "i04_1081",
   "page_count": 4,
   "order": 378,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "Detection of vowel onset points (VOPs) is important for spotting subword units in continuous speech. For consonant-vowel (CV) utterances, VOP is the instant at which the consonant part ends and the vowel part begins. Accurate detection of VOPs is important for recognition of CV units in continuous speech. In this paper, we propose an approach for detection of VOPs using autoassociative neural network (AANN) models. A pair of AANN models are trained for each CV class to capture the characteristics of speech signal in the consonant and vowel regions of that class. The trained AANN models are then used to detect VOPs in continuous speech. The results of studies show that the proposed approach leads to significantly less number of spurious hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-377"
  },
  "tamiya04_interspeech": {
   "authors": [
    [
     "Toshiki",
     "Tamiya"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Reconstruction filter design for bone-conducted speech",
   "original": "i04_1085",
   "page_count": 4,
   "order": 379,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "Bone-conducted speech is of low intelligibility, but the quality is not affected by noise. In this paper, we take into account such properties of bone-conducted speech, and address a digital filter to reconstruct the quality of the bone-conducted speech signal obtained from a speaker. The reconstruction filter design method is derived based on a model assumption of pronunciation. Experimental results show that the reconstructed speech signal has better quality than the bone-conducted speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-378"
  },
  "quintanamorales04_interspeech": {
   "authors": [
    [
     "Pedro J.",
     "Quintana-Morales"
    ],
    [
     "Juan L.",
     "Navarro-Mesa"
    ]
   ],
   "title": "Frequency warped ARMA analysis of the closed and the open phase of voiced speech",
   "original": "i04_1089",
   "page_count": 4,
   "order": 380,
   "p1": "1089",
   "pn": "1192",
   "abstract": [
    "We propose a frequency warped version of a pole-zero analysis obtained from several periods of voiced speech. We approach the estimation of the coefficients associated to the poles and zeros by minimizing a cost function based on the 'warped' reconstruction error. The results reinforce our previous work letting us an extension of the initial equations by introducing the concept of auditory perception in the formulation. This facilitates an improvement in the analysis of the phases associated to consecutive periods. With the experiments we address several objectives. First, to evaluate the importance of the time extension due to warping. Second, to obtain an optimum warping factor from a reconstruction error point of view. Third, to study the behaviour of our analysis with the period length. And fourth, to study the distribution of the error in frequency. Our results indicate that the use of warped techniques is beneficial in speech analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-379"
  },
  "doval04_interspeech": {
   "authors": [
    [
     "Boris",
     "Doval"
    ],
    [
     "Baris",
     "Bozkurt"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "Zeros of z-transform (ZZT) decomposition of speech for source-tract separation",
   "original": "i04_1093",
   "page_count": 4,
   "order": 381,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "This study proposes a new spectral decomposition method for source-tract separation. It is based on a new spectral representation called the Zeros of Z-Transform (ZZT), which is an all-zero representation of the z-transform of the signal. We show that separate patterns exist in ZZT representations of speech signals for the glottal flow and the vocal tract contributions. The ZZT-decomposition is simply composed of grouping the zeros into two sets, according to their location in the z-plane. This type of decomposition leads to separating glottal flow contribution (without a return phase) from vocal tract contributions in z domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-380"
  },
  "deng04b_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Roberto",
     "Togneri"
    ]
   ],
   "title": "Use of neural network mapping and extended kalman filter to recover vocal tract resonances from the MFCC parameters of speech",
   "original": "i04_1097",
   "page_count": 4,
   "order": 382,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "In this paper, we present a state-space formulation of a neural-network-based hidden dynamic model of speech whose parameters are trained using an approximate EM algorithm. The training makes use of the results of an off-the-shelf formant tracker (during the vowel segments) to simplify the complex sufficient statistics that would be required in the exact EM algorithm. The trained model, consisting of the state equation for the target-directed vocal tract resonance (VTR) dynamics on all classes of speech sounds (including consonant closure) and the observation equation for mapping from the VTR to acoustic measurement, is then used to recover the unobserved VTR based on Extended Kalman Filter. The results demonstrate accurate estimation of the VTRs, especially those during rapic consonant-vowel or vowel-consonant transitions and during consonant closure when the acoustic measurement alone provides weak or no information to infer the VTR values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-381"
  },
  "li04e_interspeech": {
   "authors": [
    [
     "Xiao",
     "Li"
    ],
    [
     "Jonathan",
     "Malkin"
    ],
    [
     "Jeff",
     "Bilmes"
    ]
   ],
   "title": "Graphical model approach to pitch tracking",
   "original": "i04_1101",
   "page_count": 4,
   "order": 383,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "Many pitch trackers based on dynamic programming require meticulous design of local cost and transition cost functions. The forms of these functions are often empirically determined and their parameters are tuned accordingly. Parameter tuning usually requires great effort without a guarantee of optimal performance. This work presents a graphical model framework to automatically optimize pitch tracking parameters in the maximum likelihood sense. Therein, probabilistic dependencies between pitch, pitch transition and acoustical observations are expressed using the language of graphical models, and probabilistic inference is accomplished using the Graphical Model Toolkit (GMTK). Experiments show that this framework not only expedites the design of a pitch tracker, but also yields remarkably good performance for both pitch estimation and voicing decision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-382"
  },
  "xu04b_interspeech": {
   "authors": [
    [
     "Bo",
     "Xu"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Yongguo",
     "Kang"
    ]
   ],
   "title": "A new multicomponent AM-FM demodulation with predicting frequency boundaries and its application to formant estimation",
   "original": "i04_1105",
   "page_count": 4,
   "order": 384,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "In this paper, a method using dynamic programming to predict frequency boundaries is proposed for AM-FM demodulation for speech signals. An algorithm called energy separation algorithm (ESA) has been developed to track the energy needed by a source to produce the speech signal, and this algorithm provides an efficient solution to separate output energy product into amplitude modulation and frequency modulation components. For multicomponent AM-FM signals like speech signals, a bank of bandpass filters or a set of individual bandpass filters, whose center frequency and critical bandwidth commonly are selected through experiential selection, is necessary to get monocomponent signals. Our experimental results provide that the bandpass filter with predicted frequency boundaries instead of experiential selection is more effective in AM-FM demodulation. Formant estimation based on this demodulating method also proves that it is efficient and formant tracking algorithm is not necessary at all in the estimating procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-383"
  },
  "laprie04_interspeech": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "A concurrent curve strategy for formant tracking",
   "original": "i04_2405",
   "page_count": 4,
   "order": 385,
   "p1": "2405",
   "pn": "2408",
   "abstract": [
    "Although automatic formant tracking has a wide range of potential applications it is still an open problem. We previously proposed the use of active curves that deform under the influence of the spectrogram energy. Each formant was tracked independently and a complex strategy was required to guarantee the overall formant tracking consistency. This paper describes how the interdependency between formants can be incorporated directly during the deformations of formant tracks. Iterative processes attached to each formant are interlaced. We experimented two strategies. The first consists in partitioning the spectrogram into exclusive regions, each region affiliated to a given formant. The second consists in adding a repulsion force between formants that prevents formant tracks to merge together. It turns out that the second strategy is more robust and does not necessitate a complex control strategy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-384"
  },
  "yan04b_interspeech": {
   "authors": [
    [
     "Qin",
     "Yan"
    ],
    [
     "Esfandiar",
     "Zavarehei"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Dimitrios",
     "Rentzos"
    ]
   ],
   "title": "A formant tracking LP model for speech processing",
   "original": "i04_2409",
   "page_count": 4,
   "order": 386,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "This paper investigates the modeling and estimation of spectral parameters at formants of noisy speech in the presence of car and train noise. Formant estimation using two-dimensional hidden Markov models (2D-HMM) is reviewed and employed to study the influence of noise on observations of formants. The first set of experimental results presented show the influence of car and train noise on the distribution and the estimates of the formant trajectories. Due to the shapes of the spectra of speech and car/train noise, the 1st formant is most affected by noise and the last formant is least affected. The effects of inclusion of formant features in speech recognition at different SNRs are presented. It is shown that formant features provide better performance at low SNRs compared to MFCC features. Finally, for robust estimation of noisy speech, a formant tracking method based on combination of LP-spectral subtraction and Kalman filter is presented. Average formant tracking errors at different SNRs are computed and the results show that after noise reduction the formant tracking errors of 1st formant are reduced by 60%. The de-noised formant tracking LP models can be used for recognition and/or enhancement of noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-385"
  },
  "you04_interspeech": {
   "authors": [
    [
     "Hong",
     "You"
    ]
   ],
   "title": "Application of long-term filtering to formant estimation",
   "original": "i04_2413",
   "page_count": 4,
   "order": 387,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "We propose a formant analysis algorithm that works well for highpitched speakers. The algorithm reduces the influence of pitch frequency on formant analysis. A pitch-synchronized long-term filter is optimized and applied to speech signals before LPC analysis. A weighted LPC analysis method is proposed to compute the auto-regression model parameters and hence the formants. Processing synthetic speech and natural speech show that the spectra of the longterm filtered residue exhibit the formant structure better than those of the speech signal, especially when the pitch frequency is high. A comparison between formant tracking by the proposed algorithm and ESPS waves+ shows promising results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-386"
  },
  "bozkurt04_interspeech": {
   "authors": [
    [
     "Baris",
     "Bozkurt"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Boris",
     "Doval"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ]
   ],
   "title": "A method for glottal formant frequency estimation",
   "original": "i04_2417",
   "page_count": 4,
   "order": 388,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "This study presents a method for estimation of glottal formant frequency (Fg) from speech signals. Our method is based on zeros of z-transform decomposition of speech spectra into two spectra : glottal flow dominated spectrum and vocal tract dominated spectrum. Peak picking is performed on the amplitude spectrum of the glottal flow dominated part. The algorithm is tested on synthetic speech. It is shown to be effective especially when glottal formant and first formant of vocal tract are not too close. In addition, tests on a real speech example are also presented where open quotient estimates from EGG signals are used as reference and correlated with the glottal formant frequency estimates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-387"
  },
  "bozkurt04b_interspeech": {
   "authors": [
    [
     "Baris",
     "Bozkurt"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Boris",
     "Doval"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ]
   ],
   "title": "Improved differential phase spectrum processing for formant tracking",
   "original": "i04_2421",
   "page_count": 4,
   "order": 389,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "This study presents an improved version of our previously introduced formant tracking algorithm. The algorithm is based on processing the negative derivative of the argument of the chirp-z transform (termed as the differential phase spectrum) of a given speech signal. No modeling is included in the procedure but only peak picking on differential phase spectrum. We discuss the effects of roots of z-transform to differential phase spectrum and the need to ensure that all zeros are at some distance from the circle where chirp-z transform is computed. For that, we include an additional zero-decomposition step in our previously presented algorithm to improve its robustness. The final version of the algorithm is tested for analysis of synthetic speech and real speech signals and compared to two other formant tracking systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-388"
  },
  "shao04_interspeech": {
   "authors": [
    [
     "Xu",
     "Shao"
    ],
    [
     "Ben P.",
     "Milner"
    ]
   ],
   "title": "MAP prediction of pitch from MFCC vectors for speech reconstruction",
   "original": "i04_2425",
   "page_count": 4,
   "order": 390,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "This work proposes a method of predicting pitch and voicing from mel-frequency cepstral coefficient (MFCC) vectors. Two maximum a posteriori (MAP) methods are considered. The first models the joint distribution of the MFCC vector and pitch using a Gaussian mixture model (GMM) while the second method also models the temporal correlation of the pitch contour using a combined hidden Markov model (HMM)-GMM framework. Monophone-based HMMs are connected together in the form of an unconstrained monophone grammar which enables pitch to be predicted from unconstrained speech input. Evaluation on 130,000 MFCC vectors reveals a voicing classification accuracy of over 92% and an RMS pitch error of 10Hz. The predicted pitch contour is also applied to MFCC-based speech reconstruction with the resultant speech almost indistinguishable from that reconstructed using a reference pitch.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-389"
  },
  "yu04d_interspeech": {
   "authors": [
    [
     "An-Tze",
     "Yu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "New harmonicity measures for pitch estimation and voice activity detection",
   "original": "i04_2429",
   "page_count": 4,
   "order": 391,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "Harmonic structure can be easily recognized in the time-frequency representation of speech signals even in diverse environment. The harmonicity is a measure of the completeness of harmonic structure. This paper extends the use of conventional harmonicity measure to the tasks of pitch estimation and voice activity detection. A set of hierarchical harmonicities, including grid, temporal, spectral and segmental harmonicities, is derived for this purpose. A series of experiments are conducted to show the effectiveness of using harmonicities in speech processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-390"
  },
  "nishimoto04_interspeech": {
   "authors": [
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ]
   ],
   "title": "Multi-pitch trajectory estimation of concurrent speech based on harmonic GMM and nonlinear kalman filtering",
   "original": "i04_2433",
   "page_count": 4,
   "order": 392,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "This paper describes a multi-pitch tracking algorithm of 1-channel simultaneous multiple speech. The algorithm selectively carries out the two alternative processes at each frame: frame-independent-process and frame-dependent-process. The former is the one we have previously proposed, that gives good estimates of the number of speakers and F0s with a single-frame-processing. The latter corresponds to the topic mainly described in this paper, that recursively tracks F0s using nonlinear Kalman filtering. We tested our algorithm on simultaneous speech signal data and showed higher performance than when the frame-independent-process was only used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-391"
  },
  "ferencz04_interspeech": {
   "authors": [
    [
     "Attila",
     "Ferencz"
    ],
    [
     "Jeongsu",
     "Kim"
    ],
    [
     "Yong-Beom",
     "Lee"
    ],
    [
     "Jae-Won",
     "Lee"
    ]
   ],
   "title": "Automatic pitch marking and reconstruction of glottal closure instants from noisy and deformed electro-glotto-graph signals",
   "original": "i04_2437",
   "page_count": 4,
   "order": 393,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "Pitch tracking and pitch marking (PM) are two important speech signal analysis techniques for several applications. The accuracy of both pitch marking and tracking is significant to generate smooth synthesized speech by controlling the pitch and duration of voiced speech in Text-to-Speech (TTS) system for example. In this paper, we present a novel hybrid approach, combining electro-glotto-graph (EGG)-based PM and speech signal-based PM into a single framework, to acquire more reliable and automatic PM technique. Experimental results show that the PM performance of the suggested method is excellent being capable of determining Glottal Closure Instants (GCI) precisely even in the case of noisy EGG signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-392"
  },
  "flego04_interspeech": {
   "authors": [
    [
     "Federico",
     "Flego"
    ],
    [
     "Luca",
     "Armani"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "On the use of a weighted autocorrelation based fundamental frequency estimation for a multidimensional speech input",
   "original": "i04_2441",
   "page_count": 4,
   "order": 394,
   "p1": "2441",
   "pn": "2444",
   "abstract": [
    "The problem of computing the fundamental frequency F0 in an accurate way is a known and still partially unsolved problem, especially given a noisy speech input. In this work, a distant-talking scenario is addressed, where a distributed microphone network provides multi-channel input sequences to process for speaker modeling purposes. Given this context, one may process in an independent way each channel and then apply a majority vote or other fusion methods. Otherwise, the redundancy across the channels can be exploited jointly by processing the different signals to obtain a more reliable and robust F0 estimation. The paper investigates the use of a multi-channel version of a Weighted Autocorrelation(WAUTOC)-based F0 estimation technique. A postprocessing corrective step is introduced to improve the resulting F0 accuracy. Experiments conducted on a real database show the advantages and the robustness of the proposed method in extracting the fundamental frequency with no regard about the microphone and talker position as well as the head orientation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-393"
  },
  "reddy04_interspeech": {
   "authors": [
    [
     "Aarthi M.",
     "Reddy"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "A minimum mean squared error estimator for single channel speaker separation",
   "original": "i04_2445",
   "page_count": 4,
   "order": 395,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "The problem of separating out the signals for multiple speakers from a single mixed recording has received considerable attention in recent times. Most current techniques are based on the principle of masking: in order the separate out the signal for any speaker, frequency components that are not believed to belong to that speaker are suppressed. The signals for the various speakers are reconstructed from the partial spectral information that remains. In this paper we present a different kind of technique -- one that attempts to estimate all spectral components for the desired speaker. Separated signals are derived from the complete spectral descriptions so obtained. Experiments show that this method results in superior reconstruction to masking based reconstruction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-394"
  },
  "molla04_interspeech": {
   "authors": [
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Audio source separation from the mixture using empirical mode decomposition with independent subspace analysis",
   "original": "i04_2449",
   "page_count": 4,
   "order": 396,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "In this paper we decompose the Hilbert Spectrum of an audio mixture into a number of subspaces to segregate the sources. Empirical mode decomposition (EMD) together with Hilbert transform produces Hilbert spectrum (HS), which is a fine-resolution time-frequency representation of a non-stationary signal. EMD decomposes the mixture signal into some intrinsic oscillatory modes called intrinsic mode function (IMF). HS is constructed from the instantaneous frequency responses of IMFs. Some frequency independent basis vectors are derived using independent component analysis (ICA). Kulback-Laibler divergence based k-means clustering algorithm is proposed to group the basis vectors into number of desired sources. Then projecting HS on to the grouped basis vectors derives the independent source subspaces. The time domain source signals are assembled by applying some post processing on the subspaces. We have also produced some experimental results using our proposed separation algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-395"
  },
  "oh04b_interspeech": {
   "authors": [
    [
     "In-Jung",
     "Oh"
    ],
    [
     "Hyun-Yeol",
     "Chung"
    ],
    [
     "Jae-Won",
     "Cho"
    ],
    [
     "Ho-Youl",
     "Jung"
    ],
    [
     "R.",
     "Prost"
    ]
   ],
   "title": "Audio watermarking in sub-band signals using multiple echo kernels",
   "original": "i04_2453",
   "page_count": 4,
   "order": 397,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "In this paper, we propose a modified version of echo hiding in sub-band signals, which employs multiple echo kernel. This method allows reducing the distortion of the original audio signal, with respect to both subjective distortion (perceptibility) and objective distortion such as SNR(Signal to Noise Ratio). Through the experimental results, we proved that the use of multiple echo kernel gives good performances, in terms of SNR and detection rate of watermark, compared to the one of single echo kernel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-396"
  },
  "zhang04h_interspeech": {
   "authors": [
    [
     "Jie",
     "Zhang"
    ],
    [
     "Zhenyang",
     "Wu"
    ]
   ],
   "title": "A piecewise interpolation method based on log-least square error criterion for HRTF",
   "original": "i04_2457",
   "page_count": 4,
   "order": 398,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "This paper addresses the problem of accurately realizing the interpolation of spatially discrete head-related transfer function (HRTF) for synthesis of virtual auditory space. By analyzing the advantages and disadvantages of general bilinear interpolation method in 3D-sound, associating with human auditory system's mechanism of band-pass filtering and consulting critical bands of psychoacoustics, the paper presents a piecewise interpolation method based on log-least square error criterion for HRTF's magnitude to compensate the deficiency of bilinear method in intermediate frequency. As seen from the following simulations, this new method accomplishes preferable results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-397"
  },
  "navarromesa04_interspeech": {
   "authors": [
    [
     "Juan L.",
     "Navarro-Mesa"
    ],
    [
     "Pedro J.",
     "Quintana-Morales"
    ]
   ],
   "title": "Modified realizable frequency warped ARMA modeling and its application in synthesis structures for voiced speech",
   "original": "i04_2461",
   "page_count": 4,
   "order": 399,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "Synthesis filters are of major concern in many speech applications. In this paper these filters are designed so as to track the natural variations, parameterize the perceptually relevant aspects of the pole and zero characteristics and their dynamics, and achieve reliable estimates of the filter coefficients. In this paper this is attained integrating three basic points of view in a general framework. First, frequency warping incorporates the perceptually relevant characteristics of human hearing. Second, direct implementations of efficient synthesis structures of these filters are realized with modified versions that overcome the problems associated to the incorporation of first-order all-pass sections for frequency warping. And third, reliable coefficients associated to the synthesis structures of several consecutive periods are directly estimated in the analysis. We show that the synthesis structures we apply and their coefficients offer good signal to reconstruction error ratios and error distribution in frequency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-398"
  },
  "muralishankar04_interspeech": {
   "authors": [
    [
     "R.",
     "Muralishankar"
    ],
    [
     "A. G.",
     "Ramakrishnan"
    ],
    [
     "Lakshmish N.",
     "Kaushik"
    ]
   ],
   "title": "Time-scaling of speech using independent subspace analysis",
   "original": "i04_2465",
   "page_count": 4,
   "order": 400,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "We propose a new technique for modifying the time-scale of speech using Independent Subspace Analysis (ISA). To carry out ISA, the single channel mixture signal is converted to a time-frequency representation such as spectrogram. Here, the spectrogram is generated by taking Hartley or Wavelet transform on overlapped frames of speech. We do dimensionality reduction of the autocorrelated original spectrogram using singular value decomposition. Then, we use Independent component analysis to get unmixing matrix using JadeICA algorithm. It is then assumed that the overall spectrogram results from the superposition of a number of unknown statistically independent spectrograms. By using unmixing matrix, independent sources such as temporal amplitude envelopes and frequency weights can be extracted from the spectrogram. Timescaling of speech is carried out by resampling the independent temporal amplitude envelopes. We then obtain time-scaled independent spectrograms after multiplying the independent frequency weights with time-scaled temporal amplitude envelopes. Summing all these independent spectrograms and taking inverse Hartely or wavelet transform of the sum spectrogram to reconstruct and overlap-add the reconstructed time-domain signal to get the timescaled speech. The quality of the time-scaled speech has been analyzed using Modified Bark Spectral Distortion(MBSD). From the MBSD score, one can infer that the time-scaled signal is less distorted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-399"
  },
  "girin04_interspeech": {
   "authors": [
    [
     "Laurent",
     "Girin"
    ],
    [
     "Mohammad",
     "Firouzmand"
    ],
    [
     "Sylvain",
     "Marchand"
    ]
   ],
   "title": "Long term modeling of phase trajectories within the speech sinusoidal model framework",
   "original": "i04_2469",
   "page_count": 4,
   "order": 401,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "In this paper, the problem of modeling the trajectory of the phase of speech signal is addressed within the context of the sinusoidal model of speech. A global or long-term model of the trajectory of the phase of the partials is proposed for each entire voiced section of speech, contrary to standard models, which are defined on a frame-by-frame basis. The complete analysis-modeling-synthesis process is presented. We compare two basic long-term models, namely a polynomial and a DCT-based model, with classical (frame-by-frame) interpolation schemes, given that the analysis process is the same in all cases. Promising results are given and the interest of the presented models for speech coding and speech watermarking applications is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-400"
  },
  "soltani04_interspeech": {
   "authors": [
    [
     "Tina",
     "Soltani"
    ],
    [
     "Dave",
     "Hermann"
    ],
    [
     "Etienne",
     "Cornu"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Rob",
     "Brennan"
    ]
   ],
   "title": "An acoustic shock limiting algorithm using time and frequency domain speech features",
   "original": "i04_2473",
   "page_count": 4,
   "order": 402,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "The phenomenon of acoustic shock occurs when a headset user is subjected to an audio disturbance at an uncomfortable or unsafe level. In this paper, a new acoustic shock limiting algorithm is proposed which reduces the output level by utilizing both time and frequency domains limiting features. During acoustic shocks, the algorithm improves the intelligibility of the underlying speech and minimizes the artifacts by reducing the shock in the subbands based on the narrowband and broadband characteristics of the input signal. The algorithm is implemented on a low power DSP system where the input data is analyzed in both time and frequency domains. The method is tested for sinusoidal and speech inputs. The results demonstrate shock compression and limiting, with reduced speech distortion during shock onset and good overall speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-401"
  },
  "shin04_interspeech": {
   "authors": [
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Speech probability distribution based on generalized gama distribution",
   "original": "i04_2477",
   "page_count": 4,
   "order": 403,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "In this paper, we propose a new speech probability distribution, twosided generalized gamma distribution (G_D) for an efficient parametric characterization of speech spectra. G_D forms a generalized class of parametric distributions including the Gaussian, Laplacian and Gamma probability density functions (pdf's) as special cases. All the parameters associated with the G_D are estimated by the on-line tracking procedure according to the maximum likelihood principle. Likelihoods, coefficients of variation (CV's), and Kolmogorov-Smirnov (KS) tests show that G_D can model the distribution of the real speech signal more accurately than the conventional Gaussian, Laplacian, Gamma pdf or generalized Gaussian distribution (GGD).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-402"
  },
  "zheng04c_interspeech": {
   "authors": [
    [
     "Yanli",
     "Zheng"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Sarah",
     "Borys"
    ]
   ],
   "title": "Stop consonant classification by dynamic formant trajectory",
   "original": "i04_2481",
   "page_count": 4,
   "order": 404,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "LPC analysis is one of the most powerful techniques in speech analysis. Spectral zeros during consonant or consonant-vowel transition regions introduce difficulties in estimating LPC parameters. In this paper, we propose to estimate formant frequencies from LPC model by MUSIC (Multiple Signal Classification) and ESPRIT (Estimation of Signal Parameters via Rotational Invariance Techniques). Formant candidates estimated by LS (Least Square), MUSIC and ESPRIT are combined to find an optimal solution. The effectiveness of this algorithm is verified by place classification task of stop consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-403"
  },
  "shiga04_interspeech": {
   "authors": [
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Estimating detailed spectral envelopes using articulatory clustering",
   "original": "i04_2485",
   "page_count": 4,
   "order": 405,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "This paper presents an articulatory-acoustic mapping where detailed spectral envelopes are estimated. During the estimation, the harmonics of a range of F0 values are derived from the spectra of multiple voiced speech signals vocalized with similar articulator settings. The envelope formed by these harmonics is represented by a cepstrum, which is computed by fitting the peaks of all the harmonics based on the weighted least square method in the frequency domain. The experimental result shows that the spectral envelopes are estimated with the highest accuracy when the cepstral order is 48-64 for a female speaker, which suggests that representing the real response of the vocal tract requires high-quefrency elements that conventional speech synthesis methods are forced to discard in order to eliminate the pitch component of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-404"
  },
  "engwall04b_interspeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "From real-time MRI to 3d tongue movements",
   "original": "i04_1109",
   "page_count": 4,
   "order": 406,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "Real-time Magnetic Resonance Imaging (MRI) at 9 images/s of the midsagittal plane is used as input to a three-dimensional tongue model, previously generated based on sustained articulations imaged with static MRI. The aim is two-fold, firstly to use articulatory inversion to extrapolate the midsagittal tongue movements to threedimensional movements, secondly to determine the accuracy of the tongue model in replicating the real-time midsagittal tongue shapes. The evaluation of the inversion shows that the real-time midsagittal contour is reproduced with acceptable accuracy. This means that the 3D model can be used to represent real-time articulations, eventhough the artificially sustained articulations on which it was based were hyperarticulated and had a backward displacement of the tongue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-405"
  },
  "nakamura04_interspeech": {
   "authors": [
    [
     "Mitsuhiro",
     "Nakamura"
    ]
   ],
   "title": "Coarticulatory variability and directionality in [s,..]: an EPG study",
   "original": "i04_1113",
   "page_count": 4,
   "order": 407,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "Articulatory properties of the sibilants [...] and non-sibilant [..] were studied for VCV syllables in Japanese, using the technique of electropalatography. We examined the relation between constriction width and location, and the nature of coarticulatory variability and directionality, specific to the two fricative categories. The results allow us to discuss the phonetic basis of the precision in fricative articulations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-406"
  },
  "tanabe04_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Tanabe"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ]
   ],
   "title": "Flow representation through the glottis having a polygonal boundary shape",
   "original": "i04_1117",
   "page_count": 4,
   "order": 408,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "This paper presents an analytical approach for representing the flow through the glottis. Based on the polygonal line approximation of the coronal section of the glottis, the velocity field of incompressible, inviscid flow can be conveniently represented using the method of conformal mapping. In this method, the actual physical domain is conformally connected to the canonical domain configured as the infinite strip. Since the general form of analytic potential functions for the uniform flow and the flow due to a point vortex is known in the canonical domain, such flows in the physical domain can be derived by the connecting mapping function specific to the glottal shape. Simulation results were presented to evaluate the influence of a vortex pair placed at the supraglottal section on the stream line and pressure field of the uniform flow.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-407"
  },
  "pulakka04_interspeech": {
   "authors": [
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Svante",
     "Granqvist"
    ],
    [
     "Stellan",
     "Hertegard"
    ],
    [
     "Hans",
     "Larsson"
    ],
    [
     "Anne-Maria",
     "Laukkanen"
    ],
    [
     "Per-Ake",
     "Lindestad"
    ],
    [
     "Erkki",
     "Vilkman"
    ]
   ],
   "title": "Analysis of the voice source in different phonation types: simultaneous high-sped imaging of the vocal fold vibration and glottal inverse filtering",
   "original": "i04_1121",
   "page_count": 4,
   "order": 409,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "Glottal flow waveforms estimated by inverse filtering acoustic speech pressure signals were compared to glottal area functions obtained by digital high-speed imaging of the vocal fold vibration. Speech data consisted of breathy, normal and pressed phonations produced by two male and one female subjects. The results yield both qualitative and quantitative information about the relationship between the glottal flow and the corresponding area function. It was shown, for example, that a distinct knee in the glottal flow waveform in the opening phase corresponds to the abrupt opening of the vocal folds in normal and pressed phonation. In addition, the obtained quantitative data corroborates known theoretical considerations according to which the shape of the glottal flow is more asymmetric than the corresponding area function.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-408"
  },
  "birkholz04_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Dietmar",
     "Jackel"
    ]
   ],
   "title": "Influence of temporal discretization schemes on formant frequencies and bandwidths in time domain simulations of the vocal tract system",
   "original": "i04_1125",
   "page_count": 4,
   "order": 410,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "A time domain simulation of acoustic propagation in the vocal tract requires the spatial and temporal discretization of the equations of motion and continuity. In the classic transmission line model of the vocal tract with lumped elements, the spatial discretization is provided by the piece-wise constant area function. The temporal finite-difference approximation of the differential equations can, however, vary from one implementation to the other. In this study, we have adopted a general finite-difference scheme that depends on a parameter t where 0 <= t <= 1. As special cases, this general method includes the trapezoid rule (t=0.5) as well as the implicit (t=1) and explicit (t=0) finite-difference schemes. We have examined how formant frequencies and bandwidths of simulated vowels are effected by the choice of t. The experiments were conducted for the sampling rates of 44.1 kHz and 88.2 kHz and compared with the accurate and thus desirable frequencies and bandwidths measured in frequency domain simulations of the vocal tract. It can be shown that optimal values for t are slightly above 0.5 depending on the sampling rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-409"
  },
  "toda04_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Alan",
     "Black"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Acoustic-to-articulatory inversion mapping with Gaussian mixture model",
   "original": "i04_1129",
   "page_count": 4,
   "order": 411,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "This paper describes the acoustic-to-articulatory inversion mapping using a Gaussian Mixture Model (GMM). Correspondence of an acoustic parameter and an articulatory parameter is modeled by the GMM trained using the parallel acoustic-articulatory data. We measure the performance of the GMM-based mapping and investigate the effectiveness of using multiple acoustic frames as an input feature and using multiple mixtures. As a result, it is shown that although increasing the number of mixtures is useful for reducing the estimation error, it causes many discontinuities in the estimated articulatory trajectories. In order to address this problem, we apply maximum likelihood estimation (MLE) considering articulatory dynamic features to the GMM-based mapping. Experimental results demonstrate that the MLE using dynamic features can estimate more appropriate articulatory movements compared with the GMM-based mapping applied smoothing by lowpass filter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-410"
  },
  "kim04i_interspeech": {
   "authors": [
    [
     "Jinyoung",
     "Kim"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Audio-visual spoken language processing",
   "original": "i04_1133",
   "page_count": 4,
   "order": 412,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "The multimodal character of speech processing has attracted research endeavors that range from engineers working on automatic speech recognition through to psychologists interested in language processing. This paper serves as a broad introduction to the special Audio-Visual Spoken Language (AVSP) session. The paper focuses on recent developments in the area and touches on theory as well as application. We consider how the scope of the conception of AVSP has broadened; how issues concerning the developmental aspects of AVSP have been tackled; how measures of AV processing have become more sophisticated and how new applications incorporating the visual aspects of speech have been devised. The main aim of this limited review is to highlight the range of the issues and the innovative nature of research programs that fall within this area.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-411"
  },
  "sekiyama04_interspeech": {
   "authors": [
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Issues in the development of auditory-visual speech perception: adults, infants, and children",
   "original": "i04_1137",
   "page_count": 4,
   "order": 413,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "We review research by us and others on the development of auditory-visual speech perception in order to draw out issues of interest, pinpoint unanswered questions, and highlight future research directions. Studies using the McGurk Effect have shown that Japanese adults use visual speech information less than do English language adults. Infants perceive the McGurk Effect, and for English language children, it appears that auditory-visual speech perception improves over age, especially between 6 and 8 years. However, the same is not the case for Japanese children.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-412"
  },
  "krahmer04_interspeech": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Signaling and detecting uncertainty in audiovisual speech by children and adults",
   "original": "i04_1141",
   "page_count": 4,
   "order": 414,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "We describe two experiments on signaling and detecting uncertainty in audiovisual speech by adults and children. In the first study, utterances from adult speakers and child speakers (aged 7-8) were elicitated and annotated with a set of six audiovisual features. It was found that when adult speakers are uncertain about their answer they are more likely to produce filled pauses, delays, high intonation, eyebrow movements, smiles and funny faces. The basic picture for the child speakers is similar, in that the presence of an audiovisual cue in an answer correlates with uncertainty, but the differences are relatively small and only significant for the features delay, eyebrow and funny face. In the second study both adult and child judges watched answers from adult and child speakers selected from the first study to find out whether they were able to correctly estimate a speakers' level of uncertainty. It was found that both child and adult judges give more accurate scores for answers from adult speakers than from child speakers and that child judges overall provide less accurate scores than adult judges.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-413"
  },
  "hazan04_interspeech": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Anke",
     "Sennema"
    ],
    [
     "Andrew",
     "Faulkner"
    ]
   ],
   "title": "Effect of intensive audiovisual perceptual training on the perception and production of the /l/-/r/ contrast for Japanese learners of English",
   "original": "i04_1145",
   "page_count": 4,
   "order": 415,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "This study investigates (a) the extent to which L2 learners use phonetic information from visual cues to improve the perception of a novel phonemic contrast following intensive perceptual training, and (b) the impact of perceptual training on pronunciation. 62 Japanese learners of English were tested on their perception of the /l/-/r/ contrast in audio, visual and audiovisual modalities, and then undertook ten sessions of perceptual training before being tested again. Eighteen were trained using auditory stimuli, 25 using natural audiovisual stimuli and 19 using audiovisual stimuli with a synthetic face. /l/-/r/ perception improved in all groups but learners trained audiovisually did not improve more than those trained auditorily. Auditory perception improved most for 'A training' learners and sensitivity to visual cues improved most for 'natural AV training' learners. The learners' pronunciation of /l/-/r/ improved significantly following perceptual training, with a greater improvement seen for those trained audiovisually with natural stimuli.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-414"
  },
  "vroomen04_interspeech": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Sabine van",
     "Linden"
    ],
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Paul",
     "Bertelson"
    ]
   ],
   "title": "Visual recalibration of auditory speech versus selective speech adaptation: different build-up courses",
   "original": "i04_1149",
   "page_count": 4,
   "order": 416,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "Recent work from our group has demonstrated that visual speech can recalibrate auditory speech identification [1]. Repeated exposure to an ambiguous auditory token (one intermediate between /aba/ and /ada/), combined with the sight of a face articulating one of the endpoint tokens (/aba/ or /ada/), increased during subsequent unimodal auditory identification tests the frequency of responses consistent with the preceding visual stimulus. In contrast, exposure to a combination of congruent unambiguous auditory and visual tokens (both /aba/ or both /ada/) reduced the tendency to judge the ambiguous auditory token in accordance with the exposed tokens, revealing selective speech adaptation. In the present experiment, it is shown that these two effects build up at different rates, thus bringing new evidence for the existence of different underlying processes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-415"
  },
  "davis04_interspeech": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Of the top of the head: audio-visual speech perception from the nose up",
   "original": "i04_1153",
   "page_count": 4,
   "order": 417,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "When someone speaks, linguistically relevant movements are produced. Previous work examining visual speech investigated regions around the oral aperture. Here we examined whether people could use visual information from the top part of the talker's face. In Experiment 1 observers saw two pairs of short silent video clips that showed the top part of a talker's head. The task was to judge the pair of videos in which the talker said the same sentence (different tokens were used). Observers were able to make this judgment at better than chance levels. In Experiment 2, participants were presented with auditory-visual pairs and had to select the pair in which the talker said the same sentence. Matching performance was again above chance. The results indicate that participants are sensitive to the speech relatedness of movements from the upper part of the head and face. Implications for speech processing and for the development of virtual characters are considered.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-416"
  },
  "millar04_interspeech": {
   "authors": [
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "Michael",
     "Wagner"
    ],
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "Aspects of speaking-face data corpus design methodology",
   "original": "i04_1157",
   "page_count": 4,
   "order": 418,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "This paper develops a methodology for the design of audio-video data corpora of the speaking face. Existing corpora are surveyed and the principles of data specification, data description and statistical representation are analysed both from an application-driven and from a scientifically motivated perspective. Furthermore, the possibility of \"opportunistic\" design of speaking-face data corpora is considered.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-417"
  },
  "schwartz04_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Marie",
     "Cathiard"
    ]
   ],
   "title": "Modeling audio-visual speech perception: back on fusion architectures and fusion control",
   "original": "i04_2017",
   "page_count": 4,
   "order": 419,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "In a review paper about audio-visual (AV) fusion models in speech perception, we (Schwartz et al., 1998) proposed a taxonomy of models around two basic questions: architecture and control. Six years after, it appears that the proposals we made still seem rather convenient for discussing major questions about AV fusion. Moreover -- and more importantly -- recent experimental and theoretical progress seem to provide some elements of answer in both aspects. The aim of this paper is to review these elements, and to incorporate them into the general architecture-and-control framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-418"
  },
  "sams04_interspeech": {
   "authors": [
    [
     "Mikko",
     "Sams"
    ],
    [
     "Ville",
     "Ojanen"
    ],
    [
     "Jyrki",
     "Tuomainen"
    ],
    [
     "Vasily",
     "Klucharev"
    ]
   ],
   "title": "Neurocognition of speech-specific audiovisual perception",
   "original": "i04_2021",
   "page_count": 4,
   "order": 420,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "We have studied neurocognitive mechanisms of audiovisual perception of non-meaningful speech. We demonstrate different event-related brain potential (ERP) differences to non-speech and speech audiovisual stimuli. Our imaging studies show that viewing and listening to speech activate overlapping areas in the frontal cortex, which is also activated during speech production. We also demonstrate that subjects have to expect the auditory stimuli to be speech to integrate auditory and visual speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-419"
  },
  "barbosa04_interspeech": {
   "authors": [
    [
     "Adriano Vilela",
     "Barbosa"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Andreas",
     "Daffertshofer"
    ]
   ],
   "title": "Target practice on talking faces",
   "original": "i04_2025",
   "page_count": 4,
   "order": 421,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "A method is described for video-based tracking and analysis of talking heads. Although dots on the face and head are tracked now, clean faces will be tracked once a satisfactory optical flow technique is found. The analysis consists of decomposing the head and face motion components into a common mode and the residual, thus eliminating the need for complex marker tracking and head correction of face motion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-420"
  },
  "odisio04_interspeech": {
   "authors": [
    [
     "Matthias",
     "Odisio"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Audiovisual perceptual evaluation of resynthesised speech movements",
   "original": "i04_2029",
   "page_count": 4,
   "order": 422,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "We have already presented a system that can track the 3D speech movements of a speaker's face in a monocular video sequence. For that purpose, speaker-specific models of the face have been built, including a 3D shape model and several appearance models. In this paper, speech movements estimated using this system are perceptually evaluated. These movements are re-synthesised using a Point-Light (PL) rendering. They are paired with original audio signals degraded with white noise at several SNR. We study how much such PL movements enhance the identification of logatoms, and also to what extent they influence the perception of incongruent audio-visual logatoms. In a first experiment, the PL rendering is evaluated per se. Results seem to confirm other previous studies: though less efficient than actual video, PL speech enhances intelligibility and can reproduce the McGurk effect. In the second experiment, the movements have been estimated with our tracking framework with various appearance models. No salient differences are revealed between the performances of the appearance models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-421"
  },
  "fagel04_interspeech": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Video-realistic synthetic speech with a parametric visual speech synthesizer",
   "original": "i04_2033",
   "page_count": 4,
   "order": 423,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "The author presents a new face module for MASSY, the Modular Audiovisual Speech SYnthesizer. Within this face module the system combines two approaches of visual speech synthesis. Although the articulation space is parameterized, the visual synthesis is image based. In contrary, other image based audio-visual speech synthesizers like MIKETALK and VIDEO REWRITE concatenate pre-recorded video images or sequences. The high-level visual speech synthesis generates a sequence of control commands for the visible articulation. The video synthesis searches an image database for appropriate video frames. If missing, the image is generated by deforming a neutral image. MPEG-4 FDPs and additional points in the mouth opening area and around the lower jaw are defined in the neutral image as feature points. A two-dimensional displacement vector is defined for each feature point. The displacement vector of a point in a triangle of feature points is interpolated from the displacement vectors of the vertices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-422"
  },
  "scanlon04_interspeech": {
   "authors": [
    [
     "Patricia",
     "Scanlon"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Vit",
     "Libal"
    ],
    [
     "Stephen M.",
     "Chu"
    ]
   ],
   "title": "Mutual information based visual feature selection for lipreading",
   "original": "i04_2037",
   "page_count": 4,
   "order": 424,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "Image transforms, such as the discrete cosine, are widely used to extract visual features from the speaker's mouth region to be used in automatic speechreading and audio-visual speech recognition. Typically, the spatial frequency components with the highest energy in the transform space are retained for recognition. This paper proposes an alternative technique for selecting such features, by utilizing the mutual information criterion instead. Mutual information between each individual spatial frequency component and the speech classes of interest is employed as a measure of its appropriateness for speech classification. The highest mutual information components are then selected as visual speech features. Extensions to this scheme by using joint mutual information between candidate feature pairs and classes are also considered. The algorithm is tested on visual-only speech recognition of connected-digit strings, using an appropriate audio-visual database. For low-dimensional visual feature vectors, the proposed method significantly outperforms features selected by means of energy, reducing word error rate by as much as 20% relative. These gains however diminish as higher feature dimensionalities are allowed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-423"
  },
  "lee04m_interspeech": {
   "authors": [
    [
     "Bowon",
     "Lee"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Camille",
     "Goudeseune"
    ],
    [
     "Suketu",
     "Kamdar"
    ],
    [
     "Sarah",
     "Borys"
    ],
    [
     "Ming",
     "Liu"
    ],
    [
     "Thomas",
     "Huang"
    ]
   ],
   "title": "AVICAR: audio-visual speech corpus in a car environment",
   "original": "i04_2489",
   "page_count": 4,
   "order": 425,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "We describe a large audio-visual speech corpus recorded in a car environment, as well as the equipment and procedures used to build this corpus. Data are collected through a multi-sensory array consisting of eight microphones on the sun visor and four video cameras on the dashboard. The script for the corpus consists of four categories: isolated digits, isolated letters, phone numbers, and sentences, all in English. Speakers from various language backgrounds are included, 50 male and 50 female. In order to vary the signal-to-noise ratio, each script has five different noise conditions: idling, driving at 35 mph with windows open and closed, and driving at 55 mph with windows open and closed. The corpus is available through .\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-424"
  },
  "erzin04_interspeech": {
   "authors": [
    [
     "Engin",
     "Erzin"
    ],
    [
     "Yucel",
     "Yemez"
    ],
    [
     "A. Murat",
     "Tekalp"
    ]
   ],
   "title": "Adaptive classifier cascade for multimodal speaker identification",
   "original": "i04_2493",
   "page_count": 4,
   "order": 426,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "We present a multimodal open-set speaker identification system that integrates information coming from audio, face and lip motion modalities. For fusion of multiple modalities, we propose a new adaptive cascade rule that favors reliable modality combinations through a cascade of classifiers. The order of the classifiers in the cascade is adaptively determined based on the reliability of each modality combination. A novel reliability measure, that genuinely fits to the open-set speaker identification problem, is also proposed to assess accept or reject decisions of a classifier. The proposed adaptive rule is more robust in the presence of unreliable modalities, and outperforms the hard-level max rule and soft-level weighted summation rule, provided that the employed reliability measure is effective in assessment of classifier decisions. Experimental results that support this assertion are provided.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-425"
  },
  "iba04_interspeech": {
   "authors": [
    [
     "Midori",
     "Iba"
    ],
    [
     "Anke",
     "Sennema"
    ],
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Andrew",
     "Faulkner"
    ]
   ],
   "title": "Use of visual cues in the perception of a labial/labiodental contrast by Spanish-L1 and Japanese-L1 learners of English",
   "original": "i04_2497",
   "page_count": 4,
   "order": 427,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "This study investigates the extent to which L2 learners with different L1 backgrounds are sensitive to phonetic information contained in the visual cues to a novel phonetic contrast (labial/labiodental contrast), and the degree to which this sensitivity can be increased via intensive training. 36 Spanish-L1 and 47 Japanese-L1 learners of English were initially tested on their perception of the /b,p/-/v/ contrast in audio, visual and audiovisual modalities. The Spanish-L1 learners showed better performance overall, and much greater sensitivity to visual cues to the contrast. The Japanese-L1 group achieved higher scores in the AV than in the A test condition. In Study 2, 39 of the Japanese-L1 learners undertook ten sessions of either auditory or audiovisual intensive training before being tested again. Both groups improved significantly, with a greater improvement seen for audiovisually-trained learners. The benefit from AV training did not depend on learners' sensitivity to visual cues prior to training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-426"
  },
  "zhang04i_interspeech": {
   "authors": [
    [
     "Xianxian",
     "Zhang"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Toshiki",
     "Maeno"
    ]
   ],
   "title": "Audio-visual SPeaker localization for car navigation systems",
   "original": "i04_2501",
   "page_count": 4,
   "order": 428,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "Human-computer interaction for in-vehicle information and navigation systems is a challenging problem because of the diverse and changing acoustic environments. It is proposed that the integration of video and audio information can significantly improve dialog system performance, since the visual modality is not impacted by acoustic noise. In this paper, we propose a robust audio-visual integration system for source tracking and speech enhancement for an in-vehicle speech dialog system. The proposed system integrates both audio and visual information to locate the desired speaker source. Using real data collected in car environments, the proposed system can improve speech accuracy by up to 40.75% compared with audio data alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-427"
  },
  "chaloupka04_interspeech": {
   "authors": [
    [
     "Josef",
     "Chaloupka"
    ]
   ],
   "title": "Automatic lips reading for audio-visual speech processing and recognition",
   "original": "i04_2505",
   "page_count": 4,
   "order": 429,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "This contribution is about the method for automatic lips reading from the video picture. The results of this automatic method are used for the next audio-visual speech processing and recognition. The simple image processing method for finding of the human face in the video picture is presented here. The lips are found from the marked human face in the region of interest, where the lips are, with the help of the mathematical gradient method. This gradient method is based on the image histogram. The histogram is computed from the colour value of the region of interest. The first results for visual speech recognition of isolated words are presented in conclusion. The method described here was used for face and lips detection to help speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-428"
  },
  "wagner04_interspeech": {
   "authors": [
    [
     "Michael",
     "Wagner"
    ],
    [
     "Girija",
     "Chetty"
    ]
   ],
   "title": "liveness verification in audio-video authentication",
   "original": "i04_2509",
   "page_count": 4,
   "order": 430,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "This paper proposes to use combined acoustic and visual feature vectors to distinguish live synchronous audio-video recordings from replay attacks that use audio with a still photo. Equal error rates below 2 % are achieved using a multi-dimensional eigenlip representation and EERs of 7% are achieved with a one-dimensional lip-opening ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-429"
  },
  "martinez04_interspeech": {
   "authors": [
    [
     "Maria José Sanchez",
     "Martinez"
    ],
    [
     "Juan Pablo de la Cruz",
     "Gutierrez"
    ]
   ],
   "title": "Speech recognition using motion based lipreading",
   "original": "i04_2513",
   "page_count": 4,
   "order": 431,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "This paper presents an audio-visual speaker-dependent continuous speech recognition system. The idea is extracting features from the audio- and the video- stream of a speaking person separately and use them to train a Hidden-Markov-Model based recognizer with the combined feature vectors. While the audio feature extraction follows a classical approach, the visual features are obtained by means of an advanced image processing algorithm which tracks certain regions on the speaker's lips with high robustness and accuracy. For a self-generated audio-visual database, we compare the recognition rates of audio only, video only and audio-visual based recognition systems. We compare the results of the audio and the audiovisual systems under different noise conditions. The work is part of a larger project which aims at a new man-machine interface in the form of a so-called Virtual Personal Assistant which communicates with the user based on the multimodal integration of natural communication channels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-430"
  },
  "berthommier04_interspeech": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Comparative study of linear and non-linear models for viseme in version: modeling of a cortical associative function",
   "original": "i04_2517",
   "page_count": 4,
   "order": 432,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "The strong association existing between the audio speech features and the state of mouth opening is exploited for inversion in a comparative framework, using linear and non linear models. At first, an associative map between an array of visemes and the audio features is constructed following a statistical learning process. The visemic mapping is self-organized and after convergence, the conditional mean of audio features is associated to each of them. Since the viseme states form a 2-dimensional continuum, the principle of the non linear inversion models is to drive a continuous trajectory across the output space, using less continuous audio inputs. Two strategies are proposed in order to smooth the output sequence. The first one consists in filtering (reshaping) the input trajectory, and the second one is the driving of a traveling wave. A comparative study including linear and non linear models shows that the second strategy is plausible for modeling an associative cortical function.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-431"
  },
  "cisar04_interspeech": {
   "authors": [
    [
     "Petr",
     "Cisar"
    ],
    [
     "Zdenek",
     "Krnoul"
    ],
    [
     "Milos",
     "Zelezny"
    ]
   ],
   "title": "3d lip-tracking for audio-visual speech recognition in real applications",
   "original": "i04_2521",
   "page_count": 4,
   "order": 433,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "In this paper, we present a solution to the problem of tracking 3D information about the shape of lips from 2D picture of a speaker. We focus on lip-tracking of audio-visual speech recordings from the corpus recorded in a moving car. In real conditions a head of a speaker (a car driver) can move and turn in various directions. To cope with this movements and to avoid recognition errors caused by changing 3D position of lips, our algorithm utilizes a 3D-modelbased approach to the lip-tracking process. First, we present a method for creating and clustering the lip shape models. Next, we describe an algorithm for finding the shape of the lips in a picture using image processing. Further we present application of a distance function for choosing the best model for representation of the lip shape obtained by image processing. Finally we discuss the results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-432"
  },
  "millar04b_interspeech": {
   "authors": [
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "The audio-video australian English speech data corpus AVOZES",
   "original": "i04_2525",
   "page_count": 4,
   "order": 434,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "This paper presents the Audio-Video Australian English Speech data corpus AVOZES. It contains recordings of 20 speakers uttering a variety of phrases. The corpus was designed for research on the statistical relationship of audio and video speech parameters with an audio-video (AV) automatic speech recognition (ASR) task in mind, but may be useful for other research tasks. AVOZES is the first published AV speaking-face data corpus for Australian English and is novel in its use of a stereo camera system for the video recordings and its modular design.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-433"
  },
  "hong04_interspeech": {
   "authors": [
    [
     "Ki-Hyung",
     "Hong"
    ],
    [
     "Yong-Ju",
     "Lee"
    ],
    [
     "Jae-Young",
     "Suh"
    ],
    [
     "Kyong-Nim",
     "Lee"
    ]
   ],
   "title": "Correcting Korean vowel speech recognition errors with limited lip features",
   "original": "i04_2529",
   "page_count": 4,
   "order": 435,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "In this paper, we describe audio-visual Korean vowel recognition experiments by using a limited set of lip features. We propose the lip features extracted from a snapshot image, for each vowel speech, when the speaker's mouth reaches maximum variation compared with its closing state. By using the only one snapshot image, the proposed lip features can be obtained in a simple and cost effective way. For the devices having limited computing power such as PDA or smart phone, easy and cost effective visual feature extraction is very important. We also present a N-best rescoring method to correct Korean vowel speech recognition errors. The experimental results show that the proposed N-best rescoring method and the selected lip features are very effective on audio-visual Korean vowel speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-434"
  },
  "nielsen04_interspeech": {
   "authors": [
    [
     "Kuniko",
     "Nielsen"
    ]
   ],
   "title": "Segmental differences in the visual contribution to speech inteligibility",
   "original": "i04_2533",
   "page_count": 4,
   "order": 436,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "It is well known that the presence of visual cues increases the overall intelligibility of a speech signal [1,2]. Although much is known about segmental differences in both audio-only and visual-only perception, little is known about segmental differences in terms of visual contribution to auditory-visual perception. The purpose of this study was to examine whether segments differ in their visual contribution to speech intelligibility, and whether the presence of visual cues always increases speech intelligibility. Forced-choice word-identification experiments were carried out under auditory-visual (AV) and auditory-only (A) conditions with varying S/N ratios. The experimental results reveal significant differences in the visual contribution for different consonants, with visual cues greatly improving speech intelligibility for most segments. Surprisingly, the results also suggest that the presence of visual cues can reduce intelligibility. In particular, the intelligibility of [r] decreased significantly in the AV condition, being perceived as [w] in most cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-435"
  },
  "ye04_interspeech": {
   "authors": [
    [
     "Hui",
     "Ye"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Voice conversion for unknown speakers",
   "original": "i04_1161",
   "page_count": 4,
   "order": 437,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "Voice conversion is a technique for modifying a source speaker's speech to sound as if it was spoken by a target speaker. The conventional solutions to this problem are based on training and applying conversion functions which require a substantial amount of training data from both the source and the target speaker. In this paper, we present a voice conversion technique that requires no pre-existing training data from the source speaker. This new approach uses a speech recognizer to index the target training data so that each unknown source frame can be used to retrieve similar frames from the target database. The retrieved frames are then used to estimate conversion functions in a similar way to conventional methods. The paper presents both objective and subjective evaluations of the method. It also explores a number of variants including the contrast between using single and multiple transforms, and between the cases where the content of the source speech is known or unknown. The overall conclusion of the paper is that the method presented can result in identification of the target speaker with as little as a single sentence of source data to transform, however, knowledge of the source orthography is needed to attain a close similarity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-436"
  },
  "fischer04_interspeech": {
   "authors": [
    [
     "Volker",
     "Fischer"
    ],
    [
     "Jaime Botella",
     "Ordinas"
    ],
    [
     "Siegfried",
     "Kunzmann"
    ]
   ],
   "title": "Domain adaptation methods in the IBM trainable text-to-speech system",
   "original": "i04_1165",
   "page_count": 4,
   "order": 438,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "This paper presents a comparison of domain adaptation techniques for a unit selection based text-to-speech system. The methods under investigation consider two different prerequisites, namely the absence and the existence of additional domain specific training prompts, spoken by the original voice talent. Whereas in the first case we employ domain specific pre-selection, for the latter we compare a variety of methods that range from a simple extension of the segment inventory to a complete reconstruction of the system, which also includes the training of decision trees for the domain dependent prediction of prosody targets. An experimental evaluation of the methods under consideration unveils significant improvements (up to 1.1 on a 5 point MOS scale) over the baseline system for sentences from the target domain, while showing no significant degradation when synthesizing sentences from other than the adaptation domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-437"
  },
  "zhou04c_interspeech": {
   "authors": [
    [
     "Yi",
     "Zhou"
    ],
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Zhenli",
     "Yu"
    ],
    [
     "Dongjian",
     "Yue"
    ],
    [
     "Guilin",
     "Chen"
    ]
   ],
   "title": "Applying pitch connection control in Mandarin speech synthesis",
   "original": "i04_1169",
   "page_count": 4,
   "order": 439,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "In this paper, a novel tone-based pitch connection control in unit selection is described to improve naturalness of output speech for Mandarin text-to-speech (TTS) baseline system. This study mainly focuses on pitch connections of concatenative syllables. To improve the concatenation quality, we apply offset pitch of preceding syllable and onset pitch of following syllable in unit selection. According to the statistical result on corpus, three types of pitch connection constraints are proposed. Based on the property of pitch connection constraint, corresponding tone-based cost functions play important role in unit selection for continuity improving at concatenation point. By applying the defined cost functions in unit selection, more suitable units are selected and more natural-sounding synthesized speech is achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-438"
  },
  "ney04b_interspeech": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ],
    [
     "David",
     "Suendermann"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Harald",
     "Hoege"
    ]
   ],
   "title": "A first step towards text-independent voice conversion",
   "original": "i04_1173",
   "page_count": 4,
   "order": 440,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "So far, all conventional voice conversion approaches are text-dependent, i.e., they need equivalent training utterances of source and target speaker. Since several recently proposed applications call for renouncing this requirement, in this paper, we present an algorithm which finds corresponding time frames within text-independent training data. The performance of this algorithm is tested by means of a voice conversion framework based on linear transformation of the spectral envelope. Experimental results are reported on a Spanish cross-gender corpus utilizing several objective error measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-439"
  },
  "yu04e_interspeech": {
   "authors": [
    [
     "Zhenli",
     "Yu"
    ],
    [
     "Kaizhi",
     "Wang"
    ],
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Dongjian",
     "Yue"
    ],
    [
     "Guilin",
     "Chen"
    ]
   ],
   "title": "Data pruning approach to unit selection for inventory generation of concatenative embeddable Chinese TTS systems",
   "original": "i04_1177",
   "page_count": 4,
   "order": 441,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "In this paper, a data pruning approach is presented for building acoustic unit inventory for syllable-based concatenative embeddable Chinese TTS system. A 3-portion segmentation of a syllable is proposed based on the nature of voiced/unvoiced structure of Chinese syllable. Individual factorial acoustic measurement of syllable is used to calculate the penalty of perceptual unsatisfactory for concatenation. With respect to the calculated penalties, bad syllables are removed from a cluster. The best syllable of each pruned cluster is selected with a compromised acoustic measurement. The evaluation and application result shows that the method is promising particularly to generate acoustic unit database for small footprint concatenative Chinese (Cantonese and Mandarin) TTS systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-440"
  },
  "vepa04_interspeech": {
   "authors": [
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Subjective evaluation of join cost functions used in unit selection speech synthesis",
   "original": "i04_1181",
   "page_count": 4,
   "order": 442,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "In our previous papers, we have proposed join cost functions derived from spectral distances, which have good correlations with perceptual scores obtained for a range of concatenation discontinuities. To further validate their ability to predict concatenation discontinuities, we have chosen the best three spectral distances and evaluated them subjectively in a listening test. The unit sequences for synthesis stimuli are obtained from a state-of-the-art unit selection text-to-speech system: rVoice from Rhetorical Systems Ltd. In this paper, we report listeners' preferences for each of the three join cost functions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-441"
  },
  "zen04_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Ryosuke",
     "Tsuzuki"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Constructing emotional speech synthesizers with limited speech database",
   "original": "i04_1185",
   "page_count": 4,
   "order": 443,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "This paper describes an emotional speech synthesis system based on HMMs and related modeling techniques. For concatenative speech synthesis, we require all of the concatenation units that will be used to be recorded beforehand and made available at synthesis time. To adopt this approach for synthesizing the wide variety of human emotions possible in speech, implies that this process should be repeated for every targeted emotion making this task challenging and time consuming. In this paper, we propose an emotional speech synthesis technique based on HMMs, especially for the case where only limited amount of training data is available, directly incorporating subjective evaluation results performed on the training data. Listening results performed on the synthesized speech suggest that the proposed technique helps to improve the emotional content of synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-442"
  },
  "lin04_interspeech": {
   "authors": [
    [
     "Cheng-Yuan",
     "Lin"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "A two-phase pitch marking method for TD-PSOLA synthesis",
   "original": "i04_1189",
   "page_count": 4,
   "order": 444,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "This paper describes a robust two-phase pitch marking method based on peak-valley decision and dynamic programming. In the first phase, we select either peaks or valleys for pitch mark candidates according to its similarity to an estimated pitch curve. In the second phase, we define state and transition probabilities, and then employ dynamic programming to find the most likely pitch marks. We have also designed different tests to demonstrate the feasibility of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-443"
  },
  "bonafonte04_interspeech": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Jan van",
     "Santen"
    ],
    [
     "Helenca",
     "Duxans"
    ]
   ],
   "title": "Including dynamic and phonetic information in voice conversion systems",
   "original": "i04_1193",
   "page_count": 4,
   "order": 445,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "Voice Conversion (VC) systems modify a speaker voice (source speaker) to be perceived as if another speaker (target speaker) had uttered it. Previous published VC approaches using Gaussian Mixture Models performs the conversion in a frame-by-frame basis using only spectral information. In this paper, two new approaches are studied in order to extend the GMM-based VC systems. First, dynamic information is used to build the speaker acoustic model. So, the transformation is carried out according to sequences of frames. Then, phonetic information is introduced in the training of the VC system. Objective and perceptual results compare the performance of the proposed systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-444"
  },
  "wang04j_interspeech": {
   "authors": [
    [
     "Zixiang",
     "Wang"
    ],
    [
     "Renhua",
     "Wang"
    ],
    [
     "Zhiwei",
     "Shuang"
    ],
    [
     "Zhenhua",
     "Ling"
    ]
   ],
   "title": "A novel voice conversion system based on codebook mapping with phoneme-tied weighting",
   "original": "i04_1197",
   "page_count": 4,
   "order": 446,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "This paper presents a novel voice conversion system based on codebook mapping. A new phoneme-tied weighting strategy is proposed to reduce the smoothing effects in weighted sum of code books, while a new prosodic conversion method by decision tree is proposed to cope with the complex prosody of Chinese. STRAIGHT algorithm is used to decompose spectrum and excitation for separate modification. Listening tests prove the proposed methods can effectively convert speaker's individuality while maintaining high speech quality with a small amount of training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-445"
  },
  "ling04_interspeech": {
   "authors": [
    [
     "Zhenhua",
     "Ling"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Zhiwei",
     "Shuang"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": "Compression of speech database by feature separation and pattern clustering using STRAIGHT",
   "original": "i04_1201",
   "page_count": 4,
   "order": 447,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "This paper presents an alternative solution for speech database compression aiming at the embedded application of concatenative synthesis systems. The waveform of a speech segment is firstly decomposed into a prosodic pattern and a spectral pattern by STRAIGHT - a powerful speech analysis-synthesis algorithm. Then all the prosodic and spectral patterns are clustered respectively to remove the redundant acoustic information within database. The clustering process is controllable and can export flexible compression ratio to meet the actual footprint requirement of various embedded devices. Besides, some labeling and contextual information are utilized to improve the performance of pattern clustering. Subjective listening test shows that our Mandarin synthesis system with corpus compressed by proposed method at about 2.7kbps perform corresponding to the same system compressed by G.723.1 at 5.3kps and the quality degradation is not serious as the compression ratio increases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-446"
  },
  "kataoka04b_interspeech": {
   "authors": [
    [
     "Shunsuke",
     "Kataoka"
    ],
    [
     "Nobuaki",
     "Mizutani"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Decision-tree backing-off in HMM-based speech synthesis",
   "original": "i04_1205",
   "page_count": 4,
   "order": 448,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "This paper proposes a decision-tree backing-off technique for an HMM-based speech synthesis system. In the system, a decision-tree based context clustering technique is used for constructing parameter tying structures. In the context clustering, the MDL criterion has been used as a stopping criterion. In this paper, however, huge decision-trees are constructed without any stopping criterion. In the synthesis phase, decision-trees obtained in this way are used in the proposed backing-off scheme. This enables us to adjust the cluster size dynamically at run-time according to the text to be synthesized. Results of subjective listening tests show that the proposed technique improves the synthesized speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-447"
  },
  "nishizawa04_interspeech": {
   "authors": [
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Using a depth-restricted search to reduce delays in unit selection",
   "original": "i04_1209",
   "page_count": 4,
   "order": 449,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "Unit selection algorithms are discussed to reduce delays in concatenative speech synthesis. Prompt synthesis response is very important, for example, in dialog systems. However, using a larger database for high-quality sounds, the time required for unit selection becomes longer. In this paper, beam search and DP search algorithms with a depth restriction in search trees are introduced in order to reduce delays in unit selection. In order to maintain the depth of the search tree, the root node is removed and a child node becomes the new root. In the process, all branches that do not pass through the new root are pruned. Therefore, the pruning may influence the result of the unit selection because many potential units can be removed from the search tree. The results of unit selection indicated that the depth-restricted beam search is superior to the depth-restricted DP search only when the depth limit is small.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-448"
  },
  "yamagishi04_interspeech": {
   "authors": [
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "MLLR adaptation for hidden semi-Markov model based speech synthesis",
   "original": "i04_1213",
   "page_count": 4,
   "order": 450,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "This paper describes an extension of maximum likelihood linear regression (MLLR) to hidden semi-Markov model (HSMM) and presents an adaptation technique of phoneme/state duration for an HMM-based speech synthesis system using HSMMs. The HSMMbased MLLR technique can realize the simultaneous adaptation of output distributions and state duration distributions. We focus on describing mathematical aspect of the technique and derive an algorithm of MLLR adaptation for HSMMs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-449"
  },
  "breuer04_interspeech": {
   "authors": [
    [
     "Stefan",
     "Breuer"
    ],
    [
     "Julia",
     "Abresch"
    ]
   ],
   "title": "Phoxsy: multi-phone segments for unit selection speech synthesis",
   "original": "i04_1217",
   "page_count": 4,
   "order": 451,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "A multi-phone unit specification for unit selection speech synthesis is introduced and tested with regard to its qualitative aspects by means of a listening experiment. This different concept of unit definition aims to prevent spectral discontinuities at highly critical points of concatenation and to allow for a faster creation of speech corpora, as well as a speed-up of cost calculation and unit selection at run time. The new units called phoxsy have been designed for German, but the concept can be easily extended to other languages and may also serve as a basis for new half-phone-like segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-450"
  },
  "alias04_interspeech": {
   "authors": [
    [
     "Francesc",
     "Alias"
    ],
    [
     "Xavier",
     "Llora"
    ],
    [
     "Ignasi",
     "Iriondo"
    ],
    [
     "Joan Claudi",
     "Socoro"
    ],
    [
     "Xavier",
     "Sevillano"
    ],
    [
     "Lluis",
     "Formiga"
    ]
   ],
   "title": "Perception-guided and phonetic clustering weight tuning based on diphone pairs for unit selection TTS",
   "original": "i04_1221",
   "page_count": 4,
   "order": 452,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "The quality of corpus based text-to-speech systems depends on the accuracy of the unit selection process, which relies on the values of the weights of the cost function. This paper is focused on defining a new framework for the tuning of these weights. We propose a technique for taking into account the subjective perception of speech in the selection process by means of Interactive Genetic Algorithms. Moreover, we introduce a CART-based method for unit clustering. Both techniques are applied to weight tuning based on diphone pairs. The conducted experiments analyze the feasibility of both proposals separately.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-451"
  },
  "ennajjary04_interspeech": {
   "authors": [
    [
     "Taoufik",
     "En-Najjary"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Thierry",
     "Chonavel"
    ]
   ],
   "title": "A voice conversion method based on joint pitch and spectral envelope transformation",
   "original": "i04_1225",
   "page_count": 4,
   "order": 453,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "Most of the research in Voice Conversion (VC) is devoted to spectral transformation while the conversion of prosodic features is essentially obtained through a simple linear transformation of pitch. These separate transformations lead to an unsatisfactory speech conversion quality, especially when the speaking styles of the source and target speakers are different. In this paper, we propose a method capable of jointly converting pitch and spectral envelope information. The parameters to be transformed are obtained by combining scaled pitch values with the spectral envelope parameters for the voiced frames and only spectral envelope parameters for the unvoiced ones. These parameters are clustered using a Gaussian Mixture Model (GMM). Then the transformation functions are determined using a conditional expectation estimator. Tests carried out show that, this process leads to a satisfactory pitch transformation. Moreover, it makes the spectral envelope transformation more robust.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-452"
  },
  "ennajjary04b_interspeech": {
   "authors": [
    [
     "Taoufik",
     "En-Najjary"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Thierry",
     "Chonavel"
    ]
   ],
   "title": "Fast GMM-based voice conversion for text-to-speech synthesis systems",
   "original": "i04_1229",
   "page_count": 4,
   "order": 454,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "Voice conversion (VC) can be seen as a powerful technology for customizing Text-to-Speech (TTS) systems. This paper deals with the integration of a VC method based on Gaussian Mixture Model (GMM) in a TTS system. In this framework, an algorithm that enables complexity reduction of the VC processing is proposed. The main idea is to restrict the conversion function to the most representative components of the GMM for each frame and, if necessary, to store the component indices and their associated weights in the acoustic dictionary. This method is evaluated by comparison to a classical GMM-based transformation function. Tests show that both methods yield comparable results. Furthermore, additional experiments indicate that this new technique leads to a significant decrease of the computational load involved in the conversion process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-453"
  },
  "kumar04_interspeech": {
   "authors": [
    [
     "Rohit",
     "Kumar"
    ]
   ],
   "title": "A genetic algorithm for unit selection based speech synthesis",
   "original": "i04_1233",
   "page_count": 4,
   "order": 455,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "We describe the use of a Genetic Algorithm (GA) for the Unit Selection problem, which is essentially a search/optimization problem. The various operators for the GA have been defined and comparison with optimization reached by hill climbing approaches is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-454"
  },
  "huang04d_interspeech": {
   "authors": [
    [
     "Jun",
     "Huang"
    ],
    [
     "Lex",
     "Olorenshaw"
    ],
    [
     "Gustavo",
     "Hernandez-Abrego"
    ],
    [
     "Lei",
     "Duan"
    ]
   ],
   "title": "A memory efficient grapheme-to-phoneme conversion system for speech processing",
   "original": "i04_1237",
   "page_count": 4,
   "order": 456,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "In this paper, a memory efficient, statistical data driven approach is proposed and succesfully tested for grapheme-to-phoneme (G2P) conversion. In our system, a dynamic programming (DP) based fast algorithm is formulated to estimate the optimal joint segmentation between training sequences of graphemes and phonemes. A statistical language model is trained to model the contextual information between grapheme and phoneme segments. A two-stage fast decoding algorithm is also proposed to recognize the most-likely phoneme sequences given the input test word and the n-gram graphone models. Experimental results show that this system has similar recognition accuracy as a decision-tree based G2P system and requires much less memory and processing time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-455"
  },
  "kumar04b_interspeech": {
   "authors": [
    [
     "Rohit",
     "Kumar"
    ],
    [
     "S. Prahallad",
     "Kishore"
    ]
   ],
   "title": "Automatic pruning of unit selection speech databases for synthesis without loss of naturalness",
   "original": "i04_1377",
   "page_count": 4,
   "order": 457,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "In the paper we present our experiments with automatic pruning of speech databases created by us for Unit Selection based speech synthesis systems. Several algorithms have been attempted and perceptually evaluated. An optimal size of speech database has been reached where lose of naturalness due to unit pruning is not perceptible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-456"
  },
  "lambert04_interspeech": {
   "authors": [
    [
     "Tanya",
     "Lambert"
    ],
    [
     "Andrew",
     "Breen"
    ]
   ],
   "title": "A database design for a TTS synthesis system using lexical diphones",
   "original": "i04_1381",
   "page_count": 4,
   "order": 458,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "Database designs, if based on the premise that there are about 2000 diphones in English, as stated in many publications and on-line documents, are likely to render a database of diphones, which will fail to capture some important phonological phenomena of English. This paper proposes a TTS database, which is built from diphones inclusive of their syllabic stress; we term these units lexical diphones. A comprehensive lexical diphone feature set is generated using a stress-annotated dictionary and continuous text and speech. A method based on multiple set cover algorithms, applied to wordlists of specialized English usage, and a knowledge-based phonological approach, are used to produce a core text corpus of 540 sentences. An objective evaluation of our database with other databases shows that our database (considering its size) has a higher concentration of lexical diphones; a subjective evaluation shows listeners' preference for the speech where there are more lexical than phonemic units.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-457"
  },
  "kominek04_interspeech": {
   "authors": [
    [
     "John",
     "Kominek"
    ],
    [
     "Alan W",
     "Black"
    ]
   ],
   "title": "A family-of-models approach to HMM-based segmentation for unit selection speech synthesis",
   "original": "i04_1385",
   "page_count": 4,
   "order": 459,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "For segmenting a speech database, using a family of acoustic models provides multiple estimates of each boundary point. This is more robust than a single estimate because by taking consensus values, large labeling errors are less prevalent in the synthesis catalog, which improves the resulting voice. This paper describes HMM-based segmentation in which up to 500 related models are applied to each wavefile. In a listening test of twelve utterances, human judges preferred the proposed technique over the baseline by a tally of 6 to 2, with 4 ties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-458"
  },
  "zhang04j_interspeech": {
   "authors": [
    [
     "Wei",
     "Zhang"
    ],
    [
     "Ling",
     "Jin"
    ],
    [
     "Xijun",
     "Ma"
    ]
   ],
   "title": "Mutual-information based segment pre-selection in concatenative text-to-speech",
   "original": "i04_1389",
   "page_count": 4,
   "order": 460,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "Corpus based Concatenative Text-To-Speech (CTTS) systems have been proven a successful method to produce good voice quality speech. However, It requires a large inventory of synthesis segments and complex search algorithms, which sometimes hinder the usability of CTTS. Segment pre-selection targets to prune the candidate segments to achieve the best possible synthesis quality within a predefined inventory size. Making CTTS usable in environments where memory and CPU are critically constrained. This paper presents a novel pre-selection method in which Mutual Information (MI), a well-known concept in statistics, is integrated. Objective and subjective evaluations of the synthesized speech have proven that this new approach out-performs two conventional pre-selection methods popularly used in current CTTS systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-459"
  },
  "zen04b_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Hidden semi-Markov model based speech synthesis",
   "original": "i04_1393",
   "page_count": 4,
   "order": 461,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "In the present paper, a hidden-semi Markov model (HSMM) based speech synthesis system is proposed. In a hidden Markov model (HMM) based speech synthesis system which we have proposed, rhythm and tempo are controlled by state duration probability distributions modeled by single Gaussian distributions. To synthesis speech, it constructs a sentence HMM corresponding to an arbitrarily given text and determine state durations maximizing their probabilities, then a speech parameter vector sequence is generated for the given state sequence. However, there is an inconsistency: although the speech is synthesized from HMMs with explicit state duration probability distributions, HMMs are trained without them. In the present paper, we introduce an HSMM, which is an HMM with explicit state duration probability distributions, into the HMM-based speech synthesis system. Experimental results show that the use of HSMM training improves the naturalness of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-460"
  },
  "pfitzinger04_interspeech": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "DFW-based spectral smoothing for concatenative speech synthesis",
   "original": "i04_1397",
   "page_count": 4,
   "order": 462,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "A new spectral smoothing technique is proposed and evaluated. Its performance is comparable with LSP interpolation in terms of Euclidean spectral distance measurements but its interpolated formant trajectories are more reasonable from a phonetic point of view. The approach firstly estimates derivative logarithmic magnitude spectra from both the source and the target frame represented by autoregressive filter coefficients. Then, Dynamic Programming yields the best alignment between these two spectral representations. Smoothed frequency responses are achieved by weighted linear interpolation between the corresponding source and target spectral lines whose alignment was found by DP backtracking. Finally, the spectrum is converted to autoregressive filter coefficients with the intermediate stage of autocorrelation coefficients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-461"
  },
  "min04_interspeech": {
   "authors": [
    [
     "Kyung-Joong",
     "Min"
    ],
    [
     "Un-Cheon",
     "Lim"
    ]
   ],
   "title": "Korean prosody generation and artificial neural networks",
   "original": "i04_1869",
   "page_count": 4,
   "order": 463,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "To hear more natural synthetic speech generated by a Korean TTS (Text-To-Speech) system, we have to know all the possible prosodic rules in Korean language. We can extract these rules from linguistic, phonetic knowledge or by analyzing real speech. In general, all of these rules are integrated into a prosody-generation algorithm in TTS. But this algorithm cannot cover all the possible prosodic rules in one language and it is not perfect, so the quality of synthesized speech cannot be as good as we expect. So we propose artificial neural networks(ANNs) that can learn the prosodic rules in Korean language. Multi-Layer Perceptron(MLP) using an error Back Propagation(BP) algorithm had been selected as ANNs for this study. To train and test these ANNs, we made a corpus that consists of some meaningful sentences that were made from a corpus of phonetically balanced(PB) isolated words. These sentences were read by one male speaker, recorded, and collected as a speech database. We had analyzed recorded speech to extract prosodic information of each phoneme, and made target and test patterns for artificial neural networks. We found out that ANNs could learn the prosody from real speech and generate the prosody of a sentence when it was given to ANNs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-462"
  },
  "yoon04_interspeech": {
   "authors": [
    [
     "Kyuchul",
     "Yoon"
    ]
   ],
   "title": "A prosodic phrasing model for a Korean text-to-speech synthesis system",
   "original": "i04_1873",
   "page_count": 4,
   "order": 464,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "This paper presents a prosodic phrasing model for Korean to be used in a text-to-speech synthesis (TTS) system. Read text corpora were morpho-syntactically parsed and prosodically labeled following the Penn Korean Treebank (Han et al., 2002) and K-ToBI prosodic labeling conventions (Jun, 2000) respectively. Decision trees were trained with morpho-syntactic and textual distance features to predict locations of accentual and intonational phrase breaks. Our phrasing model cross-validated on a 300-sentence (6,936 words or 21,436 syllables, an average of 72 syllables or 23 words per sentence) predicted non-breaks with F=92.4% and breaks with F=88.0% (F=72.8% for accentual phrase breaks and F=71.3% for intonational phrase breaks).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-463"
  },
  "shi04_interspeech": {
   "authors": [
    [
     "Qin",
     "Shi"
    ],
    [
     "Volker",
     "Fischer"
    ]
   ],
   "title": "A comparison of statistical methods and features for the prediction of prosodic structures",
   "original": "i04_1877",
   "page_count": 4,
   "order": 465,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "Prosody structure prediction plays an important role in text-to-speech (TTS) conversion systems, where it is a prior step to parametric prosody prediction. Dynamic programming (DP) and decision tree based methods (DT) are widely used for this purpose, but both have well-known limitations. In this paper, we present a combination of both methods, explore the relationship between corpus size and accuracy for three different prediction tasks, and report on the use various lexical features. It is shown that a combination of dynamic programming and decision trees provides the best choice for prosodic word boundary prediction, while decision trees alone give the best results for the prediction of prosodic phrase boundaries. Being originally developed for the Chinese language, we finally demonstrate the transfer of the methods to two different languages, namely Korean and German, where similar results are achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-464"
  },
  "chen04d_interspeech": {
   "authors": [
    [
     "Guilin",
     "Chen"
    ],
    [
     "Ke-Song",
     "Han"
    ]
   ],
   "title": "Letter-to-sound for small-footprint multilingual TTS engine",
   "original": "i04_1881",
   "page_count": 4,
   "order": 466,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "This paper presents two letter-to-sound (LTS) methods in building a small-footprint multilingual text-to-speech (TTS) engine. For the languages where there exist a systematic relationship between a word format and its pronunciation, we employ a rule-based method. Otherwise, we use a training-based method. In the second method, we adopted optimal sequence to implement the process of letter-to-phoneme alignment, and use CART to train the decision tree and store the results in an efficient way. Despite their merits and disadvantages, the experimental results on six languages demonstrate these methods are effective to reach an acceptable LTS precision within a reasonable small size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-465"
  },
  "xu04c_interspeech": {
   "authors": [
    [
     "Jun",
     "Xu"
    ],
    [
     "Guohong",
     "Fu"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Grapheme-to-phoneme conversion for Chinese text-to-speech",
   "original": "i04_1885",
   "page_count": 4,
   "order": 467,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "This paper reports a study of grapheme-to-phoneme (G2P) conversion for Chinese text-to-speech (TTS) system. As Chinese is a syllabic language, syllable is commonly adopted as the phonetic unit in TTS, which is represented by pinyin, the standard Chinese romanization. A Chinese G2P conversion is to find correct pinyin for polyphonic graphemes in the input text. In this paper, a complete G2P framework is presented, which includes a two-stage statistical word segmentation module, a hidden Markov model (HMM) based part-of-speech (POS) tagging module and a word-to-pinyin conversion module. In the word-to-pinyin conversion, a word grapheme is augmented by its POS tag in an effort to resolve the pronunciation disambiguation in G2P. The G2P experiments show that the polyphone G2P accuracy is improved by 9.41% after introducing POS module and further improved by 1.39% while applying the proposed word-to-pinyin method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-466"
  },
  "schroder04_interspeech": {
   "authors": [
    [
     "Marc",
     "Schröder"
    ],
    [
     "Stefan",
     "Breuer"
    ]
   ],
   "title": "XML representation languages as a way of interconnecting TTS modules",
   "original": "i04_1889",
   "page_count": 4,
   "order": 468,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "The present paper reports on a novel way of increasing the modularity and pluggability of text-to-speech (TTS) architectures. In a proof-of-concept study, two current TTS systems, both using XMLbased languages for internal data representation, are plugged together using XSLT transforms as a means of translating from one system's internal representation to the other's. This method allows one system to use modules from the other system. The potential and the limitations of the approach are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-467"
  },
  "cao04_interspeech": {
   "authors": [
    [
     "Wenjie",
     "Cao"
    ],
    [
     "Chengqing",
     "Zong"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Approach to interchange-format based Chinese generation",
   "original": "i04_1893",
   "page_count": 4,
   "order": 469,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "Interlingua-based machine translation is an important approach to implement multi-lingual speech-to-speech (S2S) translation. The natural language generation (NLG) is one of the key components in the interlingua-based machine translation systems. This paper introduces our approach to Chinese generation based on the Interchange Format (IF) developed by the C-STAR organization. In our approach, the hybrid method of feature-based deep generation method and template-based method are employed. The deep generator ensures that the generation component possesses the merits of flexibility and domain portability. The template-based generator makes the system more efficient. We also introduce another simplified Chinese generator applied in specific domain. The experimented results show that our approach is effective and practical for the natural language generation in the Interchange-Format (IF) based S2S translation system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-468"
  },
  "zovato04_interspeech": {
   "authors": [
    [
     "Enrico",
     "Zovato"
    ],
    [
     "Stefano",
     "Sandri"
    ],
    [
     "Silvia",
     "Quazza"
    ],
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "Prosodic analysis of a multi-style corpus in the perspective of emotional speech synthesis",
   "original": "i04_1897",
   "page_count": 4,
   "order": 470,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "This paper describes the collection and analysis of a multi-style emotional speech corpus, accomplished to study the variations of some acoustical parameters. Specifically, three emotional styles were considered: happiness, sadness and anger. Speech data in a neutral style were also collected, and prosodic differences of each style with respect to this neutral baseline were quantified. According to the analysis results, experiments were also carried out to synthesize emotional styles by modifying prosodically neutral signals both extracted from the original corpus and produced by our Text To Speech synthesis system. Perceptual tests were made to evaluate the effectiveness of the adopted method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-469"
  },
  "min04b_interspeech": {
   "authors": [
    [
     "Kyung-Joong",
     "Min"
    ],
    [
     "Chan-Goo",
     "Kang"
    ],
    [
     "Un-Cheon",
     "Lim"
    ]
   ],
   "title": "Number of output nodes of artificial neural networks for Korean prosody generation",
   "original": "i04_1901",
   "page_count": 4,
   "order": 471,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "We'd been studying artificial neural networks(ANNs) that can learn and generate the prosody of a Korean sentence. To hear more natural synthetic speech generated by a Korean TTS (Text-To-Speech) system, we have to know all the possible prosodic rules about Korean language and integrate all of these rules into an algorithm. We can get these rules from linguistic, phonetic knowledge or by analyzing real speech. But this algorithm cannot cover all the possible prosodic rules in a language, so the quality of synthesized speech cannot be as good as we expect. We had trained BP (Back Propagation) ANNs that can learn the energy contour and the pitch contour of a phoneme in a sentence and generate the polynomial parameters of the contours that can be used in TTS system. The prosodic contours of a phoneme can be approximated by polynomial equations and the order of the polynomial equations can be determined according to the various conditions. In this paper, we had compared the performances of ANNs with different number of output nodes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-470"
  },
  "kim04j_interspeech": {
   "authors": [
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Ju-Eun",
     "Ahn"
    ],
    [
     "Soon-Hyob",
     "Kim"
    ],
    [
     "Yang-Hee",
     "Lee"
    ]
   ],
   "title": "A Korean grapheme-to-phoneme conversion system using selection procedure for exceptions",
   "original": "i04_1905",
   "page_count": 4,
   "order": 472,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "Cultural, social, economic and other various environmental factors affect our language, and different words and terminology are used and coined for different contexts, which triggers quantitative change of vocabulary of a language. This paper presents a Korean grapheme-to-phoneme conversion system using a selection procedure for exceptions from added text corpus, which reflects such dynamic nature of the Korean language. For our experiment, we used the text corpus released by the Electronics and Telecommunications Research Institute (ETRI) for speech recognition, consisting of 53,750 sentences (740,497 Eojol), and obtained a 100% performance level of the proposed grapheme-to-phoneme conversion system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-471"
  },
  "khaorapapong04_interspeech": {
   "authors": [
    [
     "Thanate",
     "Khaorapapong"
    ],
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Keerati",
     "Inthavisas"
    ]
   ],
   "title": "Synthesis of vowels and tones in Thai language by articulatory modeling",
   "original": "i04_1909",
   "page_count": 4,
   "order": 473,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "We propose a way of synthesizing Thai vowels, monophthong and diphthong which consist of nine short-long vowel pairs and three short-long vowel pairs respectively. By adjusting the fundamental frequency or pitch contour, the five tonal of these vowels can be synthesized, leading to a total of 120 phonemes. We evaluate the synthesized monophthongs by comparing the first and second formants to standard in terms of different percentages, which result in 9.89% and 7.44% respectively. For the synthesis of diphthongs, we slide from one vocal-tract vowel to another using linear, logarithmic, or exponential functions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-472"
  },
  "shiga04b_interspeech": {
   "authors": [
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Source-filter separation for articulation-to-speech synthesis",
   "original": "i04_1913",
   "page_count": 4,
   "order": 474,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "In this paper we examine a method for separating out the vocal-tract filter response from the voice source characteristic using a large articulatory database. The method realises such separation for voiced speech using an iterative approximation procedure under the assumption that the speech production process is a linear system composed of a voice source and a vocal-tract filter, and that each of the components is controlled independently by different sets of factors. Experimental results show that the spectral variation is evidently influenced by the fundamental frequency or the power of speech, and that the tendency of the variation may be related closely to speaker identity. The method enables independent control over the voice source characteristic in our articulation-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-473"
  },
  "hisako04_interspeech": {
   "authors": [
    [
     "Asano",
     "Hisako"
    ],
    [
     "Nakajima",
     "Hideharu"
    ],
    [
     "Mizuno",
     "Hideyuki"
    ],
    [
     "Oku",
     "Masahiro"
    ]
   ],
   "title": "Long vowel detection for letter-to-sound conversion for Japanese sourced words transliterated into the alphabet",
   "original": "i04_1917",
   "page_count": 4,
   "order": 475,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "Modern Japanese texts often include Western sourced words written in the Roman alphabet. Even Japanese sourced words are sometimes transliterated into the Roman alphabet. As most of them are very new and idiosyncratic proper nouns, it is impractical to assume all those alphabetic words can be registered in the word dictionary of a text-to-speech system; their pronunciation must be derived automatically. As long vowel expressions are the same with short vowel or diphthong expressions in Japanese words transliterated into the alphabet, long vowel detection is necessary for generating highly accurate pronunciation. This paper proposes a method to detect long vowel positions using the Support Vector Machine. The cost of making the learning model for the detection is very low because the training data can be generated automatically from the text-to-speech word dictionary. 93.2% word accuracy was achieved which is almost the maximum accuracy possible from just spelling information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-474"
  },
  "clermont04_interspeech": {
   "authors": [
    [
     "Frantz",
     "Clermont"
    ],
    [
     "Thomas John",
     "Millhouse"
    ]
   ],
   "title": "Inexactness and robustness in cepstral-to-formant transformation of spoken and sung vowels",
   "original": "i04_1921",
   "page_count": 4,
   "order": 476,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "Accurate measurement of formant frequencies is important in many studies of speech production and perception. The mapping of formant frequencies from cepstra coefficients by Broad and Clermont (1989) is an important evolution in formant estimation techniques from previous studies. This method also holds significant potential in estimating formant frequencies of sung vowels due to its improved robustness over traditional formant tracking techniques. The employment of this method in the estimation of formant frequencies from a dataset of spoken and sung phonation has led to some findings, not only on the inexactness of the model, but also on the resonant nature of the sung and spoken vowels themselves.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-475"
  },
  "saitou04_interspeech": {
   "authors": [
    [
     "Takeshi",
     "Saitou"
    ],
    [
     "Naoya",
     "Tsuji"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Analysis of acoustic features affecting \"singing-ness\" and its application to singing-voice synthesis from speaking-voice",
   "original": "i04_1925",
   "page_count": 4,
   "order": 477,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "To construct a natural singing-voice synthesis system, it is important to adequately control acoustic features such as fundamental frequency (F0), spectrum shapes, and phoneme duration in the synthesis method. This paper reveals acoustic features affecting singing-voice perception by comparative analyzing singing- and speaking-voices, and then proposes a transforming method from speaking-voice into singing-voice using STRAIGHT. This method is composed of an F0 control model for generating F0 contours of singing-voices, a spectral sequence control model for modifying spectral shapes in speaking-voice, and a duration control model based on rhythm. Results showed that the proposed system could synthesize a natural singing-voice, whose sound quality is almost the same as that of real singing-voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-476"
  },
  "pollet04_interspeech": {
   "authors": [
    [
     "Vincent",
     "Pollet"
    ],
    [
     "Geert",
     "Coorman"
    ]
   ],
   "title": "Statistical corpus-based speech segmentation",
   "original": "i04_1929",
   "page_count": 4,
   "order": 478,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "An automatic speech segmentation technique is presented that is based on the alignment of a target speech signal with a set of different reference speech signals generated by a specific designed corpus-based speech synthesis system that additionally generates phoneme boundary markers. Each reference signal is then warped to the target speech signal. By synthesizing and warping many different reference speech signals, each phoneme boundary of the target signal is characterized by a distribution of warped phoneme boundary positions. The boundary distributions are statistically and acoustically processed in order to generate the final segmentation. First, some problems related to manual and automatic phoneme segmentation are addressed. Then the technique of Statistical Corpus-based Segmentation (SCS) is introduced. Finally, intra- and inter-speaker segmentation results are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-477"
  },
  "matousek04_interspeech": {
   "authors": [
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Jan",
     "Romportl"
    ],
    [
     "Daniel",
     "Tihelka"
    ],
    [
     "Zbynek",
     "Tychtl"
    ]
   ],
   "title": "Recent improvements on ARTIC: czech text-to-speech system",
   "original": "i04_1933",
   "page_count": 4,
   "order": 479,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "This paper presents recent improvements on ARTIC - the only Czech corpus-based text-to-speech system. As a statistical approach (using hidden Markov models) was applied to create an acoustic unit inventory, several improvements concerning acoustic unit modelling, clustering and segmentation have been accomplished to increase the intelligibility of the resulting speech. Two approaches to the generation of prosodic features were also proposed and implemented to increase the naturalness of synthetic speech. To produce as smooth synthetic speech as possible, a multiple unit instance scheme with online unit candidate selection was proposed as well. Our work on an alternative harmonic/noise-based speech production method is also mentioned. In addition, an important step towards multilinguality was achieved as German and Slovak language modules were implemented besides two Czech voices within the framework of ARTIC TTS system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-478"
  },
  "nam04_interspeech": {
   "authors": [
    [
     "HyeonSook",
     "Nam"
    ],
    [
     "Youngim",
     "Jung"
    ],
    [
     "Donghun",
     "Lee"
    ],
    [
     "Hyuk-chul",
     "Kwon"
    ],
    [
     "Aesun",
     "Yoon"
    ]
   ],
   "title": "Learning for transliteration of arabic-numeral expressions using decision tree for Korean TTS",
   "original": "i04_1937",
   "page_count": 4,
   "order": 480,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "Despite of much work on TTS technologies and several TTS systems customized for Korean, current TTS systems output many errors in transliterating the sounds of non-alphabetic symbols such as Arabic numerals and text symbols. This paper proposes TLAN (Transliteration learner for Arabic-Numeral Expressions(NEs)) which can efficiently disambiguate the reading and meaning of NEs in texts by using a decision tree. For the purpose of analyzing and learning data, three phases of learning elements were suggested: patterns of Arabic numerals combined with text symbols, contextual features and heuristic information were classified according to the senses and sounds of NEs. Our corpus was made up of news articles issued from January 1st, 2000 to December 31st, 2001 from 9 major newspapers in Korea. By learning the three phases of learning elements, the model shows 97.38% and 97.28% accuracies for the training set and the test set, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-479"
  },
  "beringer04_interspeech": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "How to integrate phonetic and linguistic knowledge in a text-to-phoneme conversion task: a syllabic TPC tool for French",
   "original": "i04_1941",
   "page_count": 4,
   "order": 481,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "The goal of this work is to present a text-to-phoneme conversion (TPC) check tool for generating dictionaries usable for Speech Synthesis and Speech Recognition which includes phonetic and linguistic knowledge. The aim is to improve and accelerate the task of producing canonic pronunciation dictionaries in languages where no simple text-to-phoneme conversion is possible. The prototype was developed for a French syllable-based text-to-phoneme task but it is portable to other languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-480"
  },
  "hamza04_interspeech": {
   "authors": [
    [
     "Wael",
     "Hamza"
    ],
    [
     "Ellen",
     "Eide"
    ],
    [
     "Raimo",
     "Bakis"
    ]
   ],
   "title": "Reconciling pronunciation differences between the front-end and the back-end in the IBM speech synthesis system",
   "original": "i04_2561",
   "page_count": 4,
   "order": 482,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "In this paper, methods to reconcile pronunciation differences between a rule-based front-end and the pronunciations observed in a database of recorded speech are presented. The methods are applied to the IBM Expressive Speech Synthesis System [1] for both unrestricted and limited-domain text-to-speech synthesis. One method is based on constructing a multiple pronunciation lattice for the given sentence and scoring it using word and phoneme n-gram statistics computed from the target speaker's database. A second method consists of storing observed pronunciations and introducing them as alternates in the search. We compare the strengths and weaknesses of these two methods. Results show that improvements are achieved in both limited and unrestricted domains, with the largest gains coming in the limited-domain case.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-481"
  },
  "ha04_interspeech": {
   "authors": [
    [
     "Juhong",
     "Ha"
    ],
    [
     "Yu",
     "Zheng"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ],
    [
     "Yoon-Suk",
     "Seong"
    ],
    [
     "Byeongchang",
     "Kim"
    ]
   ],
   "title": "High quality text-to-pinyin conversion using two-phase unknown word prediction",
   "original": "i04_2565",
   "page_count": 4,
   "order": 483,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "One of the enduring problems in developing high-quality Chinese TTS (text-to-speech) systems is accurate text-to-Pinyin conversion.To solve the problem, identification of words and assignment of correct POS (Part-of-Speech) tags for an input sentence are very important tasks. Also, determining the correct Pinyin of polyphonic characters in unknown words is an important problem in a Chinese TTS system. The unknown word problem has significant effects on the accuracy of the synthesized sound, so accurately predicting the category of unknown words can help a Chinese TTS system to pronounce more naturally. In this paper, we present an SVM(support vector machine)-based method that predicts the unknown words for the results of Chinese word segmentation and POS tagging. For high speed SVM processing to be used in TTS, we pre-detect the candidate boundary of the unknown words before starting the actual prediction. Results of the experiments are very promising by showing high precision and high recall with also very high speed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-482"
  },
  "kim04k_interspeech": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Ann",
     "Syrdal"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Pronunciation lexicon adaptation for TTS voice building",
   "original": "i04_2569",
   "page_count": 4,
   "order": 484,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "This paper describes reducing phone label errors in TTS voice building by means of modeling of speaker pronunciation variants. Each speaker has his or her own unique pronunciations (and context-dependent variations), so that no one standard lexicon is able to cover all of the speaker's variations. Creating speaker-dependent pronunciation lexicons for automatic speech labeling of our TTS voice databases helped to eliminate many pronunciation errors that resulted from mismatches between lexical pronunciations and how the speaker (voice talent) actually pronounced a word. We also found that it contributed other synthesis quality improvement as well. A perceptual test showed that our work contributed to MOS improvement for American English male and female voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-483"
  },
  "webster04_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Webster"
    ]
   ],
   "title": "Improving letter-to-pronunciation accuracy with automatic morphologically-based stress prediction",
   "original": "i04_2573",
   "page_count": 4,
   "order": 485,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "Robust text-to-speech (TTS) systems require a letter-to-pronunciation module for generating the pronunciations of words missing from the system lexicon. For an input orthography, both a phone sequence and the location of lexical stress must be predicted. However, letter-to-pronunciation modules that make use of a window of context letters around a target letter normally cannot \"see\" larger-context morphological information that is highly correlated with stress location. By adding a new component that uses morphological information to predict which letter of a word might receive primary stress, and then using the resulting \"stressed letters\" as input to a decision tree stressed-letter-to-pronunciation component, improvements to both stress accuracy and phone accuracy are obtained in American English, British English, and German. Using stressed letters as the input to the decision tree also improves phone accuracy when stress is not required in the output pronunciation, as is conventionally the case for automatic speech recognition (ASR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-484"
  },
  "hamza04b_interspeech": {
   "authors": [
    [
     "Wael",
     "Hamza"
    ],
    [
     "Ellen",
     "Eide"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Michael",
     "Picheny"
    ],
    [
     "John",
     "Pitrelli"
    ]
   ],
   "title": "The IBM expressive speech synthesis system",
   "original": "i04_2577",
   "page_count": 4,
   "order": 486,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "This paper introduces the IBM Expressive Speech Synthesis system. We describe recent work in improving the quality of our baseline text-to-speech system as well as extending our capabilities to generate expressive synthetic speech. We present results showing improved base quality, especially for sentences drawn from a limited domain. We also demonstrate our ability to convey good news and bad news, produce contrastive emphasis, and ask a question appropriately. In order to facilitate access to the expressive capabilities, we use some of our proposed extensions to the Speech Synthesis Markup Language (SSML).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-485"
  },
  "schnell04b_interspeech": {
   "authors": [
    [
     "Markus",
     "Schnell"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "What concept-to-speech can gain for prosody",
   "original": "i04_2581",
   "page_count": 4,
   "order": 487,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "This article proposes a concept-to-speech system with automated prosody learning based on reinforcement learning. The concept-to-speech system, named Demosthenes, is an extension of the text-to-speech system DreSS. Demosthenes is responsible for template-based text generation and symbolic prosody prediction, while DreSS takes care of acoustic prosody and speech synthesis. The prosody predictor is an application of reinforcement learning, using content, given and new, contrast, and number of words since last accented words as indicators in state space. The system is trained with a simple rule, giving reward according to prediction performance on a small sample text. For an impression of the gain in prosodic quality, we compare the concept-to-speech system to an existing text-to-speech system. The results indicate a clear preference for the concept-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-486"
  },
  "kawahara04d_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kiyotaka",
     "Uchimoto"
    ],
    [
     "Hitoshi",
     "Isahara"
    ],
    [
     "Kazuya",
     "Shitaoka"
    ]
   ],
   "title": "Dependency structure analysis and sentence boundary detection in spontaneous Japanese",
   "original": "i04_1353",
   "page_count": 4,
   "order": 488,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "This paper addresses automatic detection of dependencies between Japanese phrasal units called bunsetsus, and sentence boundaries in a spontaneous speech corpus. In spontaneous speech, the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous. In this paper, we propose two methods for improving the accuracy of sentence boundary detection in spontaneous Japanese: one based on unsupervised learning and the other based on machine learning. Experimental results show that the sentence boundary detection accuracy of 84.85 in F-measure is achieved by using the proposed methods and the accuracy of dependency structure analysis is also improved by using the information on automatically detected sentence boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-487"
  },
  "jamoussi04_interspeech": {
   "authors": [
    [
     "Salma",
     "Jamoussi"
    ],
    [
     "David",
     "Langlois"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Kamel",
     "Smaili"
    ]
   ],
   "title": "Statistical feature language model",
   "original": "i04_1357",
   "page_count": 4,
   "order": 489,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "Statistical language models are widely used in automatic speech recognition in order to constrain the decoding of a sentence. Most of these models derive from the classical n-gram paradigm. However, the production of a word depends on a large set of linguistic features : lexical, syntactic, semantic, etc. Moreover, in some natural languages the gender and number of the left context affect the production of the next word. Therefore, it seems attractive to design a language model based on a variety of word features. We present in this paper a new statistical language model, called Statistical Feature Language Model, SFLM, based on this idea. In SFLM a word is considered as an array of linguistic features, and the model is defined in a way similar to the n-gram model. Experiments were carried out for French and showed improvement in terms of perplexity and predicted words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-488"
  },
  "bigi04_interspeech": {
   "authors": [
    [
     "Brigitte",
     "Bigi"
    ],
    [
     "Yan",
     "Huang"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Vocabulary and language model adaptation using information retrieval",
   "original": "i04_1361",
   "page_count": 4,
   "order": 490,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "The goal of vocabulary optimization is to construct a vocabulary with exactly those words that are the most likely to appear in the test data. We will present a new approach to reduce the out-of-vocabulary (OOV) rate by adapting the vocabulary model during the ASR process. This method can also be used for the statistial language model (SLM) adaptation. An information retrieval system is used after the first pass of the ASR system to obtain a set of relevant documents. These documents are then used to generate the new vocabulary and/or corpus. In this paper, we propose a new retrieving method well-adapted for this purpose. Experiments were carried out on French with a 28% OOV rate reduction. Experiments were also carried out on English for the SLM adaptation, with 7.9% perplexity reduction, and minor WER improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-489"
  },
  "mori04b_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Daisuke",
     "Takuma"
    ]
   ],
   "title": "Word n-gram probability estimation from a Japanese raw corpus",
   "original": "i04_1365",
   "page_count": 4,
   "order": 491,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "Statistical language modeling plays an important role in a state-of-the-art speech recognizer. The most used language model (LM) is word n-gram model, which is based on the frequency of words and word sequences in a corpus. In various Asian languages, however, words are not delimited by whitespace, so we need to annotate sentences with word boundary information to prepare a statistically reliable large corpus. In this paper, we explain a method for building an LM directly from a raw corpus. In this method, sentences in the raw corpus are regarded as sentences annotated with stochastic word boundary information. In the experiments, we compared the predictive powers of an LM built only from a segmented corpus and an LM built from the segmented corpus and a raw corpus. The result showed that we succeeded in reducing the perplexity by 42.9% using a raw corpus by our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-490"
  },
  "chien04_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Hung-Ying",
     "Chen"
    ]
   ],
   "title": "Mining of association patterns for language modeling",
   "original": "i04_1369",
   "page_count": 4,
   "order": 492,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "Language modeling using n-gram is popular for speech recognition and many other applications. The conventional n-gram suffers from the insufficiencies of training data, domain knowledge and long distance language dependencies. This paper presents a new approach to mining long distance word associations and incorporating their mutual information into language models. We aim to discover the associations of multiple distant words from training corpus. An efficient algorithm is exploited to merge the frequent word subsets and construct the association patterns. The resulting association pattern n-gram is general with a special realization to trigger pair n-gram where only associations of two distant words are considered. To improve the modeling, we further compensate the weaknesses of sparse training data via parameter smoothing and domain mismatch via online adaptive learning. The proposed association pattern n-gram and several hybrid models are successfully applied for speech recognition. We also find that the incorporation of mutual information of association patterns can significantly reduce the perplexities of language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-491"
  },
  "chien04b_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Meng-Sung",
     "Wu"
    ],
    [
     "Hua-Jui",
     "Peng"
    ]
   ],
   "title": "On latent semantic language modeling and smoothing",
   "original": "i04_1373",
   "page_count": 4,
   "order": 493,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "Language modeling plays a critical role for automatic speech recognition. Conventionally, the n-gram language models suffer from lacking good representation of historical words and estimating unseen parameters from insufficient training data. In this work, the latent semantic information is explored for language modeling and parameter smoothing. In language modeling, we present a new representation of historical words via retrieving the most likely relevance document. Besides, we also develop a novel parameter smoothing method where the language models of seen and unseen words are estimated by interpolating those of k nearest seen words in training corpus. The interpolation coefficients are determined according to the closeness of words in semantic space. In the experiments, the proposed modeling and smoothing methods can significantly reduce the perplexities of language models with moderate computation cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-492"
  },
  "goel04_interspeech": {
   "authors": [
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Conditional maximum likelihood estimation for improving annotation performance of n-gram models incorporating stochastic finite state grammars",
   "original": "i04_2237",
   "page_count": 4,
   "order": 494,
   "p1": "2237",
   "pn": "2241",
   "abstract": [
    "Language models that combine stochastic grammars and N-grams are often used in speech recognition and language understanding systems. One useful aspect of these models is that they can be used to annotate phrases in the text with their constituent grammars; such annotation often plays an important role in subsequent processing of the text. In this paper we present an estimation procedure, under a conditional maximum likelihood objective, that aims at improving the annotation performance of these models over their maximum likelihood estimate. The estimation is carried out using the extended Baum-Welch procedure of Gopalakrishnan et.al. We find that with conditional maximum likelihood estimation the annotation accuracy of the language models can be improved by over 7% relative to their maximum likelihood estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-493"
  },
  "schofield04_interspeech": {
   "authors": [
    [
     "Edward James",
     "Schofield"
    ]
   ],
   "title": "Fast parameter estimation for joint maximum entropy language models",
   "original": "i04_2241",
   "page_count": 4,
   "order": 495,
   "p1": "2241",
   "pn": "2244",
   "abstract": [
    "This paper discusses efficient parameter estimation methods for joint (unconditional) maximum entropy language models such as whole-sentence models. Such models are a sound framework for formalizing arbitrary linguistic knowledge in a consistent manner. It has been shown that general-purpose gradient-based optimization methods are among the most efficient algorithms for parameter estimation for several tasks in natural language processing. This paper applies gradient methods to whole-sentence language models and other domains whose sample spaces are infinite or practically innumerable and require simulation. It also presents Open Source software for easily fitting and testing joint maximum entropy models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-494"
  },
  "vergyri04_interspeech": {
   "authors": [
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Kevin",
     "Duh"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Morphology-based language modeling for arabic speech recognition",
   "original": "i04_2245",
   "page_count": 4,
   "order": 496,
   "p1": "2245",
   "pn": "2248",
   "abstract": [
    "Language modeling is a difficult problem for languages with rich morphology. In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic. Class-based and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework. In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model. We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-495"
  },
  "khan04_interspeech": {
   "authors": [
    [
     "A. Nayeemulla",
     "Khan"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Speech enhanced multi-Span language model",
   "original": "i04_2249",
   "page_count": 4,
   "order": 497,
   "p1": "2249",
   "pn": "2252",
   "abstract": [
    "To capture local and global constraints in a language, statistical n-grams are used in combination with multi-span language models for improved language modelling. Use of latent semantic analysis (LSA) to capture the global semantic constraints and bigram models to capture local constraints, is shown to reduce the perplexity of the model. In this paper we propose a method in which the multi-span LSA language model can be developed based on the speech signal. Reference pattern vectors are derived from the speech signal for each word in the vocabulary. Based on the normalised distance between the reference word pattern vector and the pattern vector of a word in the training data, the LSA model is developed. We show that this model in combination with a standard bigram model performs better than the conventional bigram + LSA model. The results are demonstrated for a limited vocabulary on a database for the Indian language, Tamil.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-496"
  },
  "schwenk04_interspeech": {
   "authors": [
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Neural network language models for conversational speech recognition",
   "original": "i04_2253",
   "page_count": 4,
   "order": 498,
   "p1": "2253",
   "pn": "2256",
   "abstract": [
    "Recently there is increasing interest in using neural networks for language modeling. In contrast to the well known backoff n-gram language models (LM), the neural network approach tries to limit the data sparseness problem by performing the estimation in a continuous space, allowing by these means smooth interpolations. Therefore this type of LM is interesting for tasks for which only a very limited amount of in-domain training data is available, in particular the modeling of spontaneous speech. In this paper we analyze the generalization behavior of the neural network LM for in-domain training corpora varying from 7M to more than 21M words. In all cases, we observed significant word error reductions with respect to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the NIST rich transcriptions evaluations. We also apply ensemble learning methods and discuss their relations with LM interpolation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-497"
  },
  "mrva04_interspeech": {
   "authors": [
    [
     "David",
     "Mrva"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "A PLSA-based language model for conversational telephone speech",
   "original": "i04_2257",
   "page_count": 4,
   "order": 499,
   "p1": "2257",
   "pn": "2260",
   "abstract": [
    "This paper describes experiments with a PLSA-based language model for conversational telephone speech. This model uses a long-range history and exploits topic information in the test text to adjust probabilities of test words. The PLSA-based model was found to lower test set perplexity over a traditional word+class-based 4-gram by 13% (optimistic estimate using a reference transcript as history) or by 6% (realistic estimate using recognised transcript as history). Moreover, this paper introduces a use of confidence scores to weight words in the history, a weight of the prior topic distribution and a way of calculating perplexity that accounts for recognition errors in the model context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-498"
  },
  "louradour04_interspeech": {
   "authors": [
    [
     "Jerome",
     "Louradour"
    ],
    [
     "Regine",
     "André-Obrecht"
    ],
    [
     "Khalid",
     "Daoudi"
    ]
   ],
   "title": "Segmentation and relevance measure for speaker verification",
   "original": "i04_1401",
   "page_count": 4,
   "order": 500,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "In all the efficient speaker recognition systems, the decision score is based on the average of the likelihood ratio computed on each frame of the sentence. Except for the non speech frames which are rejected, each one has the same weight in this summation. This paper deals with the study of the speaker relevance of each frame. An automatic segmentation provides quasi stationary segments of variable length; a weight is allocated to each frame in function of its segment position and a weighted mean of the likelihood ratio is then computed. Experiments are performed with NIST 2003 speaker evaluation database. They show that the frames near segment frontiers, that is to say the transient ones, are more speaker relevant than the middle frames of long segments which correspond to the steady parts of the phones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-499"
  },
  "chetouani04_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Chetouani"
    ],
    [
     "Bruno",
     "Gas"
    ],
    [
     "Jean-Luc",
     "Zarader"
    ],
    [
     "Marcos",
     "Faundez-Zanuy"
    ]
   ],
   "title": "A new nonlinear feature extraction algorithm for speaker verification",
   "original": "i04_1405",
   "page_count": 4,
   "order": 501,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "In this paper we propose a new feature extraction algorithm based on nonlinear prediction: the Neural Predictive Coding model which is an extension of the classical LPC one. This model is applied to speaker verification by the Arithmetic-Harmonic Sphericity (AHS) method. Two different initialization methods are proposed for the coding method based on the Neural Predictive Coding (NPC): classical neural networks initialization and linear initialization. The first model obtains smaller rates. For the linear initialization, we obtain significant improvement in comparison to the most used methods (LPCC, MFCC). This study opens a new way towards different feature extraction schemes that offers better accuracy on speaker recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-500"
  },
  "shriberg04b_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Anand",
     "Venkataraman"
    ],
    [
     "Sachin",
     "Kajarekar"
    ]
   ],
   "title": "SVM modeling of \"SNERF-grams\" for speaker recognition",
   "original": "i04_1409",
   "page_count": 4,
   "order": 502,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "We describe a new approach to modeling idiosyncratic prosodic behavior for automatic speaker recognition. The approach computes prosodic features by syllable, and models syllable-feature sequences using support vector machines. We evaluate performance on development data for a system submitted to the NIST 2004 Speaker Recognition evaluation. Results show that SNERF-grams provide significant performance gains when combined with a state-of-the-art cepstral system as well as with both prosodic and word-based systems that model long-range features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-501"
  },
  "ho04_interspeech": {
   "authors": [
    [
     "Purdy",
     "Ho"
    ],
    [
     "Pedro",
     "Moreno"
    ]
   ],
   "title": "SVM kernel adaptation in speaker classification and verification",
   "original": "i04_1413",
   "page_count": 4,
   "order": 503,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "Techniques for speaker classification and verification based on discriminant based kernel methods such as Support Vector Machines are becoming more and more popular. However, when compared with state of the art statistical based techniques such as Gaussian Mixture Models, their performance suffer for two main reasons: first their inability to scale up and handle a large number of classes, and second their inability to adapt model parameters. In this paper we address the second issue.Previously [8] we have introduced a kernel based classifier that combines the best of generative methods and discriminative classifiers. Each utterance is fitted with a generative model such as a Gaussian Mixture Model (GMM) and a kernel distance is defined among GMMs. In this paper we extend this kernel with the ability to adapt to the speaker utterance by adapting the utterance GMM using Maximum Likelihood Linear Regression (MLLR) techniques. Our experimental results on two different speaker databases show that kernel adaptation is a promising technique highly effective on long utterances when compared with non-adapted kernels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-502"
  },
  "iwano04_interspeech": {
   "authors": [
    [
     "Koji",
     "Iwano"
    ],
    [
     "Taichi",
     "Asami"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Noise-robust speaker verification using F0 features",
   "original": "i04_1417",
   "page_count": 4,
   "order": 504,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "This paper proposes a noise-robust speaker verification method augmented by fundamental frequency (F0). The paper first describes a noise-robust F0 extraction method using the Hough transform. Then, it proposes a robust speaker verification method using multi-stream HMMs which fuse the extracted F0 and cepstral features. Experiments are conducted using four-connected-digit utterances of Japanese by 37 male speakers recorded at five sessions over a half year period. The utterances are contaminated with white noise at various SNR levels. Experimental results show that the F0 features improve the verification performance in all SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-503"
  },
  "chen04e_interspeech": {
   "authors": [
    [
     "Zi-He",
     "Chen"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Yau-Tarng",
     "Juang"
    ]
   ],
   "title": "Eigen-prosody analysis for robust speaker recognition under mismatch handset environment",
   "original": "i04_1421",
   "page_count": 4,
   "order": 505,
   "p1": "1421",
   "pn": "",
   "abstract": [
    "Most speaker recognition systems utilize only low-level short-term spectral features and ignore high-level long-term information, such as prosody and speaking style. This paper presents a novel eigen-prosody analysis (EPA) approach to capture long-term prosodic information of a speaker for robust speaker recognition under mismatch environment. It converts the prosodic feature contours of a speaker's speech into sequences of prosody symbols, and then transforms the speaker recognition problem into a full text document retrieval-similar task. Experimental results on the well-known HTIMIT database have shown that, even only few training/test data is available, a remarkable improvement, about 28.7% relative error rate reduction comparing with the GMM/cepstral mean subtraction (CMS) baseline, could be achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-504"
  },
  "lawson04_interspeech": {
   "authors": [
    [
     "Aaron",
     "Lawson"
    ],
    [
     "Mark",
     "Huggins"
    ]
   ],
   "title": "Triphone-based confidence system for speaker identification",
   "original": "i04_1745",
   "page_count": 4,
   "order": 506,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "The confidence system proposed evaluates the reliability of Speaker Identification (SID) result based on the compatibility of the data involved in making the judgment. The measure employed in determining data compatibility is triphone overlap between the audio clip being tested, and the audio that served as training data for the model involved in the comparison. This approach evolved out of investigations into the degree of text-dependency of data used in human speaker verification. Those tests using text-dependant SID were more successful than non-text-dependant, presumably because subtle differences in a speaker's articulation were more evident and noticeable when similar or identical sound sequences were being compared. These findings were applied to automatic SID, using a measure of degree of text-dependency between training data for the model in question and audio clip being tested. The degree of overlap measure turns out to be a very accurate predictor of the SID success.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-505"
  },
  "yoshida04b_interspeech": {
   "authors": [
    [
     "Kenichi",
     "Yoshida"
    ],
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Improved model training and automatic weight adjustment for multi-SNR multi-band speaker identification system",
   "original": "i04_1749",
   "page_count": 4,
   "order": 507,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "In our previous paper, we presented a speaker identification system using a multi-SNR multi-band method, and reported its robustness against environmental noises. This paper describes two modifications to the system for further enhancement of its noise-robustness. Firstly, 1/f noise is employed instead of white Gaussian noise to make noisy data for training multi-SNR GMMs. Secondly, recombination weights for subband likelihood are automatically adjusted based on the estimated subband noise power. For performance evaluation, text-independent speaker identification experiments were conducted on test speech data created by mixing clean speech data with 5 kinds of environmental noises: \"bus\", \"car\", \"office\", \"lobby\", and \"restaurant\" at 0 and 10 dB SNRs. By the two modifications, the identification error rate was reduced 30.3% on the average compared with the baseline multi-SNR multi-band method using white Gaussian noise and equal weights.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-506"
  },
  "mak04c_interspeech": {
   "authors": [
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Kwok-kwong",
     "Yiu"
    ],
    [
     "Ming-Cheung",
     "Cheun"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "A new approach to channel robust speaker verification via constrained stochastic feature transformation",
   "original": "i04_1753",
   "page_count": 4,
   "order": 508,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "This paper proposes a constrained stochastic feature transformation algorithm for robust speaker verification. The algorithm computes the feature transformation parameters based on the statistical difference between a test utterance and a composite GMM formed by combining the speaker and background model. The transformation is then used to transform the test utterance to fit the clean speaker model and background model before verification. By implicitly constraining the transformation, the transformed features can fit both models simultaneously. Experimental results based on the 2001 NIST evaluation set show that the proposed algorithms achieves significant improvement in both equal error rate and minimum detection cost when compared to cepstral mean subtraction and Z-norm. The performance of the proposed transformation approach is also slightly better than the short-time Gaussianization method proposed in [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-507"
  },
  "tadj04b_interspeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Christian",
     "Gargour"
    ],
    [
     "Nabil",
     "Badri"
    ]
   ],
   "title": "Best speaker-based structure tree for speaker verification",
   "original": "i04_1757",
   "page_count": 4,
   "order": 509,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "In this paper we study the use of the Wavelet transform for text-dependent speaker verification purposes. A new algorithm to construct the best admissible tree is proposed which has been used to obtain a speaker dependent tree library. Every tree in this library corresponds to the best structure for a given speaker, therefore the extracted parameters from a given tree are well suited and discriminative to the considered speaker and hence appropriate for speaker verification purposes. Experiments have been conducted using data extracted from the Yoho database. The results that have been obtained show a good level of efficiency and robustness compared to those which can be obtained by using MFCC or other wavelets and wavelets-based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-508"
  },
  "chow04_interspeech": {
   "authors": [
    [
     "David",
     "Chow"
    ],
    [
     "Waleed",
     "Abdulla"
    ]
   ],
   "title": "Robust speaker identification based on perceptual log area ratio and Gaussian mixture models",
   "original": "i04_1761",
   "page_count": 4,
   "order": 510,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "This paper presents a new feature for speaker identification called perceptual log area ratio (PLAR). PLAR is closely related to the log area ratio (LAR) feature. PLAR is derived from the perceptual linear prediction (PLP) rather than the linear predictive coding (LPC). The PLAR feature derived from PLP is more robust to noise than the LAR feature. In this paper, PLAR, LAR and MFCC features were tested in a Gaussian mixture model (GMM) based speaker identification system. The F-ratio feature analysis showed that the lower order PLAR and LAR coefficients are superior in classification performance to their MFCC counterparts. The text-independent, closed-set speaker identification accuracies, as tested on KING, YOHO and the down-sampled version of TIMIT databases were 85.29%, 97.045%, 98.81% using PLAR, 61.76%, 94.76%, 97.92% using LAR and 84.31%, 96.48%, 96.73% using MFCC. Those results showed that PLAR is better than LAR and MFCC in both clean and noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-509"
  },
  "wenndt04_interspeech": {
   "authors": [
    [
     "Stanley",
     "Wenndt"
    ],
    [
     "Richard",
     "Floyd"
    ]
   ],
   "title": "Channel frequency response correction for speaker recognition",
   "original": "i04_1765",
   "page_count": 4,
   "order": 511,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "While some powerful techniques have been developed for speaker recognition tasks [1], reliable decisions in a telephone environment can still be quite elusive. For example, using an in-house telephone database of long distance phone conversations, a GMM-UBM with 30 seconds for both training and testing yielded only a performance rate of 56%. Part of the difficulty with speaker identification for telephone applications is the variations and distortions in the telephone networks. This paper uses a blind channel estimate of the audio channel to modify the mel-features used in speaker identification tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-510"
  },
  "yang04c_interspeech": {
   "authors": [
    [
     "Jyh-Her",
     "Yang"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ]
   ],
   "title": "Unseen handset mismatch compensation based on a priori knowledge interpolation for robust speaker recognition",
   "original": "i04_1769",
   "page_count": 4,
   "order": 512,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "Unseen handset mismatch is the major source of performance degradation for speaker recognition in telecommunication environment since handset distortions are tightly coupled with speaker characteristics. In this paper, a soft-decision unseen handset characteristics estimation method based on a priori knowledge interpolation is proposed to decouple the characteristics of the unseen handset and speaker for robust speaker recognition. Experimental results on HTIMIT database showed that the proposed method improved the speaker recognition rate for both seen and unseen handsets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-511"
  },
  "padilla04_interspeech": {
   "authors": [
    [
     "Michael",
     "Padilla"
    ],
    [
     "Thomas",
     "Quatieri"
    ]
   ],
   "title": "A comparison of soft and hard spectral subtraction for speaker verification",
   "original": "i04_1773",
   "page_count": 4,
   "order": 513,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "An important concern in speaker recognition is the performance degradation that occurs when speaker models trained with speech from one type of channel are subsequently used to score speech from another type of channel, known as channel mismatch. This paper investigates the relative performance of two different spectral subtraction methods for additive noise compensation in the context of speaker verification. The first method, termed \"soft\" spectral subtraction, is performed in the spectral domain on the |DFT|^2 values of the speech frames while the second method, termed \"hard\" spectral subtraction, is performed on the Mel-filter energy features. It is shown through both an analytical argument as well as a simulation that soft spectral subtraction results in a higher signal-to-noise ratio in the resulting Mel-filter energy features. In the context of Gaussian mixture model-based speaker verification with additive noise in testing utterances, this is shown to result in an equal error rate improvement over a system without spectral subtraction of approximately 7% in absolute terms, 21% in relative terms, over an additive white Gaussian noise range of 5-25 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-512"
  },
  "radova04_interspeech": {
   "authors": [
    [
     "Vlasta",
     "Radova"
    ],
    [
     "Ales",
     "Padrta"
    ]
   ],
   "title": "Comparison of several speaker verification procedures based on GMM",
   "original": "i04_1777",
   "page_count": 4,
   "order": 514,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "In this paper, three speaker verification procedures are tested. All the procedures are based on Gaussian mixture models (GMM), however, they differ in the way, in which they use particular feature vectors of an utterance for speaker verification. A lot of experiments have been performed in a group of 329 speakers. The results showed that there is a procedure that enables to achieve better results than the commonly used procedure based on the log likelihood of the whole utterance -- the procedure based on the majority voting rule for single feature vectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-513"
  },
  "guan04_interspeech": {
   "authors": [
    [
     "Yong",
     "Guan"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Hongwei",
     "Qi"
    ],
    [
     "Jue",
     "Wang"
    ]
   ],
   "title": "Improving performance of text-independent speaker identification by utilizing contextual principal curves filtering",
   "original": "i04_1781",
   "page_count": 4,
   "order": 515,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "In this paper, a novel filtering method in feature extraction of speech is proposed for text-independent speaker identification, called Contextual Principal Curves Filtering (CPCF). The CPCF provides a good nonlinear summary of a sequence of cepstral vectors on the time context and, the most important, keeps their intrinsic trajectory characteristics, so the CPCF algorithm do improve the cepstral coefficients to represent speech feature more precisely. We apply this CPCF algorithm into two protocols in the framework of close-set text-independent speaker identification, where the experimental data are collected from a subset of 863 speech database of China National High Technology Project. The results show a steady relative error rate reduction of the identification for more than 20% compared with the use of the conventional Mel-frequency cepstral coefficients under both of the two protocols.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-514"
  },
  "chien04c_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Chuan-Wei",
     "Ting"
    ]
   ],
   "title": "Speaker identification using probabilistic PCA model selection",
   "original": "i04_1785",
   "page_count": 4,
   "order": 516,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "Gaussian mixture model (GMM) techniques are popular for speaker identification. Theoretically, each Gaussian function should have a full covariance matrix. However, the diagonal covariance matrix is usually used because the inverse of diagonal covariance matrix can be easily calculated via expectation maximization (EM) algorithm. This paper proposes a new probabilistic principal component analysis (PPCA) model for speaker identification. The full covariance of speaker's data is considered. This model is originated from factor analysis theory. The probability distributions using PPCA are well defined. In particular, GMM and PPCA are found to be equivalent when using diagonal covariance matrix. In this study, we derive a novel PPCA model selection and establish models for different speakers. Applying PPCA model selection, we can dynamically determine the numbers of speech features and mixture components. Experiments show that PPCA achieves desirable speaker recognition performance with proper model regularization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-515"
  },
  "aronowitz04b_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "David",
     "Burshtein"
    ],
    [
     "Amihood",
     "Amir"
    ]
   ],
   "title": "Text independent speaker recognition using speaker dependent word spotting",
   "original": "i04_1789",
   "page_count": 4,
   "order": 517,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "This paper is motivated by the fact that text dependent speaker recognition is inherently more accurate than text independent speaker recognition. In this work we assign models to frequent words spoken by a speaker and spot them in a test call. In this way, text-dependent speaker recognition technology can be used for text independent tasks. The approach we take is to use DTW (Dynamic Time Warp) word spotting to find words in the test that resemble words in the train set. Results on the SPIDRE corpus show that using a combined DTW spotter based system and a GMM system improves performance significantly. For very low false acceptance rate (0.1%) misdetection was reduced from 32.2% to 23.3% (28% reduction). For low false acceptance rate (1%) misdetection was reduced from 28.9% to 21.1% (27% reduction).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-516"
  },
  "wang04k_interspeech": {
   "authors": [
    [
     "Hsiao-Chuan",
     "Wang"
    ],
    [
     "Jyh-Min",
     "Cheng"
    ]
   ],
   "title": "A study on model-based equal error rate estimation for automatic speaker verification",
   "original": "i04_1793",
   "page_count": 4,
   "order": 518,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "Usually, we need a large number of testing samples to evaluate the performance of automatic speaker verification (ASV) systems. The equal error rate (EER) is a common measure for this purpose. It is derived according to the threshold determined by finding the verification score when the false rejection rate (FRR) equals to the false acceptance rate (FAR). In this paper, a method of model-based EER estimation for the ASV system is proposed. The goal is to estimate the EER directly from the speaker model parameters without running the speaker verification experiments using a large number of testing samples. The verification scores are computed using the model parameters, and then both FRR and FAR are derived. With a small number of testing samples, we can adjust the score distribution to estimate the EER of the ASV system. The experimental result shows that the proposed method is effective and very promising for feedback loop design of an ASV system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-517"
  },
  "matsui04_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Kunio",
     "Tanabe"
    ]
   ],
   "title": "Probabilistic speaker identification with dual penalized logistic regression machine",
   "original": "i04_1797",
   "page_count": 4,
   "order": 519,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "This paper investigates a probabilistic speaker identification method based on the dual Penalized Logistic Regression Machines (dPLRMs). The machines employ kernel functions which map an acoustic feature space to a higher dimensional space as is the case with the Support Vector Machines (SVMs). Nonlinearity in discriminating each speaker is implicitly handled in this space. While SVMs maximize the margin between two classes of data, dPLRMs maximize a penalized likelihood of a logistic regression model for multi-class discrimination. dPLRMs provide a probability estimate of each identification decision. We show that the performance of dPLRMs is competitive with that of SVMs through text-independent speaker identification experiments in which speech data recorded by 10 male speakers in four sessions are analized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-518"
  },
  "saeta04_interspeech": {
   "authors": [
    [
     "Javier R.",
     "Saeta"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Model quality evaluation during enrolment for speaker verification",
   "original": "i04_1801",
   "page_count": 4,
   "order": 520,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "The amount of data usually determines the robustness of speaker models in speaker recognition. In this sense, it is convenient to set a model quality measure for every speaker and to classify models into different categories according to their quality level. We propose a new quality measure, which uses only data from clients, based on the number of training utterances that surpass a predefinde threshold. If the desired quality is not high enough, the quality measure allows for the detection of non-representative utterances. Once selected, these utterances, considered as outliers, can be removed or better replaced by new ones coming from the same speaker. A database of 184 speakers in Spanish is used to obtain empirical results with connected digits. Our experiments removing outliers and replacing them by new utterances coming from the same speaker outperform the baseline experiments by 40%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-519"
  },
  "frati04_interspeech": {
   "authors": [
    [
     "Pasi",
     "Frati"
    ],
    [
     "Evgeny",
     "Karpov"
    ],
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "Real-time speaker identification",
   "original": "i04_1805",
   "page_count": 4,
   "order": 521,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "In speaker identification, most of the computation originates from distance or likelihood computations between the feature vectors of the unknown speaker and the models in the database. The identification time depends on the number of feature vectors, their dimensionality, the complexity of the speaker models and the number of speakers. In this paper, we focus on optimizing vector quantization (VQ) based speaker identification. We reduce the number of test vectors by pre-quantizing the test sequence prior to matching, and the number of speakers by pruning out unlikely speakers during the identification process. The best variants are then generalized to Gaussian mixture model (GMM) based modeling also. We obtain a speed-up factor of 16:1 with VQ-based system, and 34:1 with GMM-based system with a minor degradation in the identification error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-520"
  },
  "elyazeed04_interspeech": {
   "authors": [
    [
     "Mohammed Abu",
     "El-Yazeed"
    ],
    [
     "Nemat Abdel",
     "Kader"
    ],
    [
     "Mohammed",
     "El-Henawy"
    ]
   ],
   "title": "Multi-codebook vector quantization algorithm for speaker identification",
   "original": "i04_1809",
   "page_count": 4,
   "order": 522,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "This paper introduces an algorithm for speaker identification based on multi-codebook vector quantization (MCVQ). MCVQ combines different size codebooks to achieve high recognition accuracy for text-independent speaker identification and reduce the number of distortion calculations during matching between test frame and speakers' codebooks. Experimental work has shown that the proposed model speed up the matching process without approximately decreasing the identification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-521"
  },
  "cheung04_interspeech": {
   "authors": [
    [
     "Ming-Cheung",
     "Cheung"
    ],
    [
     "Kwok-Kwong",
     "Yiu"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "Multi-sample fusion with constrained feature transformation for robust speaker verification",
   "original": "i04_1813",
   "page_count": 4,
   "order": 523,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "This paper proposes a single-source multiple-sample fusion approach to text-independent speaker verification. In conventional speaker verification systems, the scores obtained from claimant's utterances are averaged and the resulting mean score is used for decision making. Instead of using an equal weight for all scores, we propose assigning a different weight to each score, where the weights are made dependent on the difference between the score values and a speaker-dependent reference score obtained during enrollment. As the fusion weights depend on the verification scores, we applied a technique called constrained stochastic feature transformation to minimize the mismatch between enrollment and verification data in order to enhance the scores' reliability. Experimental results based on the 2001 NIST evaluation set show that the proposed fusion approach outperforms the equal-weight approach by 22% in terms of equal error rate and 16% in terms of minimum detection cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-522"
  },
  "betser04_interspeech": {
   "authors": [
    [
     "Michael",
     "Betser"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Mathieu",
     "Ben"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "Speaker diarization using bottom-up clustering based on a parameter-derived distance between adapted GMMs",
   "original": "i04_2329",
   "page_count": 4,
   "order": 524,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "In this paper, we present an approach for speaker diarization based on segmentation followed by bottom-up clustering, where clusters are modeled using adapted Gaussian mixture models. We propose a novel inter-cluster distance in the model parameter space which is easily computable and which can both be used as the dissimilarity measure in the clustering scheme and as a stop criterion. Using adapted Gaussian mixture models enables a good description of the feature vector distribution within a cluster while adaptation prevents over-training for clusters with few data. Experiments carried out on broadcast news data in French demonstrate the potential of the proposed approach which exhibits performance similar to BIC clustering. However, our clustering method appeared to be more sensitive to segmentation errors than the BIC approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-523"
  },
  "zheng04d_interspeech": {
   "authors": [
    [
     "Nengheng",
     "Zheng"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Time -frequency analysis of vocal source signal for speaker recognition",
   "original": "i04_2333",
   "page_count": 4,
   "order": 525,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "This paper investigates the importance of spectro-temporal characteristics of the source excitation signal for speaker recognition. We propose an effective feature extraction technique for obtaining essential time-frequency information from the linear prediction (LP) residual signal, which are closely related to the glottal excitation of individual speaker. With pitch synchronous analysis, wavelet transform is applied to every two pitch cycles of the LP residual signal to generate a new feature vector, called Wavelet Octave Coefficients of Residues (WOCOR), which provides additional speaker discriminative power to the commonly used linear predictive Cepstral coefficients (LPCC). Experimental evaluation over a Cantonese speaker recognition corpus demonstrates the effectiveness of WOCOR for speaker recognition. Recognition tests with WOCOR and LPCC outperforms the conventional methods of using Mel Frequency Cepstral Coefficients (MFCC).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-524"
  },
  "gangadharaiah04_interspeech": {
   "authors": [
    [
     "Rashmi",
     "Gangadharaiah"
    ],
    [
     "Balakrishnan",
     "Narayanaswamy"
    ],
    [
     "Narayanaswamy",
     "Balakrishnan"
    ]
   ],
   "title": "A novel method for two-speaker segmentation",
   "original": "i04_2337",
   "page_count": 4,
   "order": 526,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "This paper proposes a novel method for two-speaker audio segmentation, which creates a model for each speaker from the available data on the fly. This can be viewed as building a Hidden Markov Model(HMM) for the data with speakers abstracted as the hidden states. A clustering technique using Generalized Likelihood Ratio(GLR) metric, for good initialization of each Gaussian Mixture Model(GMM), such that each state corresponds to a single speaker and not noise, silence or word classes, is described. Finally, a refinement method, similar to Viterbi Training of HMMs is presented. The proposed method does not require prior knowledge of any speaker characteristics or tuning of threshold parameters, so it can be used with confidence over new data sets. The method results in a decrease in the error rate by 84.75% on the files reported in the baseline system. It performs well even with short speaker segments of 1s each.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-525"
  },
  "yegnanarayana04_interspeech": {
   "authors": [
    [
     "Bayya",
     "Yegnanarayana"
    ],
    [
     "A.",
     "Shahina"
    ],
    [
     "M. R.",
     "Kesheorey"
    ]
   ],
   "title": "Throat microphone signal for speaker recognition",
   "original": "i04_2341",
   "page_count": 4,
   "order": 527,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "Speaker recognition systems perform better when clean speech signals are used for the task. In the presence of high levels of background noise, speech recorded from a close speaking microphone will be degraded and hence the performance of the speaker recognition system. Use of a transducer held at the throat results in a signal that is clean even in a noisy environment. This paper discusses the prospect of using such signals for speaker recognition. A study of a text-independent speaker recognition system based on features extracted from speech simultaneously recorded using a throat microphone and a close-speaking microphone in clean and simulated noisy conditions is conducted. Autoassociative neural networks are used to model the speaker characteristics based on the vocal tract system and excitation source features represented by weighted linear prediction cepstral coefficients and linear prediction residual, respectively. The results of experimental studies show that the speech collected from the throat microphone can be used for tasks like speaker recognition, especially in noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-526"
  },
  "benzeghiba04_interspeech": {
   "authors": [
    [
     "Mohamed Faouzi",
     "Ben Zeghiba"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Posteriori probabilities and likelihoods combination for speech and speaker recognition",
   "original": "i04_2345",
   "page_count": 4,
   "order": 528,
   "p1": "2345",
   "pn": "2348",
   "abstract": [
    "This paper investigates a new approach to perform simultaneous speech and speaker recognition. The likelihood estimated by a speaker identification system is combined with the posterior probability estimated by the speech recognizer. So, the joint posterior probability of the pronounced word and the speaker identity is maximized. A comparison study with other standard techniques is carried out in three different applications, (1) closed set speech and speaker identification, (2) open set speech and speaker identification and (3) speaker quantization in speaker-independent speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-527"
  },
  "mihoubi04_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Mihoubi"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "The use of typical sequences for robust speaker identification",
   "original": "i04_2349",
   "page_count": 4,
   "order": 529,
   "p1": "2349",
   "pn": "2352",
   "abstract": [
    "Speech field is affected by accidental structures such as spurious events and artifacts (breath mouth, lip clicks etc). Because the whole utterance is used during training and the identification process, these factors may represent confusable acoustic classes which do not contribute to the performance of the speaker recognition system. In this paper we propose a new approach for extracting as much essential information as possible from the acoustic data in order to estimate more robust speaker models. To this end, we make use of the Asymptotic Equipartition Property (AEP) originated from the information theory to generate a reduced feature subspace termed typical set. Results on the Spidre corpus show that the method leads to an appreciable improvement (averaging 10 % gain in performance) over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-528"
  },
  "kim04l_interspeech": {
   "authors": [
    [
     "KyungHwa",
     "Kim"
    ]
   ],
   "title": "A forensic phonetic investigation into the duration and speech rate",
   "original": "i04_2353",
   "page_count": 4,
   "order": 530,
   "p1": "2353",
   "pn": "2356",
   "abstract": [
    "In this paper, the usefulness of the duration and rate of speech is examined as speaker identification parameter in forensic phonetics. Speech samples from 2 actual criminal cases were chosen. The duration and the duration ratio of each syllable of \"yeo-bo-seyo (hello) and other repeated phrases in Korean delivered by the criminals and the suspects were measured. The results showed that duration ratio from the speech samples of one person were almost identical and were different from the other speaker. Subjects spoke at different speeds but, for more convincing results, it is necessary to consider the speaker's speaking habits of pause and hesitation, as well as different speech occasions and styles, which are also potential for speaker identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-529"
  },
  "sreenivas04b_interspeech": {
   "authors": [
    [
     "T. V.",
     "Sreenivas"
    ],
    [
     "Sameer",
     "Badaskar"
    ],
    [
     "Sameer",
     "Badaskar"
    ]
   ],
   "title": "Mixture Gaussian model training against impostor model parameters: an application to speaker identification",
   "original": "i04_2357",
   "page_count": 4,
   "order": 531,
   "p1": "2357",
   "pn": "2360",
   "abstract": [
    "In this paper we aim to improve the performance of Gaussian Mixture Model (GMM) classifier using Impostor model parameters for a closed set Speaker Identification task. We propose a novel method of speaker model training which uses the parameters of an Impostor Model to discriminatively train, in order to improve the performance of the GMM based classifier. This is unlike conventional techniques which use Minimum Classification Error (MCE) criterion which uses the ensemble of speaker data, instead. An experimental comparison between the conventional GMM and the proposed technique is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-530"
  },
  "anguita04_interspeech": {
   "authors": [
    [
     "Jan",
     "Anguita"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "Jacobian adaptation with improved noise reference for speaker verification",
   "original": "i04_2361",
   "page_count": 4,
   "order": 532,
   "p1": "2361",
   "pn": "2364",
   "abstract": [
    "Jacobian Adaptation (JA) of the acoustic models is a fast adaptation technique that has been used in Automatic Speech Recognition (ASR) systems to adapt the models from the training to the testing noise conditions. In this work we have used the JA technique in a speaker verification system. In the previous implementations of JA only one reference of the training noise conditions was used to adapt all the models. In speaker recognition systems the utterances of each speaker are only used to train his/her model. Therefore, the training noise reference can be improved by estimating a specific reference for each speaker because each speaker model is trained in different noise conditions. With this approach, which we call Model-dependent Noise Reference Jacobian Adaptation (MNRJA), a better noise estimation is obtained and therefore a better adaptation of the models. In our speaker verification tests the MNRJA approach outperformed the conventional JA technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-531"
  },
  "siafarikas04_interspeech": {
   "authors": [
    [
     "Mihalis",
     "Siafarikas"
    ],
    [
     "Todor",
     "Ganchev"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "Objective wavelet packet features for speaker verification",
   "original": "i04_2365",
   "page_count": 4,
   "order": 533,
   "p1": "2365",
   "pn": "2368",
   "abstract": [
    "Studying ways for achieving a better demarcation of human voices for the task of speaker verification and taking advantage of the flexibility provided by wavelet packet analysis, we investigate in an objective way the relative importance of constituent disjoint frequency subbands of speech signals. Based on experimental results measuring the actual contribution of these subbands in relation to the corresponding frequency resolution, we propose a novel wavelet packet-based speech feature set that is effectively designed for speaker verification. The practical significance of our approach has been evaluated in comparative experiments performed on 2001 NIST Speaker Recognition Evaluation database. The proposed wavelet packet feature set has proven to outperform the widely used Mel-frequency scaled cepstral coefficients (MFCCs), as well as other wavelet packet based features that have been successfully used for speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-532"
  },
  "chaudhari04_interspeech": {
   "authors": [
    [
     "Upendra V.",
     "Chaudhari"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ]
   ],
   "title": "Policy analysis framework for conversational biometrics",
   "original": "i04_2369",
   "page_count": 4,
   "order": 534,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "Modern speaker verification systems rely on a variety of input sources in making a decision on the validity of an identity claim. The meaning of the evidence these sources produce must be reconciled if robust decisions are to be made. In the case of Conversational Biometrics (CB), for example, this is typically accomplished via the specification of a verification policy implemented as a finite state machine. This paper presents a framework for the analysis of such complex systems guided by a policy finite state machine. The Receiver Operating Curve (ROC) associated with the acoustic speaker recognition task is transformed into a multi-dimensional Receiver Operating Map (ROM), which results from a probabilistic analysis of the policy state machine. A DCF Map can be similarly generated and we show results indicating that optimization over this surface is an appropriate way to set thresholds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-533"
  },
  "choi04c_interspeech": {
   "authors": [
    [
     "Woo-Yong",
     "Choi"
    ],
    [
     "Jung Gon",
     "Kim"
    ],
    [
     "Hyung Soon",
     "Kim"
    ],
    [
     "Sung Bum",
     "Pan"
    ]
   ],
   "title": "A new score normalization method for speaker verification with virtual impostor model",
   "original": "i04_2373",
   "page_count": 4,
   "order": 535,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "We present a score normalization method, which is suitable for embedded speaker verification system. Proposed score normalization method does not need physical or actual impostor model but utilizes the client model's mean and covariance vector to imitate an impostor model. Therefore, no additional memory for impostor model is required. Furthermore, most parts of score evaluation process for impostor model are identical with that of client model. As a result, computational burden for virtual cohort model is trivial. when comparing with un-normalized likelihood score based speaker verification system, average error rate reduction of the proposed system is higher than 6.7%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-534"
  },
  "kim04m_interspeech": {
   "authors": [
    [
     "Samuel",
     "Kim"
    ],
    [
     "Thomas",
     "Eriksson"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "On the time variability of vocal tract for speaker recognition",
   "original": "i04_2377",
   "page_count": 4,
   "order": 536,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "A novel scheme to analyze the effects of time variability of vocal tract for speaker recognition is proposed. We adopt a pitch synchronous feature extraction method to describe even more detailed characteristics of vocal tract, and decompose it into rapidly varying and slowly varying components with a specified linear filter along with time axis. Speaker identification tasks are performed with weighted combination of two decomposed feature sets and their corresponding models to show the efficiency of each decomposed feature set. Simulation results show that slowly varying components contain more speaker discriminative information than rapidly varying components do.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-535"
  },
  "desai04_interspeech": {
   "authors": [
    [
     "Veena",
     "Desai"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Distributed speaker recognition",
   "original": "i04_2381",
   "page_count": 4,
   "order": 537,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "Speech recognition systems are gaining increasing importance with the wide-spread use of mobile and portable devices and other interactive voice response systems. Because of the resource constraints on such devices and the requirements of specific applications, the need to perform speech recognition over a data network becomes inevitable. The requirements of such a system with a human at one end and a machine at the other end are clearly asymmetric. The major focus of this work is to enable speaker recognition for information access over the network. Assuming that at the client end the device is either a Personal Digital Assistant(PDA) or a cellphone, an attempt is made to perform part of computation at the client end, thus conserve bandwidth. Experiments have been performed on both TIMIT data and TIMIT data passed through a speech codec. The results indicate that by performing feature extraction at the client end, the bitrate can be reduced significantly to 13.6kbps with 96% recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-536"
  },
  "angkititrakul04_interspeech": {
   "authors": [
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "Sepideh",
     "Baghaii"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Cluster-dependent modeling and confidence measure processing for in-set/out-of-set speaker identification",
   "original": "i04_2385",
   "page_count": 4,
   "order": 538,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "In this paper, we propose an approach to address the problem of text-independent open-set speaker identification. The in-set speakers are clustered into smaller subsets without merging speaker models. The Anti-Speaker or Background Model is then adapted for each subset which minimizes the identification errors of the pseudo impostors during the training stage. Score normalization is applied to align all the in-set speaker score distributions to share a single scale. Finally, confidence measure processing is used to identify in-set versus out-of-set speakers. Experiments with TIMIT and the CU-Accent corpora show an improvement in Equal Error Rate on the average of 20.28% and 8.35% over the baseline performance respectively. Finally, a probe experiment is also included that considers prosody for in-set speaker detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-537"
  },
  "umeda04_interspeech": {
   "authors": [
    [
     "Yoshiyuki",
     "Umeda"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Fuji",
     "Ren"
    ]
   ],
   "title": "Distributed speaker recognition using earth mover's distance",
   "original": "i04_2389",
   "page_count": 4,
   "order": 539,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "In this paper, we focus on distributed speaker recognition a technique, in which quantized feature parameters are sent to a server, as with distributed speech recognition. The Gaussian mixture model is trained using the maximum likelihood approach. The GMM has output probability functions with continuous density functions. It is difficult to fit continuous density functions to quantized data. To overcome this problem, we propose a novel speaker recognition technique which does not need speaker model training. The proposed method directly calculates the distance between a set of quantized feature parameters of registered speech and a set of quantized feature parameters of test speech. To measure distance, we use Earth Mover's Distance (EMD). We conduct text-independent speaker identification experiments using the proposed method. When compared to results using the traditional GMM, the proposed method yielded relative error reductions of 80% for quantized data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-538"
  },
  "barlow04_interspeech": {
   "authors": [
    [
     "Michael",
     "Barlow"
    ],
    [
     "Mehrdad",
     "Khodai-Joopari"
    ],
    [
     "Frantz",
     "Clermont"
    ]
   ],
   "title": "A forensically-motivated tool for selecting cepstrally-consistent steady-states from non-contemporaneous vowel utterances",
   "original": "i04_2393",
   "page_count": 4,
   "order": 540,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "We describe a forensically-motivated, semi-automatic tool, which yields steady-state locations and cepstral parameters for contemporaneous and non-contemporaneous recordings of the five vowels in spoken Japanese. Using the notion of spectral prototype obtained from the mean cepstrum of a vowel's high-energy interval, coupled with the peak-sensitivity property of the index-weighted cepstral distance, the tool is able to find steady-state intervals that are the least-phonetically deviant from the prototype. In addition to the consistency in steady-state location afforded by this approach, non-contemporaneity is taken into account by seeking the minimum deviation across all recordings. The overall design of the tool draws its efficiency from the interactive ability to quickly alter settings and visualize intermediate results in the time and frequency domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-539"
  },
  "alexander04_interspeech": {
   "authors": [
    [
     "Anil",
     "Alexander"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Scoring and direct methods for the interpretation of evidence in forensic speaker recognition",
   "original": "i04_2397",
   "page_count": 4,
   "order": 541,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "In forensic speaker recognition, the strength of evidence is estimated using the likelihood ratio, which is the relative probability of observing the evidence, given the hypothesis that the suspect is the source of the questioned recording and the hypothesis that anyone else in a relevant potential population is its source. In order to calculate the likelihood ratio we use two approaches; one, directly using the likelihoods returned by the Gaussian Mixture Models (GMMs), and the other by modeling the distribution of these likelihood scores and then deriving the likelihood ratio on the basis of these score distributions. The former approach is used implicitly in speaker verification systems, although in forensic speaker recognition, the latter is preferred as it does not depend on the automatic speaker recognition technique used. We propose statistical representations in order to evaluate the strength of evidence in each of these two methods and compare them.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-540"
  },
  "kinnunen04_interspeech": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Evgeny",
     "Karpov"
    ],
    [
     "Pasi",
     "Franti"
    ]
   ],
   "title": "Efficient online cohort selection method for speaker verification",
   "original": "i04_2401",
   "page_count": 4,
   "order": 542,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "Cohort normalization is a method for normalizing the scores in speaker verification in order to reduce undesirable variation arising from acoustically mismatched conditions. A particular form of cohort normalization, unconstrained cohort normalization (UCN) is addressed in this study. The UCN method has been shown to give excellent results but its major drawback is the huge computational load arising from the search of the cohort speakers. In this paper, we propose a fast cohort search algorithm, that quantizes the test vector sequence and uses the quantized data for both impostor and claimant scoring. Results on the NIST-1999 corpus show a speed-up factor of 23:1 compared to full search. Furthermore, the equal error rates are decreased from those of the full search.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-541"
  },
  "navratil04_interspeech": {
   "authors": [
    [
     "Jiri",
     "Navratil"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ],
    [
     "Ran D.",
     "Zilca"
    ]
   ],
   "title": "Statistical model migration in speaker recognition",
   "original": "i04_2585",
   "page_count": 4,
   "order": 543,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "In large-scale deployments of speaker recognition systems the potential for legacy problems increases as the evolving technology may require configuration changes in the system thus invalidating already existing user voice accounts. Unless the entire database of original speech waveform were stored, users need to reenroll to keep their accounts functional, which, however, may be expensive and commercially not acceptable. We define model migration as a conversion of obsolete models to new-configuration models without additional data and waveform requirements and investigate ways to achieve such a migration with minimum loss of system accuracy. As a proof-of-concept, an algorithm for statistical migration in the Maximum A-Posteriori framework is studied and evaluated experimentally using the NIST SRE-03 dataset. The migration step is discussed in a wider conceptual framework of Conversational Biometrics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-542"
  },
  "khan04b_interspeech": {
   "authors": [
    [
     "A. Nayeemulla",
     "Khan"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Latent semantic analysis for speaker recognition",
   "original": "i04_2589",
   "page_count": 4,
   "order": 544,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "There exists certain traits specific to a speaker that help in easy identification of the speaker among a familiar set of speakers. These include certain disfluencies, and mannerisms like stress for certain words, frequent usage of certain phrases, manner of pronunciation and back channels. The focus of this paper is identification of a speaker using such idiolectic traits in conversational speech. Every normal conversation by a speaker contains his idiolectic signature. A model is developed in the latent semantic analysis framework to capture this signature. The similarity of the idiolectic signature in the test utterance to that captured by the model is used to hypothesise the target speaker. The technique is demonstrated for the NIST 2003 extended data task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-543"
  },
  "shao04b_interspeech": {
   "authors": [
    [
     "Yang",
     "Shao"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Model-based sequential organization for cochannel speaker identification",
   "original": "i04_2593",
   "page_count": 4,
   "order": 545,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "It is difficult to directly apply traditional speaker identification (SID) systems to cochannel speech, mixtures from two speakers. Previous work demonstrates that extraction of usable speech segments significantly improves SID performance if speaker assignment, or sequential organization of the segments, is known. We derive a joint computational objective for speaker assignment and cochannel SID, leading to a problem of search for the optimum hypothesis. We propose a hypothesis pruning method based on speaker models to make the search computationally feasible. Evaluation results show that the proposed algorithm approaches the ceiling SID performance obtained with prior pitch information, and yields significant improvement over alternative approaches on speaker assignment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-544"
  },
  "leung04_interspeech": {
   "authors": [
    [
     "Ka-Yee",
     "Leung"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "Articulatory feature-based conditional pronunciation modeling for speaker verification",
   "original": "i04_2597",
   "page_count": 4,
   "order": 546,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "Due to the differences in education background, accents, etc., different individuals have their unique way of pronunciation. This paper exploits the pronunciation characteristics of speakers and proposes a new conditional pronunciation modeling (CPM) technique for speaker verification. The proposed technique aims to establish a link between articulatory properties (such as manners and places of articulation) and phoneme sequences produced by a speaker. This is achieved by aligning two articulatory feature (AF) streams with a phoneme sequence determined by a phoneme recognizer, and formulating the probabilities of articulatory classes conditioned on the phonemes as speaker-dependent probabilistic models. The scores obtained from the AF-based pronunciation models are then fused with those obtained from a spectral-based speaker verification system, with the frame-by-frame fused scores weighted by the confidence of the pronunciation models. Evaluations based on the SPIDRE corpus demonstrate that AF-based CPM systems can recognize speakers even with short utterances and are readily combined with spectral-based systems to further enhance the reliability of speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-545"
  },
  "park04e_interspeech": {
   "authors": [
    [
     "Alex",
     "Park"
    ],
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "A comparison of normalization and training approaches for ASR-dependent speaker identification",
   "original": "i04_2601",
   "page_count": 4,
   "order": 547,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "In this paper we discuss a speaker identification approach, called ASR-dependent speaker identification, that incorporates phonetic knowledge into the models for each speaker. This approach differs from traditional methods for performing text-independent speaker identification, such as global Gaussian mixture modeling, that typically ignore the phonetic content of the speech signal. We introduce a new score normalization approach, called phone adaptive normalization, which improves upon our previous speaker adaptive normalization technique. This paper also examines the use of automatically generated transcriptions during the training of our speaker models. Experiments show that speaker models trained using automatically generated transcriptions achieve the same performance as models trained using manually generated transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-546"
  },
  "tran04_interspeech": {
   "authors": [
    [
     "Dat",
     "Tran"
    ]
   ],
   "title": "New background modeling for speaker verification",
   "original": "i04_2605",
   "page_count": 4,
   "order": 548,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "A new background speaker modelling method is presented in this paper for text-independent speaker verification using Gaussian mixture models. This method does not require speech databases of other speakers to build background speaker models. A background model can be built directly from the same claimed speaker's database and has a smaller number of Gaussian mixtures compared to the claimed speaker model. Experiments performed on the YOHO database showed a better result for speaker verification using the 64- mixture claimed speaker model and 16-mixture background model compared to current background model set methods using five closest background models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-547"
  },
  "bailly04_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Bleicke",
     "Holm"
    ],
    [
     "Veronique",
     "Auberge"
    ]
   ],
   "title": "A trainable prosodic model: learning the contours implementing communicative functions within a superpositional model of intonation",
   "original": "i04_1425",
   "page_count": 4,
   "order": 549,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "This paper introduces a new model-constrained, data-driven method to generate prosody from metalinguistic information. We refer here to the general ability of intonation to demarcate speech units and convey information about the propositional and interactional functions of these units within the discourse. Our strong hypotheses are that (1) these functions are directly implemented as prototypical prosodic contours that are coextensive to the unit(s) they apply to, (2) the prosody of the message is obtained by superposing and adding all the contributing contours. We describe here an analysis-by-synthesis scheme that consists in identifying these prototypical contours and separating out their contributions in the prosodic contours of the training data. We will show that such a trainable prosodic model generates faithful prosodic contours with very few prototypical movements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-548"
  },
  "nguyen04_interspeech": {
   "authors": [
    [
     "Dung Tien",
     "Nguyen"
    ],
    [
     "Mai Chi",
     "Luong"
    ],
    [
     "Bang Kim",
     "Vu"
    ],
    [
     "Hansjoerg",
     "Mixdorff"
    ],
    [
     "Huy Hoang",
     "Ngo"
    ]
   ],
   "title": "Fujisaki model based F0 contours in vietnamese TTS",
   "original": "i04_1429",
   "page_count": 4,
   "order": 550,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "The current paper presents preliminary work towards the integration of the Fujisaki model into the VnVoice Vietnamese TTS system, based on a set of rules to control the F0 contour. A speech corpus consisting of 20 sentences was compiled. Each of the sentences can have various meanings depending on the tone associated with a monosyllabic keyword which it contains. The corpus with a total of 46 sentences was recorded by a female speaker whose voice had also been used in the speech corpus for VnVoice, and labeled at the syllabic level. Tone contrast perception results and naturalness comparisons show that the Fujisaki model works well in modeling F0 contour of Vietnamese tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-549"
  },
  "ashimura04_interspeech": {
   "authors": [
    [
     "Kazuyuki",
     "Ashimura"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Estimating speaking rate in spontaneous speech from z-scores of pattern durations",
   "original": "i04_1433",
   "page_count": 4,
   "order": 551,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "We propose a novel method for estimating speech rate based on the durations of similar patterns as a first step in determining the relation between various speaking styles used in everyday conversation and speaker intentions or attitudes. Whereas most methods of determining speaking rate require manually obtained label information or linguistic knowledge, the proposed method uses patterns of speech-sound sequences that occur relatively frequently in dialogue speech, as detected from the speech waveform information alone. For use as an index of speaking rate, the method calculates the z-score of each pattern duration, relative to the distribution of the respective pattern groups. The method uses speech recognition to provide a rough classification of the speech sounds , i.e., as a phonetic typewriter, but without requiring accuracy of recognition in any meaningful linguistic terms. From a large body of natural dialogue speech data, it divides the label sequences obtained from the recognizer/classifier into variable length patterns according to maximum likelihood, and classifies all speech segments having the same pattern as a group. The validity of speech rate detection was evaluated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-550"
  },
  "masuko04_interspeech": {
   "authors": [
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Keisuke",
     "Miyanaga"
    ]
   ],
   "title": "A style control technique for HMM-based speech synthesis",
   "original": "i04_1437",
   "page_count": 4,
   "order": 552,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "This paper describes an approach to controlling style of synthetic speech in HMM-based speech synthesis. The style is defined as one of speaking styles and emotional expressions in speech. We model each speech synthesis unit by using a context-dependent HMM whose mean vector of the output distribution function is given by a function of a parameter vector called style control vector. We assume that the mean vector is modeled by multiple regression with the style control vector. The multiple regression matrices are estimated by EM-algorithm as well as other model parameters of HMMs. In the synthesis stage, the mean vectors are modified by transforming an arbitrarily given control vector which is associated with a desired style. The results of subjective tests show that we can control styles by choosing the style control vector appropriately.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-551"
  },
  "hasegawajohnson04b_interspeech": {
   "authors": [
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Stephen",
     "Levinson"
    ],
    [
     "Tong",
     "Zhang"
    ]
   ],
   "title": "Children's emotion recognition in an intelligent tutoring scenario",
   "original": "i04_1441",
   "page_count": 4,
   "order": 553,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "This paper presents an approach to automatically recognize emotion which children exhibit in an intelligent tutoring system. Emotion recognition can assist the computer agent to adapt its tutorial strategies to improve the efficiency of knowledge transmission. In this study, we detect three emotional classes: confidence, puzzle, and hesitation. Emotion is detected by means of lexical, prosodic, spectral, and syntactic analyses of users' speech. An automatic speech recognition system serves as the fundamental constituent of the system. A robust classification and regression tree (CART) integrates the various information sources together for final decision. The effectiveness of the proposed approach has been tested on data collected by Wizard-of-Oz (WoZ) experiments. Our emotion recognition was speaker-independent, and yielded 91.3% accuracy. The test results showed that the spectral and duration-related prosodic features played very important roles in emotion recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-552"
  },
  "hirose04b_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Use of prosodic features for speech recognition",
   "original": "i04_1445",
   "page_count": 4,
   "order": 554,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "Prosody is known to play an important role in human speech perception process. Therefore, there is an increasing need to use prosodic features for the advancement of speech recognition technology. However, prosody is related to various levels of information, from linguistic, para-linguistic, to non-linguistic, and, therefore, its acoustic manifestation is rather complicated with large variations. This fact prevents prosody to be incorporated in speech recognition process. In the current paper, discussions are given on how we can utilize prosodic features, showing our research works as examples. First, an idea of including word likelihood viewed from the accent type into the recognition process is shown. Second, a scheme of using prosody to control the pruning size in the decoding process is given. Prosodic features should be modeled rather differently form segmental features. Lastly, a new language model constructed by including prosodic events is explained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-553"
  },
  "peters04_interspeech": {
   "authors": [
    [
     "Jochen",
     "Peters"
    ],
    [
     "Christina",
     "Drexel"
    ]
   ],
   "title": "Transformation-based error correction for speech-to-text systems",
   "original": "i04_1449",
   "page_count": 4,
   "order": 555,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "We present a universal approach to uncover and correct systematic local errors in complex speech-to-text systems. Whereas previous work to minimize speech recognition errors mostly relies on N-best lists or word lattices, our approach is merely based on the first-best system output. The paradigm of Transformation-Based Learning is adapted from tagging-like applications to the more complicated task of text transformation which obstructs several basic TBL steps. On a professional spontaneous dictation task (including postprocessing and text formatting) we achieve error reductions of 9.6%rel on held-out test data. A special benefit of the approach is the easy interpretation of the learned rules which may serve for diagnostic purposes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-554"
  },
  "gutkin04_interspeech": {
   "authors": [
    [
     "Alexander",
     "Gutkin"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Phone classification in pseudo-euclidean vector spaces",
   "original": "i04_1453",
   "page_count": 4,
   "order": 556,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "Recently we have proposed a structural framework for modelling speech, which is based on patterns of phonological distinctive features, a linguistically well-motivated alternative to standard vector-space acoustic models like HMMs. This framework gives considerable representational freedom by working with features that have explicit linguistic interpretation, but at the expense of the ability to apply the wide range of analytical decision algorithms available in vector spaces, restricting oneself to more computationally expensive and less-developed symbolic metric tools. In this paper we show that a dissimilarity-based distance-preserving transition from the original structural representation to a corresponding pseudo- Euclidean vector space is possible. Promising results of phone classification experiments conducted on the TIMIT database are reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-555"
  },
  "chung04c_interspeech": {
   "authors": [
    [
     "Grace",
     "Chung"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Ed",
     "Filisko"
    ],
    [
     "Min",
     "Tang"
    ]
   ],
   "title": "Combining linguistic knowledge and acoustic information in automatic pronunciation lexicon generation",
   "original": "i04_1457",
   "page_count": 4,
   "order": 557,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "This paper describes several experiments aimed at the long term goal of enabling a conversational interface to automatically improve its pronunciation lexicon over time through direct interactions with end users and from available Web sources. We selected a set of 200 rare words from the OGI corpus of spoken names, and performed several experiments combining spelling and pronunciation information to hypothesize phonemic baseforms for these words. We evaluated the quality of the resulting baseforms through a series of recognition experiments, using the 200 words in an isolated word recognition task. Also reported is a modification to the letter-to-sound system, utilizing a letter-phoneme n-gram language model, either alone or in combination with the original \"column-bigram\" model, for additional linguistic constraint. The experiments confirm that acoustic information drawn from spoken examples of the words can greatly improve the quality of the baseforms, as measured by the recognition error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-556"
  },
  "chen04f_interspeech": {
   "authors": [
    [
     "Ken",
     "Chen"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Modeling pronunciation variation using artificial neural networks for English spontaneous speech",
   "original": "i04_1461",
   "page_count": 4,
   "order": 558,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "Pronunciation variation in conversational speech has caused significant amount of word errors in large vocabulary automatic speech recognition. Rule-based approaches and decision-tree based approaches have been previously proposed to model pronunciation variation. In this paper, we report our work on modeling pronunciation variation using artificial neural networks (ANN). The results we achieved are significantly better than previously published ones on two different corpora, indicating that ANN may be better suited for modeling pronunciation variation than other statistical models that have been previously investigated. Our experiments indicate that binary distinctive features can be used to effectively represent the phonological context. We also find that including pitch accent feature in input improves the prediction of pronunciation variation on a ToBI-labeled subset of the Switchboard corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-557"
  },
  "aalburg04_interspeech": {
   "authors": [
    [
     "Stefanie",
     "Aalburg"
    ],
    [
     "Harald",
     "Hoege"
    ]
   ],
   "title": "Foreign-accented speaker-independent speech recognition",
   "original": "i04_1465",
   "page_count": 4,
   "order": 559,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "This research investigated whether acoustic-phonetic knowledge of the mother tongue of a non-native speaker can be used to adapt an existing target language phoneme HMM recognizer. For this purpose three sets of phoneme HMMs were generated, one representing the target language (German), one the mother tongue of the non-native speaker (Turkish), and the third the foreign-accented pronunciation of the target language (German spoken by Turkish speakers). The latter served as a benchmark for the tested adaptation methods. A derived Hidden Markov Model (HMM) clustering algorithm was applied on the target language phoneme HMM set using the mother tongue phoneme HMM set of the non-native speaker. Following the HMM adaptation a phoneme-level pronunciation technique was applied to generate phoneme mapping rules for the lexicon adaptation task. The results revealed a relative reduction of about 6% in WER for the adapted HMM. No further improvements were observed from the lexicon adaptation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-558"
  },
  "heracleous04_interspeech": {
   "authors": [
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Yoshitaka",
     "Nakajima"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Non-audible murmur (NAM) speech recognition using a stethoscopic NAM microphone",
   "original": "i04_1469",
   "page_count": 4,
   "order": 560,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "In this paper, we introduce the Stethoscopic Non-Audible Murmur (NAM) microphone, and we focus on its application in automatic speech recognition systems. The NAM microphone is attached behind the talker's ear, and can capture very quietly uttered murmur (NAM speech). It is applicable in automatic speech recognition systems, when privacy is important in human-machine communication. Moreover, since the NAM microphone receives the speech signal directly from the body, it shows robustness against the environmental noises. In addition to these, it might be also used in special systems (speech recognition, speech transform, etc.) for sound-impaired people. By applying adaptation techniques, we performed automatic speech recognition experiments for NAM speech. Using Maximum A Posteriori (MAP) adaptation, and a combination with Maximum Likelihood Linear Regression (MLLR) adaptation we achieved for a 20k vocabulary dictation system a 93.5% word accuracy, which is a very promising result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-559"
  },
  "russell04_interspeech": {
   "authors": [
    [
     "Martin",
     "Russell"
    ],
    [
     "Shona",
     "D'Arcy"
    ],
    [
     "Lit Ping",
     "Wong"
    ]
   ],
   "title": "Recognition of read and spontaneous children's speech using two new corpora",
   "original": "i04_1473",
   "page_count": 4,
   "order": 561,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "This paper describes some of the results of research into automatic recognition of children's speech which has been conducted as part of the European Framework 5 'PF_STAR' project. Two new corpora of British English children's speech are described. The first comprises over 14 hours of read data from 159 children, while the second includes 1 hour and 23 minutes of spontaneous and emotional speech from 30 children. A partition of the data into training, evaluation and test sets is proposed, and the results of 'baseline' speech recognition experiments are presented. The results fail to demonstrate a significant improvement from the use of age dependent acoustic models, or that the emotional speech is more difficult to recognise than 'ordinary' spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-560"
  },
  "frankel04_interspeech": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Articulatory feature recognition using dynamic Bayesian networks",
   "original": "i04_1477",
   "page_count": 4,
   "order": 562,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "This paper describes the use of dynamic Bayesian networks for the task of articulatory feature recognition. We show that by modeling the dependencies between a set of 6 multi-leveled articulatory features, recognition accuracy is increased over an equivalent system in which features are considered independent. Results are compared to those found using artificial neural networks on an identical task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-561"
  },
  "bouwman04_interspeech": {
   "authors": [
    [
     "Gies",
     "Bouwman"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Predicting word correct rate from acoustic and linguistic confusability",
   "original": "i04_1481",
   "page_count": 4,
   "order": 563,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "When adapting an existing ASR-application for different user environments, one often gets confronted with speech that does not entirely match the training situation. Differences may stem both from acoustic and linguistic causes. In this paper we explore to what extent the word correct rate (WCR) for a given test set can be predicted from the transcription only (i.e. the linguistic representation) under the assumption that acoustic conditions are matched. We hope that, eventually, such a prediction can provide an estimate of a lower bound on WER to aim for when applying acoustic enhancement techniques. In this paper, we propose and compute measures for acoustic and linguistic confusability (AC and LC) of each entry in the vocabulary of an ASR engine. Using a tabulation of how correctness of actual recognition on a development set varies as a function of these confusability measures, we show that actually observed WCR of words from independent test sets can be predicted with high accuracy over the full ranges of AC and LC levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-562"
  },
  "ishihara04_interspeech": {
   "authors": [
    [
     "Kazushi",
     "Ishihara"
    ],
    [
     "Yuya",
     "Hattori"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Disambiguation in determining phonemes of sound-imitation words for environmental sound recognition",
   "original": "i04_1485",
   "page_count": 4,
   "order": 564,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "Onomatopoeia, or sound-imitation words (SIWs) are important in informing sound events in human-computer communication. One problem is listener-dependency in recognizing environmental sounds by means of SIWs, that is, different listener hears the same environmental sound as a different SIW even under the same condition. Therefore, the use of usual Japanese phonemes is not adequate to express SIWs. To cope with this ambiguity problem, we designed a set of new phonemes, referred to as the basic phoneme-groups (BPGs), to represent environmental sounds. The BPG consists of one or more Japanese phonemes, and thus, the ambiguity problem is resolved based on it by generating one or more SIWs for a sound event. An HMM-based recognizer generates SIWs using the phoneme-groups. Listening experiments showed that automatic SIW recognition based on the BPGs outperformed ones based on the other types of phonemes. The recall and precision rate were 56.4% and 72.2%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-563"
  },
  "anguita04b_interspeech": {
   "authors": [
    [
     "Jan",
     "Anguita"
    ],
    [
     "Stephane",
     "Peillon"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Alexandre",
     "Bramoulle"
    ]
   ],
   "title": "Word confusability prediction in automatic speech recognition",
   "original": "i04_1489",
   "page_count": 4,
   "order": 565,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "A new method to predict if two words are likely to be confused by an Automatic Speech Recognition (ASR) system is presented in this paper. A new inter-word dissimilarity measure based on Dynamic Time Warping (DTW) is used to classify the word pairs as confusable or not confusable. Firstly, the phonetic transcriptions of the two words to compare are aligned using only phonetic information. After the alignment, the accumulated distance is obtained with a new inter-phone acoustic distance calculated between the Hidden Markov Models (HMM) of the phones. In addition, we have used two different kinds of alignment: either with or without insertions and omissions. In a classical false acceptance/false rejection framework the prediction Equal Error Rate (EER) was measured to be 1.6%, a 50% of reduction with respect to the conventional DTW distance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-564"
  },
  "jou04_interspeech": {
   "authors": [
    [
     "Szu-Chen",
     "Jou"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Adaptation for soft whisper recognition using a throat microphone",
   "original": "i04_1493",
   "page_count": 4,
   "order": 566,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "This paper describes various adaptation methods applied to recognizing soft whisper recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, feature-space adaptation, and re-training with downsampling, sigmoidal low-pass filter, or linear multivariate regression. With these adaptation methods, the word error rate improves from 99.3% to 32.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-565"
  },
  "gruhn04_interspeech": {
   "authors": [
    [
     "Rainer",
     "Gruhn"
    ],
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A statistical lexicon for non-native speech recognition",
   "original": "i04_1497",
   "page_count": 4,
   "order": 567,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "Non-native speech is harder to recognize than native speech, because they pronounce words differently from native speakers. We propose a novel approach to cover non-native pronunciation variations statistically. Rather than explicitly representing those variations, discrete HMMs that model pronunciations of each word are generated. The models are initialized from a baseline lexicon. The phoneme distributions and transition probablilities are estimated on the results of a phoneme recognition on training data. The pronunciation HMMs are evaluated by performing rescoring of n-best continuous word recognition. The task consists of hotel reservation dialogs, spoken by non-native speakers of five accent groups. A pronunciation model is trained and evaluated separately for each group. The word error rate improves in average by 10.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-566"
  },
  "doss04_interspeech": {
   "authors": [
    [
     "Mathew Magimai",
     "Doss"
    ],
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Todd",
     "Stephenson"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Modeling auxiliary features in tandem systems",
   "original": "i04_1501",
   "page_count": 4,
   "order": 568,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "Tandem systems transform the cepstral features into posterior probabilities of subword units using an artificial neural networks (ANNs), which are processed to form input features for conventional speech recognition systems. They have been shown to perform better than conventional speech recognition systems using cepstral features. Recent studies have shown that modelling cepstral features with auxiliary sources of knowledge leads to improvement in the performance of speech recognition systems. In this paper, we study two approaches to incorporate auxiliary knowledge sources such as pitch frequency, short-term energy, etc. (referred to as auxiliary features), in a tandem-based automatic speech recognition system. In the first approach, we model the auxiliary features in the process of training ANNs, which are later used to extract tandem-features. In the second approach, we extract the tandem-features from an ANN trained with cepstral features only and then model them jointly with auxiliary features. Recognition studies conducted on a connected word recognition task under clean and noisy conditions show that the performance of the tandem system can be improved by incorporating auxiliary features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-567"
  },
  "bosch04_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Survey of spontaneous speech phenomena in a multimodal dialogue system and some implications for ASR",
   "original": "i04_1505",
   "page_count": 4,
   "order": 569,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "Audio recordings of speakers using speech-driven systems show phenomena that are characteristic for on-line speech responses, such as out-of-task utterances, self-talk and speech disfluencies. This paper focuses on a survey of these phenomena as they were recorded during interactions by subjects using a multimodal system, and reports on experiments concerning the treatment of these phenomena for automatic speech recognition. This study is a starting point for the study of a richer set of on-line phenomena in speech addressed to multimodal systems and the implications for automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-568"
  },
  "cincarek04_interspeech": {
   "authors": [
    [
     "Tobias",
     "Cincarek"
    ],
    [
     "Rainer",
     "Gruhn"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Speech recognition for multiple non-native accent groups with speaker-group-dependent acoustic models",
   "original": "i04_1509",
   "page_count": 4,
   "order": 570,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "In this paper, the recognition performance for non-native English speech with two different kinds of speaker-group-dependent acoustic models is investigated. The approaches for creating speaker groups include knowledge-based grouping of non-native speakers by their first language, and the automatic clustering of speakers. Clustering is based on speaker-dependent acoustic models in speaker Eigenspace. The acoustic model for each speaker group is obtained by bootstrapping with pre-segmented speech data or adaptation of a speaker-independent native baseline model. For the decoding of a non-native speaker's utterance not seen during the training or adaptation phase, the selection of a model suitable to cope with the accent characteristics of that speaker is necessary. Here, ideal selection via an oracle and parallel decoding are examined. Evaluation is conducted in a hotel reservation task for five major accent groups, including German, French, Indonesian, Chinese and Japanese speakers. Recognition results with speaker-dependent and an accent-independent non-native model will also be reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-569"
  },
  "stouten04_interspeech": {
   "authors": [
    [
     "Frederik",
     "Stouten"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Coping with disfluencies in spontaneous speech recognition",
   "original": "i04_1513",
   "page_count": 4,
   "order": 571,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "Nowadays, automatic speech recognizers have become quite good in recognizing well prepared fluent speech (e.g. news readings). However, the recognition of spontaneous speech is still problematic. Some important reasons for this are that spontaneous speech is usually less articulated and contains a lot of disfluencies. In this paper, a new methodology for coping with disfluencies is presented and evaluated. The basic idea is to detect disfluencies and to determine the nature of these disfluencies prior to the recognition, and to use that information to control/modify the search. At present,the methodology has been elaborated for filled pauses (FP) and word repetitions (WR). It enables us to eliminate about one associated (normal word) error per disfluency without introducing a significant augmentation of the computational load.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-570"
  },
  "kwon04b_interspeech": {
   "authors": [
    [
     "Soonil",
     "Kwon"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Speaker model quantization for unsupervised speaker indexing",
   "original": "i04_1517",
   "page_count": 4,
   "order": 572,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "Speaker indexing sequentially detects points where speaker identity changes in a multi-speaker audio stream, and classifies each detected segment according to the speaker's identity. In unsupervised speaker indexing scenarios, there is no prior information/data about the speakers in the target data. To address this issue, a predetermined generic \"speaker-independent\" model set, called Sample Speaker Models (SSM), was previously proposed. While this set can be useful for more accurate speaker modeling and clustering without any target speaker models, an optimal method for sampling the models from such a set is still required. To address this problem, the Speaker Model Quantization (SMQ) method, motivated by Tree Structured Vector Quantization, is proposed. Experiments were performed with telephone conversations and broadcast news. Results showed that our new sampling approach outperformed the baseline by 5.5% absolute (37.7% relative) in error rate on 2 speaker telephone conversations, 10.7% absolute (42.5% relative) on broadcast news.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-571"
  },
  "gerosa04_interspeech": {
   "authors": [
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Diego",
     "Giuliani"
    ]
   ],
   "title": "Investigating automatic recognition of non-native children's speech",
   "original": "i04_1521",
   "page_count": 4,
   "order": 573,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "This paper presents an initial effort in the area of non-native children's speech recognition by exploiting two children databases, one consisting of speech collected from native English children, the other one consisting of English sentences read by Italian learners of English in the same age range of the native speakers. First, a baseline speech recognizer for British English was trained on the corpus of native speech and applied to recognize native and non-native speech. Word error rates achieved for Italian children were 100%-600% higher than those achieved for native English children of the same age. By using a small amount of non-native speech from a group of Italian learners of English, acoustic models were adapted to this particular category of speakers. Adaptation of both context-independent and context-dependent HMMs showed to greatly improve recognition performance on non-native speech, obtaining relative reductions in word error rate up to 66.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-572"
  },
  "liu04b_interspeech": {
   "authors": [
    [
     "Yang",
     "Liu"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Mary",
     "Harper"
    ]
   ],
   "title": "Using machine learning to cope with imbalanced classes in natural speech: evidence from sentence boundary and disfluency detection",
   "original": "i04_1525",
   "page_count": 4,
   "order": 574,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "We investigate machine learning techniques for coping with highly skewed class distributions in two spontaneous speech processing tasks. Both tasks, sentence boundary and disfluency detection, provide important structural information for downstream language processing modules. We examine the effect of data set size, task, sampling method (no sampling, downsampling, oversampling, and ensemble sampling), and learning method (bagging, ensemble bagging, and boosting) for a decision tree prosody model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-573"
  },
  "jin04b_interspeech": {
   "authors": [
    [
     "Minho",
     "Jin"
    ],
    [
     "Gyucheol",
     "Jang"
    ],
    [
     "Sungrack",
     "Yun"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Hybrid utterance verification based on n-best models and model derived from kulback-leibler divergence",
   "original": "i04_1529",
   "page_count": 4,
   "order": 575,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "In this paper, utterance verification based on hybrid scores obtained from three pairs of models is investigated. The three models considered are the on-line garbage model, the antiword function model and a model derived using Kullback-Leibler divergence. The performance of utterance verification algorithm using hypothesis testing depends on the accuracy of the estimate of the alternative hypothesis. The three models offer different perspectives in the probability estimation of the alternate hypothesis. Performance comparison between hybrid scores using different model pairs is made. In addition, performance improvement over conventional algorithm is experimentally verified.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-574"
  },
  "goto04_interspeech": {
   "authors": [
    [
     "Masataka",
     "Goto"
    ],
    [
     "Koji",
     "Kitayama"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Speech spotter: on-demand speech recognition in human-human conversation on the telephone or in face-to-face situations",
   "original": "i04_1533",
   "page_count": 4,
   "order": 576,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "This paper describes a novel speech-interface function, called \"speech spotter\", which enables a user to enter voice commands into a speech recognizer in the midst of natural human-human conversation. In the past, it has been difficult to use automatic speech recognition in human-human conversation since it was not easy to judge, from only microphone input, whether a user was speaking to another person or a speech recognizer. We solve this problem by using two kinds of nonverbal speech information: a filled pause (a vowel-lengthening hesitation like \"er...\") and voice pitch. Only when a user utters a voice command with a high pitch just after a filled pause is the voice command accepted by the speech recognizer. By using this speech-spotter function, we have built two application systems: an on-demand information system for assisting human-human conversation and a music-playback system for enriching telephone conversation. The results from using these systems have shown that the speech-spotter function is robust and convenient enough to be used in face-to-face or cellular-phone conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-575"
  },
  "lee04n_interspeech": {
   "authors": [
    [
     "Kyong-Nim",
     "Lee"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Pronunciation lexicon modeling and design for Korean large vocabulary continuous speech recognition",
   "original": "i04_1537",
   "page_count": 4,
   "order": 577,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "In this paper, we describe a pronunciation lexicon model which is especially useful for constructing morpheme-based pronunciation lexicon to improve the performance of a Korean LVCSR. There are a lot of pronunciation variations occurring at morpheme boundaries in continuous speech. For modeling of cross-morpheme pronunciation variations, we usually used a context-dependent multiple pronunciation lexicon with possible multiple phonetic transcriptions for each word. Since phonemic context together with morphological category and morpheme boundary information affect Korean pronunciation variations, we have distinguished phonological rules that can be applied to phonemes in within-morpheme and crossmorpheme. However, pronunciation variations in morpheme boundaries are increasing the lexicon size; we have designed the optimized pronunciation lexicon which is decreasing the confusability and increasing pronunciation coverage. The results of Korean Broadcast News Transcription experiments show that a reduction of 18% in pronunciation lexicon size and an absolute reduction of 0.27% in WER from the same lexical entries were achieved by building a proposed pronunciation lexicon.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-576"
  },
  "moller04_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Jan Felix",
     "Krebber"
    ],
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "Performance of speech recognition and synthesis in packet-based networks",
   "original": "i04_1541",
   "page_count": 4,
   "order": 578,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "This paper addresses the impact of packet-based transmission on the performance of speech recognizers and on the quality of synthesized speech. For both cases, degradations which are typical for packet-based networks have been generated in a controlled way, using a parametric simulation model. Recognition performance measures and subjective quality judgments are compared to the quality degradation which is expected in human communication over such channels. The results show that the impact on ASR performance differs from the one on speech quality. The latter may be predicted with a quality prediction model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-577"
  },
  "james04_interspeech": {
   "authors": [
    [
     "Alastair Bruce",
     "James"
    ],
    [
     "Ben P.",
     "Milner"
    ],
    [
     "Angel Manuel",
     "Gomez"
    ]
   ],
   "title": "A comparison of packet loss compensation methods and interleaving for speech recognition in burst-like packet loss",
   "original": "i04_1545",
   "page_count": 4,
   "order": 579,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "This work compares the performance of three compensation methods for speech recognition in the presence of packet loss. Two methods, cubic interpolation and a novel maximum a posteriori (MAP) estimation, aim to restore the feature vector stream in the event of packet loss, while the third technique applies compensation in the decoding stage of recognition through missing feature theory. To improve performance in burst-like packet loss, interleaving is introduced to disperse bursts of loss. Experiments on the ETSI Aurora connected digit task show best performance to be given by a combination of missing feature theory and cubic interpolation. This raises performance from 50.3% to 69.8% at a packet loss rate of 50% and average burst length of 20 packets. Including interleaving further increases performance to over 76%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-578"
  },
  "milner04_interspeech": {
   "authors": [
    [
     "Ben P.",
     "Milner"
    ],
    [
     "Alastair Bruce",
     "James"
    ]
   ],
   "title": "An analysis of packet loss models for distributed speech recognition",
   "original": "i04_1549",
   "page_count": 4,
   "order": 580,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "The evaluation of packet loss compensation techniques for distributed speech recognition requires an effective model of packet loss that is capable of reproducing the burst-like occurrence of loss. Several models have been applied to this task and are based on two or three state Markov chains or Markov models. This work reviews these models in terms of their channel characteristics such as the probability of packet loss and average burst length. Validation of the models is made against both GSM error patterns and a wireless LAN channel which demonstrates effective simulation. A series of speech recognition tests show that similar performance is obtained on the real and simulated channels using the packet loss models. Finally a set of model parameters is presented which allows testing across a range of channel conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-579"
  },
  "minematsu04b_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Pronunciation assessment based upon the phonological distortions observed in language learners' utterances",
   "original": "i04_1669",
   "page_count": 4,
   "order": 581,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "Speech representation provided by acoustic phonetics, spectrogram, is very noisy representation in that it shows every acoustic aspect of speech. Age, gender, size, shape, microphone, room and line are completely irrelevant to speech recognition, pronunciation assessment, and so on. But the spectrogram is affected easily by these factors. This is the very essential reason why speech systems are sometimes unreliable and the author supposes that the education should not endure this inevitable characteristics. The author proposed a novel method of acoustic representation of speech where no dimensions of the above factors exist. The method was derived by implementing phonology, another speech science, on physics. This paper examines whether the new representation of speech can provide a good tool of pronunciation assessment. Results of the experiments with good and intentionally-bad pronunciations of a single speaker showed that all the students are acoustically located between the two pronunciations, indicating that all the students are judged to be acoustically closer to the speaker than the speaker himself is. This result clearly shows that the proposed method is extremely reliable and effective in CALL.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-580"
  },
  "suzuki04c_interspeech": {
   "authors": [
    [
     "Yasuo",
     "Suzuki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Makiko",
     "Muto"
    ]
   ],
   "title": "Analysis of the phone level contributions to objective evaluation of English speech by non-natives",
   "original": "i04_1673",
   "page_count": 4,
   "order": 582,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "Aiming at automatic estimation of naturalness in timing control of non-native's speech, we have analyzed the timing characteristics of non-native's speech to correlate with corresponding subjective naturalness evaluation scores given by native speakers. In addition to word level statistical characteristics showing the differences between natives and non-natives,we analyzed phone and syllable level statistics to attain an objective measure better fit to natives' judgments.An English speech corpus spoken by Japanese was collected with temporal naturalness judgments by natives. The analysis results showed that timing differences between natives and non-natives in average syllable durations, weak vowel durations and vowel duration of function words were highly correlated with natives' naturalness evaluations. A liner regression model and a regression tree model were employed to estimate naturalness evaluation score from differences between native's speech and non-natives one. The proposed naturalness evaluation model was tested its estimation accuracy using open data.These accuracies were better than one estimated by the model using word statistics only.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-581"
  },
  "wang04l_interspeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Mitchell",
     "Peabody"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Jong-mi",
     "Kim"
    ]
   ],
   "title": "An interactive English pronunciation dictionary for Korean learners",
   "original": "i04_1677",
   "page_count": 4,
   "order": 583,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "We present research towards developing a pronunciation dictionary that features sensitivity to learners' native phonology, specifically designed for Koreans learning English. We envision a system that can process learners' imitation of the dictionary pronunciation and instantly provide segmental and prosodic feedback. Towards this goal, we have collected a speech corpus to address the phonological and prosodic issues of Korean learners. We utilize the SUMMIT speech recognizer to model phonological rules, automatically identifying non-native phonological phenomena. These phonological rules account for the influence of learners' native language on the target language. Provided feedback points out the non-native phonological variations detected by the recognizer to improve pronunciation. Instructions are also given on the prosodic aspects of pronunciation, based on duration and F0 cues. We evaluated the feedback mechanism by rating 222 English utterances from six Korean subjects, before and after receiving feedback. 61% of the utterances were judged as improved after feedback.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-582"
  },
  "rhee04_interspeech": {
   "authors": [
    [
     "Seok-Chae",
     "Rhee"
    ],
    [
     "Jeon G.",
     "Park"
    ]
   ],
   "title": "Development of the knowledge-based spoken English evaluation system and its application",
   "original": "i04_1681",
   "page_count": 4,
   "order": 584,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "This paper describes the design and implementation of the practical language learning system, and the result of the automatic pronunciation scoring for non-native English speakers. Implementing the speech recognizer, featured as two refined acoustic model sets, noise-robust data compensation, phonetic alignment, reliable rejection, etc., and by using knowledge-based acoustic-phonetic parameter estimation we achieved the correlation r=0.754 with the human rating. Also we introduce a commercial language learning architecture for web-based language tutor and the other the multimedia edutainment authoring system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-583"
  },
  "bernstein04_interspeech": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Isabella",
     "Barbier"
    ],
    [
     "Elizabeth",
     "Rosenfeld"
    ],
    [
     "John H.A.L. de",
     "Jong"
    ]
   ],
   "title": "Theory and data in spoken language assessment",
   "original": "i04_1685",
   "page_count": 4,
   "order": 585,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "Spoken language performance depends on both psycholinguistic processing in the individual and on communicative uses of language in dialogue. A social-communication view of language emphasizes that the spoken form of a language is used in social settings to accomplish explicit or implicit tasks. In testing, the communicative tradition is associated with oral proficiency interviews. A psycholinguistic view of spoken language emphasizes that the development of component skills forms the basis of real-time performance in a language. The paper reviews the development and validation of psycholinguistic testing task types (e.g. elicited imitation) that measure performance in spoken language based on empirical models of processes internal to the individual speaker-listener. Data from a recent set of experiments indicates that the two theoretical approaches to testing do not produce different patterns of proficiency scores for populations of second language speakers. This data obviates the need to posit a communicative basis for language test design.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-584"
  },
  "kawahara04e_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Masatake",
     "Dantsuji"
    ],
    [
     "Yasushi",
     "Tsubota"
    ]
   ],
   "title": "Practical use of English pronunciation system for Japanese students in the CALL classroom",
   "original": "i04_1689",
   "page_count": 4,
   "order": 586,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "We have developed a CALL system which estimates the intelligibility of Japanese students' speech and ranks their errors in terms of improving their intelligibility to native English speakers. To construct this system, we introduced (1) automatic intelligiblity assessment, (2) phoneme error detection and (3) stress error detection. To estimate intelligibility, we modeled the relationship between error rates for each type of pronunciation error and intelligibility. We have begun using our CALL system for speaking practice in an actual CALL classroom. The number of recording and recognition errors during the first trial of the system was large due to improper configuration of the headset microphones. After checking the microphone settings, performance dramatically improved. Evaluation of the system by the class was quite positive.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-585"
  },
  "beskow04_interspeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Bjorn",
     "Granstrom"
    ],
    [
     "Preben",
     "Wik"
    ]
   ],
   "title": "Design strategies for a virtual language tutor",
   "original": "i04_1693",
   "page_count": 4,
   "order": 587,
   "p1": "1693",
   "pn": "1696",
   "abstract": [
    "In this paper we discuss work in progress on an interactive talking agent as a virtual language tutor in CALL applications. The ambition is to create a tutor that can be engaged in many aspects of language learning from detailed pronunciation to conversational training. Some of the crucial components of such a system is described. An initial implementation of a stress/quantity training scheme will be presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-586"
  },
  "campana04_interspeech": {
   "authors": [
    [
     "Ellen",
     "Campana"
    ],
    [
     "Michael K.",
     "Tanenhaus"
    ],
    [
     "James F.",
     "Allen"
    ],
    [
     "Roger W.",
     "Remington"
    ]
   ],
   "title": "Evaluating cognitive load in spoken language interfaces using a dual-task paradigm",
   "original": "i04_1721",
   "page_count": 4,
   "order": 588,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "As speech interfaces become more prevalent, it is becoming more crucial that they be developed in a way that minimizes cognitive load for users. One major barrier to creating systems that are more human-centered has been the lack of an accepted online methodology for directly evaluating the cognitive resource demands of different systems. The present study extends a classic tool from cognitive psychology, the dual-task paradigm, to speech interface evaluation. Participants follow simple instructions generated by a system, while simultaneously monitoring for a simple visual probe. Performance on the monitoring task is used as a measure of cognitive resource demands; whenever language understanding is more demanding, performance on the monitoring task suffers. In the present study we used this methodology to investigate patterns of reference generation and how they impact human understanding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-587"
  },
  "black04_interspeech": {
   "authors": [
    [
     "Lesley-Ann",
     "Black"
    ],
    [
     "Norman",
     "Black"
    ],
    [
     "Roy",
     "Harper"
    ],
    [
     "Michelle",
     "Lemon"
    ],
    [
     "Michael",
     "McTear"
    ]
   ],
   "title": "The voice-logbook: integrating human factors for a chronic care system",
   "original": "i04_1725",
   "page_count": 4,
   "order": 589,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "We are developing a tele-care system for the management of hypertensive type 2 diabetes mellitus (T2DM) patients using spoken dialogue technology. This paper discusses the human factors which have been considered in the development and design of 'DI@L-log', with regard to the potential complexities posed by speech systems in the medical domain. To accomplish this goal, fine tuning of event handlers, error mitigation and the appropriateness of speech as an interface to chronic care are examined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-588"
  },
  "jokinen04_interspeech": {
   "authors": [
    [
     "Kristiina",
     "Jokinen"
    ]
   ],
   "title": "Communicative competence and adaptation in a spoken dialogue system",
   "original": "i04_1729",
   "page_count": 4,
   "order": 590,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "One of the much discussed topics in building spoken dialogue systems is how to take the users into account when designing practical systems: given the more complex environment in which we have to interact with various automatic services, it is obvious that the systems are not only required to function impeccably in regard to their technical specification, but they should also fulfill requirements concerning appropriate user needs. In this paper some usability issues are discussed from the point of view of communicative competence and adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-589"
  },
  "fu04b_interspeech": {
   "authors": [
    [
     "Zhan",
     "Fu"
    ],
    [
     "Lay Ling",
     "Pow"
    ],
    [
     "Fang",
     "Chen"
    ]
   ],
   "title": "Evaluation of the difference between the driving behavior of a speech based and a speech-visual based task of an in-car compute",
   "original": "i04_1733",
   "page_count": 4,
   "order": 591,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "To select the right modality for the interaction between drivers and the in-vehicle information system (IVIS) is crucial for safety reasons. This paper presents an experimental study to address on this area. The study was carried out on a 160 degree car-driving simulation lab. There are 10 subjects participated in the experiment. We compared the subjects driving behavior on speech input/output only and speech input with speech+visual output interaction modalities with a simple IVIS. To judge the safety status of subjects' driving performance, two independent variables which includes the average division of over speed and the average division of the car out of lane were measured as dangerous extent. Result indicates that it is not significant differences of driving performance by using synthetic speech to replace the visual display in the IVIS. It indicated that the visual presentation of a multi-modal IVIS can be acts as redundancy or complementary modality for auditory presentation, which will aids in relieving the resource demand.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-590"
  },
  "moller04b_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Jan Felix",
     "Krebber"
    ],
    [
     "Paula M. T.",
     "Smeele"
    ]
   ],
   "title": "Evaluating system metaphors via the speech output of a smart home system",
   "original": "i04_1737",
   "page_count": 4,
   "order": 592,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "This paper presents an evaluation of the speech output component of a smart home system developed under the European INSPIRE project. In particular, it is investigated how the 'personality' of the system which is conveyed by the output speech is accepted by the user. Three different metaphors are compared: An assistant visualized by a talking head, an invisible assistant, and multiple intelligent devices. The results show that a personal assistant metaphor leads to more homogenous judgments for the individual voices. Potential reasons for this finding are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-591"
  },
  "hammer04_interspeech": {
   "authors": [
    [
     "Florian",
     "Hammer"
    ],
    [
     "Peter",
     "Reichl"
    ],
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "Elements of interactivity in telephone conversations",
   "original": "i04_1741",
   "page_count": 4,
   "order": 593,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "The term \"interactivity\" has been defined in numerous ways in the context of communications, but a definition of interactivity as an instrumentally measurable parameter of conversations is still missing. In this paper, we approach this issue by applying a parametric analysis to telephone conversations recorded during speech quality tests. To this end, we extract the basic conversational parameters like speech activity, mutual silence and double talk as well as a set of conversation events like speaker alternation rate and the interruption rate. Comparing two types of scenarios for conversational speech quality assessment and exploring four different situations with regard to transmission delay, we aim at understanding the interdependencies between the conversational parameters which are basic for studying interactivity. This might, ultimately, lead to an instrumental metric distilled from the relevant parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-592"
  },
  "sansegundo04_interspeech": {
   "authors": [
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "José Manuel",
     "Pardo"
    ]
   ],
   "title": "Generating gestures from speech",
   "original": "i04_1817",
   "page_count": 4,
   "order": 594,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "This article describes a first version of a system for translating speech into Spanish Sign Language. The system proposed is made up of 4 modules: speech recognizer, semantic analysis, gesture sequence generation and gesture playing. For the speech recognizer and the semantic analysis, we use modules developed by IBM and the University of Colorado respectively. The gesture sequence generation uses the semantic concepts (obtained in the semantic analysis) associating them to several Spanish Sign Language gestures. This association is carried out based on a number of generating rules. For gesture animation, we have developed an animated character and a strategy for reducing the effort in gesture generation. This strategy consists of making the system generate automatically all agent positions necessary for the gesture animation. In this process, the system uses a few main agent positions (2-3 per second) and some interpolation strategies, both issues previously generated by the service developer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-593"
  },
  "kanedera04_interspeech": {
   "authors": [
    [
     "Noboru",
     "Kanedera"
    ],
    [
     "Sumida",
     "Asuka"
    ],
    [
     "Takao",
     "Ikehata"
    ],
    [
     "Tetsuo",
     "Funada"
    ]
   ],
   "title": "Subtopic segmentation in the lecture speech",
   "original": "i04_1821",
   "page_count": 4,
   "order": 595,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "This paper proposes a method of segmentation that segments lecture video material into subtopics based on speech signals for creation of educational video contents. To represent subtopics of video segments, the text recognized by automatic speech recognition (ASR) from a lecture speech was converted into an index using independent component analysis (ICA) instead of conventional TF-IDF. This research attempted a method of segmentation using dynamic programming that minimizes the sum of cosine measures between adjacent indexes. The validity of the proposed method was evaluated using sample lecture videos. Results indicated that subtopic segmentation using automatic speech recognition performed as well as that using transcription text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-594"
  },
  "erickson04_interspeech": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Caroline",
     "Menezes"
    ],
    [
     "Akinori",
     "Fujino"
    ]
   ],
   "title": "Some articulatory measurements of real sadness",
   "original": "i04_1825",
   "page_count": 4,
   "order": 596,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "This study examines some of the aritculatory differences in production of non-linguistic information, including spontaneous emotion vs. imitated emotion, para-linguistic information, in which the phrasing and intonational patterns were the same as in the first set of utterances with non-linguistic information, and linguistic information in which the same utterances were read. The findings suggest there are differences in articulation between imitated (acted) and spontaneous emotion. Specifically, for this particular emotional speech on the vowel /i/, tongue position was found to be higher and more forward, which suggests that emotional speech may be characterized by stronger lingual gestures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-595"
  },
  "lee04o_interspeech": {
   "authors": [
    [
     "Chen-Long",
     "Lee"
    ],
    [
     "Wen-Whei",
     "Chang"
    ],
    [
     "Yuan-Chuan",
     "Chiang"
    ]
   ],
   "title": "Application of voice conversion to hearing-impaired Mandarin speech enhancement",
   "original": "i04_1829",
   "page_count": 4,
   "order": 597,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "This paper studies the application of voice conversion to hearing-impaired Mandarin speech enhancement. The system is based on the combined use of a sinusoidal analysis-synthesis model and a priori knowledge about Mandarin syllable phonetic structures. We propose a time-scale modification algorithm that finds accurate alignments between hearing-impaired and normal utterances. Using the alignments, spectral conversion is performed by a continuous probabilistic transform based on a Gaussian mixture model. Simulation results indicate that the proposed system can improve the intelligibility of hearing-impaired Mandarin speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-596"
  },
  "kweon04_interspeech": {
   "authors": [
    [
     "Oh Pyo",
     "Kweon"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "A Japanese dialogue-based CALL system with mispronunciation and grammar error detection",
   "original": "i04_1833",
   "page_count": 4,
   "order": 598,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "This paper describes a dialogue-based CALL (Computer Assisted Language Learning) system. One of the major problems in CALL systems is that learners are usually assigned a passive role. Learners have no practices in composing their own utterances. The other major problem is that lots of conventional CALL systems are pronunciation exercise systems. However, pronunciation exercise is only a part of exercise needed to increase a learner's communication skill. In this paper, we propose a dialogue-based CALL system of new concept that enables exercise of composition, grammar and conversation in addition to pronunciation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-597"
  },
  "jo04b_interspeech": {
   "authors": [
    [
     "Cheolwoo",
     "Jo"
    ],
    [
     "Ilsuh",
     "Bak"
    ]
   ],
   "title": "Statistics-based direction finding for training vowels",
   "original": "i04_1837",
   "page_count": 3,
   "order": 599,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "In this paper, we tried to develop a vowel training assistant method using vowel formant statistics. Formant statistics were obtained from PBW set consists of 452 words from 8 persons. Then, we calculated distance from input formants to each center of vowel formant space. Based on the distance, directions to correct the speaker's manner of articulations, i.e. position of jaw and tongue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-598"
  },
  "montanari04_interspeech": {
   "authors": [
    [
     "Simona",
     "Montanari"
    ],
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Elaine",
     "Andersen"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Reference marking in children's computer-directed speech: an integrated analysis of discourse and gestures",
   "original": "i04_1841",
   "page_count": 4,
   "order": 600,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "Understanding the fine details of children's speech and their gestural characteristics helps, among other things, in creating natural computer interfaces. We analyze reference marking in young children's computer-directed speech using audio-video data from 3- to 6-year-old children engaged in a series of age-appropriate computer tasks, using a Wizard of Oz technique. Along with speech transcriptions and acoustic information, discourse (referential devices, conversational repairs) and gestural characteristics (hand/head movement type) were annotated in a synchronized multi-layer system. The results point to the developmental variability and the multimodal nature of the speech young children produce while interacting with the computer agent, suggesting that interfaces addressed to this age group should be specifically designed to integrate multisensory information as well as to adjust to the child's specific needs and interactional style.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-599"
  },
  "kim04n_interspeech": {
   "authors": [
    [
     "Jong-mi",
     "Kim"
    ],
    [
     "Suzanne",
     "Flynn"
    ]
   ],
   "title": "What makes a non-native accent?: a study of Korean English",
   "original": "i04_1845",
   "page_count": 4,
   "order": 601,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "We report a set of results that are a part of a much larger study of the second language (L2) acquisition of English phonology by first language (L1) speakers of Korean. Specifically, we focus on significant differences isolated between L2 speakers' production of isolated words in English and their production of these same words in sentence phrasal contexts. Results indicate significantly more acoustically accurate production of words in isolation than in the production of these same words in phrasal contexts. The particular phonological phenomena focused on concern both stress reduction and placement. We also consider several other aspects of segmental phonology. We argue that the discrepancy in results observed between tasks may account for many of the seemingly disparate results indicated in other studies of L2 phonology. We discuss several possible explanations for these data in terms of which production task most closely provides a measurement of developing linguistic competence and which might reflect the role of either general learning strategies (overgeneralization) or reversion back to the L1 grammar under conditions of stress or when the L2 grammar is not fully developed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-600"
  },
  "kim04o_interspeech": {
   "authors": [
    [
     "Sang-Jin",
     "Kim"
    ],
    [
     "Kwang-Ki",
     "Kim"
    ],
    [
     "Minsoo",
     "Hahn"
    ]
   ],
   "title": "Study on emotional speech features in Korean with its aplication to voice color conversion",
   "original": "i04_1849",
   "page_count": 4,
   "order": 602,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "Recent researches in speech synthesis are mainly focused on naturalness, and the emotional speech synthesis becomes one of the highlighted research topics. Although quite a many studies on emotional speech in English or Japanese have been addressed, the studies in Korean can seldom be found. This paper presents an analysis of emotional speech in Korean. Emotional speech features related to human speech prosody, such as F0, the duration, and the amplitude with their variations, are exploited. Their attribution to three different types of typical human speech is tried to be quantified and modeled. By utilizing the analysis results, emotional voice color conversion from the neutral speech to the emotional one is also performed and tested.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-601"
  },
  "amano04_interspeech": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "Developmental changes in voiced-segment ratio for Japanese infants and parents",
   "original": "i04_1853",
   "page_count": 4,
   "order": 603,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "Utterances of five Japanese infants and their parents were recorded longitudinally and used to develop an infant speech database. This database was used to analyze the voiced-segment ratio to investigate the developmental changes in utterances produced by infants and parents. The voiced-segment ratio is the ratio of the summed duration of a voiced segment to the total utterance duration. The analyses showed that the ratio tends to increase in an infant's utterances before the infant starts to produce 2-word sentences. The analyses also showed that, at the same stage, the ratio is higher in parents' infant-directed speech than in parents' adult-directed speech. These results suggest that the voiced-segment ratio reflects the development of an infant's utterance ability, and that a higher voiced-segment ratio is one of the characteristics of infant-directed speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-602"
  },
  "you04b_interspeech": {
   "authors": [
    [
     "Kisun",
     "You"
    ],
    [
     "Hoyoun",
     "Kim"
    ],
    [
     "Wonyong",
     "Sung"
    ]
   ],
   "title": "Implementation of an intonational quality assessment system for a handheld device",
   "original": "i04_1857",
   "page_count": 4,
   "order": 604,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "In this paper, we describe an implementation of an intonational quality assessment system for foreign language learning using a handheld portable device. The Viterbi algorithm is employed to conduct the forced alignments that indicate the boundary of each phonemes and a pitch detector is used to extract the intonational features. The tonal type of the segmented syllables is classified and the tendency of the pitch movement is measured. Then, the score of the spoken sentence is generated based on this information. We have implemented this system on an ARM7 RISC processor based system. For real time operation, we applied fixed-point arithmetic to the signal processing kernels and rearranged the algorithm flow of the system. As a result, the system runs in real time on a 60MHz CPU clock frequency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-603"
  },
  "beautemps04_interspeech": {
   "authors": [
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Thomas",
     "Burger"
    ],
    [
     "Laurent",
     "Girin"
    ]
   ],
   "title": "Characterizing and classifying cued speech vowels from labial parameters",
   "original": "i04_1861",
   "page_count": 4,
   "order": 605,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "As part of the THIMP project (Telephony for Hearing-IMpaired People), we aim at automatically analyzing Cued Speech [1] and translating it into oral spoken language. This work focuses on vowel classification and will be part of this transcoding process as a preprocessing step of the input data analysis. Its objective is to identify vowels produced by a speaker pronouncing and coding in Cued Speech a set of French sentences, knowing: - The Cued Speech Hand Placement, - The analysis of defined Labial Parameters. Here, we will show that the crossing of these two sources of information allows to automatically identify vowels. These results have to be compared to performances of hearing-impaired people in perception of Cued Speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-604"
  },
  "takahashi04_interspeech": {
   "authors": [
    [
     "Shin-ya",
     "Takahashi"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Sakashi",
     "Maeda"
    ],
    [
     "Naoyuki",
     "Tsuruta"
    ]
   ],
   "title": "Cough detection in spoken dialogue system for home health care",
   "original": "i04_1865",
   "page_count": 4,
   "order": 606,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "This paper reports a cough detection technique in a spoken dialogue system for health care task. In conventional speech recognition systems, their targets are verbal sounds only and non-verbal sounds like coughs are processed as burden noises which cause misrecognition. Coughing, however, is one of the most important barometers of daily health check, so the cough detection can be useful for checking a progress state and degree of a disease. It therefore is important to recognize not only user's utterances but also user's coughs in the spoken dialogue system for health care. In this paper, based on acoustic analysis results, a cough detection system with two different cough acoustic models constructed from band-pass filtered signals and unfiltered signals is proposed. The experimental results show that this method can improve an accuracy of the cough detection compared with the method which uses one acoustic model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-605"
  },
  "yu04f_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Peter",
     "Mau"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Unsupervised learning from users' error correction in speech dictation",
   "original": "i04_1969",
   "page_count": 4,
   "order": 607,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "We propose an approach to adapting automatic speech recognition systems used in dictation systems through unsupervised learning from users' error correction. Three steps are involved in the adaptation: 1) infer whether the user is correcting a speech recognition error or simply editing the text, 2) infer what the most possible cause of the error is, and 3) adapt the system accordingly. To adapt the system effectively, we introduce an enhanced two-pass pronunciation learning algorithm that utilizes the output from both an n-gram phoneme recognizer and a Letter-to-Sound component. Our experiments show that we can obtain greater than 10% relative word error rate reduction using the approaches we proposed. Learning new words gives the largest performance gain while adapting pronunciations and using a cache language model also produce a small gain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-606"
  },
  "meyer04_interspeech": {
   "authors": [
    [
     "Gerard G. L.",
     "Meyer"
    ],
    [
     "Teresa M.",
     "Kamm"
    ]
   ],
   "title": "Robustness aspects of active learning for acoustic modeling",
   "original": "i04_1973",
   "page_count": 4,
   "order": 608,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "We previously proposed [1] an iterative word-selective training method to cost-effectively utilize data preparation resources without compromising system performance. We continue this work and investigate the robustness of our active learning approach with respect to the starting conditions and further propose a stopping criterion that supports our objective to make effective use of transcription effort while minimizing system error. In particular, we demonstrate robustness to seven initial conditions, showing that we can select around 20 hours of training data and achieve a range of error rates between 8.6% and 9.0%, compared to an error rate of 10% when using all 50 hours of the training set. Additionally, we give empirical evidence that our proposed stopping criterion is in general a good predictor of when the minimum error rate is achieved, demonstrated for each of the initial conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-607"
  },
  "visweswariah04b_interspeech": {
   "authors": [
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Ramesh",
     "Gopinath"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Task adaptation of acoustic and language models based on large quantities of data",
   "original": "i04_1977",
   "page_count": 4,
   "order": 609,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "We investigate use of large amounts, over 1500 hours, of untranscribed data recorded from a deployed conversational system to improve the acoustic and language models. The system that we considered allows users to perform transactions on their retirement accounts. Using all the untranscribed data we get over 19% relative improvement in word error rate over a baseline system. In contrast, a system built using 70 hours of transcribed data results in over 31% relative improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-608"
  },
  "lussier04_interspeech": {
   "authors": [
    [
     "Luc",
     "Lussier"
    ],
    [
     "Edward W.D.",
     "Whittaker"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Unsupervised language model adaptation methods for spontaneous speech",
   "original": "i04_1981",
   "page_count": 4,
   "order": 610,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "We examine the performance of three different unsupervised language model adaptation schemes applied to speech recognition of spontaneous speech lecture presentations. Each adaptation scheme is based on a combination of word n-gram and class n-gram models and uses an initial transcription hypothesis to adapt the class model. The adapted class model is linearly interpolated with the baseline word n-gram model and the combination is then applied in a subsequent recognition step. One scheme also contains an element of domain adaptation in which the transcription hypothesis is also used to determine the interpolation weights of several class models each of which is built on automatically derived clusters of presentations. We also investigate multi-pass adaptation for each scheme and show this gives additional improvements in performance. Relative improvements in word error rate of up to 12.3% (2.9% absolute) are obtained on a held-out test set with the best adaptation scheme.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-609"
  },
  "nishida04_interspeech": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yoshitaka",
     "Mamiya"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "On-line incremental adaptation based on reinforcement learning for robust speech recognition",
   "original": "i04_1985",
   "page_count": 4,
   "order": 611,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "We propose an incremental unsupervised adaptation method based on reinforcement learning in order to achieve robust speech recognition in various noisy environments. Reinforcement learning is a training method based on rewards that represents correctness of outputs instead of supervised data. Training progresses gradually based on rewards given. Our method is able to perform environmental adaptation without priori knowledge about such things as speakers and noises in noisy environments. We conducted speech recognition experiments using a connected digit recognition database. We demonstrate that our method has higher recognition performance than the conventional adaptation method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-610"
  },
  "watanabe04b_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Watanabe"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Unsupervised speaker adaptation using high confidence portion recognition results by multiple recognition systems",
   "original": "i04_1989",
   "page_count": 4,
   "order": 612,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "This paper describes an accurate unsupervised speaker adaptation method for lecture speech recognition using multiple LVCSRs. In an unsupervised speaker adaptation framework, the improvement of recognition performance by adapting acoustic models greatly depends on the accuracy of labels such as phonemes and syllables. Therefore, extraction of the adaptation data guided by the confidence measures is effective for unsupervised adaptation. In this paper, we looked for the high confidence portions based on the agreement between two LVCSRs, adapted acoustic models using the portions attached with high accurate labels, and then improved the recognition accuracy. We applied our method to the Corpus of Spontaneous Japanese (CSJ) and improved the recognition rate by about 5% in comparison with a traditional method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-611"
  },
  "dusan04b_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ],
    [
     "James",
     "Flanagan"
    ],
    [
     "Amod",
     "Karve"
    ],
    [
     "Mridul",
     "Balaraman"
    ]
   ],
   "title": "Speech coding using trajectory compression and multiple sensors",
   "original": "i04_1993",
   "page_count": 4,
   "order": 613,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "This paper presents a new method of multi-frame speech coding based upon polynomial approximation of speech feature trajectories incorporating multiple sensor signals from microphones, accelerometer, electro-glottograph, and micro-radar. The trajectory polynomial approximation exploits the inter-frame information redundancy encountered in natural speech. The trajectory method is applicable to features such as spectral parameters, gain, and pitch. The method is suitable for application to a frame vocoder to further reduce the transmission bit rate. Multiple transducers increase the intelligibility and quality of the coded speech in noisy environments. Experimental results are obtained by embedding the new method into an enhanced mixed-excitation linear prediction vocoder. The resulting vocoder operates at 1533 bps and preliminary intelligibility and quality tests show results comparable to those of the original 2400 bps vocoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-612"
  },
  "feldbauer04_interspeech": {
   "authors": [
    [
     "Christian",
     "Feldbauer"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "How sparse can we make the auditory representation of speech?",
   "original": "i04_1997",
   "page_count": 4,
   "order": 614,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "In this work we deal with a speech coder which is based on a model of the human peripheral auditory system and uses a neural auditory representation as its code. This representation consists of multi-channel sparse pulse trains but is still highly over-complete and therefore not efficient in terms of its data compression capability. The emphasis of this paper is on answering the question 'How sparse can we make the auditory representation?', i.e., on finding a bound for the number of pulses which can be omitted without degrading the quality of the reconstructed speech signal. For this purpose we incorporate a second auditory model which allows to decide whether a pulse is needed or not based on the excitation pattern caused by the signal when a single pulse is resynthesized. This model accounts for both simultaneous and temporal masking. We also propose a method for compensating for the loss of energy due to the elimination of pulses which makes a possible spectral distortion inaudible. Results show that about 74% of the pulses can be omitted while maintaining the original speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-613"
  },
  "david04_interspeech": {
   "authors": [
    [
     "Malah",
     "David"
    ],
    [
     "Slava",
     "Shectman"
    ]
   ],
   "title": "Efficient sub-optimal temporal decomposition with dynamic weighting of speech signals for coding applications",
   "original": "i04_2001",
   "page_count": 4,
   "order": 615,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "The Optimized Temporal Decomposition (OTD) technique for Line Spectral Frequencies (LSF) speech envelope representation, under a MMSE criterion, has been shown to be promising for very low bit rate speech coding for storage and broadcast applications. In order to improve perceptual speech quality, a dynamically weighted OTD (DW-OTD) technique is introduced in this work. It extends the OTD by allowing temporally changing weights, so as to improve the perceived speech quality. Use of Gardner's weighted MSE with DWOTD is found to reduce the Log Spectral Distance (LSD) measure by 0.3 dB, as compared to OTD. The original OTD algorithm delay and complexity requirements make it inappropriate for real-time speech coding. In this paper we also introduce a modification of this technique, which is sub-optimal but suitable for on-line speech coding purposes, with negligible degradation of performance (of only about 0.06 dB in LSD). With the proposed techniques we were able to encode speech spectral envelopes at 300-370 bps at LSD of 2.25- 2.1 dB, respectively, with a delay of just 7 frames.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-614"
  },
  "gunawan04_interspeech": {
   "authors": [
    [
     "Teddy Surya",
     "Gunawan"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "Perceptual wavelet packet audio coder",
   "original": "i04_2005",
   "page_count": 4,
   "order": 616,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "Traditional wavelet packet audio compression algorithms do not utilize the temporal masking properties of the human auditory system, relying instead on simultaneous masking models. This paper presents the design and implementation of a perceptual wavelet audio coder by incorporating temporal and simultaneous masking models. The efficiency of the encoder was assessed based upon the number of bits required to code wavelet packet coefficients in each critical band, while retaining perceptual transparency. Subjective listening tests conforming to ITU-R BS.1116 revealed the bit rate is reduced by more than 17% compared to using a coder that only employs a simultaneous masking model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-615"
  },
  "jung04c_interspeech": {
   "authors": [
    [
     "Sung-Kyo",
     "Jung"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Dae-Hee",
     "Youn"
    ],
    [
     "Chang-Heon",
     "Lee"
    ]
   ],
   "title": "Performance analysis of transcoding algorithms in packet-loss environments",
   "original": "i04_2009",
   "page_count": 4,
   "order": 617,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "This paper describes a robustness issue of the transcoder under packet loss channel environments. We briefly introduce the conventional transcoder between AMR and G.729A speech coders, and analyze the performance of transcoder under frame erasure environments. In a tandem method even a single packet loss significantly affects to the parametric buffers of the successive frames because it should reanalyze the distorted signals. However, the propagation effect of the transcoding method is less severe because it uses a direct bitstream mapping method. By comparing the time delay of error propagation, spectral distortion of spectral information, magnitude spectral distance of excitation information and perceptual evaluation of speech quality (PESQ), we show that the transcoding algorithm is also a promising technology under packet loss channel environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-616"
  },
  "falk04_interspeech": {
   "authors": [
    [
     "Tiago",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Speech quality estimation using Gaussian mixture models",
   "original": "i04_2013",
   "page_count": 4,
   "order": 618,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "We propose a novel method to estimate the quality of coded speech signals. The joint probability distribution of the subjective mean opinion score (MOS) and perceptual distortion feature variables is modelled using a Gaussian mixture density. The feature variables are sifted from a large pool of candidate features using statistical data mining techniques. We study what combinations of features and mixture model configuration are most effective. For our speech database, a five-feature, three-component GMM furnishes approximately 18% lower root-mean-squared MOS estimation error than ITU-T P.862 PESQ, the current best standard algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-617"
  },
  "kim04p_interspeech": {
   "authors": [
    [
     "Hong Kook",
     "Kim"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Why speech recognizers make errors ? a robustness view",
   "original": "i04_1645",
   "page_count": 4,
   "order": 619,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "The performance of large vocabulary speech recognizers often varies depending on the input speech and the quality of the trained models. The particular attributes that cause recognition errors are a research area that has not been well studied. This paper addresses this issue from a robustness perspective using a large amount of field data collected from natural language dialog services. In particular, we present a method for tracking time-varying or non-stationary extraneous events, such as music, background noise, etc. We show that this measure is a better predictor of recognition errors than a standard measure of stationary signal-to-noise ratio (SNR). Combining the two measures provides a data selection algorithm for detecting problematic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-618"
  },
  "ahadi04_interspeech": {
   "authors": [
    [
     "Mohammad",
     "Ahadi"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Robert",
     "Brennan"
    ],
    [
     "George",
     "Freeman"
    ]
   ],
   "title": "An energy normalization scheme for improved robustness in speech recognition",
   "original": "i04_1649",
   "page_count": 4,
   "order": 620,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "The log energy parameter has long been used as an extension to the basic cepstral feature vector in speech recognition. The use of a normalization technique for the log energy parameter has also been widely accepted. In this paper, a simple energy normalization scheme is introduced that allows direct use of the frame energy parameter in speech recognition and performs well in the presence of noise. Its combination with traditional cepstral mean and variance normalizations has led to error rate improvements of up to 55% on the Aurora 2 task, in comparison to the baseline clean-trained system using feature set including the log energy parameter. This achievement has been obtained with neither complicated programming nor computation expensive routines. The performance of this scheme on an utterance-wide basis has been close to that of the off-line speaker-wide normalization, which makes it a good candidate for practical systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-619"
  },
  "huerta04_interspeech": {
   "authors": [
    [
     "Juan",
     "Huerta"
    ],
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Sreeram",
     "Balakrishnan"
    ]
   ],
   "title": "Rapid on-line environment compensation for server - based speech recognition in noisy mobile environments",
   "original": "i04_1653",
   "page_count": 4,
   "order": 621,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "We present a rapid compensation technique aimed at reducing the detrimental effect of environmental noise and channel on server based mobile speech recognition. It solves two key problems for such systems: firstly how to accurately separate non-speech events (or background noise) from noise introduced by network artifacts; secondly how to reduce the latency created by the extra computation required for a codebook-based linear channel compensation technique. We address the first problem by modifying an existing energy based endpoint-detection algorithm to provide segment-type information to the compensation module. We tackle the latency issue with a codebook based scheme by employing a tree structured vector quantization technique with dynamic thresholds to avoid the computation of all codewords. Our technique is evaluated using a speech-in-car database at 3 different speeds. Our results show that our method leads to a 8.7% reduction in error rate and 35% reduction in computational cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-620"
  },
  "ansary04_interspeech": {
   "authors": [
    [
     "Leila",
     "Ansary"
    ],
    [
     "Seyyed Ali Seyyed",
     "Salehi"
    ]
   ],
   "title": "Modeling phones coarticulation effects in a neural network based speech recognition system",
   "original": "i04_1657",
   "page_count": 4,
   "order": 622,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "In this paper we have designed and implemented speech recognition models in phone recognition level to model phones coarticulation effects. We have inspired these models from two human cognitive systems: neocortex and hippocampus. In the model inspired from the neocortex the first step is a primary and coarse classification of inputs, then model adapts itself to contexts extracted from these primary recognitions and we classify inputs again according to their extracted context. In the model inspired form the hippocampus, previous contexts of inputs are used for better recognition, and in this way we use effects of previous phones of each input for better classification. Then we have designed and implemented a model with a structure of combination of two preceding models. Our models implementation showed 3.77% increase in accuracy of Persian phone recognition compared to a simple model that does not consider coarticulation effects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-621"
  },
  "willett04_interspeech": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ]
   ],
   "title": "Error - weighted discriminative training for HMM parameter estimation",
   "original": "i04_1661",
   "page_count": 4,
   "order": 623,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "Optimizing discriminative objectives in HMM parameter training proved to outperform Maximum Likelihood-based parameter estimation in numerous studies. This paper extends the Maximum Mutual Information objective by applying utterance specific weighting factors that are adjusted for minimum sentence error. In addition to that, the paper investigates tuning separate numerator and denominator weighting factors in a way that favors Maximum Likelihood parameter estimates for reasons of stability and generalization on unseen data. The experimental evaluations carried out on German digit string data show that the Error-Weighted Maximum Mutual Information approach has the potential of outperforming ordinary discriminative parameter estimates. In our experiments, we see a substantially larger word error rate reduction compared to conventional MMI training. Following the ML-preferred error-weighted discriminative training approach, we see another small improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-622"
  },
  "lo04_interspeech": {
   "authors": [
    [
     "Wai Kit",
     "Lo"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Robust verification of recognized words in noise",
   "original": "i04_1665",
   "page_count": 4,
   "order": 624,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "In this paper we investigate robust word verification issues in noise using the generalized word posterior probability (GWPP). In computing GWPP, reduced search space, relaxed time registrations of hypothesized words in the word graph, and optimal acoustic and language model weights are employed. The sensitivity of word verification errors with respect to the parameters of GWPP was tested under different SNR conditions. We found that around the optimal parameter settings, there exists a relatively stable region where the total number of word verification errors is fairly insensitive (robust) to the exact choice of the optimal values. Cross-SNR condition tests using a large vocabulary speaker independent, continuous Japanese speech database (Basic Travel Expression Corpus) confirms the robustness of the GWPP based word verification in different SNR's.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-623"
  },
  "li04f_interspeech": {
   "authors": [
    [
     "Zili",
     "Li"
    ],
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust automatic speech recognition using an optimal spectral amplitude estimator algorithm in low-SNR car environments",
   "original": "i04_2041",
   "page_count": 4,
   "order": 625,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "This paper addresses the problem of noise robustness of automatic speech recognition (ASR) systems in noisy car environments using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator (MMSE-STSA). This was accomplished by the integration of an adaptive time varying Noise Shaping Filter (NSF) with the MMSE-STSA algorithm in order to improve the speech enhancement performance by \"whitening\" the noisy speech signals. Experiments were conducted using a noisy version of speech signals extracted from the TIMIT database. The proposed NSF-based STSA algorithm is used as a processor of an ASR system in order to evaluate its robustness in severe interfering car noise environments. The HTK Hidden Markov Model Toolkit was used throughout our experiments. Results show that the proposed approach, when included in the frontend of an HTK-based ASR system, outperforms that of the conventional recognition process in severe interfering car noise environments for a wide range of SNRs down to -12 dB using a noisy version of the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-624"
  },
  "zhao04_interspeech": {
   "authors": [
    [
     "Junhui",
     "Zhao"
    ],
    [
     "Jingming",
     "Kuang"
    ],
    [
     "Xiang",
     "Xie"
    ]
   ],
   "title": "Robust speech recognition using data-driven temporal filters based on independent component analysis",
   "original": "i04_2045",
   "page_count": 4,
   "order": 626,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "In this paper, a data-driven temporal processing method based on Independent Component Analysis (ICA) is proposed for obtaining a more robust speech representation. Two different schemes of dominant temporal filters based on ICA are investigated. The one is the perceptually-based filter which always focuses on the modulation frequency range between 1 and 16 Hz and the other is the most independent component discovered by ICA algorithm. Detailed comparative analysis between the proposed ICA-derived temporal filters and the previous statistical methods including Linear Discriminant Analysis (LDA) and Principle Component Analysis (PCA) is presented. The preliminary experiments show that the performance of the ICA based temporal filtering is much better in comparison with the LDA and PCA based methods in noisy environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-625"
  },
  "kitaoka04_interspeech": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Robust distant speech recognition based on position dependent CMN",
   "original": "i04_2049",
   "page_count": 4,
   "order": 627,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "In a distant environment, channel distortion may dramatically degrade speech recognition performance. In this paper, we propose a robust speech recognition method based on position dependent Cepstral Mean Normalization (CMN). At first the system measures the transmission characteristics according to the speaker positions from some grid points in the room a priori. In the recognition stage, the system estimates the speaker position in a 3-D space based on the time delay of arrival (TDOA) between distinct microphone pairs. And then the system selects the transmission characteristics estimated a priori corresponding to the estimated position and applies a channel distortion compensation method to the speech and recognizes it. In our proposed method, we also compensate the mismatch between the cepstral means of utterances spoken by human and those emitted from loudspeaker. Our experiments showed that the proposed method improved the performance of speech recognition system in a distant environment efficiently and it could also compensate the mismatch between voices from human and loudspeaker well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-626"
  },
  "sakauchi04_interspeech": {
   "authors": [
    [
     "Sumitaka",
     "Sakauchi"
    ],
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ]
   ],
   "title": "Robust speech recognition based on HMM composition and modified wiener filter",
   "original": "i04_2053",
   "page_count": 4,
   "order": 628,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "This paper combines the HMM composition method with a highly efficient noise reduction method to create a robust speech recognition technique for additive noise environments. Speech recorded by hands-free microphones in the real world suffer from 1) low Speech/Noise [S/N] and 2) changes in S/N. In particular, S/N varies with the speaker and from utterance to utterance even in a same noise environment. To deal with the low S/N, the proposed technique uses the modified Wiener filter (WF) method for noise reduction and so keeps S/N higher than is possible with spectral subtraction (SS), as well as minimizing speech distortion. To compensate the remaining additive noise, the proposed technique uses the HMM composition method with clean speech models and a noise model trained by the remaining noise. To offset the rapid changes in S/N where S/N may not be known, HMMs composed under various S/N conditions are run in parallel to obtain better recognition results; rapid response is achieved since adaptation to handle speech distortion is not necessary. The new technique shows a reduction in average recognition error of 21.6% under various noise conditions compared to using the basic HMM composition method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-627"
  },
  "brito04_interspeech": {
   "authors": [
    [
     "Ivan",
     "Brito"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Carlos",
     "Molina"
    ]
   ],
   "title": "Feature-dependent compensation in speech recognition",
   "original": "i04_2057",
   "page_count": 4,
   "order": 629,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "Several mismatch conditions can be modeled as an additive bias. This bias is considered independent of the observation vectors, although this approximation is not always accurate. In this paper the dependence of the bias on the observation vectors is taken into consideration in the context of compensating the GSM coding distortion in speech recognition. However, the results presented here can easily be generalized to deal with other types of mismatch. The coding-decoding distortion is modeled here as feature-dependent. This model is employed to propose an Expectation-Maximization (EM) estimation algorithm of the coding-decoding distortion that is able to cancel the effect of GSM coder with as few as one adapting utterance. Finally, the feature-dependent adaptation can give word error rate (WER) 26% lower than the feature-independent model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-628"
  },
  "cox04_interspeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Using context to correct phone recognition errors",
   "original": "i04_2061",
   "page_count": 4,
   "order": 630,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "There are many circumstances in which it is useful or necessary to recognise phones rather than words, but phone recognition is inherently less accurate than word recognition. We describe here a post-recognition method for \"translating\" an errorful phone string output by a speech recogniser into a string that more closely matches the transcription. The technique owes something to Kohonen's idea of \"dynamically expanding context\" in that it learns from the errors made by the recogniser in a particular context, but it uses many contexts rather than a single context to estimate the \"translation\" of a recognised phone. The weights given to the different contexts in estimating the translation are determined discriminatively. On the WSJCAM0 database, the technique gives a 19.2% relative improvement in phone errors (including insertions) over the baseline, compared with a 6.2% improvement obtained using dynamically expanding context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-629"
  },
  "obuchi04_interspeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ]
   ],
   "title": "Improved histogram-based feature compensation for robust speech recognition and unsupervised speaker adaptation",
   "original": "i04_2065",
   "page_count": 4,
   "order": 631,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "Feature compensation for noise robust speech recognition becomes more effective if normalization of time-derivative parameters is taken into account. This paper describes an implementation of Delta- Cepstrum Normalization (DCN) that runs with only minimum response time. The proposed algorithm, referred to as Recursive DCN, provides word error rate improvements comparable to conventional DCN. Since DCN includes the procedure that adjusts the mismatch between the cepstrum part and the delta-cepstrum part, it works effectively even if only small amount of data can be used. We also investigate the possibility of applying DCN to unsupervised speaker adaptation. It is shown that DCN adaptation improves the recognition accuracy even without reference transcription of the adaptation data. Finally, DCN adaptation is combined with Feature-space Maximum Likelihood Linear Regression (FMLLR). It shows promising results in the batch mode experiments, although the improvement is rather small in the recursive mode.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-630"
  },
  "xiong04b_interspeech": {
   "authors": [
    [
     "Zhenyu",
     "Xiong"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Weighting observation vectors for robust speech recognition in noisy environments",
   "original": "i04_2069",
   "page_count": 4,
   "order": 632,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "In this paper, we propose a novel approach to robust speech recognition in noisy environments by discriminating the observation vectors. In conventional HMM-based speech recognition, all the observation vectors are treated with equal importance no matter how the corresponding speech segment is corrupted with noise. Our approach proposed here modifies the conventional decoder by weighting the likelihood scores for different observation vectors based on the signal to noise ratios (SNRs) of the corresponding speech frames when the probabilities of generating a sequence of observations are being calculated for some models. The proposed approach combined with spectral subtraction is evaluated with four different kinds of noises added to the clean speech. The experimental results show the superior performance of the proposed method over the method where only the spectral subtraction is applied, especially in the median SNR environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-631"
  },
  "tsujikawa04_interspeech": {
   "authors": [
    [
     "Masanori",
     "Tsujikawa"
    ],
    [
     "Ken-ichi",
     "Iso"
    ]
   ],
   "title": "Hands-free speech recognition using blind source separation post-processed by two-stage spectral subtraction",
   "original": "i04_2073",
   "page_count": 4,
   "order": 633,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "This paper proposes hands-free speech recognition using blind source separation (BSS) post-processed by two-stage spectral subtraction (2S-SS). The BSS using independent component analysis (ICA) estimates a target signal and jammer signals. The 2S-SS removes its residual crosstalk components and suppresses spatially-distributed noise not separated by BSS. In large vocabulary continuous speech recognition (LVCSR) evaluation, utterances by other speakers and computer-room noise were used as a jammer signal and a spatially distributed noise source, respectively. In all noisy environments, it was confirmed that the proposed method outperformed the BSS with single-channel spectral subtraction (1SS).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-632"
  },
  "gomez04_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Robust speech recognition with spectral subtraction in low SNR",
   "original": "i04_2077",
   "page_count": 4,
   "order": 634,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "Robust speech recognition in noisy environments is a very difficult task. It is desirable to search for parameters that would relate the speech enhancement technique directly with the recognizer. In this paper, Noise Reduction Rate (NRR) and Mel Cepstrum Distortion (MelCD) are investigated when using Spectral Subtraction (SS). Under low SNR such as 0dB, 5dB, 10dB, maximizing NRR nor minimizing the MelCD does not result in a better recognition performance. Thus, the conventional SS in which the the over-subtraction parameter (alpha) is a function of SNR renders to be ineffective in the point-of-view of the recognizer. Our proposed method derives alpha for SS directly from the training utterances used in creating the Hidden Markov Models (HMM) that optimizes the recognition performance. By superimposing office noise to the SS-denoised noisy speech, we achieved 26.0% and 7.6% of relative increase in word accuracy for the proposed matched and generalized alpha respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-633"
  },
  "cranen04_interspeech": {
   "authors": [
    [
     "Bert",
     "Cranen"
    ],
    [
     "Johan de",
     "Veth"
    ]
   ],
   "title": "Active perception: using a priori knowledge from clean speech models to ignore non-target features",
   "original": "i04_2081",
   "page_count": 4,
   "order": 635,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "Making ASR noise robust requires a form of data normalisation to ensure that the distributions of acoustic features in the training and test condition look similar. Usually, it is attempted to compensate for the impact of noise by estimating the noise characteristics from the signal. In this paper we explore a new method that builds on a-priori knowledge stored in clean speech models. Using Mel bank log-energy features, classical clean speech HMMs were replaced by models in which the model components corresponding to low energy are not considered during recognition. Application of the new method to clean matched data showed that recognition performance was equal or better compared to baseline when less than 45 percent of the model components were discarded. In the case of noisy data, the performance gains were marginal for the model component selections studied so far. Analysis of the results suggests that future research should focus on combining the new model-driven approach with data-driven methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-634"
  },
  "xu04d_interspeech": {
   "authors": [
    [
     "Haitian",
     "Xu"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "Spectral subtraction with full-wave rectification and likelihood controlled instantaneous noise estimation for robust speech recognition",
   "original": "i04_2085",
   "page_count": 4,
   "order": 636,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "In standard Spectral Subtraction (SS), Half-Wave Rectification SS (HWR-SS) is normally applied to avoid negative values in the Power Spectral Density (PSD) that occur mainly due to inaccurate noise estimation caused by a Voice Activity Detector (VAD). In this paper analyses show that, given accurate noise estimation, the phase relationship between speech and noise becomes the dominant cause of the negative values. Full-Wave Rectification based SS (FWR-SS) combined with Instantaneous Noise Estimation (INE) is therefore proposed to be applied instead of VAD based HWR-SS as it is better capable of maintaining the speech information in those negative values. It is also shown in the paper that FWR-SS provides optimum orthogonality between the estimated noise and speech signals. The INE method proposed in this paper is Likelihood Controlled Instantaneous Noise Estimation (LCINE), which combines long-term statistical characteristics of noise resulting from a VAD with a method of short-term INE. The combination of FWR-SS and LCINE is computationally efficient and shows a 51% error rate reduction on the Aurora 2 database in comparison to the basic Aurora front-end provided by ETSI [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-635"
  },
  "korkmazsky04_interspeech": {
   "authors": [
    [
     "Filipp",
     "Korkmazsky"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Using linear interpolation to improve histogram equalization for speech recognition",
   "original": "i04_2089",
   "page_count": 4,
   "order": 637,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "This paper presents a novel approach to speech data normalization by introducing interpolation for histogram equalization. We study different ways of histogram interpolation that inhence this normalization technique. We found that using a special weighting factor to combine current and past test sentence statistics improved speech recognition performance. For the testing that used weighted histogram interpolation we achieved 44.85% phone error rate against 49.42% phone error rate for the testing without normalization and 48.59% phone error rate, when only a single test sentence histogram was used for normalization. Recognition experiments were conducted on speech data recorded in a moving car and proved advantage of using interpolation for data normalization by histogram equalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-636"
  },
  "hasegawajohnson04c_interspeech": {
   "authors": [
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Ameya",
     "Deoras"
    ]
   ],
   "title": "A factorial HMM aproach to robust isolated digit recognition in background music",
   "original": "i04_2093",
   "page_count": 4,
   "order": 638,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "This paper presents a novel solution to the problem of isolated digit recognition in background music. A Factorial Hidden Markov Model (FHMM) architecture is proposed to accurately model the simultaneous occurrence of two independent processes, such as an utterance of a digit and an excerpt of music. The FHMM is implemented with its equivalent HMM by extending Nadas' MIXMAX algorithm to a mixture of Gaussians PDF. At around 0 dB SNR, the proposed system shows an average relative reduction in word error rate of 57% in the recognition of isolated digits in background music.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-637"
  },
  "lee04p_interspeech": {
   "authors": [
    [
     "Yoonjae",
     "Lee"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Multi-eigenspace normalization for robust speech recognition in noisy environments",
   "original": "i04_2097",
   "page_count": 4,
   "order": 639,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "In this paper, we propose an effective feature normalization scheme based on eigenspace normalization, for achieving robust speech recognition. In general, Mean and Variance Normalization (MVN) is implemented in cepstral domain. However, another MVN approach using eigenspace was recently introduced, in that the eigenspace normalization procedure performs normalization in a single eigenspace. This procedure consists of linear PCA matrix feature transformation followed by mean and variance normalization of the transformed cepstral feature. In the proposed scheme, we apply independent and unique eigenspaces to cepstra, delta and delta-delta cepstra respectively. We also normalize training data in eigenspace. In addition, a feature space rotation procedure is introduced to reduce the mismatch of training and test data distribution in noisy condition. As a result, we obtained a substantial improvement over the basic eigenspace normalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-638"
  },
  "cerisara04_interspeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Exploiting models intrinsic robustness for noisy speech recognition",
   "original": "i04_2101",
   "page_count": 4,
   "order": 640,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "We propose in this paper an original approach to build masks in the framework of missing data recognition. The proposed soft masks are estimated from the models themselves, and not from the test signal as it is usually the case. They represent the intrinsic robustness of model's log-spectral coefficients. The method is validated with cepstral models, on two synthetic and two real-life noises, at different signal-to-noise ratios. We further discuss how such masks can be combined with other signal-based masks and noise compensation techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-639"
  },
  "pujol04_interspeech": {
   "authors": [
    [
     "Pere",
     "Pujol"
    ],
    [
     "Jaume",
     "Padrell"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Dusan",
     "Macho"
    ]
   ],
   "title": "Speech recognition experiments with the SPEECON database using several robust front-ends",
   "original": "i04_2105",
   "page_count": 4,
   "order": 641,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "In this paper we deal with the robustness problem in speech recognition, using a Spanish subset of the recently collected SPEECON database, and focusing on the front-end side of the recognizer. Cross-microphone and cross-environment recognition tests are presented using both read and spontaneous continuous speech utterances. Our semi-continuous sub-word HMM back-end was fixed for all the tests. For comparison, we used both the clean-speech and the noisy-speech cepstrum-based ETSI standard front-ends, as well as a few relatively simple variants of the front-end that is based on frequency-filtering (FF) features. In all our tests, the best word error rates scores were obtained with the FF front-end. Moreover, a technique based on a long-term log spectral mean subtraction was successfully used to reduce the reverberation affecting the utterances from the furthest microphones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-640"
  },
  "ikbal04_interspeech": {
   "authors": [
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ],
    [
     "Hemant",
     "Misra"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Spectro-temporal activity pattern (STAP) features for noise robust ASR",
   "original": "i04_2109",
   "page_count": 4,
   "order": 642,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "In this paper, we introduce a new noise robust representation of speech signal obtained by locating points of potential importance in the spectrogram, and parameterizing the activity of time-frequency pattern around those points. These features are referred to as Spectro- Temporal Activity Pattern (STAP) features. The suitability of these features for noise robust speech recognition is examined for a particular parameterization scheme where spectral peaks are chosen as points of potential importance. The activity in the time-frequency patterns around these points are parameterized by measuring the dynamics of the patterns along both time and frequency axes. As the spectral peaks are considered to constitute an important and robust cue for speech recognition, this representation is expected to yield a robust performance. An interesting result of the study is that inspite of using a relatively less amount of information from the speech signal, STAP features are able to achieve a reasonable recognition performance in clean speech, when compared to the state-of-the-art features. In addition, STAP features produce a significantly better performance in high noise conditions. An entropy based combination technique in tandem frame-work to combine STAP features with standard features yields a system which is more robust in all conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-641"
  },
  "kim04q_interspeech": {
   "authors": [
    [
     "Byoung-Don",
     "Kim"
    ],
    [
     "Jin-Young",
     "Kim"
    ],
    [
     "Seung-Ho",
     "Choi"
    ],
    [
     "Young-Bum",
     "Lee"
    ],
    [
     "Kyoung-Rok",
     "Lee"
    ]
   ],
   "title": "Improvement of confidence measure performance using background model set algorithm",
   "original": "i04_2113",
   "page_count": 4,
   "order": 643,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "This study suggests the BMS (Background Model Set) algorithm used in the speaker verification to supplement the shortcoming in the process of calculation of RLJ-CM (RLJ-Confidence Measure) and normalized CM. The confidence measure shows the relative similarity between the recognized model and the unrecognized one. In calculation of the CM, the composition of anti-phone model does not have the high confidence measure because a probability and a standard deviation are calculated by using all the phonemes. Also, there is shortcoming that the recognition time increases at the calculation using all phonemes. To solve such a problem, the method is researched which re-organizes a probability and a standard deviation by using the BMS algorithm. As a result, when the BMS algorithm was applied near 17% of the MDR(Missed Detection Rate), a performance was increased to 0.104FA/KW/HR(false alarm/keyword/hour), by 50% improvement compared to the model not applied with the BMS. While the existing work recognition took on average 15 minutes to deal with the evaluation database of one person, the recognition execution through the BMS base reduced the recognition time to 10 minutes by 33%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-642"
  },
  "aradilla04_interspeech": {
   "authors": [
    [
     "Guillermo",
     "Aradilla"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Sunil",
     "Sivadas"
    ]
   ],
   "title": "Using RASTA in task independent TANDEM feature extraction",
   "original": "i04_2117",
   "page_count": 4,
   "order": 644,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "In this work, we investigate the use of RASTA filter in the TANDEM feature extraction method when trained with a task independent data. RASTA filter removes the linear distortion introduced by the communication channel which is demonstrated in a 18% relative improvement on the Numbers 95 task. Also, studies yielded a relative improvement of 35% over the basic PLP features by combining TANDEM features and conventional PLP features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-643"
  },
  "han04c_interspeech": {
   "authors": [
    [
     "Kyu Jeong",
     "Han"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Naveen",
     "Srinivasamurthy"
    ]
   ],
   "title": "A distributed speech recognition system in multi-user environments",
   "original": "i04_2121",
   "page_count": 4,
   "order": 645,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "A typical distributed speech recognition (DSR) system is a configuration that distributes computational burden in signal processing and pattern recognition between a mobile unit and a remote recognition engine. For this system to be robust and practically acceptable, distortions caused by erroneous data transmission should be minimized. In this paper, the effects of multiple simultaneous users in a wireless network on speech recognition are considered. Specifically, multiple access interference (MAI) is shown to be a significant factor in the recognition performance degradation of a DSR system. From simulation results, both a minimum-mean-square-error (MMSE) detector and a decorrelating filter are shown to be effective in reducing MAI and improving recognition accuracy. In a CDMA system with 6 interferers at an SNR of 7 dB in an AWGN channel, there was a 30% absolute reduction in word-error-rate (WER) for a connected digit recognition task by using an MMSE detector to combat MAI.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-644"
  },
  "haebumbach04_interspeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Valentin",
     "Ion"
    ]
   ],
   "title": "Soft features for improved distributed speech recognition over wireless networks",
   "original": "i04_2125",
   "page_count": 4,
   "order": 646,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "A major drawback of distributed versus terminal-based speech recognition is the fact that transmission errors can lead to degraded recognition performance. In this paper we employ \"soft features\" to mitigate the effect of bit errors on wireless transmission links: At the receiver a posteriori probabilities of the transmitted feature vectors are computed by combining bit reliability information provided by the channel decoder and a priori knowledge about residual redundancy in the feature vectors. While the first-order moment of the a posteriori probability function is the MMSE estimate, the second-order moment is a measure of the uncertainty in the reconstructed features. We conducted realistic simulations of GSM transmission and achieved significant improvements in word accuracy compared to the error mitigation strategy described in the ETSI standard.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-645"
  },
  "ebukuro04_interspeech": {
   "authors": [
    [
     "Rinzou",
     "Ebukuro"
    ]
   ],
   "title": "Analysis on disappearing and thriving of speech applications for ergonomic design guidelines and recommendations",
   "original": "i04_2217",
   "page_count": 4,
   "order": 647,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "Design guidelines and recommendations for Voice Input System (VIS) introductions composed with a concept of SAU (Suitable, Adoptable/ Adaptable/Adjustable; 3-As, and Usable) on a concept of the Customer Satisfaction (CS) are introduced in this paper. A concept of total CS on a Mutual Balancing Policy (MBP) especially required for planning of VIP system introduction is introduced in examining state of affairs of some field applications of speech products, disappearing and thriving, for industrial use. Significance of a sense of Mutual Balancing among the supplier and recipients, managements and speech input end users, and further the VIP system owners and his clients, on the concept of MBP on total CS is emphasized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-646"
  },
  "smeele04_interspeech": {
   "authors": [
    [
     "Paula M. T.",
     "Smeele"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Jan Felix",
     "Krebber"
    ]
   ],
   "title": "Evaluation of the speech output of a smart-home system in a car environment",
   "original": "i04_2221",
   "page_count": 4,
   "order": 648,
   "p1": "2221",
   "pn": "2225",
   "abstract": [
    "This paper reports on the evaluation of the speech output component of a dialogue system that enables the control of different home appliances via speech. It was simulated that participants called the system from the car and listened to feedback messages from the system. Their task was to judge the quality of the system's output while performing a driving task in the TNO driving simulator. The following effects on the quality judgements were studied: type of system prompt (natural, synthesized), transmission degradations (circuit noise, speech codec, packet loss), and the driving task. The results are presented in terms of these aspects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-647"
  },
  "haas04_interspeech": {
   "authors": [
    [
     "Ellen",
     "Haas"
    ]
   ],
   "title": "How does the integration of speech recognition controls and spatialized auditory displays affect user workload?",
   "original": "i04_2225",
   "page_count": 4,
   "order": 649,
   "p1": "2225",
   "pn": "2228",
   "abstract": [
    "The purpose of this study was to determine the effects of the integration of automatic speech recognition (ASR) and spatial audio display technologies on noisy, cognitively demanding armored vehicles. Subjects were instructed to give ASR voice commands in response to simulated monaural or spatialized audio radio communications, while performing a tank-driving task in different levels of ambient tank noise. Independent variables were auditory display, number of talkers giving radio output, and background noise level. The dependent variable was overall subject workload ratings as measured by the National Aeronautics and Space Administration Task Load Index (NASA TLX). Workload data indicated that spatialized audio radio communications, used with and without head trackers significantly decreased overall workload in the radio communication task. There were no other significant effects. These data support previous human performance data. The need for workload measures that examine multiple interfaces is discussed, and a workload modeling tool is recommended.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-648"
  },
  "chen04g_interspeech": {
   "authors": [
    [
     "Fang",
     "Chen"
    ]
   ],
   "title": "Speech interaction system - how to increase its usability?",
   "original": "i04_2229",
   "page_count": 4,
   "order": 650,
   "p1": "2229",
   "pn": "2232",
   "abstract": [
    "This paper discussed different issues related to the usability of speech interaction system. It includes the usability concept, different design approaches, design process and evaluation questions for speech interaction system. Usability is a very fuzzy concept, especially when it related to the speech interaction system: it is hard to measure and it is very much context dependent. The traditional user-centered design approach may not be suitable for the speech interaction system design since the users might not have enough knowledge to see what the technology can do. Usage-centered design may be the better method but there is not comprehensive theory and methodology for the design process and evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-649"
  },
  "beringer04b_interspeech": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "Human language acquisition methods in a machine learning task",
   "original": "i04_2233",
   "page_count": 4,
   "order": 651,
   "p1": "2233",
   "pn": "2236",
   "abstract": [
    "The goal of this study is to develop a psycho-computational model of human phoneme acquisition that includes the knowledge of linguistic universals to \"teach\" Artificial Neural Nets incrementally. Long Short-Term Memory (LSTM) artificial neural networks are capable to outperform previous recurrent networks on many tasks ranging from grammar recognition to speech and robot control. Together with our psycho-computational model they are supposed to recognize phonetic features in a way similar to humans learning to understand their first language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-650"
  },
  "dybkjaer04_interspeech": {
   "authors": [
    [
     "Laila",
     "Dybkjaer"
    ],
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "New challenges in usability evaluation - beyond task-oriented spoken dialogue systems",
   "original": "i04_2261",
   "page_count": 4,
   "order": 652,
   "p1": "2261",
   "pn": "2264",
   "abstract": [
    "There is a fairly good baseline for usability evaluation of task-oriented unimodal spoken dialogue systems (SDSs) but much is still unknown regarding the usability of multimodal and non-task-oriented SDSs. This paper reviews and discusses approaches to usability evaluation of these kinds of SDSs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-651"
  },
  "kimball04_interspeech": {
   "authors": [
    [
     "Owen",
     "Kimball"
    ],
    [
     "Chia-lin",
     "Kao"
    ],
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Teodoro",
     "Arvizo"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Using quick transcriptions to improve conversational speech models",
   "original": "i04_2265",
   "page_count": 4,
   "order": 653,
   "p1": "2265",
   "pn": "2268",
   "abstract": [
    "Using large amounts of training data may prove to be critical to attaining very low error rates in conversational speech recognition. Recent collection efforts by the LDC[1] have produced a large corpus of such data, but to be useful, it must be transcribed. Historically, the cost of transcribing conversational speech has been very high, leading us to consider quick transcription methods that are significantly faster and less expensive than traditional methods. We describe the conventions used in transcription and an automatic utterance segmentation algorithm that provides necessary timing information. Experiments with models trained on a 20-hour set demonstrate that quick transcription works as well as careful transcription, even though the quick transcripts are produced roughly eight times as fast. We also show that when added to a large corpus of carefully transcribed data, quickly transcribed data gives significant improvements in a state-of-the-art ASR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-652"
  },
  "mishra04_interspeech": {
   "authors": [
    [
     "Rohit",
     "Mishra"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Sandra",
     "Upson"
    ],
    [
     "Joyce",
     "Chen"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Stanley",
     "Peters"
    ],
    [
     "Lawrence",
     "Cavedon"
    ],
    [
     "John",
     "Niekrasz"
    ],
    [
     "Hua",
     "Cheng"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "A wizard of oz framework for collecting spoken human-computer dialogs",
   "original": "i04_2269",
   "page_count": 4,
   "order": 654,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "This paper describes a data collection process aimed at gathering human-computer dialogs in high-stress or \"busy\" domains where the user is concentrating on tasks other than the conversation, for example, when driving a car. Designing spoken dialog interfaces for such domains is extremely challenging and the data collected will help us improve the dialog system interface and performance, understand how humans perform these tasks with respect to stressful situations, and obtain speech utterances for extracting prosodic features. This paper describes the experimental design for collecting speech data in a simulated driving environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-653"
  },
  "hartikainen04b_interspeech": {
   "authors": [
    [
     "Mikko",
     "Hartikainen"
    ],
    [
     "Esa-Pekka",
     "Salonen"
    ],
    [
     "Markku",
     "Turunen"
    ]
   ],
   "title": "Subjective evaluation of spoken dialogue systems using SER VQUAL method",
   "original": "i04_2273",
   "page_count": 4,
   "order": 655,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "There is demand for subjective metrics in spoken dialogue system evaluation. SERVQUAL is a service quality evaluation method developed by marketing academics. It produces a subjective measure of the gap between expectations and perceptions in five service quality dimensions common for all services. We present how the method was applied to spoken dialogue system evaluation. In order to improve the suitability of the original method, we modified the test questionnaire and the test process. We demonstrate how the modified method was successfully used in an evaluation of a telephone-based e-mail application. The evaluation gave us directions for further development of the system. In addition, we found some interesting phenomena, such as the variation between genders. We present how the method can be further improved, for example, by dividing the questionnaire into two parts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-654"
  },
  "vasilescu04_interspeech": {
   "authors": [
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Chloe",
     "Clavel"
    ],
    [
     "Thibaut",
     "Ehrette"
    ]
   ],
   "title": "Fiction database for emotion detection in abnormal situations",
   "original": "i04_2277",
   "page_count": 4,
   "order": 656,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "The present research focuses on the acquisition and annotation of vocal resources for emotion detection. We are interested in detecting emotions occurring in abnormal situations and particularly in detecting \"fear\". The present study considers a preliminary database of audiovisual sequences extracted from movie fictions. The sequences selected provide various manifestations of target emotions and are described with a multimodal annotation tool. We focus on audio cues in the annotation strategy and we use the video as support for validating the audio labels. The present article deals with the description of the methodology of data acquisition and annotation. The validation of annotation is realized via two perceptual paradigms in which the +/-video condition in stimuli presentation varies. We show the perceptual significance of the audio cues and the presence of target emotions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-655"
  },
  "sarikaya04_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Paola",
     "Virga"
    ]
   ],
   "title": "Fast semi-automatic semantic annotation for spoken dialog systems",
   "original": "i04_2281",
   "page_count": 4,
   "order": 657,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "This paper describes a bootstrapping methodology for semiautomatic semantic annotation of a \"mini-corpus\" that is conventionally annotated manually to train an initial parser used in natural language understanding (NLU) systems. We propose to cast the problem of semantic annotation as a classification problem: each word is assigned a unique set of semantic tag(s) and/or label(s)from the universal tag/label set. This approach enables \"local\" semantic annotation resulting in partially annotated sentences. The proposed method reduces the annotation time and cost that forms a major bottleneck in the development of NLU systems. We present a set of experiments conducted on the medical domain \"mini-corpus\" that contains 10K hand-annotated sentences. Three annotation methods are compared: parser (baseline), similarity and classification--based annotations. The support vector machine (SVM) based classification scheme is shown to outperform both similarity and parsed--based annotation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-656"
  },
  "wu04b_interspeech": {
   "authors": [
    [
     "Yi-Jian",
     "Wu"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": "A study on automatic detection of Japanese vowel devoicing for speech synthesis",
   "original": "i04_2721",
   "page_count": 4,
   "order": 658,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "In corpus-based speech synthesis, the quality of the synthetic speech critically depends on the speech corpus. Since the high vowel in Japanese might be devoiced in the real speech, we should detect and transcribe them automatically in the corpus construction. In this paper, we apply the HMM-based method, and adopt two kinds of likelihood differences as voicing measures for different focuses. To improve the detection performance, the discriminative training is applied to voiced/ devoiced HMM training. Moreover, some features that can discriminate the voiced/devoiced units, including duration, energy and autocorrelation, are incorporated together with the likelihood differences in several methods. The experiments show different results for each high vowel, i.e. the devoicing is vowel dependent. For the vowel /i/, the discriminative training can improve the detection performance to a certain degree. And by cumulating the voicing features and the likelihood differences with optimized weights, the detection accuracy is improved. But for the vowel /u/, there is very limited improvement, even with the voicing features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-657"
  },
  "ciloglu04_interspeech": {
   "authors": [
    [
     "Tolga",
     "Ciloglu"
    ],
    [
     "Dinc",
     "Acar"
    ],
    [
     "Ahmet",
     "Tokatli"
    ]
   ],
   "title": "Orientel-turkish: telephone speech database description and notes on the experience",
   "original": "i04_2725",
   "page_count": 4,
   "order": 659,
   "p1": "2725",
   "pn": "2728",
   "abstract": [
    "OrienTel-Turkish includes telephone speech recordings and annotations of 1700 Turkish speakers balanced in gender, dialect, age and calling environment; approximately one third of calls are over the fixed network and the rest are over the mobile network. Each speaker contributes with 48 items containing digits, digit/number strings, time/date expressions, phonetically rich words and sentences, command words, and answers to spontaneous questions. The paper describes the contents of the completed database and presents notes on experience related to the preparation of the textual content, speaker recruitment, annotation, and error correction. SAMPA-Turkish has been created during the work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-658"
  },
  "yoon04b_interspeech": {
   "authors": [
    [
     "Tae-Jin",
     "Yoon"
    ],
    [
     "Sandra",
     "Chavarria"
    ],
    [
     "Jennifer",
     "Cole"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Intertranscriber reliability of prosodic labeling on telephone conversation using toBI",
   "original": "i04_2729",
   "page_count": 4,
   "order": 660,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "Two transcribers have labeled prosodic events independently on a subset of Switchboard corpus using adapted ToBI (TOnes and Break Indices) system. Transcriptions of two types of pitch accents (H* and L*), phrasal accents (H- and L-) and boundary tones (H% and L%) encoded independently by two transcribers are compared for intertranscriber reliability. Two commonly used methods of reliability measurement, 'transcriber-pair-word' comparison and the kappa statistic, are used for comparison with previous reports on the intertranscriber consistency. Comparison of the present results with those of previous reliability studies suggests that some higher agreement rates for this study may result from our adoption of fewer labeling distinctions in the transcription of pitch accent events. The results for phrase boundary labeling suggest that spontaneous speech of the type found in the Switchboard corpus is harder to code for the degree of disjuncture between prosodic domains than is read speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-659"
  },
  "tian04b_interspeech": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ]
   ],
   "title": "Efficient compression method for pronunciation dictionaries",
   "original": "i04_2733",
   "page_count": 4,
   "order": 661,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "Pronunciation dictionaries are often used with other data-driven methods to model the pronunciations in phoneme-based automatic speech recognition (ASR) and text-to-speech (TTS) systems. The dictionaries usually take a great amount of memory, which is a limiting factor in portable handheld devices. Compressing the pronunciation dictionaries results in minimal transmission bandwidth and less storage memory. In this paper we present a new procedure to efficiently compress pronunciation dictionaries. First, a novel method transforms the dictionary to a lower entropy representation. Second, the variability in the aligned pronunciation dictionary is reduced to further lower its entropy. Finally, generic lossless compression is applied on the transformed dictionary. Experiments were carried out on English names and words from US English CMU dictionary. The proposed scheme achieved 37.5% improvement over general-purpose lossless text compression.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-660"
  },
  "liang04_interspeech": {
   "authors": [
    [
     "Min-siong",
     "Liang"
    ],
    [
     "Dau-cheng",
     "Lyu"
    ],
    [
     "Yuang-chin",
     "Chiang"
    ],
    [
     "Renyuan",
     "Lyu"
    ]
   ],
   "title": "Construct a multi-lingual speech corpus in taiwan with extracting phonetically balanced articles",
   "original": "i04_2737",
   "page_count": 4,
   "order": 662,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "In this paper, we describe an initial stage to construct a multi-lingual speech corpus in Taiwan with selecting phonetically balanced scripts. It is expected to collect a multilingual speech corpus covering three most frequently used languages in Taiwan, including Taiwanese (Min-nan), Hakka, and Mandarin Chinese. To achieve the objective, constructing a multilingual phonetic alphabet, namely Formosa Phonetic Alphabet (ForPA), is the first step. In addition, the multilingual lexicons (Fomosa Lexicons) are also important parts for building the corpus. Recently, this corpus containing 2,300 speakers' speech database has been finished and is ready to be released. It contains about 200 hours of speech and 404,000 utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-661"
  },
  "heggtveit04_interspeech": {
   "authors": [
    [
     "Per Olav",
     "Heggtveit"
    ],
    [
     "Jon Emil",
     "Natvig"
    ]
   ],
   "title": "Automatic prosody labeling of read norwegian",
   "original": "i04_2741",
   "page_count": 4,
   "order": 663,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "In this paper we present initial work on a method for automatic stress and boundary labelling of read East-Norwegian. The context of this work is automatic corpus annotation for unit selection speech synthesis. A phonological model of Norwegian prosody is described. The identification of syllable stress and major intonational boundaries are key prosodic events for building a prosodic description of a Norwegian utterance according to this model. A CART based method for automatic classification of syllable stress is presented. Initial experiments show that the method is capable of classifying syllables as unaccented or accented with high accuracy. 92.1 % of the unaccented syllables and 90.4 % of the accented syllables were correctly classified.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-662"
  },
  "sanders04b_interspeech": {
   "authors": [
    [
     "Eric",
     "Sanders"
    ],
    [
     "Andrea",
     "Diersen"
    ],
    [
     "Willy",
     "Jongenburger"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Towards automatic word segmentation of dialect speech",
   "original": "i04_2745",
   "page_count": 4,
   "order": 664,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "This paper is about the creation of a digital dialect database, and the focus is on automatic word segmentation. Automatic word segmentation has been studied by several research groups during the last two decades. However, the task we are faced with differs in several respects from previous ones. For instance, in our case we are dealing with recordings of interviews containing spontaneous dialect speech and 'enriched' (quasi-phonetic) orthographic transcriptions (instead of 'normal' orthographic transcriptions, which are usually available). Furthermore, the nature of the task requires that the word segmentation procedure can be adapted for each interview.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-663"
  },
  "fousek04_interspeech": {
   "authors": [
    [
     "Petr",
     "Fousek"
    ],
    [
     "Frantisek",
     "Grezl"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Petr",
     "Svojanovsky"
    ]
   ],
   "title": "New nonsense syllables database - analyses and preliminary ASR experiments",
   "original": "i04_2749",
   "page_count": 4,
   "order": 665,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "The paper presents analyses, modifications, and first experiments with a new nonsense syllables database. Results of preliminary experiments with phoneme recognition are given and discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-664"
  },
  "krebber04_interspeech": {
   "authors": [
    [
     "Jan Felix",
     "Krebber"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "Speech input and output module assessment for remote access to a smart-home spoken dialog system",
   "original": "i04_2753",
   "page_count": 4,
   "order": 666,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "This paper presents the assessment of the speech input and output modules of a spoken dialog system which is accessed via a telephone connection. The spoken dialog system is being developed under the European INSPIRE project. It is used to control a smart home environment. The mentioned modules are assessed separately, with the help of a remote access simulation tool to model degradations which are typical for today's telephone lines. This tool generates impairments in a controlled way, in order to quantify their impact on recognition performance and on speech output quality. First results for the effects of coding impairments on the input and output modules of the INSPIRE dialog system are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-665"
  },
  "kim04r_interspeech": {
   "authors": [
    [
     "Dong-Hyun",
     "Kim"
    ],
    [
     "Yong-Wan",
     "Roh"
    ],
    [
     "Kwang-Seok",
     "Hong"
    ]
   ],
   "title": "An implement of speech DB gathering system using voiceXML",
   "original": "i04_2757",
   "page_count": 4,
   "order": 667,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "In this paper, we introduce speech DB gathering system using VoiceXML. In general, speech DB is very important to speech recognition and synthesis system. But, in the present system which does not use voiceXML, compatibility between different kinds of systems needs much labors and expenses. While VoiceXML is a standard dialog mark-up language for the next generation voice applications. For this reason, nowadays, many company uses VoiceXML when they implement their systems for speech applications. If we implement voice information services using VoiceXML, service developers can save their labors. And, service operators can manage their system easily. If there's need to change their service scenarios, operators can change their service scenarios using Graphical Users Interfaces. It doesn't matter to operators that whether they know technical problems or not. As the results of above mentioned, more and more companies will use VoiceXML when they implement their applications and importance of VoiceXML will be increased. In this paper, we developed speech DB gathering system using VoiceXML for the purpose of verifying technical problems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-666"
  },
  "almasganj04_interspeech": {
   "authors": [
    [
     "Farshad",
     "Almasganj"
    ]
   ],
   "title": "Precise phone boundary detection using wavelet packet and recurrent neural networks",
   "original": "i04_2761",
   "page_count": 4,
   "order": 668,
   "p1": "2761",
   "pn": "2764",
   "abstract": [
    "The automatic segmentation is an important research subject in speech processing. Many approaches are developed in this field and good results are reported. In this paper we show that choosing wavelet packet coefficients enables us to overcome the problem of the tradeoff between frequency and time resolution which appears in normal spectral features like MFCC and causes low time resolution. In addition, the phone boundaries have a transition nature which is an effect of vocal tract movement limitations. Usually, there is a transition zone between two unlike phones. We can use some aspects of this phenomenon in segmentation, by applying features near before and after the boundary to the input of the segmentation model. We used this point, and found good results, and also we found that if we use a more dynamic segmentation model with the ability of following dynamics, like recurrent neural network, it locates phone boundaries more precisely. We tested our approach using two sets of train and test Persian utterances. Experimental results showed an overall 8.14 millisecond tolerance for detected phone boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-667"
  },
  "morris04_interspeech": {
   "authors": [
    [
     "Andrew Cameron",
     "Morris"
    ],
    [
     "Viktoria",
     "Maier"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition",
   "original": "i04_2765",
   "page_count": 4,
   "order": 669,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-668"
  },
  "rhee04b_interspeech": {
   "authors": [
    [
     "Seok-Chae",
     "Rhee"
    ],
    [
     "Sook-Hyang",
     "Lee"
    ],
    [
     "Young-Ju",
     "Lee"
    ],
    [
     "Seok-Keun",
     "Kang"
    ]
   ],
   "title": "Design and construction of Korean-spoken English corpus",
   "original": "i04_2769",
   "page_count": 4,
   "order": 670,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "K-SEC(Korean-Spoken English Corpus) is a kind of speech database that is under construction by the authors of this paper. This article discusses the needs of the K-SEC from various academic disciplines and industrial circles, and it introduces the characteristics of the KSEC design, its catalogues and contents of the recorded database, exemplifying what are being considered from both Korean and English languages' phonetics and phonologies. The K-SEC can be marked as a beginning of a parallel speech corpus, and it is suggested that a similar corpus should be enlarged for the future advancements of the experimental phonetics and the speech information technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-669"
  },
  "vriend04_interspeech": {
   "authors": [
    [
     "Folkert De",
     "Vriend"
    ],
    [
     "Giulio",
     "Maltese"
    ]
   ],
   "title": "Exploring XML-based technologies and procedures for quality evaluation from a real-life case perspective",
   "original": "i04_2773",
   "page_count": 4,
   "order": 671,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "The use of Extensible Markup Language (XML) for the annotation of Spoken Language Resources (SLR) is becoming increasingly common these days. Therefore the Speech Processing EXpertise centre (SPEX), which is the SLR validation centre of the European Language Resources Association (ELRA), is also being confronted more with XML. The project \"Lexica and Corpora for Speech-to- Speech Translation Components\" (LC-STAR) is a project that uses XML for the encoding of its resources. For SPEX, XML-based annotations are still relatively new data formats, which is why at SPEX XML-based quality evaluation (validation) technologies and procedures are being explored. This is done using the XML encoded phonetic lexica developed in the LC-STAR project as a test bed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-670"
  },
  "wang04m_interspeech": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "Spoken language interface in ECMA/ISO telecommunication standards",
   "original": "i04_2777",
   "page_count": 4,
   "order": 672,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "Computer Supported Telecommunication Applications (CSTA) is a widely adopted ECMA/ISO standard suite for global and enterprise communications. As it becomes evident that spoken language systems are playing an ever important role in telecommunications, CSTA has been revised to include a specification on spoken language interfaces so as to promote the interoperability and adoption of spoken language technologies. The new specification shares the same design philosophy as the Speech Application Language Tags, or SALT, that provides simple yet rich functionality for spoken language processing. The tight integration of speech into the telecommunication infrastructure not only presents many technical benefits but also ushers in an era that can potentially make spoken language technologies even more prevalent in our daily lives. This paper describes the design considerations for the specification, and shows how this new standard can be used in conjunction with other standards to create powerful speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-671"
  },
  "davel04_interspeech": {
   "authors": [
    [
     "Marelie",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "The efficient generation of pronunciation dictionaries: machine learning factors during bootstrapping",
   "original": "i04_2781",
   "page_count": 4,
   "order": 673,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "Several factors affect the efficiency of bootstrapping approaches to the generation of pronunciation dictionaries. We focus on factors related to the underlying rule-extraction algorithms, and demonstrate variants of the Dynamically Expanding Context algorithm which are beneficial for this application. In particular, we show that continuous updating of the learned rules, coupled with a new approach to phoneme alignment and a sliding-window approach to choosing the context window, leads to an efficient and accurate bootstrapping mechanism.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-672"
  },
  "geumann04_interspeech": {
   "authors": [
    [
     "Anja",
     "Geumann"
    ]
   ],
   "title": "Towards a new level of anotation detail of multilingual speech corpora",
   "original": "i04_2785",
   "page_count": 4,
   "order": 674,
   "p1": "2785",
   "pn": "2788",
   "abstract": [
    "The aim of this paper is to highlight the actual need for corpora that have been annotated based on acoustic information. The acoustic information should be coded in features or properties and is needed to inform further processing systems, i.e. to present a basis for a speech recognition system using linguistic information. Feature annotation of existing corpora in combination with segmental annotation can provide a powerful training material for speech recognition systems, but will as well challenge the further processing of features to segments and syllables. We present here the theoretical preliminaries for our multilingual feature extraction system, that we are currently working on.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-673"
  },
  "kawaguchi04_interspeech": {
   "authors": [
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Yukiko",
     "Yamaguchi"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "CIAIR in-car speech database",
   "original": "i04_2789",
   "page_count": 4,
   "order": 675,
   "p1": "2789",
   "pn": "2792",
   "abstract": [
    "CIAIR, Nagoya University, has been compiling an in-car speech database since 1999. This paper reports on various characteristics of the database. We have developed a system called the Data Collection Vehicle (DCV), which supports synchronous recording of multichannel audio data from 16 microphones that can be placed in flexible positions, multi-channel video data from 3 cameras, and vehicle-related data. In the compilation process, each subject had conversations with three types of dialogue system: a human, the \"Wizard of OZ\" system, and a conversational system. In this paper, we present the specifications and the characteristics of the database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-674"
  },
  "bael04_interspeech": {
   "authors": [
    [
     "Christophe Van",
     "Bael"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Investigating speech style specific pronunciation variation in large spoken language corpora",
   "original": "i04_2793",
   "page_count": 4,
   "order": 676,
   "p1": "2793",
   "pn": "2796",
   "abstract": [
    "In the past, linguistic research was typically conducted on relatively small datasets that were specifically designed for the research at hand. Whereas to date many large spoken language corpora have become available, the usefulness of these corpora is still not fully established in linguistic research. The research reported on in this paper was conducted to illustrate the potential of large multi-purpose spoken language corpora for linguistic research. The possibility was investigated of identifying phonetic regularities in different speech styles. To this end, a data-driven study was conducted with a large multi-purpose spoken language corpus comprising a manually corrected broad phonetic transcription of the data. Our results show that speech style specific pronunciation processes can indeed be found in such a large corpus. This indicates that large multi-purpose spoken language corpora can contribute to linguistic research, if only for the purpose of hypothesis generation and verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-675"
  },
  "davel04b_interspeech": {
   "authors": [
    [
     "Marelie",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "The efficient generation of pronunciation dictionaries: human factors during bootstrapping",
   "original": "i04_2797",
   "page_count": 4,
   "order": 677,
   "p1": "2797",
   "pn": "2800",
   "abstract": [
    "Bootstrapping has significant potential for the efficient generation of linguistic resources such as electronic pronunciation dictionaries. We describe a system and an approach to bootstrapping that have been used to develop such dictionaries, and report on experiments that we have conducted to investigate the efficiency and effectiveness of the system. Encouraging results have been obtained: even developers with limited linguistic experience can develop accurate pronunciation models in substantially less time than a trained pronunciation expert takes using conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-676"
  },
  "moore04_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Modeling data entry rates for ASR and alternative input methods",
   "original": "i04_2285",
   "page_count": 4,
   "order": 678,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "An often-cited advantage of automatic speech recognition (ASR) is that it is 'fast'; it is quite easy for a person to speak at several hundred words a minute, well above the rates that are possible using other modes of data entry. However, in order to conduct a fair comparison between alternative data entry methods, it is necessary to consider not the input rate per se, but the rate at which it is possible to enter information that is fully correct. This paper describes a model for predicting the relative success of alternative method of data entry in terms of the effective 'throughput' that is achievable taking into account typical input data entry rates, error rates and error correction times. Results are presented for the entry of both conventional and SMS-style text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-677"
  },
  "ban04_interspeech": {
   "authors": [
    [
     "Hiromitsu",
     "Ban"
    ],
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Speech recognition using synchronization between speech and finger tapping",
   "original": "i04_2289",
   "page_count": 4,
   "order": 679,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "Behavioral synchronization between speech and finger tapping provides a novel approach to the improvement of speech recognition accuracy. We combine a sequence of finger tapping timings recorded alongside an utterance using two distinct methods: in the first method, HMM state transition probabilities at the word boundaries are controlled by the timing of the finger tapping; in the second, the probability (relative frequency) of the finger tapping is used as a 'feature' and combined with MFCC in a HMM recognition system. We evaluate these methods through connected digit recognition under different noise conditions (AURORA-2J) and LVCSR tasks. Leveraging the synchrony between speech and finger tapping provides a 46 % relative improvement and a 1 % absolute improvement in connected digit recognition experiments and LVCSR experiments, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-678"
  },
  "gupta04b_interspeech": {
   "authors": [
    [
     "Anurag Kumar",
     "Gupta"
    ],
    [
     "Tasos",
     "Anastasakos"
    ]
   ],
   "title": "Integration patterns during multimodal interaction",
   "original": "i04_2293",
   "page_count": 4,
   "order": 680,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "The development of multimodal interfaces and algorithms for multimodal integration requires knowledge of integration patterns that represent how people use multiple modalities. We analyzed multimodal interaction with three different applications. Semantic analysis revealed that multimodal inputs can exhibit cooperation other than complementary and redundancy. Analysis of the relationship between tasks and multimodality revealed a positive correlation between the type of a task and multimodal interaction. In addition, analysis revealed that referential expressions can contain more information than simple deictic terms such as instructions to select or modify the referents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-679"
  },
  "marcheret04_interspeech": {
   "authors": [
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Stephen M.",
     "Chu"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ]
   ],
   "title": "Efficient likelihood computation in multi-stream HMM based audio-visual speech recognition",
   "original": "i04_2297",
   "page_count": 4,
   "order": 681,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "Multi-stream hidden Markov models have recently been introduced in the field of automatic speech recognition as an alternative to single-stream modeling of sequences of speech informative features. In particular, they have been very successful in audio-visual speech recognition, where features extracted from video of the speaker's lips are also available. However, in contrast to single-stream modeling, their use during decoding becomes computationally intensive, as it requires calculating class-conditional likelihoods of the added stream observations. In this paper, we propose a technique to reduce this overhead by drastically limiting the number of observation probabilities computed for the visual stream. The algorithm estimates a joint co-occurrence mapping of the Gaussian mixture components that separately model the audio and visual observations, and uses it to select the visual mixture components to be evaluated, given the already selected audio ones. We report experiments using this scheme on a connected-digits audio-visual database, where we demonstrate significant speed gains at decoding with only about 5% of the visual Gaussian components requiring evaluation, as compared to the independent evaluation of audio and visual likelihoods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-680"
  },
  "choi04d_interspeech": {
   "authors": [
    [
     "Changkyu",
     "Choi"
    ],
    [
     "Donggeon",
     "Kong"
    ],
    [
     "Hyoung-Ki",
     "Lee"
    ],
    [
     "Sang Min",
     "Yoon"
    ]
   ],
   "title": "Separation of multiple concurrent speeches using audio-visual speaker localization and minimum variance beam-forming",
   "original": "i04_2301",
   "page_count": 4,
   "order": 682,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "Speaker segmentation is an important task in multi-party conversations. Overlapping speech poses a serious problem in segmenting audio into speaker turns. We propose an audio-visual speech separation system consisting of an array microphone with eight sensors and an omni-directional color camera. Multiple concurrent speeches are segmented by fusing the two heterogeneous sensors. Each segmented speech is further enhanced by a linearly constrained minimum variance beamformer. Regardless of coexisting wide-band sound sources and pictures of human in a reverberant environment the proposed system effectively separates multiple target speeches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-681"
  },
  "ariyoshi04_interspeech": {
   "authors": [
    [
     "Tokitomo",
     "Ariyoshi"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Hiroshi",
     "Tsujino"
    ]
   ],
   "title": "Multimodal expression for humanoid robots by integration of human speech mimicking and facial color",
   "original": "i04_2305",
   "page_count": 4,
   "order": 683,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "Multimodal expression is essential for humanoid robots to communicate with people naturally and intelligibly. This paper describes multimodal expression for humanoid robots by mimicking human speech with the ability of expression through \"facial colors\". Currently the robot is able to express joy (by turning yellow in the face), anger (red), sadness (blue), and relaxation (green). These colors have been selected according to color psychology. The human speech mimicking is based on prosody extraction of pitch, loudness and temporal information with speech synthesis based on the extracted prosody. The multimodal expression system implemented on Honda ASIMO shows that facial colors improve affective speech recognition by over 15%. In addition, qualitative observations that use speech and facial color with conflicting affective meanings producing complex affection have been reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-682"
  },
  "novak04b_interspeech": {
   "authors": [
    [
     "Miroslav",
     "Novak"
    ]
   ],
   "title": "Towards large vocabulary ASR on embedded platforms",
   "original": "i04_2309",
   "page_count": 4,
   "order": 684,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "In this paper we present an overview of an automatic speech recognition system implementation in the context of embedded systems. Specific challenges presented by low resource platforms will be addressed for the basic components of an ASR decoder. Our main objective is to utilize and modify the technology developed for large vocabulary ASR to achieve efficient LVCSR on embedded systems as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-683"
  },
  "fujimura04_interspeech": {
   "authors": [
    [
     "Hiroshi",
     "Fujimura"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Analysis of in-car speech recognition experiments using a large-scale multi-mode dialogue corpus",
   "original": "i04_2313",
   "page_count": 4,
   "order": 685,
   "p1": "2313",
   "pn": "2316",
   "abstract": [
    "The dependency of conversational utterances on the mode of dialogue is analyzed. A speech corpus of 800 speakers collected under three different modes, i.e., talking to a human operator, an WOZ system and an ASR system, is used for analysis. Some characteristics such as sentence ;complexity loudness of the voice and speaking-rate are found to be significantly different among the dialogue modes. Linear regression analysis results also clarify the relative importance of those characteristics on speech recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-684"
  },
  "tan04_interspeech": {
   "authors": [
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "On the integration of speech recognition into personal networks",
   "original": "i04_2317",
   "page_count": 4,
   "order": 686,
   "p1": "2317",
   "pn": "2320",
   "abstract": [
    "Mobile communication presents a number of challenges to speech technology such as the limited resources available in the terminals in addition to the bandwidth constraints and the errors occurring in transmissions over mobile networks. These challenges need to be solved before automatic speech recognition (ASR) is ready for widespread use in the context of personal communication environments. This paper gives an overview of the problems inherent in the recently developed network based ASR with an emphasis on the robustness issues that are highly influenced by network degradations. The paper further presents a number of transmission error protection and concealment schemes that are evaluated in a number of ASR experiments encompassing a range of typical real-environment transmission errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-685"
  },
  "rose04_interspeech": {
   "authors": [
    [
     "Richard",
     "Rose"
    ],
    [
     "Hong Kook",
     "Kim"
    ]
   ],
   "title": "Robust speech recognition in client-server scenarios",
   "original": "i04_2321",
   "page_count": 4,
   "order": 687,
   "p1": "2321",
   "pn": "2324",
   "abstract": [
    "This paper addresses issues that are specific to the implementation of automatic speech recognition (ASR) applications and services in client-server scenarios. It is assumed in all of these scenarios that functionality in a human-machine dialog system is distributed between mobile client devices and network based multi-user media and application servers. It is argued that, while there has already been a great deal of research addressing issues relating to the communications channels associated with these scenarios, there are many additional problems that have received relatively little attention. These include issues of how environmental and speaker robustness algorithms are implemented in mobile domains and how multiple ASR channels can be implemented more efficiently in multi-user deployments. Preliminary results are summarized showing the effect of user specific unsupervised adaptation and normalization algorithms on ASR performance in mobile domains. Results are also presented demonstrating the efficiencies that are obtainable from using intelligent algorithms for assigning ASR decoders to computation servers in multi-user deployments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-686"
  },
  "jeong04_interspeech": {
   "authors": [
    [
     "Sangbae",
     "Jeong"
    ],
    [
     "Iksang",
     "Han"
    ],
    [
     "Eugene",
     "Jon"
    ],
    [
     "Jeongsu",
     "Kim"
    ]
   ],
   "title": "Memory and computation reduction for embedded ASR systems",
   "original": "i04_2325",
   "page_count": 4,
   "order": 688,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "Nowadays, the performance of mobile devices such as personal digital assistants (PDA) or cell phones are rapidly improving. This improvement increased the demand for various functions in mobile devices. Among them, embedded speech recognizers are one of the major research topics. To guarantee an acceptable performance, the efficient usage of the hardware resources is very important. In this paper, we introduce our embedded speech recognizer and several important technologies implemented in it used to reduce hardware resources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-687"
  },
  "fukuda04_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Canonicalization of feature parameters for automatic speech recognition",
   "original": "i04_2537",
   "page_count": 4,
   "order": 689,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "Acoustic models (AMs) of an HMM-based classifier include various types of hidden variables such as gender type, speaking rate, and acoustic environment. If there exists a canonicalization process that reduces the influence of the hidden variables from the AMs, a robust automatic speech recognition (ASR) system can be realized. In this paper, we describe the configuration of a canonicalization process targeting gender type as a hidden variable. The proposed canoncalization process is composed of multiple distinctive phonetic feature (DPF) extractors corresponding to the hidden variable and a DPF selector. Experiments are carried out by comparing (A) the combination of the canonicalized DPF and a single HMM classifier, and (B) the combination of a single acoustic feature (MFCC) and multiple HMM classifiers. The result shows that the proposed canonicalization method outperforms both of conventional ASR with MFCC and a single HMM and ASR with multiple HMMs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-688"
  },
  "srinivasan04b_interspeech": {
   "authors": [
    [
     "Soundararajan",
     "Srinivasan"
    ],
    [
     "Nicoleta",
     "Roman"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "On binary and ratio time-frequency masks for robust speech recognition",
   "original": "i04_2541",
   "page_count": 4,
   "order": 690,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "A time-varying Weiner filter extracts the speech signal from a noisy mixture using the a priori signal-to-noise ratio in a local time-frequency unit. We estimate this ratio using a binaural processor and derive a ratio time-frequency mask. This mask is used to extract the speech signal, which is then fed to a conventional speech recognizer operating in the cepstral domain. We compare the performance of this system with a missing data recognizer that operates in the spectral domain using the time-frequency units dominated by speech. For use by the missing data recognizer, the same processor is used to estimate an ideal time-frequency binary mask, which selects the speech signal if it is stronger than the interference in a local time-frequency unit. We find that the performance of the missing data recognizer is better on a small vocabulary recognition task but the performance of the conventional recognizer is substantially better when the vocabulary size is larger.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-689"
  },
  "sanchis04_interspeech": {
   "authors": [
    [
     "Alberto",
     "Sanchis"
    ],
    [
     "Alfons",
     "Juan"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "New features based on multiple word graphs for utterance verification",
   "original": "i04_2545",
   "page_count": 4,
   "order": 691,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "The goal of Utterance Verification is to estimate a confidence measure which helps detecting words in the hypothesized sentence that are likely to have been missrecognized. Word graphs have been extensively employed for directly estimating the confidence measure and for extracting important predictor features. In all the cases, a single word graph which is obtained through the recognition process. In this paper we propose the use of multiple word graphs to compute new features. The experimental study shows that these proposed features outperform those computed on a single word graph and other well-known predictor features. Moreover, the combination of the proposed features along with other kind of features provides improvements in the verification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-690"
  },
  "burget04_interspeech": {
   "authors": [
    [
     "Lukas",
     "Burget"
    ]
   ],
   "title": "Combination of speech features using smoothed heteroscedastic linear discriminant analysis",
   "original": "i04_2549",
   "page_count": 4,
   "order": 692,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "Feature combination techniques based on PCA, LDA and HLDA are compared in experiments where limited amount of training data is available. Success with feature combination can be quite dependent on proper estimation of statistics required by the used technique. Insufficiency of training data is, therefore, an important problem, which has to be taken in to account in our experiments. Besides of some standard approaches increasing robustness of statistic estimation, methods based on combination of LDA and HLDA are proposed. An improved recognition performance obtained using these methods is demonstrated in experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-691"
  },
  "ikbal04b_interspeech": {
   "authors": [
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Hemant",
     "Misra"
    ],
    [
     "Sunil",
     "Sivadas"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Entropy based combination of tandem representations for noise robust ASR",
   "original": "i04_2553",
   "page_count": 4,
   "order": 693,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "In this paper, we present an entropy based method to combine tandem representations of the recently proposed Phase AutoCorrelation (PAC) based features and Mel-Frequency Cepstral Coefficients (MFCC) features. PAC based features, derived from a nonlinear transformation of autocorrelation coefficients and shown to be noise robust, improve their robustness to additive noise in their tandem representation. On the other hand, MFCC features in their tandem representation show a significant improvement in recognition performance on clean speech.An entropy based combination method investigated in this paper adaptively gives a higher weighting to the representation of MFCC features in clean speech and to the representation of PAC based features in noisy speech, thus yielding a robust recognition performance in all conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-692"
  },
  "yook04_interspeech": {
   "authors": [
    [
     "Dongsuk",
     "Yook"
    ],
    [
     "Donghyun",
     "Kim"
    ]
   ],
   "title": "Fast speech adaptation in linear spectral domain for additive and convolutional noise",
   "original": "i04_2557",
   "page_count": 4,
   "order": 694,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "In this paper, we propose a transform-based adaptation technique for robust speech recognition in unknown environments. It uses maximum likelihood spectral transform (MLST) algorithm with additive and convolutional noise parameters. Previously many adaptation algorithms have been proposed in the cepstral domain. Though the cepstral domain may be appropriate for the speech recognition, it is difficult to handle environmental noise directly in the cepstral domain. Therefore our approach deals with such noise in the linear spectral domain in which speech is directly affected by the noise. As a result, we can use a small number of noise parameters for fast adaptation. The experiments evaluated on the FFMTIMIT corpus shows promising result with only a small number of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-693"
  },
  "hetherington04_interspeech": {
   "authors": [
    [
     "Lee",
     "Hetherington"
    ]
   ],
   "title": "The MIT finite-state transducer toolkit for speech and language processing",
   "original": "i04_2609",
   "page_count": 4,
   "order": 695,
   "p1": "2609",
   "pn": "2612",
   "abstract": [
    "We present the MIT Finite-State Transducer Toolkit and briefly describe research that it has benefitted. The toolkit is a collection of command-line tools and associated C++ API for manipulating finite-state transducers (FSTs) and acceptors (FSAs) and has been designed to enable research through its flexibility, yet remain efficient enough to aid real-world computationally demanding applications such as automatic speech recognition. The toolkit supports the construction, combination, optimization, and training of weighted FSTs and FSAs, and as such is useful in many areas of human language technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-694"
  },
  "feng04_interspeech": {
   "authors": [
    [
     "Junlan",
     "Feng"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Question-answering in webtalk: an evaluation study",
   "original": "i04_2613",
   "page_count": 4,
   "order": 696,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "WebTalk is an intelligent agent that automatically mines company websites to create interactive spoken and chat-based dialog systems. In this paper, we briefly describe the technologies that WebTalk utilizes for extracting task knowledge from websites and answering questions specified in natural language. We elaborate on an evaluation of the question answering component of WebTalk. In particular, we compare this component with a deployed hand-crafted question answering agent. A detailed evaluation analysis is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-695"
  },
  "huerta04b_interspeech": {
   "authors": [
    [
     "Juan",
     "Huerta"
    ],
    [
     "Chaitanya",
     "Ekanadham"
    ]
   ],
   "title": "Automatic network optimization of voice applications",
   "original": "i04_2617",
   "page_count": 4,
   "order": 697,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "We present a technique to automatically transform a voice application's call flow, or interaction graph, into the set of application pages that encapsulate the call flow nodes and resources needed to run such application in a client-server environment. Our technique, called NOVA, performs this automatic partition by making use of a set of cost functions that model the latencies associated with the transmission of data and application resources through a network and their processing by the client components (e.g., voice browsers, grammar compilers and system engines). Our technique facilitates the existence of tools that permit application developers to focus on designing the call-flow of the application while leaving the task of segmenting and packaging the application into pages to our algorithm. The cost functions utilized by our algorithm can be dynamically computed allowing for runtime application optimization. We demonstrate the impact of our algorithm through simulation experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-696"
  },
  "rodriguezmoreno04_interspeech": {
   "authors": [
    [
     "Miguel Angel",
     "Rodriguez-Moreno"
    ],
    [
     "Heriberto",
     "Cuayahuitl"
    ],
    [
     "Juventino",
     "Montiel-Hernandez"
    ]
   ],
   "title": "Voicebuilder: a framework for automatic speech application development",
   "original": "i04_2621",
   "page_count": 4,
   "order": 698,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "In this paper we present VoiceBuilder, a framework for automating the process of developing speech applications. For such purpose, our framework allows speech User Interface (UI) specialists to introduce UIs in two ways: a stand-alone GUI application, and a web-based interface; in which speech UIs are stored in markup language previously proposed called SUIML [1], supporting either system initiative or mixed initiative dialogue strategies. For automatic coding, we propose an algorithm based on a macro-processor that generates VoiceXML code through parsing SUIML documents, this algorithm was designed to generate various kinds of code with a minimal initial effort. We performed experiments considering both system initiative and mixed initiative dialogue strategies with three different speech applications: auto-attendant, e-mail reader, and flight reservations. This framework is very useful for building speech applications in new domains with no programming effort and could be incorporated into several voice toolkits.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-697"
  },
  "facco04_interspeech": {
   "authors": [
    [
     "Andrea",
     "Facco"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Marcello",
     "Vigano"
    ]
   ],
   "title": "On the development of telephone applications: some practical issues and evaluation",
   "original": "i04_2625",
   "page_count": 4,
   "order": 699,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "The paper describes the major problems encountered during the development of two automatic services delivered through a telephone platform. Service specification, system design and development, and service evaluation and tuning will be described. A significant number of user interactions, including both speech signals and recognizer outputs, have been collected during the usage of the systems. These corpora have been used for testing/retraining both acoustic and language models. In one of the two services (i.e. the automatic payment of the road tax) both context independent and context dependent acoustic models have been compared. In this service, furthermore, an unsupervised training procedure has been evaluated. The second service, a voice portal for accessing financial information, is mainly based on word spotting. We will discuss the effort needed to both develop and tune the related speech recognition grammars.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-698"
  },
  "hamerich04_interspeech": {
   "authors": [
    [
     "Stefan",
     "Hamerich"
    ],
    [
     "Volker",
     "Schless"
    ],
    [
     "Basilis",
     "Kladis"
    ],
    [
     "Volker",
     "Schubert"
    ],
    [
     "Otilia",
     "Kocsis"
    ],
    [
     "Stefan",
     "Igel"
    ],
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Luis Fernando",
     "Dharo"
    ],
    [
     "José Manuel",
     "Pardo"
    ]
   ],
   "title": "The GEMINI platform: semi-automatic generation of dialogue applications",
   "original": "i04_2629",
   "page_count": 4,
   "order": 700,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "The EC funded research project GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) has two main objectives: On the one hand the development and implementation of a platform able to produce user-friendly interactive multilingual and multimodal dialogue interfaces to databases with a minimum of human effort and on the other hand the demonstration of the platform's efficiency through the development of two different applications using this platform. The platform consists of different assistants that help the user to semi-automatically generate dialogue applications. Its open and modular architecture simplifies the adaptability of generated applications to different use cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-699"
  },
  "kondo04_interspeech": {
   "authors": [
    [
     "Kazuhiro",
     "Kondo"
    ],
    [
     "Kiyoshi",
     "Nakagawa"
    ]
   ],
   "title": "A packet loss concealment method using recursive linear prediction",
   "original": "i04_2633",
   "page_count": 4,
   "order": 701,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "We proposed and evaluated a speech packet loss concealment method, which predicts lost segment from speech included in both packets before and after the lost packet. The lost segments are predicted by recursively using linear prediction both in the forward direction from the packet preceding the loss, as well as in the backward direction from the packet succeeding the lost segment. The predicted sample in each direction is smoothed to obtain the final interpolated signal. The adjacent segments are also smoothed to significantly reduce the discontinuity between the interpolated signals. Subjective quality of the proposed method showed higher scores than the packet loss concealment algorithm described in the ITU standard G.711 Appendix I, with MOS rating exceeding 2.4, even at an extremely high packet loss rate of 30%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-700"
  },
  "lee04q_interspeech": {
   "authors": [
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "On a n-gram model approach for packet loss concealment",
   "original": "i04_2637",
   "page_count": 4,
   "order": 702,
   "p1": "2637",
   "pn": "2640",
   "abstract": [
    "In this paper, we investigate the possibility of predicting lost packets for packet loss concealment using n-gram predictive models. Unlike the conventional repetition-based algorithms, the proposed algorithm is based on the Shannon game, which serves as a principle for predicting the speech parameters of lost packets using the previously received parameters. During training phase, we construct statistical backoff n-gram models. In test phase, the models are used to predict the speech parameters of lost packets. Experiments were performed on switchboard telephone speech database and the proposed algorithm is compared with the conventional repetition-based algorithm. The performance is evaluated in terms of the spectral distortion between the original and the predicted (or repeated) speech. The algorithm based on the backoff n-gram models reduces the spectral distortion by 8.7% over the conventional repetition-based algorithm for the first lost packet after receiving one. Further it maintains about 6.2% improvement up to six consecutive lost packets. In terms of perplexity of the predictive models, backoff n-gram approach outperforms the repetition-based algorithm by 8.65%, which is very close to the improvement rate obtained from the spectral distortion measurement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-701"
  },
  "so04_interspeech": {
   "authors": [
    [
     "Stephen",
     "So"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Efficient vector quantisation of line spectral frequencies using the switched split vector quantiser",
   "original": "i04_2641",
   "page_count": 4,
   "order": 703,
   "p1": "2641",
   "pn": "2644",
   "abstract": [
    "In this paper, we investigate the use of a switched split vector quantiser (SSVQ) for coding linear predictive coding (LPC) parameters. The SSVQ is applied to quantise the LPC parameters in terms of line spectral frequencies from the TIMIT database and its performance is compared with the split vector quantiser. Experimental results show that the SSVQ provides a better trade-off between bit-rate and distortion performance than the split VQ. In addition, the SSVQ has a lower computational (search) complexity than the split VQ, though this is attained at the expense of an increase in memory requirements. In order to achieve a spectral distortion of 1 dB, the three-part SSVQ with an 8 directional switch requires 23 bits/frame, 4.41 kflops/frame of computations and 8272 floats of memory, while the corresponding values for a traditional three-part split VQ are 25 bits/frame, 13.3 kflops/frame and 3328 floats, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-702"
  },
  "chaitanya04_interspeech": {
   "authors": [
    [
     "M.",
     "Chaitanya"
    ],
    [
     "S. R. M.",
     "Prasanna"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Enhancement of reverberant speech using excitation source information",
   "original": "i04_2645",
   "page_count": 4,
   "order": 704,
   "p1": "2645",
   "pn": "2648",
   "abstract": [
    "This paper proposes a method for the enhancement of reverberant speech using the knowledge of the excitation source of speech production. The degradation level in the reverberant speech is measured in terms of Speech-to Reverberation component Ratio (SRR). From perception and processing point of view high SRR regions are important. Hence the proposed method identifies and enhances the speech in high SRR regions. The high SRR regions are identified using the Hilbert envelope of the Linear Prediction (LP) residual, which contains information about the excitation source of speech production. The Hilbert envelope of the LP residual derived from the reverberant speech is processed by the covariance analysis to derive the weight function. The LP residual of the reverberant speech is multiplied with the weight function to enhance the excitations of speech in the high SRR regions. The speech signal synthesized from the modified LP residual is found to be less reverberant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-703"
  },
  "kinoshita04_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Masato",
     "Miyoshi"
    ]
   ],
   "title": "Improving automatic speech recognition performance and speech inteligibility with harmonicity based dereverberation",
   "original": "i04_2649",
   "page_count": 4,
   "order": 705,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "A speech signal captured by a distant microphone is generally smeared by reverberation, that severely degrades both the speech intelligibility and Automatic Speech Recognition (ASR) performance. Previously, we proposed a novel dereverberation method, named \"Harmonicity based dEReverBeration (HERB)\", which estimates the inverse filter of an unknown impulse response by utilizing the inherent speech property, harmonics. In this paper, we carry out a formal evaluation of speech intelligibility for dereverberated speech, and further investigate HERB's possibilities to improve ASR performance. Experimental results show that HERB is able to improve speech intelligibility to the level of clean speech. HERB is also found to be very effective at improving ASR performance, even under unknown severe reverberant environments by being used with MLLR and a multicondition acoustic model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-704"
  },
  "lee04r_interspeech": {
   "authors": [
    [
     "Seung Yeol",
     "Lee"
    ],
    [
     "Nam Soo",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Inner product based-multiband vector quantization for wideband speech coding at 16 kbps",
   "original": "i04_2653",
   "page_count": 4,
   "order": 706,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "This paper describes a multiband vector quantization (VQ) technique for wideband speech coding at 16 kb/s. Our approach consists of splitting the input speech into two separate bands and then applying an independent coding scheme for each band. A code excited linear prediction (CELP) coder is used in the lower band while a transform based coding strategy is applied in the higher band. The spectral components in the higher frequency band are represented by a set of modulated lapped transform (MLT) coefficients. The higher frequency band is divided into three subbands, and the MLT coefficients construct a vector for each subband. For the vector quantization (VQ) of these vectors, an inner product-based distance measure is proposed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-705"
  },
  "abad04_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Speech enhancement and recognition by integrating adaptive beamforming and wiener filtering",
   "original": "i04_2657",
   "page_count": 4,
   "order": 707,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "A robust adaptive beamforming method is presented in this paper for speech enhancement and speech recognition with microphone arrays. The proposal is based on a modification of the Generalized Sidelobe Canceller with adaptive blocking matrix and the use of a Wiener filter. Alternatively to most of the previous reported works based on microphone arrays with postfiltering, the new technique integrates the Wiener filter in the structure of the adaptive beamformer in a single stage. Experimental results show that the proposed integrated adaptive Wiener-filtering (IAW) beamformer usually is more robust to directional and ambient noises than conventional postfiltering of the beamformer output with a lower level of degradation. Speech recognition experiments which show improvements with the proposed beamformer are also reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-706"
  },
  "kim04s_interspeech": {
   "authors": [
    [
     "Kyung-Tae",
     "Kim"
    ],
    [
     "Sung-Kyo",
     "Jung"
    ],
    [
     "MiSuk",
     "Lee"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "Temporal normalization techniques for transform-type speech coding and application to split-band wideband coders",
   "original": "i04_2661",
   "page_count": 4,
   "order": 708,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "In this paper we present an efficient coding method for the upper band(4-7kHz) of wideband(0.5-7kHz) speech coding based on a band-split approach. Due to the impulse-like characteristics in upper band signal, it is very difficult to efficiently quantize the signal at low bit-rate when we use transform coding techniques. We propose two temporal normalization techniques, direct temporal energy normalization and frequency domain linear prediction, to reduce the extremely noticeable artifacts. Simulation results show that the proposed algorithm successfully encodes the upper band signal,and the new split-band type wideband coder adopting the proposed technology provides better quality than 56 kbit/s ITU-T G.722 at the bit-rate of 20 kbit/s.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-707"
  },
  "asai04_interspeech": {
   "authors": [
    [
     "Tatsunori",
     "Asai"
    ],
    [
     "Shigeki",
     "Miyabe"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Interface for barge-in free spoken dialogue system using adaptive sound field control",
   "original": "i04_2665",
   "page_count": 4,
   "order": 709,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "This paper describes a new interface for a barge-in free spoken dialogue system combining an adaptive sound field control and a microphone array. In order to actualize robustness against the change of transfer functions due to the various interferences, the barge-in free spoken dialogue system which uses sound field control and a microphone array has been proposed by one of the authors. However, this method cannot follow the large change of transfer functions. To solve the problem, we introduce a new adaptive sound field control that follows the change of transfer functions. The experimental results reveal that the proposed method can improve the reduction accuracy of response sound in comparison with the conventional acoustic echo canceller as well as the previously proposed method which simply uses fixed sound field control system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-708"
  },
  "kim04t_interspeech": {
   "authors": [
    [
     "Jong-Hark",
     "Kim"
    ],
    [
     "Jae-Hyun",
     "Shin"
    ],
    [
     "In-Sung",
     "Lee"
    ]
   ],
   "title": "Multi-mode harmonic transfrom excitation LPC coding for speech and music",
   "original": "i04_2669",
   "page_count": 4,
   "order": 710,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "A multi-mode harmonic transform coding (MMHTC) for speech and music signals is proposed. Its structure is organized as a linear prediction model with an input of harmonic and transform-based excitation. The proposed coder also utilizes harmonic prediction and an improved quantizer of excitation signal. To efficiently quantize the excitation of music signals, the modulated lapped transform (MLT) is introduced. In other words, the coder combines both the time domain (linear prediction) and the frequency domain technique to achieve the best perceptual quality. The proposed coder showed better speech quality than that of the 8kbps QCELP coder at a bit-rate of 4kbps.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-709"
  },
  "gandhi04_interspeech": {
   "authors": [
    [
     "Mital",
     "Gandhi"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Source separation using particle filters",
   "original": "i04_2673",
   "page_count": 4,
   "order": 711,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "Our goal is to study the statistical methods for source separation based on temporal and frequency specific features by using particle filtering. Particle filtering is an advanced state-space Bayesian estimation technique that supports non-Gaussian and nonlinear models along with time-varying noise, allowing for a more accurate model of the underlying system dynamics. We present a system that combines standard speech processing techniques in a novel method to separate two noisy speech sources. The system models the pitch and amplitude over time separately, and adopts particle filtering to reduce complexity by generating a discrete distribution that approximates well the desired continuous distribution. Preliminary results that demonstrate the separation of two noisy sources using this system are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-710"
  },
  "ramo04_interspeech": {
   "authors": [
    [
     "Anssi",
     "Ramo"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Sakari",
     "Himanen"
    ],
    [
     "Ari",
     "Heikkinen"
    ]
   ],
   "title": "Segmental speech coding model for storage applications",
   "original": "i04_2677",
   "page_count": 4,
   "order": 712,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "This paper introduces a novel speech coder structure for storage applications operating at low bit rates. The coder exploits the inherent segmental nature of speech signals by dividing the input into segments of variable length. Quite often the length of the segment is the same as the length of the phoneme. The individual segments are coded using adaptive techniques that take into account the relative perceptual importance of different types of speech, e.g. voiced and unvoiced speech. These main features of the proposed approach are enabled by the fact that many of the design constraints related to realtime conversational speech can be relaxed in storage applications. A practical implementation containing the speech-adaptive segmentation is described and its performance is verified in a listening test at average bit rates of about 1.0 kbps and 2.4 kbps respectively. The results show that the segmental model significantly improves the coding efficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-711"
  },
  "ju04_interspeech": {
   "authors": [
    [
     "Gwo-hwa",
     "Ju"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Improved speech enhancement by applying time-shift property of DFT on hankel matrices for signal subspace decomposition",
   "original": "i04_2681",
   "page_count": 4,
   "order": 713,
   "p1": "2681",
   "pn": "2684",
   "abstract": [
    "In previous studies, the signal subspace technique for speech enhancement was extended and a perceptually constrained generalized singular value decomposition (PCGSVD)-based algorithm [1] was developed which properly integrated the auditory masking effect and the GSVD algorithm. Both objective measures and subjective tests verified that this approach can offer better performance than the GSVD-based approach and the conventional spectral subtraction (SS) algorithm. But very high computational complexity is required in the PCGSVD-based method when performing the matrices decomposition via the GSVD algoruthm. In this paper, we properly utilize the time-shift property of DFT and the special structure of Hankel matrices to perform similar functions previously offered by GSVD, and a perceptually constrained minimum variance estimation algorithm is developed. By replacing GSVD algorithm with DFT, the computation complexity is significantly reduced, almost the same as the conventional SS algorithm. Experiments showed that comparable performance to that of the PCGSVD-based approach can be achieved, regardless of whether the additive noise is stationary or not, specially when it is non-white.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-712"
  },
  "turunen04_interspeech": {
   "authors": [
    [
     "Jari Juhani",
     "Turunen"
    ],
    [
     "Juha",
     "Tanttu"
    ],
    [
     "Frank",
     "Cameron"
    ]
   ],
   "title": "Minimum phase compensation in speech coding using hammerstein model",
   "original": "i04_2685",
   "page_count": 4,
   "order": 714,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "When using Bai's method for estimating the coefficients of both the linear and nonlinear parts of a Hammerstein model, there is no guarantee that the resulting linear filter will be stable. To obtain a stable linear filter the well-known minimum phase correction can be used. However, if it used without some compensation in the nonlinear part, one obtains a Hammerstein model with no theoretical foundation. In this paper we present a simple method for compensating the nonlinear part for corrections performed to the linear part. The compensation proved to be stable and fulfills the requirements of combined parameter estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-713"
  },
  "li04g_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Optimizing regression for in-car speech recognition using multiple distributed microphones",
   "original": "i04_2689",
   "page_count": 4,
   "order": 715,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "In this paper, we address issues in improving hands-free speech recognition performance in different car environments using multiple spatially distributed microphones. In previous work, we proposed multiple regression of the log-spectra (MRLS) for estimating the log-spectra of speech at a close-talking microphone. In this paper, the idea is extended to nonlinear regressions. Isolated word recognition experiments under real car environments show that, compared to the nearest distant microphone, recognition accuracies could be improved by about 40% for very noisy driving conditions by using the optimizing regression method, The proposed approach outperforms linear regression methods and also outperforms adaptive beamformer by 8% and 3% respectively in terms of averaged recognition accuracies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-714"
  },
  "li04h_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Speech enhancement based on magnitude estimation using the gamma prior",
   "original": "i04_2693",
   "page_count": 4,
   "order": 716,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "In this paper, we propose a speech enhancement method based on spectral magnitude estimation. We modify the noise estimation from the minimum statistics method and combine with a maximum a posterior (MAP) decomposition, using the Rice-conditional probability and a non-Gaussian statistic model of the speech. We derive two versions of magnitude decomposition and magnitude-phase decomposition and compare to spectral subtraction and other MAP methods based on the Gaussian statistic (MMSE, LSA). The experiments show the advantage of the proposed method in the improvement of both SNR (up to 12 dB) and recognition accuracy rate (up to 21 % to base line).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-715"
  },
  "errity04_interspeech": {
   "authors": [
    [
     "Andrew",
     "Errity"
    ],
    [
     "John",
     "McKenna"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Unscented kalman filtering of line spectral frequencies",
   "original": "i04_2697",
   "page_count": 4,
   "order": 717,
   "p1": "2697",
   "pn": "2700",
   "abstract": [
    "We propose a new method for estimating Line Spectral Frequency (LSF) trajectories which uses unscented Kalman filtering (UKF). This method is based upon an iterative Expectation Maximisation (EM) approach in which LSF estimates are generated during a forward pass and then smoothed during a backward pass. The EM approach also provides re-estimated Kalman filter parameters for further forward-backward passes that improve estimation. This approach exploits the non-independence of neighbouring spectra. We estimate LSFs as they have good interpolation and quantization properties. This allows us to estimate LSF trajectories that are guaranteed to result in stable filters. We analyse noisy synthetic speech using this technique. The results compare favourably with other methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-716"
  },
  "kim04u_interspeech": {
   "authors": [
    [
     "Hyoung-Gook",
     "Kim"
    ],
    [
     "Thomas",
     "Sikora"
    ]
   ],
   "title": "Speech enhancement based on smoothing of spectral noise floor",
   "original": "i04_2701",
   "page_count": 4,
   "order": 718,
   "p1": "2701",
   "pn": "2704",
   "abstract": [
    "This paper presents robust speech enhancement using noise estimation based on smoothing of spectral noise floor (SNF) for nonstationary noise environments. The spectral gain function is obtained by well-known log-spectral amplitude (LSA) estimation criterion associated with the speech presence uncertainty. The noise estimate is given by averaging actual spectral power values, using a smoothing parameter that depends on smoothing of spectral noise floor. The noise estimator is very simple but achieves a good tracking capability for a nonstationary noise. Its enhanced speech is free of musical tones and reverberation artifacts and sounds very natural compared to methods using other short-time spectrum attenuation techniques. The performance is measured by the segmental signal-to-noise ratio (SNR), the speech/ speaker recognition accuracy and the speaker change detection rate for the audio segmentation using MFCC-features (Mel-scale Frequency Cepstral Coefficients) in comparison to other single microphone noise reduction methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-717"
  },
  "li04i_interspeech": {
   "authors": [
    [
     "Junfeng",
     "Li"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Noise reduction using hybrid noise estimation technique and post-filtering",
   "original": "i04_2705",
   "page_count": 4,
   "order": 719,
   "p1": "2705",
   "pn": "2708",
   "abstract": [
    "In this paper, a novel noise reduction method using hybrid noise estimation technique and post-filtering is proposed to suppress both localized and non-localized noise components which can not be dealt with by the traditional methods [2][3][4]. To do this, a hybrid noise estimation approach is proposed by combining our previously constructed multi-channel noise estimation approach and a single-channel estimation approach to improve estimation accuracy for localized noise components. The non-localized noise components are suppressed by a single-channel post-filter based on an optimally modified log spectral amplitude (OM-LSA) estimator. To verify the superiorities of the proposed hybrid noise estimation approach and noise reduction system, they are compared to the multi-channel and single-channel scheme based systems under various noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-718"
  },
  "gabrea04_interspeech": {
   "authors": [
    [
     "Marcel",
     "Gabrea"
    ]
   ],
   "title": "An adaptive kalman filter for the enhancement of speech signals",
   "original": "i04_2709",
   "page_count": 4,
   "order": 720,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "This paper deals with the problem of speech enhancement when only a corrupted speech signal is available for processing. Kalman filtering is known as an effective speech enhancement technique, in which speech signal is usually modeled as autoregressive(AR) model and represented in the state-space domain. Various approaches based on the Kalman filter are presented in the literature. They usually operate in two steps: first, additive noise and driving process variances and speech model parameters are estimated and second, the speech signal is estimated by using Kalman filtering. In this paper sequential estimators are used for sub-optimal adaptive estimation of the unknown a priori driving process and additive noise statistics simultaneously with the system state. The estimation of time-varying AR signal model is based on weighted recursive least square algorithm with variable forgetting factor. The proposed algorithm provides improved state estimates at little computational expense.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-719"
  },
  "sreenivas04c_interspeech": {
   "authors": [
    [
     "T. V.",
     "Sreenivas"
    ],
    [
     "K.",
     "Sharath Rao"
    ],
    [
     "A.",
     "Sreenivasa Murthy"
    ]
   ],
   "title": "Improved iterative wiener filtering for non-stationary noise speech enhancement",
   "original": "i04_2713",
   "page_count": 4,
   "order": 721,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "A clean speech VQ codebook has been shown to be effective in imposing intraframe constraints in Iterative Wiener Filtering (CCIWF) for speech enhancement. However, for time-varying noises, the performance is sub-optimum. We propose a smoothed noise update technique that uses the estimated signal spectrum for subsequent signal estimation. This leads to a more effective solution than the soft-decision based noise estimate found in literature. Further, the CCIWF performance is improved using codebook constraints in the LAR domain instead of LPC domain. Also, a new iteration initialization method is proposed which results in better enhancement in over 70% of the frames. Thus, we show that a combination of a robust parameter space, an effective initialization and continuous spectrum update significantly improves the performance of speech enhancement. Speech recognition results show that the new combination provides 10-20% increase in word recognition scores whereas simple spectral subtraction results in an actual decrease in recognition score.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-720"
  },
  "qian04b_interspeech": {
   "authors": [
    [
     "Yasheng",
     "Qian"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Highband spectrum envelope estimation of telephone speech using hard/soft-classification",
   "original": "i04_2717",
   "page_count": 4,
   "order": 722,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "The bandwidth for telephony is generally defined to be from 300--3400 Hz. This bandwidth restriction has a noticeable effect on speech quality. We present an algorithm which recovers the missing highband parts from telephone speech. We describe an MMSE estimator using hard/soft-classification to create the missing highband spectrum envelope. The classification is motivated by acoustic phonetics: voiced vowels and consonants, and unvoiced phonemes demonstrate different characteristic spectra. The classification also captures gender differences. A hard classification on phoneme characteristic parameters, such as a voicing degree and a pitch lag, reduces the MMSE of the highband spectrum envelope estimates. An estimator using HMM-based soft-classification can further bring down the estimated highband spectrum distortion by taking the time evolution of the spectra into consideration. Objective measures (mean log-spectrum distortion) and spectrograms confirm the improvement noted in informal subjective tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-721"
  },
  "korkmazsky04b_interspeech": {
   "authors": [
    [
     "Filipp",
     "Korkmazsky"
    ],
    [
     "Murat",
     "Deviren"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Hidden factor dynamic Bayesian networks for speech recognition",
   "original": "i04_2801",
   "page_count": 4,
   "order": 723,
   "p1": "2801",
   "pn": "2804",
   "abstract": [
    "This paper presents a novel approach to modeling speech data by Dynamic Bayesian Networks. Instead of defining a specific set of factors that affect speech signals the factors are modeled implicitly by speech data clustering. Different data clusters correspond to different subsets of the factor values. These subsets are represented by the corresponding factor states. The factor states along with the phone states represent 2 hidden layers in the Hidden Factor Dynamic Bayesian Network (HFDBN). In this study we proved that Hidden Factor Dynamic Bayesian Networks provide a better speech recognition performance than HMMs of equal complexity. Speech recognition experiments were conducted on the speech data recorded in a moving car and demonstrated advantage of using HFDBN over HMM for clean and noisy speech data recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-722"
  },
  "mao04_interspeech": {
   "authors": [
    [
     "Mark",
     "Mao"
    ],
    [
     "Vincent",
     "Vanhoucke"
    ]
   ],
   "title": "Design of compact acoustic models through clustering of tied-covariance Gaussians",
   "original": "i04_2805",
   "page_count": 4,
   "order": 724,
   "p1": "2805",
   "pn": "2808",
   "abstract": [
    "We propose a new approach for designing compact acoustic models particularly suited to large systems that combine multiple model sets to represent distinct acoustic conditions or languages. We show that Gaussians based on mixtures of inverse covariances (MIC) with shared parameters can be clustered using an efficient Lloyd algorithm. As a result, more compact acoustic models can be built by clustering Gaussians across tied mixtures. In addition, we show that the tied parameters of MIC models can be shared across acoustic models and languages, making it possible to build more efficient multi-model systems which take advantage of a common pool of clustered Gaussians.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-723"
  },
  "raut04_interspeech": {
   "authors": [
    [
     "Chandra Kant",
     "Raut"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Model composition by lagrange polynomial approximation for robust speech recognition in noisy environment",
   "original": "i04_2809",
   "page_count": 4,
   "order": 725,
   "p1": "2809",
   "pn": "2812",
   "abstract": [
    "This paper presents a technique for estimating HMM model parameters for noisy speech from given clean speech HMM and noise HMM. The model parameters are estimated by approximating the non-linear function governing the relationship between speech and noise, by a Lagrange polynomial, and thus enabling the distribution of corrupted speech parameters to have a closed form. The method is computationally efficient, and the experimental results showed significant improvement in recognition performance of noisy speech with this approach. Typically, word accuracy increased from 9.2% with clean model to 82.8% with the model composed by the proposed method as compared to 45.4% with the model composed by PMC Log-normal approximation, on an isolated word recognition task for exhibition hall noise added at 10 dB SNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-724"
  },
  "wu04c_interspeech": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Donglai",
     "Zhu"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "A study of minimum classification error training for segmental switching linear Gaussian hidden Markov models",
   "original": "i04_2813",
   "page_count": 4,
   "order": 726,
   "p1": "2813",
   "pn": "2816",
   "abstract": [
    "In our previous works, a Switching Linear Gaussian Hidden Markov Model (SLGHMM) and its segmental derivative, SSLGHMM, were proposed to cast the problem of modelling a noisy speech utterance by a well-designed dynamic Bayesian network. We presented parameter learning procedures for both models with maximum likelihood (ML) criterion. The effectiveness of such models was confirmed by evaluation experiments on Aurora2 database. In this paper, we present a study of minimum classification error (MCE) training for SSLGHMM and discuss its relation to our earlier proposals based on stochastic vector mapping. An important implementation issue of SSLGHMM, namely the specification of switching states for a given utterance, is also studied. New evaluation results on Aurora3 database show that MCE-trained SSLGHMMs achieve a relative error reduction of 21% over a baseline system based on ML-trained continuous density HMMs (CDHMMs).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-725"
  },
  "matsuda04_interspeech": {
   "authors": [
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Speech recognition system robust to noise and speaking styles",
   "original": "i04_2817",
   "page_count": 4,
   "order": 727,
   "p1": "2817",
   "pn": "2820",
   "abstract": [
    "It is difficult to recognize speech distorted by various factors, especially when an ASR system contains only a single acoustic model. One solution is to use multiple acoustic models, one model for each different condition. In this paper, we discuss a parallel decoding-based ASR system that is robust to the noise type, SNR, speaker gender and speaking style. Our system consists of two recognition channels based on MFCC and Differential MFCC features. Each channel has several acoustic models depending on SNR, speaker gender and speaking style, and each acoustic model is adapted by fast noise adaptation. From each channel, one hypothesis is selected based on its likelihood. The final recognition result is obtained by combining hypotheses. Experiments demonstrate that the system could achieve recognition accuracy in excess of 80% for the normal speaking style data at a SNR of 0dB. For hyper-articulated speech data, the recognition accuracy improved from about 10% to over 45% compared to a system without acoustic models for hyper-articulated speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-726"
  },
  "yoma04_interspeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Ivan",
     "Brito"
    ],
    [
     "Carlos",
     "Molina"
    ]
   ],
   "title": "The stochastic weighted viterbi algorithm: a frame work to compensate additive noise and low-bit rate coding distortion",
   "original": "i04_2821",
   "page_count": 4,
   "order": 728,
   "p1": "2821",
   "pn": "2824",
   "abstract": [
    "A solution to the problem of speech recognition with signals corrupted by additive noise and distorted by low-bit rate coders is presented in this paper. The additive noise and the coding distortion are cancelled according to the following scheme: firstly, the pdf of the clean coded-decoced speech is estimated with an additive noise model; second, the pdf of the clean uncoded signal is also estimated with a coding distortion model; and finally, the HMM is compensated by using the expected value of the observation pdf in the context of the stochastic weighted Viterbi (SWV) algorithm. The approach leads to reductions as high as 50% or 60% in word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-727"
  },
  "tomko04_interspeech": {
   "authors": [
    [
     "Stefanie",
     "Tomko"
    ],
    [
     "Roni",
     "Rosenfeld"
    ]
   ],
   "title": "Shaping spoken input in user-initiative systems",
   "original": "i04_2825",
   "page_count": 4,
   "order": 729,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "A spoken dialog system performs best when users speak within the grammar that the system understands. We conducted a simple study to investigate how easily users can be persuaded to speak to a system using a restricted, less-than-natural-language input style. In a Wizard-of-Oz setting, users of a spoken dialogue system for information access were given brief instructions to \"speak simply\" to the system. During their interactions, conversational or complex input was rejected by the system while simpler, \"just-the-facts\" input was accepted. We found that all users were able to adapt their language to successfully complete tasks, and participants' post-experiment comments showed that they were consistently mindful of the form of acceptable input. These results will support further investigation into more precise shaping of user input, leading to more effective and efficient human-machine speech interaction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-728"
  },
  "pavlovski04_interspeech": {
   "authors": [
    [
     "Christopher",
     "Pavlovski"
    ],
    [
     "Jennifer",
     "Lai"
    ],
    [
     "Stella",
     "Mitchell"
    ]
   ],
   "title": "Etiology of user experience with natural language speech",
   "original": "i04_2829",
   "page_count": 4,
   "order": 730,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "This paper outlines the results of commercial field trial of a multimodal (speech and GUI) platform using natural language technologies in a mobile environment. During the trial we compared the user experience of the multimodal interface to the user experience with a comparable application providing only unimodal (GUI) interaction. A key objective of the analysis following the trial was to understand the etiology of the difference in user experience with the two systems. Speech-only usage was used to complete tasks 71% of the time on average across all participants and tasks. The remaining 29% was distributed among GUI-only usage and multimodal usage. These results indicate that for input, the natural language speech was the major contributor to the increased quality of user experience.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-729"
  },
  "rayner04_interspeech": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Beth Ann",
     "Hockey"
    ]
   ],
   "title": "Side effect free dialogue management in a voice enabled procedure browser",
   "original": "i04_2833",
   "page_count": 4,
   "order": 731,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "We describe a general side-effect free dialogue management architecture suitable for command and control tasks, which extends the \"update semantics\" framework by including task as well as dialogue information in the information state. We show that this enables simple and elegant treatments of several dialogue management problems, including corrections, confirmations, querying of the environment, and regression testing. The architecture is discussed in the context of an implemented application, a voice enabled procedure browser for an astronautics domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-730"
  },
  "lane04_interspeech": {
   "authors": [
    [
     "Ian Richard",
     "Lane"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Shinichi",
     "Ueno"
    ]
   ],
   "title": "Example-based training of dialogue planning incorporating user and situation models",
   "original": "i04_2837",
   "page_count": 4,
   "order": 732,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "To provide a high level of usability, spoken dialogue systems must generate cooperative responses for a wide variety of users and situations. We introduce a dialogue planning scheme incorporating user and situation models making dialogue adaptation possible. Manually developing a set of dialogue rules to account for all possible model combinations, would be very difficult and obstruct system portability. To overcome this problem, we propose a novel example-based training scheme for dialogue planning, where example dialogues from a role-playing simulation are collected and used to train a dialogue planning scheme using a machine learning approach. The proposed scheme is evaluated on the Kyoto city voice portal, a multi-domain spoken dialogue system. Subjects participated in a role-playing simulation where they selected appropriate system responses at each dialogue turn based on a given scenario. Experimental results show that the system successfully trains the dialogue planner and provides reasonable system performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-731"
  },
  "fujie04_interspeech": {
   "authors": [
    [
     "Shinya",
     "Fujie"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Daizo",
     "Yagi"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ]
   ],
   "title": "Prosody based attitude recognition with feature selection and its application to spoken dialog system as para-linguistic information",
   "original": "i04_2841",
   "page_count": 4,
   "order": 733,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "In this paper, prosody-based attitude recognition and its application to a spoken dialog system are discussed. Para-linguistic information plays a important role in the human communication. We aimed to recognize the user's attitude by prosody, and apply it to a spoken dialog system as para-linguistic information. In order to find important features to recognize the attitude from automatically extracted features, we applied some feature selection methods. Experimental results show the stepwise method, a combination of the forward selection method and the backward selection method, achieved the best recognition rate and some important features effective for attitude recognition are revealed. Finally, the dialog system using the speaker's attitude as para-linguistic information is developed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-732"
  },
  "ollason04_interspeech": {
   "authors": [
    [
     "David",
     "Ollason"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Siddharth",
     "Bhatia"
    ],
    [
     "Dan",
     "Herron"
    ],
    [
     "Jackie",
     "Liu"
    ]
   ],
   "title": "MS connect: a fully featured auto-attendant: system design, implementation and performance",
   "original": "i04_2845",
   "page_count": 4,
   "order": 734,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "In this paper we describe our solutions to the problems faced in building the fully featured auto-attendant application that currently provides telephone access to approximately 50,000 Microsoft employees. The paper focuses on issues that the application developer can address directly. Some of the problems tackled in building the grammar include the handling of homonyms, several people with the same name, and data requiring name normalization. Our voice UI helped with this high perplexity task by providing multiple ways to specify the required employee, including spelling their email address or selecting them from a list of employees. Over 70% of the calls analyzed were successfully completed. The system was built using the Microsoft Speech Application SDK and runs on the Microsoft Speech Server 2004.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-733"
  },
  "haebumbach04b_interspeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Sven",
     "Peschke"
    ],
    [
     "Ernst",
     "Warsitz"
    ]
   ],
   "title": "Adaptive beamforming combined with particle filtering for acoustic source localization",
   "original": "i04_2849",
   "page_count": 4,
   "order": 735,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "While the main objective of adaptive Filter-and-Sum beamforming is to obtain an enhanced speech signal for subsequent processing like speech recognition, we show how speaker localization information can be derived from the filter coefficients. To increase localization accuracy, speaker tracking is performed by non-linear Bayesian state estimation, which is realized by sequential Monte Carlo methods. Improved acquisition and tracking performance was achieved even in highly reverberant environments, in comparison with both a Kalman Filter and a recently proposed Particle Filter operating on the output of a non-adaptive Delay-and-Sum beamformer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-734"
  },
  "kwon04c_interspeech": {
   "authors": [
    [
     "Hongseok",
     "Kwon"
    ],
    [
     "Siho",
     "Kim"
    ],
    [
     "Keunsung",
     "Bae"
    ]
   ],
   "title": "Time delay estimation using weighted CPSP function",
   "original": "i04_2853",
   "page_count": 4,
   "order": 736,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "A new TDE technique using the CPSP function weighed on each band is proposed. Observed and theoretical CPSP spectra are uniformly divided into several bands, and time delay having maximum cross correlation coefficient between observed and theoretical CPSP functions on each band is searched. Then the observed CPSP function is weighted depending on the phase linearity on each band. A weighted CPSP function is constructed by summing all the weighted CPSP functions on each band. Final time delay is determined as the time-lag that maximizes the weighted CPSP function. The simulation results show that the proposed method has slightly better performance than other methods in the noisy and reverberant environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-735"
  },
  "potamitis04_interspeech": {
   "authors": [
    [
     "Ilyas",
     "Potamitis"
    ],
    [
     "Panos",
     "Zervas"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "DOA estimation of speech signals using semi-blind source separation techniques",
   "original": "i04_2857",
   "page_count": 4,
   "order": 737,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "In this paper we investigate the application of complex independent component analysis (ICA) to the direction of arrival (DOA) estimation problem of wideband signals. The ICA based technique is semi-blind in the sense that the structure of the array is known to be uniform and linear (ULA). We show that when the array is ULA the mixing matrix is forced to have the structure imposed by the directivity vectors of the microphone array. ICA is applied to the spectral bins having the higher SNRs. The DOAs are derived from the histogram and clustering of the angle of arrivals of all high SNR spectral bins. The effectiveness of the approach is evaluated on speech signals and is com-pared against a variety of wideband DOA estimation tech-niques based on second order statistics. Finally, a toolbox for DOA estimation of wideband sources is made available at: http://slt.wcl.ee.upatras.gr/potamitis/DOA_ICA.zip\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-736"
  },
  "kim04v_interspeech": {
   "authors": [
    [
     "SangGyun",
     "Kim"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Blind separation of speech and sub-Gaussian signals in underdetermined case",
   "original": "i04_2861",
   "page_count": 4,
   "order": 738,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "Conventional blind source separation (BSS) algorithms are applicable when the number of sources equals to that of observations; however, they are inapplicable when the number of sources is larger than that of observations. Most underdetermined BSS algorithms have been developed based on an assumption that all sources have sparse distributions. These algorithms are applicable to separate speech signals with super-Gaussian distribution in the underdetermined case. However, they fail to separate the underdetermined mixtures of speech signals and sub-Gaussian signals. In this paper, a novel method for separating the underdetermined mixtures of sources with both super- and sub-Gaussian distributions is proposed. In the proposed method, underdetermined BSS problem is converted to conventional BSS problem by generating hidden observations so that the probability of estimated sources is maximized. Simulation results show that the proposed method can separate the underdetermined mixtures of speech signals and sub-Gaussian signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-737"
  },
  "jang04_interspeech": {
   "authors": [
    [
     "Gil-Jin",
     "Jang"
    ],
    [
     "Changkyu",
     "Choi"
    ],
    [
     "Yong-Beom",
     "Lee"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Adaptive cross-channel interference cancellation on blind signal separation outputs using source absence/presence detection and spectral subtraction",
   "original": "i04_2865",
   "page_count": 4,
   "order": 739,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "The performances of blind source separation (BSS) are still not satisfiable to apply to the real environments. The major obstacle may seem the finite filter length of the assumed mixing model and the nonlinear sensor noises. This paper presents a two-step speech enhancement method with stereo microphone inputs. The first is an ordinary frequency-domain BSS step, and the second is the removal of the remaining cross-channel interference by a spectral cancellation approach using a probabilistic source absence/presence detection technique. Our experimental results show good separation enhancement performances on the real recordings of speech and music signals compared to the conventional BSS methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-738"
  },
  "visser04_interspeech": {
   "authors": [
    [
     "Erik",
     "Visser"
    ],
    [
     "Kwokleung",
     "Chan"
    ],
    [
     "Stanley",
     "Kim"
    ],
    [
     "Te-Won",
     "Lee"
    ]
   ],
   "title": "A comparison of simultaneous 3-channel blind source separation to selective separation on channel pairs using 2-channel BSS",
   "original": "i04_2869",
   "page_count": 4,
   "order": 740,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "A number of real-life speech applications using BSS have been reported for two channel applications but only a few have been reported for multi-channel (more than 2 channels) applications. Moreover these mostly involve simulation studies or real-life separations in controlled settings. In this paper some practical problems of multichannel applications will be analyzed. A methodology is proposed to bypass the full fledged higher dimensional BSS problem by exploiting the relative spatial arrangement of microphones to achieve improved speech enhancement by selective 2-channel BSS. The concept is illustrated on a 3 source / 3 microphone setup in speech recognition experiments involving an acoustic scene with a human speaker and interferring noise sources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-739"
  },
  "lee04s_interspeech": {
   "authors": [
    [
     "Hyun-Bok",
     "Lee"
    ]
   ],
   "title": "Towards a harmonious coexistence of spoken and written language",
   "original": "i04_2873",
   "page_count": 4,
   "order": 741,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "Spoken and written languages, despite their different nature and function in human society, have often been in conflicts rather than in harmony for centuries. Despite the incessant efforts made by phoneticians, linguists, speech therapists and more recently speech technologists to describe the interrelations and relative merits of written and spoken languages, the written language still enjoys its dominant status as a more authoritative and reliable means of communication in many countries. More serious and combined research efforts are required in future on the part of phoneticians, linguists, speech scientists and technologists, and speech therapists in order to bring about an ideal and balanced interchange and intercourse of spoken and written languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-740"
  },
  "sugito04_interspeech": {
   "authors": [
    [
     "Miyoko",
     "Sugito"
    ]
   ],
   "title": "Towards a grammar of spoken language - prosody of ill-formed utterances and listener's understanding in discourse -",
   "original": "i04_2877",
   "page_count": 4,
   "order": 742,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "So-called ill-formed utterances abound in natural discourse. This paper describes how listeners can understand such utterances in a discourse, including speech hesitations and slips-of-the-tongue. Through analysis of durations of utterances and their succeeding pauses, and of intonation patterns, and also through listener understanding experiments using ill-formed utterances, the following results are obtained. 1) Listeners can understand and memorize speech contents by compensating for the ill-formedness, but pauses are indispensable for the processing. 2) Listeners judge appropriateness of each passage in utterances and pick up significantly important phrases, using intonational peculiarities observed in speech hesitation and truncation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-741"
  },
  "kawahara04f_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kazuya",
     "Shitaoka"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ]
   ],
   "title": "Automatic transformation of lecture transcription into document style using statistical framework",
   "original": "i04_2881",
   "page_count": 4,
   "order": 743,
   "p1": "2881",
   "pn": "2884",
   "abstract": [
    "This paper addresses automatic transformation from spoken style texts to written style texts. Exact transcriptions and speech recoginition results of live lectures include many spoken language expressions, and thus, are not suitable for documents and need to be edited. In this paper, we present a method of applying of the statistical approach used in machine translation to this postprocessing task. Specifically, we implement the correction of colloquial expressions, the delection of fillers, the insertion of periods, and the insertion of particles in an integrated manner. A preliminaly evaluation confirms that the statistical transformation framework works well and we achieved high recall and precision rate of period and particle insertion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-742"
  },
  "arora04_interspeech": {
   "authors": [
    [
     "Karunesh",
     "Arora"
    ],
    [
     "Sunita",
     "Arora"
    ],
    [
     "Kapil",
     "Verma"
    ],
    [
     "Shyam Sunder",
     "Agrawal"
    ]
   ],
   "title": "Automatic extraction of phonetically rich sentences from large text corpus of indian languages",
   "original": "i04_2885",
   "page_count": 4,
   "order": 744,
   "p1": "2885",
   "pn": "2888",
   "abstract": [
    "A set of phonetically rich sentences is a requirement for representing different speech units, to be used for developing Automatic Speech Recognition and Speech Synthesis Systems. Selecting such a set from a large text corpus without modifying the characteristics of the corpus is still a difficult task. A major concern in this process is to decide on what basis sentences must be chosen so that it covers all phonetic aspects of the language under study in a minimum possible size. This paper describes a simple process of automatically extracting such set of sentences from a large text corpus of a given Indian Language and also presents an algorithm for the process. The process discussed in this paper is language independent and works for most of the Indian Languages. The extent of success, in terms of phonetic richness of the sentences, achieved in the process is also discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-743"
  },
  "calzolari04_interspeech": {
   "authors": [
    [
     "Nicoletta",
     "Calzolari"
    ]
   ],
   "title": "European initiatives to promote cooperation between speech and text communities",
   "original": "i04_2889",
   "page_count": 4,
   "order": 745,
   "p1": "2889",
   "pn": "2892",
   "abstract": [
    "I touch a few initiatives recently promoted in Europe - in particular within ENABLER, ELRA, and ELSNET - with the aim of strengthening the cooperation between the communities of Spoken and Written language processing, and of promoting a unified view of our sector. After highlighting the strategic and infrastructural role of language resources, I reflect on how we should proceed so that the global requirements of the multilingual information society can be met by the two fields together. Recommendations are provided towards the design of general strategies and an overall coordination for the field of LRs as a whole, critical to satisfy some of the requirements of the multilingual information society. This may become the new vision for LRs in the next years. My objective is to show the importance of a global strategy in EU and world-wide to achieve more coherent and useful results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-744"
  },
  "takamaru04_interspeech": {
   "authors": [
    [
     "Keiichi",
     "Takamaru"
    ]
   ],
   "title": "Evaluation of a threshold for detecting local slower phrases in Japanese spontaneous conversational speech",
   "original": "i04_2969",
   "page_count": 4,
   "order": 746,
   "p1": "2969",
   "pn": "2972",
   "abstract": [
    "I have proposed a method for detecting local slower phrases in Japanese spontaneous conversational speech. A threshold is applied to phrase-averaged mora duration in this method. It is considered that relative variation of time sequence of phrase-averaged mora duration should be taken into account for detecting slower phrases correctly. In this paper, preliminary experiments are carried out to obtain optimal threshold. Then the optimal threshold is applied to Japanese conversational speech. I have confirmed that the threshold in consideration of relative variation of phrase-averaged mora duration has better result than the simple straight threshold.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-745"
  },
  "effendy04_interspeech": {
   "authors": [
    [
     "Nazrul",
     "Effendy"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Patavee",
     "Charnvivit"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ]
   ],
   "title": "Intonation recognition for indonesian speech based on fujisaki model",
   "original": "i04_2973",
   "page_count": 4,
   "order": 747,
   "p1": "2973",
   "pn": "2976",
   "abstract": [
    "In this paper, we proposed to use the Fujisaki parameter to distinguish between declarative and interrogative intonation in Indonesian speech. Four combinations of Fujisaki parameter were selected as the features to distinguish between declarative and interrogative intonation. The first combination is only the amplitude of last accent command. The second combination consists of the amplitude of last accent command and the magnitude of last phrase command. The third combination consists of Fb, the amplitude of last accent command, and the magnitude of last phrase command. The fourth combination consists of Fb/100, the amplitude of last accent command, and the magnitude of last phrase command. The recognition rates using the neural network were 83.33 %, 90.00 %, 50.00 %, and 96.67 % for each combination. The highest recognition rate was achieved by using Fb/100, the last accent command amplitude and the last phrase command amplitude as its inputs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-746"
  },
  "zhang04k_interspeech": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Efficient tone classification of speaker independent continuous Chinese speech using anchoring based discriminating features",
   "original": "i04_2977",
   "page_count": 4,
   "order": 748,
   "p1": "2977",
   "pn": "2980",
   "abstract": [
    "Anchoring based discriminating features were proposed efficient for tone discrimination of Chinese continuous speech, and have been successfully applied before to tone classification of speaker dependent experiment. This paper presents its application to speaker independent tone classification experiments. Furthermore, we made detailed comparison experiments on the efficiencies of three groups of features: the left context dependent, the right context dependent anchoring F0 features, and the conventional F0 features. Experimental results showed that a combination of all three groups achieved a significant improvement of absolute 6.4% from 82.6% by the baseline system to 89.0%. When the three groups of features are used individually, both groups of the anchoring features led to better results than the conventional features, and the left context dependent anchoring features led to the highest performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-747"
  },
  "watanabe04c_interspeech": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ],
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Clause types and filed pauses in Japanese spontaneous monologues",
   "original": "i04_2981",
   "page_count": 4,
   "order": 749,
   "p1": "2981",
   "pn": "2984",
   "abstract": [
    "Hesitations are prevalent in spontaneous speech and believed to be relevant to on-line speech planning. We tested the complexity hypothesis that speakers are more likely to need to suspend speaking, the more complex the constituent, by examining ratios of filled pauses (fillers) at clause and case boundaries with following constituents of different degrees of complexity, using \"the Corpus of spontaneous Japanese\". The filler ratios were constantly higher as the following constituents were more complex, supporting the hypothesis. The result indicates that fillers can be clues about complexity of the upcoming constituents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-748"
  },
  "yabuta04_interspeech": {
   "authors": [
    [
     "Yohei",
     "Yabuta"
    ],
    [
     "Yasuhiro",
     "Katagiri"
    ],
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Yugo",
     "Takeuchi"
    ]
   ],
   "title": "Effect of voice prosody on the decision making process in human-computer interaction",
   "original": "i04_2985",
   "page_count": 4,
   "order": 750,
   "p1": "2985",
   "pn": "2988",
   "abstract": [
    "This paper examines whether humans perceive additional meaning in computer utterances with different voice prosody and change their decisions based on such interaction if the prosodic differences carry informational significance. We conduct a route selection experiment in which participants were asked to find a route to a goal in a 3-D maze generated by computer graphics. The maze system occasionally outputs a confirmation in response to the participant's choice of a route at a junction. The prosodic characteristics of the confirmation utterances reflect whether the route selected is right for reaching the goal in a normal alley or wrong, leading to a dead end. In this experiment, participants are able to recognize the prosodic meaning and successfully make a decision that leads to a route through the maze. This result suggests that participants can perceive the differences in pitch range of the confirmation voice, and used the pitch range in reselecting their route. Furthermore, wide pitch range confirmation voice indicates a dead end resulted in a higher reselection ratio. This suggests that participants gave a negative interpretation to wider pitch range responses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-749"
  },
  "suzuki04d_interspeech": {
   "authors": [
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Yasuhiro",
     "Katagiri"
    ]
   ],
   "title": "Alignment of human prosodic patterns for spoken dialogue systems",
   "original": "i04_2989",
   "page_count": 4,
   "order": 751,
   "p1": "2989",
   "pn": "2992",
   "abstract": [
    "An adaptive speech recognizer is a key function in the design of a robust spoken dialogue system. Our research focuses on the human tendency of prosodic alignment to one's conversational partners. A spoken dialogue system might be able to exploit this human tendency to implicitly influence people to manage their speech at the prosodic level in order to accommodate its recognition capabilities. Consequently, this would decrease recognition errors. Prosodic alignment in human-computer interaction has been studied as part of the problems of personality alignment in the context of animated conversational characters. The present study examines human prosodic alignment tendency at more micro level, and explores whether people's speech amplitude and pause length align to those of computer generated voices within a dialogue exchange. We found that people exhibit spontaneous short-term alignment of speech prosody to the slight prosodic changes in a computer's voice within a session, even without the help of animated conversational characters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-750"
  },
  "kiriyama04_interspeech": {
   "authors": [
    [
     "Shinya",
     "Kiriyama"
    ],
    [
     "Shigeyoshi",
     "Kitazawa"
    ]
   ],
   "title": "Evaluation of a prosodic labeling system utilizing linguistic information",
   "original": "i04_2993",
   "page_count": 4,
   "order": 752,
   "p1": "2993",
   "pn": "2996",
   "abstract": [
    "A prosodic labeling support system has been developed and evaluated. Large-scale prosodic databases are strongly desired for years, however, the construction of databases depend on hand labeling, because of diversity of prosody. We aim at not automating the whole labeling process, but making the hand labeling work more efficient by providing labelers with appropriate support information. A method to generate J-ToBI labels utilizing linguistic information had been already proposed. We have developed a J-ToBI labeling system using initial labels generated by the proposed method, and conducted labeling experiments using the system. Records of the labeling work in the experiments proved that time required to do the labeling work was reduced to half by using the system with the initial labels. Subjective evaluation results also verified the validity of the proposed method to support the labeling work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-751"
  },
  "blodgett04_interspeech": {
   "authors": [
    [
     "Allison",
     "Blodgett"
    ]
   ],
   "title": "Functions of intonation boundaries during spoken language comprehension in English",
   "original": "i04_2997",
   "page_count": 4,
   "order": 753,
   "p1": "2997",
   "pn": "3000",
   "abstract": [
    "Two experiments investigated the interaction of intonation boundaries and verb bias during spoken language comprehension in English. An online cross-modal naming task proved to be sensitive to the interaction of prosodic, syntactic, and semantic representations during resolution of a temporary syntactic closure ambiguity (e.g., Whenever the lady checks the room...) The results supported previous claims that intonation boundaries trigger semantic wrap-up [1] and provided new evidence that they trigger syntactic wrap-up as well. When an intonation boundary occurred at a transitive-bias verb, wrap-up resulted in a transitive interpretation and an intransitive structure. Resolution of the conflict depended on the prosodic representation associated with the structurally ambiguous noun phrase and its predictability as a direct object. While intonation boundary location can determine the initial syntactic structure for this closure ambiguity regardless of verb bias, disambiguation for transitive-bias verbs seems to depend in part on other prosodic and lexical factors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-752"
  },
  "kuhne04_interspeech": {
   "authors": [
    [
     "Marco",
     "Kühne"
    ],
    [
     "Matthias",
     "Wolff"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Voice activation using prosodic features",
   "original": "i04_3001",
   "page_count": 4,
   "order": 754,
   "p1": "3001",
   "pn": "3004",
   "abstract": [
    "In this paper we propose a voice activation method based on prosodic keyword verification. In current voice activation systems features like the fundamental frequency contour have not been considered so far. Normally a continuous listening word spotter is used to detect a certain predefined keyword. We conducted an experiment which shows that people emphasize this keyword when they address a recognizer. To capture the prosodic information we trained an HMM on the fundamental frequency and energy contour of the keyword. The prosodic model is used to verify the keyword hypotheses of a phonetic recognizer. We investigated the performance of the prosodic model to distinguish between the same keyword spoken in command and non-command phrases. The introduction of the prosodic information significantly reduced the false alarm rate whereas the detection rate was only slightly degraded.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-753"
  },
  "kim04w_interspeech": {
   "authors": [
    [
     "Sahyang",
     "Kim"
    ]
   ],
   "title": "The role of prosodic cues in word segmentation of Korean",
   "original": "i04_3005",
   "page_count": 4,
   "order": 755,
   "p1": "3005",
   "pn": "3008",
   "abstract": [
    "The current study investigates the degree to which various prosodic cues at the boundaries of a prosodic phrase in Korean (Accentual Phrase) contributed to word segmentation. Since most phonological words in Korean are produced as one AP, it was hypothesized that the detection of acoustic cues at AP boundaries would facilitate word segmentation. The prosodic characteristics of Korean APs include initial strengthening at the beginning of the phrase and pitch rise and final lengthening at the end. A perception experiment revealed that the cues that conform to the above-mentioned prosodic characteristics of Korean facilitated listeners' word segmentation. Results also showed that duration and amplitude cues were more helpful in segmentation than pitch. Further, the results showed that a pitch cue that did not conform to the Korean AP interfered with segmentation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-754"
  },
  "jun04_interspeech": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ]
   ],
   "title": "Default phrasing and attachment preference in Korean",
   "original": "i04_3009",
   "page_count": 4,
   "order": 756,
   "p1": "3009",
   "pn": "3012",
   "abstract": [
    "This paper tests the validity of the Implicit Prosody Hypothesis (IPH) (Fodor 1998, 2002) based on production and perception experiments on Korean data. IPH states that attachment of a relative clause (RC) in a sentence with a complex noun phrase is influenced by a default prosodic contour of the structure projected in silent reading. It predicts that speakers of a language who prefer high attachment would produce a prosodic break between the RC and the adjacent noun phrase. Results show that Koreans prefer high attachment of an RC, but they do not produce a larger prosodic break between the RC and the adjacent noun phrase. Instead, the most common default phrasing is to produce each word in the same prosodic unit, an Accentual Phrase. Though this does not support IPH, the perception data show some sensitivity to prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-755"
  },
  "borys04_interspeech": {
   "authors": [
    [
     "Sarah",
     "Borys"
    ],
    [
     "Aaron",
     "Cohen"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "Modeling and recognition of phonetic and prosodic factors for improvements to acoustic speech recognition models",
   "original": "i04_3013",
   "page_count": 4,
   "order": 757,
   "p1": "3013",
   "pn": "3016",
   "abstract": [
    "This paper examines the usefulness of including prosodic and phonetic context information in the phoneme model of a speech recognizer. This is done by creating a series of prosodic and phonetic models and then comparing the mutual information between the observations and each possible context variable. Prosodic variables show improvement less often than phone context variables, however, prosodic variables generally show a larger increase in mutual information. A recognizer with allophones defined using the maximum mutual information prosodic and phonetic variables outperforms a recognizer with allophones defined exclusively using phonetic variables.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-756"
  },
  "kong04_interspeech": {
   "authors": [
    [
     "Eunjong",
     "Kong"
    ]
   ],
   "title": "The role of pitch range variation in the discourse structure and intonation structure of Korean",
   "original": "i04_3017",
   "page_count": 4,
   "order": 758,
   "p1": "3017",
   "pn": "3020",
   "abstract": [
    "This study explores pitch range variation in Korean spontaneous narratives and read transcripts of the same narratives. There are two research goals. One is to examine whether pitch range variation helps mark discourse segment boundaries and signal the hierarchy of discourse segment purposes. Another is to see whether categorical differences in pitch range encode the contrast between the two intonationally marked units in the Korean prosodic hierarchy. The narratives were prosodically annotated for Accentual and Intonational Phrase boundaries and the F0 maximum was measured in each Accentual Phrase. Comparing F0 maxima across adjacent phrases shows that pitch range is reset at discourse segment boundaries in spontaneous speech, and also that size of the pitch range reset reflects the hierarchy of discourse segment levels. By contrast, there is no systematic difference in pitch range resetting values between higher Intonational Phrases as compared to lower Accentual Phrase boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-757"
  },
  "takagi04_interspeech": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Dependency analysis of read Japanese sentences using pause and F0 information: a speaker independent case",
   "original": "i04_3021",
   "page_count": 4,
   "order": 759,
   "p1": "3021",
   "pn": "3024",
   "abstract": [
    "This paper deals with the problem of exploiting prosodic information in syntactic analysis of sentences. Duration of pauses at phrase boundaries and relative F0 contour features have been found to be effective for parsing in speaker-dependent case. In this paper, effectiveness of pause and F0 information was examined in a speaker-independent manner by using prosodic features extracted from the spoken version of sentences of 44 speakers. By simplifying the estimation of both pause and F0 contour models, better performance was obtained. Linear combination of pause and F0 information gave significant improvements in parsing accuracy. It was also shown that the effectiveness of pause information was larger when pause models were estimated separately for zero-duration and non-zero-duration pauses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-758"
  },
  "speer04_interspeech": {
   "authors": [
    [
     "Shari",
     "Speer"
    ],
    [
     "Soyoung",
     "Kang"
    ]
   ],
   "title": "Effects of prosodic boundaries on ambiguous syntactic clause boundaries in Japanese",
   "original": "i04_3025",
   "page_count": 4,
   "order": 760,
   "p1": "3025",
   "pn": "3028",
   "abstract": [
    "We report two experiments investigating the effects of prosodic boundaries on resolving ambiguous syntactic structures in Japanese. The head-final, pro-drop nature of this language generates abundant syntactic attachment ambiguity for sentences containing relative clauses. Syntactic attachment preferences for two sentence types, with differing head nouns modified by relative clauses, were assessed in a written study. Results indicated that readers retrieved one clear meaning for one type whereas the other type remained generally ambiguous. A spoken sentence comprehension study was conducted using these sentences, each produced with two prosodic structures. Results demonstrated the crucial use of prosodic boundary information in determining syntactic clause boundary locations. Most importantly, prosodic boundaries affected the way listeners posited empty pronouns for the subject of main or subordinate predicates. These results demonstrated the fundamental importance of prosodic phrasal structure to the assignment of syntactic constituency, particularly in the case of a head-final, pro-drop language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-759"
  },
  "nagasaki04_interspeech": {
   "authors": [
    [
     "Yasuko",
     "Nagasaki"
    ],
    [
     "Takanori",
     "Komatsu"
    ]
   ],
   "title": "The superior effectivenes of the F0 range for identifying the context from sounds without phonemes",
   "original": "i04_3029",
   "page_count": 4,
   "order": 761,
   "p1": "3029",
   "pn": "3032",
   "abstract": [
    "This study concerned the relationship between the prosodic feature, especially F0, and the listeners' identification of contexts. It became obvious that duration and F0 range are the important cue to guess speakers' emotional state or contexts. However, as the stimuli used there were designed to keep F0 average constant, we could not see the effect of big F0 range and high F0 component separately. To clarify this problem, we presented 400 triangle waves (4duration x 10 beginning F0 x 10 ending F0) as stimuli, so that we can analyze by F0 range, F0 average, F0 maximum, and F0 minimum. 20 university students listened to the stimuli and asked if they perceived the sounds as \"disagreement,\" \"hesitation,\" or \"agreement.\" While F0 range made the largest effectiveness to the responses, neither F0 average, F0 maximum nor F0 minimum did not show any influence on identification of contexts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-760"
  },
  "li04j_interspeech": {
   "authors": [
    [
     "Tan",
     "Li"
    ],
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Thanate",
     "Khaorapapong"
    ]
   ],
   "title": "A study of tone classification for continuous Thai speech recognition",
   "original": "i04_3033",
   "page_count": 4,
   "order": 762,
   "p1": "3033",
   "pn": "3036",
   "abstract": [
    "This paper presents a study of tone classification for continuous Thai speech recognition. A modified auto-correlation algorithm was implemented with pitch detection, and the tone classifier utilized 3-layer feed-forward neural network with back-propagation. The best performance configuration of tone features was obtained with semitone scaling and mean-normalization producing a classification accuracy of 72.21%. Also, after considering the effects of final consonants, the average performance of the tone classifier improved to 77.13%. Experimental results showed that the pitch value of a tone with no final consonants has more variation than one with final consonants. Also the classification for tones with voiced final consonant gave better performance than tones with unvoiced final consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-761"
  },
  "kim04x_interspeech": {
   "authors": [
    [
     "Key-Seop",
     "Kim"
    ],
    [
     "Un",
     "Lim"
    ],
    [
     "Dong-Il",
     "Shin"
    ]
   ],
   "title": "An acoustic-analytic role for the deviation between the scansion and reading of poems",
   "original": "i04_3037",
   "page_count": 4,
   "order": 763,
   "p1": "3037",
   "pn": "3040",
   "abstract": [
    "This research aimed 1) to suggest how to make scansions of lines in fixed meter based on prosodic and metrical ground, 2) to classify the problematic feet of cadence very similar to foot types in casual speech, and 3) to clarify the relation of scansion to acoustic analysis. Lastly, it tried to establish the necessity and clarify what benefits of acoustic analysis will provide for readers of poems about problematic scansions. For the study, seemingly problemtic lines were sporadically chosen from 12 poets. And data of readings by poets themselves in the tape titled THE POET SPEAKS were acoustically analyzed. All superficially unmetrical mismatches by lines of cadence between the results of scansions and acoustic scalar values were disclosed and discussed. Among acoustic scalar values of stress intensity, foot duration, and pitch height, the first proved to be the best guide and play a greater role in scansion and reading of problematic lines in fixed meter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-762"
  },
  "ohsuga04_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Ohsuga"
    ],
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Estimating syntactic structure from prosodic features in Japanese speech",
   "original": "i04_3041",
   "page_count": 4,
   "order": 764,
   "p1": "3041",
   "pn": "3044",
   "abstract": [
    "In this study, we introduce a method of estimating the syntactic tree structure of Japanese speech on the basis of the F0 contour and the time duration. We introduce a method of estimating the syntactic structure including the following phrase by using the local prosodic features of the first and final part of the leading phrase. This method involves discriminant analysis which is statistical method based on a large amount of training data. We applied the method to the ATR 503 speech database, and performed discrimination experiments. The results indicated an estimation accuracy of 84% for the branching judgment of each sequence of three leaves. In addition, the accuracy of discrimination saturated when using only the features up to the head part of the second phrase. We consider this result to be fairly good for the difficult task of estimating a syntactic structure that includes a future part on the basis of using only local prosodic features in the past, and also consider prosodic information to be very effective in real-time communication with speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-763"
  },
  "komatsu04_interspeech": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Tsutomu",
     "Sugawara"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Perceptual discrimination of prosodic types and their preliminary acoustic analysis",
   "original": "i04_3045",
   "page_count": 4,
   "order": 765,
   "p1": "3045",
   "pn": "3048",
   "abstract": [
    "A perceptual discrimination test was conducted to investigate whether humans can discriminate prosodic types solely based on suprasegmental acoustic cues. Excerpts from Chinese, English, Spanish, and Japanese, differing in lexical accent types and rhythm types, were used. From these excerpts, \"source\" signals of the source-filter model, differing in F0, intensity, and HNR, were created and used in a perceptual experiment. In general, the results indicated that humans can discriminate these prosodic types and that the discrimination is easier if more acoustic information is available. Further, the results showed that languages with similar rhythm types are difficult to discriminate (i.e., Chinese-English, English-Spanish, and Spanish-Japanese). As to accent types, tonal/non-tonal contrast was easy to detect. We also conducted a preliminary acoustic analysis of the experimental stimuli and found that quick F0 fluctuations in Chinese contribute to the perceptual discrimination of tonal/non-tonal accents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-764"
  },
  "lhour04_interspeech": {
   "authors": [
    [
     "Johann",
     "L'Hour"
    ],
    [
     "Olivier",
     "Boeffard"
    ],
    [
     "Jacques",
     "Siroux"
    ],
    [
     "Laurent",
     "Miclet"
    ],
    [
     "Francis",
     "Charpentier"
    ],
    [
     "Thierry",
     "Moudenc"
    ]
   ],
   "title": "DORIS, a multiagent/IP platform for multimodal dialogue applications",
   "original": "i04_3049",
   "page_count": 4,
   "order": 766,
   "p1": "3049",
   "pn": "3052",
   "abstract": [
    "This article presents an effort to define a multimodal Agent-based dialogue platform cooperating with Internet technologies. We propose an open architecture integrating voice based and graphical user interfaces. We focus on third-party integration of speech technologies using the MRCP Internet protocol and streaming solutions over IP flows. This work is highlighted by a demonstrator, GEORAL, for the tourist information retrieval domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-765"
  },
  "chen04h_interspeech": {
   "authors": [
    [
     "Yu",
     "Chen"
    ]
   ],
   "title": "EVITA-RAD: an extensible enterprise voice porTAI - rapid application development tool",
   "original": "i04_3053",
   "page_count": 4,
   "order": 767,
   "p1": "3053",
   "pn": "3056",
   "abstract": [
    "EVITA-RAD (Extensible enterprise VoIce porTAl - Rapid Application Development) is a web-based voice application development tool suite for designing and creating voice applications in a componentized and extensible framework. We explain the motivations and the tool design process, then describe and evaluate the system. Developing Voice User Interface (VUI) applications is a significant endeavor. Free online development tools from several voice solutions providers allow users to test and run VoiceXML applications on the provider's telephony networks, but are targeted for dedicated developers who understand the complexities of VoiceXML, speech recognition, and dialogue design. The complexity of speech technology and lack of intuitive and easy to use tools present hurdles for acceptance of VUI by developers, and consequently, wider adoption across the industry. Our goal is to to bridge this gap by providing an intuitive, robust, and extensible development tool suite that lower the barrier of entry, and promotes wide-spread adoption of voice applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-766"
  },
  "dharo04_interspeech": {
   "authors": [
    [
     "Luis F.",
     "D'Haro"
    ],
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "José Manuel",
     "Pardo"
    ]
   ],
   "title": "Strategies to reduce design time in multimodal/multilingual dialog applications",
   "original": "i04_3057",
   "page_count": 4,
   "order": 768,
   "p1": "3057",
   "pn": "3060",
   "abstract": [
    "In this paper, we present a complete platform for the semiautomatic generation of human-machine dialog systems, that using as input a description of the database of the service, a flow model with the different states of the final application and a guided interaction step by step with the designer's intervention, generates dialogs to access the service data in different languages and two modalities, speech and web, simultaneously. We describe in detail several strategies that have been followed to reduce the time needed to do the design using the mentioned information. We also address important issues in dialog applications as mixed initiative and overanswering dialogs, confirmation handling and how to provide the user long lists of information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-767"
  },
  "aist04_interspeech": {
   "authors": [
    [
     "Gregory",
     "Aist"
    ]
   ],
   "title": "Three-way system-user-expert interactions help you expand the capabilities of an existing spoken dialogue system",
   "original": "i04_3061",
   "page_count": 4,
   "order": 769,
   "p1": "3061",
   "pn": "3064",
   "abstract": [
    "Data collection and use is absolutely necessary to contemporary spoken dialogue system research. Yet much of it is done either at the beginning of a project with human-human conversation for a particular task, or done with a \"frozen\" system as an evaluation technique. As system development proceeds, system-user dialogue gets further and further apart from the original human-human dialogues due to the inevitable (and often desired) adaptation of the task to the strengths and limitations of the computer conversational partner. At the time of any particular evaluation, only the features present in the dialogue system will be evaluated. This may reveal which areas need further work, but sheds precious little light on how to expand the system to carry out the necessary novel sub-dialogues. This paper describes a new methodology to expand the capabilities of an existing dialogue system. The central technique uses three-way dialogues between a system, user, and expert as successive approximations of what people do. We describe three substantive uses of the methodology, in various systems: an intelligent tutoring system for children's oral reading; a spoken interface to a robotic helicopter simulation; and, an intelligent procedure assistant for astronauts. In each case, three-way dialogues allowed us to determine how the dialogue system should behave in the new parts of the dialogue under study, and then to add appropriate new functionality to the dialogue system itself. The three-way dialogue methodology is a powerful new technique for spoken dialogue system development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-768"
  },
  "fabbrizio04_interspeech": {
   "authors": [
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Charles",
     "Lewis"
    ]
   ],
   "title": "Florence: a dialogue manager framework for spoken dialogue systems",
   "original": "i04_3065",
   "page_count": 4,
   "order": 770,
   "p1": "3065",
   "pn": "3068",
   "abstract": [
    "Recent advances in speech and language technology have made spoken dialogue systems mainstream in many industries. They allow customers to engage in natural speech interactions with machines instead of being compelled to navigate menus of options with touch tones inputs. VoiceXML was a major milestone for the process of using automated speech applications to expose business portals to ubiquitous telephone access. By the introduction of a uniform and universally accepted client-server browser model, the VoiceXML programming model greatly simplified previously dominant proprietary computer telephony interfaces. However, natural language spoken dialogue systems entail more complex interactions with the user which, depending upon the application domain, may require computational models that are difficult to express directly in VoiceXML. This paper describes Florence, a dialogue manager with a more general approach that uses an extensible and flexible framework to combine interchangeable and interoperable dialogue strategies as appropriate to the task. Florence's declarative XMLbased language facilitates the development of natural language applications and allows the dialogue author to encapsulate and reuse different algorithms between applications. Moreover, it addresses large-scale natural language issues related to enterprise backend access, logging, distributed deployment, and fail-over support. These issues must be addressed in a modern, industrial-strength application server environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-769"
  },
  "kawahara04g_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Recent progress of open-source LVCSR engine julius and Japanese model repository",
   "original": "i04_3069",
   "page_count": 4,
   "order": 771,
   "p1": "3069",
   "pn": "3072",
   "abstract": [
    "Continuous Speech Recognition Consortium (CSRC) was founded for further enhancement of Japanese Dictation Toolkit that had been developed by the support of a Japanese agency. Overview of its product software is reported in this paper. The open-source LVCSR (large vocabulary continuous speech recognition) engine Julius has been improved both in performance and functionality, and it is also ported to Microsoft Windows in compliance with SAPI (Speech API). The software is now used for not a few languages and plenty of applications. For plug-and-play speech recognition in various applications, we have also compiled a repository of acoustic and language models for Japanese. Especially, the acoustic model set realizes wider coverage of user generations and speech-input environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-770"
  },
  "murao04_interspeech": {
   "authors": [
    [
     "Hiroya",
     "Murao"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Yukiko",
     "Yamaguchi"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Example-based spoken dialogue system with online example augmentation",
   "original": "i04_3073",
   "page_count": 4,
   "order": 772,
   "p1": "3073",
   "pn": "3076",
   "abstract": [
    "In this paper, we propose a new method to expand an example-based spoken dialogue system to handle context dependent utterances. The dialogue system refers to the dialogue examples to find an example that is suitable to promote dialogue. Here, the dialogue contexts are expressed in the form of dialogue slots. By constructing dialogue examples with the text of utterances and the dialogue slots, the system handle context dependent dialogue. And we also propose a new framework of spoken dialogue, named 'GROW architecture' that consists of the dialogue system and a Wizard-of-OZ (WOZ) system. By using the WOZ system to add dialogue examples via network, it becomes efficient to augment dialogue examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-771"
  },
  "buhler04_interspeech": {
   "authors": [
    [
     "Dirk",
     "Bühler"
    ]
   ],
   "title": "Enhancing existing form-based dialogue managers with reasoning capabilities",
   "original": "i04_3077",
   "page_count": 4,
   "order": 773,
   "p1": "3077",
   "pn": "3080",
   "abstract": [
    "In this paper we present an approach to complement speech dialogue applications written in VoiceXML with reasoning capabilities simplifying the integration of domain knowledge into the traditionally coded dialogue flow. One of the characteristic enhancements of our approach is the use of a dedicated reasoning component for processing the knowledge items acquired during the dialogue. The approach is also novel in that, instead of building on top of a specialized research dialogue architecture, an industry-standard dialogue description language has been chosen as the basis. This enables enhancing existing voice application rather than starting from scratch. The paper describes a first implementation of the proposed interface between the traditional form-based dialogue manager and the reasoning module and includes the analysis of a sample dialogue conducted with a prototype version of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-772"
  },
  "turunen04b_interspeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Esa-Pekka",
     "Salonen"
    ],
    [
     "Mikko",
     "Hartikainen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ]
   ],
   "title": "Robust and adaptive architecture for multilingual spoken dialogue systems",
   "original": "i04_3081",
   "page_count": 4,
   "order": 774,
   "p1": "3081",
   "pn": "3084",
   "abstract": [
    "We present how robustness and adaptivity can be supported by the spoken dialogue system architecture. AthosMail is a multilingual spoken dialogue system for e-mail domain. It is being developed in the EU-funded DUMAS project. It has flexible system architecture supporting multiple components for input interpretation, dialogue management and output generation. In addition to language differences, these components have great variation in their approaches for spoken interaction. For example, the dialogue management components implement different dialogue control models. The system architecture makes the interaction robust and adaptive by utilizing different approaches for spoken interaction in a single application by selecting suitable components at runtime.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-773"
  },
  "filipe04_interspeech": {
   "authors": [
    [
     "Porfirio",
     "Filipe"
    ],
    [
     "Nuno",
     "Mamede"
    ]
   ],
   "title": "Towards ubiquitous task management",
   "original": "i04_3085",
   "page_count": 4,
   "order": 775,
   "p1": "3085",
   "pn": "3088",
   "abstract": [
    "In the near future people will be surrounded by intelligent devices embedded in everyday objects where the knowledge and understanding of device attributes and capabilities will be a key enabler. This paper describes the current state of our research in designing distributed knowledge based devices as a solution to adapt spoken dialogue systems within ambient intelligence. In this context a spoken dialogue system is a computational entity that allows universal access to ambient intelligence for anyone, anywhere, at anytime to use any device through any media. Our aim is to build knowledge-based devices to enable dynamic adaptation of the components integrated in the dialogue system architecture. An example focusing household appliances is depicted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2004-774"
  }
 },
 "sessions": [
  {
   "title": "Plenary Talks",
   "papers": [
    "lee04_interspeech",
    "lee04b_interspeech",
    "vaissiere04_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Adaptation",
   "papers": [
    "balakrishnan04_interspeech",
    "raux04_interspeech",
    "huang04_interspeech",
    "mak04_interspeech",
    "chatzichrisafis04_interspeech",
    "visweswariah04_interspeech",
    "giuliani04_interspeech",
    "mu04_interspeech",
    "stemmer04_interspeech",
    "wang04_interspeech",
    "han04_interspeech",
    "mak04b_interspeech",
    "jeon04_interspeech",
    "wang04b_interspeech",
    "tanaka04_interspeech",
    "suzuki04_interspeech",
    "watanabe04_interspeech",
    "tsai04_interspeech",
    "kim04_interspeech",
    "kim04b_interspeech",
    "wolfel04_interspeech",
    "bocchieri04_interspeech",
    "fujii04_interspeech",
    "liu04_interspeech",
    "nagino04_interspeech"
   ]
  },
  {
   "title": "Spoken Language Identification, Translation and Retrieval I",
   "papers": [
    "gauvain04_interspeech",
    "huckvale04_interspeech",
    "levit04_interspeech",
    "crego04_interspeech",
    "schultz04_interspeech",
    "misu04_interspeech",
    "lee04c_interspeech",
    "itoh04_interspeech",
    "sedogbo04_interspeech",
    "cordoba04_interspeech",
    "hansen04_interspeech",
    "ferragne04_interspeech",
    "fung04_interspeech",
    "tur04_interspeech",
    "kurimo04_interspeech",
    "jiang04_interspeech",
    "moreau04_interspeech",
    "chung04_interspeech",
    "ajmera04_interspeech",
    "sanders04_interspeech",
    "amaral04_interspeech",
    "barkatdefradas04_interspeech",
    "wang04c_interspeech",
    "kuo04_interspeech",
    "masahiko04_interspeech",
    "zhang04_interspeech",
    "wong04_interspeech",
    "zhou04_interspeech",
    "blanchon04_interspeech"
   ]
  },
  {
   "title": "Linguistics, Phonology, and Phonetics",
   "papers": [
    "kim04c_interspeech",
    "noiray04_interspeech",
    "hunt04_interspeech",
    "tian04_interspeech",
    "cutler04_interspeech",
    "maskey04_interspeech",
    "broersma04_interspeech",
    "lee04d_interspeech",
    "murano04_interspeech",
    "dusan04_interspeech",
    "fang04_interspeech",
    "jian04_interspeech",
    "jian04b_interspeech",
    "kim04d_interspeech",
    "park04_interspeech",
    "son04_interspeech",
    "abresch04_interspeech",
    "kim04e_interspeech",
    "shigeyoshi04_interspeech",
    "tsukada04_interspeech",
    "cho04_interspeech",
    "cho04b_interspeech",
    "kaminskaia04_interspeech",
    "oh04_interspeech",
    "bunnell04_interspeech",
    "minematsu04_interspeech",
    "yoshida04_interspeech"
   ]
  },
  {
   "title": "Biomedical Applications of Speech Analysis",
   "papers": [
    "godinollorente04_interspeech",
    "jo04_interspeech",
    "fu04_interspeech",
    "mori04_interspeech",
    "kawahara04_interspeech",
    "manfredi04_interspeech",
    "kubin04_interspeech",
    "choi04_interspeech"
   ]
  },
  {
   "title": "Robust Speech Recognition on AURORA",
   "papers": [
    "ji04_interspeech",
    "vanhamme04_interspeech",
    "vanhamme04b_interspeech",
    "hirsch04_interspeech",
    "zhang04b_interspeech",
    "myrvoll04_interspeech",
    "sasou04_interspeech",
    "wang04d_interspeech",
    "shannon04_interspeech",
    "muhammad04_interspeech",
    "segura04_interspeech",
    "yamada04_interspeech",
    "setiawan04_interspeech",
    "ding04_interspeech",
    "au04_interspeech",
    "lee04e_interspeech",
    "yeung04_interspeech",
    "tsai04b_interspeech"
   ]
  },
  {
   "title": "Spoken / Multimodal Dialogue System",
   "papers": [
    "fugen04_interspeech",
    "lee04f_interspeech",
    "oshikawa04_interspeech",
    "zitouni04_interspeech",
    "williams04_interspeech",
    "konashi04_interspeech",
    "ito04_interspeech",
    "hartikainen04_interspeech",
    "houck04_interspeech",
    "oneill04_interspeech",
    "sagawa04_interspeech",
    "yang04_interspeech",
    "yang04b_interspeech",
    "wolf04_interspeech",
    "douglas04_interspeech",
    "edlund04_interspeech",
    "weng04_interspeech",
    "hanrieder04_interspeech",
    "stuttle04_interspeech",
    "pan04_interspeech",
    "harris04_interspeech",
    "hayashi04_interspeech",
    "fernandez04_interspeech",
    "lau04_interspeech",
    "poller04_interspeech",
    "iida04_interspeech",
    "toptsis04_interspeech",
    "bernsen04_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Search",
   "papers": [
    "novak04_interspeech",
    "zhang04c_interspeech",
    "hori04_interspeech",
    "yu04_interspeech",
    "smidl04_interspeech",
    "tendeau04_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue and Systems",
   "papers": [
    "ohtsuki04_interspeech",
    "rosset04_interspeech",
    "levow04_interspeech",
    "reichl04_interspeech",
    "seneff04_interspeech",
    "denecke04_interspeech"
   ]
  },
  {
   "title": "Speech Perception",
   "papers": [
    "shatzman04_interspeech",
    "sakamoto04_interspeech",
    "otake04_interspeech",
    "brahimi04_interspeech",
    "kato04_interspeech",
    "yoneyama04_interspeech"
   ]
  },
  {
   "title": "Multi-Lingual Speech-to-Speech Translation",
   "papers": [
    "waibel04_interspeech",
    "kikui04_interspeech",
    "ney04_interspeech",
    "lee04g_interspeech",
    "lee04h_interspeech",
    "lazzari04_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Large Vocabulary",
   "papers": [
    "fohr04_interspeech",
    "ramabhadran04_interspeech",
    "pylkkonen04_interspeech",
    "alumae04_interspeech",
    "han04b_interspeech",
    "wang04e_interspeech",
    "zheng04_interspeech",
    "ghadiyaram04_interspeech",
    "nouza04_interspeech",
    "siohan04_interspeech",
    "zhang04d_interspeech",
    "zhang04e_interspeech",
    "xu04_interspeech",
    "matton04_interspeech",
    "itaya04_interspeech",
    "grezl04_interspeech",
    "soong04_interspeech",
    "sako04_interspeech",
    "zhou04b_interspeech",
    "choi04b_interspeech",
    "ma04_interspeech",
    "hernandezabrego04_interspeech",
    "messaoudi04_interspeech",
    "shinozaki04_interspeech",
    "schultz04b_interspeech",
    "ohtsuki04b_interspeech",
    "doumpiotis04_interspeech",
    "shafran04_interspeech",
    "zhang04f_interspeech",
    "kim04f_interspeech",
    "stolcke04_interspeech",
    "venkataraman04_interspeech",
    "qian04_interspeech"
   ]
  },
  {
   "title": "Speech Science",
   "papers": [
    "due04_interspeech",
    "engwall04_interspeech",
    "cooper04_interspeech",
    "kawahara04b_interspeech",
    "kim04g_interspeech",
    "sachiyo04_interspeech",
    "green04_interspeech",
    "ciocca04_interspeech",
    "iseijaakkola04_interspeech",
    "alku04_interspeech",
    "hsu04_interspeech",
    "schnell04_interspeech",
    "yan04_interspeech",
    "kitamura04_interspeech",
    "tsuji04_interspeech",
    "motoki04_interspeech",
    "vijayalakshmi04_interspeech",
    "kacha04_interspeech",
    "jovicic04_interspeech",
    "marinaki04_interspeech"
   ]
  },
  {
   "title": "Novel Features in ASR",
   "papers": [
    "minami04_interspeech",
    "ou04_interspeech",
    "gharavian04_interspeech",
    "markov04_interspeech",
    "alsteris04_interspeech"
   ]
  },
  {
   "title": "Spoken and Natural Language Understanding",
   "papers": [
    "lieb04_interspeech",
    "fukumoto04_interspeech",
    "shriberg04_interspeech",
    "hasegawajohnson04_interspeech",
    "wang04f_interspeech",
    "chen04_interspeech",
    "wutiwiwatchai04_interspeech",
    "itoh04b_interspeech",
    "jung04_interspeech",
    "li04_interspeech",
    "eun04_interspeech",
    "horowitz04_interspeech",
    "huang04b_interspeech",
    "wu04_interspeech",
    "wang04g_interspeech",
    "trancoso04_interspeech",
    "kitade04_interspeech",
    "ohno04_interspeech",
    "minker04_interspeech",
    "lee04i_interspeech",
    "irie04_interspeech",
    "banerjee04_interspeech",
    "yildirim04_interspeech",
    "kawahara04c_interspeech",
    "park04b_interspeech",
    "suzuki04b_interspeech",
    "badino04_interspeech",
    "lee04j_interspeech"
   ]
  },
  {
   "title": "Speaker Segmentation and Clustering",
   "papers": [
    "valente04_interspeech",
    "jin04_interspeech",
    "lamel04_interspeech",
    "miro04_interspeech",
    "aronowitz04_interspeech",
    "raux04b_interspeech"
   ]
  },
  {
   "title": "Speech Processing in a Packet Network Environment",
   "papers": [
    "paliwal04_interspeech",
    "srinivasamurthy04_interspeech",
    "eriksson04_interspeech",
    "chou04_interspeech",
    "kabal04_interspeech",
    "kataoka04_interspeech",
    "kim04h_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling",
   "papers": [
    "jouvet04_interspeech",
    "yoshizawa04_interspeech",
    "fohr04b_interspeech",
    "silva04_interspeech",
    "stadermann04_interspeech",
    "knoblauch04_interspeech",
    "cho04c_interspeech",
    "olsen04_interspeech",
    "livescu04_interspeech",
    "kuo04b_interspeech",
    "zhu04_interspeech",
    "chan04_interspeech",
    "park04c_interspeech",
    "jitsuhiro04_interspeech",
    "somervuo04_interspeech",
    "macherey04_interspeech",
    "diehl04_interspeech",
    "jung04b_interspeech",
    "shozakai04_interspeech",
    "koo04_interspeech",
    "bridle04_interspeech"
   ]
  },
  {
   "title": "Prosody Modeling and Generation",
   "papers": [
    "li04b_interspeech",
    "sreenivasarao04_interspeech",
    "zheng04b_interspeech",
    "read04_interspeech",
    "escuderomancebo04_interspeech",
    "ni04_interspeech",
    "mouline04_interspeech",
    "aguero04_interspeech",
    "zervas04_interspeech",
    "xiong04_interspeech",
    "hu04_interspeech",
    "tooher04_interspeech",
    "dellwo04_interspeech",
    "gu04_interspeech",
    "dohen04_interspeech",
    "nemala04_interspeech",
    "krishna04_interspeech",
    "jokisch04_interspeech",
    "xydas04_interspeech",
    "vesela04_interspeech"
   ]
  },
  {
   "title": "Multi-Sensor ASR",
   "papers": [
    "graciarena04_interspeech",
    "demiroglu04_interspeech",
    "raub04_interspeech",
    "mochiki04_interspeech",
    "sagayama04_interspeech",
    "heck04_interspeech"
   ]
  },
  {
   "title": "Multi-Lingual Speech Processing",
   "papers": [
    "marino04_interspeech",
    "caballero04_interspeech",
    "oria04_interspeech",
    "romsdorfer04_interspeech",
    "badino04b_interspeech",
    "georgiou04_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "demiroglu04b_interspeech",
    "hu04b_interspeech",
    "zhang04g_interspeech",
    "srinivasan04_interspeech",
    "nakatani04_interspeech",
    "delcroix04_interspeech"
   ]
  },
  {
   "title": "Speech and Affect",
   "papers": [
    "campbell04_interspeech",
    "chateau04_interspeech",
    "lee04k_interspeech",
    "robinson04_interspeech",
    "li04c_interspeech",
    "nichasaide04_interspeech",
    "jiang04b_interspeech",
    "yu04b_interspeech",
    "fujisawa04_interspeech",
    "tao04_interspeech",
    "iwai04_interspeech",
    "cho04d_interspeech",
    "hirose04_interspeech"
   ]
  },
  {
   "title": "Speech Features",
   "papers": [
    "hegde04_interspeech",
    "yu04c_interspeech",
    "murthy04_interspeech",
    "kwon04_interspeech",
    "zhu04b_interspeech",
    "chen04b_interspeech",
    "sreenivas04_interspeech",
    "nakatoh04_interspeech",
    "ishizuka04_interspeech",
    "ishi04_interspeech",
    "demuynck04_interspeech",
    "athineos04_interspeech",
    "li04d_interspeech",
    "wang04h_interspeech",
    "ramirez04_interspeech",
    "park04d_interspeech",
    "kemp04_interspeech",
    "laskowski04_interspeech",
    "tsai04c_interspeech",
    "deng04_interspeech",
    "kubin04b_interspeech",
    "cho04e_interspeech"
   ]
  },
  {
   "title": "Language Modeling, Multimodal & Multilingual Speech Processing",
   "papers": [
    "chung04b_interspeech",
    "chen04c_interspeech",
    "lyu04_interspeech",
    "sluis04_interspeech",
    "gupta04_interspeech",
    "lee04l_interspeech",
    "tadj04_interspeech",
    "pedersen04_interspeech",
    "yamamoto04_interspeech",
    "moberg04_interspeech",
    "komatani04_interspeech",
    "sakti04_interspeech",
    "rashwan04_interspeech",
    "akita04_interspeech",
    "dolfing04_interspeech",
    "itou04_interspeech",
    "sethy04_interspeech"
   ]
  },
  {
   "title": "Detection and Classification in ASR",
   "papers": [
    "huang04c_interspeech",
    "zdansky04_interspeech",
    "wang04i_interspeech",
    "lahti04_interspeech",
    "sukittanon04_interspeech",
    "gangashetty04_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "tamiya04_interspeech",
    "quintanamorales04_interspeech",
    "doval04_interspeech",
    "deng04b_interspeech",
    "li04e_interspeech",
    "xu04b_interspeech",
    "laprie04_interspeech",
    "yan04b_interspeech",
    "you04_interspeech",
    "bozkurt04_interspeech",
    "bozkurt04b_interspeech",
    "shao04_interspeech",
    "yu04d_interspeech",
    "nishimoto04_interspeech",
    "ferencz04_interspeech",
    "flego04_interspeech",
    "reddy04_interspeech",
    "molla04_interspeech",
    "oh04b_interspeech",
    "zhang04h_interspeech",
    "navarromesa04_interspeech",
    "muralishankar04_interspeech",
    "girin04_interspeech",
    "soltani04_interspeech",
    "shin04_interspeech",
    "zheng04c_interspeech",
    "shiga04_interspeech"
   ]
  },
  {
   "title": "Speech Production",
   "papers": [
    "engwall04b_interspeech",
    "nakamura04_interspeech",
    "tanabe04_interspeech",
    "pulakka04_interspeech",
    "birkholz04_interspeech",
    "toda04_interspeech"
   ]
  },
  {
   "title": "Audio-Visual Speech Processing",
   "papers": [
    "kim04i_interspeech",
    "sekiyama04_interspeech",
    "krahmer04_interspeech",
    "hazan04_interspeech",
    "vroomen04_interspeech",
    "davis04_interspeech",
    "millar04_interspeech",
    "schwartz04_interspeech",
    "sams04_interspeech",
    "barbosa04_interspeech",
    "odisio04_interspeech",
    "fagel04_interspeech",
    "scanlon04_interspeech",
    "lee04m_interspeech",
    "erzin04_interspeech",
    "iba04_interspeech",
    "zhang04i_interspeech",
    "chaloupka04_interspeech",
    "wagner04_interspeech",
    "martinez04_interspeech",
    "berthommier04_interspeech",
    "cisar04_interspeech",
    "millar04b_interspeech",
    "hong04_interspeech",
    "nielsen04_interspeech"
   ]
  },
  {
   "title": "Spoken Language Generation and Synthesis III",
   "papers": [
    "ye04_interspeech",
    "fischer04_interspeech",
    "zhou04c_interspeech",
    "ney04b_interspeech",
    "yu04e_interspeech",
    "vepa04_interspeech",
    "zen04_interspeech",
    "lin04_interspeech",
    "bonafonte04_interspeech",
    "wang04j_interspeech",
    "ling04_interspeech",
    "kataoka04b_interspeech",
    "nishizawa04_interspeech",
    "yamagishi04_interspeech",
    "breuer04_interspeech",
    "alias04_interspeech",
    "ennajjary04_interspeech",
    "ennajjary04b_interspeech",
    "kumar04_interspeech",
    "huang04d_interspeech",
    "kumar04b_interspeech",
    "lambert04_interspeech",
    "kominek04_interspeech",
    "zhang04j_interspeech",
    "zen04b_interspeech",
    "pfitzinger04_interspeech",
    "min04_interspeech",
    "yoon04_interspeech",
    "shi04_interspeech",
    "chen04d_interspeech",
    "xu04c_interspeech",
    "schroder04_interspeech",
    "cao04_interspeech",
    "zovato04_interspeech",
    "min04b_interspeech",
    "kim04j_interspeech",
    "khaorapapong04_interspeech",
    "shiga04b_interspeech",
    "hisako04_interspeech",
    "clermont04_interspeech",
    "saitou04_interspeech",
    "pollet04_interspeech",
    "matousek04_interspeech",
    "nam04_interspeech",
    "beringer04_interspeech",
    "hamza04_interspeech",
    "ha04_interspeech",
    "kim04k_interspeech",
    "webster04_interspeech",
    "hamza04b_interspeech",
    "schnell04b_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Language Model",
   "papers": [
    "kawahara04d_interspeech",
    "jamoussi04_interspeech",
    "bigi04_interspeech",
    "mori04b_interspeech",
    "chien04_interspeech",
    "chien04b_interspeech",
    "goel04_interspeech",
    "schofield04_interspeech",
    "vergyri04_interspeech",
    "khan04_interspeech",
    "schwenk04_interspeech",
    "mrva04_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "louradour04_interspeech",
    "chetouani04_interspeech",
    "shriberg04b_interspeech",
    "ho04_interspeech",
    "iwano04_interspeech",
    "chen04e_interspeech",
    "lawson04_interspeech",
    "yoshida04b_interspeech",
    "mak04c_interspeech",
    "tadj04b_interspeech",
    "chow04_interspeech",
    "wenndt04_interspeech",
    "yang04c_interspeech",
    "padilla04_interspeech",
    "radova04_interspeech",
    "guan04_interspeech",
    "chien04c_interspeech",
    "aronowitz04b_interspeech",
    "wang04k_interspeech",
    "matsui04_interspeech",
    "saeta04_interspeech",
    "frati04_interspeech",
    "elyazeed04_interspeech",
    "cheung04_interspeech",
    "betser04_interspeech",
    "zheng04d_interspeech",
    "gangadharaiah04_interspeech",
    "yegnanarayana04_interspeech",
    "benzeghiba04_interspeech",
    "mihoubi04_interspeech",
    "kim04l_interspeech",
    "sreenivas04b_interspeech",
    "anguita04_interspeech",
    "siafarikas04_interspeech",
    "chaudhari04_interspeech",
    "choi04c_interspeech",
    "kim04m_interspeech",
    "desai04_interspeech",
    "angkititrakul04_interspeech",
    "umeda04_interspeech",
    "barlow04_interspeech",
    "alexander04_interspeech",
    "kinnunen04_interspeech",
    "navratil04_interspeech",
    "khan04b_interspeech",
    "shao04b_interspeech",
    "leung04_interspeech",
    "park04e_interspeech",
    "tran04_interspeech"
   ]
  },
  {
   "title": "Processing of Prosody by Humans and Machines",
   "papers": [
    "bailly04_interspeech",
    "nguyen04_interspeech",
    "ashimura04_interspeech",
    "masuko04_interspeech",
    "hasegawajohnson04b_interspeech",
    "hirose04b_interspeech"
   ]
  },
  {
   "title": "Contemporary Issues in ASR",
   "papers": [
    "peters04_interspeech",
    "gutkin04_interspeech",
    "chung04c_interspeech",
    "chen04f_interspeech",
    "aalburg04_interspeech",
    "heracleous04_interspeech",
    "russell04_interspeech",
    "frankel04_interspeech",
    "bouwman04_interspeech",
    "ishihara04_interspeech",
    "anguita04b_interspeech",
    "jou04_interspeech",
    "gruhn04_interspeech",
    "doss04_interspeech",
    "bosch04_interspeech",
    "cincarek04_interspeech",
    "stouten04_interspeech",
    "kwon04b_interspeech",
    "gerosa04_interspeech",
    "liu04b_interspeech",
    "jin04b_interspeech",
    "goto04_interspeech",
    "lee04n_interspeech",
    "moller04_interspeech",
    "james04_interspeech",
    "milner04_interspeech"
   ]
  },
  {
   "title": "Second Language Learning and Spoken Language Processing",
   "papers": [
    "minematsu04b_interspeech",
    "suzuki04c_interspeech",
    "wang04l_interspeech",
    "rhee04_interspeech",
    "bernstein04_interspeech",
    "kawahara04e_interspeech",
    "beskow04_interspeech"
   ]
  },
  {
   "title": "Emerging Research: Human Factors in Speech and Communication Systems",
   "papers": [
    "campana04_interspeech",
    "black04_interspeech",
    "jokinen04_interspeech",
    "fu04b_interspeech",
    "moller04b_interspeech",
    "hammer04_interspeech"
   ]
  },
  {
   "title": "Interdisciplinary Topics in Spoken Language Processing",
   "papers": [
    "sansegundo04_interspeech",
    "kanedera04_interspeech",
    "erickson04_interspeech",
    "lee04o_interspeech",
    "kweon04_interspeech",
    "jo04b_interspeech",
    "montanari04_interspeech",
    "kim04n_interspeech",
    "kim04o_interspeech",
    "amano04_interspeech",
    "you04b_interspeech",
    "beautemps04_interspeech",
    "takahashi04_interspeech"
   ]
  },
  {
   "title": "Towards Adaptive Machines: Active and Unsupervised Learning",
   "papers": [
    "yu04f_interspeech",
    "meyer04_interspeech",
    "visweswariah04b_interspeech",
    "lussier04_interspeech",
    "nishida04_interspeech",
    "watanabe04b_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "dusan04b_interspeech",
    "feldbauer04_interspeech",
    "david04_interspeech",
    "gunawan04_interspeech",
    "jung04c_interspeech",
    "falk04_interspeech"
   ]
  },
  {
   "title": "Robust ASR",
   "papers": [
    "kim04p_interspeech",
    "ahadi04_interspeech",
    "huerta04_interspeech",
    "ansary04_interspeech",
    "willett04_interspeech",
    "lo04_interspeech",
    "li04f_interspeech",
    "zhao04_interspeech",
    "kitaoka04_interspeech",
    "sakauchi04_interspeech",
    "brito04_interspeech",
    "cox04_interspeech",
    "obuchi04_interspeech",
    "xiong04b_interspeech",
    "tsujikawa04_interspeech",
    "gomez04_interspeech",
    "cranen04_interspeech",
    "xu04d_interspeech",
    "korkmazsky04_interspeech",
    "hasegawajohnson04c_interspeech",
    "lee04p_interspeech",
    "cerisara04_interspeech",
    "pujol04_interspeech",
    "ikbal04_interspeech",
    "kim04q_interspeech",
    "aradilla04_interspeech",
    "han04c_interspeech",
    "haebumbach04_interspeech"
   ]
  },
  {
   "title": "Emerging Research",
   "papers": [
    "ebukuro04_interspeech",
    "smeele04_interspeech",
    "haas04_interspeech",
    "chen04g_interspeech",
    "beringer04b_interspeech"
   ]
  },
  {
   "title": "Spoken Language Resources and Technology Evaluation I",
   "papers": [
    "dybkjaer04_interspeech",
    "kimball04_interspeech",
    "mishra04_interspeech",
    "hartikainen04b_interspeech",
    "vasilescu04_interspeech",
    "sarikaya04_interspeech",
    "wu04b_interspeech",
    "ciloglu04_interspeech",
    "yoon04b_interspeech",
    "tian04b_interspeech",
    "liang04_interspeech",
    "heggtveit04_interspeech",
    "sanders04b_interspeech",
    "fousek04_interspeech",
    "krebber04_interspeech",
    "kim04r_interspeech",
    "almasganj04_interspeech",
    "morris04_interspeech",
    "rhee04b_interspeech",
    "vriend04_interspeech",
    "wang04m_interspeech",
    "davel04_interspeech",
    "geumann04_interspeech",
    "kawaguchi04_interspeech",
    "bael04_interspeech",
    "davel04b_interspeech"
   ]
  },
  {
   "title": "Multi-Modal / Multi-Media Processing",
   "papers": [
    "moore04_interspeech",
    "ban04_interspeech",
    "gupta04b_interspeech",
    "marcheret04_interspeech",
    "choi04d_interspeech",
    "ariyoshi04_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition in the Context of Mobile Communications",
   "papers": [
    "novak04b_interspeech",
    "fujimura04_interspeech",
    "tan04_interspeech",
    "rose04_interspeech",
    "jeong04_interspeech"
   ]
  },
  {
   "title": "Robust Features for ASR",
   "papers": [
    "fukuda04_interspeech",
    "srinivasan04b_interspeech",
    "sanchis04_interspeech",
    "burget04_interspeech",
    "ikbal04b_interspeech",
    "yook04_interspeech"
   ]
  },
  {
   "title": "Towards Rapid Speech and Natural Language Application Development: Tooling, Architectures, Components and Standards",
   "papers": [
    "hetherington04_interspeech",
    "feng04_interspeech",
    "huerta04b_interspeech",
    "rodriguezmoreno04_interspeech",
    "facco04_interspeech",
    "hamerich04_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Enhancement",
   "papers": [
    "kondo04_interspeech",
    "lee04q_interspeech",
    "so04_interspeech",
    "chaitanya04_interspeech",
    "kinoshita04_interspeech",
    "lee04r_interspeech",
    "abad04_interspeech",
    "kim04s_interspeech",
    "asai04_interspeech",
    "kim04t_interspeech",
    "gandhi04_interspeech",
    "ramo04_interspeech",
    "ju04_interspeech",
    "turunen04_interspeech",
    "li04g_interspeech",
    "li04h_interspeech",
    "errity04_interspeech",
    "kim04u_interspeech",
    "li04i_interspeech",
    "gabrea04_interspeech",
    "sreenivas04c_interspeech",
    "qian04b_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling for Robust ASR",
   "papers": [
    "korkmazsky04b_interspeech",
    "mao04_interspeech",
    "raut04_interspeech",
    "wu04c_interspeech",
    "matsuda04_interspeech",
    "yoma04_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Technology and Systems",
   "papers": [
    "tomko04_interspeech",
    "pavlovski04_interspeech",
    "rayner04_interspeech",
    "lane04_interspeech",
    "fujie04_interspeech",
    "ollason04_interspeech"
   ]
  },
  {
   "title": "Multi-Channel Speech Processing",
   "papers": [
    "haebumbach04b_interspeech",
    "kwon04c_interspeech",
    "potamitis04_interspeech",
    "kim04v_interspeech",
    "jang04_interspeech",
    "visser04_interspeech"
   ]
  },
  {
   "title": "Intersection of Spoken Language Processing and Written Language Processing",
   "papers": [
    "lee04s_interspeech",
    "sugito04_interspeech",
    "kawahara04f_interspeech",
    "arora04_interspeech",
    "calzolari04_interspeech"
   ]
  },
  {
   "title": "Prosodic Recognition and Analysis",
   "papers": [
    "takamaru04_interspeech",
    "effendy04_interspeech",
    "zhang04k_interspeech",
    "watanabe04c_interspeech",
    "yabuta04_interspeech",
    "suzuki04d_interspeech",
    "kiriyama04_interspeech",
    "blodgett04_interspeech",
    "kuhne04_interspeech",
    "kim04w_interspeech",
    "jun04_interspeech",
    "borys04_interspeech",
    "kong04_interspeech",
    "takagi04_interspeech",
    "speer04_interspeech",
    "nagasaki04_interspeech",
    "li04j_interspeech",
    "kim04x_interspeech",
    "ohsuga04_interspeech",
    "komatsu04_interspeech"
   ]
  },
  {
   "title": "Towards Rapid Speech and Natural Language Application Development",
   "papers": [
    "lhour04_interspeech",
    "chen04h_interspeech",
    "dharo04_interspeech",
    "aist04_interspeech",
    "fabbrizio04_interspeech",
    "kawahara04g_interspeech",
    "murao04_interspeech",
    "buhler04_interspeech",
    "turunen04b_interspeech",
    "filipe04_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2004"
}