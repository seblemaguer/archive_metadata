{
 "title": "4th International Conference on Spoken Language Processing (ICSLP 1996)",
 "location": "Philadelphia, PA, USA",
 "startDate": "3/10/1996",
 "endDate": "6/10/1996",
 "chair": "General Chair: H. Timothy Bunnell",
 "conf": "ICSLP",
 "year": "1996",
 "name": "icslp_1996",
 "series": "ICSLP",
 "SIG": "",
 "title1": "4th International Conference on Spoken Language Processing",
 "title2": "(ICSLP 1996)",
 "date": "3-6 October 1996",
 "booklet": "icslp_1996.pdf",
 "papers": {
  "cutler96_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "The comparative study of spoken-language processing",
   "original": "i96_0001",
   "page_count": 1,
   "order": 1,
   "p1": "1",
   "pn": "",
   "abstract": [
    "Psycholinguists are saddled with a paradox. Their aim is to construct a model of human language processing, which will hold equally well for the processing of any language, but this aim cannot be achieved just by doing experiments in any language. They have to compare processing of many languages, and actively search for effects which are specific to a single language, even though a model which is itself specific to a single language is really the last thing they want.\n",
    ""
   ]
  },
  "flanagan96_icslp": {
   "authors": [
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "Natural communication with machines - progress and challenge",
   "original": "i96_2522",
   "page_count": 1,
   "order": 2,
   "p1": "2522",
   "pn": "",
   "abstract": [
    "In less than one life time the technologies of electronic communication and computing have emerged, advanced, and coalesced. Initially the fields developed separately, constituting recognized independent disciplines. Great scientific effort went into each, with results that have fueled manifold multiplication of the Gross Domestic Product of developed countries.\n",
    "Now, on the threshold of the 21st century, the merging capabilities of communication and computing surpass most of the unfettered dreams of the mid 1900s. But, the benefits are not easy for humans to reap. More and more, a central focus is on natural communication with machines-using modalities comfortable for the human-sight, sound and touch. An ideal is communication replicating that between humans, but for the foreseeable time this complete ideal may remain on the horizon. Nevertheless, machine comprehension of human-generated information in the visible, auditory and tactile domains is advancing apace, as is the ability of the machine to generate responses in the same domains. Just as the mouse and icon-based software liberated computer users from many tyrannies of the keyboard, natural voice communication, gesture recognition, and automatic scene processing will lead to enhanced freedom and greater utility in communication and computing. This report draws a brief perspective on this outlook.\n",
    ""
   ]
  },
  "li96_icslp": {
   "authors": [
    [
     "Z.",
     "Li"
    ],
    [
     "M.",
     "Heon"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "New developments in the INRS continuous speech recognition system",
   "original": "i96_0002",
   "page_count": 4,
   "order": 3,
   "p1": "2",
   "pn": "5",
   "abstract": [
    "New techniques are developed for the second pass search in our large vocabulary continuous speech recognition system. The merging of recognition hypotheses is proposed in order to linearize the exponential growth of the tree structure in the depth first search. Branching ordering of the first pass word graph and pruning at both word and phone levels are used to further speed up the search. The algorithm has been shown to be effective on the speaker-independent WSJ task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-1"
  },
  "lamel96_icslp": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "On designing pronunciation lexicons for large vocabulary, continuous speech recognition",
   "original": "i96_0006",
   "page_count": 5,
   "order": 4,
   "p1": "6",
   "pn": "9",
   "abstract": [
    "Creation of pronunciation lexicons for speech recognition is widely acknowledged to be an important, but labor-intensive, aspect of system development. Lexicons are often manually created and make use of knowledge and expertise that is difficult to codify. In this paper we describe our American English lexicon developed primarily for the ARPA WSJ/NAB tasks. The lexicon is phonemically represented, and contains alternate pronunciations for about 10% of the words. Tools have been developed to add new lexical items, as well as to help ensure consistency of the pronunciations. Our experience in large vocabulary, continuous speech recognition is that systematic lexical design can improve system performance. Some comparative results with commonly available lexicons are given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-2"
  },
  "fetter96_icslp": {
   "authors": [
    [
     "Pablo",
     "Fetter"
    ],
    [
     "Frédéric",
     "Dandurand"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Word graph rescoring using confidence measures",
   "original": "i96_0010",
   "page_count": 4,
   "order": 5,
   "p1": "10",
   "pn": "13",
   "abstract": [
    "This paper presents a novel approach to using confidence scores for word graph rescoring. For each word in the system's vocabulary, we computed the probability that the observation is correct given its acoustic score. Afterwards, we used these probabilities for rescoring word graphs outputted by the recognizer. We will present some implementation details as well as accuracy improvements obtained using this method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-3"
  },
  "aubert96_icslp": {
   "authors": [
    [
     "X. L.",
     "Aubert"
    ],
    [
     "Peter",
     "Beyerlein"
    ],
    [
     "Meinhard",
     "Ullrich"
    ]
   ],
   "title": "A bottom-up approach for handling unseen triphones in large vocabulary continuous speech recognition",
   "original": "i96_0014",
   "page_count": 4,
   "order": 6,
   "p1": "14",
   "pn": "17",
   "abstract": [
    "This paper presents an extension of bottom-up state-tying towards improved handling of unseen triphones. As opposed to the usual backing-off to diphones and monophones, the current method aims at finding a triphone model that has proven to exhibit some similarity with the unseen triphone. It is based on a probabilistic mapping of unseen contexts to clusters of triphone-states observed in the training data. This algorithm has been applied to dictation tasks for three languages with vocabulary sizes ranging from 20k to 64k. The results compare favorably with those obtained using standard back-off rules. This technique also offers an alternative to top-down decision-tree procedures which are frequently used especially for their generalization capabilities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-4"
  },
  "valtchev96_icslp": {
   "authors": [
    [
     "V.",
     "Valtchev"
    ],
    [
     "P. C.",
     "Woodland"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Discriminative optimisation of large vocabulary recognition systems",
   "original": "i96_0018",
   "page_count": 4,
   "order": 7,
   "p1": "18",
   "pn": "21",
   "abstract": [
    "This paper describes a framework for optimising the structure and parameters of a continuous density HMM-based large vocabulary recognition system using the Maximum Mutual Information Estimation (MMIE) criterion. To reduce the computational complexity of the MMIE training algorithm, confusable segments of speech are identified and stored as word lattices of alternative utterance hypotheses. An iterative mixture splitting procedure is also employed to adjust the number of mixture components in each state during training such that the optimal balance between number of parameters and available training data is achieved. Experiments are presented on various test sets from the Wall Street Journal database using the full SI-284 training set. These show that the use of lattices makes MMIE training practicable for very complex recognition systems and large training sets. Furthermore, experimental results demonstrate that MMIE optimisation of system structure and parameters can yield useful increases in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-5"
  },
  "matsuoka96_icslp": {
   "authors": [
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Katsutoshi",
     "Ohtsuki"
    ],
    [
     "Takeshi",
     "Mori"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Japanese large-vocabulary continuous-speech recognition using a business-newspaper corpus",
   "original": "i96_0022",
   "page_count": 4,
   "order": 8,
   "p1": "22",
   "pn": "25",
   "abstract": [
    "We studied Japanese large-vocabulary continuous-speech recognition (LV CSR) for a Japanese business newspaper. To enable word N-grams to be used, sentences were first segmented into words (morphemes) using a morphological analyzer. Newspaper articles for about five years were used to train N-gram language models. To evaluate our recognition system, we recorded speech data for sentences from another set of articles. Using the speech corpus, LV CSR experiments were conducted. For 7k vocabulary, the word error rate was 82.8% when no grammar and context-independent acoustic models were used. This improved to 20.0% when both bigram language models and context-dependent acoustic models were used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-6"
  },
  "carter96_icslp": {
   "authors": [
    [
     "David",
     "Carter"
    ],
    [
     "Jaan",
     "Kaja"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Manny",
     "Rayner"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Mats",
     "Wiren"
    ]
   ],
   "title": "Handling compound nouns in a Swedish speech-understanding system",
   "original": "i96_0026",
   "page_count": 4,
   "order": 9,
   "p1": "26",
   "pn": "29",
   "abstract": [
    "This paper describes and evaluates a simple and general solution to the handling of compound nouns in Swedish and other languages in which compounds can be formed by concatenation of single words. The basic idea is to split compounds into their components and treat these components as recognition units equivalent to other words in the language model. By using a principled grammar-based language-processing architecture, it is then possible to accommodate input in split-compound format.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-7"
  },
  "maciasguarasa96_icslp": {
   "authors": [
    [
     "J.",
     "Macias-Guarasa"
    ],
    [
     "A.",
     "Gallardo"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "José M.",
     "Pardo"
    ],
    [
     "L.",
     "Villarrubia"
    ]
   ],
   "title": "Initial evaluation of a preselection module for a flexible large vocabulary speech recognition system in",
   "original": "i96_0030",
   "page_count": 4,
   "order": 10,
   "p1": "30",
   "pn": "33",
   "abstract": [
    "We are improving a flexible, large vocabulary, speaker independent, isolated-word recognition system in a telephone environment, originally designed as an integrated system doing all the recognition process in one step. We have transformed it, by adopting the hypothesis-verification paradigm. In this paper, we will describe the architecture and results of the hypothesis subsystem. We will show the system evolution and the modifications adopted to face such a difficult task, achieving significant improvements using automatically clustered phoneme-like units, semi-continuous HMMs, and multiple models per unit. Also, system behavior for vocabulary dependent and independent tasks and vocabularies up to 10000 words will be tested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-8"
  },
  "alissali96_icslp": {
   "authors": [
    [
     "Mamoun",
     "Alissali"
    ],
    [
     "Paul",
     "Deleglise"
    ],
    [
     "Alexandrina",
     "Rogozan"
    ]
   ],
   "title": "Asynchronous integration of visual information in an automatic speech recognition system",
   "original": "i96_0034",
   "page_count": 4,
   "order": 11,
   "p1": "34",
   "pn": "37",
   "abstract": [
    "This paper deals with the integration of visual data in automatic speech recognition systems. We first describe the framework of our research; the development of advanced multi-user multi-modal interfaces. Then we present audiovisual speech recognition problems in general, and the ones we are interested in, in particular. After a very brief discussion of existing systems, the major part of the paper describes the systems we developed according to two different approaches to the problem of integration of visual data in speech recognition systems. Section 3 presents the architecture of our audio-only reference and baseline systems. Our audio-visual systems are described in Section 2. We first describe a system we developed according to the first approach (called the direct integration model) and show its limitations. Our approach, which we call asynchronous integration, is then presented in Sectio 4.2. After the general guidelines, we go into some details about the distributed architecture and the variant of the N-best algorithm we developed for the implementation of this approach. In Section 6 the performances of these different systems are compared, then we conclude by a brief discussion of the performance improvements we obtain and future work.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-9"
  },
  "matthews96_icslp": {
   "authors": [
    [
     "I. A.",
     "Matthews"
    ],
    [
     "J.",
     "Bangham"
    ],
    [
     "S. J.",
     "Cox"
    ]
   ],
   "title": "Audiovisual speech recognition using multiscale nonlinear image decomposition.",
   "original": "i96_0038",
   "page_count": 4,
   "order": 12,
   "p1": "38",
   "pn": "41",
   "abstract": [
    "There has recently been increasing interest in the idea of enhancing speech recognition by the use of visual information derived from the face of the talker. This paper demonstrates the use of nonlinear image decomposition, in the form of a sieve, applied to the task of visual speech recognition. Information derived from the mouth region is used in visual and audiovisual speech recognition of a database of the letters A-Z for four talkers. A scale histogram is generated directly from the grayscale pixels of a window containing the talkers mouth on a per frame basis. Results are presented for visual-only, audio-only and in a simple audiovisual case.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-10"
  },
  "su96_icslp": {
   "authors": [
    [
     "Qin",
     "Su"
    ],
    [
     "Peter L.",
     "Silsbee"
    ]
   ],
   "title": "Robust audiovisual integration using semicontinuous hidden Markov models",
   "original": "i96_0042",
   "page_count": 4,
   "order": 13,
   "p1": "42",
   "pn": "45",
   "abstract": [
    "We describe an improved method of integrating audio and visual information in a HMM-based audiovisual ASR system. The method uses a modified semicontinuous HMM (SCHMM) for integration and recognition. Our results show substantial improvements over earlier integration methods at high noise levels. Our integration method relies on the assumption that, as environmental conditions deviate from those under which training occurred, the underlying probability distributions will also change. We use phoneme based SCHMMs for classification of isolated words. The probability models underlying the standard SCHMM are Gaussian; thus, low probability estimates will tend to be associated with high confidences (small differences in the feature values cause large proportional differences in probabilities, when the values are in the tail of the distribution). Therefore, during classification, we replace each Gaussian with a scoring function which looks Gaussian near the mean of the distribution but has a heavier tail. We report results comparing this method with an audio-only system and with previous integration methods. At high noise levels, the system with modified scoring functions shows a better than 50 recognition does suffer when noise is low. Methods which can adjust the relative weight of the audio and visual information can still potentially outperform the new method, provided that a reliable way of choosing those weights can be found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-11"
  },
  "schumeyer96_icslp": {
   "authors": [
    [
     "Richard P.",
     "Schumeyer"
    ],
    [
     "Kenneth E.",
     "Barner"
    ]
   ],
   "title": "The effect of visual information on word initial consonant perception of dysarthric speech",
   "original": "i96_0046",
   "page_count": 4,
   "order": 14,
   "p1": "46",
   "pn": "49",
   "abstract": [
    "Disabled individuals will realize many benefits from automatic speech recognition. To date, most automatic speech recognition research has focused on normal speech. However, many individuals with physical disabilities also exhibit speech disorders. While limited research has been conducted focusing on dysarthric speech recognition, the preliminary results indicate that additional study is necessary. Recently, increasing attention has been given to multimodal speech recognition schemes that utilize multiple input sources - most commonly audio and video. This multimodal approach has been applied to normal speech with demonstrated effectiveness. Through studying the effect of audio and visual information in a human perception experiment, this study attempts to discover whether such an approach would be useful for dysarthric speech recognition. Results of a closed vocabulary perception test are presented. In this test, 15 normal hearing viewers were presented with videotapes of three dysarthric speakers speaking a series of one syllable nonsense words. These words differed only in the initial consonant. The words were presented in both audio-only and audio-visual modes. Perception rates in both modes were measured. The results are analyzed and compared to other studies of visual speech perception and dysarthric speech articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-12"
  },
  "chandramohan96_icslp": {
   "authors": [
    [
     "Devi",
     "Chandramohan"
    ],
    [
     "Peter L.",
     "Silsbee"
    ]
   ],
   "title": "A multiple deformable template approach for visual speech recognition",
   "original": "i96_0050",
   "page_count": 4,
   "order": 15,
   "p1": "50",
   "pn": "53",
   "abstract": [
    "In this paper, we propose an improved deformable template algorithm for modeling the shape of a talker's mouth. We use a two step approach which begins by classifying mouth images into broad categories. The classification procedure yields both a set of template parameters (in effect, a unique template) and a set of initial conditions. The second step is to allow the deformable template to converge using standard techniques. The multi-model approach is significantly more flexible than single-model approaches and consistently provides better solutions. We present examples of single and multiple template solutions which support this statement. In a small recognition experiment, recognition of consonants improved from 16% to 33%, based only on visual information, when multiple templates were used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-13"
  },
  "cosi96_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "E. Magno",
     "Caldognetto"
    ],
    [
     "Franco",
     "Ferrero"
    ],
    [
     "M.",
     "Dugatto"
    ],
    [
     "K.",
     "Vagges"
    ]
   ],
   "title": "Speaker independent bimodal phonetic recognition experiments",
   "original": "i96_0054",
   "page_count": 5,
   "order": 16,
   "p1": "54",
   "pn": "57",
   "abstract": [
    "A speaker independent bimodal phonetic classification experiment regarding the Italian plosive consonants is described. The phonetic classification scheme is based on a feed forward recurrent back-propagation neural network working on audio and visual information. The speech signal is processed by an auditory model producing spectral-like parameters, while the visual signal is processed by a specialized hardware, called ELITE, computing lip and jaw kinematics parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-14"
  },
  "luettin96_icslp": {
   "authors": [
    [
     "Juergen",
     "Luettin"
    ],
    [
     "Neil A.",
     "Thacker"
    ],
    [
     "Steve W.",
     "Beet"
    ]
   ],
   "title": "Speechreading using shape and intensity information",
   "original": "i96_0058",
   "page_count": 4,
   "order": 17,
   "p1": "58",
   "pn": "61",
   "abstract": [
    "We describe a speechreading system that uses both, shape information from the lip contours and intensity information from the mouth area. Shape information is obtained by tracking and parameterising the inner and outer lip boundary in an image sequence. Intensity information is extracted from a grey level model, based on principal component analysis. In comparison to other approaches, the intensity area deforms with the shape model to ensure that similar object features are represented after non-rigid deformation of the lips. We describe speaker independent recognition experiments based on these features and Hidden Markov Models. Preliminary results suggest that similar performance can be achieved by using either shape or intensity information and slightly higher performance by their combined use.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-15"
  },
  "luettin96b_icslp": {
   "authors": [
    [
     "Juergen",
     "Luettin"
    ],
    [
     "Neil A.",
     "Thacker"
    ],
    [
     "Steve W.",
     "Beet"
    ]
   ],
   "title": "Speaker identification by lipreading",
   "original": "i96_0062",
   "page_count": 4,
   "order": 18,
   "p1": "62",
   "pn": "65",
   "abstract": [
    "This paper describes a new approach for speaker identification based on lipreading. Visual features are extracted from image sequences of the talking face and consist of shape parameters which describe the lip boundary and intensity parameters which describe the grey-level distribution of the mouth area. Intensity information is based on principal component analysis using eigenspaces which deform with the shape model. The extracted parameters account for both, speech dependent and speaker dependent information. We built spatio-temporal speaker models based on these features, using HMMs with mixtures of Gaussians. Promising results were obtained for text dependent and text independent speaker identification tests performed on a small video database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-16"
  },
  "gowjr96_icslp": {
   "authors": [
    [
     "David W.",
     "Gow Jr."
    ],
    [
     "Janis",
     "Melvold"
    ],
    [
     "Sharon",
     "Manuel"
    ]
   ],
   "title": "How word onsets drive lexical access and segmentation: evidence from acoustics, phonology and processing",
   "original": "i96_0066",
   "page_count": 4,
   "order": 19,
   "p1": "66",
   "pn": "69",
   "abstract": [
    "We will argue that the beginnings of words are perceptual \"islands of reliability\" in connected speech, and that their perceptual and temporal properties allow them to drive critical aspects of spoken word recognition including lexical segmentation. This argument rests on three generalizations derived from research in speech science, phonology, and psycholinguistics. We suggest that word onsets differ from other parts of words in that: (1) they offer more robust and redundant acoustic evidence about phonetic features, (2) they are generally protected from phonological assimilation, neutralization and deletion and therefore show less lawful variation in surface realization, and (3) they may activate lexical representations which facilitate word perception and thus diminish listeners dependence on veridical acoustic-phonetic processing of other portions of words. These properties of word onsets allow them to drive lexical segmentation by facilitating the recognition of items that begin with clear onsets. The implications of these findings for several models of lexical segmentation and spoken word recognition are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-17"
  },
  "kuijk96_icslp": {
   "authors": [
    [
     "David van",
     "Kuijk"
    ],
    [
     "Peter",
     "Wittenburg"
    ],
    [
     "Ton",
     "Dijkstra"
    ]
   ],
   "title": "RAW: a real-speech model for human word recognition",
   "original": "i96_0070",
   "page_count": 4,
   "order": 20,
   "p1": "70",
   "pn": "73",
   "abstract": [
    "In recent years computational models have become more and more important in testing processing mechanisms assumed to underlie human spoken-word recognition. Models like TRACE (McClelland & Elman, 1986) and Shortlist (Norris, 1994) have given us much insight in the effects of, for instance, competition between words in the mental lexicon and the use of lexical information during word recognition. However, these models neglect the effects of coarticulation and variability over time by using mock speech instead of real speech input. Here we describe a new connectionist model for spoken-word recognition which differs on a number of points from other models, in that it takes real speech as input, is based on a new architecture for the representation of time, and can adapt its own weights. Simulations with the model accurately reproduce some important effects found in human word recognition. However, the representations of words in the model and the implementation of the frequency effect should be investigated more thoroughly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-18"
  },
  "meftah96_icslp": {
   "authors": [
    [
     "Mehdi",
     "Meftah"
    ],
    [
     "Sami",
     "Boudelaa"
    ]
   ],
   "title": "How facilitatory can lexical information be during word recognition? evidence from moroccan arabic",
   "original": "i96_0074",
   "page_count": 4,
   "order": 21,
   "p1": "74",
   "pn": "77",
   "abstract": [
    "Two experiments were conducted, using Moroccan Arabic data, to evaluate conflicting predictions of autonomous and interactive models of spoken word recognition. In Experiment 1, lexical decision response times indicated the presence of strong lexical effects both with monosyllabic and bisyllabic words. In experiment 2, a General Phoneme Monitoring task was used in which subjects were asked to monitor for a target phoneme located at four different positions before and after the Uniqueness Point (UP). Strong lexical effetcs were obtained before UP. The bearings of these results on current autonomous and interactive models are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-19"
  },
  "haveman96_icslp": {
   "authors": [
    [
     "Alette P.",
     "Haveman"
    ]
   ],
   "title": "Effects of frequency on the auditory perception of open- versus closed-class words",
   "original": "i96_0078",
   "page_count": 4,
   "order": 22,
   "p1": "78",
   "pn": "81",
   "abstract": [
    "Over the past couple of decades, it has been repeatedly investigated whether open-class items are processed in a different way from closed-class items. Most studies, however, have been bedeviled by difficulties in controlling all relevant distinctions between open- and closed-class items. For example, whereas open-class items have a relatively low frequency of occurrence, closed-class words have a very high frequency. The current study investigates auditory lexical decision on open- versus closed-class items when the effect of frequency is controlled for. Results revealed faster responses to high frequency open-class items when compared to closed-class items of similar frequency. Furthermore, responses to both low frequency open-class items and non-words were significantly different from the responses to the high frequency open-class items, but not from responses to the high frequency closed-class items. Similar latencies for closed-class items and nonwords suggest that the open/closed-class distinction might be due to the clear lexical meaning of open-class items as opposed to the more grammatical function of closed-class words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-20"
  },
  "vitevitch96_icslp": {
   "authors": [
    [
     "Michael S.",
     "Vitevitch"
    ],
    [
     "Paul A.",
     "Luce"
    ],
    [
     "Jan",
     "Charles-Luce"
    ],
    [
     "David",
     "Kemmerer"
    ]
   ],
   "title": "Phonotactic and metrical influences on adult ratings of spoken nonsense words",
   "original": "i96_0082",
   "page_count": 4,
   "order": 23,
   "p1": "82",
   "pn": "85",
   "abstract": [
    "This study examined the phonological intuitions of adults by having them rate the phonological \"goodness\" of nonsense words. Subjects were asked to use a ten point scale to rate how \"English-like\" each stimulus, which was presented auditorily, sounded. The stimuli were phonotactically legal (in English) bisyllabic, CVCCVC nonsense words that varied in their phonotactic probability and primary stress placement. Subjects rated highly probable phonotactic stimuli as more \"Englishlike.\" In addition, stimuli with the primary stress on the first syllable were judged more \"English-like\" than stimuli with the primary stress on the second syllable. No interaction between phonotactic probability and stress was found. Our results show that subjects have consistent and reliable intuitions regarding phonotactic configurations and stress patterning, demonstrating that fairly detailed probabilistic segmental and suprasegmental information resides in memory for form-based lexical representations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-21"
  },
  "auerjr96_icslp": {
   "authors": [
    [
     "Edward T.",
     "Auer Jr."
    ],
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Lipreading supplemented by voice fundamental frequency: to what extent does the addition of voicing increase lexical uniqueness for the lipreader?",
   "original": "i96_0086",
   "page_count": 4,
   "order": 24,
   "p1": "86",
   "pn": "89",
   "abstract": [
    "Lipreading in combination with an acoustic indication of voice fundamental frequency (F0) has been shown to greatly enhance word recognition accuracy with sentence stimuli [1]. A possible explanation for this effect is that F0 delivers information for consonantal voicing. In Experiment 1, we showed with a computational model how voicing information affects the uniqueness of lipread words in a large phonemically transcribed machine-readable lexicon. In Experiment 2, the same computational methods were used to simulate the results obtained by McGrath and Summerfield [2] for lipreading with and without acoustic F0. The model failed to account in full for the behaviorally observed enhancements. It is suggested that lexical biasing in word recognition can account for the difference between the model and the behavioral results. (This work was supported by NIH Grant DC-00695.)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-22"
  },
  "riele96_icslp": {
   "authors": [
    [
     "S. te",
     "Riele"
    ],
    [
     "Sieb G.",
     "Nooteboom"
    ],
    [
     "H.",
     "Quené"
    ]
   ],
   "title": "Strategies used in rhyme-monitoring",
   "original": "i96_0090",
   "page_count": 4,
   "order": 25,
   "p1": "90",
   "pn": "93",
   "abstract": [
    "This study investigates whether subjects use a strategy of word recognition in a rhyme-monitoring task. Results suggest that this is indeed the case. In addition, however, the task introduces an effect of phonological priming of the cue word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-23"
  },
  "donselaar96_icslp": {
   "authors": [
    [
     "Wilma van",
     "Donselaar"
    ],
    [
     "Cecile",
     "Kuijpers"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "How do dutch listeners process words with epenthetic schwa?",
   "original": "i96_0094",
   "page_count": 4,
   "order": 26,
   "p1": "94",
   "pn": "97",
   "abstract": [
    "Dutch words with certain final consonant clusters are subject to optional schwa epenthesis. The present research aimed at investigating how Dutch listeners deal with this type of phonological variation. By means of syllable monitoring experiments, it was investigated whether Dutch listeners process words with epenthetic schwa (e.g., balluk) as bisyllabic words or rather as monosyllabic words. Real words (e.g., balk, balluk) and pseudowords (e.g., golk, golluk) were compared, to examine effects of lexical representation. No difference was found between monitoring times for BAL targets in balluk carriers as compared to balk carriers. This suggests that words with epenthetic schwa are not processed as bisyllabic words. The effects for the pseudo-words paralleled those for the real words, which suggests that they are not due to lexical representation but rather to the application of phonological rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-24"
  },
  "juola96_icslp": {
   "authors": [
    [
     "Patrick",
     "Juola"
    ],
    [
     "Philip",
     "Zimmermann"
    ]
   ],
   "title": "Whole-word phonetic distances and the PGPfone alphabet",
   "original": "i96_0098",
   "page_count": 4,
   "order": 27,
   "p1": "98",
   "pn": "101",
   "abstract": [
    "Like many cryptosystems, PGPfone[13] requires a method of reliably exchanging binary data over noisy phone lines. This paper describes a method of encoding binary data into a \\radio alphabet,\" using a feature-based distance metric to measure phonetic confusibility, then using this metric in a GA to select appropriate words from a larger list of candidate words. This work indicates several larger issues that should be addressed in any (human) language engineering project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-25"
  },
  "ran96_icslp": {
   "authors": [
    [
     "Shuping",
     "Ran"
    ],
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "Phil",
     "Rose"
    ]
   ],
   "title": "Automatic vowel quality description using a variable mapping to an eight cardinal vowel reference set",
   "original": "i96_0102",
   "page_count": 4,
   "order": 28,
   "p1": "102",
   "pn": "105",
   "abstract": [
    "This paper investigates the possibility of describing vowels phonetically using an automated method. Models of the phonetic dimensions of the vowel space are built using two multi-layer perceptrons trained using eight cardinal vowels. The paper aims to improve the positioning of vowels in the open-close dimension by experimenting with a parameter in the model alpha which is the parameter which controls the slope of the sigmoid function employed in the multi-layer perceptrons.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-26"
  },
  "kipp96_icslp": {
   "authors": [
    [
     "Andreas",
     "Kipp"
    ],
    [
     "Maria-Barbara",
     "Wesenick"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Automatic detection and segmentation of pronunciation variants in German speech corpora",
   "original": "i96_0106",
   "page_count": 4,
   "order": 29,
   "p1": "106",
   "pn": "109",
   "abstract": [
    "In this paper we present a hybrid statistical and rule-based segmentation system which takes into account phonetic variation of German. Input to the system is the orthographic representation and the speech signal of an utterance to be segmented. The output is the transcription (SAM-PA) with the highest overall likelihood and the corresponding segmentation of the speech signal. The system consists of three main parts: In a first stage the orthographic representation is converted into a linear string of phonetic units by lexicon lookup. Phonetic rules are applied yielding a graph that contains the canonic form and presumed variations. In a second HMM-based stage the speech signal of the concerning utterance is time-aligned by a Viterbi search which is constrained by the graph of the first stage. The outcome of this stage is a string of phonetic labels and the corresponding segment boundaries. A rule-based refinement of the segment boundaries using phonetic knowledge takes place in a third stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-27"
  },
  "seneff96_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Raymond",
     "Lau"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "ANGIE: a new framework for speech analysis based on morpho-phonological modelling",
   "original": "i96_0110",
   "page_count": 4,
   "order": 30,
   "p1": "110",
   "pn": "113",
   "abstract": [
    "This paper describes a new system for speech analysis, ANGIE, which characterizes word substructure in terms of a trainable grammar. ANGIE capture morpho-phonemic and phonological phenomena through a hierarchical framework. The terminal categories can be alternately letters or phone units, yielding a reversible letter-to-sound/ sound-to-letter system. In conjunction with a segment network and acoustic phone models, the system can produce phonemic-to-phonetic alignments for speech waveforms. For speech recognition, ANGIE uses a one-pass bottom-up best-first search strategy. Evaluated in theATIS domain, ANGIE achieved a phone error rate of 36%, as compared with 40% achieved with a baseline phone-bigram based recognizer under similar conditions. ANGIE potentially offers many attractive features, including dynamic vocabulary adaptation, as well as a framework for handling unknown words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-28"
  },
  "yang96_icslp": {
   "authors": [
    [
     "Byunggon",
     "Yang"
    ]
   ],
   "title": "Perceptual contrast in the Korean and English vowel system normalized",
   "original": "i96_0114",
   "page_count": 4,
   "order": 31,
   "p1": "114",
   "pn": "117",
   "abstract": [
    "This study applied the uniform scaling method (Nordstroem and Lindblom, 1975) within and across the two languages to the formant data collected equivalently from 40 healthy subjects, which formed four groups of 10 subjects each: Korean males, Korean females, American males, and American females. Then, the formant values were converted to a perceptual unit, mel, and plotted on the first formant against the second formant axes. From the cross-language comparison we observed that the vowels are placed to maintain sufficient perceptual contrast within each vowel system which supports the notion of Lindblom's (1990) \"sufficient perceptual contrast\". There were greater cross-language differences for the vowels [u] and [a] than for the others. If Korean [u] has a relatively low F2 value it might be confused with Korean [I]. Therefore, Korean [u] has a relatively low F2 value to keep sufficient perceptual distance from [i] and [u]. The AE speakers separate sufficiently the two low vowels [a] and [oe] by around 400 mel. On the other hand, Korean [a] is placed at the corner of a regular triangle formed by acoustically neighboring vowels [E] and [U] simply to maintain sufficient contrast because there're no competitive vowels along the F2 dimension. Similar perceptual contrast is maintained between the AE tense and lax vowels. Lax vowels are pushed inside the AE vowel space to secure sufficient perceptual distance to adjacent vowels. This way each vowel system seems to maintain sufficient perceptual contrast.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-29"
  },
  "lee96_icslp": {
   "authors": [
    [
     "Yong-Ju",
     "Lee"
    ],
    [
     "Sook-Hyang",
     "Lee"
    ]
   ],
   "title": "On phonetic characteristics of pause in the Korean read speech",
   "original": "i96_0118",
   "page_count": 3,
   "order": 32,
   "p1": "118",
   "pn": "120",
   "abstract": [
    "To guarantee a good quality of synthesized speech in a rule-based synthesis, detailed and accurate segmental and suprasegmental rules for a given text are necessary. Especially, much work on the prosodic structure is needed. The goal of this study is to examine roles of syntactic information, intonation pattern, and phrase final lengthening in formation of prosodic units or perception of a phrase boundary. It investigates (a) whether location of (perceived) pause or a phrase boundary, one of the phonetic aspects of prosody, is syntactically determined and (b) how phonetic features such as a silence interval, intonation pattern, and lengthened vowel duration interacts with one another in formation of prosodic units. Radio news of two male announcers were recorded. Perception test of a phrase boundary was performed on the recorded data and at the same time both syntactic and acoustic analyses of them were done. The results showed that perceived pause were accompanied by at least one of the three phonetic features. Many cases were observed where the major syntactic boundary was not accompanied by any of the three phonetic features indicating that syntactic information does not play a crucial role in defining prosodic units. Rather, intonation pattern was proved to play a crucial role in formation of prosodic units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-30"
  },
  "boudelaa96_icslp": {
   "authors": [
    [
     "Sami",
     "Boudelaa"
    ],
    [
     "Mehdi",
     "Meftah"
    ]
   ],
   "title": "Cross-language effects of lexical stress in word recognition: the case of Arabic English bilinguals",
   "original": "i96_0121",
   "page_count": 4,
   "order": 33,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "Two lexical decison experiments examined the effects of lexical stress on word processing in Arabic-English bilinguals. In Experiment 1, Arabic and English minimal stress pairs sreved as primes either to semantically related targets, to targets related to the second member of the pair, or to control targets. English minimal stress pairs were processed like homophones, but Arabic ones were not. In experiment 2, the effects of mis-stressing Strong-Weak (SW) and Weak-Strong (WS) common words (ie., words that are not members of a minimal stress pair) was investigated. Only realizing a /SW/ word in a /WS/ stress pattern was adverse in English. In Arabic, however, mis-stressing had an adverse effect both in the case of SW and WS words. Taken together, the results suggest (a) that the time course of lexical stress effects are language dependent and (b) that Arabic-English bilinguals function monolingually with respect to lexical stress information. These resuslts are explained in terms of the asymmetry underlying the phonological structures of the two languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-31"
  },
  "wesenick96_icslp": {
   "authors": [
    [
     "Maria-Barbara",
     "Wesenick"
    ]
   ],
   "title": "Automatic generation of German pronunciation variants",
   "original": "i96_0125",
   "page_count": 4,
   "order": 34,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "The subject of this paper is a rule corpus of approx.1500 phonetic rules that models segmental variation of pronunciation in German connected speech. The phonetic rules express on a broad-phonetic level phenomena of phonetic reduction in German that occur within words and across word boundaries. The rule corpus has been designed as a component of the Munich AUtomatic Segmentation System (MAUS), which is an HMM-based system that produces the transcription of a speech signal and corresponding segment boundaries given the orthographic representation of the concerning utterance (refer to Kipp et al. [2] for details). The fact that speech is highly variable has been taken into account using the rules to complement the statistical modelling of German speech sounds and constrain the Viterbi-search. In this paper first a short introduction to the phenomenon of variability of speech and our approach of dealing with this problem in a technical application is presented. This is followed by a formal description of the syntax of the rules and the inventory of symbols that is used. Finally, I give an outline of reduction phenomena in German and how they are represented in the phonetic rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-32"
  },
  "wesenick96b_icslp": {
   "authors": [
    [
     "Maria-Barbara",
     "Wesenick"
    ],
    [
     "Andreas",
     "Kipp"
    ]
   ],
   "title": "Estimating the quality of phonetic transcriptions and segmentations of speech signals",
   "original": "i96_0129",
   "page_count": 4,
   "order": 35,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Our approach to the problem of evaluating segmentations and transcriptions of speech data is presented. We developed an automatic pattern-matching procedure that relates different manual or automatic segmentations to each other. The comparison of segmentations refers to the degree of identity concerning the chosen labels and of identity of segment boundaries. As we exemplify our evaluation method on the basis of automatic transcriptions of the Munich AUtomatic Segmentation System (MAUS) that is currently being developed at the IPSK (Kipp et al. [4]) our data also give information on the quality of the systems segmentation and transcription performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-33"
  },
  "petek96_icslp": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ],
    [
     "Rastislav",
     "Sustarsic"
    ],
    [
     "Smiljana",
     "Komar"
    ]
   ],
   "title": "An acoustic analysis of contemporary vowels of the standard slovenian language",
   "original": "i96_0133",
   "page_count": 4,
   "order": 36,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Slovenian language is among the richest Slavic languages in view of the number of dialects. More than 40 dialects in seven dialect groups can be found on a territory of about 21,000 km2 and population of 2 million. Given the richness of influencing factors on the Standard Slovenian language we decided to undertake an acoustic analysis of its contemporary vowel system. Results on vowel formant frequencies and durations using the Kay Elemetrics Computerized Speech Lab are presented for the monophthongs uttered by educated speakers of Standard Slovenian. The emphasis of research reported in this paper is on a comparative analysis of durations of the Slovenian vowels. The measurements support the basic division of vowels into long stressed and short unstressed ones, while the further subdivision of stressed vowels into long and short only seems to be valid for one particular vowel, i.e., the open central vowel /a/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-34"
  },
  "robbe96_icslp": {
   "authors": [
    [
     "Sandrine",
     "Robbe"
    ],
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Sylvie",
     "Coste"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Using decision trees to construct optimal acoustic cues",
   "original": "i96_0137",
   "page_count": 4,
   "order": 37,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "This paper presents an approach to the optimization of acoustic cues used for stop identification in the context of an acoustic-phonetic decoding system which uses automatic acoustic event extractors (a formant tracking algorithm and a burst analyzer). The acoustic cues have been designed on the basis of acoustic studies on stops and spectrogram reading experiments. This ensures that these cues have a certain amount of discriminating power but we do not know either the optimal thresholds nor which combination of cues are the most efficient. Therefore, we propose to use the decision tree theory [4] to choose the most discriminating cues and to improve their discrimination power. Considering the stop occurrences of a training corpus, the best cues are those which allow the decision tree leading to the best partition to be constructed. We have considered all the cues derived from the ones provided by the phonetician on formant transitions and burst characteristics. The improvement of the cues has been achieved on a corpus of 941 stops.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-35"
  },
  "erickson96_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Osamu",
     "Fujimura"
    ]
   ],
   "title": "Maximum jaw displacement in contrastive emphasis",
   "original": "i96_0141",
   "page_count": 4,
   "order": 38,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "Maximum jaw displacement for a syllable (or word), measured as the lowest vertical jaw position relative to the maxillary occlusal plane using x-ray microbeam data, was examined in the context of sets of utterances differing in terms of contrastive emphasis. It was found that there is a consistent and significant increase in jaw opening on the emphasized word for the three speakers examined. Moreover, the jaw opening on the other words in the utterance is also affected by emphasis according to this preliminary study: The amount of jaw opening of the syllables following emphasis is reduced, and the amount of drop in jaw opening from the emphasized syllable to the following syllable is increased.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-36"
  },
  "herman96_icslp": {
   "authors": [
    [
     "Rebecca",
     "Herman"
    ],
    [
     "Mary",
     "Beckman"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Subglottal pressure and final lowering in English",
   "original": "i96_0145",
   "page_count": 4,
   "order": 39,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "Quantitative models of intonation in a variety of languages typically specify a long-range downtrend across the sentence that provides a declining backdrop for the steeper rises and falls of more local pitch events such as accents and word tones. Several studies of this \"declination\" in English and several other languages have isolated a component of somewhat steeper decline that covers only the last few centiseconds of \"lab speech\" utterances. Other studies suggest that this \"final lowering\" may be particular to utterances with a \"declarative intonation\" pattern, and that it is associated particularly with the ends of discourse units. Thus final lowering seems to be associated pragmatically with a sense of fading off or finality. To see whether final lowering can be attributed in part to a fading off of subglottal pressure, we examined the two measures together in two databases of utterances that varied in intonation contour. To minimize confounds from more local pitch specifications, we looked at the relationship between final lowering and subglottal pressure only in the intonational \"tail\" - i.e., the portion of the contour after the last pitch accent. The slope of the decline of the subglottal pressure varied as a function of the phonological specification of the tones in the tail. Utterances with declarative intonation or with any other contour sharing the phonological specification of a low tone at the end of the tail consistently showed a decline in subglottal pressure, whereas utterances with \"yes-no question intonation\" or any other contour sharing the phonological specification of a final high tone showed lesser declines or even increases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-37"
  },
  "kuijpers96_icslp": {
   "authors": [
    [
     "Cecile",
     "Kuijpers"
    ],
    [
     "Wilma van",
     "Donselaar"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Phonological variation: epenthesis and deletion of schwa in Dutch",
   "original": "i96_0149",
   "page_count": 4,
   "order": 40,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "Two types of phonological variation in Dutch, resulting from optional rules, are schwa epenthesis and schwa deletion. In a lexical decision experiment it was investigated whether the phonological variants were processed similarly to the standard forms. It was found that the two types of variation patterned differently. Words with schwa epenthesis were processed faster and more accurately than the standard forms, whereas words with schwa deletion led to less fast and less accurate responses. The results are discussed in relation to the role of consonant-vowel alternations in speech processing and the perceptual integrity of onset clusters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-38"
  },
  "mahshie96_icslp": {
   "authors": [
    [
     "James J.",
     "Mahshie"
    ]
   ],
   "title": "Feedback considerations for speech training systems",
   "original": "i96_0153",
   "page_count": 4,
   "order": 41,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "The aim of this study was to explore the role of visual feedback of speech information on the development and use of specific speech production skills by a deaf learner. A young, deaf, female subject received training in which visual feedback of airflow was used to teach production of a voicing distinction. The results showed that provision of the feedback resulted in improvements in the segments being taught, and that providing the feedback during production resulted in improved performance during some phases of teaching, but not during others. The findings have implications on our understanding of speech learning that relies on the use of a visual surrogate for hearing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-39"
  },
  "oster96_icslp": {
   "authors": [
    [
     "Anne-Marie",
     "Öster"
    ]
   ],
   "title": "Clinical applications of computer-based speech training for children with hearing impairment",
   "original": "i96_0157",
   "page_count": 4,
   "order": 42,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "Computer-based visual speech training has become widely used both within medical and pedagogical rehabilitation in Sweden. When learning speech motor ability clear instruction is of great importance in order for the learner to realise what is deviant and what is correct in his or her pattern of behaviour. The correct behaviour should then be established and automated through extensive training for it to be transferred to untrained situations. This paper discusses teaching strategies and reports positive training results obtained through the visually contrastive feedback that this modern technical speech training aid offers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-40"
  },
  "hazan96_icslp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Andrew",
     "Simpson"
    ]
   ],
   "title": "Enhancing information-rich regions of natural VCV and sentence materials presented in noise",
   "original": "i96_0161",
   "page_count": 4,
   "order": 43,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "Two sets of experiments to test the perceptual benefits of enhancing information-rich regions of consonants in natural speech were performed. In the first set, hand-annotated consonantal regions of natural VCV stimuli were amplified to increase their salience, and filtered to stylize the cues they contained. In the second set, natural semantically unpredictable sentence (SUS) material was annotated and enhanced in the same way. Both sets of stimuli were combined with speech-shaped noise and presented to normally-hearing listeners. Both sets of experiments showed statistically significant improvements in intelligibility as a result of enhancement, although the increase was greater for VCV than for SUS. These results demonstrate the benefits gained from enhancement techniques which use knowledge about acoustic cues to phonetic contrasts to improve the resistance of speech to noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-41"
  },
  "hazan96b_icslp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Alan",
     "Adlard"
    ]
   ],
   "title": "Speech perceptual abilities of children with specific reading difficulty (dyslexia)",
   "original": "i96_0165",
   "page_count": 4,
   "order": 44,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "A group of 13 children with specific reading difficulty (SRD), 12 chronological-age and 12 reading-age controls were tested on a battery of speech-perceptual, psychoacoustic and reading tests. As a group, the SRD children performed worse than controls on all reading tasks, on the speech identification tasks, on discrimination tasks involving consonant clusters contrasts, and in their discrimination of stop consonants in nonsense VCVs. However, only a sub-group (30%) of SRD children showed high error rates in the speech discrimination tasks whilst the rest of the SRD group performed within norms. For this sub-group, discrimination performance was particularly poor for consonant contrasts differing in a single feature which was not acoustically salient, and errors were not limited to stop contrasts. Poor performance was also obtained in the identification tests, especially when the number of acoustic cues marking the contrast was reduced. Their performance did not differ from controls for the psychoacoustic tasks but they showed higher error rates in their reading of nonwords. It is concluded that only a proportion of SRD children show a speech perceptual weakness which seems to be related to a poor ability to discriminate phonemes which are acoustically similar.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-42"
  },
  "paarmann96_icslp": {
   "authors": [
    [
     "Larry D.",
     "Paarmann"
    ],
    [
     "Michael K.",
     "Wynne"
    ]
   ],
   "title": "Bimodal perception of spectrum compressed speech",
   "original": "i96_0169",
   "page_count": 4,
   "order": 45,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "In this paper, the results of both normal-hearing, and profoundly hearing-impaired adults, tested with spectrum compressed speech via the modified chirp-z algorithm, with and without visual stimuli, are reported. Ten normal-hearing adult listeners and three profoundly hearing-impaired adult listeners were asked to identify nonsense syllables presented auditorily and bimodally (audition and vision) via video tape in two conditions: lowpass filtered or unprocessed, and spectrum compressed. The lowpass filtered and spectrum com-pressed speech occupies the same spectrum width of 840 Hz; at 900 Hz and above, the attenuation is at least 60 dB. The spectrum compression is performed by means of a modified chirp-z algorithm, and is described in this paper. The testing results are significant and are reported in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-43"
  },
  "baraccikoja96_icslp": {
   "authors": [
    [
     "Dragana",
     "Barac-Cikoja"
    ],
    [
     "Sally",
     "Revoile"
    ]
   ],
   "title": "Effect of sentential context on syllabic stress perception by hearing-impaired listeners",
   "original": "i96_0173",
   "page_count": 3,
   "order": 46,
   "p1": "173",
   "pn": "175",
   "abstract": [
    "Although carrier phrases are extensively used in clinical testing of speech recognition, their influence on target identification by hearing-impaired (HI) listeners is poorly understood. The present study examined the effect of sentential context on the identification of the stressed syllable within the target VCV by listeners with mild to severe hearing loss. The study focused on the influence of prosodic structure of the carrier sentence as determined by the location of sentential stress and speaking rate. The target VCVs were presented in contexts that varied in the amount of the prosodic information available about the sentential stress pattern and speaking rate. That is, the target VCVs were presented embedded either in a sentence, in a noise simulation of the sentence, or extracted from the sentence and presented in isolation. Results indicated that both the prosodic context and the presentation condition affected performance. When sentential stress was on the target VCV, performance accuracy was highest. The finding is linked to the acoustic salience of the stressed syllable. The effect of the presentation condition is discussed in terms of the hearing-impaired listeners possible difficulties in segmenting the sentence and allocating attention.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-44"
  },
  "russell96_icslp": {
   "authors": [
    [
     "Martin",
     "Russell"
    ],
    [
     "Catherine",
     "Brown"
    ],
    [
     "Adrian",
     "Skilling"
    ],
    [
     "Rob",
     "Series"
    ],
    [
     "Julie",
     "Wallace"
    ],
    [
     "Bill",
     "Bohnam"
    ],
    [
     "Paul",
     "Barker"
    ]
   ],
   "title": "Applications of automatic speech recognition to speech and language development in young children",
   "original": "i96_0176",
   "page_count": 4,
   "order": 47,
   "p1": "176",
   "pn": "179",
   "abstract": [
    "Since 1990 the DRA Speech Research Unit has conducted research into applications of speech recognition technology to speech and language development for young children. This has been done in collaboration with Hereford and Worcester County Council Education Department (HWCC) and, more recently, with Sherston Software Limited, one of the UKs leading independent educational software publishers. An initial project, known as STAR (Speech Training Aid Research), was prompted by HWCCs awareness of a requirement by teachers for a computerised Speech Training Aid tool to aid young children in the development of a range of communications and language skills. The goal was to develop a computer-based system which was able to distinguish between good and poor pronunciations of a word, spoken by a child in response to a textual, pictorial or verbal prompt, from a 1,000 word childrens vocabulary. The same speech recognition technology has subsequently been integrated into Sherston Softwares commercially successful range of animated Talking Books, which use stored digitised speech to enable the computer to read words out-loud to a child. This converts them into Talking & Listening Books which, in addition to the existing functions, are able to listen to a child reading and indicate words which have been read incorrectly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-45"
  },
  "campbell96_icslp": {
   "authors": [
    [
     "D. R.",
     "Campbell"
    ]
   ],
   "title": "Sub-band adaptive speech enhancement for hearing aids",
   "original": "i96_0180",
   "page_count": 4,
   "order": 48,
   "p1": "180",
   "pn": "183",
   "abstract": [
    "This paper presents: a summary of features of the human auditory system and aspects of SHL that support suspicion of an adaptive sub-band binaural noise-cancellation process; description of a diverse sub-band adaptive processing (DSBAP) approach; experimental results indicating that DSBAP shows promise as a method of speech enhancement for hearing aids.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-46"
  },
  "portele96_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Jürgen",
     "Krämer"
    ]
   ],
   "title": "Adapting a TTS system to a reading machine for the blind",
   "original": "i96_0184",
   "page_count": 4,
   "order": 49,
   "p1": "184",
   "pn": "187",
   "abstract": [
    "Synthesis systems that convert orthographic text into speech usually make assumptions about the input that are no longer valid when used in combination with a scanner and OCR software.This paper describes our experience of adapting our TTS system for use use in such a reading machine.\n",
    "As synthesis systems move from the laboratory to applications, some surprises may be in store. This paper is intended to share our experiences with other developers in order to prevent them from repeating our mistakes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-47"
  },
  "shirai96_icslp": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Modeling of spoken dialogue with and without visual information",
   "original": "i96_0188",
   "page_count": 4,
   "order": 50,
   "p1": "188",
   "pn": "191",
   "abstract": [
    "Auditory information is the major factor in the human communication but in practical conversations, visual information such as gesture, facial expression, and head movement clearly makes it much smoother and more natural. Most researches related to analysis of spoken dialogue are based on only auditory information. We are trying to clarify how human utilize the knowledge of spoken dialogue management by dealing with more natural communication that includes visual information. Above all, by using visual information we can deal with listener's attitude against the speaker that cannot be done by using only auditory information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-48"
  },
  "seneff96b_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "David",
     "Goddeau"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Joseph",
     "Polifroni"
    ]
   ],
   "title": "Multimodal discourse modelling in a multi-user multi-domain environment",
   "original": "i96_0192",
   "page_count": 4,
   "order": 51,
   "p1": "192",
   "pn": "195",
   "abstract": [
    "This paper describes the discourse component of GALAXY, a multidomain, multimodal conversational system. In designing this module, we are attempting to develop domain-independent mechanisms, controlled via declarative tables, to promote convenient instantiation of a discourse component for each new domain. Direct anaphoric reference as well as elliptical reference are dealt with appropriately. Users can also refer verbally to items selected via mouse clicks. Cross domain references are particularly challenging, as is the ambiguity problem arising from different case roles for different subdomains. Users often utter fragments, sometimes in response to serverinitiated dialogue exchanges, so an extensive fragment interpretation mechanism is supported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-49"
  },
  "kita96_icslp": {
   "authors": [
    [
     "Kenji",
     "Kita"
    ],
    [
     "Yoshikazu",
     "Fukui"
    ],
    [
     "Masaaki",
     "Nagata"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "Automatic acquisition of probabilistic dialogue models",
   "original": "i96_0196",
   "page_count": 4,
   "order": 52,
   "p1": "196",
   "pn": "199",
   "abstract": [
    "In the work described here, we automatically deduce dialogue structures from a corpus with probabilistic methods. Each utterance in the corpus is annotated with a speaker label and an utterance type called IFT (Illocutionary Force Type). We use an Ergodic HMM (Hidden Markov Model) and the ALERGIA algorithm, an algorithm for learning probabilistic automata by means of state merging, to model the speaker-IFT sequences. Our experiments successfully extract typical dialogue structures such as turn-taking and speech act sequencing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-50"
  },
  "heisterkamp96_icslp": {
   "authors": [
    [
     "Paul",
     "Heisterkamp"
    ],
    [
     "Scott",
     "McGlashan"
    ]
   ],
   "title": "Units of dialogue management: an example",
   "original": "i96_0200",
   "page_count": 4,
   "order": 53,
   "p1": "200",
   "pn": "203",
   "abstract": [
    "This paper concerns dialogue management for spoken dialogue. We show why we do not use speech-act related units or intentions. We base our approach on belief states of the system. Layered units are used to construct a pragmatic interpretation of these states and to determine the dialogue continuation as a local optimisation over a set of dynamic dialogue goals. We point to systems that successfully employ this approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-51"
  },
  "oviatt96_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ],
    [
     "Robert",
     "VanGent"
    ]
   ],
   "title": "Error resolution during multimodal human-computer interaction",
   "original": "i96_0204",
   "page_count": 4,
   "order": 54,
   "p1": "204",
   "pn": "207",
   "abstract": [
    "Recent research indicates clear performance advantages and a strong user preference for interacting multimodally with computers. However, in the problematic area of error resolution, possible advantages of multimodal interface design remain poorly understood. In the present research, a semi-automatic simulation method with a novel error-generation capability was used to collect within-subject data before and after recognition errors, and at different spiral depths in terms of number of repetitions required to resolve an error. Results indicated that users adopt a strategy of switching input modalities and lexical expressions when resolving errors, strategies that they use in a linguistically contrastive manner to distinguish a repetition from original failed input. Implications of these findings are discussed for the development of user-centered predictive models of linguistic adaptation during human-computer error resolution, and for the development of improved error handling in advanced recognition-based interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-52"
  },
  "sarukkai96_icslp": {
   "authors": [
    [
     "Ramesh R.",
     "Sarukkai"
    ],
    [
     "Dana H.",
     "Ballard"
    ]
   ],
   "title": "Improved spontaneous dialogue recognition using dialogue and utterance triggers by adaptive probability boosting",
   "original": "i96_0208",
   "page_count": 4,
   "order": 55,
   "p1": "208",
   "pn": "211",
   "abstract": [
    "Based on the observation that the unpredictable nature of conversational speech makes it almost impossible to reliably model sequential word constraints, the notion of word set error criteria is proposed for improved recognition of spontaneous dialogues. The basic idea in the TAB algorithm is to predict a set of words based on some a priori information, and perform a re-scoring pass wherein the probabilities of the words in the predicted word set are amplified or boosted in some manner. An adaptive gradient descent procedure for tuning the word boosting factor has been formulated. Two novel models which predict the required word sets have been presented: utterance triggers which capture within-utterance long-distance word inter-dependencies, and dialogue triggers which capture local temporal dialogue-oriented word relations. The proposed Trigger and Adaptive Boosting (TAB) algorithm have been experimentally tested on a subset of the TRAINS-93 spontaneous dialogues and the TRAINS-95 semispontaneous corpus, and have resulted in improved performances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-53"
  },
  "hubener96_icslp": {
   "authors": [
    [
     "Kai",
     "Hübener"
    ],
    [
     "Uwe",
     "Jost"
    ],
    [
     "Henrik",
     "Heine"
    ]
   ],
   "title": "Speech recognition for spontaneously spoken German dialogues",
   "original": "i96_0212",
   "page_count": 4,
   "order": 56,
   "p1": "212",
   "pn": "215",
   "abstract": [
    "This paper presents a HMM speech recognition system for spontaneously spoken dialogues. It has been developed as part of the German Verbmobil project whose aim is the development of a translation support system for face-to-face conversations. The HTK-based decoder deals successfully with some of the hard problems of recognizing fluent speech and reaches quite competitive recognition results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-54"
  },
  "taylor96_icslp": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Stephen",
     "Isard"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Jacqueline",
     "Kowtko"
    ]
   ],
   "title": "Using prosodic information to constrain language models for spoken dialogue",
   "original": "i96_0216",
   "page_count": 4,
   "order": 57,
   "p1": "216",
   "pn": "219",
   "abstract": [
    "We present work intended to improve speech recognition performance for computer dialogue by taking into account the way that dialogue context and intonational tune interact to limit the possibilities for what an utterance might be. We report here on the extra constraint achieved in a bigram language model, expressed in terms of entropy, by using separate submodels for different sorts of dialogue acts, and trying to predict which submodel to apply by analysis of the intonation of the sentence being recognised.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-55"
  },
  "heeman96_icslp": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Kyung-ho",
     "Loken-Kim"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "Combining the detection and correction of speech repairs",
   "original": "i96_0362",
   "page_count": 4,
   "order": 58,
   "p1": "362",
   "pn": "365",
   "abstract": [
    "Previous approaches to detecting and correcting speech repairs have for the most part separated these two problems. In this paper, we present a statistical model of speech repairs that uses information about the possible correction to help decide whether a speech repair actually occurred. By better modeling the interactions between detection and correction, we are able to improve our detection results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-56"
  },
  "sagawa96_icslp": {
   "authors": [
    [
     "Yuji",
     "Sagawa"
    ],
    [
     "Wataru",
     "Sugimoto"
    ],
    [
     "Noboru",
     "Ohnishi"
    ]
   ],
   "title": "Generating spontaneous elliptical utterance",
   "original": "i96_0366",
   "page_count": 4,
   "order": 59,
   "p1": "366",
   "pn": "369",
   "abstract": [
    "To generate spontaneous utterances, it is important to omit some items from them appropriately. Previously, generation of elliptical utterances is discussed only from a view of whether an item in question is known or unknown in the context of dialogue. We point out that some items that are already known should not be omitted in some situation. For example, omitting too many items from utterances makes a hearer to feel that the speaker is impolite. In this paper, we propose some strategies to generate appropriate elliptical English utterances, given a politeness level. Our system can translate utterances with no ellipses into appropriate elliptical utterances using the strategies. The strategies are obtained by analysis of some dialogues. But we show that they are applicable to other dialogues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-57"
  },
  "bruce96_icslp": {
   "authors": [
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Marcus",
     "Filipsson"
    ],
    [
     "Johan",
     "Frid"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "Merle",
     "Horne"
    ],
    [
     "David",
     "House"
    ],
    [
     "Birgitta",
     "Lastow"
    ],
    [
     "Paul",
     "Touati"
    ]
   ],
   "title": "Developing the modelling of Swedish prosody in spontaneous dialogue",
   "original": "i96_0370",
   "page_count": 4,
   "order": 60,
   "p1": "370",
   "pn": "373",
   "abstract": [
    "The main goal of our current research is the development of the Swedish prosody model. In our analysis of discourse and dialogue intonation we are exploiting model-based resynthesis. By comparing synthesized default and fine-tuned pitch contours for dialogues under study we are able to isolate relevant intonation patterns. This analysis of intonation is related to an independent modelling of topic structure consisting of lexical-semantic analysis and text segmentation. Some results from our model-based acoustic analysis are presented, and the implementation in text-to-speech-synthesis is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-58"
  },
  "pan96_icslp": {
   "authors": [
    [
     "Shimei",
     "Pan"
    ],
    [
     "Kathleen R.",
     "McKeown"
    ]
   ],
   "title": "Spoken language generation in a multimedia system",
   "original": "i96_0374",
   "page_count": 4,
   "order": 61,
   "p1": "374",
   "pn": "377",
   "abstract": [
    "In this paper we address two important issues in generating spoken language within a multimedia system: the design of a speech generator to facilitate coordination between media, and extensions to the functionality of a written language generation system to produce natural speech output. We demonstrate how a speech generator can produce information that allows for temporal coordination between multiple media. We describe how our speech generator takes advantage of rich and accurate syntactic and semantic information during text planning and speech realization. This enables the system to accurately predict, generate, and utilize prosodic features to facilitate coordination of speech with graphical actions such as highlighting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-59"
  },
  "hirose96_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Mayumi",
     "Sakata"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ]
   ],
   "title": "Synthesizing dialogue speech of Japanese based on the quantitative analysis of prosodic features",
   "original": "i96_0378",
   "page_count": 4,
   "order": 62,
   "p1": "378",
   "pn": "381",
   "abstract": [
    "Through the analyses of fundamental frequency contours and speech rates of dialogue speech and also of read speech, prosodic rules were derived for the synthesis of spoken dialogue. As for the fundamental frequency contours, they were first decomposed into phrase and accent components based on the superpositional model, and then their command magnitudes/amplitudes were analyzed by the method of multiple regression analysis. As for the speech rate, the reduction rate of mora duration from reading-style to dialogue-style was calculated. After normalizing the sentence length, the mean reduction rate was calculated as an average over utterances without complicated syntactic structure. Results of the above analyses were incorporated in the prosodic rules for dialog speech synthesis. Using a formerly developed formant speech synthesizer, synthesis was conducted using both the former rules of read speech and the newly developed rules. A hearing test showed that the new rules can produce better prosody as dialogue speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-60"
  },
  "tanaka96_icslp": {
   "authors": [
    [
     "Shuichi",
     "Tanaka"
    ],
    [
     "Shu",
     "Nakazato"
    ],
    [
     "Keiichiro",
     "Hoashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Spoken dialogue interface in a dual task situation",
   "original": "i96_0382",
   "page_count": 4,
   "order": 63,
   "p1": "382",
   "pn": "385",
   "abstract": [
    "In this paper, we examined the effects of the spoken dialogue interface in a dual task situation. One of the well suited situations for the use of the spoken dialogue interface is that including dual tasks, e.g. when several tasks are being carried on at the same time. In order to develop a spoken dialogue system, it is important to consider such a situation and the effects of the spoken dialogue interface in it. It has been said that the merit of spoken dialogue interface is that it doesn't make the user's manual and visual modalities busy. Therefore, it is expected that this interface is effective in a dual task situation. We made experiments to prove this hypothesis. As the result of these experiments, it became clear that spoken dialogue interface was effective in a dual task situation and that considering the situation is important for the design and evaluation of the interface.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-61"
  },
  "niimi96_icslp": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Yutaka",
     "Kobayashi"
    ]
   ],
   "title": "A dialogue control strategy based on the reliability of speech recognition",
   "original": "i96_0534",
   "page_count": 4,
   "order": 64,
   "p1": "534",
   "pn": "537",
   "abstract": [
    "This paper considers a dialog control strategy based on the reliability of speech recognition. The spoken dialog system using this strategy accepts an utterance of the user if the reliability is high, while it rejects the utterance and asks the user to speak again (the basic strategy), or confirms the content of the utterance if the reliability is low. The purpose of this paper is to estimate two quantities Pac and N, given the performance of the speech recognizer used in a dialog system. Pac is the probability that information included in user's utterance is conveyed to the system correctly, and N is the average number of turns taken between the user and the system until the subdialog on user's first utterance terminates. The analysis has proven that the direct confirmation can increase Pac and the indirect confirmation can reduce N in comparison with the basic strategy. The paper also reported a numerical evaluation of the dialog control strategy conducted by using a real speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-62"
  },
  "rudnicky96_icslp": {
   "authors": [
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Stephen",
     "Reed"
    ],
    [
     "Eric H.",
     "Thayer"
    ]
   ],
   "title": "Speechwear: a mobile speech system",
   "original": "i96_0538",
   "page_count": 4,
   "order": 65,
   "p1": "538",
   "pn": "541",
   "abstract": [
    "We describe a system that allows ambulating users to perform data entry and retrieval using a speech interface to a wearable computer. The interface is a speech-enabled Web browser that allows the user to access both locally stored documents as well as remote ones through a wireless link.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-63"
  },
  "meng96_icslp": {
   "authors": [
    [
     "Helen",
     "Meng"
    ],
    [
     "Senis",
     "Busayapongchai"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "David",
     "Goddeau"
    ],
    [
     "Lee",
     "Hetherington"
    ],
    [
     "Edward",
     "Hurley"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "WHEELS: a conversational system in the automobile classifieds domain",
   "original": "i96_0542",
   "page_count": 4,
   "order": 66,
   "p1": "542",
   "pn": "545",
   "abstract": [
    "WHEELS is a conversational system which provides access to a database of eletronic automobile classified advertisements. It leverages off the existing spoken language technologies from our GALAXY system, and enables users to search through a database of 5,000 automobile classifieds. The current end-to-end system can respond to spoken or typed inputs, and produces a short list of entries meeting the constraints specified by the user. The system operates in mixed-initiative mode, asking for specific information but not requiring compliance. The output information is conveyed to the user with visual tables and synthesized speech. This system incorporates a new type of category bigram, created with the innovative use of the natural language component. Future plans to extend the system include operating in a displayless mode, and porting the system to Spanish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-64"
  },
  "sadek96_icslp": {
   "authors": [
    [
     "M. D.",
     "Sadek"
    ],
    [
     "A.",
     "Ferrieux"
    ],
    [
     "A.",
     "Cozannet"
    ],
    [
     "P.",
     "Bretier"
    ],
    [
     "F.",
     "Panaget"
    ],
    [
     "J.",
     "Simonin"
    ]
   ],
   "title": "Effective human-computer cooperative spoken dialogue: the AGS demonstrator",
   "original": "i96_0546",
   "page_count": 4,
   "order": 67,
   "p1": "546",
   "pn": "549",
   "abstract": [
    "This paper describes a spoken dialogue system based on a generic specification of a cooperative communicating rational agent. We present some theoretical and practical apects of the overall approach, along with the speech-specific and natural language related issues raised by the effective implementation of the system. An account is also given of an evaluation of the system with naive users on the task of voice services directory (AGS) inquiry.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-65"
  },
  "bennacef96_icslp": {
   "authors": [
    [
     "S. K.",
     "Bennacef"
    ],
    [
     "L.",
     "Devillers"
    ],
    [
     "S.",
     "Rosset"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Dialog in the RAILTEL telephone-based system",
   "original": "i96_0550",
   "page_count": 4,
   "order": 68,
   "p1": "550",
   "pn": "553",
   "abstract": [
    "Dialog management is of particular importance in telephone-based services. In this paper we describe our recent activities in dialog management and natural language generation in the LIMSI RAILTEL system for access to rail travel information. The aim of LEMLAP project RAILTEL was to assess the capabilities of spoken language technology for interactive telephone information services. Because all interaction is over the telephone, oral dialog management and response generation are very important aspects of the overall system design and usability. Each dialog is analysed to determine the source of any errors (speech recognition, understanding, information retreival, processing, or dialog management). An analysis is provided for 100 dialogs taken from the RAILTEL field trials with naive subjects accessing timetable information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-66"
  },
  "lavie96_icslp": {
   "authors": [
    [
     "Alon",
     "Lavie"
    ],
    [
     "Lori",
     "Levin"
    ],
    [
     "Yan",
     "Qu"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "Donna",
     "Gates"
    ],
    [
     "Marsal",
     "Gavaldà"
    ],
    [
     "Laura",
     "Mayfield"
    ],
    [
     "Maite",
     "Taboada"
    ]
   ],
   "title": "Dialogue processing in a conversational speech translation system",
   "original": "i96_0554",
   "page_count": 4,
   "order": 69,
   "p1": "554",
   "pn": "557",
   "abstract": [
    "Attempts at discourse processing of spontaneously spoken dialogue face several difficulties: multiple hypotheses that result from the parsers attempts to make sense of the output from the speech recognizer, ambiguity that results from segmentation of multi-sentence utterances, and cumulative error - errors in the discourse context which cause further errors when subsequent sentences are processed. In this paper we will describe our robust parsers, our procedures for segmenting long utterances, and two approaches to discourse processing that attempt to deal with ambiguity and cumulative error.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-67"
  },
  "niesler96_icslp": {
   "authors": [
    [
     "T. R.",
     "Niesler"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Combination of word-based and category-based language models",
   "original": "i96_0220",
   "page_count": 4,
   "order": 70,
   "p1": "220",
   "pn": "223",
   "abstract": [
    "A language model combining word-based and category-based ngrams within a backoff framework is presented. Word n-grams conveniently capture sequential relations between particular words, while the category-model, which is based on part-of-speech classifications and allows ambiguous category membership, is able to generalise to unseen word sequences and therefore appropriate in backoff situations. Experiments on the LOB, Switchboard and WSJ0 corpora demonstrate that the technique greatly improves language model perplexities for sparse training sets, and offers significantly improved complexity versus performance tradeoffs when compared with standard trigram models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-68"
  },
  "valverdealbacete96_icslp": {
   "authors": [
    [
     "Francisco J.",
     "Valverde-Albacete"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "A multi-level lexical-semantics based language model design for guided integrated continuous speech recognition",
   "original": "i96_0224",
   "page_count": 4,
   "order": 71,
   "p1": "224",
   "pn": "227",
   "abstract": [
    "We present a continuous speech recognition architecture with a tightly coupled language model that tries to improve the dwindling performance of the normal stack decoder with increasing lexicon size. We solve the problem of recognition by means of two mutually recursive functions. The first one uses an auxiliary retrieval function to obtain lexicalized (already built) solutions to the problem, and merges these solutions with the ones built by the second function. This second one describes the acoustical and semantic recognition process as a search problem defined with the help of the first function, and solved with the help of the A* strategy. As a linguistic model, we use a hierarchy of linguistic levels each of which has a particular meaning structure, a lexicon of lexicalized forms, their lexicalization probabilities, and a local lexical grammar describing how the semantic categories of the level can be built. The process can further be optimized if targets, constraints on the possible solutions, are given to the recognition process to guide and restrict it. Target guidance implies a mechanism for target focusing, locally matching targets to the recognition state, and target prediction with the help of a lexical local grammar. We are testing the architecture in a DARPA RM-like application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-69"
  },
  "gallwitz96_icslp": {
   "authors": [
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "A category based approach for recognition of out-of-vocabulary words",
   "original": "i96_0228",
   "page_count": 4,
   "order": 72,
   "p1": "228",
   "pn": "231",
   "abstract": [
    "In almost all applications of automatic speech recognition, especially in spontaneous speech tasks, the recognizer vocabulary cannot cover all occurring words. There is always a significant amount of out-of-vocabulary words even when the vocabulary size is very large. In this paper we present a new approach for the integration of out-of-vocabulary words into statistical language models. We use category information for all words in the training corpus to define a function that gives an approximation of the out-of-vocabulary word emission probability for each word category. This information is integrated into the language models. Although we use a simple acoustic model for out-of-vocabulary words, we achieve a 6% reduction of word error rate on spontaneous speech data with about 5% out-of-vocabulary rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-70"
  },
  "seymore96_icslp": {
   "authors": [
    [
     "Kristie",
     "Seymore"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Scalable backoff language models",
   "original": "i96_0232",
   "page_count": 4,
   "order": 73,
   "p1": "232",
   "pn": "235",
   "abstract": [
    "When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff models perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-71"
  },
  "iyer96_icslp": {
   "authors": [
    [
     "R.",
     "Iyer"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Modeling long distance dependence in language: topic mixtures vs. dynamic cache models",
   "original": "i96_0236",
   "page_count": 4,
   "order": 74,
   "p1": "236",
   "pn": "239",
   "abstract": [
    "In this paper,we investigate a new statistical language model which captures topic-related dependencies of words within and across sentences. First, we develop a sentence-level mixture language model that takes advantage of the topic constraints in a sentence or article. Second, we introduce topic-dependent dynamic cache adaptation techniques in the framework of the mixture model. Experiments with the static (or unadapted) mixture model on the 1994 WSJ task indicated a 21% reduction in perplexity and a 3-4% improvement in recognition accuracy over a general n-gram model. The static mixture model also improved recognition performance over an adapted n-gram model. Mixture adaptation techniques contributed a further 14%reduction in perplexity and a small improvement in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-72"
  },
  "federico96_icslp": {
   "authors": [
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Bayesian estimation methods for n-gram language model adaptation",
   "original": "i96_0240",
   "page_count": 4,
   "order": 75,
   "p1": "240",
   "pn": "243",
   "abstract": [
    "Stochastic n-gram language models have been successfully applied in continuous speech recognition for several years. Such language models provide many computational advantages but also require huge text corpora for parameter estimation. Moreover, the texts must exactly reflect, in a statistical sense, the user's language. Estimating a language model on a sample that is not representative severely affects speech recognition performance. A solution to this problem is provided by the Bayesian learning framework. Beyond the classical estimates, a Bayes derived interpolation model is proposed. Empirical comparisons have been carried out on a 10,000-word radiological reporting domain. Results are provided in terms of perplexity and recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-73"
  },
  "siu96_icslp": {
   "authors": [
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Modeling disfluencies in conversational speech",
   "original": "i96_0386",
   "page_count": 4,
   "order": 76,
   "p1": "386",
   "pn": "389",
   "abstract": [
    "Conversational speech is notably different from read speech in several ways, particularly in the presence of disfluencies but also in the frequent use of a small set of words that mark the flow of the discourse. Disfluencies are sometimes viewed as a \\problem\" in language modeling, where most previous work has focused on written text. In this paper, we take the view that disfluencies provide information themselves. In particular, we give evidence that filled pauses serve different functions, including marking linguistic unit and restart boundaries, and signaling hesitation where the speaker wants to hold the floor. The different functions can be connected to similar functions of other words common in spontaneous but not written speech, and the particular function affects the word conditioning choices in a variable n-gram model. Thus, at least some of the idiosyncrasies of spontaneous speech can be viewed as a source of information for language modeling rather than an interruption in the linguistic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-74"
  },
  "miller96_icslp": {
   "authors": [
    [
     "John",
     "Miller"
    ],
    [
     "Fil",
     "Alleva"
    ]
   ],
   "title": "Evaluation of a language model using a clustered model backoff",
   "original": "i96_0390",
   "page_count": 4,
   "order": 77,
   "p1": "390",
   "pn": "393",
   "abstract": [
    "In this paper, we describe and evaluate a language model using word classes automatically generated from a word clustering algorithm. Class based language models have been shown to be effective for rapid adaptation, training on small datasets, and reduced memory usage. In terms of model perplexity, prior work has shown diminished returns for class based language models constructed using very large training sets. This paper describes a method of using a class model as a backoff to a bigram model which produced significant benefits even when trained from a large text corpus. Tests results on the Whisper continuous speech recognition system show that for a given word error rate, the clustered bigram model uses 2/3 fewer parameters compared to a standard bigram model using unigram backoff.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-75"
  },
  "bonafonte96_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "José B.",
     "Mariño"
    ]
   ],
   "title": "Language modeling using x-grams",
   "original": "i96_0394",
   "page_count": 4,
   "order": 78,
   "p1": "394",
   "pn": "397",
   "abstract": [
    "In this paper, an extension of n-grams is proposed. In this extension, the memory of the model (n) is not fixed a priori. Instead, first, large memories are accepted and afterwards, merging criteria are applied to reduce complexity and to ensure reliable estimations. The results show how the perplexity obtained with x-grams is smaller than that of n-grams. Furthermore, the complexity is smaller than trigrams and can become close to bigrams.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-76"
  },
  "ries96_icslp": {
   "authors": [
    [
     "Klaus",
     "Ries"
    ],
    [
     "Finn Dag",
     "Buo"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Class phrase models for language modelling",
   "original": "i96_0398",
   "page_count": 4,
   "order": 79,
   "p1": "398",
   "pn": "401",
   "abstract": [
    "Previous attempts to automatically determine multi-words as the basic unit for language modeling have been successful for extending bigram models [10, 9, 2, 8] to improve the perplexity of the language model and/or the word accuracy of the speech decoder. However, none of these techniques gave improvements over the trigram model so far, except for the rather controlled ATIS task [8]. We therefore propose an algorithm, that minimizes the perplexity improvement of a bigram model directly. The new algorithm is able to reduce the trigram perplexity and also achieves word accuracy improvements in the Verbmobil task. It is the natural counterpart of successful word classification algorithms for language modeling [4, 7] that minimize the leaving-one-out bigram perplexity. We also give some details on the usage of class finding techniques and m-gram models, which can be crucial to successful applications of this technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-77"
  },
  "geutner96_icslp": {
   "authors": [
    [
     "Petra",
     "Geutner"
    ]
   ],
   "title": "Introducing linguistic constraints into statistical language modeling",
   "original": "i96_0402",
   "page_count": 4,
   "order": 80,
   "p1": "402",
   "pn": "405",
   "abstract": [
    "Building robust stochastic language models is a major issue in speech recognition systems. Conventional word-based n-gram models do not capture any linguistic constraints inherent in speech. In this paper the notion of function and content words (open/closed word classes) is used to provide linguistic knowledge that can be incorporated into language models. Function words are articles, prepositions, personal pronouns - content words are nouns, verbs, adjectives and adverbs. Based on this class definition resulting in function and content word markers, a new language model is defined. A combination of the word-based model with this new model will be introduced. The combined model shows modest improvements both in perplexity results and recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-78"
  },
  "hu96_icslp": {
   "authors": [
    [
     "Jianying",
     "Hu"
    ],
    [
     "William",
     "Turin"
    ],
    [
     "Michael K.",
     "Brown"
    ]
   ],
   "title": "Language modeling with stochastic automata",
   "original": "i96_0406",
   "page_count": 4,
   "order": 81,
   "p1": "406",
   "pn": "409",
   "abstract": [
    "It is well known that language models are effective for increasing accuracy of speech and handwriting recognizers, but large language models are often required to achieve low model perplexity (or entropy) and still have adequate language coverage. We study three efficient methods for stochastic language modeling in the context of the stochastic pattern recognition problem and give results of a comparative performance analysis. In addition we show that a method which combines two of these language modeling techniques yields even better performance than the best of the single techniques tested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-79"
  },
  "sun96_icslp": {
   "authors": [
    [
     "Don X.",
     "Sun"
    ]
   ],
   "title": "Feature dimension reduction using reduced-rank maximum likelihood estimation for hidden Markov models",
   "original": "i96_0244",
   "page_count": 4,
   "order": 82,
   "p1": "244",
   "pn": "247",
   "abstract": [
    "This paper presents a new method of feature dimension reduction in hidden Markov modeling (HMM) for speech recognition. The key idea is to apply reduced rank maximum likelihood estimation in the M-step of the usual Baum-Welch algorithm for estimating HMM parameters such that the estimates of the Gaussian distribution parameters are restricted in a sub-space of reduced dimensionality. There are two main advantages of applying this method in HMM: 1) feature dimension reduction is achieved simultaneously with the estimation of HMM parameters, therefore it guarantees that the likelihood function is monotonically increasing; 2) it requires very little extra computation in addition to the standard Baum-Welch algorithm, hence it can be easily incorporated in the existing speech recognition systems using HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-80"
  },
  "hubener96b_icslp": {
   "authors": [
    [
     "Kai",
     "Hübener"
    ]
   ],
   "title": "Using multi-level segmentation coefficients to improve HMM speech recognition",
   "original": "i96_0248",
   "page_count": 4,
   "order": 83,
   "p1": "248",
   "pn": "251",
   "abstract": [
    "This paper presents a new kind of acoustic features for HMM speech recognition. These features try to capture phone-specific segmentation information using multiple temporal resolutions. Experiments show that word accuracy can be improved by 7% when combining these features with traditional mel-cepstral coefficients in a speaker-independent word recogniser. This improvement is mostly due to a reduced number of insertion and deletion errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-81"
  },
  "eisele96_icslp": {
   "authors": [
    [
     "T.",
     "Eisele"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "D.",
     "Langmann"
    ]
   ],
   "title": "A comparative study of linear feature transformation techniques for automatic speech recognition",
   "original": "i96_0252",
   "page_count": 4,
   "order": 84,
   "p1": "252",
   "pn": "255",
   "abstract": [
    "Although widely used, there are still open questions concerning which properties of Linear Discriminant Analysis (LDA) do account for its success in many speech recognition systems. In order to gain more insight into the nature of the transformation we compare LDA with mel-cepstral feature vectors with respect to the following criteria: decorrelation and ordering property, invariance under linear transforms, automatic learning of dynamical features, and data dependence of the transformation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-82"
  },
  "milner96_icslp": {
   "authors": [
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Inclusion of temporal information into features for speech recognition",
   "original": "i96_0256",
   "page_count": 4,
   "order": 85,
   "p1": "256",
   "pn": "259",
   "abstract": [
    "Conventional methods for incorporating temporal information into speech features apply regression to a series of successive cepstral vectors to generate differential cepstra, or apply a cosine transform to generate cepstral-time matrices. This paper aims to generalise these techniques such that a series of stacked cepstral vectors is multiplied by a temporal transform matrix to produce the final speech feature. This can made to incorporate both static and dynamic speech information. Using this method, the coding of temporal information is not restricted to regression or cosine coefficients - any suitable transform may used. Results are presented for a variety of transforms, such as Legendre, Karhunen-Loeve, Cosine, Rectangle, where it is shown that the transform based techniques offer higher performance than conventional differential cepstrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-83"
  },
  "wassner96_icslp": {
   "authors": [
    [
     "Hubert",
     "Wassner"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "New cepstral representation using wavelet analysis and spectral transformation for robust speech recognition",
   "original": "i96_0260",
   "page_count": 4,
   "order": 86,
   "p1": "260",
   "pn": "263",
   "abstract": [
    "The goal is to improve recognition rate by optimisation of Mel Frequency Cepstral Coefficients (MFCCs): modifications concern the time-frequency representations used to estimate these coefficients. There are many ways to obtain a spectrum out of a signal which differ in the method itself (Fourier, Wavelets,...), and in the normalisation. We show here that we can obtain noise resistant cepstral coefficients, for speaker independent connected word recognition.The recognition system is based on a continuous whole word hidden Markov model. An error reduction rate of approximately 50% is achieved with word models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-84"
  },
  "long96_icslp": {
   "authors": [
    [
     "C. J.",
     "Long"
    ],
    [
     "S.",
     "Datta"
    ]
   ],
   "title": "Wavelet based feature extraction for phoneme recognition",
   "original": "i96_0264",
   "page_count": 4,
   "order": 87,
   "p1": "264",
   "pn": "267",
   "abstract": [
    "In an effort to provide a more efficient representation of the acoustical speech signal in the pre-classification stage of a speech recognition system, we consider the application of the Best-Basis Algorithm of Coifman and Wickerhauser. This combines the advantages of using a smooth, compactly-supported wavelet basis with an adaptive time-scale analysis dependent on the problem at hand. We start by briefly reviewing areas within speech recognition where the Wavelet Transform has been applied with some success. Examples include pitch detection, formant tracking, phoneme classification. Finally, our wavelet based feature extraction system is described and its performance on a simple phonetic classification problem given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-85"
  },
  "drygajlo96_icslp": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "New fast wavelet packet transform algorithms for frame synchronized speech processing",
   "original": "i96_0410",
   "page_count": 4,
   "order": 88,
   "p1": "410",
   "pn": "413",
   "abstract": [
    "In this paper we present orthogonal overlapped block transforms as a frame synchronized signal analysis tool with the capability of arbitrary multiresolution time-spectral decomposition of speech signals. Our prime interest is in the representation of nonstationary discrete-time signals in terms of wavelet packets, and we concentrate on their fast transform algorithms. Wavelet packet representations provide a local time-spectral description which reveals the nonstationary nature of a signal. They allow the speech signal to be accurately parameterised for such applications as speech and speaker recognition, where a front-end is responsible for the frame synchronized feature extraction. In this case the fast overlapped block transform algorithms represent an elegant and efficient solution to the implementation of wavelet packet transforms, since their FFT-like lattice block structure provides all possible multiresolution time-spectral coefficients. The frame synchronization is also preserved in subbands which allows a new subband-based approach for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-86"
  },
  "umesh96_icslp": {
   "authors": [
    [
     "S.",
     "Umesh"
    ],
    [
     "L.",
     "Cohen"
    ],
    [
     "N.",
     "Marinovic"
    ],
    [
     "D.",
     "Nelson"
    ]
   ],
   "title": "Frequency-warping in speech",
   "original": "i96_0414",
   "page_count": 4,
   "order": 89,
   "p1": "414",
   "pn": "417",
   "abstract": [
    "In this paper we present results that indicate that the formant frequencies between different speakers scale differently at different frequencies. Based on our experiments on speech data, we then numerically compute a universal frequency-warping function, to make the scale-factor independent of frequency in the warped domain. The proposed warping function is found to be similar to the mel-scale, which has previously been derived from purely psycho-acoustic experiments. The motivation for the present experiments stems from our recently proposed use of scale-transform based cepstral coefficients [6] as acoustic features, since they provide superior separability of vowels than mel-cepstral coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-87"
  },
  "kobayashi96_icslp": {
   "authors": [
    [
     "Daisuke",
     "Kobayashi"
    ],
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Extracting speech features from human speech-like noise",
   "original": "i96_0418",
   "page_count": 4,
   "order": 90,
   "p1": "418",
   "pn": "421",
   "abstract": [
    "Human speech-like noise (HSLN) is a kind of bubble noise generated by superimposing independent speech signals typically more than one thousand times. Since the basic feature of HSLN varies from that of overlapped speech to stationary noise with keeping long time spectra in the same shape, we investigate perceptual discrimination of speech from stationary noise and its acoustic correlates using HSLN of various numbers of superposition. First we con- firm the perceptual score, i.e. how much the HSLN sounds like stationary noise, and that the number of superposition of HSLN is proportional by subjective tests. Then, we show that the amplitude distribution of difference signal of HSLN approaches the Gaussian distribution from the Gamma distribution as the number of superposition increases. The other subjective test to perceive three HSLN of different dynamic characteristics clarifys that the temporal change of spectral envelope plays an important roll in discriminating speech from noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-88"
  },
  "kajita96_icslp": {
   "authors": [
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Subband-crosscorrelation analysis for robust speech recognition",
   "original": "i96_0422",
   "page_count": 4,
   "order": 91,
   "p1": "422",
   "pn": "425",
   "abstract": [
    "This paper describes subband-crosscorrelation (SBXCOR) analysis using two channel signals. The SBXCOR analysis is an extended signal processing technique of subband-autocorrelation (SBCOR) analysis that extracts periodicities present in speech signals. In this paper, the performance of SBXCOR is investigated using a DTW word recognizer, under simulated acoustic conditions on computer and a real environmental condition. Under the simulated condition, it is assumed that speech signals in each channel are perfectly synchronized while noises are not correlated. Consequently, the effective signal-to-noise ratio of the signal generated by simply summing the two signals is raised about 3dB. In such a case, it is shown that SBXCOR is less robust than SBCOR extracted from the two-channel-summed signal, but more robust than the conventional one-channel SBCOR. The resultant performance was much better than that of smoothed group delay spectrum and mel-frequency cepstral coefficient. In a real computer room, it is shown that SBXCOR is more robust than the two-channel-summed SBCOR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-89"
  },
  "bourlard96_icslp": {
   "authors": [
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Stéphane",
     "Dupont"
    ]
   ],
   "title": "A new ASR approach based on independent processing and recombination of partial frequency bands",
   "original": "i96_0426",
   "page_count": 4,
   "order": 92,
   "p1": "426",
   "pn": "429",
   "abstract": [
    "In the framework of hidden Markov models (HMM) or hybrid HMM/Artificial Neural Network (ANN) systems, we present a new approach towards automatic speech recognition (ASR). The general idea is to split the whole frequency band (represented in terms of critical bands) into a few sub-bands on which different recognizers are independently applied and then recombined at a certain speech unit level to yield global scores and a global recognition decision. The preliminary results presented in this paper show that such an approach, even using quite simple recombination strategies, can yield at least comparable performance on clean speech while providing better robustness in the case of noisy speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-90"
  },
  "nadeu96_icslp": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "José B.",
     "Mariño"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "Frequency and time filtering of filter-bank energies for HMM speech recognition",
   "original": "i96_0430",
   "page_count": 4,
   "order": 93,
   "p1": "430",
   "pn": "433",
   "abstract": [
    "In speech recognition, a discriminative quefrency weighting can be achieved by somewhat decorrelating the frequency sequence of log mel-scaled filter-bank energies with a computationally inexpensive filter. In this paper, we show how the spectral parameters that result from this kind of frequency filtering, both alone and combined with filtering of their time trajectories, are competitive with respect to the conventional cepstral representations of speech signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-91"
  },
  "laprie96_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Marie-Odile",
     "Berger"
    ]
   ],
   "title": "Extraction of tongue contours in x-ray images with minimal user interaction",
   "original": "i96_0268",
   "page_count": 4,
   "order": 94,
   "p1": "268",
   "pn": "271",
   "abstract": [
    "In spite of the development of new imaging techniques, X-ray images still keep a prominent place to studying articulatory phenomena. Indeed, they are still unsurpassed to obtain an overall view of the moving vocal tract. However X-ray images require that articulators contours are extracted by hand which is a tedious task. This paper describes an approach towards the automatization of the tongue contour extraction. The \"Snake\"method introduced in computer vision to extract contours is unable alone to achieve the task. Therefore we make \"Snakes\" cooperate with an optical flow method applied where contours are not sufficiently isolated from spurious contours. Our experiments have shown that the tongue is tracked successfully when it is visible, and that interaction with the user remains necessary when the tongue is obscured.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-92"
  },
  "demolin96_icslp": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "Thierry",
     "Metens"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "Three-dimensional measurement of the vocal tract by MRI",
   "original": "i96_0272",
   "page_count": 32,
   "order": 95,
   "p1": "272",
   "pn": "275",
   "abstract": [
    "Recent studies have shown that MRI techniques are reliable to measure mid-sagittal cuts of the vocal tract. Three-dimensional images of the vocal tract have been proposed e.g. by Foldvick et al. [6], [7] but despite this advance, these images do not offer any accurate measurement of sagittal cuts or area functions. Advance in MRI performances allows to collect data much faster and with a better accuracy. In this paper we propose to use this new technology to make measurement of sagittal, coronal, coronal oblique and transversal cuts. The data collected allow accurate measurement of area at different points along the vocal tract. The different cuts obtained by this method are then used to make measurements of the vocal tract shape. The study focuses on French oral vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-93"
  },
  "gleason96_icslp": {
   "authors": [
    [
     "Philip",
     "Gleason"
    ],
    [
     "Betty",
     "Tuller"
    ],
    [
     "J. A. Scott",
     "Kelso"
    ]
   ],
   "title": "Syllable affiliation of final consonant clusters undergoes a phase transition over speaking rates",
   "original": "i96_0276",
   "page_count": 3,
   "order": 96,
   "p1": "276",
   "pn": "278",
   "abstract": [
    "Previous work (Tuller & Kelso, 1990) reported a change in syllable affiliation, which was reflected by a change in the relative phase, of glottal and lip movements, as the rate of speaking a VC syllable increased. That is /ip#ip#.../ becomes /pi#pi#.../ as speaking rate is systematically increased. Here we report a change in the syllable affiliation of part of a final consonant cluster; /opt#opt#.../ becomes /top#top#.../ at higher speaking rates. The relative phase between the time series generated by two supralaryngeal structures, the lip and tongue tip, was measured in three ways, by discrete Fourier transform, by the cospectrum, and by dynamic time warping. Speaking rate was controlled using a metronome that began at 1 Hz and increased to 3.25 Hz in 0.083 Hz steps every 12 repetitions. Subjects repeated the target word \"opt,\" \"hopped,\" or \"top\" in time with the metronome while movements of their tongue, tongue blade, lower lip, and jaw were tracked by an alternating magnetic field device, the Articulograph AG100. Phonetic analysis confirmed the presence of a transition from VCC to CVC syllable at higher speaking rates. The perceptual change corresponds to a statistically significant change in the measurements of relative phase, indicating that perceived syllable affiliation i s determined, to some extent, by relative timing of articulatory events. This effect is consistent across the five subjects whose results are reported here. Three methods of measuring relative timing are compared, and the theoretical issues behind these methods are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-94"
  },
  "lobo96_icslp": {
   "authors": [
    [
     "Arthur",
     "Lobo"
    ],
    [
     "Michael",
     "O'Malley"
    ]
   ],
   "title": "Towards a biomechanical model of the larynx",
   "original": "i96_0279",
   "page_count": 4,
   "order": 97,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "In this paper, the development of a large-displacement large-strain 3-D finite element model of the vocal fold is reported. A fold is discretized into 720 8-node brick elements with a total of 1001 nodes and 3003 displacement degrees of freedom. The structure has realistic dimensions and geometry. The model includes geometric and material nonlinearities. The geometric nonlinearity appears in the strain-displacement relation due to the second order displacement derivatives and the material nonlinearity refers to the constitutive law. The Mooney-Rivlin rubber material formulation for an anisotropic tissue medium is used to characterize the tissue rheology. The elasticity tensor and the stress tensor for the Total Lagrangian formulation are obtained from the partial derivatives of the strain energy density function (SEDF) with respect to the Green-Lagrange strain tensor. Incompressibility constraints have been added using a mixed displacement-pressure (1 constant pressure term) finite element - a hydrostatic pressure work term (Lagrange Multiplier) being added to the SEDF. The structure is subjected to a sinusoidally time-varying half cosine pressure profile applied on 117 medial surface nodes. The dynamic equilibrium equations are solved using an incremental-iterative strategy and the Newmark method of time integration for the implicit initial-boundary-value problem. The deformation of the vocal fold at various phases of the applied load was studied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-95"
  },
  "morlec96_icslp": {
   "authors": [
    [
     "Yann",
     "Morlec"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Vèronique",
     "Aubergé"
    ]
   ],
   "title": "Generating intonation by superposing gestures",
   "original": "i96_0283",
   "page_count": 4,
   "order": 98,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "A dynamic model for synthesizing intonation is presented. This model is based on the following assumptions: intonation is the result of superposed and independent prototypical gestures belonging to diverse linguistic levels: sentence, clause, group, subgroup... Prototypical movements are progressively stored in a prosodic lexicon and used by the speaker in given communication tasks. Our recent application of this model is an association of sequential neural networks (SNNs). Each dynamic module is in charge of the melodic prediction of a specific linguistic level. The resulting melody is the weighted sum of SNNs outputs. We presently focused on the sentence level. We built a corpus consisting of various length utterances pronounced with 6 attitudes. We then designed SNNs able to perform the expansion of prosodic sentence movement. Preliminary results show that these simple SNNs can give acceptable F0 prediction and keep essential features of each attitude whatever the syllabic length of the sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-96"
  },
  "kawahara96_icslp": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Hiroko",
     "Kato"
    ],
    [
     "J. C.",
     "Williams"
    ]
   ],
   "title": "Effects of auditory feedback on F0 trajectory generation",
   "original": "i96_0287",
   "page_count": 5,
   "order": 99,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "In this paper, a method is proposed to evaluate contributions of auditory feedback to speech F0 trajectory generation. This method is based on data obtained in a series of new auditory feedback experiments (TAF: transformed auditory feedback) in which quantitative measurements were taken of interactions between speech perception and production under natural speech conditions. Experimental results revealed that the effects on power spectra of F0 trajectories vary among subjects and that the maximum magnitude exceeds 10 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-97"
  },
  "burnett96_icslp": {
   "authors": [
    [
     "I. S.",
     "Burnett"
    ],
    [
     "J. J.",
     "Parry"
    ]
   ],
   "title": "On the effects of accent and language on low rate speech coders",
   "original": "i96_0291",
   "page_count": 4,
   "order": 100,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "Telecommunications networks are exposed to a plethora of accents and languages. Fundamental to current and future systems are low rate speech coders. This paper examines the problems associated with speech coding of different languages and accents. Our investigations show that most low-rate (8kb/s and below) speech coders show bias towards non-accented English. When the coders are used for heavily accented English or other languages, significant performance degradation is noted. This paper examines the reasons for such variations and some approaches for improving coder performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-98"
  },
  "pan96b_icslp": {
   "authors": [
    [
     "J. S.",
     "Pan"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "VQ codevector index assignment using genetic algorithms for noisy channels",
   "original": "i96_0295",
   "page_count": 4,
   "order": 101,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "The multi-mutation rates, multi-crossover rates and a scheme of reinitialization are applied to parallel genetic algorithm for assigning the codevector indices for noisy channels for the purpose of minimizing the distortion caused by bit errors. Experimental results based on the memoryless binary symmetric channel for any bit error demonstrate the robustness of this new approach compared with our previous work [1]. The property of multiple global optima is also emphasized in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-99"
  },
  "cawley96_icslp": {
   "authors": [
    [
     "Gavin C.",
     "Cawley"
    ]
   ],
   "title": "An improved vector quantization algorithm for speech transmission over noisy channels",
   "original": "i96_0299",
   "page_count": 3,
   "order": 102,
   "p1": "299",
   "pn": "301",
   "abstract": [
    "Vector quantisation (VQ) is a method widely used in low bit-rate coding and transmission of speech signals. Unfortunately, a single bit error in the transmitted index, due to noise in the transmission channel, could degrade perceived speech quality at the receiver quite dramatically, as the reference vector retrieved by the corrupted index may differ greatly from the vector corresponding to the intended index. The index assignment (IA) process (an NP-complete combinatorial optimisation problem) attempts to re-order the code book to minimise the effects of single-bit errors, but generally only at considerable computational expense. This paper presents an improved vector quantisation algorithm, based on Kohonen's Self-Organising Feature Map (K-SOFM), that jointly optimises the quantisation error and resistance to noise in the transmission channel. This is achieved using a neighbourhood function based on the Hamming distance between code book indices, rather than the normal Euclidean distance across a two dimensional feature map. As a result, similar reference vectors are recalled by indices with similar binary patterns, minimising the effect of errors in the transmitted index introduced by noise in the transmission channel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-100"
  },
  "murgia96_icslp": {
   "authors": [
    [
     "C.",
     "Murgia"
    ],
    [
     "G.",
     "Feng"
    ],
    [
     "A. Le",
     "Guyader"
    ],
    [
     "C.",
     "Quinquis"
    ]
   ],
   "title": "Very low delay and high quality coding of 20 hz-15 khz speech signals at 64 kbit/s",
   "original": "i96_0302",
   "page_count": 4,
   "order": 103,
   "p1": "302",
   "pn": "305",
   "abstract": [
    "In this paper, an algorithm for coding 20 Hz - 15 kHz speech signals at 64 kbit/s with a very low delay (frame of 0.16 ms) is presented. To achieve a quality near to transparency, we propose adapting the Low-Delay CELP coder [1] to the 15 kHz bandwidth and suggest a new noise shaping method based on a psycho-acoustic model. In this way we take advantage of linear predictive coding and masking properties of the human perception system. Finally, an algebraic codebook is proposed, allowing an important reduction of coder computational complexity, without decreasing the perceived quality of signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-101"
  },
  "ribeiro96_icslp": {
   "authors": [
    [
     "Carlos M.",
     "Ribeiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Application of speaker modification techniques to phonetic vocoding",
   "original": "i96_0306",
   "page_count": 4,
   "order": 104,
   "p1": "306",
   "pn": "309",
   "abstract": [
    "The goal of the work described in this paper is to develop a very low bit rate vocoding scheme. The vocoder is a typical LPC vocoder, whose parameters are post-processed on a phone-by-phone basis, resulting in a variable bit rate segment vocoder. Given the well known speaker recognizability problems presented by vocoders at such low bit rates, we have attempted to integrate a speaker modification method based on altering the formant frequencies and bandwidths of vowel segments. This is done by transmitting the mean value and standard deviation of the radius and angle of the poles corresponding to formant frequencies for each phone. In the decoder stage, the phone index is used to retrieve a set of normalized values from a codebook of typical phones. This set is speaker adapted to preserve the static characteristics (average and standard deviation) but relies in the typical phone to represent the dynamic characteristics such as formant trajectories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-102"
  },
  "yonezaki96_icslp": {
   "authors": [
    [
     "Tadashi",
     "Yonezaki"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Entropy coded vector quantization with hidden Markov models",
   "original": "i96_0310",
   "page_count": 4,
   "order": 105,
   "p1": "310",
   "pn": "313",
   "abstract": [
    "We propose a new vector quantization approach, which consists of Hidden Markov Models(HMMs) and entropy coding scheme. The entropy coding system is determined depending on the speech status modeled by HMMs, so the proposing approach can adaptively allocate suitable numbers of bits to the codewords. This approach realizes about 0.3[dB] coding gain in cepstrum distance(8 states HMMs). In other words, 8 bit-codebook is represented by about 6.5 bits for average code length. We also research for robustness to the channel error. HMMs and the entropy coding system, which seem to be weak to the channel error, are augmented to be robust, so that the influence of the channel error is decreased into one-third.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-103"
  },
  "kohata96_icslp": {
   "authors": [
    [
     "Minoru",
     "Kohata"
    ]
   ],
   "title": "An application of recurrent neural networks to low bit rate speech coding",
   "original": "i96_0314",
   "page_count": 4,
   "order": 106,
   "p1": "314",
   "pn": "317",
   "abstract": [
    "It is well known that the LSP coefficient which represents the speech spectrum envelope as one of the linear prediction coefficients, shows a good performance of spectral interpolation along the time axis, but it is also known that the duration of interpolation is limited up to 20 ~ 30 ms. This limitation makes it difficult to reduce the bit rate in very low bit rate speech coding. To resolve this problem, recurrent neural networks (RNN) were applied to interpolate LSP coefficients, and it was possible to increase the duration of interpolation to about 100 ms without so much degradation of the synthesized speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-104"
  },
  "koishida96_icslp": {
   "authors": [
    [
     "Kazuhito",
     "Koishida"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "CELP coding system based on mel-generalized cepstral analysis",
   "original": "i96_0318",
   "page_count": 4,
   "order": 107,
   "p1": "318",
   "pn": "321",
   "abstract": [
    "This paper presents a CELP speech coding system based on mel-generalized cepstral analysis. In the mel-generalized cepstral analysis, we can vary the model spectrum continuously from AR to cepstral modeling by changing the value of a parameter y and we can choose an appropriate model spectrum. Furthermore, the spectrum represented by mel-generalized cepstrum has frequency resolution similar to that of human ear. Since the perceptual weighting and postfiltering are carried out through the mel-generalized cepstrum, we expect the perceptual performance of the proposed coder to be improved. The subjective performance test indicates that the quality of the proposed CELP coder is about 2 dB higher than that of the conventional one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-105"
  },
  "chan96_icslp": {
   "authors": [
    [
     "Cheung-Fat",
     "Chan"
    ],
    [
     "Wai-Kwong",
     "Hui"
    ]
   ],
   "title": "Wideband re-synthesis of narrowband CELP-coded speech using multiband excitation model",
   "original": "i96_0322",
   "page_count": 4,
   "order": 108,
   "p1": "322",
   "pn": "325",
   "abstract": [
    "In this paper, a method for improving the quality of narrowband CELP-coded speech is present. The approach is to reduce the hoarse voice in CELP-coded speech by enhancing the pitch periodicity in the reproduction signal and also to reduce the muffing characteristics of narrowband speech by regenerating the highband components of speech spectra from the reproduction signal. In the proposed method, multiband excitation (MBE) analysis is performed on the reproduction speech signal from a CELP decoder and the pitch periodicity is enhanced by resynthesizing the speech signal using a harmonic synthesizer according to the MBE model. The highband magnitude spectra are regenerated by matching to lowband spectra using a trained wideband spectral codebook. Information about the voiced/unvoiced (V/UV) excitation in the highband are derived from a training procedure and then stored alongside with the wideband spectral codebook so that they can be recovered by indexing to the codebook using the matched lowband index. Simulation results indicate that the quality of the wideband resynthesized speech is significantly improved over the narrowband CELP-coded speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-106"
  },
  "koizumi96_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Mikio",
     "Mori"
    ],
    [
     "Shuji",
     "Taniguchi"
    ],
    [
     "Mitsutoshi",
     "Maruya"
    ]
   ],
   "title": "Recurrent neural networks for phoneme recognition",
   "original": "i96_0326",
   "page_count": 4,
   "order": 109,
   "p1": "326",
   "pn": "329",
   "abstract": [
    "This paper deals with recurrent neural networks of multi-layer perceptron type which are well-suited for speech recognition, specially for phoneme recognition. The ability of these networks has been investigated by phoneme recognition experiments using a number of Japanese words uttered by a native male speaker in a quiet environment. Results of the experiments show that recognition rates achieved with these networks are higher than those obtained with conventional non-recurrent neural networks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-107"
  },
  "mokhtar96_icslp": {
   "authors": [
    [
     "M. A.",
     "Mokhtar"
    ],
    [
     "A.",
     "Zein-el-Abddin"
    ]
   ],
   "title": "A model for the acoustic phonetic structure of arabic language using a single ergodic hidden Markov model",
   "original": "i96_0330",
   "page_count": 4,
   "order": 110,
   "p1": "330",
   "pn": "333",
   "abstract": [
    "It is proposed to model the acoustic-phonetic structure of the Arabic language using a single ergodic hidden Markov model (HMM), since a single HMM (about 40-50 states) can be used to represent all acoustic phonetic effects. In this paper, we represent the techniques and algorithms used to perform that model, the problems associated with representing the whole acoustic-phonetic structure, the characteristics of the model, and how it performs as a phonetic decoder for recognition of fluent Arabic speech. The model is trained, segmented (manually and automatically), and labeled using a fixed number of phonemes, each of which has a direct correspondence to the states of the model. The model assumes that the observed spectral vectors were generated by a Gaussian source. The inherent variability of each phoneme is modeled as the observable random process of the Markov chain, while the phonotactic model of the unobservable phonetic sequence is represented by the state transition matrix of the HMM. The model incorporated the variable duration feature densities in each state. It is shown that the difficulties in developing an acoustic-phonetic model are due to the choice of the phonemes to be modeled, the selected parametrization of the data, and appropriate choice of the variant of the ergodic HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-108"
  },
  "gong96_icslp": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Modelling long term variability information in mixture stochastic trajectory framework",
   "original": "i96_0334",
   "page_count": 4,
   "order": 111,
   "p1": "334",
   "pn": "337",
   "abstract": [
    "The problem of acoustic modeling for speech recognizers is addressed. We distinguish two types of speech variability, long term (speaker identity, stationary noise, channel distortion) and short term (phoneme class). Currently, most recognizers model the two variabilities without considering their specificities, which may result in flat distributions with limited discriminability. In our system, the long term variability (environment) is modeled by a mixture model, where each mixture is modeled by a Mixture Stochastic Trajectory Model (MSTM). We propose the Environment Dependent Mixture Stochastic Trajectory Model (ED-MSTM) to model a set of environments. The parameters of ED-MSTM are estimated using the Maximum Likelihood (ML) estimation criterion by the Expectation-Maximisation (EM) algorithm. Our model has been tested on a 1011 word vocabulary, multi-speaker continuous French recognition task with noisy speech. In the experiments, we assume that speakers can be grouped into a pre-determined number of classes and that class label of a speaker is missing. The use of environmental modeling cut down the error rate produced by the multi-speaker system by about 15%, which is a statistically significant improvement. The idea of environment modeling is applicable to other acoustic modeling techniques such as Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-109"
  },
  "moudenc96_icslp": {
   "authors": [
    [
     "T.",
     "Moudenc"
    ],
    [
     "R.",
     "Sokol"
    ],
    [
     "Guy",
     "Mercier"
    ]
   ],
   "title": "Segmental phonetic features recognition by means of neural-fuzzy networks and integration in an n-best solutions post-processing",
   "original": "i96_0338",
   "page_count": 4,
   "order": 112,
   "p1": "338",
   "pn": "341",
   "abstract": [
    "In this paper, we present investigations on using segmental phonetic features in an N-best solutions post processing of an HMM based ASR system. These phonetic features are extracted by means of neural-fuzzy networks. Specialized neural-fuzzy networks are defined to recognize specific phonetic features (consonant/vowel, voiced/unvoiced, ...). Each of these neural networks furnishes a segmental coefficient (resulting from the output layers) which enables the computation of a segmental post-processing score for the N-best solutions of an HMM based ASR system. This post-processing is based on the computation of segmental score for each solution respectively under the hypotheses of a correct solution and an incorrect solution. Preliminary experiments were conducted on 3 speaker-independent telephone databases. An error rate reduction up to 20 % was achieved on the Digit corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-110"
  },
  "illina96_icslp": {
   "authors": [
    [
     "Irina",
     "Illina"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Stochastic trajectory model with state-mixture for continuous speech recognition",
   "original": "i96_0342",
   "page_count": 4,
   "order": 113,
   "p1": "342",
   "pn": "345",
   "abstract": [
    "The problem of acoustic modeling for continuous speech recognition is addressed. To deal with coarticulation effects and interspeaker variability, an extension of the Mixture Stochastic Trajectory Model (MSTM) is proposed. MSTMis a segment-based model using phonemes as speech units. In MSTM, the observations of a phoneme are modeled by a set of stochastic trajectories. The trajectories are modeled by a mixture of probability density functions (pdf) of state sequences. Each state is associated with a multivariate Gaussian density function. In this paper, we propose to replace the state single Gaussian pdf by a mixture of Gaussian pdfs (MSTM with State-Mixture, SM-MSTM). The parameters of the model are estimated under the ML criterion, using the Expectation-Maximisation (EM) algorithm. The tests of the system on a speaker-dependent continuous speech recognition task show a reduction in the word error rate by about 15% over the baseline MSTM, even for an equal number of parameters. Experiments based on a multispeaker continuous speech recognition task do not lead to signifi- cant improvement over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-111"
  },
  "hild96_icslp": {
   "authors": [
    [
     "Hermann",
     "Hild"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Recognition of spelled names over the telephone",
   "original": "i96_0346",
   "page_count": 4,
   "order": 114,
   "p1": "346",
   "pn": "349",
   "abstract": [
    "Recognition of spelled names over the telephone line is essential for applications such as telephone directory assistance, or automatic mail ordering. We present recognition results on the spelling section of the OGI Spelled and Spoken Word Telephone Corpus, using a Multi-State Time Delay Neural Network (MS-TDNN). Many applications allow for strong language modeling constraints. In our experiments we examined the beneficial effects of reducing the search space to a list of last names, ranging from about 1000 to 14 million entries. We compare tree search methods and show that significant improvements can be achieved by enriching the search trees with probabilities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-112"
  },
  "boulianne96_icslp": {
   "authors": [
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Patrick",
     "Kenny"
    ]
   ],
   "title": "Optimal tying of HMM mixture densities using decision trees",
   "original": "i96_0350",
   "page_count": 4,
   "order": 115,
   "p1": "350",
   "pn": "353",
   "abstract": [
    "Decision trees have been used in speech recognition with large numbers of context-dependent HMM models, to provide models for contexts not seen in training. Trees are usually created by successive node splitting decisions, based on how well a single Gaussian or Poisson density fits the data associated with a node. We introduce a new node splitting criterion, derived from the maximum likelihood fitting of the complex node distributions with Gaussian tied-mixture densities. We also carry the use of decision trees for tying HMM models a step further. In addition to questions about phonetic class of neighbouring phonemes,we allow questions about the HMM model state to be asked. The resulting decision tree maximizes the likelihood by adjusting the amount of parameter tying simultaneously across state and context. Accuracy improvement and model size reduction were evaluated on a gender-dependent 5K closed-vocabulary WSJ task, using the SI-84 and SI-284 training sets, for tied-mixture and continuous HMM models. The new decision trees are shown to reduce both error rate and model size, while being computationally cheap enough to allow consideration of two preceding and two following phones for the context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-113"
  },
  "choi96_icslp": {
   "authors": [
    [
     "Hwan Jin",
     "Choi"
    ],
    [
     "Yung Hwan",
     "Oh"
    ]
   ],
   "title": "Speech recognition using an enhanced FVQ based on a codeword dependent distribution normalization and codeword weighting by fuzzy objective function",
   "original": "i96_0354",
   "page_count": 4,
   "order": 116,
   "p1": "354",
   "pn": "357",
   "abstract": [
    "The paper presents a new variant of parameter estimation methods for discrete hidden Markov models(HMM) in speech recognition. This method makes use of a codeword dependent distribution normalization(CDDN) and a distance weighting by fuzzy contribution in dealing with the problems of robust state modeling in a FVQ based modeling. The proposed method is compared with the existing techniques using speaker-independent phonetically balanced isolated words recognition. The results have shown that the recognition rate of the proposed method is improved 4.5% over the conventional FVQ based method and the distance weighting to the smoothing of output probability is more efficient than the distance based codeword weighting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-114"
  },
  "kurimo96_icslp": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Panu",
     "Somervuo"
    ]
   ],
   "title": "Using the self-organizing map to speed up the probability density estimation for speech recognition with mixture density HMMs",
   "original": "i96_0358",
   "page_count": 4,
   "order": 117,
   "p1": "358",
   "pn": "361",
   "abstract": [
    "This paper presents methods to improve the probability density estimation in hidden Markov models for phoneme recognition by exploiting the Self-Organizing Map (SOM) algorithm. The advantage of using the SOM is based on the created approximative topology between the mixture densities by training the Gaussian mean vectors used as the kernel centers by the SOM algorithm. The topology makes the neighboring mixtures to respond strongly for the same inputs and so most of the nearest mixtures used to approximate the current observation probability will be found in the topological neighborhood of the \"winner\" mixture. Also the knowledge about the previous winners are used to speed up the the search for the new winners. Tree-search SOMs and segmental SOM training are studied aiming at faster search and suitability for HMM training. The framework for the presented experiments includes melcepstrum features and phoneme-wise tied mixture density HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-115"
  },
  "lang96_icslp": {
   "authors": [
    [
     "Carrie E.",
     "Lang"
    ],
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Temporal cues for vowels and universals of vowel inventories",
   "original": "i96_0434",
   "page_count": 4,
   "order": 118,
   "p1": "434",
   "pn": "437",
   "abstract": [
    "Stevens suggested that certain features of consonants are auditorily robust. Such features are abrupt; manifest in 10-30 msec, e.g., [voice], [nasal], [continuant]. Other features such as [palatalized], [pharyngealized] are less robust and are carried on top of (and presumably require more time to be manifested than) the robust features. Robust features are used first-and sometimes exclusively-by languages in constructing a consonant inventory. The less robust features may not be used at all but if they are, the language has already used features from the robust set. We sought to test a similar hypothesis regarding vowels: within the first few tens of msec. only as many vowel contrasts can be differentiated as are attested most commonly in languages of the world - something like (IPA) /i e a o u/. Analysis of the confusion matrices made by listeners identifying end-gated versions of 11 N. Am. English vowels lends support to the hypothesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-116"
  },
  "syrdal96_icslp": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "Acoustic variability in spontaneous conversational speech of american English talkers",
   "original": "i96_0438",
   "page_count": 4,
   "order": 119,
   "p1": "438",
   "pn": "441",
   "abstract": [
    "Speaker variability strongly impacts human perception and technology performance, yet large-scale, systematic study of the acoustic characteristics involved is rarely undertaken. This study provides statistics on selected segmental and suprasegmental acoustic parameters from measures made on spontaneous conversational telephone speech from 160 speakers in the Switchboard Corpus.[1] Since spontaneous conversational speech is more dynamically variable than read speech and is representative of actual human communication, it was preferred for our applied research purposes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-117"
  },
  "willerman96_icslp": {
   "authors": [
    [
     "Raquel",
     "Willerman"
    ],
    [
     "Patricia K.",
     "Kuhl"
    ]
   ],
   "title": "Cross-language speech perception: Swedish, English, and Spanish speakers' perception of front rounded vowels",
   "original": "i96_0442",
   "page_count": 4,
   "order": 120,
   "p1": "442",
   "pn": "445",
   "abstract": [
    "Cross-language research on adult speech perception demonstrates a strong effect of linguistic experience on consonant perception but not on vowel perception. Our paper re-examines the effect of linguistic experience on adults vowel perception. First, identification and goodness functions for the high front quadrant of the vowel space were mapped for speakers of Swedish, English, and Spanish. Second, speakers performed a discrimination task for one vector in this vowel space. Stimuli along this vector were identified by Swedish speakers as belonging to the Swedish front rounded vowel series /ç:/ - /ö:/. However, English and Spanish speakers reported that the stimuli were not in their language. Significant differences in discriminability of these stimuli were observed across speakers of different languages. Our results show that linguistic experience plays a significant role in vowel discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-118"
  },
  "ingram96_icslp": {
   "authors": [
    [
     "John C. L.",
     "Ingram"
    ],
    [
     "See-Gyoon",
     "Park"
    ]
   ],
   "title": "Inter-language vowel perception and production by Korean and Japanese listeners",
   "original": "i96_0446",
   "page_count": 4,
   "order": 121,
   "p1": "446",
   "pn": "449",
   "abstract": [
    "This paper investigates the influence of phonological learning upon the perception of non-native vowels. Four groups of Korean and Japanese English learners, at two levels of English experience, and a group of older monolingual Korean listeners were assessed on the perception and production of Australian English monophthongal front vowels: /i: w e æ a:/. Korean is of interest, because of a recent phonological merger of two front vowels (/e/ and /e/), which has produced a generation split among speakers of Seoul dialect above and below 45-50 years of age (Hong, 1991). The present study is the first reported case of how a phonemic merger, resulting in cross-generation differences within a speech community, can influence speakers' perception and production of non-native vowels. The effects of phonological learning on vowel perception were also observed in the tendency of the Japanese, but not the Korean listeners, to normalise tokens of non-native vowels for speaker-dependent durational variation, consistent with the respective phonological roles of vowel length in Japanese and Korean.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-119"
  },
  "kewleyport96_icslp": {
   "authors": [
    [
     "Diane",
     "Kewley-Port"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Intelligibility and acoustic correlates of Japanese accented English vowels",
   "original": "i96_0450",
   "page_count": 4,
   "order": 122,
   "p1": "450",
   "pn": "453",
   "abstract": [
    "To produce near-native American English (AE) vowels, Japanese speakers must extend their five vowel system with at least six new vowels. Three experiments have been conducted to acquire both perceptual and acoustic measures about Japanese accented English (JE)vowels. Six non-back vowels of AE in several phonetic environments were recorded from four Japanese male talkers with moderate English skills. Intelligibility was assessed by a panel of six Americans as the percent of JE vowels identified as intended. The first experiment was an open-set identification task. Two vowels, /i/ and /e/, were fully intelligible (>98%) while others ranked from 81% (/e/) to 23% (/./) intelligible. The second experiment used minimal-pair responses to assess intelligibility in terms of three acoustic properties of vowels, the spectral target, dynamic formants and duration. The results indicated that the spectral property was not communicated effectively, dynamics were partially effective, and duration was used effectively. In the third experiment a stepwise multiple linear regression analysis determined how these acoustic properties contributed to the intelligibility. For the two vowels analyzed, /w/ and /æ/, the direction and extent of the formant movement of JE vowels in relation to the AE targets was the vowel property that contributed the most to intelligibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-120"
  },
  "yoneyama96_icslp": {
   "authors": [
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "Segmentation strategies for spoken language recognition: evidence from semi-bilingual Japanese speakers of English",
   "original": "i96_0454",
   "page_count": 4,
   "order": 123,
   "p1": "454",
   "pn": "457",
   "abstract": [
    "The present study investigates speech segmentation of English by semi-bilingual Japanese speakers of English. Two experiments were conducted with forty subjects. The first experiment used a syllable-monitoring task and the results did not show a moraic segmentation strategy, which is a languagespecific segmentation strategy for native speakers of Japanese. This suggests that they employed a general or segment-bysegment sub-lexical segmentation strategy rather than a language-specific moraic segmentation strategy for accessing the lexicon. The second experiment used a phonememonitoring task and here the results supported a languagespecific moraic segmentation. Taken together, experiments suggest that semi-bilingual Japanese speakers have two segmentation strategies for spoken language recognition, supporting a model proposed by Cutler and colleagues. These results also suggest that these semi-bilinguals only partly suppress their native language-specific segmentation, which makes them comparable to the perfectly balanced French- English bilinguals in a similar study by Cutler and colleagues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-121"
  },
  "lee96b_icslp": {
   "authors": [
    [
     "Geunbae",
     "Lee"
    ],
    [
     "Jong-Hyeok",
     "Lee"
    ],
    [
     "Kyubong",
     "Park"
    ],
    [
     "Byung-Chang",
     "Kim"
    ]
   ],
   "title": "Integrating connectionist, statistical and symbolic approaches for continuous spoken Korean processing",
   "original": "i96_0458",
   "page_count": 4,
   "order": 124,
   "p1": "458",
   "pn": "461",
   "abstract": [
    "This paper presents a multi-strategic and hybrid approach for large-scale integrated speech and natural language processing, employing connectionist, statistical and symbolic techniques. The developed spoken Korean processing engine (SKOPE) integrates connectionist TDNN-based phoneme recognition technique with statistical Viterbi-based lexical decoding and symbolic morphological/phonological analysis techniques. The modular large-scale TDNNs are organized to recognize all 41 Korean phonemes using 10 component networks combined through 3 glue networks. In performance phase, continuously shifted TDNN outputs are integrated with HMM-based Viterbi decoding using a tree-structured lexicon. The Viterbi beam search is integrated with Korean morphotactics and phonological modeling, and produces a morpheme-graph for high-level parsing module. Currently, SKOPE shows average 76.2% phoneme spotting performance for all 41 Korean phonemes (including silence) from continuous speech signals and exhibits average 92.6% morpheme spotting performance from erroneous TDNN outputs after morphological analysis. Other extensive experiments verify that the multi-strategic approaches are promising for complex integrated speech and natural language processing, and the approaches can be extended to other morphologically complex agglutinative languages such as Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-122"
  },
  "hermansky96_icslp": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Sangita",
     "Timberwala"
    ],
    [
     "Misha",
     "Pavel"
    ]
   ],
   "title": "Towards ASR on partially corrupted speech",
   "original": "i96_0462",
   "page_count": 4,
   "order": 125,
   "p1": "462",
   "pn": "465",
   "abstract": [
    "A new highly parallel approach to automatic recognition of speech, inspired by early Fletchers research on Articulation Index, and based on independent probability estimates in several sub-bands of the available speech spectrum, is presented. The approach is especially suitable for situations when part of the spectrum of speech is corrupted. In such cases, it can yield an order-of-magnitude improvement in the error rate over a conventional full-band recognizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-123"
  },
  "gish96_icslp": {
   "authors": [
    [
     "Herbert",
     "Gish"
    ],
    [
     "Kenney",
     "Ng"
    ]
   ],
   "title": "Parametric trajectory models for speech recognition",
   "original": "i96_0466",
   "page_count": 4,
   "order": 126,
   "p1": "466",
   "pn": "469",
   "abstract": [
    "The basic motivation for employing trajectory models for speech recognition is that sequences of speech features are statistically dependent and that the effective and efficient modeling of the speech process will incorporate this dependency. In our previous work [1] we presented an approach to modeling the speech process with trajectories. In this paper we continue our development of parametric trajectory models for speech recognition. We extend our models to include time-varying covariances and describe our approach for defining a metric between speech segments based on trajectory models; it is important in developing mixture models of trajectories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-124"
  },
  "knill96_icslp": {
   "authors": [
    [
     "K. M.",
     "Knill"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Use of Gaussian selection in large vocabulary continuous speech recognition using HMMs",
   "original": "i96_0470",
   "page_count": 4,
   "order": 127,
   "p1": "470",
   "pn": "473",
   "abstract": [
    "This paper investigates the use of Gaussian Selection (GS) to reduce the state likelihood computation in HMM-based systems. These likelihood calculations contribute significantly (30 to 70%) to the computational load. Previously, it has been reported that when GS is used on large systems the recognition accuracy tends to degrade above a x3 reduction in likelihood computation. To explain this degradation, this paper investigates the trade-offs necessary between achieving good state likelihoods and low computation. In addition, the problem of unseen states in a cluster is examined. It is shown that further improvements are possible. For example, using a different assignment measure, with a constraint on the number of components per state per cluster, enabled the recognition accuracy on a 5k speaker-independent task to be maintained up to a x5 reduction in likelihood computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-125"
  },
  "hogberg96_icslp": {
   "authors": [
    [
     "J.",
     "Hogberg"
    ],
    [
     "Kare",
     "Sjölander"
    ]
   ],
   "title": "Cross phone state clustering using lexical stress and context",
   "original": "i96_0474",
   "page_count": 4,
   "order": 128,
   "p1": "474",
   "pn": "477",
   "abstract": [
    "This study deals with acoustic phonetic modelling in HMM based continuous speech recognition. Context dependent phone models were derived by a decision tree clustering algorithm. In particular, lexical stress was introduced as a clustering variable in addition to the phonetic context. The parameter sharing model was extended by tying HMM states across different target phones. For instance, one or more states of a tense vowel and the corresponding lax vowel were tied if they proved to be acoustically similar. The results indicate that the use of lexical stress information in acoustic modelling might be fruitful when large amounts of training data are available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-126"
  },
  "lleidasolano96_icslp": {
   "authors": [
    [
     "Eduardo",
     "Lleida-Solano"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Likelihood ratio decoding and confidence measures for continuous speech recognition",
   "original": "i96_0478",
   "page_count": 4,
   "order": 129,
   "p1": "478",
   "pn": "481",
   "abstract": [
    "Automatic speech recognition (ASR) systems are being integrated into a wider variety of tasks involving human{machine interaction. In evaluating these systems, however, it has become clear that more accurate means must be developed for detecting when portions of the decoded recognition hypotheses are either incorrect or represent out{of{vocabulary utterances. This paper describes the use of confidence measures based on likelihood ratio based optimization procedures for decoding and rescoring word hypotheses in an HMM based speech recognizer. These techniques are applied to spontaneous utterances obtained from a \\movie locator\" based dialog task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-127"
  },
  "ma96_icslp": {
   "authors": [
    [
     "Xiaohui",
     "Ma"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Yuqing",
     "Fu"
    ],
    [
     "Jiren",
     "Lu"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "A study on continuous Chinese speech recognition based on stochastic trajectory models",
   "original": "i96_0482",
   "page_count": 4,
   "order": 130,
   "p1": "482",
   "pn": "485",
   "abstract": [
    "This papers first introduces the theory of Stochastic Trajectory Models (STMs). STM represents the acoustic observations of a speech unit as clusters of trajectories in a parameter space. The trajectories are modeled by mixture of probability density functions of random sequence of states. Each state is associated with a multi-variate Gaussian density function, optimized at state sequence level. The effect of not using the HMM assumptions in STM is that STM can exploit information, such as time correlation within an observation sequence, which is hidden by HMM assumptions. After analyzing the characteristics of Chinese speech, the acoustic units for recognizing continuous Chinese speech taking advantage of Stochastic Trajectory Models are discussed and phone-like units, which are similar to or smaller than Initial-Final-like units, are suggested. The total number of the phone-like units (about 50) is the smallest in almost Chinese speech recognition system. Consequently, the training database can be very small. The performance of continuous Chinese speech recognition based on STM is studied using the VINICS system. The experimental results demonstrate the efficiency of STM and the consistency of phone-like units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-128"
  },
  "itoh96_icslp": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Jiro",
     "Kiyama"
    ],
    [
     "Hiroshi",
     "Kojima"
    ],
    [
     "Susumu",
     "Seki"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "A proposal for a new algorithm of reference interval-free continuous DP for real-time speech or text retrieval",
   "original": "i96_0486",
   "page_count": 4,
   "order": 131,
   "p1": "486",
   "pn": "489",
   "abstract": [
    "This paper proposes a new frame-synchronous algorithm for spotting similar intervals by comparing arbitrary intervals in a reference pattern sequence with arbitrary intervals in an input pattern sequence. The algorithm is called Reference Interval-free Continuous DP (RIFCDP) and the experimental results show that RIFCDP is successful in detecting the similar intervals between a reference pattern and an input. We have applied this algorithm to speech retrieval from a speech database and showed the possibility of real-time speech/text retrieval. The proposed algorithm can offer a wide range of applications such as digesting of continuous speech by checking the duplication of input data (same word utterance), and location identification of a mobile robot.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-129"
  },
  "ito96_icslp": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Masaki",
     "Kohda"
    ]
   ],
   "title": "Language modeling by string pattern n-gram for Japanese speech recognition",
   "original": "i96_0490",
   "page_count": 4,
   "order": 132,
   "p1": "490",
   "pn": "493",
   "abstract": [
    "This paper describes a new powerful statistical language model based on N-gram model for Japanese speech recognition. In English, a sentence is written word-by-word. On the other hand, a sentence in Japanese has no word boundary character. Therefore, a Japanese sentence requires word segmentation by morphemic analysis before the construction of word N-gram. We propose an N-gram based language model which requires no word segmentation. This model uses character string patterns as units of N-gram. The string patterns are chosen from the training text according to a statistical criterion. We carried out several experiments to compare perplexities of the proposed and the conventional models, which showed the advantage of our model. For many of the readers' interest, we applied this method to English text. As the result of a preliminary experiment, the proposed method got better performance than conventional word trigram.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-130"
  },
  "kneser96_icslp": {
   "authors": [
    [
     "Reinhard",
     "Kneser"
    ]
   ],
   "title": "Statistical language modeling using a variable context length",
   "original": "i96_0494",
   "page_count": 4,
   "order": 133,
   "p1": "494",
   "pn": "497",
   "abstract": [
    "In this paper we investigate statistical language models with a variable context length. For such models the number of relevant words in a context is not fixed as in conventional M-gram models but depends on the context itself. We develop a measure for the quality of variable-length models and present a pruning algorithm for the creation of such models, based on this measure. Further we address the question how the use of a special backing-off distribution can improve the language models. Experiments were performed on two data bases, the ARPA-NAB corpus and the German Verbmobil corpus, respectively. The results show that variable-length models outperform conventional models of the same size. Furthermore it can be seen that if a moderate loss in performance is acceptable, the size of a language model can be reduced drastically by using the presented pruning algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-131"
  },
  "johansen96_icslp": {
   "authors": [
    [
     "Finn Tore",
     "Johansen"
    ]
   ],
   "title": "A comparison of hybrid HMM architectures using global discriminative training",
   "original": "i96_0498",
   "page_count": 4,
   "order": 134,
   "p1": "498",
   "pn": "501",
   "abstract": [
    "This paper presents a comparison of different model architectures for TIMIT phoneme recognition. The baseline is a conventional diagonal covariance Gaussian mixture HMM. This system is compared to two different hybrid MLP/HMMs, both adhering to the same restrictions regarding input context and output states as the Gaussian mixtures. All free parameters in the three systems are jointly optimised using the same global discriminative criterion. A Forward decoder, with total likelihood scoring, is used for recognition. While the global discriminative training method is found to improve the baseline HMM significantly, the differences between Gaussian and MLP-based architectures are small. The Gaussian mixture system however performs slightly better at the lowest complexity levels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-132"
  },
  "wei96_icslp": {
   "authors": [
    [
     "Wei",
     "Wei"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "Improved probability estimation with neural network models",
   "original": "i96_0502",
   "page_count": 4,
   "order": 135,
   "p1": "502",
   "pn": "505",
   "abstract": [
    "Neural network classifiers can provide outputs that estimate Bayesian posterior probabilities under the assumptions that an infinite amount of training data are available, the network is sufficiently complex and the training can reach the global minimum. In practice, however, the number of training tokens is limited and may not accurately reflect the prior class probabilities and true likelihood distributions. Additionally, computational constraints place a limit on the complexity of the network. Consequently, practical networks often fall far short of being ideal estimators. We address this problem and propose a new method of improved probability estimation by combining neural network models with empirical probability estimation methods. We use a histogram-based estimation method to remap the network outputs to match the data and thereby improve the accuracy of the probability estimates. Our current experiments on the OGI Census Year corpus resulted in a 20.6% reduction in recognition errors at the utterance level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-133"
  },
  "yu96_icslp": {
   "authors": [
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "A neural network using acoustic sub-word units for continuous speech recognition",
   "original": "i96_0506",
   "page_count": 4,
   "order": 136,
   "p1": "506",
   "pn": "509",
   "abstract": [
    "A subword-based neural network model for continuous speech recognition is proposed. The system consists of three modules, and each module is composed of simple neural networks. The speech input is segmented into non-uniform units by the network in the first module. Non-uniform unit can model phoneme variations which spread for several phonemes and between words. The second module recognizes segmented units. The unit has stationary and transition parts, and the network is divided according to the two parts. The last module spots words by modeling temporal representation. The results of speaker independent word spotting of 520 words are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-134"
  },
  "bosch96_icslp": {
   "authors": [
    [
     "Louis F. M. ten",
     "Bosch"
    ],
    [
     "Roel",
     "Smits"
    ]
   ],
   "title": "On the error criteria in neural networks as a tool for human classification modelling",
   "original": "i96_0510",
   "page_count": 4,
   "order": 137,
   "p1": "510",
   "pn": "513",
   "abstract": [
    "Multi Layer Perceptrons (MLPs) can be applied as a tool to model human classification behaviour. In the present theoretical study we attempt to interpret MLPs within the framework of mathematical psychological models for human classification behaviour, more specifically the General Recognition Theory and the Generalized Context Model. Next, four error criteria are discussed that can be used in training and test of the MLPs, in relation to two types of data representation: in terms of individual deterministic responses or in terms of probabilistic responses. All error measures considered are additive, i.e. can be written as a sum across individual stimuli. It will be shown that some of these error measures have very different properties given a training set, and that the interpretation of the MLP as a means to provide knowledge about the underlying human decision process depends on the complexity of the MLP-topology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-135"
  },
  "ramsay96_icslp": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ]
   ],
   "title": "A non-linear filtering approach to stochastic training of the articulatory-acoustic mapping using the EM algorithm",
   "original": "i96_0514",
   "page_count": 4,
   "order": 138,
   "p1": "514",
   "pn": "517",
   "abstract": [
    "Current techniques for training representations of the articulatory-acoustic mapping from data rely on artificial simulations to provide codebooks of articulatory and acoustic measurements, which are then modelled by simple functional approximations. This paper outlines a stochastic framework for adapting an artificial model to real speech from acoustic measurements alone, using the EM algorithm. It is shown that parameter and state estimation problems for articulatory-acoustic inversion can be solved by adopting a statistical approach based on non-linear filtering.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-136"
  },
  "yang96b_icslp": {
   "authors": [
    [
     "Y. P.",
     "Yang"
    ],
    [
     "J. R.",
     "Deller Jr."
    ]
   ],
   "title": "A tool for automated design of language models",
   "original": "i96_0518",
   "page_count": 4,
   "order": 139,
   "p1": "518",
   "pn": "521",
   "abstract": [
    "An interactive software tool for design and performance analysis of language models (LMs) is described. The tool obviates on-line simulation of the recognition system in which the LM is to employed. By exploiting parallels with signal detection theory, a profile of the LM is given in an receiver-operating-characteristic-like (ROC) display.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-137"
  },
  "freitag96_icslp": {
   "authors": [
    [
     "F.",
     "Freitag"
    ],
    [
     "E.",
     "Monte"
    ]
   ],
   "title": "Acoustic-phonetic decoding based on elman predictive neural networks",
   "original": "i96_0522",
   "page_count": 4,
   "order": 140,
   "p1": "522",
   "pn": "525",
   "abstract": [
    "In this paper we present a phoneme recognition system based on the Elman predictive neural networks. The recurrent neural networks are used to predict the observation vectors of speech frames. Recognition of phonemes is done using the prediction error as distortion measure in the Viterbi algorithm. The performance of the neural predictive networks is evaluated on both the training database and on a speaker independent test database. The results obtained on the training database are similar to a four state continuous density HMM, results on the test database results are comparable to a three state HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-138"
  },
  "lee96c_icslp": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "On improving discrimination capability of an RNN based recognizer",
   "original": "i96_0526",
   "page_count": 4,
   "order": 141,
   "p1": "526",
   "pn": "529",
   "abstract": [
    "This paper presents a set of effective and efficient techniques to improve the discrimination capability of a recurrent neural network (RNN) based isolated word recognizer. The recognizer contains a set of individually trained RNN speech models (RSMs). Each of them represents a different word in the vocabulary. Speech recognition is performed by selecting the RSM that best matches the input utterance. For temporal supervised training of the RSMs, a new error function is introduced, in which the contributions of all phonetic components are equalized regardless of their difference in duration. The learning rate for recurrent connections is amplified. This is aimed at strengthening temporal dependency in the RSMs to capture dynamic characteristics of speech signals. Furthermore, a hierarchical training strategy is employed to facilitate more efficient discriminative training among the RSMs. A series of speaker-dependent recognition experiments are performed to evaluate the effectiveness of the proposed techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-139"
  },
  "wakita96_icslp": {
   "authors": [
    [
     "Yumi",
     "Wakita"
    ],
    [
     "Jun",
     "Kawai"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "An evaluation of statistical language modeling for speech recognition using a mixed category of both words and parts-of-speech",
   "original": "i96_0530",
   "page_count": 4,
   "order": 142,
   "p1": "530",
   "pn": "533",
   "abstract": [
    "In our previous paper, we proposed a mixed category of words and parts-of-speech names the MWP category based on class N-gram modeling [1]. However, we had not confirmed the efficiency of MWP category. In this paper, we evaluate the proposed MWP category. At first we use \\coverage of words and category sequences to open data\" and \\perplexity to training data\" for the evaluation and we confirmed the characteristics of parts-of-speech are useful to for generating a suitable class N-gram modeling. As a result of the speech recognition experimentation, we also confirmed that the class N-gram modeling using MWP category is effective in improving the recognition rate for open data that shows a low coverage of words and category sequences, without decreasing the recognition rate much for closed data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-140"
  },
  "aleksandrovsky96_icslp": {
   "authors": [
    [
     "Boris",
     "Aleksandrovsky"
    ],
    [
     "James",
     "Whitson"
    ],
    [
     "Gretchen",
     "Andes"
    ],
    [
     "Gary",
     "Lynch"
    ],
    [
     "Richard",
     "Granger"
    ]
   ],
   "title": "Novel speech processing mechanism derived from auditory neocortical circuit analysis",
   "original": "i96_0558",
   "page_count": 4,
   "order": 143,
   "p1": "558",
   "pn": "561",
   "abstract": [
    "Analysis of the prominent anatomical and physiological features of auditory thalamus and neocortex has enabled construction of models designed to identify functionality emergent from these biological circuits. These models have recently been shown to provide powerful computational mechanisms for processing of continuous time-varying sequences such as speech; testing on speech databases has yielded positive initial results that are reported here. The model constitutes a novel hypothesis of underlying functions of auditory neocortex, and also represents a novel approach to speech processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-141"
  },
  "tang96_icslp": {
   "authors": [
    [
     "Ping",
     "Tang"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Modeling neurons in the anteroventral cochlear nucleus for amplitude modulation (AM) processing: application to speech sound",
   "original": "i96_0562",
   "page_count": 4,
   "order": 144,
   "p1": "562",
   "pn": "565",
   "abstract": [
    "The aim of this work is to explore the representation of speech in the anteroventral cochlear nucleus by computational models based on neuroanatomical and neurophysiological data. A computational model of the anteroventral cochlear nucleus stellate cells with chop-S type response properties is described. Input that is excitatory and inhibitory to the model is in the form of simulated auditory-nerve spikes. The model is capable of generating a wide range of realistic chop-S unit responses. We investigate the speech information processing performed by our system that includes a peripheral model (cochlea, hair cells, auditory nerve spikes generation and chop-S units).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-142"
  },
  "vereecken96_icslp": {
   "authors": [
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Noise suppression and loudness normalization in an auditory model-based acoustic front-end",
   "original": "i96_0566",
   "page_count": 4,
   "order": 145,
   "p1": "566",
   "pn": "569",
   "abstract": [
    "It is commonly acknowledged that the presence of additive and convolutional noise and speech level variations can seriously deteriorate the performance of a speech recognizer. In case an auditory model is used as the acoustic front-end, it turns out that compensation techniques such as spectral subtraction and log-spectral mean subtraction can be outperformed by time-domain techniques operating on the band-pass filtered signals which are supplied to the haircell models. In [1] we showed that additive noise could be removed effectively by means of center clippers put in front of the haircell models. This technique, which was called linear noise magnitude subtraction (NMS), is further improved in this paper. The nonlinear NMS proposed here outperforms the linear one, especially for low Signal-to-Noise Ratios. To compensate for speech level variations and convolutional noise, we have adopted the same filosophy: remove the effects before the signal is supplied to the haircell models. This is accomplished by introducing normalization gains in front of the haircell models. It is shown that this loudness mean normalization (LMN) technique when used in combination whith NMS offers a highly robust speech representation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-143"
  },
  "hant96_icslp": {
   "authors": [
    [
     "Jim",
     "Hant"
    ],
    [
     "Brian",
     "Strope"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "A psychoacoustic model for the noise masking of voiceless plosive bursts",
   "original": "i96_0570",
   "page_count": 4,
   "order": 146,
   "p1": "570",
   "pn": "573",
   "abstract": [
    "A model for predicting the masked thresholds of the voiceless plosive bursts /k,t,p/ in background noise is proposed. Because plosive bursts are brief, are generated by a noise source, and have different spectral characteristics, the modeling approach must account for duration, center frequency, signal bandwidth and type. To achieve this goal, noise-in-noise masking experiments are conducted using a broad band masker and bandpass noise signals of varying bandwidth (1-8 CB), duration (10-300 ms), and center frequency (0.4-4 kHz). The results of these experiments are used to parameterize an auditory filter model in which the effective bandwidths of the filters and the signal-to-noise ratio at threshold are frequency and duration-dependent. The duration-dependent filter model is then used to predict the thresholds of both synthetic and naturally-spoken plosive bursts in background noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-144"
  },
  "hunke96_icslp": {
   "authors": [
    [
     "Martin",
     "Hunke"
    ],
    [
     "Thomas",
     "Holton"
    ]
   ],
   "title": "Training machine classifiers to match the performance of human listeners in a natural vowel classification task",
   "original": "i96_0574",
   "page_count": 4,
   "order": 147,
   "p1": "574",
   "pn": "577",
   "abstract": [
    "The purpose of this research is to determine how models of human auditory physiology can improve the performance of automatic speech recognition systems. In this study, a series of experiments was undertaken to discover how humans categorize and confuse vowels in natural speech. The recognition task comprised a large number of vowel nuclei isolated from naturally spoken sentences of a large number of talkers. Machine vowel classifiers were trained to match the results of these vowel categorization experiments using two input feature representations: a spectral-energy feature representation, and a representation derived from an auditory model. Classifiers trained to input representations derived from the auditory model match human performance and are more robust in the presence of noise and spectral filtering than classifiers trained to spectral-energy representations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-145"
  },
  "aikawa96_icslp": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ]
   ],
   "title": "A neural matrix model for active tracking of frequency-modulated tones",
   "original": "i96_0578",
   "page_count": 4,
   "order": 148,
   "p1": "578",
   "pn": "581",
   "abstract": [
    "Previous paper has shown that the dynamic process of perceiving frequency-modulated (FM) tones can be well simulated by a sound tracking model represented by second-order systems. This paper proposes a new neural operational model for tracking FM tones using a tonotopic neural matrix. The neural cells are connected to build up an auto-regressive architecture. This neural matrix model is characterized by a novel spectral interpolation algorithm based on Lp-norm. The new tracking model enables simultaneous tracking of multiple FM tones. This paper demonstrates that the neural tracking model can successfully simulate perceptual effect such as pitch bounce caused by crossed linear sweep tones. The perceptual overshoot effect is also reasonably explained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-146"
  },
  "rose96_icslp": {
   "authors": [
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Eduardo",
     "Lleida-Solano"
    ],
    [
     "G. W.",
     "Erhart"
    ],
    [
     "R. V.",
     "Grubbe"
    ]
   ],
   "title": "A user-configurable system for voice label recognition",
   "original": "i96_0582",
   "page_count": 4,
   "order": 149,
   "p1": "582",
   "pn": "585",
   "abstract": [
    "A set of techniques for configuring a speech recognition system to a particular user are described in the context of voice label recognition over the public switched telephone network. User-configurable vocabularies are provided through automatic acoustic baseform determination based on an inventory of speaker independent subword acoustic units. The tendency of input utterances to contain out-of-vocabulary or non-speech information is accounted for using likelihood ratio based utterance verification procedures. Mismatch between a given user's utterances and the HMM model is accounted for using a frequency warping approach to speaker normalization. The performance of these techniques was evaluated on utterances taken from a trial version of a voice label recognition service.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-147"
  },
  "gelin96_icslp": {
   "authors": [
    [
     "Philippe",
     "Gelin"
    ],
    [
     "Chris. J.",
     "Wellekens"
    ]
   ],
   "title": "Keyword spotting enhancement for video soundtrack indexing",
   "original": "i96_0586",
   "page_count": 4,
   "order": 150,
   "p1": "586",
   "pn": "589",
   "abstract": [
    "Multimedia databases contain an increasing amount of videos that are hardly semantically accessed. Among the useful indices that can be extracted from the sound track, the presence of a keyword at some place plays a prominent role. This paper deals with the specificities of such a keyword spotter and the enhancement brought to our previous technique, [1] based on frame labeling. To be useful, such a keyword spotter has to be speaker independent. Moreover it has to be able to detect any word out of an open vocabulary. This directly implies the use of a phonemic representation of the word. These constraints usually lead to an excessively time consuming tool. The division of the indexing process into two parts, the first one off-line, the second one at the query time, allows a faster response.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-148"
  },
  "meliani96_icslp": {
   "authors": [
    [
     "Rachida El",
     "Méliani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "New efficient fillers for unlimited word recognition and keyword spotting",
   "original": "i96_0590",
   "page_count": 4,
   "order": 151,
   "p1": "590",
   "pn": "593",
   "abstract": [
    "This paper describes our complete results for improved lexical fillers as well as two new kinds of fillers, gives their results in unlimited speech recognition as well as for keyword spot- ting and compares them to the acoustic-phonetic filler in the case of keyword spotting. Tests have been conducted on different vocabularies derived from ATIS and the Wall Street Journal database. Results for keyword spotting show the superiority of the independent lexical phonemic filler that combines accuracy (92% for a false alarm rate of 1.2 FA/h/kw) as well as task-independent training. As for new-word detection, the syllabic and the independent lexical fillers perform quite well, and allow relevant detection of the phonetic transcription.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-149"
  },
  "spina96_icslp": {
   "authors": [
    [
     "Michelle S.",
     "Spina"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Automatic transcription of general audio data: preliminary analyses",
   "original": "i96_0594",
   "page_count": 4,
   "order": 152,
   "p1": "594",
   "pn": "597",
   "abstract": [
    "The task of automatically transcribing general audio data is very different from the transcription task typically required of current automatic speech recognition systems. The general goal of this work is to quantify the difficult issues posed by such data, thus leading to an understanding of how a speech recognition system may have to be altered to accommodate the added complexities. Specifically,we describe some preliminary analyses and experiments we have conducted on data collected from a radio news program. We found that using relatively straightforward acoustic measurements and classification techniques,we were able to achieve better than 80% classification accuracy for seven salient sound classes present in the data, and nearly 94% classification accuracy for a speech/non-speech decision. In addition, lexical analysis revealed that while the vocabulary size of a single broadcast is moderate, it grows exponentially as more shows are added.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-150"
  },
  "kubala96_icslp": {
   "authors": [
    [
     "Francis",
     "Kubala"
    ],
    [
     "Tasos",
     "Anastasakos"
    ],
    [
     "Hubert",
     "Jin"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Transcribing radio news",
   "original": "i96_0598",
   "page_count": 4,
   "order": 153,
   "p1": "598",
   "pn": "601",
   "abstract": [
    "We have recently extended the capabilities of BBN's large vocabulary discrete-utterance speech recognition system (BYBLOS) to operate on raw audio recordings of radio news programming. The recordings are given to the system as large monolithic waveforms without any additional side-information. Our goal is to transcribe all speech in the input with the highest accuracy possible. The problem is very challenging because radio news programming has frequent changes in speaker, speaking style, dialect, accent, topic, channel, and environmental conditions. Furthermore, the monolithic input presents new problems for recognition algorithms and language models since all useful boundaries (such as speaker turns or sentence ends) are unknown.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-151"
  },
  "setlur96_icslp": {
   "authors": [
    [
     "Anand R.",
     "Setlur"
    ],
    [
     "Rafid A.",
     "Sukkar"
    ],
    [
     "John",
     "Jacob"
    ]
   ],
   "title": "Correcting recognition errors via discriminative utterance verification",
   "original": "i96_0602",
   "page_count": 4,
   "order": 154,
   "p1": "602",
   "pn": "605",
   "abstract": [
    "Utterance verification (UV) is a process by which the output of a speech recognizer is verified to determine if the input speech actually includes the recognized keyword(s). The output of the speech verifier is a binary decision to accept or reject the recognized utterance based on a UV confidence score. In this paper, we extend the notion of utterance verification to not only detect errors but also selectively correct them. We perform error correction by flipping the hypotheses produced by an N-best recognizer in cases when the top candidate has a UV confidence score that is lower than that of the next candidate. We propose two measures for computing confidence scores and investigate the use of a hybrid confidence measure that combines the two measures into a single score. Using this hybrid confidence measure and an N-best algorithm, we obtained an 11% improvement in word-error rate on a connected digit recognition task. This improvement was achieved while still maintaining reliable detection of non-keyword speech and misrecognitions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-152"
  },
  "akahaneyamada96_icslp": {
   "authors": [
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Yoh'ichi",
     "Tohkura"
    ],
    [
     "Ann R.",
     "Bradlow"
    ],
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Does training in speech perception modify speech production?",
   "original": "i96_0606",
   "page_count": 4,
   "order": 155,
   "p1": "606",
   "pn": "609",
   "abstract": [
    "To examine the relationship between speech perception and production in second language acquisition, this study investigated whether training in the perception domain transfers to improvement in the production domain. Native speakers of Japanese were trained to identify English /r/-/l/ minimal pairs. Recordings were made of the subjects productions of minimal pairs before and after identification training. American-English listeners then perceptually evaluated these productions. The subjects showed significant improvements from pretest to post-test in perception as well as in production. Furthermore, the subjects retained these abilities in follow-up tests given three months and six months after the conclusion of training. These results demonstrate that training in the perception domain produces long-term modifications in both perception and production,\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-153"
  },
  "ueyama96_icslp": {
   "authors": [
    [
     "Motoko",
     "Ueyama"
    ]
   ],
   "title": "Phrase-final lengthening and stress-timed shortening in the speech of native speakers and Japanese learners of English",
   "original": "i96_0610",
   "page_count": 4,
   "order": 156,
   "p1": "610",
   "pn": "613",
   "abstract": [
    "This study is intended to describe and analyze the durational patterns of native Japanese speakers learning English, with a focus on two major prosodic effects: phrase-final lengthening and stress-timed shortening. To investigate the relative contribution of these effects, a production experiment was conducted, adapting the methodological framework of Beckman and Edwards (1990). The effects of three degrees of boundary strength (the boundaries separating the members of a compound; two phonological phrases; and two intonational phrases) on the two phenomena were analyzed. Native English speakers, beginning Japanese learners of English, and advanced Japanese learners of English were compared.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-154"
  },
  "yamada96_icslp": {
   "authors": [
    [
     "Nobuko",
     "Yamada"
    ]
   ],
   "title": "Japanese accentuations by foreign students and Japanese speakers of non-tokyo dialect",
   "original": "i96_0614",
   "page_count": 4,
   "order": 157,
   "p1": "614",
   "pn": "617",
   "abstract": [
    "This study is the first attempt toward the unified theory of acquisition of Tokyo dialect accentuation by non-native speakers of Japanese and the Japanese speakers of non-Tokyo dialect, i e., speakers from Ibaraki prefecture in Japan. The data are analyzed in regards to the interim accentual system, which is predicted to be produced in the process of the acquisition and be different from that of their mother tongue (dialect) or that of target language (dialect), and which is called interlanguage [ 1] or interdialect [9]. The characteristics of both interlanguage and interdialect was examined and it was found that all subjects seemed to have created the same system under the strong influence of their target language accentual system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-155"
  },
  "varden96_icslp": {
   "authors": [
    [
     "J. Kevin",
     "Varden"
    ],
    [
     "Tsutomu",
     "Sato"
    ]
   ],
   "title": "Devoicing of Japanese vowels by taiwanese learners of Japanese",
   "original": "i96_0618",
   "page_count": 4,
   "order": 158,
   "p1": "618",
   "pn": "621",
   "abstract": [
    "Three Taiwanese speakers studying in Japan were tested for application of the Japanese rule of High Vowel Devoicing. Three native speakers of Japanese served as controls. Five repetitions each of tokens containing two possible devoicing sites were collected from each participant at speaker-determined slow and fast speeds. Of the Taiwanese participants, only one showed any significant speech rate effect (t = 1.88, p < .05 for the first vowel of the tokens; t =2.29, p < .025 for both vowels combined). One other showed no speech rate effects whatsoever, instead inconsistently devoicing some tokens, while the third did not devoice any vowels within the tokens, instead only devoicing a small number of sentence-final vowels. It appears that each of the three learners has begun to devoice vowels according to different processes - for one speaker, a speech rate dependent rule; for another, a speech rate insensitive rule whose environment is not yet solidified; for the third, as a rule sporadically applied to the end of a sentence. Most surprisingly, native speakers also showed a lack of significant speech rate effects. This would imply that High Vowel Devoicing is losing its status as a fast speech rule in the Tokyo dialect. Further, larger studies will show if this indication is correct.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-156"
  },
  "archambault96_icslp": {
   "authors": [
    [
     "Danièle",
     "Archambault"
    ],
    [
     "Catherine",
     "Foucher"
    ],
    [
     "Blagovesta",
     "Maneva"
    ]
   ],
   "title": "Fluency and use of segmental dialect features in the acquisition of a second language (French) by English speakers",
   "original": "i96_0622",
   "page_count": 4,
   "order": 159,
   "p1": "622",
   "pn": "625",
   "abstract": [
    "This study investigates the use of two parameters, fluency and use of segmental dialect features (accent) to rate the overall ability of speakers of French as a second language. A group of ten native English Canadians read a short text of 139 words in French (their second language). Their degree of fluency was established by a combination of the following measures: speech rate (words/min, syll/min, syll/s), number and duration of pauses, and hesitations. We also investigated their use of various segmental dialect features such as diphthonguization of long vowels, affrication of /t d/, and high vowel deletion. These results were then compared to those of a group of ten native French speakers of the same region (Montreal).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-157"
  },
  "martland96_icslp": {
   "authors": [
    [
     "P.",
     "Martland"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Steve W.",
     "Beet"
    ],
    [
     "L.",
     "Baghai-Ravary"
    ]
   ],
   "title": "Estimating child and adolescent formant frequency values from adult data",
   "original": "i96_0626",
   "page_count": 4,
   "order": 160,
   "p1": "626",
   "pn": "629",
   "abstract": [
    "This paper introduces a model being developed for estimating child and adolescent formant frequency values from adult data. The model approximates adult male and female pharyngeal and oral cavity lengths, and scales these along the corresponding male or female growth curve. The second and third formant frequencies are estimated directly from these scaled vocal tract dimensions. Two methods of establishing the first formant from the scaled data are discussed. Initial results obtained in the scaling of adult data to child values suggest that age, height and gender are all significant when estimating child formant frequency values. Furthermore, averaging of male and female data is found to be inappropriate since the differing growth rates of males and females imply that vocal tract dimensions cannot be linearly related.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-158"
  },
  "sluijter96_icslp": {
   "authors": [
    [
     "Agaath M. C.",
     "Sluijter"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Acoustic correlates of linguistic stress and accent in dutch and american English",
   "original": "i96_0630",
   "page_count": 4,
   "order": 161,
   "p1": "630",
   "pn": "633",
   "abstract": [
    "In the literature the same acoustic correlates of stress and accent have been established for Dutch and English, i.e. F0 movement, duration, intensity and vowel quality. Sluijter and Van Heuven (1996a) showed that F0 movement and overall intensity in Dutch differentiate only between accented and non-accented syllables, rather than between stressed and unstressed. The most reliable acoustic correlates of stress were duration and highfrequency emphasis. Vowel quality differed significantly only in lexical items, but was only a weak correlate in reiterant speech copies. In this study we reconsider the acoustical correlates of stress and accent in American English (AE) and compare the results with the Dutch results. We offer an analysis of the discriminating strength of the parameters in an attempt to optimally distinguish initial and final stressed tokens by machine, using LDA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-159"
  },
  "fujisaki96_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Osamu",
     "Tomita"
    ]
   ],
   "title": "On the levels of accentuation in spoken Japanese",
   "original": "i96_0634",
   "page_count": 4,
   "order": 162,
   "p1": "634",
   "pn": "637",
   "abstract": [
    "Accentuation serves to express both the discrete information concerning the accent type of a prosodic word and the continuous information concerning its prominence. This paper examines the latter aspect of accentuation using recorded radio news read by announcers. The amplitude of the accent command was extracted from an F0 contour and used as an index for the level of accentuation. Statistical analysis of the accent command amplitude confirmed the difference between accented and unaccented types. Further analysis of the relationship between amplitudes of two adjoining accent commands also revealed a marked difference in characteristics of these two types.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-160"
  },
  "thibault96_icslp": {
   "authors": [
    [
     "Linda",
     "Thibault"
    ],
    [
     "Marise",
     "Ouellet"
    ]
   ],
   "title": "Tonal distinctions between emphatic stress and pretonic lengthening in quebec French",
   "original": "i96_0638",
   "page_count": 4,
   "order": 163,
   "p1": "638",
   "pn": "641",
   "abstract": [
    "This study compares the tonal structures of stressed penultimate syllables in Quebec French and emphatic stress in the same spoken variety. Two main experiments have been conducted: the first was designed to highlight the tonal characteristics of emphatic stress in read and spontaneous speech. The second was concerned with the phonetic and tonal description of stressed, penultimate syllables as a result of a possible stress shift. Our results do not confirm the common assumption that penultimate, stressed syllables in Quebec French are the result of emphatic prominence. Emphatic stress is characterized by a LH tone on the target syllable followed by a more or less abrupt fall covering a particular domain depending on the speech style. By contrast, penultimate stressed syllables are characterized by a falling F0 modulation covering the lengthened syllable. The hypothesis that the tonal anchoring happens on the penultimate syllable could explain the variety of tonal patterns observed on the final syllables in those precise cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-161"
  },
  "elsner96_icslp": {
   "authors": [
    [
     "Anja (Petzold)",
     "Elsner"
    ]
   ],
   "title": "Distinction between 'normal' focus and 'contrastive/emphatic' focus",
   "original": "i96_0642",
   "page_count": 4,
   "order": 164,
   "p1": "642",
   "pn": "645",
   "abstract": [
    "A method to extract and classify focus accents has been developed. It works for German spontaneous speech. The method tries to distinguish 'normal' and 'contrastive/emphatic' focus accents using phrase boundaries. It was found that contrastive/emphatic accents tend to have greater distances to phrase boundaries than normal focus accents. Moreover, for contrastive/emphatic accents there was found a much steeper F0 rise for accents with a rather high distance from the next phrase boundary\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-162"
  },
  "nishinuma96_icslp": {
   "authors": [
    [
     "Yukihiro",
     "Nishinuma"
    ],
    [
     "Masako",
     "Arai"
    ],
    [
     "Takako",
     "Ayusawa"
    ]
   ],
   "title": "Perception of tonal accent by americans learning Japanese",
   "original": "i96_0646",
   "page_count": 4,
   "order": 165,
   "p1": "646",
   "pn": "649",
   "abstract": [
    "We studied how American subjects learning Japanese perceive tonal accents under different experimental conditions. The perceptual experiment included 3 tests, each one containing 24 words consisting of 3, 4, and 5 moras with different tonal accents. Two groups of 54 American students were asked to detect words accent. An analysis of variance showed that test, mora, and accent type factors were highly significant. Results, by accent type, suggest that the perception of tonal accent seems to be mother tongue-dependent; Accent types corresponding to English intonation patterns were preferred.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-163"
  },
  "shriberg96_icslp": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "D. Robert",
     "Ladd"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Modeling intra-speaker pitch range variation: predicting F0 targets when \"speaking up\"",
   "original": "i96_0650",
   "page_count": 4,
   "order": 166,
   "p1": "650",
   "pn": "653",
   "abstract": [
    "We study F0 variation produced by \"speaking up\", as part of a larger study of pitch range variation within and across speakers [1]. We provide a function to predict target F0 values in this \"raised\" mode from F0 values at corresponding locations in speech produced in a neutral mode. Targets were F0 measurements at points of low internal variability in read Dutch sentences produced by 15 speakers. Across speakers, the relationship between normal and raised targets was well fit by an additive-multiplicative function. The offset for this function was set to a speaker-specific measured value. In the transformed space, the free parameters can be interpreted as (1) an upward shift in F0, and (2) a multiplier affecting the size of F0 excursions. Parameter values varied independently across speakers, suggesting that individuals use different strategies when speaking up. We conclude that at least three speaker-specific values (offset, shift factor, and multiplier) are required for pitch range scaling in this domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-164"
  },
  "reithinger96_icslp": {
   "authors": [
    [
     "Norbert",
     "Reithinger"
    ],
    [
     "Ralf",
     "Engel"
    ],
    [
     "Michael",
     "Kipp"
    ],
    [
     "Martin",
     "Klesen"
    ]
   ],
   "title": "Predicting dialogue acts for a speech-to-speech translation system",
   "original": "i96_0654",
   "page_count": 4,
   "order": 167,
   "p1": "654",
   "pn": "657",
   "abstract": [
    "We present the application of statistical language modeling methods for the prediction of the next dialogue act. This prediction is used by different modules of the speech-to-speech translation system VERBMOBIL. The statistical approach uses deleted interpolation of n-gram frequencies as basis and determines the interpolation weights by a modified version of the standard optimization algorithm. Additionally, we present and evaluate different approaches to improve the prediction process, e.g. including knowledge from a dialogue grammar. Evaluation shows that including the speaker information and mirroring the data delivers the best results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-165"
  },
  "muller96_icslp": {
   "authors": [
    [
     "Johannes",
     "Müller"
    ],
    [
     "Holger",
     "Stahl"
    ],
    [
     "Manfred",
     "Lang"
    ]
   ],
   "title": "Automatic speech translation based on the semantic structure",
   "original": "i96_0658",
   "page_count": 4,
   "order": 168,
   "p1": "658",
   "pn": "661",
   "abstract": [
    "This paper describes a system for the semantic-based translation of spoken or written limited-domain utterances. The semantic structure as output of a semantic decoder serves as the interlingua-level. A word chain generator combined with a linguistic post-processor produces the according word chain in the target language. Both the semantic decoder and the word chain generator work with pure stochastic and trainable knowledge bases. The grammatical features of certain words can be easily extracted by the help of both the word chain and the semantic structure. Keywords: automatic speech translation, speech understanding, semantic decoding, language production, semantic structure, semantic model, syntactic model, inflectional model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-166"
  },
  "norton96_icslp": {
   "authors": [
    [
     "Lewis M.",
     "Norton"
    ],
    [
     "Carl E.",
     "Weir"
    ],
    [
     "K. W.",
     "Scholz"
    ],
    [
     "Deborah A.",
     "Dahl"
    ],
    [
     "Ahmed",
     "Bouzid"
    ]
   ],
   "title": "A methodology for application development for spoken language systems",
   "original": "i96_0662",
   "page_count": 3,
   "order": 169,
   "p1": "662",
   "pn": "664",
   "abstract": [
    "A major bottleneck in the development of practical spoken language (SL) applications is the interface between the SL system and back-end application software. In theory, the range of potential back-end software for an SL interface is unlimited, but integration of an SL system with other software requires adapting the SL system to conform to the input formats of the back end. This is typically accomplished by ad hoc, labor-intensive methods, which are highly application-specific. Some SL systems have addressed this problem by attempting to handle only relational database interface applications, but this approach limits the usefulness of such systems, since relational databases represent only a fraction of the applications which could benefit from an SL interface. This paper describes a general, flexible, rule-based approach to integrating SL applications to arbitrary back-end software.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-167"
  },
  "seneff96c_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Joseph",
     "Polifroni"
    ]
   ],
   "title": "A new restaurant guide conversational system: issues in rapid prototyping for specialized domains",
   "original": "i96_0665",
   "page_count": 4,
   "order": 170,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "Over the past year,we have begun the development of a new restaurant guide domain, DINEX, which utilizes all of the same core technology as our other GALAXY systems [2]. The domain server has been adapted from our CityGuide domain within our GALAXY framework, using the same mechanisms to locate places on the map or to give directions. The system has information on several features for restaurants, such as hours, cuisine, parking availability, and an on-line menu. DINEX knows over 450 restaurants in the Boston area, and can also provide directions from subway stops or well-known landmarks. DINEX makes use of a relational database derived from several on-line sources, including a nationally known travel guide. This paper particularly emphasizes the language tools we have developed to construct the relational database semi-automatically and to derive from user queries the appropriate SQL queries for accessing the database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-168"
  },
  "kumamoto96_icslp": {
   "authors": [
    [
     "Tadahiko",
     "Kumamoto"
    ],
    [
     "Akira",
     "Ito"
    ]
   ],
   "title": "Semantic interpretation of a Japanese complex sentence in an advisory dialogue - focused on the postpositional word \"KEDO,\" which works as a conjunction between clauses",
   "original": "i96_0669",
   "page_count": 4,
   "order": 171,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "We are developing a spoken dialogue system that helps a computer user perform his tasks through question-answering. In this paper, we propose a method for semantic interpretation of a complex sentence connected by a conjunction, \"KEDO.\" \"KEDO\" is a postpositional word, and forms an adverbial clause. First, complex sentences containing \"KEDO\" are extracted from an advisory dialogue database and the semantic role of \"KEDO\" in each sentence is analyzed. Next, each sentence is decomposed into the part before \"KEDO\" and the part after \"KEDO,\" and each part is expressed in a semantic structure we developed. Finally, the relationship between the semantic structures and the semantic roles is formalized into rules for semantic interpretation. Our method interprets a complex sentence semantically by applying the rules to the two semantic structures generated from the sentence. As a result, the semantic structures are modified or transformed into another if necessary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-169"
  },
  "hong96_icslp": {
   "authors": [
    [
     "Youngkuk",
     "Hong"
    ],
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Gijoo",
     "Yang"
    ]
   ],
   "title": "A Korean morphological analyzer for speech translation system",
   "original": "i96_0673",
   "page_count": 4,
   "order": 172,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "This paper describes a Korean morphological analyzer which can be used as a part of language processor for a speech translation system. We have modified the CYK algorithm so that we are able to analyze many phenomena occurring in spontaneous speech such as ellipsis, shorter words, poor and mispronounced words and so on. And we also have constructed a rule set with 112 connection rules and seven kinds of dictionaries, in which there are totally about 81,000 keywords. Currently, we have achieved the success rate of 93.0% with a text corpus of dialogue for hotel reservation domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-170"
  },
  "carlson96_icslp": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ]
   ],
   "title": "Generic and domain-specific aspects of the waxholm NLP and dialog modules",
   "original": "i96_0677",
   "page_count": 4,
   "order": 173,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "In this paper we give an overview of the NLP and dialog component in the Waxholm spoken dialog system. We will discuss how the dialog and the natural language component are modeled from a generic and a domain-specific point of view. Dialog management based on grammar rules and lexical semantic features is implemented in our parser. The notation to describe the syntactic rules has been expanded to cover some of our special needs to model the dialog. The parser is running with two different time scales corresponding to the words in each utterance and to the turns in the dialog. Topic selection is accomplished based on probabilities calculated from user initiatives. Results from parser performance and topic prediction are included.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-171"
  },
  "kameyama96_icslp": {
   "authors": [
    [
     "Megumi",
     "Kameyama"
    ],
    [
     "Goh",
     "Kawai"
    ],
    [
     "Isao",
     "Arima"
    ]
   ],
   "title": "A real-time system for summarizing human-human spontaneous spoken dialogues",
   "original": "i96_0681",
   "page_count": 4,
   "order": 174,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "We have built a prototype Automatic Dialogue Summarizer (ADS) | a real{time system that automatically generates simple summaries of completely spontaneous human{human spoken dialogues without the machine interrupting the natural flow of conversation. Two dialogue participants (client and clerk) discuss conference room reservations (CRR) in Japanese, and the system dynamically updates summaries of what rooms were reserved or canceled for what times and by whom. This paper describes the system's architecture, its component technologies, and its performance. We discuss the robustness, efficiency, and effectiveness of the system, and the use of a spontaneous dialogue corpus for development and testing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-172"
  },
  "hildebrandt96_icslp": {
   "authors": [
    [
     "Bernd",
     "Hildebrandt"
    ],
    [
     "Heike",
     "Rautenstrauch"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Evaluation of spoken language understanding and dialogue systems",
   "original": "i96_0685",
   "page_count": 4,
   "order": 175,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "A spoken language understanding and dialogue system in the domain of appointment schedule is presented. The system is capable of understanding complex times, e.g. it correctly combines discontinuous constituents and resolves ambiguities. A distributed representation of surface structure models and an incremental semantic analysis is used to manage the complexity. An elaborate evaluation of the system based on measurements of accuracy was carried out. Our approach combines pattern recognition with linguistic aspects forming a system of measurement consisting of word accuracy, constituent accuracy, and concept accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-173"
  },
  "kakita96_icslp": {
   "authors": [
    [
     "Kuniko",
     "Kakita"
    ]
   ],
   "title": "Inter-speaker interaction of F0 in dialogs",
   "original": "i96_0689",
   "page_count": 4,
   "order": 176,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "This paper presents some preliminary results of a study on inter-speaker interaction of fundamental frequency of voice (F0) during dialogs. Simple question-answer type dialogs (in Japanese) were recorded and analyzed. The results revealed all three possible patterns of F0 interaction. In some cases, the two speakers' F0 converged as the dialog progressed, while in other cases they diverged; there were also cases in which they maintained a parallel relation to each other throughout the dialog. The results also indicated that dialog-initial difference in the two speakers' F0 values was closely related to consequent overall pattern of F0 interaction during dialogs. Partner-dependent reorganization of dialog-initial F0 was observed for some of the speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-174"
  },
  "brandtpook96_icslp": {
   "authors": [
    [
     "Hans",
     "Brandt-Pook"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Bernd",
     "Hildebrandt"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "A robust dialogue system for making an appointment",
   "original": "i96_0693",
   "page_count": 4,
   "order": 177,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "A complete dialogue system within the task domain of making an appointment is presented. It is based on a semantic network representation of linguistic knowledge and a word recognition system that communicates with the interpretation component bidirectionally. System robustness is achieved using a special metascore that evaluates the advance of the linguistic interpretation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-175"
  },
  "takagi96_icslp": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Segmentation of spoken dialogue by interjections, disfluent utterances and pauses",
   "original": "i96_0697",
   "page_count": 4,
   "order": 178,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "This paper attempts to segment spontaneous speech of human-to-human spoken dialogues into a relatively large unit of speech, that is, a sub-phrasal unit segmented by interjections, disfluent utterances and pauses. A spontaneous speech model incorporating prosody was developed, in which three kinds of speech segment models and the transition probabilities among them were specified. The segmentation experiments showed that 87.6 % of the segment boundaries were located correctly within 50 msec, 81.2 % within 30 msec, which showed 10.1 point increase in performance comparing with the initial model without prosodic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-176"
  },
  "goddeau96_icslp": {
   "authors": [
    [
     "David",
     "Goddeau"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Senis",
     "Busayapongchai"
    ]
   ],
   "title": "A form-based dialogue manager for spoken language applications",
   "original": "i96_0701",
   "page_count": 4,
   "order": 179,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "A popular approach to dialogue management is based on a finite-state model, where user utterances trigger transitions between the dialogue states, and these states, in turn, determine the systems response. This paper describes an alternative dialogue planning algorithm based on the notion of filling in an electronic form, or \"E-form.\" Each slot has associated prompts that guide the user through the dialogue, and a priority that determines the order in which the system tries to acquire information. These slots can be optional or mandatory. However, the user is not restricted to follow the systems lead, and is free to ignore the prompts and take the initiative in the dialogue. The E-form-based dialogue planner has been used in an application to search a database of used car advertisements. The goal is to assist the user in selecting, from this database, a small list of cars which match their constraints. For a large number of dialogues collected from over 600 naive users,we found over 70% compliance in answering specific system prompts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-177"
  },
  "whittaker96_icslp": {
   "authors": [
    [
     "S. J.",
     "Whittaker"
    ],
    [
     "D. J.",
     "Attwater"
    ]
   ],
   "title": "The design of complex telephony applications using large vocabulary speech technology",
   "original": "i96_0705",
   "page_count": 4,
   "order": 180,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "Almost every speech application involves integration with real world databases which may be large or complex. Telephony based examples include call-centre automation, customer identification and directory assistance. Many such applications are intrinsically large vocabulary problems with complex data requirements. This paper illustrates the architectural, technical and dialogue issues relevant to the development of such systems. The implementation of the Brimstone corporate directory trial system developed at BT Laboratories is described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-178"
  },
  "sutton96_icslp": {
   "authors": [
    [
     "Stephen",
     "Sutton"
    ],
    [
     "David G.",
     "Novick"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Pieter",
     "Vermeulen"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "Johan",
     "Schalkwyk"
    ],
    [
     "Mark",
     "Fanty"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "Building 10,000 spoken dialogue systems",
   "original": "i96_0709",
   "page_count": 4,
   "order": 181,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "Spoken dialogue systems are not yet ubiquitous. But with an easy-enough development tool, at a low-enough cost, and on portable-enough software, advances in spoken-dialogue technology could soon enable the rapid development of 10,000 or more spoken dialogue systems for a wide variety of applications. To achieve this goal, we propose a toolkit approach for research and development of spoken dialogue systems. This paper presents the CSLU toolkit, which integrates spoken-dialogue technology with an easy-to-use interface. The toolkit supports rapid prototyping, iterative design, empirical evaluation, training of specialized speech recognizers and tools for conducting research to improve the underlying technology. We describe the toolkit with an emphasis on graphical creation of spoken dialogue systems; the transition of the toolkit into the user community; and research directed toward improvements in the toolkit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-179"
  },
  "yang96c_icslp": {
   "authors": [
    [
     "Yen-Ju",
     "Yang"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Speaker intention modeling for large vocabulary Mandarin spoken dialogues",
   "original": "i96_0713",
   "page_count": 4,
   "order": 182,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "This paper presents a statistical speaker intention modeling approach of speech act types (SATs)[1] prediction for large vocabulary Mandarin spoken dialogues. A SAT is an abstraction of speakers intention in terms of the type of action that the speaker intends by the utterance. With this approach, spoken dialogue systems can be constructed to predict speakers intention and make a proper action in advance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-180"
  },
  "kenne96_icslp": {
   "authors": [
    [
     "P. E.",
     "Kenne"
    ],
    [
     "Mary",
     "O'Kane"
    ]
   ],
   "title": "Hybrid language models and spontaneous legal discourse",
   "original": "i96_0717",
   "page_count": 4,
   "order": 183,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "The idea of using multiple speech recognizers and alternate language models in a spoken language task is a familiar one. The problems which arise are deciding which language model(s) and/or recognizer(s) to use, or when to change language models. We use the idea of local perplexity of a corpus to determine when to change to an alternate language model and compare this strategy to a number of other strategies in the context of court interactions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-181"
  },
  "kenne96b_icslp": {
   "authors": [
    [
     "P. E.",
     "Kenne"
    ],
    [
     "Mary",
     "O'Kane"
    ]
   ],
   "title": "Topic change and local perplexity in spoken legal dialogue",
   "original": "i96_0721",
   "page_count": 4,
   "order": 184,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "The notion of local perplexity of a corpus (over a range of language models) is examined, and the local behaviour of perplexity (over several court transcripts) is examined to determine if there is there any correspondence between local variations in perplexity and some event in the dialogue, such as topic change.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-182"
  },
  "venditti96_icslp": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Intonational cues to discourse structure in Japanese",
   "original": "i96_0725",
   "page_count": 4,
   "order": 185,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "This study examines the extent to which intonation plays a role in the structuring of information in a Japanese monologue. The role of pitch accent in the intonational system of Japanese is very different from that in languages like Dutch or English: in Japanese, pitch accent is a lexical property of words and cannot be used to lend prominence to words at the sentence level. Therefore, we wondered if (and how) intonation can cue discourse structure in Japanese, comparable to how it is being used in Dutch and English. Results show that fundamental frequency (F0), amplitude, and duration of the final accents in each sentence did not serve to cue the boundaries of discourse segments, contrary to our expectation. However, pitch range variations on NPs, examined in terms of their position in a discourse segment and their information status, did show a correlation with discourse structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-183"
  },
  "bernsen96_icslp": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Hans",
     "Dybkjær"
    ],
    [
     "Laila",
     "Dybkjær"
    ]
   ],
   "title": "Principles for the design of cooperative spoken human-machine dialogue",
   "original": "i96_0729",
   "page_count": 4,
   "order": 186,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "This paper presents a consolidated set of 24 principles of cooperative spoken human-machine dialogue which are based on the development and controlled user testing of the dialogue component of the Danish dialogue system as well as on comparison with human-human dialogue theory. Potentially, the principles could be used as effective and systematic dialogue development and evaluation tools both during early design and in later phases of dialogue evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-184"
  },
  "jenkin96_icslp": {
   "authors": [
    [
     "Karen L.",
     "Jenkin"
    ],
    [
     "Michael S.",
     "Scordilis"
    ]
   ],
   "title": "Development and comparison of three syllable stress classifiers",
   "original": "i96_0733",
   "page_count": 4,
   "order": 187,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "This paper describes the development of three alternative techniques for the classification of syllable stress in fluent speech. They are based on: (1) neural networks that use contextual syllabic information, (2) first and second order Markov chains that depend on a new dynamic vector quantization approach, and (3) a rule-based approach. Both the neural network and the statistical approach achieved performance above 80%, with the neural networks slightly outperforming the Markov models. Experimental results also show that stress classification could enhance speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-185"
  },
  "jamieson96_icslp": {
   "authors": [
    [
     "D. G.",
     "Jamieson"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "M.",
     "Price"
    ],
    [
     "Vijay",
     "Parsa"
    ],
    [
     "J.",
     "Till"
    ]
   ],
   "title": "Interaction of speech disorders with speech coders: effects on speech intelligibility",
   "original": "i96_0737",
   "page_count": 4,
   "order": 188,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "Modern speech coding schemes have been developed to address the demand for economical spoken language telecommunication of acceptable quality. A variety of speech coding algorithms have been described, which compress speech to facilitate efficient transmission of spoken language over communication networks [2,3,4]. Most such speech coding algorithms are lossy in the sense that the \"processed\" speech is not identical to the original speech. As a result, some distortion is invariably introduced with any lossy speech coding strategy. For this reason, candidate coders undergo detailed evaluation to ensure that the associated speech output is of acceptable quality [1].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-186"
  },
  "vieira96_icslp": {
   "authors": [
    [
     "Maurílio N.",
     "Vieira"
    ],
    [
     "Arnold G. D.",
     "Maran"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Detecting arytenoid cartilage misplacement through acoustic and electroglottographic jitter analysis",
   "original": "i96_0741",
   "page_count": 4,
   "order": 189,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "This paper describes a comparative study of acoustic and electroglottographic (EGG) jitter extracted from simultaneous recordings of 45 dysphonic patients. The estimated jitter values agreed to within ±20% in 67% of the patients, the difference being above +20% and below -20% in 13% and 20% of the cases, respectively. In the group below -20%, EGG jitter increased due to an increase in EGG shimmer and videoendoscopic images revealed abnormal movements of the arytenoid cartilage caused, possibly, by minor selective paralysis of the intrinsic laryngeal muscles. This observation suggests a possible application of signal analysis techniques in support of the diagnosis of laryngeal disorders.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-187"
  },
  "vieira96b_icslp": {
   "authors": [
    [
     "Maurílio N.",
     "Vieira"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Robust F0 and jitter estimation in pathological voices",
   "original": "i96_0745",
   "page_count": 4,
   "order": 190,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "Dysphonic voices were used to compare electroglottographic (EGG) and acoustic measures of fundamental frequency (F0) and jitter using a wavematching and an event-based technique. Continuous speech was considered in the first pan of the study, where the effects of pre-filtering the acoustic signals and linearly smoothing [he F0 contours were analysed. The second pan of the investigation compared jitter from sustained vowels (/i/, /a/, /a/), resulting in poor agreement for HI and Ai/. In W vowels, however, a relatively small mean normalised absolute difference (10.95%) was obtained with a method that is being proposed, which combines peak-picking and zero crossings, being able to detect a waveform pattern observed in such vowels and reject unreliable measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-188"
  },
  "plante96_icslp": {
   "authors": [
    [
     "F.",
     "Plante"
    ],
    [
     "H.",
     "Kessler"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "J.",
     "Earis"
    ]
   ],
   "title": "Speech monitoring of infective laryngitis",
   "original": "i96_0749",
   "page_count": 4,
   "order": 191,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "Many types of parameters have been proposed for the evaluation of vocal cord abnormalities by speech waveform analysis. However, none of them taken separately allows a reliable assessment of the presence and the degree of abnormality. In this study we proposed to combine three different parameters which take into account different consequences of the hoarseness. To assess the effectiveness of the parameters, a group of subjects during and after acute infective laryngitis are compared with control subjects. The jitter, the glottal to noise excitation (GNE) and the normalised error prediction (NEP) are the parameters studied. Preliminary results indicate that reliable discrimination between normal and abnormal patients may be possible using these three parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-189"
  },
  "schoentgen96_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Raoul de",
     "Guchteneere"
    ]
   ],
   "title": "Searching for nonlinear relations in whitened jitter time series",
   "original": "i96_0753",
   "page_count": 4,
   "order": 192,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "Even in sustained vowels, durations of successive glottal cycles are not identical. They fluctuate quasi-randomly around an average. This phenomenon is known as jitter. More recently, correlation analysis has shown that perturbations of neighboring glottal cycles are interdependent, i.e. they are not purely random. We have shown that the non-random component of jitter can be modeled by means of a linear auto-regressive time series model which absorbs correlations between fluctuations of adjacent cycles and leaves a purely random component. The problem here is that nonlinear relations may be missed by correlation analysis or linear auto-regressive modeling. Nonlinear relations could be the signature of chaotic vibratory patterns which some authors expect for some pathological conditions of the vocal folds. We therefore decided to search inside whitened jitter time series (i.e. time series from which any linear correlations had been removed) for nonlinear or other anomalous dependencies between neighboring cycles. The results showed the following. Of the 265 time series, 231 appeared to have been correctly represented by linear auto-regressive models. For 29 series, out of the 34 remaining, deviations from pure randomness could be traced to isolated anomalous glottal cycles which statistical time series models had not taken into account. Finally, five signals, produced by three speakers, were detected which displayed relations between neighboring cycles which could not be traced either to linear correlations or to isolated glitches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-190"
  },
  "gavidiaceballos96_icslp": {
   "authors": [
    [
     "Liliana",
     "Gavidia-Ceballos"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "James F.",
     "Kaiser"
    ]
   ],
   "title": "Vocal fold pathology assessment using AM autocorrelation analysis of the teager energy operator",
   "original": "i96_0757",
   "page_count": 4,
   "order": 193,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "Traditional speech processing methods for laryngeal pathology assessment assume linear speech production, with measures derived from an estimated glottal flow waveform. They normally require the speaker to achieve complete glottal closure, which for many vocal fold pathologies cannot be accomplished. To address this, a nonlinear signal processing approach is proposed which employs a differential Teager energy operator and the energy separation algorithm to obtain formant AM and FM modulations from bandpass filtered speech recordings. A new speech measure is proposed based on parameterization of the autocorrelation envelop of the AM response. Using a cubic model of the autocorrelation envelop, a three dimensional space is formed to assess changes in speech quality. This approach is shown to achieve exemplary detection performance for a set of muscular tension dysphonias. Unlike flow characterization using numerical solutions of Navier Stokes equations, this method is extremely computationally attractive, requiring only NlogN +8N multiplications and N square roots for N samples, and is therefore suitable for real time applications due to its computational simplicity. The new non-invasive method shows conclusively that a fast, effective digital speech processing technique can be developed for vocal fold pathology assessment, without the need for (i) direct glottal flow estimation or (ii) complete glottal closure by the speaker. The proposed method also confirms that alternative nonlinear methods can begin to address the limitations of previous linear approaches for speech pathology assessment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-191"
  },
  "kuehn96_icslp": {
   "authors": [
    [
     "David P.",
     "Kuehn"
    ]
   ],
   "title": "Continuous positive airway pressure (CPAP) in the treatment of hypernasality",
   "original": "i96_0761",
   "page_count": 3,
   "order": 194,
   "p1": "761",
   "pn": "763",
   "abstract": [
    "The purpose of this presentation is to provide a progress report and preliminary data pertaining to a study designed to test the efficacy of continuous positive airway pressure (CPAP) therapy in reducing hypernasality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-192"
  },
  "espywilson96_icslp": {
   "authors": [
    [
     "Carol Y.",
     "Espy-Wilson"
    ],
    [
     "Venkatesh R.",
     "Chari"
    ],
    [
     "Caroline B.",
     "Huang"
    ]
   ],
   "title": "Enhancement of alaryngeal speech by adaptive filtering",
   "original": "i96_0764",
   "page_count": 4,
   "order": 195,
   "p1": "764",
   "pn": "767",
   "abstract": [
    "Artificial larynxes enable adequate communication for people who are unable to use their larynxes. However, the resulting speech has an unnatural quality and is significantly less intelligible than normal speech. One of the major problems with the widely-used Transcutaneous Artificial Larynx (TAL) is the presence of a steady background noise due to the leakage of acoustic energy. In the present study, a novel adaptive filtering architecture was designed and implemented for the purpose of removing the background noise. Perceptual tests were conducted to assess speech from 2 laryngectomees and 2 normal speakers using the Servox TAL, before and after processing by the adaptive filter. Results from the perceptual tests indicate a clear preference for the processed speech and spectral analysis of the reveals a significant reduction in the background source radiation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-193"
  },
  "deng96_icslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Xuemin",
     "Shen"
    ],
    [
     "D. G.",
     "Jamieson"
    ],
    [
     "J.",
     "Till"
    ]
   ],
   "title": "Simulation of disordered speech using a frequency-domain vocal tract model",
   "original": "i96_0768",
   "page_count": 4,
   "order": 196,
   "p1": "768",
   "pn": "771",
   "abstract": [
    "In this paper, we address the issue of how the perception of disorderness in selected types of speech disorders may be correlated with the abnormal articulatory structure and with the related acoustic properties. As a first step towards this end we have developed an articulatory synthesizer based on frequency-domain simulation of vocal-tract wave propagation. The synthesizer has been implemented by three numerical methods - Runge-Kutta, ABCD matrix, and Finite difference, which provide frequency-domain solutions to the transmission-line equation characterizing a lossy vocal tract. The synthesizer is applied in preliminary experiments where the synthesizers outputs are used to match samples from a corpus of steady-state speech sound, obtained from a dysarthric speaker, uttered in the /hV/ context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-194"
  },
  "endo96_icslp": {
   "authors": [
    [
     "Yasuo",
     "Endo"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "A stochastic model of fundamental period perturbation and its application to perception of pathological voice quality",
   "original": "i96_0772",
   "page_count": 4,
   "order": 197,
   "p1": "772",
   "pn": "775",
   "abstract": [
    "This paper proposes a stochastic model of fundamental period perturbation and applies it to the perception of pathological voice quality. A fundamental period perturbation is generated by a second order auto-regressive moving average (ARMA) model which includes excitation by a random white noise. Standard deviation, pole frequency and pole bandwidth were used as parameters of the model. Sustained vowels were synthesized by systematically manipulating the model parameters and subjected to the perceptual experiment to understand roles of the parameters in the perceived quality of pathological voice. Five subjects participated in the experiment and were asked to judge whether a vowel sample was normal or pathological. It was found that perceptual impression of the pathological voice was associated not only with the magnitude but also the pole frequency and bandwidth of the fundamental period perturbation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-195"
  },
  "wallen96_icslp": {
   "authors": [
    [
     "Eric J.",
     "Wallen"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A screening test for speech pathology assessment using objective quality measures",
   "original": "i96_0776",
   "page_count": 4,
   "order": 198,
   "p1": "776",
   "pn": "779",
   "abstract": [
    "Vocal dysfunctions and pathologies can be devastating to one's ability to produce speech properly. A novel method for approaching the problem of speech pathology assessment is presented in this paper. The focus is not to detect or measure all possible pathologies, but rather to assess quality for the case where the probablility of pathology is high. The system is a screening test that combines objective quality measures that examine both excitation and vocal tract characteristics. The five integrated measures are pitch perturbation, amplitude perturbation, a main cepstral peak measure, the log-likelihood measure, and an energy-weighted log-likelihood measure. They are evaluated over six speech phoneme classes and their ability to assess the quality of speech is examined. Ultimately, these measures will be seamlessly integrated into an overall pathology assessment system using a hidden Markov model (HMM) recognizer. To demonstrate the ability of the quality measures to probe the multidimensional perceptual quality space, a neural network based speech pathology detection scheme was established. This system attained an average classification rate of 85.8% for healthy and pathology speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-196"
  },
  "cairns96_icslp": {
   "authors": [
    [
     "Douglas A.",
     "Cairns"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "James F.",
     "Kaiser"
    ]
   ],
   "title": "Recent advances in hypernasal speech detection using the nonlinear teager energy operator",
   "original": "i96_0780",
   "page_count": 4,
   "order": 199,
   "p1": "780",
   "pn": "783",
   "abstract": [
    "Speakers with a defective velopharyngeal mechanism produce speech with inappropriate nasal resonance. It is of clinical interest to detect hypernasality as it is indicative of an anatomical, neurological, or peripheral nervous system problem. While clinical techniques exist for detecting hypernasality, a preferred approach would be noninvasive to maximize patient comfort and naturalness of speaking. In this study, a noninvasive technique based on the Teager Energy operator is proposed. Employing a proposed model for normal and nasalized speech, a significant difference between the Teager Energy profile for lowpass and bandpass filtered nasalized speech is shown, which is nonexistent for normal speech. An optimum classification algorithm is formulated that detects the presence of hypernasality using a measure of the difference in the Teager Energy profiles. The classification algorithm was evaluated using native English speakers producing front and mid vowels. Results show that the presence of hypernasality in speech can be reliably detected (94.7%) using the proposed classification algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-197"
  },
  "honda96_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Shinji",
     "Maeda"
    ],
    [
     "Michiko",
     "Hashi"
    ],
    [
     "Jim",
     "Dembowski"
    ],
    [
     "John R.",
     "Westbury"
    ]
   ],
   "title": "Human palate and related structures: their articulatory consequences",
   "original": "i96_0784",
   "page_count": 4,
   "order": 200,
   "p1": "784",
   "pn": "787",
   "abstract": [
    "The vowel space reflects the right-angled shape of the vocal tract, and many consonants exploit the palatal wall. These two facts suggest the importance of the geometry of peripheral structure in speech production. In this study, the relationship between geometry and articulatory variation was examined using a database of English and Japanese speakers. The geometry of each speaker's vocal tract was defined by a quadrilateral bounded by the palatal plane and other rigid structures. This quadrilateral, whose area we refer to as the articulatory (or A) space, provides indices of pharyngeal distance, lower facial height, mandibular position and inclination, and head rotation. The A-spaces of different speakers vary in size and form: the speakers with longer pharyngeal distance tend to have shorter lower facial height. There is also significant variation among speakers in the degree of inclination of the mandibular symphysis. Qualitative comparisons suggested that speakers' vowel articulations adapt to the form of their respective A-space, while consonant articulations seem to be independent of the A-space.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-198"
  },
  "davis96_icslp": {
   "authors": [
    [
     "Edward P.",
     "Davis"
    ],
    [
     "Andrew",
     "Douglas"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "A continuum mechanics representation of tongue deformation",
   "original": "i96_0788",
   "page_count": 5,
   "order": 201,
   "p1": "788",
   "pn": "792",
   "abstract": [
    "Understanding the kinematics of the tongue during normal speech will provide important information both for accurate modeling of the acoustics of the vocal tract and for clinical diagnosis and enhanced treatment of persons with abnormal speech due to tongue motion impediments. Measuring tongue motion can be done using magnetic resonance imaging (MRI) which cannot provide the temporal resolution required for normal speech, but can create pseudomovement by imaging a single moment in a vocalization from multiple repetitions of a speech gesture. Finding the relevant biomechanical information from these measurements requires a global kinematic model of tongue motion. Preliminary results from modeling the tongue show that the motion is not that of a rigid body. Indeed, a kinematic model can reveal important biomechanical features, such as regional muscle stretch and velocity requirements as well as the strain distribution. The parameters which describe the kinematic model also provide a quantitative means of comparing the tongue motion, and hence the acoustic control of different subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-199"
  },
  "bangayan96_icslp": {
   "authors": [
    [
     "Philbert",
     "Bangayan"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "From MRI and acoustic data to articulatory synthesis: a case study of the lateral approximants in american English",
   "original": "i96_0793",
   "page_count": 4,
   "order": 202,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "Articulatory patterns of the lateral approximant /l/, both dark and light allophones, in American English were analyzed through magnetic resonance imaging (MRI) and electropalatography (EPG). Vocal tract lengths, area functions, and cavity volumes, were measured from the MR images, while EPG data were used for studying inter-and intra-speaker variabilities in lingua-palatal contact patterns. Acoustic modeling was based on the MRI-derived vocal-tract area functions and the acoustic spectra of these sounds. Acoustic modeling utilized the analog circuit simulator HSPICE.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-200"
  },
  "narayanan96_icslp": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Abigail",
     "Kaun"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Peter",
     "Ladefoged"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Liquids in tamil",
   "original": "i96_0797",
   "page_count": 4,
   "order": 203,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "Tamil is unusual among the world's languages in that some of its dialects have five liquids. This paper focuses on the articulatory characterization of these sounds, with the ultimate goal of modeling their production dynamics and articulatory-acoustic mappings. Articulatory data were obtained using different techniques: palatography, magnetic resonance imaging (MRI), and magnetometer (EMMA). This study illustrates the use of multiple techniques for investigating both static and dynamic articulatory characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-201"
  },
  "yang96d_icslp": {
   "authors": [
    [
     "Chang-Sheng",
     "Yang"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Speaker individualities of vocal tract shapes of Japanese vowels measured by magnetic resonance images",
   "original": "i96_0949",
   "page_count": 4,
   "order": 204,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "Three dimensional vocal tract shapes of three males and three females were measured from the magnetic resonance images that were taken during sustained phonation of the five Japanese vowels. Dimensional differences in the vocal tract length of the subjects were quantitatively measured by dividing the entire vocal tract into the oral, the pharyngeal and the laryngeal sections. To investigate main factors that contribute to the differences of the formant patters, uniform and non-uniform normalization methods were applied to the vocal tract shapes. The formant frequencies were also computed from the normalized area functions and compared. The results show that the physiological dimensions of the vocal tract continuously distributed from females to males. The normalization experiments suggest that the non-uniform scaling of the vocal tract was not significant and back/ front cavity volume in addition to the vocal tract length is an important factor to describe speaker individualities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-202"
  },
  "elmasri96_icslp": {
   "authors": [
    [
     "S.",
     "El-Masri"
    ],
    [
     "X.",
     "Pelorson"
    ],
    [
     "P.",
     "Saguet"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Vocal tract acoustics using the transmission line matrix (TLM) method",
   "original": "i96_0953",
   "page_count": 4,
   "order": 205,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "Most traditional theories of speech production are currently based on plane waves and on one-dimensional analysis. It i s however well-known that when the frequency of sound reaches a cut-on frequency, higher acoustical modes start to propagate and can become predominant. It is therefore important to evaluate the effects of these higher modes, especially in order to improve acoustical models of the vocal tract. This paper describes a new numerical method to study the propagation and the radiation of speech sounds, and to compute acoustic characteristics of the vocal tract. This method, named Transmission Line Matrix or Modelling (TLM), has been used for simulating electromagnetic wave propagation and is used here for the first time in acoustics. The TLM method provides time domain solutions in 2D and 3D spaces. The main advantage of this method is the simplicity of formulation and programming for a large range of applications. We first describe the principle on which the TLM method is based. The method as well as the boundary conditions used are validated using classical tests. A systematic study of higher order mode propagation and radiation is then presented. We focus on the influence of some critical parameters such as vocal tract width and location of the sound source. In particular, we show how, using TLM simulation, it is possible to derive modal reflection and transmission characteristics of the vocal tract. A typical example of simulation is presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-203"
  },
  "bailly96_icslp": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Building sensori-motor prototypes from audiovisual exemplars",
   "original": "i96_0957",
   "page_count": 9,
   "order": 206,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "This paper shows how an articulatory model, able to produce acoustic signals from articulatory motion, can learn to speak, i.e. coordinate its movements in such a way that it utters meaningful sequences of sounds belonging to a given language. This complex learning procedure is accomplished in four major steps: (a) a babbling phase, where the device builds up a model of the forward transforms, i.e. the articulatory-to-audiovisual mapping; (b) an imitation stage, where it tries to reproduce a limited set of sound sequences produced by a distal \"teacher\"; (c) a \"shaping\" stage, where phonemes are associated with the most efficient sensori-motor representation; and finally, (d) a \"rhythmic\" phase, where it learns the appropriate coordination of the activations of these sensori-motor targets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-204"
  },
  "bavegard96_icslp": {
   "authors": [
    [
     "Mats",
     "Båvegård"
    ],
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Parameterized VT area function inversion",
   "original": "i96_0961",
   "page_count": 4,
   "order": 207,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "The purpose of our study is to contribute tools for inversion of articulatory to acoustics relations, in specific to perform an estimate of vocal tract area-function parameters from formant frequencies. The inversion is performed in two steps. A first approximation is attained from either a codebook or a neural net and a final optimization is performed by an iterative interpolation for finding a perfect or acceptable match. The study is based on a three-parameter vocal tract model. The codebook relates each of the possible combinations of constriction location, Xc, constriction area, Ac, and the lip parameter, l0/A0 to a corresponding F1, F2, F3 pattern. The neural network output provides the same choice of possible VT states as the codebook. The input to the neural network is normally programmed in terms of formant frequencies but other acoustic attributes can be selected or added. Present experience is limited to vocalic area functions. Our present system provides a rapid conversion of formant frequency data to VT parameters and has provided promising results for short sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-205"
  },
  "dang96_icslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "An improved vocal tract model of vowel production implementing piriform resonance and transvelar nasal coupling",
   "original": "i96_0965",
   "page_count": 4,
   "order": 208,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "This paper proposes an improved vocal tract model of vowel production, which incorporates acoustic effects of the piriform fossa and transvelar nasal coupling. In this study, the vocal tract model was derived from the MRI data of a subject. The piriform fossa was modeled based on the MRI data as a side branch of the vocal tract. The velum wall was modeled as a cascaded impedance of a viscous resistance, a mass and a stiffness, which were estimated by acoustic and mechanical experiments conducted on three subjects. Transfer functions of vowels /a/ and /i/ were computed under the conditions of with and without the piriform fossa and transvelar nasal coupling. The results showed that both the piriform fossa and transvelar coupling play important roles in shaping the first two formants for closed vowels, while the piriform fossa is the main factor affecting the formants of open vowels. By comparing computed transfer functions with real speech spectra for the same subject, it is clarified that our improved model gives a more realistic performance than the traditional model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-206"
  },
  "blackburn96_icslp": {
   "authors": [
    [
     "C. S.",
     "Blackburn"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Pseudo-articulatory speech synthesis for recognition using automatic feature extraction from x-ray data",
   "original": "i96_0969",
   "page_count": 4,
   "order": 209,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "We describe a self-organising pseudo-articulatory speech production model (SPM) trained on an X-ray microbeam database, and present results when using the SPM within a speech recognition framework. Given a time-aligned phonemic string, the system uses an explicit statistical model of co-articulation to generate pseudo-articulator trajectories. From these, parametrised speech vectors are synthesised using a set of artificial neural networks (ANNs). We present an analysis of the articulatory information in the database used, and demonstrate the improvements in articulatory modelling accuracy obtained using our co-articulation system. Finally, we give results when using the SPM to re-score N-best utterance transcription lists as produced by the CUED HTK Hidden Markov Model (HMM) speech recognition system. Relative reductions of 18% in the phoneme error rate and 15% in the word error rate are achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-207"
  },
  "oviatt96b_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ],
    [
     "Gina-Anne",
     "Levow"
    ],
    [
     "Margaret",
     "MacEachern"
    ],
    [
     "Karen",
     "Kuhn"
    ]
   ],
   "title": "Modeling hyperarticulate speech during human-computer error resolution",
   "original": "i96_0801",
   "page_count": 4,
   "order": 210,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "Hyperarticulate speech to computers remains a poorly understood phenomenon, in spite of its association with elevated recognition errors. The present research analyzes the type and magnitude of linguistic adaptations that occur when people engage in error resolution with computers. A semi-automatic simulation method incorporating a novel error generation capability was used to collect speech data immediately before and after system recognition errors, and under conditions varying in error base-rates. Data on original and repeated spoken input, which were matched on speaker and lexical content, then were examined for type and magnitude of linguistic adaptations. Results indicated that speech during error resolution primarily was longer in duration, including both elongation of the speech segment and substantial relative increases in the number and duration of pauses. It also contained more clear speech phonological features and fewer spoken disfluencies. Implications of these findings are discussed for the development of more user-centered and robust error handling in next-generation systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-208"
  },
  "potisuk96_icslp": {
   "authors": [
    [
     "Siripong",
     "Potisuk"
    ],
    [
     "Mary P.",
     "Harper"
    ],
    [
     "Jackson T.",
     "Gandour"
    ]
   ],
   "title": "Using stress to disambiguate spoken Thai sentences containing syntactic ambiguity",
   "original": "i96_0805",
   "page_count": 4,
   "order": 211,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "We have developed a Bayesian classifier to determine whether syllables in connected Thai speech are weakly or strongly stressed by using five acoustic parameters: syllable rhyme duration, mean F0, F0 standard deviation, mean energy, and the standard deviation of the energy. With speaker-dependent data normalization, we achieved a classification accuracy of 99%. The classification accuracy drops to 96% when we used speaker-independent normalization. We have also developed prosodic constraints that can use this stress information to syntactically disambiguate a class of ambiguous sentences that arise from the use of compounding in spoken Thai.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-209"
  },
  "hsieh96_icslp": {
   "authors": [
    [
     "Hung-yun",
     "Hsieh"
    ],
    [
     "Ren-yuan",
     "Lyu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Use of prosodic information to integrate acoustic and linguistic knowledge in continuous Mandarin speech recognition with very large vocabulary",
   "original": "i96_0809",
   "page_count": 4,
   "order": 212,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "This paper presents a new approach to use prosodic information for the integration of acoustic and linguistic knowledge in continuous Mandarin speech with very large vocabulary. Since the overhead computation incurred from unification of search space is confined to the syllable boundaries, the use of prosodic information to reduce the syllable boundary hypotheses as well as the syllable matching length is shown to be effective. The inherent complexity with the very large vocabulary is also reduced by the use of phrase boundary hypotheses conjectured via the phrase-final lengthening. Experimental results show a 47.2% recognition time save with only 5.67% error rate increase using the syllable and phrase boundary hypotheses conjectured from prosodic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-210"
  },
  "rao96_icslp": {
   "authors": [
    [
     "G. V. Ramana",
     "Rao"
    ],
    [
     "J.",
     "Srichand"
    ]
   ],
   "title": "Word boundary detection using pitch variations",
   "original": "i96_0813",
   "page_count": 4,
   "order": 213,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "This paper proposes a method for detecting word boundaries. This method is based on the behaviour of the pitch frequency across the sentences. The pitch frequency (F0) is found to rise in a word and fall to the next word. The presence of this fall is proposed as a means of detecting word boundaries. Four major Indian languages are used and the results show that nearly 85% of the word boundaries were correctly detected. The same method used for German language shows that nearly 65% of the word boundaries were correctly detected. The implications of these result in the development of a continuous speech recognition system are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-211"
  },
  "sakurai96_icslp": {
   "authors": [
    [
     "Atsuhiro",
     "Sakurai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Detection of phrase boundaries in Japanese by low-pass filtering of fundamental frequency contours",
   "original": "i96_0817",
   "page_count": 4,
   "order": 214,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "Major syntactic boundaries are often accompanied by a rise in the phrase component of the fundamental frequency (F0) contour. Detecting such rises, therefore, can be significantly helpful to the speech recognition process. We developed a method to detect syntactic boundaries with phrase-component rise (henceforth, phrase boundaries), based on the compression of the accent component of the F0 contour (in logarithmic scale), using a low-pass filter. In this method, F0 contours are viewed as signals in the time domain, which can be roughly separated into phrase and accent components due to their different frequency contents. Phrase boundaries are detected whenever a significant rise occurs in the derivative of the filtered F0 contour. (The concepts of phrase and accent components can be found in [1]). The method managed to detect about 77% of manually detectable phrase boundaries, though with a relatively high insertion rate. The insertion rate can be reduced by using the partial AbS method, proposed by the authors [7].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-212"
  },
  "pagel96_icslp": {
   "authors": [
    [
     "Vincent",
     "Pagel"
    ],
    [
     "Noelle",
     "Carbonell"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "A new method for speech delexicalization, and its application to the perception of French prosody",
   "original": "i96_0821",
   "page_count": 4,
   "order": 215,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "This paper describes a perception experiment which aims at testing the ability of subjects to discriminate prosodic boundaries when provided with fundamental frequency and duration. Their task consists in listening to a high quality speech synthesizer producing nonsense speech that copies the prosodic parameters of a French corpus. Subjects have marked boundary syllables with a device that synchronizes sound and orthographic transcription. Results show that the delexicalization procedure is ecological, and that duration strongly influences the perception of fundamental frequency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-213"
  },
  "bub96_icslp": {
   "authors": [
    [
     "Udo",
     "Bub"
    ]
   ],
   "title": "Task adaptation for dialogues via telephone lines",
   "original": "i96_0825",
   "page_count": 4,
   "order": 216,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "This paper describes our successful ongoing approaches toward better recognition accuracy for flexible interactive systems in automatic speech recognition. Degradation in performance of speech recognition systems is observed whenever any current application differs from the conditions during training time. Main speaker independent causes for these deteriorations are changes in transmission channels and changes in the task to be fulfilled. We present our results of research on changing tasks, i.e. more specifically on changing dictionaries. We propose an in-service adaptation technique that is speaker independent, works under unsupervised conditions, and has a long term memory. On 2000 adaptation words a reduction of error rate of more than 40% at negligible computational costs is achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-214"
  },
  "cole96_icslp": {
   "authors": [
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Troy",
     "Bailey"
    ]
   ],
   "title": "The influence of bigram constraints on word recognition by humans: implications for computer speech recognition",
   "original": "i96_0829",
   "page_count": 4,
   "order": 217,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "The gap between human and machine performance on speech recognition tasks is still very large. Recognition of words in telephone conversations is slightly better than 50%, based on results reported on the Switchboard corpus by leading researchers using state of the art HMM systems. We know from our own experience that human perception typically delivers much more accurate word recognition over the telephone. Why is there such a large gap between machine and human performance, and what can be done to close this gap? One way to address this question is to study the sources of linguistic information in the speech signal that are known to be important for word recognition, and measure how well machine systems utilize this information relative to humans. As an initial step in this direction, we measured word recognition performance of listeners presented with words from the Switchboard corpus. Stimuli consisted of actual utterances excised from the Switchboard corpus, nigh quality recordings of utterances that occurred in Switchboard conversations, and recordings of word sequences with zero, medium and high bigram probabilities based on a language model computed from transcriptions of the Switchboard corpus. The results show that human listeners are very good at recognizing words in the absence of word sequence constraints, and that statistical language models fail to capture much of the high level lingusitic information needed to recognize words in fluent speech. The results are discussed in terms of their implications to current approaches to acoustic and language modeling in computer speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-215"
  },
  "kobayashi96b_icslp": {
   "authors": [
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "ALICE: acquisition of language in conversational environment - an approach to weakly supervised training of spoken language system for language porting",
   "original": "i96_0833",
   "page_count": 4,
   "order": 218,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "Aiming at reducing the woik required for the language potting of spoken language system, a conversational second language acquisition system is proposed. This system need only small lexicon in the initial stage. It need neither hand-description of rules nor the collection/annotation of large corpus. It refer the corpus of semantic frames which is obtained through development/use of first language version of the system. Then, it make hypotheses which leed to reasonable semantic frames and parse the sentence with them. The system drive the back-end system with the interpretation and confirm if the result is suit for the user's will. With above process, the weakly supervised training of the spoken language system is realized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-216"
  },
  "yoshimura96_icslp": {
   "authors": [
    [
     "Takashi",
     "Yoshimura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Hiroshi",
     "Ohmura"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Pitch pattern clustering of user utterances in human-machine dialogue",
   "original": "i96_0837",
   "page_count": 9,
   "order": 219,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "This paper argues about pitch pattern variations of user utterances in human-machine dialogue. For intelligent human-machine communication, it is essential that machines understand prosodic characteristics which imply a user's various attitude, emotion and intention beyond vocabulary. Our original focus is on particularly distinct pitch patterns and their roles in the actual dialogues. We used human-machine dialogues collected by a Wizard of OZ simulation, Many utterance segments belonged to clusters that were prosodically flat patterns. From the result, we considered that utterances which belonged to the other clusters and those which were far from the centroids included non-verbal information. In these utterances, there were talks to themselves and questions to the machine including emotional expressions of a puzzle or a surprise. These pitch patterns were not only rich in ups and downs, but also their slopes were upward, while the pitch pattern were generally even or a little downward. These results indicate that peculiar pitch period patterns show non-verbal expressions. In order to actually utilize such information on human-machine interactions, the representative pitch patterns should be investigated concerning their relationship to various types of communication.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-217"
  },
  "amengual96_icslp": {
   "authors": [
    [
     "J. C.",
     "Amengual"
    ],
    [
     "Enrique",
     "Vidal"
    ],
    [
     "J. M.",
     "Benedí"
    ]
   ],
   "title": "Simplifying language through error-correcting decoding",
   "original": "i96_0841",
   "page_count": 4,
   "order": 220,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "In many speech processing tasks, most of the sentences generally convey rather simple meanings. In these tasks, the \"wordrecognition\" problem is much more difficult than the underlying \"speech understanding\" problem would be. Accordingly we try to develop an adequate framework to focus on a properly defined \"understanding\" of the sentences rather than \"recognizing\" the (possibly) superfluous words. This can be seen as closely related with Spontaneous Language Understanding and Disfluence Modeling. In our approach, these problems are placed under the framework of Error-Correcting Decoding (ECD). A complex task is modeled in terms of a basic stochastic grammar, G, and an Error Model, E (taking insertions, substitutions and deletions into account). G should account for the basic (syntactic) structures underlying this task which would convey the semantics. E should account for general vocabulary variations, speech disfluencies, word disappearance, superfluous words, and so on. Each \"complex\" user sentence, x, will thus be considered as a corrupted version (according to E) of some \"simple\" sentence y of L(G). Recognition can then be seen as an ECD process: given x, find a sentence y of L(G) with maximum posterior probability. We introduce fast ECD techniques and adequate procedures for simultaneously training G and E and apply these ideas to a simple task with results showing the potential of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-218"
  },
  "cettolo96_icslp": {
   "authors": [
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Anna",
     "Corazza"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "A mixed approach to speech understanding",
   "original": "i96_0845",
   "page_count": 4,
   "order": 221,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "This paper presents a mixed approach to spoken language understanding that tries to make best use of the advantages of both statistical and knowledge-based algorithms. Results obtained on ATIS (Air Travel Information System) scenario transferred to Italian language are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-219"
  },
  "gauvain96_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "J. J.",
     "Gangolf"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Speech recognition for an information kiosk",
   "original": "i96_0849",
   "page_count": 4,
   "order": 222,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "In the context of the ESPRIT MASK project we face the problem of adapting a \"state-of-the-art\" laboratory speech recognizer for use in the real world with naive users. The speech recognizer is a software-only system that runs in real-time on a standard Risc processor. All aspects of the speech recognizer have been reconsidered from signal capture to adaptive acoustic models and language models. The resulting system includes such features as microphone selection, response cancellation, noise compensation, query rejection capability and decoding strategies for real-time recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-220"
  },
  "strik96_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Albert",
     "Russel"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Localizing an automatic inquiry system for public transport information",
   "original": "i96_0853",
   "page_count": 4,
   "order": 223,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "This paper reports on the development of a spoken dialogue system for providing information about public transport in the Netherlands. It is explained how a German prototype was adapted for Dutch. Emphasis is laid on the specific approach chosen to collect speech material that could be used to gradually improve the system. The pros and cons of this method are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-221"
  },
  "marcus96_icslp": {
   "authors": [
    [
     "Stephen M.",
     "Marcus"
    ],
    [
     "Deborah W.",
     "Brown"
    ],
    [
     "Randy G.",
     "Goldberg"
    ],
    [
     "Max S.",
     "Schoeffler"
    ],
    [
     "William R.",
     "Wetzel"
    ],
    [
     "Richard R.",
     "Rosinski"
    ]
   ],
   "title": "Prompt constrained natural language - evolving the next generation of telephony services",
   "original": "i96_0857",
   "page_count": 4,
   "order": 224,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "This paper describes the design and development of an automated car reservation system using large vocabulary natural language speech recognition. Reservations were made over the public switched telephone network by calling an 800 number from anywhere in the United States, and car availability checked in real-time with a major international car rental company. This system was designed to support first time users without requiring extensive written instructions, using current state-of-the art product-grade speech recognition technology. A Prompt Constrained Natural Language (PCNL) paradigm was used to encourage users to respond within the constraints of the recognition technology. The results of an extended technology trial demonstrate the viability of services using automatic speech recognition to support transaction processing services. The extension of this technology to new domains will be described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-222"
  },
  "kawahara96b_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Key-phrase detection and verification for flexible speech understanding",
   "original": "i96_0861",
   "page_count": 4,
   "order": 225,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "A novel framework of robust speech understanding is presented. It is based on a detection and verification strategy. It extracts the semantically significant parts and rejects the irrelevant parts rather than decoding the whole utterances. There are two key features in our strategy. Firstly, discriminative verifier is integrated to suppress false alarms. It uses anti-subword models specifically trained to verify the recognition results. The second feature is the use of a keyphrase network as the detection unit. It embeds stochastic constraint of keyword and keyphrase connections to improve the coverage and detection rates. The automatic generation of the keyphrase network structure is also addressed. This top-down variable-length language model can be trained with a small corpus and ported to different tasks. This property coupled with the vocabulary-independent detector and verifier enhances the portability of our framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-223"
  },
  "suhm96_icslp": {
   "authors": [
    [
     "Bernhard",
     "Suhm"
    ],
    [
     "Brad",
     "Myers"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Interactive recovery from speech recognition errors in speech user interfaces",
   "original": "i96_0865",
   "page_count": 4,
   "order": 226,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "We present a multimodal approach to interactive recovery from speech recognition errors for the design of speech user interfaces. We propose a framework to compare various error recovery methods, arguing that a rational user will prefer interaction methods which provide an optimal Irade off between accuracy, speed and naturalness. We describe a prototypical implementation of multimodal interactive error recovery and present results from a preliminary evaluation in form filling and speech to speech translation tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-224"
  },
  "issar96_icslp": {
   "authors": [
    [
     "Sunil",
     "Issar"
    ]
   ],
   "title": "Estimation of language models for new spoken language applications",
   "original": "i96_0869",
   "page_count": 4,
   "order": 227,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "Spoken language interfaces can provide natural communication for many database retrieval tasks. The CMU ATIS system provides an example of accessing airline information using spoken natural language queries. However, a lot of training data is needed to develop a spoken language application. For example, we need training data to generate a language model that can be used by the recognizer to reduce the search space. In this paper, we will address some issues arising from small amount of training data available for a new spoken language application. We are working on a spoken language interface to access information from a library catalogue. The catalogue contains around 13,000 titles, 6000 authors and 19000 subjects. There are more than 20,000 words in the dictionary. The user can seek information about books, authors, subjects, publishers, etc. For example, \"Id like to see books dealing with science fiction by Clarke.\" We will describe some language modelling experiments for this task. We will briefly describe a speech interface [5] for a library catalogue. We will also review class-based language models and describe their limitations. Finally, we will present our approach to building statistical language models for new spoken language applications. This is important because a lot of training data is normally needed to generate a language model. However, it is not practical to have or collect a large corpus of data for each new spoken language application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-225"
  },
  "shen96_icslp": {
   "authors": [
    [
     "Xuemin",
     "Shen"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Anisa",
     "Yasmin"
    ]
   ],
   "title": "H-infinity filtering for speech enhancement",
   "original": "i96_0873",
   "page_count": 4,
   "order": 228,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "In this paper, a new approach based on the H-infinity filtering is presented for speech enhancement. This approach differs from the traditional modified Wiener/Kalman filtering approach in the following two aspects: 1) no a priori knowledge of the noise statistics is required; instead the noise signals are only assumed to have finite energy; 2) the estimation criterion for the filter design is to minimize the worst possible amplification of the estimation error signal in terms of the modeling errors and additive noises. Since most additive noises in speech are not Gaussian, this approach is highly robust and is more appropriate in practical speech enhancement. The global signal-to-noise ratio (SNR), time domain speech representation and listening evaluations are used to verify the performance of the H-infinity filtering algorithm. Experimented results show that the filtering performance is better than other speech enhancement approaches in the literature under similar experimental conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-226"
  },
  "vaseghi96_icslp": {
   "authors": [
    [
     "Saeed V.",
     "Vaseghi"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "A comparitive analysis of channel-robust features and channel equalization methods for speech recognition",
   "original": "i96_0877",
   "page_count": 4,
   "order": 229,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "The use of a speech recognition system with telephone channel environments, or different microphones, requires channel equalisation. In speech recognition, the speech models provide a bank of statistical information that can be used in the channel identification and equalisation process. in this paper we consider HMM-based channel equalistaion, and present results demonstrating that substantial improvement can be obtained through the equalisation process. An alternative method is to use a set of features which is more robust to channel distortion. Channel distortions result in an amplitude-tilt of the speech cepstrum, and so differential cepstral features should provide a measure of immunity to channel distortions. In particular the cepstral-time feature matrix, in addition to providing a framework for representing speech dynamics, can be made robust to channel distortions. We present results demonstrating that a major advantage of cepstral-time matrices is their channel insensitive character.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-227"
  },
  "shen96b_icslp": {
   "authors": [
    [
     "Jia-lin",
     "Shen"
    ],
    [
     "Wen-liang",
     "Hwang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Robust speech recognition features based on temporal trajectory filtering of frequency band spectrum",
   "original": "i96_0881",
   "page_count": 4,
   "order": 230,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "This paper presents the use of a variety of filters in the temporal trajectories of frequency band spectrum to extract speech recognition features for environmental robustness. Three kind of filters for emphasizing the statistically important parts of speech are proposed. First, a bank of RASTA-like band-pass filters to fit the statistical peaks of modulation frequency band spectrum of speech are used. Secondly, a three-channel octave band-filter band with a smoothed rectangular window spline is applied. Thirdly, a data- driven filter is developed. Experimental results show that significant improvements for speech recognition using the proposed feature extraction approach under noisy environments can be achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-228"
  },
  "power96_icslp": {
   "authors": [
    [
     "Kevin",
     "Power"
    ]
   ],
   "title": "Durational modelling for improved connected digit recognition",
   "original": "i96_0885",
   "page_count": 4,
   "order": 231,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "A durational modelling technique is proposed for CDHMM-based connected digit recognition. This reduces the insertion error rate, which is typically the most frequent recognition error observed when no grammar constraint is applied. Insertion errors can be attributed in part to the acknowledged weakness of the acoustic models for accurate temporal modeling of speech signals. Two forms of durational model are investigated: an expanded-state model and an explicit model. Both forms of model significantly reduce the number of insertion errors and hence the digit string error rate. A modification to the explicit model which also accounts for speaking rate is described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-229"
  },
  "avendano96_icslp": {
   "authors": [
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Study on the dereverberation of speech based on temporal envelope filtering",
   "original": "i96_0889",
   "page_count": 4,
   "order": 232,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "In this paper we explore speech dereverberation techniques whose principle is the recovery of the envelope modulations of the original (anechoic) speech [3],[4]. Based on our previous experience with such kind of processing for additive noise reduction applications, we apply a data designed filter-bank technique [2] to the reverberant speech. Comparing our results with other works we discuss the effectiveness and limitations of this type of approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-230"
  },
  "brants96_icslp": {
   "authors": [
    [
     "Thorsten",
     "Brants"
    ]
   ],
   "title": "Estimating Markov model structures",
   "original": "i96_0893",
   "page_count": 4,
   "order": 233,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "We investigate the derivation of Markov model structures from text corpora. The structure of a Markov model is its number of states plus the set of outputs and transitions with non-zero probability. The domain of the investigated models is part-of-speech tagging. Our investigations concern two methods to derive Markov models and their structures. Both are able to form categories and allow words to belong to more than one of them. The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models. The second method is model splitting, which is the inverse procedure and starts with a small and general model. States are successively split to generate larger and more specific models. In an experiment, we show that the combination of these techniques yields tagging accuracies that are at least equivalent to those of standard approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-231"
  },
  "ringger96_icslp": {
   "authors": [
    [
     "Eric K.",
     "Ringger"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "A fertility channel model for post-correction of continuous speech recognition",
   "original": "i96_0897",
   "page_count": 4,
   "order": 234,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "We have implemented a post-processor called SPEECHPP to correct word-level errors committed by an arbitrary speech recognizer. Applying a noisy-channel model, SPEECHPP uses aViterbi beam-search that employs language and channel models. Previous work demonstrated that a simple word-for-word channel model was sufficient to yield substantial increases in word accuracy. This paper demonstrates that some improvements in word accuracy result from augmenting the channel model with an account of word fertility in the channel. This work further demonstrates that a modern continuous speech recognizer can be used in \"black-box\" fashion for robustly recognizing speech for which the recognizer was not originally trained. This work also demonstrates that in the case where the recognizer can be tuned to the new task, environment, or speaker, the post-processor can also contribute to performance improvements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-232"
  },
  "yasukawa96_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Yasukawa"
    ]
   ],
   "title": "Restoration of wide band signal from telephone speech using linear prediction error processing",
   "original": "i96_0901",
   "page_count": 4,
   "order": 235,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "This paper describes a system that can enhance the quality of speech signals that are severely band limited during regular telephone speech transmission. We have already proposed a spectrum widening method that utilizes aliasing in sampling rate conversion and digital filtering for spectrum shaping. This paper proposes a new method using linear prediction (LP) residual error processing and parameter (coefficients) mapping using linear extrapolation that offers improved performance in terms of the spectrum distortion characteristics. Real time hardware implementation procedures are discussed. The proposed method can effectively enhance speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-233"
  },
  "matsumoto96_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Noboru",
     "Naitoh"
    ]
   ],
   "title": "Smoothed spectral subtraction for a frequency-weighted HMM in noisy speech recognition",
   "original": "i96_0905",
   "page_count": 4,
   "order": 236,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "This paper proposes improved methods of smoothed spectral subtraction to enhance the recognition performance of a frequency- weigh ted HMM (HMM-FVV) in very noisy environments. The conventional spectral subtraction tends to produce discontinuity in estimated power spectra. This distortion is undesirable for HMM-FW which uses group delay spectra as feature vectors. In order to remove this distortion, this paper proposes two frequency smoothing methods in log-spectral domain: (1) a low-pass Liftering by JDCT, and (2) a weighted minimum mean square error method (WMSE) which fits cosine series to an estimated log-power spectrum. The results shows that the smoothers are very effective under very noisy conditions, especially for the frequency-weighted HMM. The WMSE method combined with HMM-FW achieves the highest recognition accuracies, for instance, improving recognition rate from 687c to 88% at -6dB SNR of car noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-234"
  },
  "woods96_icslp": {
   "authors": [
    [
     "William S.",
     "Woods"
    ],
    [
     "Martin",
     "Hansen"
    ],
    [
     "Thomas",
     "Wittkop"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "A simple architecture for using multiple cues in sound separation",
   "original": "i96_0909",
   "page_count": 4,
   "order": 237,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "The present work concerns a system aimed at enhancing a target talker under varying signal conditions based on the use of several different types of information or \\cues\". Toward this end, an architecture designed to combine separately operating estimators is described and evaluated. The architecture is currently implemented using spatial- and periodicity-based enhancement algorithms, and evaluated using a male target talker and female jammer talker under several spatial and target-to-jammer ratio (TJR) conditions. Using a TJR estimation algorithm, the implementation is shown to yield improved TJR under all tested input TJRs (-4, 0, 4, and 8 dB) and spatial conditions (target and jammer straight ahead; target ahead and jammer at 60 degrees). Improvement ranges from 1.4 to 4.5 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-235"
  },
  "petek96b_icslp": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "On the robust automatic segmentation of spontaneous speech",
   "original": "i96_0913",
   "page_count": 4,
   "order": 238,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "The results from applying an improved algorithm in the task of automatic segmentation of spontaneous telephone quality speech are presented, and compared to the results from those resulting from super imposing white noise. Three segmentation algorithms are compared which are all based on variants of the Spectral Variation Function. Experimental results are obtained on the OGI multi-language telephone speech corpus (OGI TS).We show that the use of the auditory forward and backward masking effects prior to the SVF computation increases the robustness of the algorithm to white noise. When the average signal-to-noise ratio (SNR) is decreased to 10dB the peak ratio (defined as the ratio of the number of peaks measured at the target over the original SNRs) is increased by 16%, 12%, and 11% for theMFC(Mel-FrequencyCepstra), RASTA(RelAtive SpecTrAl processing), and the FBDYN (Forward-Backward auditory masking DYNamic cepstra) SVF segmentation algorithms, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-236"
  },
  "miglietta96_icslp": {
   "authors": [
    [
     "C. G.",
     "Miglietta"
    ],
    [
     "C.",
     "Mokbel"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "Bayesian adaptation of speech recognizers to field speech data",
   "original": "i96_0917",
   "page_count": 4,
   "order": 239,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "This work studies a Bayesian (or Maximum A Posteriori MAP) approach to the adaptation of Continuous Density Hidden Markov Models (CDHMMs) to a specific condition of a speech recognition application. In order to improve the model robustness, CDHMMs formerly trained from laboratory data are then adapted using context dependent field utterances. Two specific problems have to be faced when using the MAP approach: the estimation of the a priori distribution parameters and the lack of field adaptation data for some distributions of the CDHMM. To estimate the a priori distribution parameters, we need to identify different realizations of the model parameters. Three different solutions are proposed and evaluated. To overcome the lack of adaptation data, field acoustical training frames may be shared among similar distributions. This is performed using an acoustical tree, obtained by progressively clustering the model distributions. Recognition results show that MAP adapted models significantly outperform those trained by Maximum Likelihood (ML), specifically when the field data set is small.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-237"
  },
  "darlington96_icslp": {
   "authors": [
    [
     "A. J.",
     "Darlington"
    ],
    [
     "D. J.",
     "Campbell"
    ]
   ],
   "title": "Sub-band adaptive filtering applied to speech enhancement",
   "original": "i96_0921",
   "page_count": 4,
   "order": 240,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "An adaptive noise cancellation scheme for speech processing is proposed. In this, the adaptive filters are implemented in frequency-limited sub-bands. In previous work, the filters had been distributed in a linear fashion in the frequency domain. This work investigates the effects of spacing the filters more in sympathy with the signal power and spectral characteristics. It emerges that improvements in signal-to-noise ratio of processed noisy speech signals may be obtained when the sub-bands are spaced according to a published cochlear function.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-238"
  },
  "openshaw96_icslp": {
   "authors": [
    [
     "J. P.",
     "Openshaw"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Noise robust estimate of speech dynamics for speaker recognition",
   "original": "i96_0925",
   "page_count": 4,
   "order": 241,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "This paper investigates the robustness of cepstral based features with respect to additive noise, and details two methods of increasing the robustness with minimal need for a-priori knowledge of the noise statistics. The first approach is a form of noise masking which adds a fixed offset to the linear spectral estimate. The second is a form of sub-band filtering, again in the linear domain, which estimates the dynamic content of the speech using Fourier transforms. This avoids negative values normally inherent in such filtering and which presents difficulties in deriving log estimates. Both methods are shown to provide useful levels of robustness to additive noise, for example, speaker identification error rates in SNR mis-matched conditions of 15 dB are reduced from 60.5% for standard mel cepstra to 13.8% and 24.1% for the two approaches respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-239"
  },
  "ortegagarcia96_icslp": {
   "authors": [
    [
     "Javier",
     "Ortega-García"
    ],
    [
     "Joaquín",
     "González-Rodríguez"
    ]
   ],
   "title": "Overview of speech enhancement techniques for automatic speaker recognition",
   "original": "i96_0929",
   "page_count": 4,
   "order": 242,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "Real world conditions differ from ideal or laboratory conditions, causing mismatch between training and testing phases, and consequently, inducing performance degradation in automatic speaker recognition systems [1]. Many strategies have been adopted to cope with acoustical degradation; in some applications of speaker identification systems a clean sample of speech, prior to the recognition stage, is needed. This has justified the use of procedures that may reduce the impact of acoustical noise on the desired signal, giving rise to techniques involved in the enhancement of noisy speech [2, 9]. In this paper, a comparative performance analysis of single-channel (based in classical spectral subtraction and some derived alternatives), dual-channel (based in adaptive noise cancelling) and multi-channel (using microphone arrays) speech enhancement techniques, with different types of noise at different SNRs, as a pre-processing stage to an ergodic HMM-based speaker recognizer, is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-240"
  },
  "harte96_icslp": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Saeed V.",
     "Vaseghi"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Dynamic features for segmental speech recognition",
   "original": "i96_0933",
   "page_count": 4,
   "order": 243,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "Speech models and features that emphasise the dynamic aspects of speech can provide improved speech recognition. The cepstral time matrix has been established as a successful method of encoding dynamics. This paper extends this set of dynamic features, considering cepstral time features on both a segmental and subsegmental level. This offers the potential of using a conditional pdf for the state observation within a HMM and incorporating this into the training stage. Methods of linear discriminative analysis are applied to the new feature set to identify the subset of features making the greatest contribution to the task of recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-241"
  },
  "koizumi96b_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Mikio",
     "Mori"
    ],
    [
     "Shuji",
     "Taniguchi"
    ]
   ],
   "title": "Speech recognition based on a model of human auditory system",
   "original": "i96_0937",
   "page_count": 4,
   "order": 244,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "This paper deals with a new phoneme recognition system based on a model of human auditory system. This system is made up of a model of human cochlea and a simple multi-layer recurrent neural network which has feedback connections of self-loop type. The ability of this system has been investigated by a phoneme recognition experiment using a number of Japanese words uttered by a native male speaker. The result of the experiment shows that recognition accuracies achieved with this system in the presence of noise are higher than those obtained by a combination of frequency spectral analysis by DFT and conventional feedforward neural network and that the cochlea model effectively prevents the deterioration due to noise of recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-242"
  },
  "salavedra96_icslp": {
   "authors": [
    [
     "J. M.",
     "Salavedra"
    ],
    [
     "E.",
     "Masgrau"
    ]
   ],
   "title": "APVQ encoder applied to wideband speech coding",
   "original": "i96_0941",
   "page_count": 4,
   "order": 245,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "This paper describes a coding scheme for broadband speech (sampling frequency 16KHz). We present a wideband speech encoder called APVQ (Adaptive Predictive Vector Quantization). It combines Subband Coding, Vector Quantization and Adaptive Prediction as it is represented in Fig.1. Speech signal is split in 16 subbands by means of a QMF filter bank and so every subband is 500Hz wide. This APVQ encoder can be seen as a vectorial extension of a conventional ADPCM encoder. In this scheme, signal vector is formed with one sample of the normalized prediction error signal coming from different subbands and then it is vector quantized. Prediction error signal is normalized by its gain and normalized prediction error signal is the input of the VQ and therefore an adaptive Gain-Shape VQ is considered. This APVQ Encoder combines the advantages of Scalar Prediction and those of Vector Quantization. We evaluate wideband speech coding in the range from 1.5 to 2 bits/sample, that leads to a coding rate from 24 to 32 kbps.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-243"
  },
  "zhou96_icslp": {
   "authors": [
    [
     "Jin",
     "Zhou"
    ],
    [
     "Yair",
     "Shoham"
    ],
    [
     "Ali",
     "Akansu"
    ]
   ],
   "title": "Simple fast vector quantization of the line spectral frequencies",
   "original": "i96_0945",
   "page_count": 4,
   "order": 246,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "In this paper we propose a simple fast-search VQ of the LSFs to be used on top of the split VQ, that is, in each of the sub-vector domains. The main trait of the proposed method is that no suboptimal codebooks are used and there is no further reduction in performance. In each sub-vector domain, a full-size optimally trained codebook, typically of size 1024, is searched using a fast-search algorithm. The result of this search is identical to that of a full-search, yet, only about 25% of a full-search complexity is needed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-244"
  },
  "matsui96_icslp": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "N-best-based instantaneous speaker adaptation method for speech recognition",
   "original": "i96_0973",
   "page_count": 4,
   "order": 247,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "An instantaneous speaker adaptation method is proposed that uses N-best decoding for continuous mixture-density hidden-Markov-model based speech recognition systems. An N-best paradigm of multiple-pass search strategies is used that makes this method effective even for speakers whose decodings using speaker-independent models are error-prone. To cope with an insufficient amount of data, our method uses constrained maximum a posteriori estimation, in which the parameter vector space is clustered, and a mixture-mean bias is estimated for each cluster. Moreover, to maintain continuity between clusters, a bias for each mixture-mean is calculated as the weighted sum of the estimated biases. Performance evaluation using connected-digit (four-digit strings) recognition experiments performed over actual telephone lines showed more than a 20% reduction in the error rates, even for speakers whose decodings using speaker-independent models were error-prone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-245"
  },
  "montacie96_icslp": {
   "authors": [
    [
     "C.",
     "Montacié"
    ],
    [
     "M.-J.",
     "Caraty"
    ],
    [
     "C.",
     "Barras"
    ]
   ],
   "title": "Mixture splitting technic and temporal control in a HMM-based recognition system",
   "original": "i96_0977",
   "page_count": 4,
   "order": 248,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "In this paper, we study various technics to improve the performance, to reduce the computation cost and the required memory of a recognition system based on HMM. For the efficiency of the system, we first study the optimization of the number of HMM parameters according to training data. We experiment a temporal control of the phonetic transitions on lexical decoding task with a significant 5% improvement. Finally, a preliminary method selecting dynamically a sublexicon is studied in order to reduce the lexical decoding cost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-246"
  },
  "yao96_icslp": {
   "authors": [
    [
     "Lei",
     "Yao"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "A unified spectral transformation adaptation approach for robust speech recognition",
   "original": "i96_0981",
   "page_count": 4,
   "order": 249,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "In this paper, Canonical Correlation Based Compensation (CCBC) is proposed as an unified approach to cope with the mismatch between training and test set. The mismatch between training and test conditions can be simply clustered into three classes: differences of speakers, changes of recording channel and effects of noisy environment. In previous work, we had used CCBC approach with some modifications to make our speech recognizer robust to the noisy environment successfully [1]. Recently, the same approach has been extended for speaker and channel adaptation. The results of our experiments show that CCBC approach well compensated all three kinds of distortion source between training and test conditions. In order to compare the performance of CCBC with that of some conventional adaptation approaches, the capacities of the techniques of cepstral mean normalization, RASTA and Lin-Log RASTA are tested. We find that CCBC has better performance than them. As an very important problem in CCBC approach, the selection of appropriate reference speech data is also discussed in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-247"
  },
  "huo96_icslp": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "On-line adaptive learning of the correlated continuous density hidden Markov models for speech recognition",
   "original": "i96_0985",
   "page_count": 4,
   "order": 250,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "We extend our previously proposed quasi-Bayes adaptive learning framework to cope with the correlated continuous density hidden Markov models with Gaussian mixture state observation densities in which all mean vectors are assumed to be correlated and have a joint prior distribution. A successive approximation algorithm is proposed to implement the correlated mean vectors' updating. As an example, by applying the method to on-line speaker adaptation application, the algorithm is experimentally shown to be asymptotic convergent as well as being able to enhance the efficiency and the effectiveness of the Bayes learning by taking into account the correlation information between different models. The technique can be used to cope with the time-varying nature of some acoustic and environmental variabilities, including mismatches caused by changing speakers, channels, transducers, environments and so on.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-248"
  },
  "strom96_icslp": {
   "authors": [
    [
     "Nikko",
     "Ström"
    ]
   ],
   "title": "Speaker adaptation by modeling the speaker variation in a continuous speech recognition system",
   "original": "i96_0989",
   "page_count": 4,
   "order": 251,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "A method for unsupervised instantaneous speaker adaptation is presented and evaluated on a continuous speech recognition task in a man-machine dialogue system. The method is based on modeling of the systematic speaker variation. The variation is modeled by a low-dimensional speaker space and the classification of speech segments is conditioned by the position in the speaker space. Because the effect of the speaker space position on the classification is determined in an off-line training procedure using the speakers in a training database, complex systematic speaker variation can be modeled. Speaker adaptation is achieved only by the constraint that the position in the speaker space is constant over each utterance. Therefore, no separate adaptation session is needed and the adaptation is present from the first utterance. Consequently, for a user there is no noticeable difference between this system and a speaker-independent system. The speaker model and the phonetic classification are implemented in the ANN part of a hybrid ANN/HMM system. In experiments with a pilot system, word accuracy is improved for utterances longer than three words and utterance level results are improved for utterances of all lengths.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-249"
  },
  "ariki96_icslp": {
   "authors": [
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Shigeaki",
     "Tagashira"
    ]
   ],
   "title": "An enquiring system of unknown words in TV news by spontaneous repetition (application of speaker normalization by speaker subspace projection)",
   "original": "i96_0993",
   "page_count": 4,
   "order": 252,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "We tried to construct a system in which we can enquire unknown words, included in TV news speech by repeating them spontaneously. For example, we hear \"Japan would join PKO.\" from TV news and if \"PKO\" is an unknown word, then we can enquire it by saying \"What's the PKO?\" The system recognizes the word \"PKO\" and explains its meaning. In this system, it estimates a common section between announcer's speech and user speech, and recognizes the word corresponding to the common section. We solved a problem of speaker difference in extracting common sections by speaker subspace projection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-250"
  },
  "zhang96_icslp": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Beiqian",
     "Dai"
    ],
    [
     "Changfu",
     "Wang"
    ],
    [
     "Hingkeung",
     "Kwan"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Adaptive recognition method based on posterior use of distribution pattern of output probabilities",
   "original": "i96_1129",
   "page_count": 4,
   "order": 253,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "We propose a new adaptation scheme for speaker independent recognition. The basic idea lies in the change of the likelihood from the ordinary HMM scores to the combined observational scores. The new likelihood is computed based on a combination of HMM scores which we called a Distribution Pattern of Output Probabilities (POPD). The system needs to calculate only the POPD for each new speaker. Re-estimation of acoustic model parameters is unnecessary. Preliminary experiments on Chinese icolated-syllable recognition indicate the methods effectiveness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-251"
  },
  "woodland96_icslp": {
   "authors": [
    [
     "P. C.",
     "Woodland"
    ],
    [
     "D.",
     "Pye"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Iterative unsupervised adaptation using maximum likelihood linear regression",
   "original": "i96_1133",
   "page_count": 4,
   "order": 254,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "Maximum likelihood linear regression (MLLR) is a parameter transformation technique for both speaker and environment adaptation. In this paper the iterative use ofMLLR is investigated in the context of large vocabulary speaker independent transcription of both noise free and noisy data. It is shown that iterative application of MLLR can be beneficial especially in situations of severe mismatch. When word lattices are used it is important that the lattices contain the correct transcription and it is shown that global MLLR based on rough initial transcriptions of the data can be very useful in generating high quality lattices. MLLR can also be used in an iterative fashion to re- fine the transcriptions of the test data and adapt models based on the current transcriptions. These techniques were used by the HTKlarge vocabulary system for the November 1995 ARPA H3 evaluation. It is shown that iterative application MLLR prior to lattice generation and for iterative refinement proved to be very effective.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-252"
  },
  "anastasakos96_icslp": {
   "authors": [
    [
     "Tasos",
     "Anastasakos"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "A compact model for speaker-adaptive training",
   "original": "i96_1137",
   "page_count": 4,
   "order": 255,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "In this work we formulate a novel approach to estimating the parameters of continuous density HMMs for speaker-independent (SI) continuous speech recognition. It is motivated by the fact that variability in SI acoustic models is attributed to both phonetic variation and variation among the speakers of the training population, that is independent of the information content of the speech signal. These two variation sources are decoupled and the proposed method jointly annihilates the inter-speaker variation and estimates the HMM parameters of the SI acoustic models. We compare the proposed training algorithm to the common SI training paradigm within the context of supervised adaptation. We show that the proposed acoustic models are more efficiently adapted to the test speakers, thus achieving significant overall word error rate reductions of 19% and 25% for 20K and 05K vocabulary tasks respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-253"
  },
  "homma96_icslp": {
   "authors": [
    [
     "Shigeru",
     "Homma"
    ],
    [
     "Jun-ichi",
     "Takahashi"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Iterative unsupervised speaker adaptation for batch dictation",
   "original": "i96_1141",
   "page_count": 4,
   "order": 256,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "This paper describes an automatic batch-style dictation paradigm in which the entire dictated speech is fully utilized for speaker adaptation and is recognized using the speaker adaptation results. The key point is that the same speech data is used both for recognition as the target and for speaker adaptation. Two steps, speech recognition and speaker adaptation which uses recognition results as means of supervision, are iterated to maximize the advantage of closed-data speaker adaptation. Recognition errors are reduced by 37% in a practical application of batch-style speech-to-text conversion of recorded dictation of Japanese medical diagnoses compared to speaker-independent recognition. To select only reliable recognition results, a supervision improvement procedure is used by which erroneous recognition results can be eliminated from the supervision. In this procedure, 59-74% of the data are extracted from the tentative recognition results and their reliability is 89-93%. This procedure also reduces recognition errors by 45%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-254"
  },
  "burnett96b_icslp": {
   "authors": [
    [
     "Daniel C.",
     "Burnett"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "Rapid unsupervised adaptation to children's speech on a connected-digit task",
   "original": "i96_1145",
   "page_count": 4,
   "order": 257,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "We are exploring ways in which to rapidly adapt our neural network classifiers to new speakers and conditions using very small amounts of speech, say, one or a few words. Our approach is to perform a speaker-dependent warping of the frequency scale by selecting a Bark offset for each speaker. We choose the offset for a speaker to be the one that maximizes our recognizer output score on the adaptation utterance. We then use the speaker's offset during evaluation of all other utterances by the speaker. To test our approach, we evaluate an adult-speech trained recognizer on children's speech from the same task both before and after adaptation to each child's voice. Using only a single digit for adaptation, we have reduced the word error rate for children's speech from 9.6% to 4.2%. Using a seven-digit utterance further reduced the error rate to 3.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-255"
  },
  "ishii96_icslp": {
   "authors": [
    [
     "Jun",
     "Ishii"
    ],
    [
     "Masahiro",
     "Tonomura"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Speaker adaptation using tree structured shared-state HMMs",
   "original": "i96_1149",
   "page_count": 4,
   "order": 258,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper proposes a novel speaker adaptation method that flexibly controls state-sharing of HMMs according to the amount of adaptation data. In our scheme, acoustic modeling is combined with adaptation to efficiently utilize the acoustic models sharing characteristics for adaptation. The shared-state set of HMMs is determined by using tree-structured shared-state HMMs created from the history recorded for acoustic model generation. The proposed method is applied to the parameter-tying and parameter-smoothing techniques. Experiments have been performed on a Japanese phoneme recognition test using continuous density mixture Gaussian HMMs. Using 50 adaptation phrases, a 42% reduction in the phoneme recognition error rate from the speaker-independent model was achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-256"
  },
  "schwartz96_icslp": {
   "authors": [
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Scott",
     "Miller"
    ],
    [
     "David",
     "Stallard"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Language understanding using hidden understanding models",
   "original": "i96_0997",
   "page_count": 4,
   "order": 259,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "We describe the first sentence understanding system that is completely based on learned methods both for understanding individual sentences, and determinig their meaning in the context of preceding sentences. We describe the models used for each of three stages in the understanding: semantic parsing, semantic classification, and discourse modeling. When we ran this system on the last test (December, 1994) of the ARPA Air Travel Information System (ATIS) task, we achieved 14.5% error rate. The error rate for those sentences that are context-independent (class A) was 9.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-257"
  },
  "gorin96_icslp": {
   "authors": [
    [
     "Allen L.",
     "Gorin"
    ]
   ],
   "title": "Processing of semantic information in fluently spoken language",
   "original": "i96_1001",
   "page_count": 4,
   "order": 260,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "We are interested in constructing machines which learn to understand and act upon fluently spoken input. For any particular task, certain linguistic events are critical to recognize correctly, others not so. This notion can be quantified via salience, which measures the information content of an event for a task. In previous papers, salient words have been exploited to learn the mapping from spoken input to machine action for several tasks. In this work, a new algorithm is presented which automatically acquires salient grammar fragments for a task, exploiting both linguistic and extra-linguistic information in the inference process. Experimental results will be reported for a database of fluently spoken customer requests to operators, responding to the open-ended prompt of Hello, this is AT&T. How may I help you?\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-258"
  },
  "stolcke96_icslp": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Automatic linguistic segmentation of conversational speech",
   "original": "i96_1005",
   "page_count": 4,
   "order": 261,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into linguistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of several word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-259"
  },
  "boros96_icslp": {
   "authors": [
    [
     "M.",
     "Boros"
    ],
    [
     "W.",
     "Eckert"
    ],
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Günther",
     "Görz"
    ],
    [
     "G.",
     "Hanrieder"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Towards understanding spontaneous speech: word accuracy vs. concept accuracy",
   "original": "i96_1009",
   "page_count": 4,
   "order": 262,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "In this paper we describe an approach to automatic evaluation of both the speech recognition and understanding capabilities of a spoken dialogue system for train time table information. We use word accuracy for recognition and concept accuracy for understanding performance judgement. Both measures are calculated by comparing these modules' output with a correct reference answer. We report evaluation results for a spontaneous speech corpus with about 10000 utterances. We observed a nearly linear relationship between word accuracy and concept accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-260"
  },
  "minker96_icslp": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ],
    [
     "S. K.",
     "Bennacef"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "A stochastic case frame approach for natural language understanding",
   "original": "i96_1013",
   "page_count": 4,
   "order": 263,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "A stochastically based approach for the semantic analysis component of a natural spoken language system for the ATIS task has been developed. The semantic analyzer of the spoken language system already in use at LIMSI makes use of a rule-based case grammar. In this work, the system of rules for the semantic analysis is replaced with a relatively simple, first order Hidden Markov Model. The performance of the two approaches can be compared because they use identical semantic representations despite their rather different methods for meaning extraction. We use an evaluation methodology that assesses performance at different semantic levels, including the database response comparison used in the ARPA ATIS paradigm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-261"
  },
  "seide96_icslp": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Bernhard",
     "Rüber"
    ],
    [
     "Andreas",
     "Kellner"
    ]
   ],
   "title": "Improving speech understanding by incorporating database constraints and dialogue history",
   "original": "i96_1017",
   "page_count": 4,
   "order": 264,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "In the course of a (man-machine) dialogue, the system's belief concerning the user's intention is continuously being built up. Moreover, restricting the discourse to a narrow application domain further constrains the variety of possible user reactions. In this paper, we will show how these knowledge sources may be utilized in a stochastic framework to improve speech understanding. On field-test data collected with our automatic exchange board prototype PADIS1, a relative reduction of attribute errors by 27% has been obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-262"
  },
  "buo96_icslp": {
   "authors": [
    [
     "Finn Dag",
     "Buo"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Learning to parse spontaneous speech",
   "original": "i96_1153",
   "page_count": 4,
   "order": 265,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "We describe and experimentally evaluate a system, FeasPar, that learns parsing spontaneous speech. To train and run FeasPar (Feature Structure Parser), only limited handmodeled knowledge is required. The FeasPar architecture consists of neural networks and a search. The networks spilt the incoming sentence into chunks, which are labeled with feature values and chunk relations. Then, the search finds the most probable and consistent feature structure. FeasPar is trained, tested and evaluated with the Spontaneous Scheduling Task, and compared with two samples of a handmodeled GLR* parser, developed for 4 months and 2 years, respectively. The handmodeling effort for FeasPar is 2 weeks. FeasPar performes better than the GLR* parser developed 4 months in all six comparisons that are made and has a similar performance as the GLR* parser developed for 2 years.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-263"
  },
  "antoine96_icslp": {
   "authors": [
    [
     "Jean-Yves",
     "Antoine"
    ]
   ],
   "title": "Spontaneous speech and natural language processing ALPES: a robust semantic-led parser",
   "original": "i96_1157",
   "page_count": 4,
   "order": 266,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "The need of robust parsers is more and more essential as spoken human-machine communication meets an impressive development. Because of its uncontrolled nature, spontaneous speech presents indeed a high rate of extragrammatical constructions (hesitations, repetitions, self-corrections, etc.). As a result, spontaneous speech rapidly catches out most syntactic parsers, in spite of the frequent addition of some corrective methods [7]. Therefore, most dialog systems restrict the linguistic analysis of the spoken utterances to a simple extraction of keywords [2]. This selective approach led to significant results in some restricted applications (ATIS), but it does not seem appropriate for higher level tasks, for which the utterances cannot be reduced to a simple set of keywords. As a result, neither the syntactic methods nor the selective approaches can fully satisfy the constraints of robustness and exhaustivity required by the human-machine communication. This paper precisely presents a detailed semantic parser (ALPES) which masters most spoken utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-264"
  },
  "alvarezcercadillo96_icslp": {
   "authors": [
    [
     "J.",
     "Alvarez-Cercadillo"
    ],
    [
     "J.",
     "Caminero-Gil"
    ],
    [
     "C.",
     "Crespo-Casas"
    ],
    [
     "D.",
     "Tapias-Merino"
    ]
   ],
   "title": "The natural language processing module for a voice assisted operator at telefónica i+D",
   "original": "i96_1161",
   "page_count": 4,
   "order": 267,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "A Natural Language Processor module has been implemented at Telefónica I+D. This module is composed by a Semantic Parser, a Message Generator, and a Dialog Controller which coordinates all the process. The Dialog Controller is an inference engine that works through a semantic tree. This tree hierarchically organizes the information going from general concepts, towards more specific ones. The dialog strategy at each tree node can be chosen by selecting the node type from among five pre-defined node types. Output Messages are selected by the Message Generator. There are six different types of messages and two recovery strategies that the Message Generator can follow. Several output messages for each node in the tree can be defined in order to provide variability. Based on this scheme, a conversational system call ATOS (Automatic Telephone Operator Service) has been designed at Telefónica I+D. In this document we explain the main characteristics of the system an how every module works.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-265"
  },
  "berton96_icslp": {
   "authors": [
    [
     "André",
     "Berton"
    ],
    [
     "Pablo",
     "Fetter"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Compound words in large-vocabulary German speech recognition systems",
   "original": "i96_1165",
   "page_count": 4,
   "order": 268,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "This paper analyzes the impact of German compound words on speech recognition. It is well known that, due to an idiosyncrasy of German orthography, compound words make up a major fraction of German vocabulary. And most Out-Of-Vocabulary (OOV) compounds are composed of frequent words already in the lexicon. This paper introduces a new method for handling the components of compounds rather than the compounds themselves. This not only reduces the vocabulary, and therefore the perplexity, but also improves word accuracy. And reduced perplexity means a more robust language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-266"
  },
  "batliner96_icslp": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "A.",
     "Feldhaus"
    ],
    [
     "S.",
     "Geissler"
    ],
    [
     "T.",
     "Kiss"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Prosody, empty categories and parsing - a success story",
   "original": "i96_1169",
   "page_count": 4,
   "order": 269,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "We describe a number of experiments that demonstrate the usefulness of prosodic information for a processing module which parses spoken utterances with a feature-based grammar employing empty categories. We show that by requiring certain prosodic properties from those positions in the input, where the presence of an empty category has to be hypothesized, a derivation can be accomplished more efficiently. The approach has been implemented in the machine translation project Verbmobil and results in a significant reduction of the work-load for the parser.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-267"
  },
  "srinivas96_icslp": {
   "authors": [
    [
     "B.",
     "Srinivas"
    ]
   ],
   "title": "almost parsing technique for language modeling",
   "original": "i96_1173",
   "page_count": 4,
   "order": 270,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "In this paper we present an approach that incorporates structural information into language models without really parsing the utterance. This approach brings together the advantages of a n-gram language model { speed, robustness and the ability to integrate with the speech recognizer with the need to model syntactic constraints, under a uniform representation. We also show that our approach produces better language models than language models based on part-of-speech tags.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-268"
  },
  "chino96_icslp": {
   "authors": [
    [
     "Tetsuro",
     "Chino"
    ],
    [
     "Hiroyuki",
     "Tsuboi"
    ]
   ],
   "title": "A new discourse structure model for spontaneous spoken dialogue",
   "original": "i96_1021",
   "page_count": 4,
   "order": 271,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "In this paper, a new discourse structure model is proposed, and based on the model, we report the results of an analysis on Japanese-language dialogues over the telephone. As a result, a method for describing and analyzing the structure of spontaneous spoken dialogue is provided, and some characteristics of spontaneous spoken dialogue over the telephone were clarified.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-269"
  },
  "duff96_icslp": {
   "authors": [
    [
     "David",
     "Duff"
    ],
    [
     "Barbara",
     "Gates"
    ],
    [
     "Susann",
     "LuperFoy"
    ]
   ],
   "title": "An architecture for spoken dialogue management",
   "original": "i96_1025",
   "page_count": 4,
   "order": 272,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "We propose an architecture for integrating discourse processing and speech recognition (SR) in spoken dialogue systems. It was first developed for computer-mediated bilingual dialogue in voice-to-voice machine translation applications and we apply it here to a distributed battlefield simulation system used for military training. According to this architecture discourse functions previously distributed through the interface code are collected into a centralized discourse capability. The Dialogue Manager (DM) acts as a third-party mediator overseeing the translation of input and output utterances between English and the command language of the backend system. The DM calls the Discourse Processor (DP) to update the context representation each time an utterance is issued or when a salient non-linguistic event occurs in the simulation. The DM is responsible for managing the interaction among components of the interface system and the user. For task-based human-computer dialogue systems it consults three sources of non-linguistic context constraint in addition to the linguistic Discourse State: (1) a User Model, (2) a static Domain Model containing rules for engaging the backend system, with a grammar for the language of well-formed, executable commands, and (3) a dynamic Backend Model (BEM) that maintains updated status for salient aspects of the non-linguistic context. In this paper we describe its four-step recovery algorithm invoked by DM whenever an item is unclear in the current context, or when an interpretation error is, and show how parameter settings on the algorithm can modify the overall behavior of the system from Tutor to Trainer. This is offered to illustrate how limited (inexpensive) dialogue processing functionality, judiciously selected, and designed in conjunction with expectations for human dialogue behavior can compensate for inevitable limitations in SR, NL processor, the backend software application, or even in the users understanding of the task or the software system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-270"
  },
  "donzel96_icslp": {
   "authors": [
    [
     "Monique E. van",
     "Donzel"
    ],
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "Pausing strategies in discourse in dutch",
   "original": "i96_1029",
   "page_count": 4,
   "order": 273,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "This paper describes an experiment in which the different pausing strategies in discourse in Dutch were investigated. Spontaneous discourses were recorded from four male and four female native Dutch speakers. Silent and filled pauses were located in the speech signal, as well as lengthened words. These were subsequently related to different discourse structures, obtained independently from prosodic features. Results show that there are basically three different types of pausing: silent pauses, filled pauses, and lengthening of words. Speakers apply these means in different ways to achieve pausing, by using one specific pause type or a combination of more than one. The way of applying pausing is rather uniform within one speaker, whereas the choice of a particular strategy is largely speaker dependent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-271"
  },
  "swerts96_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Anne",
     "Wichmann"
    ],
    [
     "Robbert-Jan",
     "Beun"
    ]
   ],
   "title": "Filled pauses as markers of discourse structure",
   "original": "i96_1033",
   "page_count": 4,
   "order": 274,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "This study aims to test quantitatively whether filled pauses (FPs) may highlight discourse structure. More specifically, it is first investigated whether FPs are more typical in the vicinity of major discourse boundaries. Secondly, the FPs are analyzed acoustically, to check whether those occurring at major discourse boundaries are segmentally and prosodically different from those at shallower breaks. Analyses of twelve spontaneous monologues (Dutch) show that phrases following major discourse boundaries more often contain FPs. Additionally, FPs after stronger breaks tend to occur phrase-initially, whereas the majority of the FPs after weak boundaries are in phrase-internal position. Also, acoustic observations reveal that FPs at major discourse boundaries are both segmentally and prosodically distinct. They also differ with respect to the distribution of neighbouring silent pauses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-272"
  },
  "seong96_icslp": {
   "authors": [
    [
     "Cheol-jae",
     "Seong"
    ],
    [
     "Minsoo",
     "Hahn"
    ]
   ],
   "title": "The prosodic analysis of Korean dialogue speech - through a comparative study with read speech",
   "original": "i96_1037",
   "page_count": 4,
   "order": 275,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "This paper describes the prosodic features of Korean dialogue speech. With 25 sentences for scheduling, one speaker uttered in two manners, viz. 'read' and 'dialogue'. The main discriminating features would be some aspects in speech rate and boundary signal. We discriminated each prosodic phrase in a sentence to investigate pre-boundary, boundary, and post-boundary features. The durational aspect in dialogue speech shows much more drastic characteristics than that in read. We can see that the boundary syllables of dialogue seem to be 2.3 times longer than that in preboundary syllable. The final syllables are about 1.7 times longer than prefinal syllables. Pitch analysis shows that dialogues are pronounced 14.3 % higher than read. Emotional factor of dialogue seems to raise the average pitch. It was interesting that the minimum pitch values are about 72 % of sentential mean for both similarly. In dialogue, there was great difference between the pitch of prefinal and that of final syllable, i.e., the final syllables are almost 15 % higher. The results confirms our general ideas that 1) the duration is more dynamic in dialogue than in read speech, 2) pitch contour fluctuation is larger in dialogue than in read speech, 3) dialogue is usually uttered in higher tone, 4) and sentential final part may play an decisive role in speech style determination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-273"
  },
  "okane96_icslp": {
   "authors": [
    [
     "Mary",
     "O'Kane"
    ],
    [
     "P. E.",
     "Kenne"
    ]
   ],
   "title": "Changing the topic: how long does it take?",
   "original": "i96_1041",
   "page_count": 4,
   "order": 276,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "We examine the frequency of topic change in Australian court dialogue with a view to automatically changing language models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-274"
  },
  "westendorf96_icslp": {
   "authors": [
    [
     "Christian-Michael",
     "Westendorf"
    ],
    [
     "Jens",
     "Jelitto"
    ]
   ],
   "title": "Learning pronunciation dictionary from speech data",
   "original": "i96_1045",
   "page_count": 4,
   "order": 277,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "In this paper an algorithm and first results from our investigations in automatically learning pronunciation variations from speech data are presented. Pronunciation dictionaries establish an important feature in state-of-the-art speech recognition systems. In most systems only simple dictionaries containing the canonical pronunciation forms are implemented. However, for a good recognition performance more sophisticated dictionaries including pronunciation variations are essential. The generation of such dictionaries by hand is an extremely time consuming task, and the introduction of errors and inconsistencies is probable. We show an approach for automatically generating suitable pronunciation dictionaries from the speech data base itself, as they are desirable not only for speech recognition tasks but also for speech technology and phonologic research in general. The only knowledge sources besides the data base are the (unlabeled) signals and their transliterations on word level. First experiments yielding promising results have been performed with the software system DataLab [6], which integrates the recognition system of the TU Dresden.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-275"
  },
  "rathinavelu96_icslp": {
   "authors": [
    [
     "C.",
     "Rathinavelu"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "The trended HMM with discriminative training for phonetic classification",
   "original": "i96_1049",
   "page_count": 4,
   "order": 278,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "In this paper, we extend the Maximum Likelihood (ML) training algorithm to the Minimum Classification Error (MCE) training algorithm for optimal estimation of the state-dependent polynomial coefficients in the trended HMM [2]. The problem of automatic speech recognition is viewed as a discriminative dynamic data-fitting problem, where relative (not absolute) closeness in fitting an array of dynamic speech models to the unknown speech data sequence provides the recognition decision. In this view, the properties of the MCE formulation for training the trended HMM are analyzed by fitting raw speech data using MCE-trained trended HMMs, contrasting the poor discriminative fitting using the ML-trained models. Comparisons between the phonetic classification as well as data-fitting results obtained with ML and with MCE training algorithms demonstrate the effectiveness of the discriminatively trained trended HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-276"
  },
  "lazarides96_icslp": {
   "authors": [
    [
     "Ariane",
     "Lazaridès"
    ],
    [
     "Yves",
     "Normandin"
    ],
    [
     "Roland",
     "Kuhn"
    ]
   ],
   "title": "Improving decision trees for acoustic modeling",
   "original": "i96_1053",
   "page_count": 4,
   "order": 279,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "In the last few years, the power and simplicity of classification trees as acoustic modeling tools have gained them much popularity. In [1], we studied \"tree units\", which cluster parameters at the HMM level. Building on this earlier work, we examine some new variants of Young et als \"tree states\", which cluster parameters at the state level [2]. We have experimented with: 1. Making unitary models (which contain additional information about the context) 2. Pruning trees with various severity levels (idea introduced in [1]) 3. Pooling some leaves (idea adapted from [2]) 4. Refining the questions 5. Questions about the position of the phone within the word 6. Lookahead search 7. Making a single tree for each phone\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-277"
  },
  "li96b_icslp": {
   "authors": [
    [
     "Gongjun",
     "Li"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "An improved training algorithm in HMM-based speech recognition",
   "original": "i96_1057",
   "page_count": 4,
   "order": 280,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "In HMM-based speech recognition, estimation of parameters of HMMs is viewed as counterpart of training or learning in traditional sequential pattern recognition since speech signal can be represented by a sequence of n-dimension vectors after features are extracted from the speech signal. However, due to variation of duration of the phone with speakers and context and its randomness, speech samples contribute differently to estimation of parameters of HMMs. While only smaller training set is accessible, for instance, in the case of speaker adaptation, the problem becomes very serious. In this paper, we analyze the impact of different duration of the phone on the output probability likelihood. To combat the above problem, two approaches are proposed to make proportionate the contribution of speech samples to estimation of parameters of HMM: geometrically averaged probability likelihood method and centralized parametric space method. Several experiments are conducted to verify the advantage of the above approaches in HMM-based speech recognition. The results show that the recognition rate can be improved to a certain degree when any one of the above approaches is employed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-278"
  },
  "ming96_icslp": {
   "authors": [
    [
     "J.",
     "Ming"
    ],
    [
     "P.",
     "O'Boyle"
    ],
    [
     "J.",
     "McMahon"
    ],
    [
     "F. J.",
     "Smith"
    ]
   ],
   "title": "Speech recognition using a strong correlation assumption for the instantaneous spectra",
   "original": "i96_1061",
   "page_count": 4,
   "order": 281,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "The conventional independence assumption made for the evolving speech spectra is replaced by a strong correlation assumption, which then leads to a new stochastic model. This model implements a nonlinear interpolation between the lower and upper bounds of the joint probability distributions. The advantage of the new model over other correlation-based modelling approaches is that it has a low parameter complexity, the same as that in models based on the independence assumption. Experiments on a speaker-independent E-set database show the effectiveness of this new modelling approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-279"
  },
  "pachesleal96_icslp": {
   "authors": [
    [
     "Pau",
     "Pachès-Leal"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "On parameter filtering in continuous subword-unit-based speech recognition",
   "original": "i96_1065",
   "page_count": 4,
   "order": 282,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "Simple IIR or FIR fillers have been widely used in isolated or connected word recognition tasks to filter the time sequence of speech spectral parameters, since, despite their simplicity, they significantly improve recognition performance. Those filters, when applied to continuous speech recognition, where phoneme-sized modelling units are used, induce spectral transition spreading and a cross-boundary effect. In this work, we show how the use of context-dependent units reduces the side effects of the filters and may result in improved recognition performance. When dynamic parameters are not used, filtering seems to be especially useful, even for clean speech, and when they are, filters do well under unmatched training and testing conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-280"
  },
  "okawa96_icslp": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Estimation of statistical phoneme center considering phonemic environments",
   "original": "i96_1069",
   "page_count": 5,
   "order": 283,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "This paper presents a new scheme of acoustic modeling for speech recognition based on an idea of Statistical Phoneme Center. The Statistical Phoneme Center has several properties that are feasible to realize a higher-reliable phoneme extraction. First, we assume that there is a fictitious center point in every phoneme. The center is determined statistically by an iterative procedure to maximize the local likelihood using a large amount of speech data. Next, in order to evaluate the performance of phoneme extraction, phoneme recognition is realized by optimizing the likelihood based on Dynamic Time Warping technique. As the experimental result, 71.6% recognition accuracy is obtained for speaker independent phoneme recognition. This result demonstrate that the proposed SPC is a new effective concept to obtain more stabilized acoustic model for speaker independent speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-281"
  },
  "wang96_icslp": {
   "authors": [
    [
     "Xue",
     "Wang"
    ],
    [
     "Louis F. M. ten",
     "Bosch"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Integration of context-dependent durational knowledge into HMM-based speech recognition",
   "original": "i96_1073",
   "page_count": 4,
   "order": 284,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "This paper presents research on integrating context-dependent durational knowledge into HMM-based speech recognition. The first part of the paper presents work on obtaining relations between the parameters of the context-free HMMs and their durational behaviour, in preparation for the context-dependent durational modelling presented in the second part. Duration integration is realised via rescoring in the post-processing step of our N-best monophone recogniser. We use the multi-speaker TIMIT database for our analyses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-282"
  },
  "fukada96_icslp": {
   "authors": [
    [
     "T.",
     "Fukada"
    ],
    [
     "M.",
     "Bacchiani"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Speech recognition based on acoustically derived segment units",
   "original": "i96_1077",
   "page_count": 4,
   "order": 285,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "This paper describes a new method of word model generation based on acoustically derived segment units (henceforth ASUs). An ASU-based approach has the advantages of growing out of human pre-determined phonemes and of consistently generating acoustic units by using the maximum likelihood (ML) criterion. The former advantage is effective when it is difficult to map acoustics to a phone such as with highly co-articulated spontaneous speech. In order to implement an ASU-based modeling approach in a speech recognition system, we must first solve two points: (1) How do we design an inventory of acoustically-derived segmental units and (2) How do we model the pronunciations of lexical entries in terms of the ASUs. As for the second question, we propose an ASU-based word model generation method by composing the ASU statistics, that is, their means, variances and durations. The effectiveness of the proposed method is shown through spontaneous word recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-283"
  },
  "vergin96_icslp": {
   "authors": [
    [
     "Rivarol",
     "Vergin"
    ],
    [
     "Azarshid",
     "Farhat"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust gender-dependent acoustic-phonetic modelling in continuous speech recognition based on a new automatic male/female classification",
   "original": "i96_1081",
   "page_count": 4,
   "order": 286,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "In this paper we present a new automatic male/female classification method based on the location in the frequency domain of the first 2 formants. This classification is based on a new automatic formant extraction which is faster than a peak picking technique. Gender-dependent acoustic-phonetic models stemming from this classification are used in the INRS Continuous speech recognition system with ATIS corpora. An improvement of 14% is obtained with these models in comparison to the baseline speaker-independent system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-284"
  },
  "yang96e_icslp": {
   "authors": [
    [
     "Tae Young",
     "Yang"
    ],
    [
     "Won Ho",
     "Shin"
    ],
    [
     "Weon Goo",
     "Kim"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "A codebook adaptation algorithm for SCHMM using formant distribution",
   "original": "i96_1085",
   "page_count": 4,
   "order": 287,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "This paper describes a codebook adaptation process improving the performance of speaker adaptation. The proposed method is performed prior to Bayesian speaker adaptation method using the formant distribution of adaptation data. The reference codebook is adapted to represent the formant distribution of a new speaker. The average recognition rate of Bayesian adaptation is improved from 91.4% to 95.1% using the proposed method. The proposed method is effective particularly when there exists a large mismatch between the reference codebook and a target speaker in feature space. In this cases the average recognition rate is 95.0% while 89.9% is obtained when only Bayesian adaptation is performed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-285"
  },
  "simonin96_icslp": {
   "authors": [
    [
     "J.",
     "Simonin"
    ],
    [
     "S.",
     "Bodin"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "K.",
     "Bartkova"
    ]
   ],
   "title": "Parameter tying for flexible speech recognition",
   "original": "i96_1089",
   "page_count": 4,
   "order": 288,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "This paper presents two parameter tying techniques which enable a trade-off between computational cost and recognition performances of a speaker independent flexible speech recognition system working over the telephone network. Parameter tying is conducted at phonetic and acoustic levels. At the phonetic level, allophone and triphone based phonetic modeling are used simultaneously to achieve the best trade-off between computational cost and recognition performances. This decreases error rate with a controlled computational cost as compared to an allophone modeling. At the acoustic level, the tying is performed by clustering the Gaussian densities of mixture distributions. After clustering, a particular density may be use by several distribution. This allows the total number of Gaussian densities to be divided by two while improving the recognition performances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-286"
  },
  "nitta96_icslp": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Shin'ichi",
     "Tanaka"
    ],
    [
     "Yasuyuki",
     "Masai"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ]
   ],
   "title": "Word-spotting based on inter-word and intra-word diphone models",
   "original": "i96_1093",
   "page_count": 4,
   "order": 289,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "In this paper, we propose a precise but simple inter-word diphone model (IDM) for ward-spotting based on SMQ/HMM. We have applied ordinary diphone models to a speaker-independent, large-vocabulary word recognition unit. However, because users are apt to add words and/or extraneous speech, accuracy degrades due to the mismatch of models at word-boundaries. The IDM represents transition from the preceding phonemes to a word or from a word to the succeeding phonemes. An experiment showed that the IDMa reduce error rates by about 5% for speech containing unknown words and extraneous speech. The experiment also showed that the proposed method ensured performance good enough for the practical use of a large-vocabulary, isolated-word recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-287"
  },
  "bonafonte96b_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Josep",
     "Vidal"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "Duration modeling with expanded HMM applied to speech recognition",
   "original": "i96_1097",
   "page_count": 4,
   "order": 290,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "In this paper, the occupancy of the HMM states is modeled by means of a Markov chain. A linear estimator is introduced to compute the probabilities of the Markov chain. The distribution functions (DF) represents accurately the observed data. Representing the DF as a Markov chain allows the use of standard HMM recognizers. The increase of complexity is negligible in training and strongly limited during recognition. Experiments performed on acoustic-phonetic decoding shows how the phone recognition rate increases from 60.6 to 61.1. Furthermore, on a task of database inquires, where phones are used as subword units, the correct word rate increases from 88.2 to 88.4.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-288"
  },
  "cordoba96_icslp": {
   "authors": [
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Different strategies for distribution clustering using discrete, semicontinuous and continuous HMMs in CSR",
   "original": "i96_1101",
   "page_count": 4,
   "order": 291,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "We present an overview of different strategies and refinements to share parameters in HMM models at distribution (state) level for continuous speech recognition, showing the advantages and drawbacks of the different kinds of modeling [6]. We compare them with sharing at model level [5], achieving an error reduction close to 20% [4]. Discrete, semi continuous and continuous HMM models are also compared using these approaches. We consider two ways to smooth discrete distributions (interpolate detailed context dependent with robust context independent) derived from deleted interpolation [1] and coocurrence smoothing [10].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-289"
  },
  "zeljkovic96_icslp": {
   "authors": [
    [
     "Ilija",
     "Zeljkovic"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Improved HMM phone and triphone models for realtime ASR telephony applications",
   "original": "i96_1105",
   "page_count": 4,
   "order": 292,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "Development of human-machine dialog applications for messaging and information retrieval over the telephone pose stringent requirements on accuracy and speed of the automatic speech recognition (ASR) system. In this paper, we describe strategies for improved acoustic-phone modeling directed toward increasing recognition accuracy while maintaining the number of phone units low. Specifically, this paper considers: (1) The development of an improved set of head-tail context-dependent (CD) triphones. (2) A novel criterion for better selection of the number of states assigned to each phone unit based on the coefficient of variation measure of feature components in HMM-Gaussians. Performance of the models is evaluated using data that represent real telephony applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-290"
  },
  "minami96_icslp": {
   "authors": [
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Improved extended HMM composition by incorporating power variance",
   "original": "i96_1109",
   "page_count": 4,
   "order": 293,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "This paper describes a way of improving extended HMM composition that can precisely adapt HMMs to both noisy and distorted speech. To do this, we incorporate the variance of power into extended HMM composition using quantization to approximate the Gaussian distribution of the 0th order cepstrum. Consequently, a distribution of noisy speech is approximated in the linear spectral domain as a mixture of log normal distributions. This method is evaluated by a four-digit recognition experiment when the number of digits is known. Two types of noise, computer room noise and car noise, are used and noisy and distorted speech data is made by adding these types of noise to speech data recorded using a boundary microphone. Results show that the proposed method improves recognition rates for noisy and distorted speech compared with our previous method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-291"
  },
  "ramsay96b_icslp": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Optimal filtering and smoothing for speech recognition using a stochastic target model",
   "original": "i96_1113",
   "page_count": 4,
   "order": 294,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "This paper presents a stochastic target model of speech production, where articulator motion in the vocal tract is represented by the state of a Markov-modulated linear dynamical system, driven by a piecewise-deterministic control trajectory, and observed through a non-linear function representing the articulatory-acoustic mapping. Optimal filtering and smoothing algorithms for estimating the hidden states of the model from acoustic measurements are derived using a measure-change technique, and require solution of recursive integral equations. A sub-optimal approximation is developed, and illustrated using examples taken from real speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-292"
  },
  "hu96b_icslp": {
   "authors": [
    [
     "Zhihong",
     "Hu"
    ],
    [
     "Johan",
     "Schalkwyk"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Speech recognition using syllable-like units",
   "original": "i96_1117",
   "page_count": 4,
   "order": 295,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "It is well known that speech is dynamic and that frame-based systems lack the ability to realistically model the dynamics of speech. Segment-based systems offer the potential to integrate the dynamics of speech, at least within the phoneme boundaries, although it is difficult to obtain accurate phonemic segmentation in fluent speech. In this paper we propose a new approach which uses syllable-like units in recognition. In the proposed approach, syllable-like units are defined by rules and used as the basic units of recognition. The motivation for using syllable-like units is (1) by modeling perceptually more meaningful units, better modeling of speech can be achieved; and (2) this method provides a better framework for incorporating dynamic modeling techniques into the recognition system. The proposed approach has achieved the same recognition performance on the task of recognizing months of the year as compared to the best frame-based recognizer available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-293"
  },
  "junqua96_icslp": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Lorenzo",
     "Vassallo"
    ]
   ],
   "title": "Context modeling and clustering in continuous speech recognition",
   "original": "i96_2262",
   "page_count": 4,
   "order": 296,
   "p1": "2262",
   "pn": "2265",
   "abstract": [
    "In this paper, we report on the performance of two variants of wellknown statistical-based clustering techniques and present an evaluation on the TIMIT and TI-Digit databases. A clustering approach which 1) is based on a divergence criterion, 2) separates \"good\" and \"bad\" models using a class-dependent adjustable threshold on the number of examples per model, and 3) guides the clustering by limiting the number of models per class between two constants Nmin and Nmax, gave the best results. On the TI-Digit database, the combination of triphone modeling and divergence-based clustering yielded greater accuracy than that obtained with word models for a similar system complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-294"
  },
  "deng96b_icslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Jim Jian-Xiong",
     "Wu"
    ]
   ],
   "title": "Hierarchical partition of the articulatory state space for overlapping-feature based speech recognition",
   "original": "i96_2266",
   "page_count": 4,
   "order": 297,
   "p1": "2266",
   "pn": "2269",
   "abstract": [
    "We describe our recent work on improving an overlapping articulatory feature (sub-phonemic) based speech recognizer with robustness to the requirement of training data. A new decision-tree algorithm is developed and applied to the recognizer design which results in hierarchical partitioning of the articulatory state space. The articulatory states associated with common acoustic correlates, a phenomenon caused by the many-to-one articulation-to-acoustics mapping well known in speech production, are automatically clustered by the decision-tree algorithm. This enables effective prediction of the unseen articulatory states in the training, thereby increasing the recognizers robustness. Some preliminary experimental results are provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-295"
  },
  "oppizzi96_icslp": {
   "authors": [
    [
     "Olivier",
     "Oppizzi"
    ],
    [
     "David",
     "Fournier"
    ],
    [
     "Philippe",
     "Gilles"
    ],
    [
     "Henri",
     "Méloni"
    ]
   ],
   "title": "A fuzzy acoustic-phonetic decoder for speech recognition",
   "original": "i96_2270",
   "page_count": 4,
   "order": 298,
   "p1": "2270",
   "pn": "2273",
   "abstract": [
    "In this paper, a general framework of acoustic-phonetic modelling is developed. Context sensitive rules are incorporated into a knowledge-based automatic speech recognition (ASR) system and are assessed with control based on fuzzy decision making. The reliability measure is outlined: a tests collection is run and a confusion matrix is built for each rule. During the recognition procedure the fuzzy set of trained values related to the phonetic unit to be recognized is computed, and its membership function is automatically drawn. Tests were done on an isolated-word speech database of French with 1000 utterances and with 33 rules. The results with a one-speaker low training rate are established via a two-step procedure: a word recognition and a word rejection test bed with five speakers who were never involved during the training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-296"
  },
  "kirchhoff96_icslp": {
   "authors": [
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Syllable-level desynchronisation of phonetic features for speech recognition",
   "original": "i96_2274",
   "page_count": 3,
   "order": 299,
   "p1": "2274",
   "pn": "2276",
   "abstract": [
    "This paper describes a novel approach to speech recognition which is based on phonetic features as basic recognition units and the delayed synchronisation of these features within a higher-level prosodic domain, viz. the syllable. The object of this approach is to avoid a rigid segmentation of the speech signal as it is usually carried out by standard segment-based recognition systems. The architectural setup of the system will be described, as well as evaluation tests carried out on a medium-sized corpus of spontaneous speech (German). Syllable and phoneme recognition results will be given and compared to recognition rates obtained by a standard triphone-based HMMrecogniser trained and tested on the same data set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-297"
  },
  "glass96_icslp": {
   "authors": [
    [
     "James",
     "Glass"
    ],
    [
     "Jane",
     "Chang"
    ],
    [
     "Michael",
     "McCandless"
    ]
   ],
   "title": "A probabilistic framework for feature-based speech recognition",
   "original": "i96_2277",
   "page_count": 4,
   "order": 300,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "Most current speech recognizers use an observation space which is based on a temporal sequence of \"frames\" (e.g., Mel-cepstra). There is another class of recognizer which further processes these frames to produce a segment-based network, and represents each segment by fixed-dimensional \"features.\" In such feature-based recognizers the observation space takes the form of a temporal network of feature vectors, so that a single segmentation of an utterance will use a subset of all possible feature vectors. In this work we examine a maximum a posteriori decoding strategy for feature-based recognizers and develop a normalization criterion useful for a segment-based Viterbi or A* search. We report experimental results for the task of phonetic recognition on the TIMIT corpus where we achieved context-independent and context-dependent (using diphones) results on the core test set of 64.1% and 69.5% respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-298"
  },
  "wu96_icslp": {
   "authors": [
    [
     "Jim Jian-Xiong",
     "Wu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Jacky",
     "Chan"
    ]
   ],
   "title": "Modeling context-dependent phonetic units in a continuous speech recognition system for Mandarin Chinese",
   "original": "i96_2281",
   "page_count": 4,
   "order": 301,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "We study the problem of phonetic modeling for continuous Mandarin speech recognition by providing a systematic performance comparison for systems based on following primitive speech units: syllable, demi-syllable (Initials and Finals), context-independent phones, left-or-right context-dependent phones (diphones), and left-and-right context-dependent phones (triphones). In our speaker-dependent continuous speech recognition experiments, a generalized triphone system has achieved the best performance among all. Our best system contrasts most other Mandarin speech recognition systems which have been based on demi-syllable units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-299"
  },
  "coker96_icslp": {
   "authors": [
    [
     "Cecil H.",
     "Coker"
    ],
    [
     "M. H.",
     "Krane"
    ],
    [
     "B. Y.",
     "Reis"
    ],
    [
     "R. A.",
     "Kubli"
    ]
   ],
   "title": "Search for unexplored effects in speech production",
   "original": "i96_1121",
   "page_count": 4,
   "order": 302,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "Speech coders invariably spend about 1/3 of their bits replicating a number of effects collectively termed \"excitation\". The need for so much bandwidth stems from two causes. The frame rate must be relatively high, because transient changes must be resolved. The data needed for each frame is high because unpredictable broad-band components must be reproduced. Here we discuss three projects to learn more about these elusive aspects of speech. One project models the transient behavior. Two others seek to characterize stochastic processes that accompany periodic vibration in voiced sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-300"
  },
  "badin96_icslp": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Christian",
     "Abry"
    ]
   ],
   "title": "Articulatory synthesis from x-rays and inversion for an adaptive speech robot",
   "original": "i96_1125",
   "page_count": 4,
   "order": 303,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "This paper describes a speech robotic approach to articulatory synthesis. An anthropomorphic speech robot has been built, based on a real reference subjects data. This speech robot, called the Articulotron, has a set of relevant degrees of freedom for speech articulators, jaw, tongue, lips, and larynx. The associated articulatory model has been elaborated from cineradiographic midsagittal profiles recorded in synchrony with front lips views; the model of noise source for fricative excitation has been derived from acoustic and aerodynamic measurements on the same reference subject. In a first phase, the Articulotron has been used to perform the copy synthesis of the vowels, fricative and plosive consonants in the X-ray corpus. This allows to assess the performance of the Articulotron in producing fairly high quality speech, and provides a reference against which other attempts of articulatory synthesis can be compared. In a second phase, the Articulotron has be used to recover articulatory gestures from audio-visual speech prototypes. At the present stage, a gradient descent algorithm is used to learn the articulatory trajectories of the robot by optimisation, starting from the formant trajectories and the knowledge of constraints for the consonantal constriction or closure, in order to mimic the original VCV audio-visual sequences. The adaptive skill of the robot is demonstrated through articulator perturbation experiments and through the elaboration of relevant strategies in the hyper/hypo speech paradigm. A video tape will demonstrate an animation of the Articulotron, displaying the jaw, the tongue and the lips, for various examples of adaptive articulatory synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-301"
  },
  "suzuki96_icslp": {
   "authors": [
    [
     "Hisayoshi",
     "Suzuki"
    ],
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Hirosi",
     "Sakakibara"
    ]
   ],
   "title": "Analysis of acoustic properties of the nasal tract using 3-d FEM",
   "original": "i96_1285",
   "page_count": 4,
   "order": 304,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "In order to examine acoustic effects of complicated morphological construction of the nasal tract, we have analyzed acoustic models constructed according to a measurement by a magnetic resonance imaging (MRI). A prototype model was made to be as similar as possible with the actual nasal tract, which is asymmetrical in the left passage and the right, and has complicated cross sectional shapes. Sound pressure and particle velocity and sound intensity in the model were calculated by finite element method (FEM). Several modifications were applied on the shape of the prototype model in order to learn what acoustical effects are produced by such modifications: (1) models having elliptic shape, (2) models with and without a pair of maxillary sinuses, (3) a left-right symmetry model in which the one passage is modified to be identical with the other passage, (4) models having narrowed or stuffed passages. Result shows that the poles and zeros are produced and shifted by a mutual branching effect caused by left-right asymmetry of the nasal passages and the additional side branch effect of the sinus cavities. Those effects are also produced by complicated cross section shapes of the nasal tract at 3kHz region. The reduced cross section in the narrowed passage models causes th shift of pole and zero frequency and weakens the mutual side branch effect of the left and right when either of two passages are excessively narrowed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-302"
  },
  "liljencrants96_icslp": {
   "authors": [
    [
     "Johan",
     "Liljencrants"
    ]
   ],
   "title": "Experiments with analysis by synthesis of glottal airflow",
   "original": "i96_1289",
   "page_count": 4,
   "order": 305,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "The glottal model is based on a mechanical system with two basic degrees of freedom and is a variation on the classical two-mass model of Ishizaka and Flanagan (1972). A distinctive characteristic is that the two resonators are here organized as a translational system and a superimposed rotational system. Both resonator systems use one and the same mass element. The resonators are separately driven by the aerodynamics, the translational by the space average pressure in the glottal passage, and the rotational by the pressure gradient in the flow direction. The resonators are thus indirectly coupled by the aerodynamics. Experiences in static and dynamic approximations to natural voices using an analysis by synthesis strategy are summarized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-303"
  },
  "ouellet96_icslp": {
   "authors": [
    [
     "Marise",
     "Ouellet"
    ],
    [
     "Benoît",
     "Tardif"
    ]
   ],
   "title": "From segmental duration properties to rhythmic structure: a study of interactions between high and low level",
   "original": "i96_1177",
   "page_count": 4,
   "order": 306,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "This study deals with the temporal structure of utterances in a variety of Canadian French spoken in the Province of Quebec. In order to illustrate the way segment intrinsic durations could influence the temporal structure on which prosody takes place in Quebec French, data from read utterances and spontaneous speech were gathered and compared to results from a previous study on the variety spoken in France. The observations confirmed the preservation of a length distinction in spontaneous speech in Quebec French and showed how segmental features are apt to change the durational frame of a stress group. By so doing, the results brought up the question of the merits of using general claims about French stress patterning, segmental phonology, and syllable division for the study of Canadian French prosody. We demonstrated that the stress group, instead of the lexical units, can be used to capture segment intrinsic duration properties.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-304"
  },
  "wang96b_icslp": {
   "authors": [
    [
     "Xue",
     "Wang"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Louis F. M. ten",
     "Bosch"
    ]
   ],
   "title": "Analysis of context-dependent segmental duration for automatic speech recognition",
   "original": "i96_1181",
   "page_count": 4,
   "order": 307,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "This paper presents statistical analyses of context-dependent phone durations using the hand-segmented TIMIT database, for the purpose of improving automatic speech recognition. Two main approaches were used. (1) Duration distributions were found under the influence of individual contextual factors, such as broader classes specified by long or short vowels, word stress, syllable position within the word and within an utterance, postvocalic consonants, and utterance speaking rate. (2) A hierarchically structured analysis of variance was used to study the numerical contributions of 11 different contextual factors to the variation in duration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-305"
  },
  "dahan96_icslp": {
   "authors": [
    [
     "Delphine",
     "Dahan"
    ]
   ],
   "title": "The role of the rhythmic groups in the segmentation of continuous French speech",
   "original": "i96_1185",
   "page_count": 4,
   "order": 308,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "French is characterized by the presence of a final stress at the end of rhythmic groups. Lexical processing could be facilitated for words whose right boundary also corresponds to the rhythmic-group boundary. Sentences were constructed with a target syllable at various positions relative to word and rhythmic-group boundaries. These sentences were presented to French listeners (experiment 1) and to Dutch listeners (experiment 2), whose task was to detect a target syllable. RT analysis suggests an influence of the syllable position within the rhythmic group, independently of its position in the word. The closer to the right edge of the rhythmic group the syllable was, the faster its detection. Although this pattern was quite similar for French and Dutch listeners, some differences suggest that French listeners, but not Dutch listeners, have taken advantage of specificities of French rhythm. Universal and language-specific procedures in speech perception are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-306"
  },
  "mcrobbieutasi96_icslp": {
   "authors": [
    [
     "Zita",
     "McRobbie-Utasi"
    ]
   ],
   "title": "The implications of temporal patterns for the prosody of boundary signaling in connected speech",
   "original": "i96_1189",
   "page_count": 4,
   "order": 309,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "The objective of the study reported on here is to further examine the implications of recent research concerning the status of duration in boundary signaling in languages in which duration plays a contrastive role in the suprasegmental system. The issues addressed here are (i) the extent to which durational patterns in the temporal organization of connected speech differ from those observed in controlled experiments, and (ii) how the realization of timing strategies can be related to the prosodic manifestations of boundary signaling. It will be argued that the four timing strategies in Skolt Sámi (a Finno-Ugric language) that were observed during a series of controlled experiments appear to have a clearly noticeable hierarchy in terms of their occurrence in connected speech. This study will evaluate the characteristics of this hierarchy. Concerning the second issue it will be claimed that the analysis described has produced results which correspond with the implications of other recent research [2], [4]: i.e. showing that languages with contrastive duration tend not to utilize duration for additional functions in the grammar, in this case boundary signaling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-307"
  },
  "lee96d_icslp": {
   "authors": [
    [
     "Hyunbok",
     "Lee"
    ],
    [
     "Cheol-jae",
     "Seong"
    ]
   ],
   "title": "Experimental phonetic study of the syllable duration of Korean with respect to the positional effect",
   "original": "i96_1193",
   "page_count": 4,
   "order": 310,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "The aim of this paper is to describe the prosodic structure of Korean related to the syllable duration varying with its positional difference. An attempt is made in this study to ananlyze and describe the concrete correlation between the syllable lengthening and its position in the utterance at the initial and final positions. Using the syllable [na] at the final and initial position of a prosodic phrase in the Korean version of 'the North Wind and the Sun', it has found that the ratio of phrase final versus phrase initial syllable lengthening was approximately 1.8:1 for 4 subjects taking part in the test. In the case of nonsense data, we found that the ratio was approximately 1.6:1 for 2 out of 3 subjects. The results of this study might indicate that Korean tends to have a high rate of final lengthening. We can tentatively classify it, therefore, as a stress-timed language. Still, there is no denying that further studies should be done before we can be absolutely certain about the classification of languages along the dichotomy scale.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-308"
  },
  "hermes96_icslp": {
   "authors": [
    [
     "Dik J.",
     "Hermes"
    ]
   ],
   "title": "Timing of pitch movements and accentuation of syllables",
   "original": "i96_1197",
   "page_count": 4,
   "order": 311,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "In this study, the relation is investigated between the timing of a rising or falling pitch movement and the syllable it accentuates. The experiments were carried out with the five-syllable utterance /mamamamama/ provided with a rising or falling pitch movement. The timing of the pitch movement was systematically varied and subjects were asked to indicate which syllable they perceived as accented. In order to find out where in the pitch movement the cue which induces the percept of accentuation is located, the duration of the pitch movement was varied. The results show that the percept of accentuation is induced by a change in pitch at the start of the movement. The moment at which the course of pitch has changed significantly determines which syllable is perceived as accented. If this moment lies some tens of ms before the vowel onset, the syllable preceding this vowel onset is perceived as accented. For a rise, a high accent is perceived, for a fall a low accent. If the pitch change occurs after this moment, the syllable which contains this vowel onset is perceived as accented. For the rise, a low accent is then perceived, for the fall a high accent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-309"
  },
  "ying96_icslp": {
   "authors": [
    [
     "Goangshiuan S.",
     "Ying"
    ],
    [
     "Leah H.",
     "Jamieson"
    ],
    [
     "Carl D.",
     "Michell"
    ]
   ],
   "title": "A probabilistic approach to AMDF pitch detection",
   "original": "i96_1201",
   "page_count": 4,
   "order": 312,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "We present a probabilistic error correction technique to be used with an average magnitude difference function (AMDF) based pitch detector. This error correction routine provides a very simple method to correct errors in pitch period estimation. Used in conjunction with the computationally efficient AMDF, the result is a fast and accurate pitch detector. In performance tests on the CSTR (Center for Speech Technology Research) database, probabilistic error correction reduced the gross error rate from 6.07% to 3.29%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-310"
  },
  "soquet96_icslp": {
   "authors": [
    [
     "Alain",
     "Soquet"
    ],
    [
     "Véronique",
     "Lecuit"
    ],
    [
     "Thierry",
     "Metens"
    ],
    [
     "Didier",
     "Demolin"
    ]
   ],
   "title": "From sagittal cut to area function: an RMI investigation",
   "original": "i96_1205",
   "page_count": 4,
   "order": 313,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "This paper presents a comparative study of transformations used to compute the area of a cross sections of the vocal tract from the midsagittal diameter of the vocal tract. MRI techniques have been used to obtain cross sections of the vocal tract in a study of French oral vowels uttered by two subjects. The measured cross sectional areas are compared to the cross sectional areas computed by the different transformations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-311"
  },
  "janer96_icslp": {
   "authors": [
    [
     "Léonard",
     "Janer"
    ],
    [
     "Juan José",
     "Bonet"
    ],
    [
     "Eduardo",
     "Lleida-Solano"
    ]
   ],
   "title": "Pitch detection and voiced/unvoiced decision algorithm based on wavelet transforms",
   "original": "i96_1209",
   "page_count": 4,
   "order": 314,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "An improvement of an existing Pitch Detection Algorithm is presented in this paper. The solution reduces the computational load of its precedent algorithm and introduces a voiced/unvoiced decision step to reduce the number of errors. The efficiency of this improved system is tested with a semi-automatically segmented speech data base according to the information delivered by an attached laryngograph signal. The results show its periodicity detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-312"
  },
  "stylianou96_icslp": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Decomposition of speech signals into a deterministic and a stochastic part",
   "original": "i96_1213",
   "page_count": 4,
   "order": 315,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "In this contribution a novel method enabling en efficient decomposition of the speech signal into a deterministic and a stochastic part is presented. The deterministic part is modeled by harmonically related sinusoids; the proposed method makes use of a third order polynomial with real coefficients for the harmonic amplitudes and the phase is assumed to be linear. To obtain the stochastic part the deterministic part is simply subtracting from the original speech signal in the time domain. The results obtained on large speech database demonstrate effective decomposition of the speech signal between the two parts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-313"
  },
  "jo96_icslp": {
   "authors": [
    [
     "Cheol-Woo",
     "Jo"
    ],
    [
     "Ho-Gyun",
     "Bang"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Improved glottal closure instant detector based on linear prediction and standard pitch concept",
   "original": "i96_1217",
   "page_count": 4,
   "order": 316,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "This paper proposes an improved method of glottal closure instant detection using linear prediction and standard pitch concept. The main improvements are on its speed of computation and error reduction on position finding for the cases that were not possible or caused many errors using previous methods. Our method can resolve the problems occurring in current methods to some extent. The false location detection rate is reduced to its inherent interpolation capability. Also the amount of computation is reduced. Another benefit from our method is that it does not need additional post processing to find peaks or smoothing of the pitch tracks. All is contained in itself. Also we compared results among three different kinds of linear prediction based pitch detectors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-314"
  },
  "wang96c_icslp": {
   "authors": [
    [
     "Xihong",
     "Wang"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Stefan",
     "Auberg"
    ]
   ],
   "title": "Analysis of speech segments using variable spectral/temporal resolution",
   "original": "i96_1221",
   "page_count": 4,
   "order": 317,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "In this paper we present an approach for efficiently computing a compact temporal/spectral feature set for representing a segment of speech, with effective resolution depending on both frequency and time position within the segment. The goal is to mimic the resolution properties of the human auditory system, but using a computationally efficient FFT-based front end rather than a more complex auditory model. In particular we apply both frequency and time \"warping\" to FFT spectra to obtain good frequency resolution at low frequencies and good time resolution at high frequencies. Time resolution is also varied so that the center of the segment is better represented than the endpoints. The resolution can be varied by the selection of \"warping\" functions controlled using a small number of parameters. The method was experimentally verified for the classification of six stops extracted from the TIMIT continuous speech data base. The best classification rate obtained was 81.2% for test data using 50 features computed with the method presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-315"
  },
  "eberman96_icslp": {
   "authors": [
    [
     "Brian",
     "Eberman"
    ],
    [
     "William",
     "Goldenthal"
    ]
   ],
   "title": "Time-based clustering for phonetic segmentation",
   "original": "i96_1225",
   "page_count": 4,
   "order": 318,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "This paper describes a approach to speech segmentation. Unlike approaches based on spectral measurements, our algorithm iteratively clusters on an LPC representation of time waveform blocks. The algorithm uses a generalized maximum likelihood criterion for deciding when two neighboring pieces of the signal should be joined. This paper describes the algorithm and shows that it yields superior results when compared to metrics based on spectral or cepstral measurements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-316"
  },
  "zolfaghari96_icslp": {
   "authors": [
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Formant analysis using mixtures of Gaussians",
   "original": "i96_1229",
   "page_count": 4,
   "order": 319,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "This paper describes a new formant analysis technique whereby the formant parameters are represented in the form of Gaussian mixture distributions. These are estimated from the Discrete Fourier Transform (DFT) magnitude spectrum of the speech signal. The parameters obtained are the means, variances and the masses of the density functions, which are used to calculate centre frequencies, bandwidths and amplitudes of formants within the spectrum. In order to better fit the mixture distributions various modifications to the DFT magnitude spectrum, based on simple models of perception, were investigated. These include reduction of dynamic range, cepstral smoothing, use of the Mel scale and pre-emphasis of speech. Results are presented for these as well as formant tracks from analysing speech using the final formant analysis system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-317"
  },
  "richards96_icslp": {
   "authors": [
    [
     "Hywel B.",
     "Richards"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "Melvyn J.",
     "Hunt"
    ],
    [
     "John S.",
     "Bridle"
    ]
   ],
   "title": "Deriving articulatory representations from speech with various excitation modes",
   "original": "i96_1233",
   "page_count": 4,
   "order": 320,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "A new approach is described which estimates vocal tract shape sequences for speech consisting of voiceless speech and periods of silence as well as voiced speech. This method, based on the use of articulatory codebooks, has proved successful in identifying the place position of stops and fricatives. Secondly, we focus on voiced speech in particular. A fast analysis-by-synthesis scheme, which gives continuously-valued area estimates, has been developed. Savings in computation of 50:1 have been achieved by using an MLP to perform the synthesis in this method. The technique also allows a more complex dynamic model to be used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-318"
  },
  "sharma96_icslp": {
   "authors": [
    [
     "Manish",
     "Sharma"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "blind speech segmentation: automatic segmentation of speech without linguistic knowledge",
   "original": "i96_1237",
   "page_count": 4,
   "order": 321,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "A new automatic speech segmentation procedure, called the \"Blind\" speech segmentation, is presented. This procedure allows a speech sample to be segmented into sub-word units without the knowledge of any linguistic information (such as, orthographic or phonetic transcription). Hence, this procedure involves finding the optimal number of sub-word segments in the given speech sample, before locating the sub-word segment boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-319"
  },
  "ohmura96_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Ohmura"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Speech synthesis using a nonlinear energy damping model for the vocal folds vibration effect",
   "original": "i96_1241",
   "page_count": 4,
   "order": 322,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "From a theoretical viewpoint, the vocal folds vibration affects the vocal tract transfer characteristics through nonlinear time-varying interaction between the glottis and vocal tract. Therefore, it is crucial to investigate and model such effects in order to improve voice quality in parametric rule-based speech synthesis systems. In this paper, we first conducted analytic experiments on the vocal folds vibration effects on the appeared in formant energy damping patterns and then modeled them by a nonlinear 2nd order differential equation for speech synthesis. At last, we confirmed the feasibility of the nonlinear model by speech wave reconstruction experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-320"
  },
  "namba96_icslp": {
   "authors": [
    [
     "Munehiro",
     "Namba"
    ],
    [
     "Hiroyuki",
     "Kamata"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Neural networks learning with L1 criteria and its efficiency in linear prediction of speech signals",
   "original": "i96_1245",
   "page_count": 4,
   "order": 323,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "The classical learning technique such as the back-propagation algorithm minimizes the expectation of the squared error that arise between the actual output and the desired output of supervised neural networks. The network trained by such a technique, however, does not behave in the desired way, when it is embedded in the system that deals with non-Gaussian signals. As the least absolute estimation is known to be robust for noisy signals or a certain type of non-Gaussian signals, the network trained with this criterion might be less sensitive to the type of signals. This paper discusses the least absolute error criterion for the error minimization in supervised neural networks. We especially pay attention to its efficiency for the linear prediction of speech. The computational loads of the conventional approaches to this estimation have been much heavier than the usual least squares estimator. But the proposed approach can significantly improve the analysis performance, since the method is based on the simple gradient descent algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-321"
  },
  "esposito96_icslp": {
   "authors": [
    [
     "Anna",
     "Esposito"
    ],
    [
     "C. E.",
     "Ezin"
    ],
    [
     "M.",
     "Ceccarelli"
    ]
   ],
   "title": "Preprocessing and neural classification of English stop consonants [b,d,g,p,t,k]",
   "original": "i96_1249",
   "page_count": 4,
   "order": 324,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "Neural networks are accepted as powerful learning tools in pattern recognition in which they proved their performance. Nevertheless, many problems like phoneme classifies lion with multi-speaker continuous speech database are hard even for Neural Networks. Our aim is to propose an Artificial Neural Network architecture that detects acoustic features in speech signals and classifies them correctly. We reached this goal with English stop consonants [b, d, g, p, t, k] extracted from the general multi-speaker database (TTMIT) by modifying some parameter values m the preprocessing algorithm and by using a modified TDNN ( Time Delay Neural Network) architecture. Our net performed a good classification giving as testing recognition percentage the following results; 92.9 for [b], 91.8 for [d], 92.4 for [g], 80.3 for [p], 90.2 for [t], 94.2 for [k].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-322"
  },
  "ananthakrishnan96_icslp": {
   "authors": [
    [
     "K. S.",
     "Ananthakrishnan"
    ]
   ],
   "title": "A comparison of modified k-means(MKM) and NN based real time adaptive clustering algorithms for articulatory space codebook formation",
   "original": "i96_1253",
   "page_count": 5,
   "order": 325,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "This paper proposes the use of a neural network based real time adaptive clustering* algorithm for the formation of a codebook of limited set of acoustical representation of finite set of vocal tract shapes from an articulatory space. Modified k-means algorithm (MKM) used for clustering nearly 10000 vocal tract shapes into 1000 cluster centers to form a codebook of articulatory shapes is computationally intensive for our application. An investigative study on the use of NN based algorithm over MKM algorithm at the peripheral level, for our application on Computer Aided Pronunciation-education, suggests the former for less intensive computation, with the possibility of improving the performance of the system by implementing the algorithm using a dedicated neural computer. In this paper, preliminary results of this study are reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-323"
  },
  "ding96_icslp": {
   "authors": [
    [
     "Wen",
     "Ding"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "A novel approach to the estimation of voice source and vocal tract parameters from speech signals",
   "original": "i96_1257",
   "page_count": 5,
   "order": 326,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "This paper presents a novel adaptive pitch-synchronous analysis method for simultaneous estimation of voice source and vocal tract (formant / antiformant) parameters from the speech signal. The method uses a parametric Rosenberg-Klatt model to generate a glottal waveform and an aut degressive with exogenous input (ARX) model for representing speech production process. The time-varying coefficients of the model are estimated with an adaptive algorithm based on Kalman filter, while the parameters of the Rosenberg-Klatt model are optimized using the simulated annealing method. In addition, a new hybrid error criterion is used to optimize the glottal opening instant. Furthermore, in order to estimate the fundamental period parameter To. it is defined as two successive glottal closure instants, and is estimated automatically based on the obtained differentiated glottal waveforms. Experiments using two-channel speech signals (speech and electroglottograph (EGG) signal) and continuous speech show a good estimation performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-324"
  },
  "pfitzinger96_icslp": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ],
    [
     "Susanne",
     "Burger"
    ],
    [
     "Sebastian",
     "Heid"
    ]
   ],
   "title": "Syllable detection in read and spontaneous speech",
   "original": "i96_1261",
   "page_count": 4,
   "order": 327,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "Automatic syllable detection is an important task when analysing very large speech corpora in order to answer questions concerning prosody, rhythm, speech rate, speech recognition and synthesis. In this paper a new method for automatic detection of syllable nuclei is presented. Two large spoken language corpora (PhonDatII, Verbmobil) were labelled by three phoneticians and then used to adjust the key parameters of the algorithm and to evaluate its error rate. Additionally, parts of the corpora were used to test the inter- and intra-individual consistency of the transcribers. The evaluation of the algorithm currently shows an error rate of 12.87% for read speech and 21.03% for spontaneous speech. The inter-individual consistency of 95.8% might be considered as an upper limit for any automatic detection method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-325"
  },
  "wang96d_icslp": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Maximum likelihood learning of auditory feature maps for stationary vowels",
   "original": "i96_1265",
   "page_count": 4,
   "order": 328,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "In this paper, a mathematical framework for learning the acoustic features from a central auditory representation is presented. We adopt a statistical approach that models the learning process as to achieve a maximum likelihood estimation of the signal distribution. An algorithm, called statistical matching pursuit (SMP), is introduced to identify regions on the cortical surface where the features for each sound class are most prominent. We model the features with distributions of Gaussian mixture densities, and employ the expectation-maximization (EM) procedure to both improve the parameterization and refine iteratively the selection of cortical regions from which the features are extracted. The learning algorithm is applied to vowel classification on TIMIT database where all the vowels (excluding diphthongs, nine in total) are regarded as individual classes. Experimental results show that models trained under SMP/EM algorithm achieve a comparable recognition accuracy to that of conventional recognizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-326"
  },
  "bonafonte96c_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Albino",
     "Nogueiras"
    ],
    [
     "Antonio",
     "Rodriguez-Garrido"
    ]
   ],
   "title": "Explicit segmentation of speech using Gaussian models",
   "original": "i96_1269",
   "page_count": 4,
   "order": 329,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "In this paper we investigate an automatic method to segment labeled speech. The method needs an initial estimation of the segmentation which is provided by an alignment based on HMM. Afterwards, the boundaries are refined moving the frontier frames to the segment which is more similar to the speech frame. Gaussian pdf are used as a similarity measure. The performance of the method is evaluated using the TIMIT database. If boundary deviation (from the reference position) larger than 20 ms. are counted as errors, then the replacement of the boundaries reduces the error in a 30%. Additional experiments show how the proposed method turns the performance quite independent of the speaker dependent or speaker independent data used to estimate the HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-327"
  },
  "mousset96_icslp": {
   "authors": [
    [
     "E.",
     "Mousset"
    ],
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "José A. R.",
     "Fonollosa"
    ]
   ],
   "title": "A comparison of several recent methods of fundamental frequency and voicing decision estimation",
   "original": "i96_1273",
   "page_count": 4,
   "order": 330,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "In this paper, we are interested in the comparison of several kinds of methods for fundamental frequency estimation and GCI (Glottal Closure Instant) detection. These methods operate in various domains (time-, frequency- or joint time-frequency domains). Their performances have been compared for both fundamental frequency estimation and voicing decision tasks as well as GCI detection, where applicable. This comparison was designed to be as unbiased as possible, so as to reflect the intrinsic properties of each method. A method based on a \"Born-Jordan\" kernel bilinear time-frequency representation of speech signals achieves the best performance in terms of GCI detection accuracy but is not as robust to inter-speaker variability as the SIFT algorithm. An auditory model, which has been applied on the same data in a previous study, has been shown to compare favourably to other methods (such as SIFT) in adverse noisy conditions only.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-328"
  },
  "abe96_icslp": {
   "authors": [
    [
     "Toshihiko",
     "Abe"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Robust pitch estimation with harmonics enhancement in noisy environments based on instantaneous frequency",
   "original": "i96_1277",
   "page_count": 9,
   "order": 331,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "In this paper, we propose an approach for estimating pitch of speech in noisy environments based on instantaneous frequency(IF). First, we define the IF amplitude spectrum, which is obtained by projecting the STFT amplitude spectrum onto the IF axis. Based on the IF amplitude spectrum, we can perform harmonics enhancement by suppressing the aperiodic components. Next, we define an evaluation function to find pitch. This is done by expanding the IF amplitude spectrum to the tune region. Then we propose a method for obtaining a continuous pitch contour using the dynamic programming. Experiments show accuracy and robustness of our method especially when noise exists.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-329"
  },
  "moreno96_icslp": {
   "authors": [
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Miquel",
     "Rutllán"
    ]
   ],
   "title": "Integrated polispectrum on speech recognition",
   "original": "i96_1281",
   "page_count": 4,
   "order": 332,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "This paper deals with the application of Higher Order Statistics (HOS) in a recognition system where the speech input is corrupted by noise. Cumulants, Biespectrum and Trispectrum are being used in noisy speech signal processing because of their immunity to noise. In this paper we introduce the use of integrated polispectrum and triespectrum in a recognition system based on a filter bank analysis. The results improves other HOS based methods without a substantial increase of the computational load.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-330"
  },
  "neto96_icslp": {
   "authors": [
    [
     "Joao P.",
     "Neto"
    ],
    [
     "Ciro A.",
     "Martins"
    ],
    [
     "Luís B.",
     "Almeida"
    ]
   ],
   "title": "An incremental speaker-adaptation technique for hybrid HMM-MLP recognizer",
   "original": "i96_1293",
   "page_count": 4,
   "order": 333,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "One of the problems of the speaker-independent continuous speech recognition systems is their inability to cope with the inter-speaker variability. When we find test speakers with different characteristics from the ones presented in the training pool we observe a large degradation on the system performance. To overcome this problem speaker-adaptation techniques may be used to provide near speaker-dependent accuracy. In this work we present a speaker-adaptation technique applied to a hybrid HMM-MLP system for large vocabulary, continuous speech recognition. This technique is based on an architecture that employs a trainable Linear Input Network (LIN) to map the speaker specific features input vectors to the speaker-independent system. This speaker-adaptation technique will be evaluatedin an incremental speaker-adaptation task using the Wall Street Journal (WSJ) database. Both supervised and unsupervised modes are evaluated. The results show that speaker-adaptation within the hybrid framework can substantially improve system performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-331"
  },
  "suh96_icslp": {
   "authors": [
    [
     "Youngjoo",
     "Suh"
    ],
    [
     "Youngjik",
     "Lee"
    ]
   ],
   "title": "Phoneme segmentation of continuous speech using multi-layer perceptron",
   "original": "i96_1297",
   "page_count": 4,
   "order": 334,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "In this paper, we propose a new method of phoneme segmentation using MLP (multi-layer perceptron). The structure of the proposed segmenter consists of three parts: preprocessor, MLP-based phoneme segmenter, and postprocessor. The preprocessor utilizes a sequence of 44 order feature parameters for each frame of speech, based on the acoustic-phonetic knowledge. The MLP has one hidden layer and an output layer. The feature parameters for four consecutive inter-frame features (176 parameters) are served as input data. The output value decides whether the current frame is a phoneme boundary or not. In postprocessing, we decide the positions of phoneme boundaries using the output of the MLP. We obtained 84 % for 5 msec-accuracy and 87 % for 15 msec-accuracy with an insertion rate of 9 % for open test. By adjusting the threshold value of the MLP output, we achieved higher accuracy. When we decreased the threshold by 0.4, we obtained 5 msec-accuracy of 92 % with insertion rate of 3.4 % for the insertions that are more than 15 msec apart from phoneme boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-332"
  },
  "bilmes96_icslp": {
   "authors": [
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Su-Lin",
     "Wu"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Stochastic perceptual speech models with durational dependence",
   "original": "i96_1301",
   "page_count": 4,
   "order": 335,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "In [6], we develop statistical model of speech recognition where emphasis is placed on the perceptually-relevant and information-rich portion of the speech signal. In that model, speech is viewed as a sequence of elementary decisions or Auditory Events (avents) that are made in response to loci of significant spectral change. These decision points are interleaved with periods during which insufficient information has been accumulated to make the next decision. We have called this a Stochastic Perceptual Avent Model, or SPAM. In the work reported here, we have extended our initial experimental implementation [7] to include other probabilistic dependencies specified in the original theory, particularly the dependence on the time from the current frame back to the previous hypothesized avent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-333"
  },
  "cook96_icslp": {
   "authors": [
    [
     "G. D.",
     "Cook"
    ],
    [
     "A. J.",
     "Robinson"
    ]
   ],
   "title": "Boosting the performance of connectionist large vocabulary speech recognition",
   "original": "i96_1305",
   "page_count": 4,
   "order": 336,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "Hybrid connectionist-hidden Markov model large vocabulary speech recognition has, in recent years, been shown to be competitive with more traditional HMM systems [4]. Connectionist acoustic models generally use considerably less parameters than HMMs, allowing real-time operation without significant degradation of performance. However, the small number of parameters in connectionist acoustic models also poses a problem - how do we make the best use of large amounts of training data? This paper proposes a solution to this problem in which a \"smart\" procedure makes selective use of training data to increase performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-334"
  },
  "pican96_icslp": {
   "authors": [
    [
     "Nicolas",
     "Pican"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-François",
     "Mari"
    ]
   ],
   "title": "HMMs and OWE neural network for continuous speech recognition",
   "original": "i96_1309",
   "page_count": 4,
   "order": 337,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "The phonetic context has a large effect on stop consonants in a continuous speech signal [1]. Therefore recognition systems that model allophones using context-dependent Hidden Markov Models have been implemented [3]. HMMs have a great ability for the segmentation in the temporal domain [4][6] but have some difficulties in the recognition because the MLE training (Maximum Likelihood Estimation) is not discriminant, whereas the discrimination is one of the abilities of the Artificial Neural Networks models. In the last three years we have developed a new ANN model named OWE (Orthogonal Weight Estimator)[9][10]. The principle of the OWE is a ANN that classifies an input pattern according to contextual environment. This new ANN architecture tackles the problem of context dependent behaviour training. Roughly, the principle is based on main MLP (Multilayered Perceptron) in which each synaptic weight connection value is estimated by another MLP (an OWE) with respect to context representation. In this paper, we present a hierarchical system for phoneme recognition: first the system segments the input signal using 48 context independent HMMs. Then the stop consonant are reordered by a OWE ANN. Experiments on TIMIT show 78 % of correct recognition rate on the 6 stop consonants (/p, t, k, b, d, g).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-335"
  },
  "waterhouse96_icslp": {
   "authors": [
    [
     "Steve",
     "Waterhouse"
    ],
    [
     "Dan",
     "Kershaw"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Smoothed local adaptation of connectionist systems",
   "original": "i96_1313",
   "page_count": 4,
   "order": 338,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "abbot is the hybrid connectionist hidden Markov model (HMM) large vocabulary continuous speech recognition system developed at Cambridge University Engineering Department. abbot makes effective use of the linear input network (LIN) adaptation technique to achieve speaker and channel adaptation. Although the LIN is effective at adapting to new speakers or a new environment (e.g. a different microphone), the transform is global over the input space. In this paper we describe a technique by which the transform may be made locally linear over different regions of the input space. The local linear transforms are combined by an additional network using a non-linear transform. This scheme falls naturally into the mixtures of experts framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-336"
  },
  "yamada96b_icslp": {
   "authors": [
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Robust speech recognition with speaker localization by a microphone array",
   "original": "i96_1317",
   "page_count": 4,
   "order": 339,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "This paper proposes robust speech recognition with Speaker Localization by a Arrayed Microphone (SLAM) to realize hands-free speech interface in noisy environments. In order to localize a speaker direction accurately in low SNR conditions, a speaker localization algorithm based on extracting a pitch harmonics is introduced. To evaluate the performance of the proposed system, speech recognition experiments are carried out both in computer simulation and real environments. These results show that the proposed system attains the much higher speech recognition performance than that of a single microphone not only in computer simulation but also in real environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-337"
  },
  "jan96_icslp": {
   "authors": [
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "Sound source localization in reverberant environments using an outlier elimination algorithm",
   "original": "i96_1321",
   "page_count": 4,
   "order": 340,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "Differences in arrival times of acoustic waves at multiple sensors permit the computation of source location. The computation depends upon delay estimation between sensor pairs. In severe acoustic environments, the estimates are degraded by reverberation and interfering noise, and some estimates are poor, constituting outlier. This report describes a computational method for outlier elimination to improve the accuracy of source location.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-338"
  },
  "kershaw96_icslp": {
   "authors": [
    [
     "Dan",
     "Kershaw"
    ],
    [
     "Tony",
     "Robinson"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "The 1995 abbot LVCSR system for multiple unknown microphones",
   "original": "i96_1325",
   "page_count": 4,
   "order": 341,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "ABBOT is the hybrid connectionist-hidden Markov model large-vocabulary speech recognition system developed at Cambridge University. In this system, a recurrent network maps each acoustic vector to an estimate of the posterior probabilities of the phone classes, which are used as observation probabilities within an HMM. This paper describes the system which participated in the November 1995 ARPA Hub-3 Multiple Unknown Microphones (MUM) evaluation of continuous speech recognition systems, under the guise of the CU-CON system. The emphasis of the paper is on the changes made to the 1994 ABBOT system, specifically to accomodate the H3 task. This includes improved acoustic modelling using limited word-internal context-dependent models, training on the Wall Street Journal secondary channel database, and using the linear input network for speaker and environmental adaptation. Experimental results are reported for various test and development sets from the November 1994 and 1995 ARPA benchmark tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-339"
  },
  "giuliani96_icslp": {
   "authors": [
    [
     "D.",
     "Giuliani"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "P.",
     "Svaizer"
    ]
   ],
   "title": "Experiments of speech recognition in a noisy and reverberant environment using a microphone array and HMM",
   "original": "i96_1329",
   "page_count": 4,
   "order": 342,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "The use of a microphone array for hands-free continuous speech recognition in noisy and reverberant environment is investigated. An array of four omnidirectional microphones is placed at 1.5 m distance from the talker; given the array signals, a Time Delay Compensation (TDC) module provides a beamformed signal, that is shown effective as input to a Hidden Markov Model (HMM) based recognizer. Given a small amount of sentences collected from a new speaker in a real environment, HMM adaptation further improves recognition rate. These results are confirmed both by experiments conducted in a noisy office environment and by simulations. In the latter case, different SNR and reverberation conditions were recreated by using the image method to reproduce synthetic array microphone signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-340"
  },
  "gonzalezrodriguez96_icslp": {
   "authors": [
    [
     "Joaquín",
     "González-Rodríguez"
    ],
    [
     "Javier",
     "Ortega-García"
    ],
    [
     "César",
     "Martin"
    ],
    [
     "Luis",
     "Hernández"
    ]
   ],
   "title": "Increasing robustness in GMM speaker recognition systems for noisy and reverberant speech with low complexity microphone arrays",
   "original": "i96_1333",
   "page_count": 4,
   "order": 343,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "In this paper we describe the additive robustness obtained through the combined use of a first acoustic processing step based on a low complexity microphone array, followed by a spectral normalization step. Microphone arrays have shown to provide good results in reducing different sources of acoustic degradation. However, microphone arrays produce linear filtering effects that need to be compensated in order to obtain a minimal spectral distortion. In this contribution we will present the combination of a microphone array together with different well known spectral normalization techniques as preprocessing stages to a Gaussian Mixture Models (GMM) based text-independent speaker recognition system. We will show that the combination of these extensively used techniques in the fields of speech enhancement and robust speaker recognition respectively, greatly improves the results obtained when the system is tested in noisy reverberant environments with short utterances from unconstrained conversational speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-341"
  },
  "yen96_icslp": {
   "authors": [
    [
     "Kuan-Chieh",
     "Yen"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Robust automatic speech recognition using a multi-channel signal separation front-end",
   "original": "i96_1337",
   "page_count": 4,
   "order": 344,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "A multi-channel signal separation front-end for robust automatic speech recognition under time-varying interference conditions is developed. The speech signals acquired by a dual-channel system are restored by adaptive decorrelation filtering, and then examined by a time-domain or frequency-domain source signal detection technique to determine the active regions of each source signal. The front-end is integrated with an HMM-based speaker-independent continuous speech recognition system by providing the restored signals within the active regions for recognition. Under a simulated room acoustic condition, the overall system shows very promising performance. For the conditions with SNR above -10 dB, the achieved word recognition accuracies are very close to that of the interference-free condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-342"
  },
  "lindstrom96_icslp": {
   "authors": [
    [
     "Anders",
     "Lindström"
    ],
    [
     "Ivan",
     "Bretan"
    ],
    [
     "Mats",
     "Ljungqvist"
    ]
   ],
   "title": "Prosody generation in text-to-speech conversion using dependency graphs",
   "original": "i96_1341",
   "page_count": 4,
   "order": 345,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "The present paper addresses the problem of prosody assignment in the context of a system for text-to-speech conversion of Swedish. The question of what type of textual analysis is needed for prosodic purposes is discussed and the use of Dependency Graphs to this end is proposed. It is argued that this type of analysis provides sufficient syntactic information so that prosodic phrasing and prominence assignment algorithms can be successfully applied. The ability of the system to generate acceptable prosodic descriptions is evaluated by comparison with examples of prosodically analysed naturally spoken sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-343"
  },
  "asano96_icslp": {
   "authors": [
    [
     "Hisako",
     "Asano"
    ],
    [
     "Hisashi",
     "Ohara"
    ],
    [
     "Yoshifumi",
     "Ooyama"
    ]
   ],
   "title": "Extraction method of non-restrictive modification in Japanese as a marked factor of prosody",
   "original": "i96_1345",
   "page_count": 4,
   "order": 346,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper shows by a quantitative evaluation that non-restrictive modification in Japanese is a marked factor of prosody, and describes how to extract non-restrictive modification. In Japanese, restrictive and non-restrictive modification have the same syntactic structure, so it is difficult to distinguish them. We propose rules to extract non-restrictive modification in news-text based on the classification of non-restrictive modification and the combination patterns of the head noun and the modifier.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-344"
  },
  "prevost96_icslp": {
   "authors": [
    [
     "Scott",
     "Prevost"
    ]
   ],
   "title": "Modeling contrast in the generation and synthesis of spoken language",
   "original": "i96_1349",
   "page_count": 5,
   "order": 347,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "This paper presents an implemented model of spoken language processing that accounts for intonational phenomena associated with semantic contrasts. The model determines accentual patterns based on sets of alternative properties from a knowledge base and a contrastive stress algorithm. The results of applying the model to a natural language generation program illustrate the advantages over previous models based on lexical \"givenness.\"\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-345"
  },
  "tsukada96_icslp": {
   "authors": [
    [
     "Hajime",
     "Tsukada"
    ]
   ],
   "title": "A left-to-right processing model of pausing in Japanese based on limited syntactic information",
   "original": "i96_1353",
   "page_count": 4,
   "order": 348,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "This paper proposes a new model for determining where to pause in Japanese. Our model, which simulates the left-to-right processing of human speech production, incorporates the two main factors of pausing: phrase length and limited syntactic information. By taking this approach, our model can explain several types of pausing phenomena in Japanese, which cannot be explained by the naive pausing model. To prove the validity of our model, we implemented it in a text-to-speech conversion system. Through a listening test, we compared the proposed model with a naive pausing model, mainly based on phrase length. Results show that the proposed model performs well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-346"
  },
  "galanis96_icslp": {
   "authors": [
    [
     "D.",
     "Galanis"
    ],
    [
     "V.",
     "Darsinos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Modeling of intonation bearing emphasis for TTS-synthesis of greek dialogues",
   "original": "i96_1357",
   "page_count": 4,
   "order": 349,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "TTS-synthesis of neutral style Greek with good intelligibility and quality has been achieved some time ago. As a further step towards expanding the applications domain of the TTS-system developed in our laboratory, the incorporation of emphasis into speech used in man-machine dialogues according to their context has been studied recently. In this paper the method applied for the analysis of intonation patterns, the results of this analysis, the algorithm established for the creation of desired intonation patterns and its implementation in the existing TTS-system for Greek is reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-347"
  },
  "heuft96_icslp": {
   "authors": [
    [
     "Barbara",
     "Heuft"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Synthesizing prosody: a prominence-based approach",
   "original": "i96_1361",
   "page_count": 4,
   "order": 350,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "A method for generating acoustic prosody is presented that starts from a very simple symbolic input. We present evidence that prominence is a central factor influencing both perception and acoustic parameters. Results of statistical analysis of a large speech corpus are shown, these results have led to the development of a rule system that predicts fundamental frequency and syllable duration. Besides the prominence of syllables and boundaries, position, context and syllable structure are considered by these rules. Finally, the outcome of two evaluation experiments is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-348"
  },
  "sproat96_icslp": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ]
   ],
   "title": "Multilingual text analysis for text-to-speech synthesis",
   "original": "i96_1365",
   "page_count": 4,
   "order": 351,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "We present a model of text analysis for text-to-speech (TTS) synthesis based on weighted finite-state transducers, which serves as the text-analysis module of the multilingual Bell Labs TTS system. The transducers are constructed using a lexical toolkit that allows declarative descriptions of lexicons, morphological rules, numeral-expansion rules, and phonological rules, inter alia. To date, the model has been applied to eight languages: Spanish, Italian, Romanian, French, German, Russian,Mandarin and Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-349"
  },
  "ooyama96_icslp": {
   "authors": [
    [
     "Yoshifumi",
     "Ooyama"
    ],
    [
     "Hisako",
     "Asano"
    ],
    [
     "Koji",
     "Matsuoka"
    ]
   ],
   "title": "Spoken-style explanation generator for Japanese kanji using a text-to-speech system",
   "original": "i96_1369",
   "page_count": 5,
   "order": 352,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "In this paper we describe a spoken explanation generator, PLANET,1 for Japanese Kanji (ideograms), especially Kanji used in people's names. A number of text-to-speech systems for Kanji texts have been proposed but this is the first one that can explain Kanji characters so as to disambiguate characters from many homophone Kanji candidates. To accomplish this the generator explains the Kanji by using both internal composition and use in other words. The system has a database of over 6,000 Kanji characters that breaks down any given Kanji into its components and a text corpus of explanations for explaining two or more Kanji characters. It is capable of generating both other words that include the Kanji characters in question, and identifying information. Using these other words and the information the system makes phrases and sentences. Furthermore, this system generates natural prosodic information by classifying the pattern of semantic connections between words and phrases. The explanations are output through a natural-sounding voice synthesizer. Hearing examinations confirmed that this system achieves high accuracy in disambiguating Kanji characters from among many candidates. This system will make it possible to provide advanced and user-friendly human-computer interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-350"
  },
  "magata96_icslp": {
   "authors": [
    [
     "Ken-ichi",
     "Magata"
    ],
    [
     "Tomoki",
     "Hamagami"
    ],
    [
     "Mitsuo",
     "Komura"
    ]
   ],
   "title": "A method for estimating prosodic symbol from text for Japanese text-to-speech synthesis",
   "original": "i96_1373",
   "page_count": 4,
   "order": 353,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "This report describes a method for estimating the separation degree at the bunsetsu boundary (SD) for Japanese text-to-speech synthesis. Our method gives us the prosodic symbol without using complicated linguistic analysis. First we classify bunsetsus according to the final morpheme. Each classified bunsetsu has a temporary separation degree in advance. We call this \\the estimated separation degree\" (ESD). ESD is derived from the SD's statistical tendency regarding each bunsetsu. The SD is decided by rules that correct the ESD as an initial degree. Correction rules are constructed by comparing the ESD, and the SD is observed from natural speech to cancel the frequently occurring mismatches. An absolute evaluation test of five grades was performed upon 300 sentences with prosodic symbols given by our method. As a result, the ratio of \\Natural\" and \\Somewhat unnatural but tolerable\" exceeded 2/3. The proportion of \\Serious error\" was less than 10%, thus giving us satisfactory results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-351"
  },
  "lopezgonzalo96_icslp": {
   "authors": [
    [
     "E.",
     "López-Gonzalo"
    ],
    [
     "J. M.",
     "Rodríguez-García"
    ]
   ],
   "title": "Statistical methods in data-driven modeling of Spanish prosody for text to speech",
   "original": "i96_1377",
   "page_count": 4,
   "order": 354,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "In [1], we proposed an automatic data-driven methodology to model both fundamental frequency and segmental duration in TTS converters from a monospeaker recorded corpus. Therefore, it had the advantage that could be adapted to a specific corpus or a particular speaker. The main disadvantage was the size of the obtained prosodic database. In this paper, we propose to use some statistical methods for reducing the prosodic database required in this methodology. A 50% of reduction can be obtained without compromising the naturalness of the synthetic speech obtained by our previous methodology with the same prosodic corpus. A compromise between variability and reduction in prosodic contours is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-352"
  },
  "lee96e_icslp": {
   "authors": [
    [
     "Jung-Chul",
     "Lee"
    ],
    [
     "Youngjik",
     "Lee"
    ],
    [
     "Sang-Hun",
     "Kim"
    ],
    [
     "Minsoo",
     "Hahn"
    ]
   ],
   "title": "Intonation processing for TTS using stylization and neural network learning method",
   "original": "i96_1381",
   "page_count": 4,
   "order": 355,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "In this paper, we propose a new model for synthesizing fundamental frequency (F0) contours using a stylization and a neural network learning method. The F0 contour is described as the superposition of 4 layered features; global tune, word pitch bias, lexical tone, and the syllabic pitch pattern. We firstly stylize the F0 contour of speech material, and analyze stylized data by statistical approach according to grammatical attributes. We then construct a melodic table, and train lexical tone with a neural network. Finally we develop the intonation generation rules for TTS conversion. This model produces a good neutral declarative intonation, and there is little difference between synthesized speech with original F0 contour and that with the rule generated contour when tested with our TD-PSOLA synthesizer[6][7].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-353"
  },
  "black96_icslp": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Andrew J.",
     "Hunt"
    ]
   ],
   "title": "Generating F0 contours from toBI labels using linear regression",
   "original": "i96_1385",
   "page_count": 4,
   "order": 356,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "This paper describes a method for generating F0 contours from ToBI labelled utterances. The method uses linear regression to predict F0 target values for the start, mid-vowel and end of every syllable, using features representing the ToBI labels, stress and syllable position. Contours generated by this method for an English database have a correlation of 0.62 and 34.8 Hz RMS error when compared with originals from test data. These results are significant improvements on a previous rule driven method (0.40 and 44.7), and the new method contours are preferred by human listeners. The technique has also been successfully applied to Japanese ToBI with similar improvements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-354"
  },
  "wang96e_icslp": {
   "authors": [
    [
     "Wern-Jun",
     "Wang"
    ],
    [
     "Shaw-Hwa",
     "Hwang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "The broad study of homograph disambiguity for Mandarin speech synthesis",
   "original": "i96_1389",
   "page_count": 4,
   "order": 357,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "How to increase the intelligibility and naturalness of synthetic speech have drawn much attentions in the recent Mandarin text-to-speech(TTS) researches. They have always been treated as bottleneck due to their effects are explicit for human perception. However, as qualities of synthetic speech increase for syllables, words or phrase, there is also an increasing need to improve the various components of the text processing. One of these desired improvements for Mandarin speech synthesis is the accuracy of character-to-sound(CTS) process. From the viewpoint of application, the purpose of speech synthesis should be aimed at making the synthetic speech understandable by human and minimize the misunderstanding between them. It thus is very important to increase the accuracy of CTS process. Such process is designed to predict phonetic pronunciations from a coarse surface text input and the difficulty mainly result from ambiguous homograph characters. In this paper, we proposed some effective analysis method incorporated with linguistic knowledge to resolve homograph ambiguity. The methods we used in the following experiments are discriminating lexical association and tree-based language model. From the experiment results, we can get about 10% more improvement on the average accuracy rate than traditional maximum frequency guess approach for most ambiguous homograph character.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-355"
  },
  "dutoit96_icslp": {
   "authors": [
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Vincent",
     "Pagel"
    ],
    [
     "N.",
     "Pierret"
    ],
    [
     "F.",
     "Bataille"
    ],
    [
     "O. Van der",
     "Vrecken"
    ]
   ],
   "title": "The MBROLA project: towards a set of high quality speech synthesizers free of use for non commercial purposes",
   "original": "i96_1393",
   "page_count": 4,
   "order": 358,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "The aim of the MBROLA project, recently initiated by the Faculte Polytechnique de Mons (Belgium), is to obtain a set of speech synthesizers for as many voices, languages and dialects as possible, free of use for non-commercial and non-military applications. The ultimate goal is to boost up academic research on speech synthesis, and particularly on prosody generation, known as one of the biggest challenges taken up by Text-to-Speech synthesizers for the years to come. Central to the MBROLA project is MBROLA 2.00, a speech synthesizer based on the concatenation of diphones. Executable files of this synthesizer have been made freely available for many computers/ operating systems, as well as a first diphone database for a French male voice. We describe here the terms of participation to the project, as a user, as an associated developer, or as a database provider.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-356"
  },
  "hashimoto96_icslp": {
   "authors": [
    [
     "Makoto",
     "Hashimoto"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Training data selection for voice conversion using speaker selection and vector field smoothing",
   "original": "i96_1397",
   "page_count": 4,
   "order": 359,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "We have previously proposed a spectral mapping method (SSVFS), for the purpose of voice conversion with a small amount of training data using speaker selection and vector field smoothing techniques. It has already been shown that SSVFS is effective for spectral mapping by both objective and subjective evaluations, and that it can operate with a very small amount of training data - as little as only one word [1]. This paper proposes a criterion for selecting effective training data for SSVFS. We defined coverage of parameter space with respect to training procedure of SSVFS as the criterion. This criterion is useful not only for the selection of effective training samples, which is important for the efficient learning of spectral characteristics, but also for the estimation of the degree to which learning is carried out. To evaluate the validity of the proposed criterion, we measured the correlation between spectral resemblance and coverage. The result showed that the mean correlation coefficient for eight target speakers is -0.74 with the proposed criterion, and -0.59 without consideration of the training procedure. We conclude that the proposed criterion is useful in selecting effective training samples for SSVFS.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-357"
  },
  "lee96f_icslp": {
   "authors": [
    [
     "Ki Seung",
     "Lee"
    ],
    [
     "Dae Hee",
     "Youn"
    ],
    [
     "Il Whan",
     "Cha"
    ]
   ],
   "title": "A new voice transformation method based on both linear and nonlinear prediction analysis",
   "original": "i96_1401",
   "page_count": 4,
   "order": 360,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "In this paper, we describe a voice transformation method which changes source speaker's acoustic features to those of a target speaker. The method developed here, acoustic features are divided into two parts, linear and nonlinear parts. Linear parts are characterized by LPC cepstrum coefficients which are obtained from LP analysis. As for nonlinear part, which represent the excitation signal, is modelled by the long-delay nonlinear predictor using a neural net. Conversion rules for excitation signal are generated by the average pitch ratio and the mapping codebook, and those for LPC cepstrum coefficients are based on the orthogonal vetctor space conversion. In addition, the spectral envelope compensation is proposed to correct spectral distortion in the transformed speech. A listening test shows that the proposed method makes it possible to convert speaker's individuality while maintaining high quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-358"
  },
  "baudoin96_icslp": {
   "authors": [
    [
     "G.",
     "Baudoin"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "On the transformation of the speech spectrum for voice conversion",
   "original": "i96_1405",
   "page_count": 4,
   "order": 361,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "In many speech applications, control of the speech individuality is required. These applications include the personalization of the voice of speech synthesizers, the restoral of voice individuality for interpreting telephony, the improvment of abnormal speech intelligibility. It is generally admitted that both prosodic and spectral parameters have to be changed in order to modify the speech individuality. Several algorithms have recently been proposed for the spectrum control. This paper presents some improvments added to these previously proposed methods and compares 4 approaches in the same common framework of voice conversion for application to text to speech synthesizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-359"
  },
  "delogu96_icslp": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea",
     "Paoloni"
    ],
    [
     "Susanna",
     "Ragazzini"
    ],
    [
     "Paola",
     "Ridolfi"
    ]
   ],
   "title": "Spectral analysis of synthetic speech and natural speech with noise over the telephone line",
   "original": "i96_1409",
   "page_count": 4,
   "order": 362,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "In order to explain the different performances obtained with natural and synthetic speech at different linguistic levels over the telephone line, we analyzed the data collected in an experiment where 108 randomized stimuli were presented to 96 subjects. Subjects were required to identify the consonant in 51 CV and 57 VCV meaningful or meaningless words. There were 20 different listening conditions: 6 TTS systems (3 formant-based (SF) and 3 diphone-based (SD)), a pure natural voice (NV) and 3 signal-to-noise (S/N) ratios (6, 0, and -6 dB) for a total of 10 systems, presented both in good and in telephone condition. The comparison between consonant confusions occurred for natural and synthetic speech with comparable overall levels of intelligibility performance showed that the distributions of the consonant confusions for natural and synthetic speech were often quite different in each condition. Some analyses of different spectrograms suggests that such confusions are due to some problems in the phonetic rules and to the telephone line.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-360"
  },
  "zhu96_icslp": {
   "authors": [
    [
     "Weizhong",
     "Zhu"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "A new speech synthesis system based on the ARX speech production model",
   "original": "i96_1413",
   "page_count": 4,
   "order": 363,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "In this paper, we present a new formant-type speech analysis-synthesis system based on the ARX (Auto-Regressive with Exogenous Input) speech production model. The model consists of cascade formant-antiformant synthesizers driven by a voicing source and an unvoiced turbulent noise source. One of the key features of the proposed method is that we have an algorithm to automatically measure the voicing source, unvoiced source and formant-antiformant parameters of the synthesizer directly from natural speech waveforms. After having automatically obtained estimates of the parameters from natural speech, one can manipulate the estimates using a flexible editing tool that has been developed as a part of the system. By changing values of the fundamental frequency, glottal open quotient, spectral tilt parameter, turbulent noise level, formant-antiformant frequencies and bandwidths, we can synthesize natural sounding speech with various voice qualities including modal, breathy, tense, and whisper voice. Acoustic correlates of these voice qualities could be systematically investigated using the proposed system. Since our analysis-editing-synthesis system has been developed on the MS-Windows platform, it is expected that it will be a useful tool in various basic areas of speech science and technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-361"
  },
  "campos96_icslp": {
   "authors": [
    [
     "Geraldo Lino de",
     "Campos"
    ],
    [
     "Evandro Bacci",
     "Gouvêa"
    ]
   ],
   "title": "Speech synthesis using the CELP algorithm",
   "original": "i96_1417",
   "page_count": 4,
   "order": 364,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "This paper presents a phoneme/diphone based speech synthesis system for the (Brazilian) Portuguese language. The basic idea bearing this system is the construction of a library of phonetic units, and processing of those basic units to build an utterance. The system is complemented by a text to phoneme translator described in [Cam95]. The phoneme's representation in the library is based on a linear prediction model; the filter which models the vocal tract is represented by Line Spectrum pairs, and the excitation by Code Excited Linear Prediction (CELP) parameters. Thus paper is organized as follows. After a brief introduction, CELP coding is briefly presented in part 2. Part 3 presents the relevant points to be applied in speech synthesis. Part 4 and 5 constitutes the main contribution of this paper, detailing the process of building the phoneme library and the interpolation techniques used. part 6 presents some concluding remarks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-362"
  },
  "hwang96_icslp": {
   "authors": [
    [
     "Shaw-Hwa",
     "Hwang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ],
    [
     "Yih-Ru",
     "Wang"
    ]
   ],
   "title": "A Mandarin text-to-speech system",
   "original": "i96_1421",
   "page_count": 4,
   "order": 365,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "In this paper, the implementation of a high-performance Mandarin TTS system is presented. The system is composed of four main parts: text analysis (TA), prosodic information generation (PIG), waveform table (WT) of 411 base-syllables, and PSOLA-based waveform synthesis (P-SOLA). In TA, a statistical model based method is first employed to automatically tag the input text to obtain the word sequence and the associated part-of-speech (POS) sequence. A lexicon containing about 80000 words is used in the tagging process. Then the corresponding base-syllable sequence is found and used to get from WT the basic wave-form sequence. Some linguistic features used in PIG are also extracted in TA. In PIG, a four-layer recurrent neural network (RNN) is employed to generate some prosodic information including pitch contour, energy level, initial duration and final duration of syllable as well as inter-syllable pause duration. Finally, in PSOLA the basic waveform sequence is modified using the prosodic information to generate output synthetic speech. The whole system is implemented by software on a PC/AT 486 with a 16-bit Sound Blaster add-on card. Only 3.2 Mbyte memory space is required. It can synthesize speech in real-time for any input Chinese text. Informal listening tests by many native Chinese living in Taiwan confirmed that the synthetic speech sounded very fluent and natural.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-363"
  },
  "edgington96_icslp": {
   "authors": [
    [
     "Mike D.",
     "Edgington"
    ],
    [
     "A.",
     "Lowry"
    ]
   ],
   "title": "Residual-based speech modification algorithms for text-to-speech synthesis",
   "original": "i96_1425",
   "page_count": 4,
   "order": 366,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "This paper presents a set of novel algorithms for the signal modification component of concatenative text-to-speech systems. The algorithms described here are based around the LPC analysis/synthesis framework, and achieve prosodic modification by time-domain processing of the LPC residual. The modified residual is then recombined with the all-pole spectral estimate to synthesise the new speech signal. The methods differ in the processing applied to the residual signal. The first method uses a modified version of TD-PSOLA, relying on assumptions of decorrelation and spectral flatness to avoid spectral distortion. The second method uses multiple windowing within each pitch period, enabling a given pitch modification to be realised by shifting several windowed segments by small amounts rather than a large shift of a single window. Again the aim is to reduce phase distortion introduced by the time-shifting process. The third method is based on a smoothly varying resampling of the residual, rather than windowed overlap-add. TD-PSOLA and the residual-based methods were subject to informal listening tests both with pitch and time-scaled natural speech, and also integrated into the signal processing stage of the BT Laureate text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-364"
  },
  "heggtveit96_icslp": {
   "authors": [
    [
     "Per Olav",
     "Heggtveit"
    ]
   ],
   "title": "A generalized LR parser for text-to-speech synthesis",
   "original": "i96_1429",
   "page_count": 4,
   "order": 367,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "The development of a parser for a Norwegian text-to-speech system is reported. The Generalized Left Right (GLR) algorithm [1] is applied, which is a generalization of the well known LR algorithm for parsing computer languages. This paper describes briefly the GLR algorithm, the integration of a probabilistic scoring model, our implementation of the parser in C++, attribute structures, lexical interface, and the application of the parser to part-of-speech (POS) tagging for Norwegian. Applied to a small test set of about 4 000 words this method correctly tags 96 % of the known words, which is close to the performance of other POS-taggers trained on large text databases [2] [3]. 85 % of the unknown words are tagged correctly, and the probability of choosing the wrong pronunciation of a word from lexicon is less than 0.1 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-365"
  },
  "pollard96_icslp": {
   "authors": [
    [
     "M. P.",
     "Pollard"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "C. C.",
     "Goodyear"
    ],
    [
     "Mike D.",
     "Edgington"
    ],
    [
     "A.",
     "Lowry"
    ]
   ],
   "title": "Enhanced shape-invariant pitch and time-scale modification for concatenative speech synthesis",
   "original": "i96_1433",
   "page_count": 4,
   "order": 368,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "To preserve shape-invariance when pitch or time-scale modifying sinusoidally modelled voiced speech, the phases of the sinusoids used to model the glottal excitation are made to add coherently at estimated excitation points. Previous methods achieve this by estimating excitation phases at synthesis frame boundaries, disregarding the frequency modulation that may occur between the frame boundary and the nearest modified excitation point. This approximation can produce a significant mis-alignment of the excitation phases, leading to distortion of the temporal structure of the synthetic speech. In this paper, a shape-invariant technique is proposed which aligns the excitation phases at excitation points, whilst allowing for variations in the frequency of the sinusoidal components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-366"
  },
  "arai96_icslp": {
   "authors": [
    [
     "Yasuhiko",
     "Arai"
    ],
    [
     "Ryo",
     "Mochizuki"
    ],
    [
     "Hirofumi",
     "Nishimura"
    ],
    [
     "Takashi",
     "Honda"
    ]
   ],
   "title": "An excitation synchronous pitch waveform extraction method and its application to the VCV-concatenation synthesis of Japanese spoken words",
   "original": "i96_1437",
   "page_count": 4,
   "order": 369,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "A novel pitch waveform extraction method has been proposed. Being different from the conventional pitch mark decision algorithm such as the peak search method, this new algorithm decides excitation points based on the Phase Equalized Residual Excited Linear Prediction (PE-RELP) model. A pitch waveform is extracted from two adjacent excitation intervals by using the asymmetrical Hanning window. The new pitch waveform extraction method takes advantages of being free from the extraction errors caused by the formant resonance and being fully automatic. Therefore, no manipulation is required and no roughness is heard in the pitch modified speech sound. The superiority of the new method has been ensured by means of the spectral distortion measurement and the subjective quality evaluation. Finally, the spoken word generation by means of the VCV-waveform concatenation is demonstrated. Consequently, it has been shown that the generation of very natural sounding spoken words is possible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-367"
  },
  "wang96f_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Qinfeng",
     "Liu"
    ],
    [
     "Difei",
     "Tang"
    ]
   ],
   "title": "A new Chinese text-to-speech system with high naturalness",
   "original": "i96_1441",
   "page_count": 4,
   "order": 370,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "This paper introduces a new Chinese text-to-speech system that produces far more natural and intelligible synthesized speech than existing systems. There are two distinguishing features in this system. One is the perfect prosodic rules that were summed up from the linguistic knowledge and statistical results made on a standard Chinese database. These rules are successfully used to modify the elemental synthesis units to get high naturalness while concatenate them to a sentence. The other feature is that, the Log Magnitude Approximate<LMA) filter is used as the synthesis filter in the introduced system. With the LMA filter, the prosody of the synthesized speech can be modified at a wide range while maintain high intelligence and naturalness, hi this paper the formulated prosodic rules are presented, and the LMA filter based speech synthesis is described in details.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-368"
  },
  "rinscheid96_icslp": {
   "authors": [
    [
     "Ansgar",
     "Rinscheid"
    ]
   ],
   "title": "Voice conversion based on topological feature maps and time-variant filtering",
   "original": "i96_1445",
   "page_count": 4,
   "order": 371,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "This paper presents a new voice conversion algorithm. This algorithm allows voices to be adapted using a small amount of adaptation data. Only a few short adaptation units (phonemes or short words) are needed. The voice conversion is performed using a time-variant digital filter, topological feature maps and a map of filter coefficients. The filter coefficients of the time-variant filter are selected by the feature map dependent on the short-time spectrum. The spectral envelope of the input signal is modified by a time-variant filter using the selected coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-369"
  },
  "yoram96_icslp": {
   "authors": [
    [
     "Meron",
     "Yoram"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Language training system utilizing speech modification",
   "original": "i96_1449",
   "page_count": 4,
   "order": 372,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "In this paper, a computer assisted language training system, focusing on speech input and output, is described. The system is intended to help students of foreign language (typically Japanese or English) to improve their pronunciation, with an emphasis on prosodic features of speech. The system incorporates a combination of speech processing techniques, in order to analyze the input speech, and to produce effective speech feedback. The system is implemented on a Unix PC, with audio I/O capability, in a window environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-370"
  },
  "jamieson96b_icslp": {
   "authors": [
    [
     "D. G.",
     "Jamieson"
    ],
    [
     "K.",
     "Yu"
    ]
   ],
   "title": "Perception of English /r/ and /l/ speech contrasts by native Korean listeners with extensive English-language experience",
   "original": "i96_1453",
   "page_count": 4,
   "order": 373,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "Native speakers of one language often have difficulty perceiving and producing the sounds of other languages correctly. For example, native speakers of Korean often have difficulty perceiving the English/r/-/l/ sounds [3,12,17]. While English has two separate labels for the sounds we call hi and /I/, the Korean language groups these sounds into a single category [3]. In Korean, the sound heard by English-speaking listeners as hi occurs only intervocalically; that heard as /!/ occurs only in word-final position [13]. Particular problems may arise when communicating against a background of noise. Non-native listeners may display greater difficulty than native listeners under difficult listening conditions [16,20], Notwithstanding these factors, it is now clear that laboratory training can quickly and substantially improve listeners' perceptions of non-native speech contrasts [9.10,14,21]. In the case /t/-/l/ identification, Logan et al. [14] demonstrated that training with natural English tokens improved M-I\\l identification for native speakers of Japanese. In view of the foregoing, we sought to examine the degree to which very extensive exposure to the English language as an adult, through many years of living in a predominantly English-language environment, affects the perception of \"difficult\" English speech contrasts such as M-N. The present study was therefore designed to study aspects of the speech perception abilities of adults who were native speakers of Korean but who had lived for many years in an English-speaking environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-371"
  },
  "neumeyer96_icslp": {
   "authors": [
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Mitchel",
     "Weintraub"
    ],
    [
     "Patti",
     "Price"
    ]
   ],
   "title": "Automatic text-independent pronunciation scoring of foreign language student speech",
   "original": "i96_1457",
   "page_count": 4,
   "order": 374,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "SRI International is currently involved in the development of a new generation of software systems for automatic scoring of pronunciation as part of the Voice Interactive Language Training System (VILTS) project. This paper describes the goals of the VILTS system, the speech corpus, and the algorithm development. The automatic grading system uses SRIs Decipher (TM) continuous speech recognition system [1] to generate phonetic segmentations that are used to produce pronunciation scores at the end of each lesson. The scores produced by the system are similar to those of expert human listeners. Unlike previous approaches in which models were built for specific sentences or phrases, we present a new family of algorithms designed to perform well even when knowledge of the exact text to be used is not available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-372"
  },
  "simoes96_icslp": {
   "authors": [
    [
     "Antônio",
     "Simoes"
    ]
   ],
   "title": "Assessing the contribution of instructional technology in the teaching of pronunciation",
   "original": "i96_1461",
   "page_count": 4,
   "order": 375,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "This investigation discusses the implementation of computer-based teaching of Spanish pronunciation, especially intonation, in the classroom. It examines the potential and the actual use of a technologically-based system, and the results of such a system in the improvement of students' pronunciation in one semester of instruction. The effects of instructional technology were studied through recordings of students made at the beginning and end of a semester. Their recordings were evaluated by native speakers of Spanish in two ways. First, native speakers evaluated the overall pronunciation of each recording according to a scale from 0-8, using 8 as the equivalent of native speaker pronunciation. Then, the native speakers were asked to give their intuitive evaluation of the language musicality of the same recordings. In this second task, native speakers were asked to decide if the recordings of second language learners sounded: (0) not like singing, (1) a little like singing, (2) much like singing, and (3) so much singing that it was annoying. Quantitative analysis of the overall results is still undetermined, despite possible improvement in nearly 40-50% of participating students. More promising and revealing, however, is the qualitative analysis of the data, and the experiment itself, all of which reveal significant information in favor of the present technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-373"
  },
  "eskenazi96_icslp": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Detection of foreign speakers' pronunciation errors for second language training - preliminary results",
   "original": "i96_1465",
   "page_count": 4,
   "order": 376,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "With the present generation of speech recognizers, dealing with speaker-independent continuous speech and medium-sized vocabularies, the possibilities of applications become larger. Yet some applications have not yet been tried, or have been tried with heavy constraints on the user, due to expected poor recognition performance. And the lack of results to date in the domain of prosody has severely limited use of that information. Researchers may be overly pessimistic. Herein we explore the possibility of using CMUs SPHINX II recognizer and of obtaining correct prosody information in order to implement it in a system to aid in foreign language learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-374"
  },
  "mixdorff96_icslp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ]
   ],
   "title": "Foreign accent in intonation patterns - a contrastive study applying a quantitative model of the F0 contour",
   "original": "i96_1469",
   "page_count": 4,
   "order": 377,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "The present study examines the influence of intonational contrasts of German and Japanese on the F0 patterns of Japanese learners of German. It was found that Japanese learners generally place the word accent in compound words on the right-most (the generic) term which suggests that they adopt rules of accent concatenation of Japanese. In contrast to native speakers of German, they tend to place substantially more falling accents at utterance-medial phrase boundaries. The intonational patterns for marking questions are different from those found in utterances of German speakers, where generally only yes-/no-questions feature a question final rise. Prosodic phrases in the Japanese group are generally shorter and are often not determined by the linguistic content of an utterance but by the occurrence of phonetically 'difficult' or possibly unknown words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-375"
  },
  "markham96_icslp": {
   "authors": [
    [
     "Duncan J.",
     "Markham"
    ],
    [
     "Yasuko",
     "Nagano-Madsen"
    ]
   ],
   "title": "Input modality effects in foreign accent",
   "original": "i96_1473",
   "page_count": 4,
   "order": 378,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "A group of higher than average ability learners of the phonetic characteristics of a foreign language attempted to imitate stimuli from Japanese in tasks involving long and complex phrases and shorter minimal pair contrasts. Three information modalities were used (aural, visual, orthographic), introduced progressively, in order to test the subjects' use of different input sources. Their performance is described and the conclusion is drawn that these speakers function largely within an aural modality, but do assimilate external, primarily visual, information, as reflected in rapid improvement in the rhythmic and timing characteristics of the imitations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-376"
  },
  "bernstein96_icslp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "For speech perception by humans or machines, three senses are better than one",
   "original": "i96_1477",
   "page_count": 4,
   "order": 379,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "A growing assemblage of researchers has, in recent years, adopted methods and theories that acknowledge and exploit the multisensory nature of speech perception. This paper, which is an introduction to the special session, \"The Senses of Speech Perception,\" gives a brief historical review of research concerning the multiple senses of speech perception, discusses major issues, and suggests directions for future research. (Work supported in part by NIH grant DC00695.)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-377"
  },
  "sekiyama96_icslp": {
   "authors": [
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Yoh'ichi",
     "Tohkura"
    ],
    [
     "Michio",
     "Umeda"
    ]
   ],
   "title": "A few factors which affect the degree of incorporating lip-read information into speech perception",
   "original": "i96_1481",
   "page_count": 4,
   "order": 380,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "This paper describes how people incorporate visual lip-read information into speech perception, depending on one's native language/culture and experience of learning a second language. Studies on lipreading show that humans can easily make a distinction between labial consonants and nonlabial ones. Then we investigated how people integrate auditory and visual speech information, by using the \"McGurk effect\" paradigm in which labial-nonlabial conflict is introduced. Cross-language examinations were done across Japanese, American English, and Chinese. The Japanese and Chinese subjects were less susceptible to the McGurk effect than the American subjects, indicating a cultural/linguistic factor. The results for the Chinese subjects showed a correlation between the magnitude of the McGurk effect and the length of time they lived in a foreign country (Japan), suggesting a change due to second language learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-378"
  },
  "vatikiotisbateson96_icslp": {
   "authors": [
    [
     "E.",
     "Vatikiotis-Bateson"
    ],
    [
     "K. G.",
     "Munhall"
    ],
    [
     "Y.",
     "Kasahara"
    ],
    [
     "F.",
     "Garcia"
    ],
    [
     "H.",
     "Yehia"
    ]
   ],
   "title": "Characterizing audiovisual information during speech",
   "original": "i96_1485",
   "page_count": 4,
   "order": 381,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "In this paper, several analyses relating facial motion with perioral muscle behavior and speech acoustics are described. The results suggest that linguistically relevant visual information is distributed over large regions of the face and can be modeled from the same control source as the acoustics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-379"
  },
  "reed96_icslp": {
   "authors": [
    [
     "Charlotte M.",
     "Reed"
    ]
   ],
   "title": "The implications of the tadoma method of speechreading for spoken language processing",
   "original": "i96_1489",
   "page_count": 8,
   "order": 382,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "Strong support of the capacity of touch as a communicative sense is provided by the Tadoma method of communication. Through this method, individuals who are deaf-blind have been able to acquire a full range of spoken language abilities. In the Tadoma method, direct contact is made between the hand of the deaf-blind receiver and the face of a talker to monitor the various articulatory actions that occur during speech. Studies conducted with a group of experienced deaf-blind practitioners of Tadoma have documented their abilities for speech reception, speech production, and linguistic competence. The results of this research indicate that individuals who suffered deaf-blindness in early childhood (e.g., around 18 months of age) can understand speech produced at slow-to-normal rates with reasonable accuracy, can produce speech that is reasonably intelligible to many listeners, and have an extensive command of English that compares favorably in many areas to that of hearing individuals. The performance of these deaf-blind individuals implies the adequacy of the tactual sense to support the development of speech and language and thereby provides a strong impetus for continued research on the development of sensory-substitution devices for spoken language processing. Current efforts on the development and evaluation of artificial tactile devices for speech communication will be discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-380"
  },
  "campbell96b_icslp": {
   "authors": [
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Seeing speech in space and time: psychological and neurological findings",
   "original": "i96_1493",
   "page_count": 4,
   "order": 383,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "Unlike heard speech, some aspects of seen speech can be processed independently of their dynamic (temporal) characteristics. The extent to which speechreading may be supported, separately, by visual mechanisms for the analysis of still and moving events is considered in relation to some experimental findings with normal speaker/viewers, to results with neurological patients with circumscribed lesions to parts of the visual system that support the perception of movement, and to cortical imaging studies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-381"
  },
  "green96_icslp": {
   "authors": [
    [
     "Kerry P.",
     "Green"
    ]
   ],
   "title": "Studies of the mcgurk effect: implications for theories of speech perception",
   "original": "i96_1652",
   "page_count": 4,
   "order": 384,
   "p1": "1652",
   "pn": "1655",
   "abstract": [
    "Studies of the McGurk effect demonstrate that observers integrate auditory information with visual information from a talkers face during speech perception. The findings from these studies pose challenges for theories of speech perception that must account for how and why the auditory and visual information are integrated. One theoretical issue concerns the objects of speech perception. Some researchers claim that the objects of speech perception are articulatory gestures while others argue that the objects are auditory in nature. The McGurk effect is often taken as evidence for gestural approaches because such theories provide a good account for why the auditory and visual information are integrated during perception. The findings from various studies of the McGurk effect including cross-modal context effects, developmental influences, and neuromagnetic measures of brain activation will be reviewed. The implication of these findings will be discussed with regard to whether the metric for combining the auditory and visual information is best thought of as auditory or gestural in nature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-382"
  },
  "brooke96_icslp": {
   "authors": [
    [
     "N. M.",
     "Brooke"
    ]
   ],
   "title": "Using the visual component in automatic speech recognition",
   "original": "i96_1656",
   "page_count": 4,
   "order": 385,
   "p1": "1656",
   "pn": "1659",
   "abstract": [
    "The movements of talkers faces are known to convey visual cues that can improve speech intelligibility, especially where there is noise or hearing-impairment. This suggests that visible facial gestures could be exploited to enhance speech intelligibility in automatic systems. Handling the volume of data represented by images of talkers faces implies some form of data compression. Rather than using conventional feature extraction approaches, image coding and compression can be achieved using data-driven, statistically-oriented techniques such as artificial neural-networks (ANNs) or principal component analysis (PCA). A major issue is the combination of the audio and visual data so that the best use can be made of the two modalities together. Perceptual experiments may offer guidance on suitable machine architectures, many of which currently use hidden Markov models (HMMs).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-383"
  },
  "remez96_icslp": {
   "authors": [
    [
     "Robert E.",
     "Remez"
    ]
   ],
   "title": "Perceptual organization of speech in one and several modalities: common functions, common resources",
   "original": "i96_1660",
   "page_count": 4,
   "order": 386,
   "p1": "1660",
   "pn": "1663",
   "abstract": [
    "In order to understand speech the perceiver meets two challenges: 1) to Þnd a speech signal within ongoing sensory activity, and 2) to project its properties into linguistic phonetic attributes. These functions have customarily been designated as perceptual organization and perceptual analysis. The case of multimodal perceptual organization is revealing to consider because the perceiver Þnds sensory ingredients spanning modalities. Contemporary accounts offer alternative conceptualizations of these functions based largely on the study of single modalities. A Gestalt-derived account hypothesizes that perceptual organization precedes analysis, grouping sensory elements into perceptual streams by a variety of similarity criteria. An account deriving from probabilistic functionalism describes analysis occurring within modalities preceding a stage of organization that binds the derived features. These alternatives and their hybrids appear implausible on empirical and theoretical grounds for accommodating multimodal perceptual organization. Additionally, our studies using sinewave replicas of utterances reveal that the customary models are untenable accounts of unimodal no less than multimodal perceptual organization. A third way, justiÞed by our results, describes auditory perceptual organization of sinewave sentences as a speciÞc instance of the general susceptibility to coherent sensory variation. This account potentially allows a single description of uni- and multimodal perceptual organization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-384"
  },
  "pisoni96_icslp": {
   "authors": [
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Helena M.",
     "Saldaña"
    ],
    [
     "Sonya M.",
     "Sheffert"
    ]
   ],
   "title": "Multi-modal encoding of speech in memory: a first report",
   "original": "i96_1664",
   "page_count": 4,
   "order": 387,
   "p1": "1664",
   "pn": "1667",
   "abstract": [
    "Why do people like to watch videos on TV? Why is there now increased interest in video telephones and multi-media technologies that were developed back in the 1960s? Obviously, the availability of new digital technology has played an enormous role in this transition. But, we also believe this is in part due to the same operating principle that encourages listeners in noisy environments to orient toward a talkers face. A multi-modal speech signal is extremely robust and informative and provides information that perceivers are able to exploit during perceptual analysis. In this paper, we present results from two experiments that examined performance in immediate memory and serial recall tasks with normal-hearing listeners using unimodal (auditory-only) and multi-modal (auditory+visual) presentation. Our findings suggest that the addition of visual information in the stimulus display about the speakers articulation affects the efficiency of initial encoding operations at the time of perception and also results in more detailed and robust representations of the stimulus events in memory. These results have implications for current theories of speech perception and spoken language processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-385"
  },
  "strom96b_icslp": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Christina",
     "Widera"
    ]
   ],
   "title": "What's in the \"pure\" prosody?",
   "original": "i96_1497",
   "page_count": 4,
   "order": 388,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "Detectors for accents and phrase boundaries have been developed which derive prosodic features from the speech signal and its fundamental frequency to support other modules of a speech understanding system in an early analysis stage, or in cases where no word hypotheses are available. The detectors' underlying Gaussian distribution classifiers were trained with 50 minutes and tested with 30 minutes of spontaneous speech, yielding recognition rates of 74% for accents and 86% for phrase boundaries. Since this material was prosodically hand labelled, the question was, which labels for phrase boundaries and accentuation were only guided by syntactic or semantic knowledge, and which ones are really prosodically marked. Therefore a small test subset has been resynthesized in such a way that comprehensibility was lost, but the prosodic characteristics were kept. This subset has been re-labelled by 11 listeners with nearly the same accuracy as the detectors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-386"
  },
  "swerts96b_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Eva",
     "Strangert"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "F0 declination in read-aloud and spontaneous speech",
   "original": "i96_1501",
   "page_count": 4,
   "order": 389,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "This paper deals with a prosodic comparison of spontaneous and read-aloud speech. More specifically, the study reports data on F0 declination in these two speaking modes using Swedish materials. For both speaking styles the analysis revealed negative slopes, a steepness-duration dependency with declination being less steep in longer utterances than in shorter ones and resetting at utterance boundaries. However, there was a difference in degree of declination between the two speaking styles, read-aloud speech in general having steeper slopes, a more apparent time-dependency and stronger resetting than spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-387"
  },
  "kim96_icslp": {
   "authors": [
    [
     "Yeon-jun",
     "Kim"
    ],
    [
     "Yung-hwan",
     "Oh"
    ]
   ],
   "title": "Prediction of prosodic phrase boundaries considering variable speaking rate",
   "original": "i96_1505",
   "page_count": 4,
   "order": 390,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "This paper proposes a model for predicting the prosodic phrase boundaries of speech with variable speaking rates. Speakers can produce a sentence in several ways without altering its meaning or naturalness, i.e., a sequence of words can have a number of prosodic phrase boundaries. There are many factors which influence the variability of prosodic phrasing, such as syntactic structure, focus, speaker differences, speaking rate and the need to breathe. In this work, we adopt dependency grammar, similar to link grammar, to efficiently combine speaking rates. The proposed model reduced prosodic phrase boundary prediction error by 20% compared the model using only syntactic informations. We show a potential way to make use of a read speech corpus in the training of prosodic phrasing for spontaneous speech. The proposed model is expected to make synthesized speech more natural, and improve the robustness of spontaneous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-388"
  },
  "yamashita96_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Prediction of F0 parameter of contextualized utterances in dialogue",
   "original": "i96_1509",
   "page_count": 4,
   "order": 391,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "In order to synthesize natural spoken dialogue, it is necessary to incorporate dialogue information into generation of the surface sentence and the prosody. This paper describes the prediction of F0 maximum for minor phrases in dialogue based on a two-step predictive method. Special attentions are directed to specific phrases containing the person's name or the day of the week in the schedule arrangement task in order to narrow the diversity of characteristics of F0 parameters in dialogue. Seven features were identified as dialogue information which are useful to predict the F0 parameter. Two D-rule sets derived from the person's name or the day of the week are very similar to one another. They reduce the total prediction errors by about 50% for the data which have much influence of dialogue context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-389"
  },
  "makarova96_icslp": {
   "authors": [
    [
     "V.",
     "Makarova"
    ],
    [
     "J.",
     "Matsui"
    ]
   ],
   "title": "The production and perception of potentially ambiguous intonation contours by speakers of Russian and Japanese",
   "original": "i96_1513",
   "page_count": 4,
   "order": 392,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "The present study deals with the production and perception of short Russian and Japanese declaratives, interrogatives and exclamations with similar segments and rise-fall or falling intonation contours. Results indicate that intonation can successfully function as the only means of sentence type disambiguation even for short phrases with similar (potentially ambiguous) contours for Russian listeners, whereas interrogative and exclamatory contours remain ambiguous for the Japanese. Factors accounting for sentence type misidentification include differences between Russian and Japanese intonation contour parameters such as peak timings, end timings, pitch slope, and pitch range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-390"
  },
  "eklund96_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "What is invariant and what is optional in the realization of a FOCUSED word? a cross-dialectal study of Swedish sentences with moving focus",
   "original": "i96_1517",
   "page_count": 4,
   "order": 393,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "State-of-the-art speech recognition systems handle continuous speech and are speaker-independent. However, the linguistic information conveyed in the intonational contour is neglected. To be able to fully recognize speech, this information must be interpreted. To this end, explicit knowledge of dialectal and individual variation is required. In this paper some acoustic correlates of wh-focus in three Swedish dialects are described. Variation within and between dialects is accounted for, as well as individual differences and optional phenomena.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-391"
  },
  "shadle96_icslp": {
   "authors": [
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "Sheila J.",
     "Mair"
    ]
   ],
   "title": "Quantifying spectral characteristics of fricatives",
   "original": "i96_1521",
   "page_count": 4,
   "order": 394,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "In a search for spectral parameters that can be used to distinguish and to model fricatives, spectral moments, dynamic amplitude, and slope above maximum amplitude were computed for a fricative corpus including sustained fricatives at different effort levels, and fricatives in vowel context. Moments varied significantly by frequency range used in computation. M3 appeared to vary the least across fricative, contrasting with Forrest et al.'s 1988 study. Dynamic amplitude separated sibilants and non-sibilants, as predicted; slope above the maximum amplitude varied significantly with effort level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-392"
  },
  "warner96_icslp": {
   "authors": [
    [
     "Natasha",
     "Warner"
    ]
   ],
   "title": "Acoustic characteristics of ejectives in ingush",
   "original": "i96_1525",
   "page_count": 4,
   "order": 395,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "The purpose of this paper is to investigate acoustic characteristics which distinguish ejectives from pulmonic stops in Ingush (a Northeast Caucasian language), and to compare Ingush ejectives to those of other languages. The articulation of ejectives is relatively well understood, but their acoustic effects are less clear. Working with a native speaker of Ingush, ejectives and pulmonic voiceless stops were compared for VOT, closure duration, and post-burst power; and pitch, amplitude, and voice quality of the following vowel. Ingush results are compared to published descriptions of ejectives in several other languages. Ingush ejectives do not have all the same acoustic features as any other language studied. The characteristics of ejectives vary with each language, and do not pattern together to form just two types of ejectives, as has been claimed based on binary comparisons.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-393"
  },
  "son96_icslp": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "An acoustic profile of consonant reduction",
   "original": "i96_1529",
   "page_count": 4,
   "order": 396,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "Vowel reduction has been studied for years. It is a universal phenomenon that reduces the distinction of vowels in informal speech and unstressed syllables. How consonants behave in situations where vowels are reduced is much less well known. In this paper we compare durational and spectral data (for both intervocalic consonants and vowels) segmented from read speech with otherwise identical segments from spontaneous speech. On a global level, it shows that consonants reduce like vowels when the speaking style becomes informal. On a more detailed level there are differences related to the type of the consonant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-394"
  },
  "archambault96b_icslp": {
   "authors": [
    [
     "Danièle",
     "Archambault"
    ],
    [
     "Blagovesta",
     "Maneva"
    ]
   ],
   "title": "Devoicing in post-vocalic canadian-French obstruants",
   "original": "i96_1533",
   "page_count": 4,
   "order": 397,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "In Canadian French, besides periodic phonation, other cues can be associated with the voiced-voiceless distinction due to the application of phonological rules. These cues, mainly duration and vowel quality, may be present in the consonant itself (voiced consonants are shorter than their voiceless counterparts) and in the preceding vowels (duration and vowel quality). The cues are related to the application of an allophonic rule and the presence in the phoneme inventory of intrinsically long and short vowels. The tendancy towards devoicing of some portion of a normally voiced consonant in postvocalic word-final positions is found in many languages. This study investigates the patterns of devoicing in post-vocalic obtruants in Canadian French and attempts to verify the following functional hypothesis: a consonant will be more resistant to devoicing (absence of periodic structure) if no (or few) other cues of the voiced-voiceless distinction can be found either in the consonant itself or in the preceding vowel. The data will serve as a reference for current studies on patients with apraxia of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-395"
  },
  "francis96_icslp": {
   "authors": [
    [
     "Alexander L.",
     "Francis"
    ],
    [
     "Howard C.",
     "Nusbaum"
    ]
   ],
   "title": "Paying attention to speaking rate",
   "original": "i96_1537",
   "page_count": 4,
   "order": 398,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "Variability in speaking rate results in a many-to-many mapping between acoustic properties in speech and the linguistic interpretation of an utterance. In order to recognize the phonetic structure of an utterance, listeners must calibrate their phonetic decisions against the rate at which the speech was produced. This process of rate normalization is fast and effective allowing listeners to maintain phonetic constancy in spite of changes in speaking rate. Most of the research on rate normalization has investigated the sources of information used by listeners to determine the speaking rate. There is an assumption in much of this research that the normalization process is a passive, automatized filtering process that strips the effects of rate variation away from the signal prior to recognition. The present study starts from a different perspective by assuming that speech perception is carried out by an active perceptual process that is specifically needed to address the lack of invariance problem (Nusbaum & Henly, in press). This perspective predicts that increased variability from any source, including rate variability, should increase the cognitive load during speech perception. Our results support this prediction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-396"
  },
  "appelbaum96_icslp": {
   "authors": [
    [
     "Irene",
     "Appelbaum"
    ]
   ],
   "title": "The lack of invariance problem and the goal of speech perception",
   "original": "i96_1541",
   "page_count": 4,
   "order": 399,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "The overall goal of speech perception research is to explain how spoken language is recognized and understood. In the current research framework it is usually assumed that the key to achieving this overall goal is to solve the lack of invariance problem. But nearly half a century of sustained effort in a variety of theoretical perspectives has failed to solve this problem. It is argued that this lack of progress in explaining speech perception is not, in the first instance, due to the failure of individual theories to solve the lack of invariance problem, but rather to the common background assumption that doing so is in fact the key to explaining speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-397"
  },
  "andruski96_icslp": {
   "authors": [
    [
     "Jean E.",
     "Andruski"
    ],
    [
     "Patricia K.",
     "Kuhl"
    ]
   ],
   "title": "The acoustic structure of vowels in mothers' speech to infants and adults",
   "original": "i96_1545",
   "page_count": 4,
   "order": 400,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "Research has shown that exposure to a specific language alters infants' perception of vowel sounds by the time they reach 6 months of age. This raises the important question of how infants develop language-specific patterns of vowel perception from the language-general pattern they appear to be born with. Language spoken to infants may exert an important influence in this regard. The present study compares the acoustic structure of vowels in the words \"sheep\" and \"shoes\" produced by 10 mothers in conversation with their infants, with their acoustic structure when produced by the same 10 women in conversation with an adult. Mothers were instructed to play with their infants using a toy sheep and pair of shoes. They were asked to use the same words in conversation with an adult. The infant-directed tokens exhibited higher F0, greater pitch excursions and longer duration than the adult-directed tokens, as is typical of motherese. Although F0 was significantly higher in infant-directed vowels, F1 remained at essentially the same frequency as in adult-directed /i/ and /u/. In contrast, F2 was significantly higher in infant-directed /i/ and significantly lower in infant-directed /u/ than in the adult-directed tokens. Thus, the infant-directed tokens reached more extreme acoustic targets. The formant structure also indicated an equivalent or slightly greater degree of coarticulation of infant-directed /u/ with the preceding fricative in \"shoes.\" Overall, mothers in this study consistently hyperarticulated vowels in speech to infants. Hyperarticulation may contribute to the acquisition of native language vowel categories by increasing the degree of acoustic separation between vowel categories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-398"
  },
  "clement96_icslp": {
   "authors": [
    [
     "Chris J.",
     "Clement"
    ],
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Acoustical characteristics of sound production of deaf and normally hearing infants",
   "original": "i96_1549",
   "page_count": 4,
   "order": 401,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "Several recent studies have shown that speech production develops in an organized way, already in the first twelve months of life. This development is determined by several factors such as anatomical growth and physiological constraints. Studying the sound production of deaf infants and comparing this with that of normally hearing infants, can give more insight into the role of auditory speech perception on sound production. So far, no systematic work has been reported on the development of sound production of deaf infants in the first months of life. The present study is intended to address this topic in a systematic and controlled way. Preliminary results indicate differences in sound production between deaf and normally hearing infants, for instance with respect to utterance duration, even within the first half year of life. These findings strongly suggest that already in this early stage of speech development sound production is not solely determined by anatomical and physical constraints, but also by auditory perception and feedback. These results may contribute to a better understanding of the current models of early speech acquisition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-399"
  },
  "halle96_icslp": {
   "authors": [
    [
     "P. A.",
     "Halle"
    ],
    [
     "Toshisada",
     "Deguchi"
    ],
    [
     "Yuji",
     "Tamekawa"
    ],
    [
     "B.",
     "Boysson-Bardies"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Word recognition by Japanese infants",
   "original": "i96_1557",
   "page_count": 5,
   "order": 402,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "Building a lexicon is a necessary step in the process of language acquisition. The emergence and development of vocabulary have usually been observed in naturalistic settings through their external manifestations: the first attempts at producing words, and the various signs showing that an infant comprehends words. Naturalistic approaches, however, may underestimate the productive lexicon on one hand, and overestimate the receptive lexicon on the other hand. An experimental approach, using the Headturn Preference Procedure, has been used to show that 11-to-12-month-old French infants can recognize familiar words without specific training (Hallé and Boysson-Bardies, 1994). The spontaneous preference for familiar words, interpreted as word recognition, revealed the formation of an early receptive lexicon comprising a significant part of the familiar words that were used. At 12 months, recognition seemed to be firmly established, while it seemed to be just emerging at 11 months. Using the same procedure, the present study examined familiar word recognition in Japanese infants: Twelve-month-olds, but not 10-month-olds, showed a preference for familiar words, similar in intensity to that shown by 11-month-old French infants. These results are again interpreted as revealing the formation of a nascent receptive lexicon in Japanese infants by 12 months of age. Commonalties and differences between the two language groups are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-400"
  },
  "jusczyk96_icslp": {
   "authors": [
    [
     "Peter W.",
     "Jusczyk"
    ]
   ],
   "title": "Investigations of the word segmentation abilities of infants",
   "original": "i96_1561",
   "page_count": 4,
   "order": 403,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "Evidence that I presented at the last meeting in Yokohama indicated that English-learning infants first show some capacity for segmenting words from fluent speech at about 7.5 months of age. Further studies that we have conducted suggest that English-learning infants initially rely on a prosodically based strategy which may cause them to mis-segment words beginning with weak syllables. However, by 10.5 months, English-learners appear to draw on other potential sources of information to locate word boundaries, even for words beginning with weak syllables. Our studies suggest that infants do retain information about the sound patterns of words for up to 24 hours, and that they generalize across different talker's pronunciations of the same words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-401"
  },
  "hayashi96_icslp": {
   "authors": [
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Yuji",
     "Tamekawa"
    ],
    [
     "Toshisada",
     "Deguchi"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Developmental change in perception of clause boundaries by 6- and 10-month-old Japanese infants",
   "original": "i96_1565",
   "page_count": 4,
   "order": 404,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "Present study investigated whether Japanese infants are sensitive to acoustic cues of clausal units in Japanese speech. Groups of 6- and 10-month-old infants were tested using the headturn preference procedure (HPP). Two types of speech samples (child-directed speech) were examined: \"Coincident\" samples were created by inserting one- second pauses at all clause boundary locations, and \"Noncoincident\" samples were created by inserting the same number of pauses between words within clauses. Preferences were determined by assessing the listening times for each 12 samples (6 of each type). For the 6-month-old infants, there was no difference in the listening times between the two stimulus types. For the 10-month-old infants, however, the listening time for the \"Noncoincident\" samples was significantly longer than that for the \"Coincident\" samples. These results indicate that Japanese clauses are not perceptual units for very young infants, but Japanese infants come to be sensible to the clausal units up lo 10-month-old.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-402"
  },
  "alku96_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Erkki",
     "Vilkman"
    ]
   ],
   "title": "A frequency domain method for parametrization of the voice source",
   "original": "i96_1569",
   "page_count": 4,
   "order": 405,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "A new frequency domain method for the quantification of glottal volume velocity waveforms is presented in this study. This technique, called Parabolic Spectral Parameter (PSP), is based on fitting a parabolic function to a pitch-synchronously-computed spectrum of the estimated voice source. The PSP-algorithm gives a single numerical value that describes how the spectral decay of an obtained glottal flow behaves with respect to theoretical bounds corresponding to maximal and minimal spectral tilting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-403"
  },
  "marasek96_icslp": {
   "authors": [
    [
     "Krzysztof",
     "Marasek"
    ]
   ],
   "title": "Glottal correlates of the word stress and the tense/lax opposition in German",
   "original": "i96_1573",
   "page_count": 4,
   "order": 406,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "The influence of word stress and the tense/lax vowels opposition on the phonation process in German was investigated. A set of stimulus words in which tense and lax vowels occurs in both stressed and unstressed position was read by ten speakers of German. The phonation behaviour was investigated using the electroglottographic signal (EGG) recorded by the Laryngograph. The EGG signal was segmented and described using a set of timing and shape parameters. To compare the results with the literature the analysis concentrates on the Open and Speed Quotients. The ANOVA analysis shows that the main correlates of stress are the increase of fundamental frequency and the increase of closing and opening slopes steepness of the EGG signal. The Open and Speed Quotients depend primairly on other factors, like speakers sex or vowel type. Tenseness causes small changes in the Open Quotient, but the effect often interacts with stress.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-404"
  },
  "boyce96_icslp": {
   "authors": [
    [
     "Suzanne",
     "Boyce"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Coarticulatory stability in american English /r/",
   "original": "i96_1577",
   "page_count": 4,
   "order": 407,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "A number of different researchers have reported a substantial degree of variability in how American English /r/ coarticulates with neighboring segments. Acoustic and articulatory data were used to investigate this variability for speakers of \"rhotic\" American English dialects. The major issue addressed is the degree to which segmental context affects articulatory movement as reflected in the F3 trajectory. In particular, we ask whether the duration of the F3 trajectory is affected by conflicting vs. nonconflicting articulatory specifications. The F3 formant trajectory durations were measured by automatic procedure and compared for nonsense words of the form /'waCrav/ and /waC'rav/, where C indicates a labial, alveolar or velar consonant. These durations were compared to F3 trajectory durations in /'warav/ and /wa'rav/. Results indicated F3 trajectory durations were similar across consonant contexts, suggesting that coarticulation of /r/ is achieved by overlap of a stable /r/-related articulatory gesture with gestures for neighboring sounds. This interpretation, and the concordance of F3 time course with tongue movement for /r/, was supported by direct measures of tongue movement for one subject.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-405"
  },
  "masaki96_icslp": {
   "authors": [
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Mark K.",
     "Tiede"
    ],
    [
     "Yasuhiro",
     "Shimada"
    ],
    [
     "Ichiro",
     "Fujimoto"
    ]
   ],
   "title": "An MRI-based analysis of the English /r/ and /l/ articulations",
   "original": "i96_1581",
   "page_count": 4,
   "order": 408,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "Midsagittal tongue shapes for sustained English hi and IM sounds between native speakers of American English (AE) and Japanese were compared using [he Magnetic Resonance Imaging (MRI) technique. The hi sound as produced by AE speakers was characterized by a constriction at the anterior pan of the hard palate and the existence of a sublingual cavity, and for /I/ sounds, apical contact to the front teeth and/or alveolar ridge, and the absence of the sublingual cavity. For Japanese speakers, strategies to form the tongue shape contrast between hi and /I/ productions were categorized into four types depending on the type of contact/constriction and presence/absence of the sublingual cavity. The first type showed a pattern of tongue shape similar to AE speakers. The second and third types were characterized by hi and 11/ oriented production, respectively, for both sounds. In the last type, the distinction between hi and IV was formed only by the absence or presence of apical contact, while a sublingual cavity was produced iot both sounds. These types are discussed in the context of a perceptual evaluation and an acoustical analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-406"
  },
  "kuijk96b_icslp": {
   "authors": [
    [
     "David van",
     "Kuijk"
    ]
   ],
   "title": "Does lexical stress or metrical stress better predict word boundaries in Dutch?",
   "original": "i96_1585",
   "page_count": 4,
   "order": 409,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "For both human and automatic speech recognizers it is difficult to segment continuous speech into discrete units such as words. Word segmentation is so hard because there seem to be no self-evident cues for word boundaries in the speech stream. However, it has been suggested that English listeners can profit from the occurrence of full vowels (i.e. vowels with metrical stress) in the speech stream to make a first good guess about the location of word boundaries. The CELEX-database study described here investigates whether such a strategy is also feasible for Dutch, and whether the occurrence of full vowels or the occurrence of vowels with primary word stress (i.e. vowels with lexical stress) is a better cue for word boundaries. The CELEX-counts suggest that for Dutch metrical stress seems to be a better predictor of word boundaries than lexical stress.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-407"
  },
  "wrench96_icslp": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "A. D.",
     "McIntosh"
    ],
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "Optopalatograph (OPG): a new apparatus for speech production analysis",
   "original": "i96_1589",
   "page_count": 4,
   "order": 410,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "This paper describes the early development of a device for measuring tongue-palate distance, contact and pressure across the whole of the hard palate. The Optopalatograph (OPG) is similar in principle to the Glossometer and similar in configuration to the Electropalatograph. It uses optical fibres to relay light to and from the palate and distance sensing is acheived by measuring the amount of light reflected from the surface of the tongue. This paper compares the performance of an optical-fibre-based OPG sensor with that of the direct Glossometer configuration and demonstrates improvements in accuracy and sensitivity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-408"
  },
  "carre96_icslp": {
   "authors": [
    [
     "René",
     "Carré"
    ]
   ],
   "title": "Prediction of vowel systems using a deductive approach",
   "original": "i96_1593",
   "page_count": 4,
   "order": 411,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "A deductive approach is developed to predict vocalic systems. First, vowels are proposed from an efficient and simple use of an acoustic tube. Then, a maximum acoustic dispersion criterion is applied to classify the obtained vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-409"
  },
  "mair96_icslp": {
   "authors": [
    [
     "Sheila J.",
     "Mair"
    ],
    [
     "Celia",
     "Scully"
    ],
    [
     "Christine H.",
     "Shadle"
    ]
   ],
   "title": "Distinctions between [t] and [tch] using electropalatography data",
   "original": "i96_1597",
   "page_count": 4,
   "order": 412,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "Electropalatography data are presented for the English alveolar plosive [t] and palaio-alveolar affricate [tf] in VCV sequences produced by an adult female speaker. Selected composite contact patterns are presented for the closure frame, the frame of maximum contact, and the pre-release and release frames. The results are discussed with particular reference to (1) place of articulation, (2) constriction shape, (3) speed of release of the articulatory closure and (4) the effects of vowel context. The data suggest that [t] has a more anterior articulation than [ti], though the distinction is less apparent in the initial part of the closure phase. The release appears to be faster for [t] than [J], and faster for both consonants in open vowel contexts than close vowel contexts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-410"
  },
  "hashi96_icslp": {
   "authors": [
    [
     "Michiko",
     "Hashi"
    ],
    [
     "Raymond D.",
     "Kent"
    ],
    [
     "John R.",
     "Westbury"
    ],
    [
     "Mary J.",
     "Lindstrom"
    ]
   ],
   "title": "Relating formants and articulation in intelligibility test words",
   "original": "i96_1601",
   "page_count": 4,
   "order": 413,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "The present study represents an attempt to describe the relationships between formant histories and lingual fleshpoint movements for selected words spoken by five normal, young-adult speakers of American English. The data arc drawn from the X-ray Mierobcam (XRMB) Speech Production Database (1). A major purpose of the study is to examine the appropriateness of simple acoustic-to-articulatory inferences that are common in speech research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-411"
  },
  "znagui96_icslp": {
   "authors": [
    [
     "Imad",
     "Znagui"
    ],
    [
     "Mohamed",
     "Yeou"
    ]
   ],
   "title": "The role of coarticulation in the perception of vowel quality in modern standard Arabic",
   "original": "i96_1605",
   "page_count": 4,
   "order": 414,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "In Znagui (1995), on investigation was made of the coarticulatory influence of lingual consonants differing in place of articulation (interdental /S, ©/. alveolar/s, z/, palatal /f, 3/, postpalatal /k/, uvular 1%, a, q/, pharyngeal /h, 1/ and pharyngealized /t\\ s , d9, C / on the adjacent vowels /a, a:, i;, i:. u, u:/ in Modern Standard Arabic (MSA). Measurements were taken of the distance between the frequency of Fl and F2 in the vowel steady state. The results showed that two categories of vowels could be distinguished as a function of the distance between Fl and F2. The objective of the present study is to investigate the perceptual significance of this acoustico-phonetic classification. One question of interest is to see if native speakers of Arabic can discriminate between these categories of vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-412"
  },
  "arnfield96_icslp": {
   "authors": [
    [
     "Simon",
     "Arnfield"
    ],
    [
     "Wilf",
     "Jones"
    ]
   ],
   "title": "Updating the reading EPG",
   "original": "i96_1609",
   "page_count": 4,
   "order": 415,
   "p1": "1609",
   "pn": "1611",
   "abstract": [
    "The Reading EPG [1,2] (Electropalatograph) was developed in the Speech Research Laboratory at The University of Reading as a device to allow the number, shape and position of contacts between the tongue and the roof of the mouth to be measured and recorded. This device is used both as a research tool[3, 4] and in speech therapy as a bio-feedback device. The preferred equipment used by major speech research laboratories has shifted in recent years from discrete devices and PCs to more powerful Unix workstations running speech and signal processing packages such as Entropic's ESPS/Waves+. This paper describes the design and implementation of an update to the Reading EPG 3: hardware (called the EPG 3+) has been developed to acquire data from the EPG 3 direct-to-disk on a Unix workstation and software has been developed to integrate EPG into ESPS/Waves+, to calculate a variety of analyses and to provide real{time display of EPG patterns.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-413"
  },
  "ying96b_icslp": {
   "authors": [
    [
     "Goangshiuan S.",
     "Ying"
    ],
    [
     "Leah H.",
     "Jamieson"
    ],
    [
     "Ruxin",
     "Chen"
    ],
    [
     "Carl D.",
     "Mitchell"
    ]
   ],
   "title": "Lexical stress detection on stress-minimal word pairs",
   "original": "i96_1612",
   "page_count": 4,
   "order": 416,
   "p1": "1612",
   "pn": "1615",
   "abstract": [
    "We present a study on the use of lexical stress classification to aid in the recognition of phonetically similar words. In this study, we use a simple pattern recognition approach to determine which syllable is lexically stressed for phonetically similar word pairs (e.g., PERfect, perFECT) extracted from continuously spoken sentences. We use a combination of two features from the acoustic correlates of lexical stress, and assume multivariate Gaussian distributions to form a Bayesian classifier. The features used are normalized energy and duration of the vowel for each syllable of the word. We evaluate several normalization methods. Two sets of sentences were designed for this study. For the pilot experiment, the classification accuracy on words from the natural sentence set was 89.9% and on words from the control sentence set was 100%. To improve the performance, three-feature classifiers, which included two normalized energy features and one normalized duration feature, were developed. The classification accuracy on words from the natural sentence set was 97.23%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-414"
  },
  "wang96g_icslp": {
   "authors": [
    [
     "Jing",
     "Wang"
    ]
   ],
   "title": "An acoustic study of the interaction between stressed and unstressed syllables in spoken Mandarin",
   "original": "i96_1616",
   "page_count": 5,
   "order": 417,
   "p1": "1616",
   "pn": "1619",
   "abstract": [
    "This study examines the acoustic correlates of relative prominence in connected Mandarin speech, particularly those acoustic manifestations that reveal an interaction between stressed and unstressed syllables. The acoustic measurements are primarily concerned with segment duration, intensity, and F0. This study shows that: (1) stressed CV syllables have significantly longer consonant and vowel durations and greater intensity when preceding unstressed syllables than stressed syllables; (2) vowel intensity and duration of stressed syllables are significantly affected by the stressed status of preceding and succeeding syllables, and anticipatory effects are greater on vowel duration but carryover ones are greater on vowel intensity; (3) apart from their lexical stress status, the relative prominence of consecutive syllables is mainly determined by interaction - i.e., unstressed syllables render neighboring syllables more prominent, while stressed syllables cause neighboring syllables to become weak, engendering a pattern of alternating strong and weak syllables. This study also provides insights into the metrical structure of Mandarin and how stress functions in Mandarin speech, where stress and accent interact with lexical tones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-415"
  },
  "minematsu96_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Automatic detection of accent nuclei at the head of words for speech recognition",
   "original": "i96_1620",
   "page_count": 4,
   "order": 418,
   "p1": "1620",
   "pn": "1623",
   "abstract": [
    "A new scheme is proposed to incorporate prosodic processing into speech recognition, where the accent nuclei at the head of words are detected automatically and used to limit the searching space in speech recognition, that is, to preselect candidate words. Especially in this paper, the proposed method for the automatic detection of the accent nuclei and its performance are described. Using this scheme, it is expected that the recognition speed is improved. This scheme is derived from a finding by perceptual experiments conducted previously by the first author. Results of the experiments indicated that the accent nucleus at the first mora has acceleration effect on perceiving the word. This effect can be explained by the earlier identification of the word accent type as type 1 by its nucleus at the first mora. In other words, the accent nucleus at the head of a word can limit the searching space effectively in the mental lexicon. This mechanism was implemented using HMMs and examined for isolated words on a machine, where the vowel detection by broad segmental features and the rejection of words with a devoiced vowel at the first or second mora were introduced at the same time. Evaluation experiments showed 94.7% and 90.0% as recall factor and precision factor of the accent nucleus detection respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-416"
  },
  "chou96_icslp": {
   "authors": [
    [
     "Fu-chiang",
     "Chou"
    ],
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Automatic generation of prosodic structure for high quality Mandarin speech synthesis",
   "original": "i96_1624",
   "page_count": 4,
   "order": 419,
   "p1": "1624",
   "pn": "1627",
   "abstract": [
    "A key problem for today's speech synthesis technology is to automatically generate an appropriate hierarchical prosodic structure for text input and incorporate it into synthesized speech[1][2]. This paper presents a method for such a problem in Mandarin Chinese. This method uses a speech database for the training of a statistical model to generate the prosodic structure and determine prosodic parameters such as syllable duration, pause, energy and intonation. The experimental results show that an accuracy of 83.1% in the prediction of prosodic structure can be achieved. Furthermore, a Chinese text-to-speech system can be developed based on the proposed prosodic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-417"
  },
  "hamagami96_icslp": {
   "authors": [
    [
     "Tomoki",
     "Hamagami"
    ],
    [
     "Ken-ichi",
     "Magata"
    ],
    [
     "Mitsuo",
     "Komura"
    ]
   ],
   "title": "A study on Japanese prosodic pattern and its modeling in restricted speech",
   "original": "i96_1628",
   "page_count": 4,
   "order": 420,
   "p1": "1628",
   "pn": "1631",
   "abstract": [
    "This report proposes a simple and practical model for generating relatively monotonous, but sufficiently natural, prosodic features by analyzing restricted natural speech. The basic assumption of this model is that the natural F0 pattern can be obtained without complicated linguistic analysis. To achieve this prosodic control, we have analyzed and modeled this speech subject that is recoded so that it will appear in the following. First we composed the hypothesis that a Japanese Major Phrase (MP) could be modeled with the combination of a minor phrase (mp) sequence limited to fewer than three. The number of the combination is decided by the accentual type of minor phrase and intrasentence position. The combination types have 28 patterns. To confirm the hypothesis, the restricted speech (RSP) subjects were collected and analyzed by having the speaker utter the subject sentence without emotional effect or attention to prosodic features. Furthermore, to evaluate the performance of the model, a pattern-matching process (two-level DP) was used between the synthesized F0 pattern and the restricted real F0 pattern. We thus confirmed that our model would create a synthesized F0 pattern sufficiently similar the restricted-speech patterns. The synthesized speech using this model sounds relatively monotonous, but is sufficiently natural as compared with general spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-418"
  },
  "hoskins96_icslp": {
   "authors": [
    [
     "Steve",
     "Hoskins"
    ]
   ],
   "title": "A phonetic study of focus in intransitive verb sentences",
   "original": "i96_1632",
   "page_count": 4,
   "order": 421,
   "p1": "1632",
   "pn": "1635",
   "abstract": [
    "In English, focus is one of the factors determining the prosodic characteristics of an utterance. Some recent linguistic analyses (Selkirk 1995) claim that focus is represented in the phonology by pitch accents, and the placement of these accents is determined by both the type of focus (broad, narrow), syntactic structure, and other factors such as verbargument structure. In this study, an experiment was conducted to test the interaction of focus types and different argument structures within sentences with intransitive verbs. A prosodic analysis was conducted, durations and fundamental frequency of the subject nouns and verbs of these sentences were measured. The prosodic and acoustic analyses support Selkirk (1995): specifically, the verb may be deaccented under broad focus when the subject is a theme (in unaccusatives and passives) but not when an agent (unergatives). Also, there is a tendency for the nuclear accented word to have a greater duration and F0 under narrow focus, which seems to be related to a more frequent occurance of the L+H* pitch accent for this condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-419"
  },
  "rapp96_icslp": {
   "authors": [
    [
     "Stefan",
     "Rapp"
    ]
   ],
   "title": "Goethe for prosody",
   "original": "i96_1636",
   "page_count": 6,
   "order": 422,
   "p1": "1636",
   "pn": "1639",
   "abstract": [
    "In this paper, we describe the way in which a recording of Goethes \"Die Leiden des jungen Werther\" published on a multimedia CDROM [7] was made accessible for prosody research. The recording is interesting for prosody research because of its prosodic richness as it displays a large variety of registers and speaking styles. Application areas are: development of prosody models for German TTS, unsupervised learning of pitch accent types, corpus search for research on prosody-semantics and prosody-syntax interaction, and the study of more global prosodic parameters (speaking rate, pitch range) defining registers or speaking style. The four hour recording was segmented into phonemes, syllables and words using HMM speech recognition techniques [5, 13] together with a large pronunciation lexicon [1]. A part of speech tagger [14] was applied to the corpus to yield time aligned POS tags. The German adaptation of the tone sequence model of intonation used in Stuttgart [11, 6] inspired the parametrization of fundamental frequency. An intermediate phonetic representation layer is described that uses the syllable alignment to parametrize the F0 contour into a superposition of three functions: a hyperbolic tangent, a gaussian and a constant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-420"
  },
  "straub96_icslp": {
   "authors": [
    [
     "K. A.",
     "Straub"
    ]
   ],
   "title": "Prosodic cues in syntactically ambiguous strings; an interactive speech planning mechanism",
   "original": "i96_1640",
   "page_count": 4,
   "order": 423,
   "p1": "1640",
   "pn": "1643",
   "abstract": [
    "This paper presents data suggesting that the clarity with which the speech signal is produced may be mediated by the presence of extra-syntactic factors such as the presence of strongly biasing context. An elicited speech experiment examining prepositional phrase attachment ambiguities produced within either strongly biasing or non-biasing contexts reveals that although the expected acoustic correlates of syntactic structure are consistently observed, the salience of these cues is diminished when alternate sources of disambiguating information are present for the listener. A second experiment shows that the cues are predictive with respect to listeners' interpretation of the ambiguous segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-421"
  },
  "ni96_icslp": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Deyu",
     "Xia"
    ]
   ],
   "title": "A functional model for generation of the local components of F0 contours in Chinese",
   "original": "i96_1644",
   "page_count": 4,
   "order": 424,
   "p1": "1644",
   "pn": "1647",
   "abstract": [
    "In this paper, a new functional model is introduced, which is designed to simulate the control mechanism for the generation of the local component of F0 contour. With the model two kinds of motor command are defined to control the model parameters to generate two basic rise-fall feature patterns, on the logarithmic scale of fundamental frequency the local component of a F0 contour is approximated by the algebra sum of these patterns. The experimental results in analyzing and synthesizing Chinese F0 contours indicate that the real F0 contours can always be approximated very closely by the model, and a close correlation exists between the model parameters and the structure of rise-fall pattern.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-422"
  },
  "fellbaum96_icslp": {
   "authors": [
    [
     "Marie",
     "Fellbaum"
    ]
   ],
   "title": "The acquisition of voiceless stops in the interlanguage of second language learners of English and Spanish",
   "original": "i96_1648",
   "page_count": 4,
   "order": 425,
   "p1": "1648",
   "pn": "1651",
   "abstract": [
    "This paper presents the preliminary results from work in progress of a paired study of the acquisition of voiceless stops by Spanish speakers learning English and American English speakers learning Spanish. For this study the hypothesis was that the American speakers would have no difficulty suppressing the aspiration in Spanish unaspirated stops; the Spanish speakers would have difficulty acquiring the aspiration necessary for English voiceless stops, according to Eckman's Markedness Differential Hypothesis. The null hypothesis was proved. The results also reveal that a simple report of means will not distinguish the speakers and the respective language learning situation; measurements must also include the range of acceptability of VOT for phonetic segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-423"
  },
  "mellor96_icslp": {
   "authors": [
    [
     "B. A.",
     "Mellor"
    ],
    [
     "C.",
     "Baber"
    ],
    [
     "C.",
     "Tunley"
    ]
   ],
   "title": "Evaluating automatic speech recognition as a component of a multi-input device human-computer interface",
   "original": "i96_1668",
   "page_count": 4,
   "order": 426,
   "p1": "1668",
   "pn": "1671",
   "abstract": [
    "This paper reports on an investigation into the basic properties and requirements of automatic speech recognition as an input device to a trial human computer interface. The experiments required subjects to carry out a simulated target acquisition and report filling task, with the available input devices being automatic speech recognition, trackball, function keys or a simultaneous combination of all three. Experiments were carried out under varying workload to examine the degradation of overall interface and individual input device performance under user stress.\n",
    "An approach at modelling interface performance using a critical path analysis approach is introduced. Modelling of the interface developed here has shown a good match to the experimental results.\n",
    "Although use of the prototype speech recogniser was found to be both slower and less accurate than the manual mode inputs it was possible to estimate a required word accuracy of around 94% which would allow speech entry to provide an equivalent performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-424"
  },
  "life96_icslp": {
   "authors": [
    [
     "A.",
     "Life"
    ],
    [
     "I.",
     "Salter"
    ],
    [
     "J. N.",
     "Temem"
    ],
    [
     "F.",
     "Bernard"
    ],
    [
     "S.",
     "Rosset"
    ],
    [
     "S. K.",
     "Bennacef"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Data collection for the MASK kiosk: WOz vs prototype system",
   "original": "i96_1672",
   "page_count": 4,
   "order": 427,
   "p1": "1672",
   "pn": "1675",
   "abstract": [
    "The MASK consortium is developing a prototype multimodal multimedia service kiosk for train travel information and reservation, exploiting state-of-the-art speech technology. In this paper we report on our efforts aimed at evaluating alternative user interface designs and at obtaining acoustic and language modeling data for the spoken language component of the overall system. Simulation methods with increasing degrees of complexity have been used to test the interface design, and to experiment with alternate operating modes. The majority of the speech data has been collected using successive versions of the spoken language system, whose capabilities are incrementally expanded after analysis of the most frequent problems encountered by users of the preceding version. The different data requirements of user interface design and speech corpus acquisition are discussed in light of the experience of the MASK project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-425"
  },
  "karaorman96_icslp": {
   "authors": [
    [
     "M.",
     "Karaorman"
    ],
    [
     "T. H.",
     "Applebaum"
    ],
    [
     "T.",
     "Itoh"
    ],
    [
     "M.",
     "Endo"
    ],
    [
     "Y.",
     "Ohno"
    ],
    [
     "M.",
     "Hoshimi"
    ],
    [
     "T.",
     "Kamai"
    ],
    [
     "K.",
     "Matsui"
    ],
    [
     "K.",
     "Hata"
    ],
    [
     "S.",
     "Pearson"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "An experimental Japanese/English interpreting video phone system",
   "original": "i96_1676",
   "page_count": 4,
   "order": 428,
   "p1": "1676",
   "pn": "1679",
   "abstract": [
    "In this paper we report on the architectural design issues and experiences gained while building and demonstrating an experimental interpreting video phone (IVP) system. The IVP system has been demonstrated in an internet home shopping simulation simultaneously before live audiences in Japan and the U.S. An American shop assistant and a Japanese customer engaged in task-directed dialogues, using their native languages. In addition to their direct audio/visual contact by ISDN video phone, each participant heard a translation of the remote speakers utterances in a synthetic voice in real-time. Each site used a medium-size vocabulary, a continuous speech recognition system and a text-to-speech synthesis (TTS) system for the local language. Recognition results were transmitted over the internet to the remote site, where the corresponding translated sentence was spoken by TTS in the listeners native language. All of the speech and language processing software components of the system were independently developed proprietary technologies of the authors laboratories which were integrated using commercially available hardware and communication media. Difficulties encountered in developing the system, the accommodations which were made, and other experiences gained through the process are reported in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-426"
  },
  "basson96_icslp": {
   "authors": [
    [
     "Sara",
     "Basson"
    ],
    [
     "Stephen",
     "Springer"
    ],
    [
     "Cynthia",
     "Fong"
    ],
    [
     "Hong",
     "Leung"
    ],
    [
     "Ed",
     "Man"
    ],
    [
     "Michele",
     "Olson"
    ],
    [
     "John",
     "Pitrelli"
    ],
    [
     "Ranvir",
     "Singh"
    ],
    [
     "Suk",
     "Wong"
    ]
   ],
   "title": "User participation and compliance in speech automated telecommunications applications",
   "original": "i96_1680",
   "page_count": 4,
   "order": 429,
   "p1": "1680",
   "pn": "1683",
   "abstract": [
    "This paper reviews results from a number of field trials assessing speech recognition feasibility for telecommunications services. Several applications incorporating speech automation are explored: Directory Assistance Call Completion (DACC), partial speech automation of Directory Assistance (OSF - Operator Store and Forward), banking over the telephone (Money Talks) and partial speech automation of a customer calling center (PREVIU). The experimental results presented here were collected through Wizard-of-Oz experiments as feasibility precursors to speech recognition automation. Speech interfaces were clearly superior to Touch-Tone in one experiment (PREVIU), with caller participation increasing by 30%. In another experiment (Money Talks), speech recognition interfaces did not improve caller participation and, in fact, provided no advantages over Touch-Tone automation. A number of prompting strategies have been identified as advantageous in increasing caller participation and compliance in automated services. Ultimately, success with speech automated services will rely on identifying the services most suitable for speech automation and then carefully crafting the user interface.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-427"
  },
  "bayer96_icslp": {
   "authors": [
    [
     "Samuel",
     "Bayer"
    ]
   ],
   "title": "Embedding speech in web interfaces",
   "original": "i96_1684",
   "page_count": 4,
   "order": 430,
   "p1": "1684",
   "pn": "1687",
   "abstract": [
    "In this paper, we will describe work in progress at the MITRE Corporation on embedding speech-enabled interfaces in Web browsers. This research is part of our work to establish the infrastructure to create Web-hosted versions of prototype multimodal interfaces, both intelligent and otherwise. Like many others, we believe that the Web is the best potential delivery and distribution vehicle for complex software applications, and that the functionality of these Web-hosted applications should match the functionality available in standalone applications. In this paper, we will discuss our approach to several aspects of this goal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-428"
  },
  "isobe96_icslp": {
   "authors": [
    [
     "Toshihiro",
     "Isobe"
    ],
    [
     "Masatoshi",
     "Morishima"
    ],
    [
     "Fuminori",
     "Yoshitani"
    ],
    [
     "Nobuo",
     "Koizumi"
    ],
    [
     "Ken'ya",
     "Murakami"
    ]
   ],
   "title": "Voice-activated home banking system and its field trial",
   "original": "i96_1688",
   "page_count": 6,
   "order": 431,
   "p1": "1688",
   "pn": "1691",
   "abstract": [
    "Speech recognition techniques are most useful when used over the phone. A telephone speech recognizer was developed and many field trials were carried out[1][2][3]. We have developed telephone speech recognition hardware for a voice-activated home banking system based on a client-server network configuration[4]. The speech recognition unit is a workstation with six boards for dealing with simultaneous multi-channel processing. The speech recognition algorithm implemented in the boards, each of which has three DSPs and an MPU, handles various tasks, such as recognizing connected digits, bank name, branch name, money amount, and confirmation for completing the service dialogs. Experimental field trials on 90 subjects showed that with proper instructions and guidance, the service task was successfully achieved in 85% of trials. We sent out a questionnaire, and one third of the subjects replied that speech recognition was useful.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-429"
  },
  "lee96g_icslp": {
   "authors": [
    [
     "Sangho",
     "Lee"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "A text analyzer for Korean text-to-speech systems",
   "original": "i96_1692",
   "page_count": 4,
   "order": 432,
   "p1": "1692",
   "pn": "1695",
   "abstract": [
    "In developing a text-to-speech system, it is well known that the accuracy of information extracted from a text is crucial to produce high quality synthesized speech. In this paper, by transferring probabilistic natural language processing techniques into TTS system field, we develop a more robust text analyzer with high accuracy for Korean TTS systems. The proposed system is composed of five modules: a preprocessor, a morphological analyzer, a part-of-speech tagger, a grapheme-to-phoneme module, and a parser. Among these modules, the part-of-speech tagger and the parser are designed under probabilistic framework, and trained automatically. Given a text, our system produces the structures of word phrases, word pronunciations, and governor-dependent relationships that represents the structure of the sentence. Experimental results showed that the tagger got 90.33% correctness for finding the structure of word phrases in the word level, and the parser, 80.87% for finding governor-dependent relationships of sentences respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-430"
  },
  "karn96_icslp": {
   "authors": [
    [
     "Helen E.",
     "Karn"
    ]
   ],
   "title": "Design and evaluation of a phonological phrase parser for Spanish text-to-speech",
   "original": "i96_1696",
   "page_count": 4,
   "order": 433,
   "p1": "1696",
   "pn": "1699",
   "abstract": [
    "This paper presents and evaluates a phonological phrase parser for a Spanish text-to-speech system. The parser consists of three stages: 1) lexical lookup, using a small dictionary (428 words); 2) preliminary phrase boundary placement, using a modification of Liberman and Churchs (1992) function group parser; and 3) readjustment of phrase boundaries, using syllable count and punctuation. A corpus of 382 hand-parsed sentences (1,691 phrases) was used to evaluate the parser. The parser generated almost the same number of phrases (1,692) as the hand-parsed sentences with 70% (1,186) agreement. Suggestions for improving the parsers performance include the expansion of the verb lexicon, performing simple morphological analysis for verbs, and relaxing the syllable count in phrases before verb forms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-431"
  },
  "andersen96_icslp": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Ariane",
     "Lazaridès"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Jürgen",
     "Haas"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Comparison of two tree-structured approaches for grapheme-to-phoneme conversion",
   "original": "i96_1700",
   "page_count": 4,
   "order": 434,
   "p1": "1700",
   "pn": "1703",
   "abstract": [
    "Recently, we described a two-step self-learning approach for grapheme-to-phoneme (G2P) conversion [1]. In the first step, grapheme and phoneme strings in the training data are aligned via an iterative Viterbi procedure that may insert graphemic and phonemic nulls where required. In the second step, a Trie structure encoding pronunciation rules is generated. In this paper we describe the alignment module, and give alignment accuracies on the NETtalk database. We also compare transcription accuracies for two approaches to the second step on three databases: the NETtalk database, the CMU dictionary and the French part of the ONOMASTICA lexicon. The two transcription approaches applied in this research are a Trie approach [1] and an approach based on binary decision trees grown by means of the Gelfand-Ravishankar-Delp algorithm [2,3,4]. We discuss the choice of questions for these decision trees - it may be possible to formulate questions about groups of characters (e.g., \"is the next letter a vowel?\") that yield better trees than those that only use questions about individual characters (e.g., \"is the next letter an A ?\"). Finally, we discuss the implications of our work for G2P conversion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-432"
  },
  "adamson96_icslp": {
   "authors": [
    [
     "M. J.",
     "Adamson"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "A recurrent network that learns to pronounce English text",
   "original": "i96_1704",
   "page_count": 4,
   "order": 435,
   "p1": "1704",
   "pn": "1707",
   "abstract": [
    "Previous attempts to derive connectionist models for text-to-phoneme conversion - such as NETtalk and NETspeak - have generally used pre-aligned training data and purely feedforward networks, both of which represent simplifications of the problem. In this work, we explore the potential of recurrent networks to perform the conversion task when trained on non-aligned data. Initially, our use of a single recurrent network produced disappointing results. This led to the definition of a two-phase model in which the hidden-unit representation of an auto-associative network was fed forward to a recurrent network. Although this model currently does not perform as well as NETspeak, it is solving a harder problem. Also, we propose several possible avenues for improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-433"
  },
  "albano96_icslp": {
   "authors": [
    [
     "Eleonora Cavalcante",
     "Albano"
    ],
    [
     "Agnaldo Antonio",
     "Moreira"
    ]
   ],
   "title": "Archisegment-based letter-to-phone conversion for concatenative speech synthesis in Portuguese",
   "original": "i96_1708",
   "page_count": 4,
   "order": 436,
   "p1": "1708",
   "pn": "1711",
   "abstract": [
    "A letter-to-phone conversion scheme is proposed for Portuguese which excludes representation of allophonic detail. Phonetically unstable segments are treated as archisegments, their articulatory weakness being analyzed in terms of feature underspecification. Besides solving classical problems of allophony and allomorphy, this analysis provides an efficient principle for building a unit inventory for concatenative speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-434"
  },
  "yoshida96_icslp": {
   "authors": [
    [
     "Yuki",
     "Yoshida"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ],
    [
     "Kazuo",
     "Hakoda"
    ],
    [
     "Tomohisa",
     "Hirokawa"
    ]
   ],
   "title": "A new method of generating speech synthesis units based on phonological knowledge and clustering technique",
   "original": "i96_1712",
   "page_count": 4,
   "order": 437,
   "p1": "1712",
   "pn": "1715",
   "abstract": [
    "This paper proposes a new method for generating synthesis units using context dependent phonemes to achieve high quality text-to-speech (TTS) synthesis. If all phoneme triplets (triphones) in Japanese axe considered, the number of synthesis units is very large; therefore, we introduce two techniques to reduce the number of synthesis units. The first technique decreases approximately 15,000 triphones to about 6,000 triphones based on phonological knowledge. The second technique is based on a segment quantization, which reduces the number of units even more. Experimental tests show that the proposed method is effective in improving articulation and intelligibility scores, that the number of synthesis units can be decreased without significant loss in TTS quality, and that the preference score is proportional to the number of synthesis units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-435"
  },
  "grice96_icslp": {
   "authors": [
    [
     "Martine",
     "Grice"
    ],
    [
     "Matthias",
     "Reyelt"
    ],
    [
     "Ralf",
     "Benzmüller"
    ],
    [
     "Jörg",
     "Mayer"
    ],
    [
     "Anton",
     "Batliner"
    ]
   ],
   "title": "Consistency in transcription and labelling of German intonation with GToBI",
   "original": "i96_1716",
   "page_count": 4,
   "order": 438,
   "p1": "1716",
   "pn": "1719",
   "abstract": [
    "A diverse set of speech data was labelled in three sites by 13 transcribers with differing levels of expertise, using GToBI, a consensus transcription system for German intonation. Overall inter-transcriber-consistency suggests that, with training, labellers can acquire sufficient skill with GToBI for large-scale database labelling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-436"
  },
  "batliner96b_icslp": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Andreas",
     "Kiessling"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Syntactic-prosodic labeling of large spontaneous speech data-bases",
   "original": "i96_1720",
   "page_count": 4,
   "order": 439,
   "p1": "1720",
   "pn": "1723",
   "abstract": [
    "In automatic speech understanding, the division of continuously running speech into syntactic chunks is a great problem. Syntactic boundaries are often marked by prosodic means. For the training of statistic models for prosodic boundaries large data-bases are necessary. For the German Verbmobil project (automatic speech-to-speech translation), we developed a syntactic-prosodic labeling scheme where two main types of boundaries (major syntactic boundaries and syntactically ambiguous boundaries) and some other special boundaries are labeled for a large Verbmobil spontaneous speech corpus. We compare the results of classifiers (multilayer perceptrons and language models) trained on these syntactic-prosodic boundary labels with classifiers trained on perceptual-prosodic and pure syntactic labels. The main advantage of the rough syntactic-prosodic labels presented in this paper is that large amounts of data could be labeled within a short time. Therefore, the classifiers trained with these labels turned out to be superior (recognition rates of up to 96%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-437"
  },
  "koopmansvanbeinum96_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Monique E. van",
     "Donzel"
    ]
   ],
   "title": "Relationship between discourse structure and dynamic speech rate",
   "original": "i96_1724",
   "page_count": 4,
   "order": 440,
   "p1": "1724",
   "pn": "1727",
   "abstract": [
    "This paper regards one specific element of a larger research project on the acoustic determinants of information structure in spontaneous and read discourse in Dutch. From a previous experiment within that project it turned out that listeners used two main cues (viz. speaking rate and intonation) to differentiate between spontaneous and read speech [7]. The aim of the present experiment is to investigate the role of one of these prosodic cues, i.e., the local variability in speaking rate, and to study the relationship between the information structure of a spoken discourse at the one hand, and dynamic speaking rate measurements of that discourse at the other hand. Results show that there is a large variability in average syllable duration over the various interpausal speech runs for each of the eight speakers. No straightforward relation is found between the number of syllables within a run and the average syllable duration. We hypothesize that, at least in spontaneous speech, variations in speaking rate are related to the (global and/or local) information structures in the discourse. Global analysis of the discourse structure in paragraphs and clauses reveals that for each of the speakers the average syllable duration of the first run of a paragraph is longer than the overall mean value per speaker in more than 60 % of the cases. Inspection of the quartiles of runs with highest ASD-values and those with lowest ASD-values for each of the speakers shows quite different structures, which can be explained on the basis of partly local and partly global discourse characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-438"
  },
  "ward96_icslp": {
   "authors": [
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "Using prosodic clues to decide when to produce back-channel utterances",
   "original": "i96_1728",
   "page_count": 5,
   "order": 441,
   "p1": "1728",
   "pn": "1731",
   "abstract": [
    "Back-channel feedback is required in order to build spoken dialog systems that are responsive. This paper reports a model of back-channel feedback in Japanese dialog. It turns out that a low pitch region is a good clue that the speaker is ready for back-channel feedback. A rule based on this fact matches corpus data on respondents' production of back-channel feedback. A system based on this rule meets the expectations of live speakers, sometimes well enough to fool them into thinking they are conversing with a human.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-439"
  },
  "mast96_icslp": {
   "authors": [
    [
     "Marion",
     "Mast"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Andreas",
     "Kiessling"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Ernst G.",
     "Schukat-Talamazzini"
    ],
    [
     "Volker",
     "Warnke"
    ]
   ],
   "title": "Dialog act classification with the help of prosody",
   "original": "i96_1732",
   "page_count": 4,
   "order": 442,
   "p1": "1732",
   "pn": "1735",
   "abstract": [
    "This paper presents automatic methods for the segmentation and classification of dialog acts (DA). In Verbmobil it is often sufficient to recognize the sequence of DAs occurring during a dialog between the two partners. Since a turn can consist of one or more successive DAs we conduct the classification of DAs in a two step procedure: First each turn has to be segmented into units which correspond to a DA and second the DA categories have to be identified. For the segmentation we use polygrams and multi-layer perceptrons, using prosodic features. The classification of DAs is done with semantic classification trees and polygrams.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-440"
  },
  "kuijk96c_icslp": {
   "authors": [
    [
     "David van",
     "Kuijk"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Using lexical stress in continuous speech recognition for dutch",
   "original": "i96_1736",
   "page_count": 4,
   "order": 443,
   "p1": "1736",
   "pn": "1739",
   "abstract": [
    "The acoustic realization of vowels with lexical stress generally differs substantially from their unstressed counterparts, which are more reduced in spectral quality, shorter in duration, weaker in intensity and tend to have a flatter spectral tilt. Therefore, in an automatic speech recognizer it would appear profitable to train separate models for the stressed and unstressed variants of each vowel. A problem is how to define the mapping from the theoretical stress of words to the actual realization of stress in fluent speech. We compared several hypotheses about this mapping applied in both training and testing of the recognizer. The recognition results on an independent test-set showed that recognition rates did not increase by this use of stress in our ASR. Possible explanations are discussed and future research plans are outlined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-441"
  },
  "kumpf96_icslp": {
   "authors": [
    [
     "Karsten",
     "Kumpf"
    ],
    [
     "Robin W.",
     "King"
    ]
   ],
   "title": "Automatic accent classification of foreign accented australian English speech",
   "original": "i96_1740",
   "page_count": 4,
   "order": 444,
   "p1": "1740",
   "pn": "1743",
   "abstract": [
    "An automatic classification system for foreign accents in Australian English speech based on accent dependent parallel phoneme recognition (PPR) has been developed. The classifier is designed to process continuous speech and to discriminate between native Australian English (AuE) speakers and two migrant speaker groups with foreign accents, whose first languages are Lebanese Arabic (LA) and South Vietnamese (SV). The training of the system can be automated and is novel in that it does not require manually labelled accented data. The test utterances are processed in parallel by three (AuE, SV and LA) accent-specific recognizers incorporating the accent-specific HMMs and phoneme bigram language models to produce accent discrimination likelihood scores. The best average accent classification rates were 85.3% and 76.6% for accent pair and three accent class discrimination tasks, respectively. Analyses of the contributions to accent discrimination by the phoneme level processing, and by the language model, are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-442"
  },
  "korkmazskiy96_icslp": {
   "authors": [
    [
     "F.",
     "Korkmazskiy"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Discriminative adaptation for speaker verification",
   "original": "i96_1744",
   "page_count": 4,
   "order": 445,
   "p1": "1744",
   "pn": "1747",
   "abstract": [
    "This paper describes a speaker verification system in which the talker and imposter models are adapted to achieve maximum discrimination, or equivalently minimum verification error. This goal is accomplished by extending the minimum error classification criterion (MCE) and generalized probabilistic descent (GPD) algorithm to the task of adapting talker model parameters and the corresponding anti-talker model parameters to the test environments so as to minimize an empirical estimate of the verification error rate. We address in the current study adaptation of two types of parameters: the model parameters and the decision threshold. We have obtained substantial improvements in equal error rate by applying combined techniques involving a simplified MAP (maximum a posteriori) method and the GPD algorithm. The equal error rate for a database of 43 talkers with 5 adaptation utterances each was reduced from the previously reported best result of 5.41% [1] to 2.17%. We will discuss several alternative methods that have been investigated in this work to provide comparative insights for the use of discriminative methods in speaker verification tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-443"
  },
  "stockmal96_icslp": {
   "authors": [
    [
     "V.",
     "Stockmal"
    ],
    [
     "D.",
     "Muljani"
    ],
    [
     "Z. S.",
     "Bond"
    ]
   ],
   "title": "Perceptual features of unknown foreign languages as revealed by multi-dimensional scaling",
   "original": "i96_1748",
   "page_count": 4,
   "order": 446,
   "p1": "1748",
   "pn": "1751",
   "abstract": [
    "Adult listeners are able to discriminate between and often identify spoken samples of languages that are unknown to them. Two studies were designed to explore which perceptual properties inherent within the phonological structure of languages are salient to foreign language listeners. In study one, fifteen subjects were asked to judge whether pairs of spoken foreign language sentences were selected from same or different languages and to explain how they had made the judgement. A multi-dimensional scaling (MDS) was conducted on the subject responses for the 'same language' condition. The resulting map revealed that responses could be characterized along two dimensions: phonologically based psychoacoustic properties and talker specific characteristics. The two dimensions define the distinctiveness of the languages and elicited different perceptual feature relationships in subjects. In study two, this perceptual feature relationship was tested using similarity judgements. Thirty subjects rated similarity on a sevenpoint scale for the same set of sentence pairs that had been judged in study one. MDS analysis revealed that the 'different language' condition yielded a map in which the language proximities closely approximated those which had been derived by focusing on phonological properties. This finding suggests that since analysis of both 'different language' and 'same language' sentence pairs produced similar maps, perceived language similarity among foreign languages depends upon the listeners' salient organizational categories inherent within the phonological structure of language and the talker specific characteristics of voice quality and speech rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-444"
  },
  "yu96b_icslp": {
   "authors": [
    [
     "Kin",
     "Yu"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "On-line incremental adaptation for speaker verification using maximum likelihood estimates of CDHMM parameters",
   "original": "i96_1752",
   "page_count": 4,
   "order": 447,
   "p1": "1752",
   "pn": "1755",
   "abstract": [
    "This papers investigates two approaches to on-line incremental adaptation of CDHMM parameters. First the popular MAP approach is examined, highlighting difficulties in automatically setting the adaptation rate. To overcome these problems we introduce a new approach based on the multi-observation estimation equations of the forward-backward algorithm called a cumulative likelihood estimate (CLE). Experimental results using these two approaches are compared with and without the use of a speech model for enrolment on isolated word speaker models. In both enrolment procedures, the CLE approach can achieve approximately an EER of 1% for six adaptation sequences using a single digit test token.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-445"
  },
  "genoud96_icslp": {
   "authors": [
    [
     "Dominique",
     "Genoud"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Combining methods to improve speaker verification decision",
   "original": "i96_1756",
   "page_count": 4,
   "order": 448,
   "p1": "1756",
   "pn": "1759",
   "abstract": [
    "The aim of this paper is to describe how the combination of speaker verification algorithms with a priori decision thresholds can improve the overall robustness of a real application. The evaluation is performed in the context of a field application where each client is verified from a 7 digit pin code. This paper demonstrate that it is possible to increase the global performances of the system on combining the result of several algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-446"
  },
  "martindelalamo96_icslp": {
   "authors": [
    [
     "Cesar",
     "Martín del Alamo"
    ],
    [
     "J.",
     "Alvarez"
    ],
    [
     "C. de la",
     "Torre"
    ],
    [
     "F. J.",
     "Poyatos"
    ],
    [
     "Lúis",
     "Hernández"
    ]
   ],
   "title": "Incremental speaker adaptation with minimum error discriminative training for speaker identification",
   "original": "i96_1760",
   "page_count": 4,
   "order": 449,
   "p1": "1760",
   "pn": "1763",
   "abstract": [
    "Minimum Classification Error (MCE) has shown to be effective in improving the performance of a speaker identification system [1]. However, there are still problems to solve, such as the variability of the voice characteristics of a particular speaker through time. In this work, we analyze the degradation of a GMM-based text-independent speaker identification system when using test data recorded over 6 months after the training session. And trying to avoid this degradation we study the use of supervised adaptation based on Maximum a Posteriori (MAP), and MCE. These techniques have been shown to provide good results for speaker adaptation in speech recognition. The major result we have obtained is that by starting with GMM models trained with only speech from session 1, similar identification results can be obtained for all the other sessions using an incremental adaptation using only 2.5 seconds of speech per speaker and session as data for the MCE training adaptation procedure. We have also found that, in our extreme experimental setup, MAP becomes unhelpful when combined with MCE adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-447"
  },
  "markov96_icslp": {
   "authors": [
    [
     "Konstantin P.",
     "Markov"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Frame level likelihood normalization for text-independent speaker identification using Gaussian mixture models",
   "original": "i96_1764",
   "page_count": 4,
   "order": 450,
   "p1": "1764",
   "pn": "1767",
   "abstract": [
    "In this paper we propose a new speaker identification system, where the likelihood normalization technique, widely used for speaker verification, is introduced. In the new system, which is based on Gaussian Mixture Models, every frame of the test utterance is inputed to all the reference models in parallel. In this procedure, for each frame, likelihoods from all the models are available, hence they can be normalized at every frame. A special kind of likelihood normalization, called Weighting Models Rank, is also proposed. Experiments were performed using two databases - TIMIT and NTT. Evaluation results clearly show that frame level likelihood normalization technique is superior to the standard accumulated likelihood approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-448"
  },
  "thymegobbel96_icslp": {
   "authors": [
    [
     "Ann E.",
     "Thymé-Gobbel"
    ],
    [
     "Sandra E.",
     "Hutchins"
    ]
   ],
   "title": "On using prosodic cues in automatic language identification",
   "original": "i96_1768",
   "page_count": 4,
   "order": 451,
   "p1": "1768",
   "pn": "1771",
   "abstract": [
    "This paper presents an effort to explore the utility of prosodic information in language identification/ discrimination (LED) tasks. We present our model and results from pair-wise LID lasks with English, Spanish, Japanese and Mandarin using multi-speaker elicited spontaneous speech and a selected set of prosodic parameters. These languages represent four different types of languages, varying in pitch use and timing. Parameters were designed to capture pilch and amplitude contours on a syllable-by- syllable basis, and to be insensitive to overall amplitude, pitch, and speaking rate. Results show that prosodic cues alone can distinguish between some language pairs with results comparable to many non-prosodic systems, indicating that prosodic parameters are highly useful in automatic LTD. However, the statistical relationships between, a number of individual features deduced from timing and pitch measurements are needed to begin to capture such complex perceptual events as rhythm. Strengths of individual prosodic parameters and classes of parameters -primarily pilch, secondarily duration and location - reflect differences between the four languages mostly as expectated based on the linguistic literature, suggesting that effective use of prosodic parameters is aided by an understanding of the relationships between physical measurements and perceived linguistic events.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-449"
  },
  "kitamura96_icslp": {
   "authors": [
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Shinsai",
     "Takei"
    ]
   ],
   "title": "Speaker recognition model using two-dimensional mel-cepstrum and predictive neural network",
   "original": "i96_1772",
   "page_count": 4,
   "order": 452,
   "p1": "1772",
   "pn": "1775",
   "abstract": [
    "This paper describes a speaker recognition model using Two-Dimensional Mel-Cepstrum and predictive neural network. The speaker model consists of two networks. The first one is a self-organizing VQ map (Kohonen's feature map). The second part is the predictive network and learns transitional patterns on the feature map of each speaker's model. TDMC consists of averaged features and dynamic features of the two-dimensional mel-log spectra in the analyzed interval. The measure for speaker recognition is obtained by using a combination of the VQ distortion on the feature map and the prediction error on the predictive network. In the study, text-independent speaker identification experiments for 8 speakers were carried out. The experimental results have shown that a combination of a feature map and a predictive network is very effective, and that the proposed model using TDMC shows the robustness for time interval.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-450"
  },
  "kwan96_icslp": {
   "authors": [
    [
     "Hingkeung",
     "Kwan"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Unknown language rejection in language identification system",
   "original": "i96_1776",
   "page_count": 4,
   "order": 453,
   "p1": "1776",
   "pn": "1779",
   "abstract": [
    "The number of languages in the world is much larger than the number of target languages that current language identification systems can handle. Therefore, we propose here the use of a multilayer perceptron neural network as a means to prevent those unknown language inputs from being misidentified as one of the target languages. We consider not only the target language identification rate but also the unknown language rejection rate. Results reveal that with the use of phonemic unigram as the input features to the neural network, a target language identification rate of 93.5% can be achieved for 3 languages. By varying the thresholds of the outputs, good unknown language rejection rate can also be obtained at the expense of lower identification rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-451"
  },
  "hieronymus96_icslp": {
   "authors": [
    [
     "James L.",
     "Hieronymus"
    ],
    [
     "Shubha",
     "Kadambe"
    ]
   ],
   "title": "Spoken language identification using large vocabulary speech recognition",
   "original": "i96_1780",
   "page_count": 4,
   "order": 454,
   "p1": "1780",
   "pn": "1783",
   "abstract": [
    "A task independent spoken Language Identification (LID) system which uses a Large Vocabulary Automatic Speech Recognition (LVASR) module for each language to choose the most likely language spoken is described in detail. The system has been trained on 5 languages: English, German, Japanese, Mandarin Chinese and Spanish. In this paper it is demonstrated that the performance of a LID system which is based on LVASR gives very good performance, when trained and tested on a 5 language subset (English, German, Spanish, Japanese, and Mandarin Chinese) of the Oregon Graduate Institute 11 language data base. The performance advantage is shown for both long (50 second) and short (10 second) test utterances. The five language results show 88% correct recognition for 50 second utterances with-out confidence measures and 98 % correct with confidence measures. The recognition rate is 81 % correct for 10 second utterances without confidence measures and 93 % correct with confidence measures. The best performance has been obtained for systems trained on phonetically hand labeled speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-452"
  },
  "teixeira96_icslp": {
   "authors": [
    [
     "Carlos",
     "Teixeira"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "António",
     "Serralheiro"
    ]
   ],
   "title": "Accent identification",
   "original": "i96_1784",
   "page_count": 4,
   "order": 455,
   "p1": "1784",
   "pn": "1787",
   "abstract": [
    "Foreign accent identification is a new challenging problem closely related to other relatively recent fields of the multilinguality area such as dialect identification and language identification. This paper describes an automatic identification system for English accents from 6 different European countries. The approachis basedon a parallel set of ergodic nets with context independent HMM units. The ergodic topology was also substituted by pronunciation transcription constraints in order to integrate accent specific automatic word recognisers. Considering the complexity of the task, the results can be considered encouraging for further research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-453"
  },
  "vuuren96_icslp": {
   "authors": [
    [
     "Sarel van",
     "Vuuren"
    ]
   ],
   "title": "Comparison of text-independent speaker recognition methods on telephone speech with acoustic mismatch",
   "original": "i96_1788",
   "page_count": 4,
   "order": 456,
   "p1": "1788",
   "pn": "1791",
   "abstract": [
    "We compare speaker recognition performance of Vector Quantization (VQ), Gaussian Mixture Modeling (GMM) and the Arithmetic Harmonic Sphericity measure (AHS) in adverse telephone speech conditions. The aim is to address the question: how do multimodal VQ and GMM typically compare to the simpler unimodal AHS for matched and mismatched training and testing environments. We study identification (closed set) and verification errors on a new multi-environment database. We consider LPC and PLP features as well as their RASTA derivatives. We conclude that RASTA processing can remove redundancies from the features. We affirm that even when we use channel and noise compensation schemes speaker recognition errors remain high when there is acoustic mismatch.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-454"
  },
  "yang96f_icslp": {
   "authors": [
    [
     "Xue",
     "Yang"
    ],
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "Iain",
     "Macleod"
    ]
   ],
   "title": "On the sources of inter- and intra-speaker variability in the acoustic dynamics of speech",
   "original": "i96_1792",
   "page_count": 4,
   "order": 457,
   "p1": "1792",
   "pn": "1795",
   "abstract": [
    "In this paper, we briefly report the experimental procedure and some results in testing our hypothesis that differences in size and shape of the vocal tract influence the dynamics of the formant trajectories of the speech signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-455"
  },
  "berkling96_icslp": {
   "authors": [
    [
     "Kay M.",
     "Berkling"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Language identification with inaccurate string matching",
   "original": "i96_1796",
   "page_count": 4,
   "order": 458,
   "p1": "1796",
   "pn": "1799",
   "abstract": [
    "We describe a system designed to recognize the language of an utterance spoken by any native speaker over the telephone. The current approach extends our previous work on language-identification based on sequences of speech units [2]. To improve performance we extend this work to allow for inaccurate matches of such sequences. Results are reported for distinguishing between English and German. The strength of this algorithm lies in the generalizability from training to test set. We have obtained a means of discriminating between languages based on statistical derivations. Matching sequences inaccurately in a controlled manner allows us to account for variabilities within languages without sacrificing cross language discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-456"
  },
  "carey96_icslp": {
   "authors": [
    [
     "M. J.",
     "Carey"
    ],
    [
     "E. S.",
     "Parris"
    ],
    [
     "H.",
     "Lloyd-Thomas"
    ],
    [
     "S. J.",
     "Bennett"
    ]
   ],
   "title": "Robust prosodic features for speaker identification",
   "original": "i96_1800",
   "page_count": 4,
   "order": 459,
   "p1": "1800",
   "pn": "1803",
   "abstract": [
    "This paper describes the use of prosodic features for speaker identification. Features based on the pitch and energy contours of speech are described and the relative importance of each feature for speaker identification is investigated. The mean and variance of the pitch period in voiced sections of speech are shown to be particularly useful at discriminating between speakers. Fusing these features with a Hidden Markov Model speaker identification system gave a marked improvement in figure of merit, over 30% gain was achieved on the six NIST 1995 Evaluation tests presented. Handset variability is known to have an adverse effect on performance when traditional spectral features are used e.g. cepstra. Results are presented showing that the prosodic features are more robust to handset variability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-457"
  },
  "monte96_icslp": {
   "authors": [
    [
     "E.",
     "Monte"
    ],
    [
     "J.",
     "Hernando"
    ],
    [
     "X.",
     "Miró"
    ],
    [
     "A.",
     "Adolf"
    ]
   ],
   "title": "Text independent speaker identification on noisy environments by means of self organizing maps",
   "original": "i96_1804",
   "page_count": 4,
   "order": 460,
   "p1": "1804",
   "pn": "1807",
   "abstract": [
    "In this paper we propose a new architecture for speaker recognition. This architecture is independent of the text, robust with the presence of noise, and is based on the Self Organizing Maps (SOM) [1]. We compare the performance of this architecture for different parametrizations, different signal to noise ratios, with another method for speaker identification based on the arithmetic-harmonic spherity measure on covariance matrices [2],[3].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-458"
  },
  "dalsgaard96_icslp": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "Hanne",
     "Hesselager"
    ],
    [
     "Bojan",
     "Petek"
    ]
   ],
   "title": "Language identification using language-dependent phonemes and language-independent speech units",
   "original": "i96_1808",
   "page_count": 4,
   "order": 461,
   "p1": "1808",
   "pn": "1811",
   "abstract": [
    "This paper reports on results from ongoing research on language-identification (LID) performed on the three languages: American-English, German and Spanish. The speech material used is from the Oregon Graduate Institute Spontaneous Telephone Speech Corpus, OGI_TS.\n",
    "The baseline LID-system consists of three parallel phoneme recognisers each of which are followed by three language modelling modules each characterising the bigram probabilities. The phoneme models used are derived on the basis of the combined speech corpus comprising the three languages. The phonemes are handled differently in analysis performed in two experiments. In the first experiment they are trained and tested language-specifically. In the second, they are separated into a number of groups, one of which contains those language-independent speech units which are similar enough to be equated across the training languages, the remaining containing the non-combinable language-dependent phonemes for each of the languages. A data-driven technique has been devised to separate the speech sounds contained within the training corpus into these groups.In order to prepare for an optimal separation between the input classes, a linear discriminant analysis is performed on the training speech material.\n",
    "Results from a number of experiments show that average language-identification scores of close to 90% can be retained by the LIDsystem presented here even for a high number of language-independent speech units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-459"
  },
  "scherer96_icslp": {
   "authors": [
    [
     "Klaus R.",
     "Scherer"
    ]
   ],
   "title": "Adding the affective dimension: a new look in speech analysis and synthesis",
   "original": "i96_1811",
   "page_count": 0,
   "order": 462,
   "p1": "1811",
   "pn": "",
   "abstract": [
    "This introduction to a special session on \"Emotion in recognition and synthesis\" highlights the need to understand the effects of affective speaker states on voice and speech on a psychophysiological level. It is argued that major advances in speaker verification, speech recognition, and natural-sounding speech synthesis depend on increases in our knowledge of the mechanisms underlying voice and speech production under emotional arousal or other attitudinal states, as well as on a more adequate understanding of listener decoding of affect from vocal quality. A brief review of the current state of the art is provided.\n",
    ""
   ]
  },
  "ohala96_icslp": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Ethological theory and the expression of emotion in the voice",
   "original": "i96_1812",
   "page_count": 4,
   "order": 463,
   "p1": "1812",
   "pn": "1815",
   "abstract": [
    "A useful source for unifying theories guiding research on the expression of emotions by the voice as well as by accompanying visual gestures (kinesics) is provided by ethology, the science devoted to the comparative study of behavior. Ethology, examining human and non-human behavior, maintains that much of behavior is shaped by phylogenetic adaptations. In this paper I will review and present evidence relevant to some of the fundamental theoretical issues addressed by ethology: · Does a signal of emotion reflect the inner (psycho)physiological state of the signaler or is it rather primarily designed to induce in receivers behavior that benefits the signaler? · Are there cross-cultural and cross-species similarities in emotional signals? · Are the separate components of emotional signals, e.g., voice quality, F0, as well as kinesic elements like eyebrow level, independent of each other or are they correlated in a way to mutually enhance the conveyance of a given message? · What is the basic vocabulary of emotions? How many are there?\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-460"
  },
  "murray96_icslp": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "John L.",
     "Arnott"
    ]
   ],
   "title": "Synthesizing emotions in speech: is it time to get excited?",
   "original": "i96_1816",
   "page_count": 4,
   "order": 464,
   "p1": "1816",
   "pn": "1819",
   "abstract": [
    "Modern speech synthesis systems with very high intelligibility are readily available in a number of languages. However, the output from all present systems is still readily identifiable as being machine-generated - the output does not sound \"natural\". One aspect of naturalness is the variability introduced by the emotional state of the speaker, and related pragmatic effects; no current commercial systems include such variation. Comparatively little work has been done to investigate how a speakers emotional state creates variation in the speech signal, and this work has traditionally been performed by psychologists and has remained distinct from mainstream speech science. Current research suggests that there will be considerable effort involved in producing any accurate description of pragmatic variations in speech, but there has recently been increasing interest in this area due to potential applications in many branches of speech technology. This paper describes a prototype system which has been constructed to simulate emotion in speech synthesized by rule. The system is based on emotion information from the literature, and it simulates a range of emotions using a commercial synthesiser. The use of emotion models and their applicability in the area of speech technology is discussed. The limitations of our current knowledge in the area of vocal emotion are discussed, and suggestions are presented for future research in this area.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-461"
  },
  "dellaert96_icslp": {
   "authors": [
    [
     "Frank",
     "Dellaert"
    ],
    [
     "Thomas",
     "Polzin"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Recognizing emotion in speech",
   "original": "i96_1970",
   "page_count": 4,
   "order": 465,
   "p1": "1970",
   "pn": "1973",
   "abstract": [
    "This paper explores several statistical pattern recognition techniques to classify utterances according to their emotional content. We have recorded a corpus containing emotional speech with over a 1000 utterances from different speakers. We present a new method of extracting prosodic features from speech, based on a smoothing spline approximation of the pitch contour. To make maximal use of the limited amount of training data available, we introduce a novel pattern recognition technique: majority voting of subspace specialists. Using this technique, we obtain classification performance that is close to human performance on the task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-462"
  },
  "heuft96b_icslp": {
   "authors": [
    [
     "Barbara",
     "Heuft"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Monika",
     "Rauth"
    ]
   ],
   "title": "Emotions in time domain synthesis",
   "original": "i96_1974",
   "page_count": 4,
   "order": 466,
   "p1": "1974",
   "pn": "1977",
   "abstract": [
    "A preliminary test exploring 4 emotions showed that conveying emotions by time domain synthesis may be possible. Therefore, a more sophisticated test was carried out in order to determine the influence of the prosodic parameters in the perception of a speaker's emotional state. Six different emotional states were investigated. The stimuli of the second test were used in three different testing procedures: as natural speech, resynthesized and reduced to a sawtooth signal. The recognition rates were lower than in the preliminary test, although the differences between the recognition rates of natural and synthetic speech were comparable for both tests. The outcome of the sawtooth test showed that the amount of information about a speaker's emotional state transported by F0, energy and overall duration is rather small. 0 However, we could determine relations between the acoustic prosodic parameters and the emotional content of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-463"
  },
  "arnfield96b_icslp": {
   "authors": [
    [
     "Simon",
     "Arnfield"
    ]
   ],
   "title": "Word class driven synthesis of prosodic annotations",
   "original": "i96_1978",
   "page_count": 3,
   "order": 467,
   "p1": "1978",
   "pn": "1980",
   "abstract": [
    "Prosody is an important aspect of speech that current text to speech synthesis systems fail to mimic in a convincing or natural way [1,2,3,4]. This paper describes research on a partial system for prosodic synthesis using easily derived low level syntactic information. A computer program has been developed that can annotate unseen text with prosodic stress and tone marks using the sequence of part of speech tags previously assigned to each word by a tagging system. Training and testing material was taken from the Lancaster/IBM Spoken English Corpus (SEC). Co-occurrence measures were calculated relating stress and tone mark annotations to the word class annotation information. A model was developed around the statistical information which calculates a score for all possible mappings between a given part of speech sequence and all the potential stress/tone annotations. The highest scoring pattern is selected as that which is the most likely \\baseline\" annotation, according to the model. Performance figures attain up to 91% agreement with the original corpus annotations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-464"
  },
  "banbrook96_icslp": {
   "authors": [
    [
     "M.",
     "Banbrook"
    ],
    [
     "S.",
     "McLaughlin"
    ]
   ],
   "title": "Dynamical modelling of vowel sounds as a synthesis tool",
   "original": "i96_1981",
   "page_count": 7,
   "order": 468,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "Speech synthesis can be produced using many varied techniques from formant/parametric synthesis to concatenation approaches. This paper presents a novel technique, based on the nonlinear dynamics of speech rather than than the time or frequency domain representations. It is demonstrated that the technique can be implemented effectively and used to produce high quality synthesised speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-465"
  },
  "johnstone96_icslp": {
   "authors": [
    [
     "Tom",
     "Johnstone"
    ]
   ],
   "title": "Emotional speech elicited using computer games",
   "original": "i96_1985",
   "page_count": 4,
   "order": 469,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "The potential of using computer games and simulations as elicitors of affective speech, particularly in research targeting current computer based speech technologies, is discussed. Initial results of acoustical analyses which have been performed on speech samples collected from 30 subjects playing an interactive computer game are presented. The game was modified to manipulate two of the stimulus evaluation checks (intrinsic pleasantness and goal conduciveness) which have been proposed by Scherer [1] as antecedents of emotion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-466"
  },
  "cowie96_icslp": {
   "authors": [
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Ellen",
     "Douglas-Cowie"
    ]
   ],
   "title": "Automatic statistical analysis of the signal and prosodic signs of emotion in speech",
   "original": "i96_1989",
   "page_count": 4,
   "order": 470,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "We highlight two broader domains surrounding specific attributions of emotion and the specific features of speech that underlie them, and argue for caution over compartmentalising these broader domains. It seems to be a general rule that variations in what we call the augmented prosodic domain (APD) are emotive - perhaps because they signal departure from a reference point corresponding to a well-controlled, neutral state. Our studies show that various departures from that reference point are reflected in the APD, including central and sensory impairments (schizophrenia and deafness) as well as emotion. Intuitively it seems right to acknowledge that departures from well-controlled neutrality are highly confusable, and it is unclear that phonetics should to try draw those distinctions more sharply than listeners tend to. A system called ASSESS automatically measures properties in the APD, opening the way to explore it in an empirical spirit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-467"
  },
  "lee96h_icslp": {
   "authors": [
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Wu",
     "Chou"
    ],
    [
     "J. J.",
     "Molina-Perez"
    ]
   ],
   "title": "A study on task-independent subword selection and modeling for speech recognition",
   "original": "i96_1820",
   "page_count": 4,
   "order": 471,
   "p1": "1820",
   "pn": "1823",
   "abstract": [
    "We study two key issues in task-independent training, namely selection of a universal set of subword units and modeling of the selected units. Since no a priori knowledge about the application vocabulary and syntax was used in the collection of the training corpus and the recognition task is frequently changing, the conventional strategy can no longer provide the best performance across many different tasks. We present a new approach that use the complete sets of right and left context-dependent units as the basis phone sets. Training of these models is accomplished by a new training criterion that maximizes phone separation between competing models. The proposed phone selection and modeling approach was evaluated across different tasks in American English. Good recognition results were obtained for both context-independent and context-dependent phone models even for unseen tasks. The same strategy has also been applied to two other languages, Mandarin Chinese and Spanish, with similar success.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-468"
  },
  "rahim96_icslp": {
   "authors": [
    [
     "Mazin G.",
     "Rahim"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Simultaneous ANN feature and HMM recognizer design using string-based minimum classification error (MCE) training",
   "original": "i96_1824",
   "page_count": 4,
   "order": 472,
   "p1": "1824",
   "pn": "1827",
   "abstract": [
    "Conventional features used in state-of-the-art hidden Markov model (HMM) based speech recognition systems are commonly inspired by scientific knowledge and expertise of the human vocal and auditory system. Although the intent when performing feature analysis is to extract \\relevant\" and \\discriminative\" information from the signal that is useful for speech recognition, this information may not be consistent with the objective of minimizing error rate in the recognition process. In this paper, we utilize feed-forward artificial neural networks (ANNs) to generate a new class of features for speech recognition. We propose a system for integrating the feature extraction process with the recognition process under a unified statistical framework with a consistent objective function that is designed to minimize recognition error rate. Results on a telephone-based speaker-independent connected digit task indicate that this integrated system with 12 ANNs is able to reduce the per digit error rate by a further 28% over a similar system using a single ANN and 16% over our previously best results in which feature transformation was not incorporated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-469"
  },
  "gupta96_icslp": {
   "authors": [
    [
     "Sunil K.",
     "Gupta"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Raziel",
     "Haimi-Cohen"
    ]
   ],
   "title": "Quantizing mixture-weights in a tied-mixture HMM",
   "original": "i96_1828",
   "page_count": 4,
   "order": 473,
   "p1": "1828",
   "pn": "1831",
   "abstract": [
    "In this paper, we describe new techniques to significantly reduce computational, storage and memory access requirements of a tied-mixture HMM based speech recognition system. Although continuous mixture HMMs offer improved recognition performance, we show that tied-mixture HMMs may offer significant advantage in complexity reduction for low-cost implementations. In particular, we consider two tasks: (a) connected digit recognition in car noise; and (b) sub-word modeling for command word recognition in a noisy office environment. We show that quantization of mixture weights can provide an almost three fold reduction in mixture-weight storage requirements without any significant loss in recognition performance. Furthermore, we show that by combining mixture-weight quantization with techniques such as VQ-Assist the computational and memory access requirements can be reduced by almost 60-80% without any degradation in recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-470"
  },
  "gales96_icslp": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "D.",
     "Pye"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Variance compensation within the MLLR framework for robust speech recognition and speaker adaptation",
   "original": "i96_1832",
   "page_count": 4,
   "order": 474,
   "p1": "1832",
   "pn": "1835",
   "abstract": [
    "This paper investigates the use of maximum likelihood linear regression (MLLR) for both speaker and environment adaptation. MLLR transforms the mean and variance parameters of a set of HMMs. In this paper a number of different types of linear transformations of the variances are examined including full, block diagonal, and diagonal transformation matrices. Experiments on large vocabulary speaker independent data sets are described. On all the data sets examined the use of MLLR mean and variance compensation reduced the error rate compared to mean-only compensation. Furthermore, the use of a block diagonal or full transformation of the variances on the clean data task showed slight improvements over the diagonal case. However, when some environmental mismatch was present there was no difference in performance between using multiple diagonal variance transformations and a more complex single variance transform.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-471"
  },
  "surendran96_icslp": {
   "authors": [
    [
     "A. C.",
     "Surendran"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Mazin G.",
     "Rahim"
    ]
   ],
   "title": "Maximum-likelihood stochastic matching approach to non-linear equalization for robust speech recognition",
   "original": "i96_1836",
   "page_count": 4,
   "order": 475,
   "p1": "1836",
   "pn": "1839",
   "abstract": [
    "In this paper we present a new technique in the stochastic matching framework to compensate for non-linear distortions in speech recognition. The features of the test data and the means of the trained model are both transformed using neural networks to better fit each other. The parameters of the neural network are estimated using a novel combination of the generalized EM (GEM) and the back-propagation algorithms. In the feature transformation case, when the exact Q-functions cannot be calculated, approximations are heuristically derived. The mathematical properties of the new algorithm are analyzed. The performance of the algorithm is also studied under different mismatch conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-472"
  },
  "chien96_icslp": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ],
    [
     "Lee-Min",
     "Lee"
    ]
   ],
   "title": "Estimation of channel bias for telephone speech recognition",
   "original": "i96_1840",
   "page_count": 4,
   "order": 476,
   "p1": "1840",
   "pn": "1843",
   "abstract": [
    "In this study, we propose a maximum a posterior (MAP) estimation of channel bias to compensate the channel mismatch in telephone speech recognition. For a telephone speech, the channel bias is estimated by maximizing a posterior probability. Because a posterior probability is composed of a likelihood function and a prior density, we introduce a scale factor to evaluate their weights in MAP estimation. To further improve the performance, a prior channel statistics is extended to multiple components and the channel mismatch is separately compensated for different segments. Besides, a rapid MAP estimation applied in feature domain is also proposed for reducing the computational complexity. Experiments show that proposed method can significantly improve recognition rates and computational complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-473"
  },
  "johnson96_icslp": {
   "authors": [
    [
     "M. E.",
     "Johnson"
    ]
   ],
   "title": "Synthesis of English intonation using explicit models of reading and spontaneous speech",
   "original": "i96_1844",
   "page_count": 4,
   "order": 477,
   "p1": "1844",
   "pn": "1847",
   "abstract": [
    "A model of English intonation is presented in which a variety of intonation contours can be generated from a quantitative prominence labelling of stressed syllables. In one style of speech production, spontaneous speech, a short-lookahead model can generate a variety of contours from the same quantitative prominence labelling. For another style, reading aloud, a long-lookahead model determines the types of intonation patterns associated with texts. These typically come out as sequences of downward-stepping contours, given appropriate initial conditions (though there is no explicit down-step constant in the model). In both styles, the intonation contours are generated on the basis of a quantitative model of contour pitch prominence, in which the pitch prominence of the contour segments which make up the accent contours (and thence the intonation contours) is computed as a nonlinear function of the duration of the contour segment, the ratio of the F0 value at one end of the segment to that at the other, and a rhythm constant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-474"
  },
  "horne96_icslp": {
   "authors": [
    [
     "Merle",
     "Horne"
    ],
    [
     "Marcus",
     "Filipsson"
    ]
   ],
   "title": "Implementation and evaluation of a model for synthesis of Swedish intonation",
   "original": "i96_1848",
   "page_count": 4,
   "order": 478,
   "p1": "1848",
   "pn": "1851",
   "abstract": [
    "An abstract prosodic structure is implemented in terms of acoustic parameters realizing underlying word accents and boundary markers. The system is further evaluated in order to determine whether listeners prefer intonation contours produced with 1) default focal accent placement (sentence stress) on the last content word in each prosodic phrase vs focal accent assigned to the last new content word in each prosodic phrase, 2) minimal prosodic boundary signalling at commas and full stops vs more detailed prosodic boundary signalling generated using an underlying prosodic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-475"
  },
  "katae96_icslp": {
   "authors": [
    [
     "Nobuyuki",
     "Katae"
    ],
    [
     "Shinta",
     "Kimura"
    ]
   ],
   "title": "Natural prosody generation for domain specific text-to-speech systems",
   "original": "i96_1852",
   "page_count": 4,
   "order": 479,
   "p1": "1852",
   "pn": "1855",
   "abstract": [
    "This paper proposes a new method for generating natural prosody for text-to-speech systems. In this method, sentences are composed by inlaying a variable word into each slot in prepared sentence structures. This method can be used for domain specific text-to-speech applications that dont require so many number of sentence structures but many words. Important parameters for prosody are declination of F0 contour, accent strength of word, and position and duration of pause. So we construct a database containing these parameters manually extracted from natural speech samples that have the sentence structures to be synthesized. In the process of prosody generation, these parameters are retrieved by the type of the sentence structure, and the other parameters are generated with rules. The F0 contour is generated by superposing these components on baseline frequency. Mean opinion score of prosody naturalness for the speech synthesized by the proposed method is 3.6. This is 1.2 points better than that of the former method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-476"
  },
  "tatham96_icslp": {
   "authors": [
    [
     "Mark",
     "Tatham"
    ],
    [
     "Eric",
     "Lewis"
    ]
   ],
   "title": "Improving text-to-speech synthesis",
   "original": "i96_1856",
   "page_count": 4,
   "order": 480,
   "p1": "1856",
   "pn": "1859",
   "abstract": [
    "Naturalness in human speech is dependent on a number of factors and the extent to which a text-to-speech synthesis system can account for these factors in its model will be a measure of its success in the marketplace. As well as the obvious factors of rhythm and intonation there is the more difficult question of modelling the variability in human speech. This paper discusses how SPRUCE [1], a high-level text-to-speech synthesis system, incorporates several different types of variability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-477"
  },
  "boughazale96_icslp": {
   "authors": [
    [
     "Sahar E.",
     "Bou-Ghazale"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Synthesis of stressed speech from isolated neutral speech using HMM-based models",
   "original": "i96_1860",
   "page_count": 4,
   "order": 481,
   "p1": "1860",
   "pn": "1863",
   "abstract": [
    "In this study, a novel approach is proposed for modeling speech parameter variations between neutral and stressed conditions and employed in a technique for stressed speech synthesis. The proposed method consists of modeling the variations in pitch contour, voiced speech duration, and average spectral structure using Hidden Markov Models (HMMs). While HMMs have traditionally been used for recognition applications, here they are used to statistically model characteristics needed for generating pitch contour and spectral slope patterns to modify the speaking style of isolated neutral words. An algorithm is developed based on an analysis-synthesis speech model, and HMM pitch and spectral stress characteristics for stress perturbation. Informal listener evaluations of the stress modified speech confirm the HMMs ability to capture the parameter variations under stressed conditions. The proposed HMM models are both speaker and word-independent, but unique to each speaking style. While the modeling scheme is applicable to a variety of stress and emotional speaking styles, the evaluations presented in this study focus on angry, Lombard effect, and loud spoken speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-478"
  },
  "dobnikar96_icslp": {
   "authors": [
    [
     "Ales",
     "Dobnikar"
    ]
   ],
   "title": "Modeling segment intonation for Slovene TTS system",
   "original": "i96_1864",
   "page_count": 4,
   "order": 482,
   "p1": "1864",
   "pn": "1867",
   "abstract": [
    "A scheme for modeling the F0 contour for different types of intonation units for the Slovene language is presented. It is based on results of analyzing F0 contours, using a quantitative model. Data from ten speakers was collected, resulting in a large corpora, mainly of declarative sentences. A way of generating the F0 contour for given utterances was defined, using only the text of the utterance as input. Near-to-natural synthesized F0 contour was obtained by rules which regard the F0 contour as the sum of global and local components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-479"
  },
  "shriberg96b_icslp": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Word predictability after hesitations: a corpus-based study",
   "original": "i96_1868",
   "page_count": 4,
   "order": 483,
   "p1": "1868",
   "pn": "1871",
   "abstract": [
    "We ask whether lexical hesitations in spontaneous speech tend to precede words that are difficult to predict. We define predictability in terms of both transition probability and entropy, in the context of an N-gram language model. Results show that transition probability is significantly lower at hesitation transitions, and that this is attributable to both the following word and the word history. In addition, results suggest that fluent transitions in sentences with a hesitation elsewhere are significantly more likely than transitions in fluent sentences to contain out-of-vocabulary words and novel word combinations. Such findings could be used to improve statistical language modeling for spontaneous-speech applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-480"
  },
  "yang96g_icslp": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Interruptions and intonation",
   "original": "i96_1872",
   "page_count": 4,
   "order": 484,
   "p1": "1872",
   "pn": "1875",
   "abstract": [
    "In this paper we examine how interruptions are manifested in the intonational structure of a discourse. Such interactions come about because of the mutual negotiating to satisfy the differing needs, interests and knowledge states of participants in a conversation. The specific pitch height of an interruption is found to be determined jointly by the need to attract attention, the intensity of the emotion present, and the strength of signal needed to overcome the attention and focus on the current topic.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-481"
  },
  "lickley96_icslp": {
   "authors": [
    [
     "Robin J.",
     "Lickley"
    ],
    [
     "Ellen Gurman",
     "Bard"
    ]
   ],
   "title": "On not recognizing disfluencies in dialogue",
   "original": "i96_1876",
   "page_count": 4,
   "order": 485,
   "p1": "1876",
   "pn": "1879",
   "abstract": [
    "This paper tests the hypothesis that listeners miss disfluencies or fail to transcribe them accurately because disfluencies interfere with the normal relationship between speech sound and linguistic context in human spoken word recognition. In a word-level gating experiment 16 listeners heard a total of 56 disfluent utterances selected from a corpus of spontaneous speech, 56 length-matched fluent controls, and 56 fluent foils. The proportion of words never recognized was greater in disfluent utterances than in controls. The failures clustered around the point where the disfluency interrupted the utterance, ocurring particularly within the reparanda, but were not found at corresponding locations in uninterrupted controls. Repetition disfluencies, where pre-and post-interruption portions might easily be construed together, allowed more successful word recognitions than recast disfluencies, where reconstruction of a single intended utterance would be difficult, if not impossible. The results have implications both for understanding human speech recognition and for improving the robustness of ASR systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-482"
  },
  "garner96_icslp": {
   "authors": [
    [
     "Phil",
     "Garner"
    ],
    [
     "Sue",
     "Browning"
    ],
    [
     "Roger",
     "Moore"
    ],
    [
     "Martin",
     "Russell"
    ]
   ],
   "title": "A theory of word frequencies and its application to dialogue move recognition",
   "original": "i96_1880",
   "page_count": 4,
   "order": 486,
   "p1": "1880",
   "pn": "1883",
   "abstract": [
    "Dialogue move recognition is taken as being representative of a class of spoken language applications where inference about high level semantic meaning is required from lower level acoustic, phonetic or word based features. Topic identification is another such application. In the particular case of inference from words, the multinomial distribution is shown to be inadequate for modelling word frequencies, and the multivariate Poisson is a more reasonable choice. Zipf's law is used to model a prior distribution. This more rigorous mathematical formulation is shown to improve dialogue move classiffcation both subjectively and quantitatively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-483"
  },
  "traum96_icslp": {
   "authors": [
    [
     "David R.",
     "Traum"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "Utterance units and grounding in spoken dialogue",
   "original": "i96_1884",
   "page_count": 6,
   "order": 487,
   "p1": "1884",
   "pn": "1887",
   "abstract": [
    "Defining an utterance unit in spoken dialogue has remained a difficult issue. To shed light on this question,we consider grounding behavior in dialogue, and examine co-occurrences between turn-initial grounding acts and utterance unit signals that have been proposed in the literal, namely prosodic boundary tones and pauses. Preliminary results indicate high correlation between grounding and boundary tones, with a secondary correlation for longer pauses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-484"
  },
  "novick96_icslp": {
   "authors": [
    [
     "David G.",
     "Novick"
    ],
    [
     "Brian",
     "Hansen"
    ],
    [
     "Karen",
     "Ward"
    ]
   ],
   "title": "Coordinating turn-taking with gaze",
   "original": "i96_1888",
   "page_count": 4,
   "order": 488,
   "p1": "1888",
   "pn": "1891",
   "abstract": [
    "This paper explores the role of gaze in coordinating turn-taking in mixed-initiative conversation and specifically how gaze indicators might be usefully modeled in computational dialogue systems. We analyzed about 20 minutes of videotape of eight dialogues by four pairs of subjects performing a simple face-to-face cooperative laboratory task. We extend previous studies by explicating gaze patterns in face-to-face conversations, formalizing the most frequent pattern as a computational model of turn-taking, and testing the model through an agent-based simulation. Prior conversation simulations of conversational control acts relied on abstract speech-act representations of control. This study advances the computational account of dialogue through simulation of direct physical expression of gaze to coordinate conversational turns.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-485"
  },
  "roach96_icslp": {
   "authors": [
    [
     "Peter",
     "Roach"
    ],
    [
     "Simon",
     "Arnfield"
    ],
    [
     "William J.",
     "Barry"
    ],
    [
     "J.",
     "Baltova"
    ],
    [
     "Marian",
     "Boldea"
    ],
    [
     "Adrian",
     "Fourcin"
    ],
    [
     "W.",
     "Gonet"
    ],
    [
     "Ryszard",
     "Gubrynowicz"
    ],
    [
     "E.",
     "Hallum"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Krzysztof",
     "Marasek"
    ],
    [
     "Alain",
     "Marchal"
    ],
    [
     "E.",
     "Meister"
    ],
    [
     "Klára",
     "Vicsi"
    ]
   ],
   "title": "BABEL: an eastern european multi-language database",
   "original": "i96_1892",
   "page_count": 2,
   "order": 489,
   "p1": "1892",
   "pn": "1893",
   "abstract": [
    "BABEL is a joint European project under the COPERNICUS scheme (Project #1304) comprising partners from five Eastern European countries and three Western ones. The project is producing a multi-language database of five of the most widely-differing Eastern European languages. The collection and formatting of the data conforms to the protocols established by the ESPRIT SAM project and the resulting EUROM databases.\n",
    ""
   ]
  },
  "wang96h_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Deyu",
     "Xia"
    ],
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Bicheng",
     "Liu"
    ]
   ],
   "title": "USTC95---a putonghua corpus",
   "original": "i96_1894",
   "page_count": 4,
   "order": 490,
   "p1": "1894",
   "pn": "1897",
   "abstract": [
    "A large Putonghua corpus is introduced, which is primarily designed to support research in Chinese speech recognition, analysis and recognition system evaluation. This corpus consists of four major sub-corpora corresponding to isolated syllables, multi-syllable words, sentences, and telephone speech. With an elaborate design, the corpus encompasses all the phones and mono-syllables, as well as the co-articulation effects in the Putonghua; besides, keeps as little redundancy as possible. This parsimonious corpus makes it possible to acquire acoustic-phonetic knowledge for isolated words recognition and continuous Chinese recognition, to provide speech data for training telephone speech recognizer, also to provide a common test base for the performance assessment of recognizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-486"
  },
  "hurley96_icslp": {
   "authors": [
    [
     "Edward",
     "Hurley"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Telephone data collection using the world wide web",
   "original": "i96_1898",
   "page_count": 4,
   "order": 491,
   "p1": "1898",
   "pn": "1901",
   "abstract": [
    "Over the past year our group has begun development of telephone-based speech understanding capability for our GALAXY conversational system. An important part of this process has been the collection of telephone speech which was used for training and evaluation. In the first phase of data collection our goal was to collect read speech from a wide variety of talkers, telephone handsets, and noise/channel conditions. In the second phase of data collection our additional goal was to collect spontaneous telephone speech from subjects actually using the system. In order to maximize variation in telephone conditions, as well as ease of use for subjects, the data collection software was designed to telephone-subjects at their specified phone numbers around North America. Subjects initiate the data collection session by submitting an electronic form accessible by a WWW browser. For read speech collection, a set of prompts is automatically generated for the subject. This paper describes the design of the data collection system we are using for these purposes. To date we have collected over 9,000 utterances from over 270 subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-487"
  },
  "falcone96_icslp": {
   "authors": [
    [
     "M.",
     "Falcone"
    ],
    [
     "A.",
     "Gallo"
    ]
   ],
   "title": "The \"SIVA\" speech database for speaker verification: description and evaluation",
   "original": "i96_1902",
   "page_count": 4,
   "order": 492,
   "p1": "1902",
   "pn": "1905",
   "abstract": [
    "The description and characterization of the Italian speech database SIVA is given. After a brief review of the available corpora designed for speaker verification task, we introduce the \"Speaker Identification and Verification Archives: SIVA\", a database that consists actually of more than two thousands calls, collected over the public switched telephone network. A detailed description of speech material, a proposal for an acoustic characterization, and the performances obtained using a speaker verification reference system are presented and discussed herein after.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-488"
  },
  "draxler96_icslp": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "A multi-level description of date expressions in German telephone speech",
   "original": "i96_1906",
   "page_count": 4,
   "order": 493,
   "p1": "1906",
   "pn": "1909",
   "abstract": [
    "The ability to efficiently and reliably process date expressions is crucial to many speech-based applications. A multi-level description which contains the syntactic deep structure, the orthographic surface form, a citation form representation, and the phonetic transcription is proposed. With this representation, highlevel constraints, e.g. on syntactic structure, can be exploited to restrict the search space on the lower levels. This multi-level representation is applied to read and spontaneous date expressions of the German SpeechDat(M) telephone speech corpus, and the results of an analysis of the transcriptions of 3000 recordings are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-489"
  },
  "halstead96_icslp": {
   "authors": [
    [
     "Robert H. Jr.",
     "Halstead"
    ],
    [
     "Ben",
     "Serridge"
    ],
    [
     "Jean-Manuel Van",
     "Thong"
    ],
    [
     "William",
     "Goldenthal"
    ]
   ],
   "title": "Viterbi search visualization using vista: a generic performance visualization tool",
   "original": "i96_1910",
   "page_count": 4,
   "order": 494,
   "p1": "1910",
   "pn": "1913",
   "abstract": [
    "The capability to view detailed events taking place inside of a speech search engine can be essential for reducing the computational complexity and enhancing the accuracy of a speech recognition system. This paper describes a generic search visualization and performance tuning tool, Vista, which permits a user to interactively examine data within the search module of a speech recognition system. Vista's key attributes include the ability to display a large number of active search paths simultaneously, to display data relevant to these paths, and to update the paths and data as the search progresses in time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-490"
  },
  "altosaar96_icslp": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "A multilingual phonetic representation and analysis system for different speech databases",
   "original": "i96_1914",
   "page_count": 4,
   "order": 495,
   "p1": "1914",
   "pn": "1917",
   "abstract": [
    "A multilingual phonetic representation and analysis system for different speech databases is presented. The need for such a system is first justified and then one is proposed based on the Worldbet phonetic alphabet. A phonetic class hierarchy is developed and a description of the hierarchical structural representation follows. Database access is based on the latter and is accomplished by defining predicate search functions and applying them to a database. Immediate signal analysis of the results is possible since the multilingual phonetic representation system is seamlessly integrated into a digital signal processing environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-491"
  },
  "langmann96_icslp": {
   "authors": [
    [
     "D.",
     "Langmann"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "E. den",
     "Os"
    ]
   ],
   "title": "FRESCO: the French telephone speech data collection - part of the european Speechdat(m) project",
   "original": "i96_1918",
   "page_count": 4,
   "order": 496,
   "p1": "1918",
   "pn": "1921",
   "abstract": [
    "This paper describes the design, collection and postprocessing of the French SpeechDat corpus FRESCO. Being a database of approximately 35,000 utterances recorded from 1000 callers over the terrestrial telephone network in France, it comprises immediately usable and relevant speech for the initial training and assessment of speaker-independent phoneme-model or word-model based speech recognizers, as they are employed in automated telephone services. FRESCO is one of the 1000- speaker telephone speech databases produced as \"case studies\" within the European project SpeechDat(M).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-492"
  },
  "muller96b_icslp": {
   "authors": [
    [
     "Johannes",
     "Müller"
    ],
    [
     "Holger",
     "Stahl"
    ],
    [
     "Manfred",
     "Lang"
    ]
   ],
   "title": "Predicting the out-of-vocabulary rate and the required vocabulary size for speech processing applications",
   "original": "i96_1922",
   "page_count": 4,
   "order": 497,
   "p1": "1922",
   "pn": "1925",
   "abstract": [
    "This paper describes an approach for predicting both the vocabulary size and the resulting out-of-vocabulary rate (OOV-rate) for a hypothetical extension of an existing text corpus. By splitting the original corpus into two different sub-corpora, vocabulary and OOV-rate can be determined for that special constellation. Average values are calculated for all combinations of sub-corpora and can be approximated by analytic function terms. These functions enable the easy prediction of the vocabulary size and the OOV-rate. The prediction accuracy results in a relative error below 4.6%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-493"
  },
  "parlangeau96_icslp": {
   "authors": [
    [
     "Nathalie",
     "Parlangeau"
    ],
    [
     "Alain",
     "Marchal"
    ]
   ],
   "title": "AMULET: automatic MUltisensor speech labelling and event tracking: study of the spatio-temporal correlations in voiceless plosive production",
   "original": "i96_1926",
   "page_count": 4,
   "order": 498,
   "p1": "1926",
   "pn": "1929",
   "abstract": [
    "Speech production is a complex process relying on coordinated gestures, but the acoustic signal does not depict its underlaying organization. Accepting that articulatory gestures are directly recognized through the coarticulation process, our proposal is to investigate the correlations between acoustic and articulatory informations and to assess gestural phonetic theory. We present here the framework for this investigation, the automatic articulatory labelling of the multi-sensor speech database ACCOR, and the study of the spatio-temporal correlations in the voiceless plosive production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-494"
  },
  "hahn96_icslp": {
   "authors": [
    [
     "Minsoo",
     "Hahn"
    ],
    [
     "Sanghun",
     "Kim"
    ],
    [
     "Jung-Chul",
     "Lee"
    ],
    [
     "Yong-Ju",
     "Lee"
    ]
   ],
   "title": "Constructing multi-level speech database for spontaneous speech processing",
   "original": "i96_1930",
   "page_count": 4,
   "order": 499,
   "p1": "1930",
   "pn": "1933",
   "abstract": [
    "This paper describes a new database, called muti-level speech database, for spontaneous speech processing. We designed the database to cover textual and acoustic variations from declarative speech to spontaneous speech. The database is composed of 5 categories which are, in the order of decreasing spontaneity, spontaneous speech, interview, simulated interview, declarative speech with context, and declarative speech without context. We collected total 112 sets from 23 subjects(male: 19, female: 4). Then the database was firstly transcribed using 15 transcription symbols according to our own transcription rules. Secondly, prosodic information will be added. The goal of this research is a comparative textual and prosodic analysis at each level, quantification of spontaneity of diversified speech database for dialogue speech synthesis and recognition. From the preliminary analysis of transcribed texts, the spontaneous speech has more corrections, repetitions, and pauses than the others as expected. In addition, average number of sentences per turn of spontaneous speech is greater than the others. From the above results, we can quantify the spontaneity of speech database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-495"
  },
  "boldea96_icslp": {
   "authors": [
    [
     "Marian",
     "Boldea"
    ],
    [
     "Alin",
     "Doroga"
    ],
    [
     "Tiberiu",
     "Dumitrescu"
    ],
    [
     "Maria",
     "Pescaru"
    ]
   ],
   "title": "Preliminaries to a romanian speech database",
   "original": "i96_1934",
   "page_count": 4,
   "order": 500,
   "p1": "1934",
   "pn": "1937",
   "abstract": [
    "This paper presents the design and early recording stages of a Romanian speech database to be used for development of both speech recognition and speech synthesis systems. The recognition part is built around a core patterned after the EUROM_1 [4] design, so that an as good as possible compatibility to exist with this, and includes both read and semispontaneous speech. The synthesis part consists of a read speech corpus from which diphones are to be extracted to build concatenation-based TTS systems, and read material to serve as benchmark data for the administration of a Romanian version of the Modified Rhyme Test [2].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-496"
  },
  "kohler96_icslp": {
   "authors": [
    [
     "Klaus J.",
     "Kohler"
    ]
   ],
   "title": "Labelled data bank of spoken standard German the kiel corpus of read/spontaneous speech",
   "original": "i96_1938",
   "page_count": 8,
   "order": 501,
   "p1": "1938",
   "pn": "1941",
   "abstract": [
    "This paper outlines the successive steps in the setting up of a labelled data bank of German read and spontaneous speech at IPDS Kiel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-497"
  },
  "hetherington96_icslp": {
   "authors": [
    [
     "Lee",
     "Hetherington"
    ],
    [
     "Michael",
     "McCandless"
    ]
   ],
   "title": "SAPPHIRE: an extensible speech analysis and recognition tool based on tcl/tk",
   "original": "i96_1942",
   "page_count": 4,
   "order": 502,
   "p1": "1942",
   "pn": "1945",
   "abstract": [
    "The SAPPHIRE system is a powerful, extensible, object-oriented toolkit allowing researchers to rapidly build and configure customized speech analysis tools. Implemented in Tcl/Tk and C, the current version of SAPPHIRE provides a wide range of functionality, including the ability to configure and run the SUMMIT speech recognition system. We now use SAPPHIRE widely in almost all aspects of our speech analysis and recognition research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-498"
  },
  "kiyama96_icslp": {
   "authors": [
    [
     "Jiro",
     "Kiyama"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Automatic detection of topic boundaries and keywords in arbitrary speech using incremental reference interval-free continuous DP",
   "original": "i96_1946",
   "page_count": 4,
   "order": 503,
   "p1": "1946",
   "pn": "1949",
   "abstract": [
    "We propose a new approach for detecting topic boundaries and keywords in arbitrary speech, with neither recognition nor prosodic processing, aiming at quick access to the content of recorded raw speech. This approach is based on the general tendency that frequently-repeated phrases/words in speech are characteristic of topics in discourse, so it uses pairs of phonetically similar segments (PPSSs) of speech to represent topics in speech. This approach has the advantage of being domain and language-independent and robust against variations in the speaker and background noise, as it needs neither a language nor acoustic model in advance. Experiments using simulated dialogues confirmed the good performance of this approach. We also propose Incremental Reference Interval-free Continuous Dynamic Programming (IRIFCDP) as an algorithm for detecting PPSSs in speech for the above method. IRIFCDP can detect PPSSs efficiently in synchronization with the speech, so it is suitable for handling long speech samples.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-499"
  },
  "bai96_icslp": {
   "authors": [
    [
     "Bo-Ren",
     "Bai"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Very-large-vocabulary Mandarin voice message file retrieval using speech queries",
   "original": "i96_1950",
   "page_count": 4,
   "order": 504,
   "p1": "1950",
   "pn": "1953",
   "abstract": [
    "In order to solve the problem with the new environment of fast growth of audio resources on the Internet, this paper presents a new approach which is capable of retrieving Mandarin voice message files using queries of unconstrained speech. By properly utilizing the monosyllabic structure of the Chinese language, the proposed approach performs the statistical similarity estimation between the speech queries and the voice message files, and executes the complete matching process directly at the phonetic level using syllable-based statistical information. Based on this approach, some experiments are tested and encouraging results are demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-500"
  },
  "melin96_icslp": {
   "authors": [
    [
     "H.",
     "Melin"
    ]
   ],
   "title": "Gandalf - a Swedish telephone speaker verification database",
   "original": "i96_1954",
   "page_count": 4,
   "order": 505,
   "p1": "1954",
   "pn": "1957",
   "abstract": [
    "The Gandalf speech database has been designed for use in research on automatic speaker verification. 86 customer speakers have been recorded in up to 24 telephone calls per speaker during a period of up to 12 months, and an additional 100 impostor speakers are currently being recorded. In addition to speech files, Gandalf includes a relational database with a twofold function: it stores information on subjects and calls, and it is a tool for making quantitative and qualitative analyses of speaker verification test data. The customer speaker part of the database is described, and some of the motivation behind the design is given. A small speaker verification experiment is then described that demonstrates how test results can be qualitatively analyzed using the relational database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-501"
  },
  "bard96_icslp": {
   "authors": [
    [
     "Ellen Gurman",
     "Bard"
    ],
    [
     "C.",
     "Sotillo"
    ],
    [
     "A. H.",
     "Anderson"
    ],
    [
     "M. M.",
     "Taylor"
    ]
   ],
   "title": "The DCIEM map task corpus: spontaneous dialogue under sleep deprivation and drug treatment",
   "original": "i96_1958",
   "page_count": 4,
   "order": 506,
   "p1": "1958",
   "pn": "1961",
   "abstract": [
    "This paper describes a resource for the study of spontaneous speech under stress, a corpus of 216 unscripted task-oriented dialogues conducted by normal Canadian adults in the course of a sleep deprivation experiment under 3 drug conditions. Speakers carried out the route-communication task (see [1]) in alternation with a battery of other tasks over a 6-day study which included a 60-hour sleepless period. Each speaker participated in 12 dialogues. The design permits comparisons within speakers for sleep deprivation (baseline, deprived, post-recovery), and between speakers for drug condition (placebo, d-amphetamine, Modafinil) and number of conversational partners encountered (1, 2). Preliminary examination of dialogue length, task performance, and aspects of dialogue strategy indicate effects of all these variables. Effects of sleep-deprivation and drug condition are less severe than those found in simpler tasks [7].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-502"
  },
  "menendezpidal96_icslp": {
   "authors": [
    [
     "Xavier",
     "Menéndez-Pidal"
    ],
    [
     "James B.",
     "Polikoff"
    ],
    [
     "Shirley M.",
     "Peters"
    ],
    [
     "Jennie E.",
     "Leonzio"
    ],
    [
     "H. T.",
     "Bunnell"
    ]
   ],
   "title": "The nemours database of dysarthric speech",
   "original": "i96_1962",
   "page_count": 4,
   "order": 507,
   "p1": "1962",
   "pn": "1965",
   "abstract": [
    "The Nemours database is a collection of 814 short nonsense sentences; 74 sentences spoken by each of 11 male speakers with varying degrees of dysarthria. Additionally, the database contains two connected-speech paragraphs produced by each of the 11 speakers. The database was designed to test the intelligibility of dysarthric speech before and after enhancement by various signal processing methods, and is available on CD-ROM. It can also be used to investigate general characteristics of dysarthric speech such as production error patterns. The entire database has been marked at the word level and sentences for 10 of the 11 talkers have been marked at the phoneme level as well. This paper describes the database structure and techniques adopted to improve the performance of a Discrete Hidden Markov Model (DHMM) labeler used to assign initial phoneme labels to the elements of the database. These techniques may be useful in the design of automatic recognition systems for persons with speech disorders, especially when limited amounts of training data are available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-503"
  },
  "hennebert96_icslp": {
   "authors": [
    [
     "Jean",
     "Hennebert"
    ],
    [
     "Dijana Petrovska",
     "Delacrétaz"
    ]
   ],
   "title": "POST: parallel object-oriented speech toolkit",
   "original": "i96_1966",
   "page_count": 4,
   "order": 508,
   "p1": "1966",
   "pn": "1969",
   "abstract": [
    "We give a short overview of POST, a parallel speech toolkit that is distributed freeware to academic institutions. The underlying idea of POST is that large computational problems, like the ones involved in Automatic Speech Recognition (ASR), can be solved more cost effectively by using the aggregate power and memory of many computers. in its current version (January 96) and amongst other things, POST can perform simple feature extraction, training and testing of word and subword Hidden Markov Models (HMM) with discrete and multigaussian statistical modelling. In this paper, the implementation of the parallelism is discussed and an evaluation of the performances on a telephone database is presented. A short introduction to Parallel Virtual Machine (PVM), the library through which the parallelism is achieved, is also given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-504"
  },
  "zhang96b_icslp": {
   "authors": [
    [
     "Xiaoyu",
     "Zhang"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "Channel and noise normalization using affine transformed cepstrum",
   "original": "i96_1993",
   "page_count": 4,
   "order": 509,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "This paper addresses the environmental mismatch problem that arises from noise and channel variabilities. A new feature mapping technique based on an optimal affine transform of the cepstrum is proposed to solve the mismatch problem observed over the speaker recognition systems. It is designed based on the fact that both the channel and noise interferences basically cause the cepstrum space to undergo an affine transformation. By taking an inverse transformation, we can easily decouple from the speech the effects of the channel and noise. Alternatively, we can take a forward transform of the training data to simulate the operating conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-505"
  },
  "claes96_icslp": {
   "authors": [
    [
     "Tom",
     "Claes"
    ],
    [
     "Fei",
     "Xie"
    ],
    [
     "Dirk van",
     "Compernolle"
    ]
   ],
   "title": "Spectral estimation and normalisation for robust speech recognition",
   "original": "i96_1997",
   "page_count": 4,
   "order": 510,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "Speech recognition in adverse conditions remains a difficult but challenging problem. It is already shown [1] that normalisation of the dynamic range (SNR1) of the frequency channels in a mel scale triangular filterbank (MFCC) [2], improves the robustness against both additive and convolutional noise. Nevertheless, because the method is based on a masking-technique, the improvement is small in the case of SNR values that are smaller than the target (normalised) SNR. A solution for this problem can be found in first enhancing the filterbank energies before the masking-technique is applied. For this purpose we developed a Non-linear Spectral Estimator (NSE) for speech recognition that operates on the log filterbank energies. NSE enhances these filterbank energies and makes use of SNR-normalisation also effective at very low SNRs. Experimental results are given on the NOISEX-92 [3] database. Better recognition performance is seen even at 0dB SNR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-506"
  },
  "chou96b_icslp": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Nambi",
     "Seshadri"
    ],
    [
     "Mazin G.",
     "Rahim"
    ]
   ],
   "title": "Trellis encoded vector quantization for robust speech recognition",
   "original": "i96_2001",
   "page_count": 4,
   "order": 511,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "In this paper, a joint data (features) and channel (bias) estimation framework for robust speech recognition is described. A trellis encoded vector quantizer is used as a pre-processor to estimate the channel bias using blind maximum likelihood sequence estimation. Sequential constraint in the feature vector sequence is explored and used in two ways, namely, a) the selection of the quantized signal constellation, b) the decoding process in joint data and channel estimation. A two state trellis encoded vector quantizer is designed for signal bias removal applications. Comparing with the conventional memoryless VQ based approach in signal bias removal, the preliminary experimental results indicate that incorporating sequential constraint in joint data and channel estimation for robust speech recognition is advantageous.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-507"
  },
  "mak96_icslp": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Phone clustering using the bhattacharyya distance",
   "original": "i96_2005",
   "page_count": 4,
   "order": 512,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "In this paper we study using the classification-based Bhattacharyya distance measure to guide biphone clustering. The Bhattacharyya distance is a theoretical distance measure between two Gaussian distributions which is equivalent to an upper bound on the optimal Bayesian classification error probability. It also has the desirable properties of being computationally simple and extensible to more Gaussian mixtures. Using the Bhattacharyya distance measure in a data-driven approach together with a novel 2-Level Agglomerative Hierarchical Biphone Clustering algorithm, generalized left/right biphones (BGBs) are derived. A neural-net based phone recognizer trained on the BGBs is found to have better frame-level phone recognition than one trained on generalized biphones (BCGBs) derived from a set of commonly-used broad categories. We further evaluate the new BGBs on an isolated-word recognition task of perplexity 40 and obtain a 16.2% error reduction over the broad-category generalized biphones (BCGBs) and a 41.8% error reduction over the monophones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-508"
  },
  "wakao96_icslp": {
   "authors": [
    [
     "Atsushi",
     "Wakao"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Variability of lombard effects under different noise conditions",
   "original": "i96_2009",
   "page_count": 4,
   "order": 513,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "In this paper, variability of Lombard speech under different noise conditions and an adaptation method to the different Lombard speech are discussed. For this purpose, various kinds of Lombard speech are recorded under different conditions of noise injected into a earphone with controlled feedback of voice. First, DTW word recognition experiments using clean speech as a reference are performed to show that the higher the noise level becomes the more seriously the utterance is affected. Second, linear transformation of the cepstral feature vector is tested to show that when given enough (more than 100 words) training data, the transformation matrix can be correctly learned for each of the noise conditions. Interpolation of the transfer matrix is then proposed in order to reduce the adaptation parameter and number of training samples. We show, finally, that five words are enough for the learning interpolated transformation matrix for unknown noise conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-509"
  },
  "chi96_icslp": {
   "authors": [
    [
     "Sang-mun",
     "Chi"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Lombard effect compensation and noise suppression for noisy Lombard speech recognition",
   "original": "i96_2013",
   "page_count": 4,
   "order": 514,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "The performance of speech recognition system degrades rapidly in the presence of ambient noise. To reduce the degradation, a degradation model is proposed which represents the spectral changes of speech signal uttered in noisy environments. The model uses frequency warping and amplitude scaling of each frequency band to simulate the variations of formant location, formant bandwidth, pitch, spectral tilt, and energy in each frequency band by Lombard effect. Another Lombard effect, the variation of overall vocal intensity is represented by a multiplicative constant term depending on spectral magnitude of input speech. The noise contamination is represented by an additive term in the frequency domain. According to this degradation model, the cepstral vector of clean speech is estimated from that of noisy-Lombard speech using spectral subtraction, spectral magnitude normalization, band-pass filter in LIN-LOG spectral domain, and multiple linear transformation. Noisy-Lombard speech data is collected by simulating the noisy environments using noises from automobile cabins, an exhibition hall, telephone booths in downtown, crowded streets, and computer rooms. The proposed method significantly reduces error rates in the recognition of 50 Korean word. For example, the recognition rate is 95.91% with this method, and 79.68% without this method at SNR (Signal-to-Noise Ratio) 10 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-510"
  },
  "huggins96_icslp": {
   "authors": [
    [
     "A. W. F.",
     "Huggins"
    ],
    [
     "Yogen",
     "Patel"
    ]
   ],
   "title": "The use of shibboleth words for automatically classifying speakers by dialect",
   "original": "i96_2017",
   "page_count": 4,
   "order": 515,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "Real-world applications using speech recognition must perform well over a range of dialects. Differences in dialect between the speakers in the training database and the target users often leads to degraded recognition performance. For the BBN Hark Hidden Markov Model (HMM) based system, we have already developed a reasonably effective technique [1] for dealing with multiple US dialects. The solution involves building separate HMM sets for each dialect from representative training speech data. This requires that training speakers be accurately classified by dialect, which is difficult to do reliably even by hand. In this paper we describe a recognition based pseudo-automatic scheme for partitioning a pool of US English training speakers into groups, such that the speakers within each group share the same pronunciation characteristics. Our scheme is speech-data driven, and involves using transcript-level word hypotheses generated by a recognizer to partition the pool of training speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-511"
  },
  "kudo96_icslp": {
   "authors": [
    [
     "Ikuo",
     "Kudo"
    ],
    [
     "Takao",
     "Nakama"
    ],
    [
     "Tomoko",
     "Watanabe"
    ],
    [
     "Reiko",
     "Kameyama"
    ]
   ],
   "title": "Data collection of Japanese dialects and its influence into speech recognition",
   "original": "i96_2021",
   "page_count": 5,
   "order": 516,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "This paper reports the successful completion of Japanese POLYPHONE project, Voice Across Japan (VAJ) data collection project. The database has the following characteristic, 1) large speakers database (8,866 spk.) through telephone line, 2) to gather participant's personal information such as gender, age, growing place, and so on, and 3) to put data segmented by phone or word boundary. This paper describes several aspects of Japanese dialects and also, reports the results of experiments. How much percents do dialects make influence on speech recognition. In our result, dialects makes 2-4% influence on speech recognition rate. The results are useful information for building practical speech recognition system as well as data collection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-512"
  },
  "miller96b_icslp": {
   "authors": [
    [
     "David R.",
     "Miller"
    ],
    [
     "James",
     "Trischitta"
    ]
   ],
   "title": "Statistical dialect classification based on mean phonetic features",
   "original": "i96_2025",
   "page_count": 3,
   "order": 517,
   "p1": "2025",
   "pn": "2027",
   "abstract": [
    "Our paper describes work done on a text-dependent method for automatic utterance classification and dialect model selection using mean cepstral and duration features on a per phoneme basis. From transcribed dialect data, we build a linear discriminant to separate the dialects in feature space. This method is potentially much faster than our previous selection algorithm. We have been able to achieve error rates of 8% for distinguishing Northern US speakers from Southern US speakers, and average error rates of 13% on a variety of finer pairwise dialect discriminations. We also present a description of the training and test corpora collected for this work.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-513"
  },
  "kvale96_icslp": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ]
   ],
   "title": "Norwegian numerals: a challenge to automatic speech recognition",
   "original": "i96_2028",
   "page_count": 4,
   "order": 518,
   "p1": "2028",
   "pn": "2031",
   "abstract": [
    "This paper addresses the problem of speaker-independent connected numeral recognition over telephone lines. Increasing the vocabulary from digits (0-9) to numerals (0-99) opens for more user-friendly services, but it also introduces many new, language-specific problems. This paper investigates morphological, phonemic and allophonic variations in the pronunciation of numerals in Norwegian. If improvements in recognition performance are to be achieved these language-specific issues have to be considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-514"
  },
  "torre96_icslp": {
   "authors": [
    [
     "C. de la",
     "Torre"
    ],
    [
     "J.",
     "Caminero-Gil"
    ],
    [
     "J.",
     "Alvarez"
    ],
    [
     "Cesar",
     "Martín del Alamo"
    ],
    [
     "Lúis",
     "Hernández-Gómez"
    ]
   ],
   "title": "Evaluation of the telefónica i+d natural numbers recognizer over different dialects of Spanish from Spain and America",
   "original": "i96_2032",
   "page_count": 4,
   "order": 519,
   "p1": "2032",
   "pn": "2035",
   "abstract": [
    "In this paper we present the results obtained when evaluating the Natural Numbers Recognizer of Telefónica I+D over some particular dialects of Spanish from Spain and America. The evaluation was made over two different data sets corresponding to two different situations. A first set includes dialects of Spanish from Spain, that were considered in the training and design of our baseline system, and a second set corresponds to Argentinian Spanish, that was not considered to train the original system. Just because we are interested in a system able to be used by a wide range of users, we tested the possibilities of MAP (Maximum-A-Priori techniques) to adapt the original HMMs in order to represent all the dialects. The experimental results show the capabilities of our recognizer to be used in applications spread over a great number of Spanish-speaking countries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-515"
  },
  "cummins96_icslp": {
   "authors": [
    [
     "Fred",
     "Cummins"
    ],
    [
     "Robert F.",
     "Port"
    ]
   ],
   "title": "Rhythmic constraints on English stress timing",
   "original": "i96_2036",
   "page_count": 4,
   "order": 520,
   "p1": "2036",
   "pn": "2039",
   "abstract": [
    "The evidence for isochrony of stress timing is weak at best for ordinary prose, but this does not mean that the timing of stresses is always unaffected by global constraints. We asked subjects to continually repeat the phrase Take a pack of cards and to temporally align the words take and cards with an auditorily presented stimulus consisting of just the words take and cards repeated several times. The phase of the cards stimulus relative to a reference cycle defined by the take-take interval was varied over the range 0.3-0.65 in eight equal-sized phase steps. The distribution of actually produced phases for the vowel onset of the syllable cards, however, was strongly trimodal. Subjects showed a powerful preference for phases close to 0.5, and somewhat weaker preferences for phases near 0.36 and 0.6. These values are close to (although systematically different from) 0.33 and 0.66 predicted by a simple harmonic model for stress timing. The observed distribution had this form whether the subjects were speaking along with the stimulus, or trying to maintain the prescribed timing after cessation of the stimulus. Furthermore, the observed phase was influenced by the phase produced on previous trials, suggesting dynamic control with hysteresis between competing stable patterns of timing. These results demonstrate strong rhythmic constraints on the timing of stresses within a phrase,where the domain of phrase in this artificial speaking task is simply the repeated text. The rhythmic constraints are similar to those observed for limb movements. Modeling these constraints should provide insight into the form of a general dynamic control regime for global speech timing, and may allow improved characterization of natural timing patterns in English speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-516"
  },
  "vogel96_icslp": {
   "authors": [
    [
     "Irene",
     "Vogel"
    ],
    [
     "Steve",
     "Hoskins"
    ]
   ],
   "title": "On the interaction of clash, focus and phonological phrasing",
   "original": "i96_2040",
   "page_count": 4,
   "order": 521,
   "p1": "2040",
   "pn": "2043",
   "abstract": [
    "This paper examines several phonetic and phonological issues related to the rhythmic form of English sentences. Specifically, we are concerned with the acoustic cues signaling the Rhythm Rule as well as the ways phonological phrasing, clash and focus contribute to the application of this rule. On the basis of acoustic and a TOBI style pitch accent analysis of a relatively large corpus of sentences read by 5 subjects, we show how (Phonological) Phrase Final Lengthening and focus have especially strong effects on the duration of our target words, and in particular on the final (as opposed to the first) rhyme of the words. On the basis of the application and specific manifestation of the Rhythm Rule, we found that narrow focus on a word seems to induce a break immediately following it, as was expected. It also appeared that focus may induce a break before the word, contrary to our expectations, leading to more complex analysis of the acoustic effect of focus in the relevant contexts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-517"
  },
  "fant96_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ]
   ],
   "title": "On the quantal nature of speech timing",
   "original": "i96_2044",
   "page_count": 4,
   "order": 522,
   "p1": "2044",
   "pn": "2047",
   "abstract": [
    "This is a review of regularities we have observed in the analysis of text reading, mostly Swedish, directed to the timing of vowels and consonants, syllables, interstress intervals and pauses. We have found tendencies of quantal aspects of temporal structure, superimposed on more gradual variations, which add quasi-rhythmical elements to speech. A local average of interstress intervals of the order of 0.5 sec appears to function as a reference quantum for the planning of pause durations. A recent study, confirming our previous findings of multiple peaks with about 0.5 sec spacing in histograms of pause durations, provides support to this model. It is well established that pause durations tend to increase with increasing syntactic level of boundaries. However, these variations tend to be quantally scaled even within a specific boundary category, e.g. between sentences or between paragraphs. Relatively short pauses, as between phrases or clauses, show durations in complementary relation to terminal lengthening. There are indications of approximately 1, 1/2, 1/4, 1/8 ratios of average durations of interstress intervals, stressed syllables, unstressed syllables and phoneme segments which adds to the observed regularities. The timing of syllables and phonetic segments with due regard to relative distinctiveness and reading speed will be discussed and also tempovariations within a sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-518"
  },
  "house96_icslp": {
   "authors": [
    [
     "David",
     "House"
    ]
   ],
   "title": "Differential perception of tonal contours through the syllable",
   "original": "i96_2048",
   "page_count": 4,
   "order": 523,
   "p1": "2048",
   "pn": "2051",
   "abstract": [
    "This paper proposes a model of differential tonal perception in which perception of tonal movement in spoken language i s influenced by syllable structure. In the proposed model, tonal movement at syllable onset is perceptually coded differently from movement in the syllable nucleus or in the coda. An F0 contour through the syllable onset or at the beginning of the nucleus is coded as a level tone while a contour through the nucleus or at the beginning of the coda is coded as a contour tone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-519"
  },
  "vainio96_icslp": {
   "authors": [
    [
     "Martti",
     "Vainio"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Pitch, loudness, and segmental duration correlates: towards a model for the phonetic aspects of finnish prosody",
   "original": "i96_2052",
   "page_count": 4,
   "order": 524,
   "p1": "2052",
   "pn": "2055",
   "abstract": [
    "Neural networks are used widely today for modeling a variety of different aspects of spoken language. We use them to model Finnish lexical prosody with an aim to shed light on the interaction between the main prosodic parameters: segmental durations, loudness and pitch. We have analyzed the performance of a group of networks which were all trained to generate values for a different prosodic parameter given similar input information. The experiments were performed on speech material contained within our Finnish speech database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-520"
  },
  "minematsu96b_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Prosodic manipulation system of speech material for perceptual experiments",
   "original": "i96_2056",
   "page_count": 4,
   "order": 525,
   "p1": "2056",
   "pn": "2059",
   "abstract": [
    "In perceptual experiments, quantitative manipulation of acoustic features in speech material is often required. And obviously, it can be realized only with speech synthesis techniques. Some of the authors have conducted a series of perceptual experiments, through which they have felt necessity of a system to generate more natural speech. With these backgrounds, a speech stimuli generation system was developed using an analysis re-synthesis technique, where users can freely manipulate prosodic features of input speech and the manipulated material is obtained as synthetic speech. Degree of resemblance to human speech (henceforth, RHS degree) of the synthesized material was investigated in evaluation experiments. As a result, no perceptual difference was found between synthesized sentences with wrong accents and spoken sentences with the same wrong accents. Furthermore, RHS degree of synthesized sentences with correct accents exceeded that of spoken sentences with flat F0 contours. These results clearly indicate that this system is useful for the preparation of speech stimuli in perceptual experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-521"
  },
  "ueberla96_icslp": {
   "authors": [
    [
     "J. P.",
     "Ueberla"
    ],
    [
     "I. R.",
     "Gransden"
    ]
   ],
   "title": "Clustered language models with context-equivalent states",
   "original": "i96_2060",
   "page_count": 3,
   "order": 526,
   "p1": "2060",
   "pn": "2062",
   "abstract": [
    "In this paper, a hierarchical context definition is added to an existing clustering algorithm in order to increase its robustness. The resulting algorithm, which clusters contexts and events separately, is used to experiment with different ways of defining the context a language model takes into account. The contexts range from standard bigram and trigram contexts to part of speech five-grams. Although none of the models can compete directly with a backoff trigram, they give up to 9% improvement in perplexity when interpolated with a trigram. Moreover, the modified version of the algorithm leads to a performance increase over the original version of up to 12%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-522"
  },
  "yonezawa96_icslp": {
   "authors": [
    [
     "Yuji",
     "Yonezawa"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Modeling of contextual effects and its application to word spotting",
   "original": "i96_2063",
   "page_count": 4,
   "order": 527,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "We propose a model of spectral contextual effects to simulate the superior recognition ability of humans and apply it to a front-end processor for word spotting. This model assumes that perceived spectra are influenced by adjacent spectral peaks and that the magnitude of the influence can be estimated by the minimum classification error criterion. Three experiments were carried out to evaluate the performance of the model. The results show that the model can compensate for neutralized spectra and bring them to their typical patterns. This improves word spotting accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-523"
  },
  "junkawitsch96_icslp": {
   "authors": [
    [
     "J.",
     "Junkawitsch"
    ],
    [
     "L.",
     "Neubauer"
    ],
    [
     "Harald",
     "Höge"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "A new keyword spotting algorithm with pre-calculated optimal thresholds",
   "original": "i96_2067",
   "page_count": 4,
   "order": 528,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "Keyword spotting is a very forward-looking and promising branch of speech recognition. This paper presents a HMM-based keyword spotting system, which works with a new algorithm. The first discussion topic is the description of the search algorithm, that needs no representation of the non-keyword parts of the speech signal. For this purpose, the computation of the HMM scores and the Viterbi algorithm had to be modified. The keyword HMMs are not concatenated with other HMMs, so that there is no necessity for filler or garbage models. As a further advantage, this algorithm needs only low computional expense and storage requirement. The second discussion topic is the determination of a optimal decision threshold for each keyword. In order two decide between the two possibilities \"keyword was spoken\" and \"keyword was not spoken\", the scores of the keywords are compared with keyword specific decision thresholds. This paper introduces a method to fix decision thresholds in advance. Starting with measured phoneme distributions, the score distributions of whole keyword models can be calculated. Furthermore, these keyword distributions form the basis of the computation of decision thresholds. Tests with spontaneous speech databases yielded 73.9% Figure-Of-Merit when using context-dependent HMMs. The detection rate at 10 fa/kw/h comes to 80%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-524"
  },
  "lacouture96_icslp": {
   "authors": [
    [
     "Roxane",
     "Lacouture"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "Detection of ambiguous portions of signal corresponding to OOV words or misrecognized portions of input",
   "original": "i96_2071",
   "page_count": 4,
   "order": 529,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "One of the key problems for large-vocabulary ASR is the detection of unknown or misrecognized portions of the input. This paper presents results obtained using a local rejection algorithm. The algorithm is derived from the two-pass recognition algorithm by Murveit [3] and is used to detect misrecognized portions based on the number per frame of active words during the second pass. The hypothesis underlying the algorithm is that recognition on unexpected data, i.e. noise or out-of-vocabulary (OOV) words, is likely to result in activation of more words, since no word matches the data well; on the other hand, when the match is good, fewer words should be active. The algorithm was tried on part of the WSJ 5K November 1993 test, in which there were no OOV words (3370 words in total) and on the digit-strings-only Macrophone data (14686 words of which 895 were OOV). The results obtained indicate that our approach is promising, both for the detection of OOV words and misrecognized portions of the input. It may provide the base on which to build tools for dealing with these phenomena. These tools might include dialogue mechanisms based on the list of activated words corresponding to a rejected portion, display mechanisms such as reverse video or rescoring schemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-525"
  },
  "brugnara96_icslp": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Techniques for approximating a trigram language model",
   "original": "i96_2075",
   "page_count": 4,
   "order": 530,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "In this paper several methods are proposed for reducing the size of a trigram language model (LM), which is often the biggest data structure in a continuous speech recognizer, without affecting its performance. The common factor shared by the different approaches is to select only a subset of the available trigrams, trying to identify those trigrams that mostly contribute to the performance of the full trigram LM. The proposed selection criteria apply to trigram contexts, both of length one or two. These criteria rely on information theory concepts, the back-off probabilities estimated by the LM, or on a measure of the phonetic/linguistic uncertainty relative to a given context. Performance of the reduced trigrams LMs are compared both in terms of perplexity and recognition accuracy. Results show that all the considered methods perform better than the naive frequency shifting method. In fact, a 50% size reduction is obtained on a shift-1 trigram LM, at the cost of a 5% increase in word error rate. Moreover, the reduced LMs improve by around 15% the word error rate of a bigram LM of the same size.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-526"
  },
  "takagi96b_icslp": {
   "authors": [
    [
     "Keizaburo",
     "Takagi"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Hiroaki",
     "Hattori"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Unsupervised and incremental speaker adaptation under adverse environmental conditions",
   "original": "i96_2079",
   "page_count": 4,
   "order": 531,
   "p1": "2079",
   "pn": "2082",
   "abstract": [
    "A new speaker adaptation method is described. In practical applications of speaker adaptation, adaptation and testing environments change significantly and are unknown before-hand. In such cases, since the speaker adaptation adapts a reference pattern to the adaptation utterances in regard to differences in both environment and speaker at the same time, performance in speaker adaptation would be degraded. To cope with this problem, our proposed method first eliminates the environmental differences between each input utterance and a reference pattern by using a rapid environment adaptation algorithm based on spectrum equalization (RE-ALISE) [2]. Then we apply an unsupervised and incremental speaker adaptation with autonomous control using tree structure pdf's (ACTS) [1] to the environmentally adapted reference pattern. By combining these two methods, the resulting system is expected to perform well under adverse environmental conditions and to show a stable improvement regardless of the amount of adaptation data. Evaluation experiments were carried out for utterances under three vehicle speed conditions. Recognition rates for a 100-Japanese-word recognition task after 100-word adaptation were improved from 92% (ACTS alone) to 95% (proposed method).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-527"
  },
  "vanhamme96_icslp": {
   "authors": [
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Filip Van",
     "Aelten"
    ]
   ],
   "title": "An adaptive-beam pruning technique for continuous speech recognition",
   "original": "i96_2083",
   "page_count": 4,
   "order": 532,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "Pruning is an essential paradigm to build HMM-based large vocabulary speech recognisers that use reasonable computing resources. Unlikely sentence, word or subword hypotheses are removed from the search space when their likelihood falls outside a beam relative to the best scoring hypothesis. A method for automatically steering this beam such that the search space attains a predefined size is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-528"
  },
  "avendano96b_icslp": {
   "authors": [
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Sarel van",
     "Vuuren"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Data based filter design for RASTA-like channel normalization in ASR",
   "original": "i96_2087",
   "page_count": 4,
   "order": 533,
   "p1": "2087",
   "pn": "2090",
   "abstract": [
    "RASTA processing has proven to be a successful technique for channel normalization in automatic speech recognition (ASR). We present two approaches to the design of RASTA-like filters from training data. One consists of finding the solution to a constrained optimization problem on the feature time trajectories while the other uses Linear Discriminant Analysis (LDA). Whereas LDA is often applied to one or a few frames of the feature vectors we apply LDA to feature time trajectories. Both approaches result in similar filters which are consistent with the ad hoc designed RASTA filter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-529"
  },
  "ortmanns96_icslp": {
   "authors": [
    [
     "S.",
     "Ortmanns"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "I.",
     "Lindam"
    ]
   ],
   "title": "A comparison of time conditioned and word conditioned search techniques for large vocabulary speech recognition",
   "original": "i96_2091",
   "page_count": 4,
   "order": 534,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "In this paper, we compare the search effort of the word conditioned and the time conditioned tree search methods. Both methods are based on a time-synchronous, left-to-right beam search using a tree-organized lexicon. Whereas the word conditioned method is well known and widely used, the time conditioned method is novel in the context of 20.000 word vocabulary recognition. We extend both methods to handle trigram language models in a one-pass strategy. Both methods were tested on a train schedule inquiry task (1.850 words, telephone speech) and on the North American Business (Nov.94) development corpus (20.000 words).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-530"
  },
  "ortmanns96b_icslp": {
   "authors": [
    [
     "S.",
     "Ortmanns"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "A.",
     "Eiden"
    ]
   ],
   "title": "Language-model look-ahead for large vocabulary speech recognition",
   "original": "i96_2095",
   "page_count": 4,
   "order": 535,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "In this paper, we present an efficient look-ahead technique which incorporates the language model knowledge at the earliest possible stage during the search process. This so-called language model look-ahead is built into the time synchronous beam search algorithm using a tree-organized pronunciation lexicon for a bigram language model. The language model look-ahead technique exploits the full knowledge of the bigram language model by distributing the language model probabilities over the nodes of the lexical tree for each predecessor word. We present a method for handling the resulting memory requirements. The recognition experiments performed on the 20 000-word North American Business task (Nov.96) demonstrate that in comparison with the unigram look-ahead a reduction by a factor of 5 in the acoustic search effort can be achieved without loss in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-531"
  },
  "husson96_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Husson"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "A new search algorithm in segmentation lattices of speech signals",
   "original": "i96_2099",
   "page_count": 4,
   "order": 536,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "This paper describes a new segmentation system using a multi-level representation, called dendrogram [1]. We address the issues of estimating the confidence of one path, and finding the N most reliable paths in the segmentation lattice. Our approach rests on automatically trained criteria and on an efficient strategy to prune the search space.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-532"
  },
  "yamada96c_icslp": {
   "authors": [
    [
     "Tomokazu",
     "Yamada"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "LR-parser-driven viterbi search with hypotheses merging mechanism using context-dependent phone models",
   "original": "i96_2103",
   "page_count": 4,
   "order": 537,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "This paper describes a Viterbi search algorithm for continuous speech recognition using context-dependent phone models under the constraint defined by a context-free grammar (CFG). It is based on a frame synchronous LR parser which dynamically generates a finite state network (FSN) from the CFG with an efficient path merging mechanism. Full context-dependency (intra- and interword context) is taken into account in the likelihood calculation process. This paper first describes the algorithm and the processing mechanism, then compares the experimental results of our algorithm and the conventional tree-based HMM-LR speech recognition algorithm which uses HMMs and an LR parser in phone-synchronous processing. The experiments show that our algorithm runs faster than the conventional HMM-LR algorithm with an equivalent recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-533"
  },
  "nouza96_icslp": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Discrete-utterance recognition with a fast match based on total data reduction",
   "original": "i96_2107",
   "page_count": 4,
   "order": 538,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "In the paper, a two-level classification scheme applicable to practical discrete-utterance recognition systems is presented. Both the fast and fine match employ CDHMM whole-word models. The fast match is based on total data reduction, which includes both the minimalization of the acoustic data flow (the numbers of speech frames and features) and the reduction of the basic HMM parameters (the numbers of states and mixtures). The optimal choice of the fast match parameters is a subject of the procedure that aims at minimizing the total classification time while preserving the maximum available recognition accuracy. On a medium-size vocabulary task (121 city names) the fast match reduced recognition time to approx. 20% (compared with the original one-level system) with a negligible loss of accuracy. The time savings were even more considerable in case of a system with multi-mixture HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-534"
  },
  "caminerogil96_icslp": {
   "authors": [
    [
     "J.",
     "Caminero-Gil"
    ],
    [
     "C. de la",
     "Torre"
    ],
    [
     "L.",
     "Villarrubia"
    ],
    [
     "Cesar",
     "Martín del Alamo"
    ],
    [
     "Lúis",
     "Hernández"
    ]
   ],
   "title": "On-line garbage modeling with discriminant analysis for utterance verification",
   "original": "i96_2111",
   "page_count": 4,
   "order": 539,
   "p1": "2111",
   "pn": "2114",
   "abstract": [
    "Out-of-vocabulary (OOV) utterance detection and rejection are specially important and difficult problems in large-vocabulary and continuous speech recognition. In [1] we proposed an utterance verification procedure based on the use of frame-by-frame best acoustic state scores instead of using explicit garbage models. This procedure is usually referred to as on-line garbage modeling. In this contribution we extend our previous work in two major directions: a) we analyze, through the use of Discriminant Analysis, the possibilities of using L-best local scores and N-best utterance hypotheses scores for utterance verification; b) we present experimental results not only for a spontaneously spoken natural number recognition task, as in [1], but also for a flexible large vocabulary recognition task. All the results, based on a telephone database, show that the proposed on-line garbage modeling procedure outperforms, both in performance and computational cost, to other approaches based on the use of explicit garbage models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-535"
  },
  "placeway96_icslp": {
   "authors": [
    [
     "Paul",
     "Placeway"
    ],
    [
     "John",
     "Lafferty"
    ]
   ],
   "title": "Cheating with imperfect transcripts",
   "original": "i96_2115",
   "page_count": 4,
   "order": 540,
   "p1": "2115",
   "pn": "2118",
   "abstract": [
    "Most speech recognition systems try to reconstruct a word sequence given an acoustic input, using prior information about the language being spoken. In some cases, there is more information available to the decoder than simply the acoustics. When decoding a television news broadcast, for example, the closed-caption information that is often recorded for hearing impaired viewers may also be available. While these captions are generally not completely accurate transcriptions, they can be considered to be a strong hint as to what was actually spoken. In this paper, we present a formalization of this problem in terms of the source channel paradigm. We propose a simple translation model for mapping caption sequences to word sequences which updates the language model with the prior information inherent in the captions. We also describe an efficient implementation of the search in a Viterbi decoder, and present results using this system in the broadcast news domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-536"
  },
  "iwahashi96_icslp": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ]
   ],
   "title": "Novel training method for classifiers used in speaker adaptation",
   "original": "i96_2119",
   "page_count": 4,
   "order": 541,
   "p1": "2119",
   "pn": "2122",
   "abstract": [
    "This paper describes a novel method for training a classifier that will perform well after it has been adapted to input speakers. For off-line (batch-mode) adaptation methods which are based on the transformation of classifier parameters, we propose a method for training classifiers. In this method, the classifier is trained while the adaptation to each speaker in the training data is being carried out. The objective function for the training is given based on the recognition performance obtained by the adapted classifier. The utility of the proposed training method is demonstrated by experiments in a five-class Japanese vowel pattern recognition task with speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-537"
  },
  "minamino96_icslp": {
   "authors": [
    [
     "Katsuki",
     "Minamino"
    ]
   ],
   "title": "Large vocabulary word recognition based on a graph-structured dictionary",
   "original": "i96_2123",
   "page_count": 4,
   "order": 542,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "In this paper, a structural search using a word-node graph is proposed to speed up the isolated word recognition based on hidden markov models (HMMs). We define a distance measure for comparing pairs of words, and construct a graph keeping the structure of word distribution. Based on this graph, the number of words to be examined are restricted. Experiments show that the search complexity is considerably reduced with little degradation of the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-538"
  },
  "tran96_icslp": {
   "authors": [
    [
     "Bach-Hiep",
     "Tran"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "Volker",
     "Steinbiss"
    ]
   ],
   "title": "A word graph based n-best search in continuous speech recognition",
   "original": "i96_2127",
   "page_count": 4,
   "order": 543,
   "p1": "2127",
   "pn": "2130",
   "abstract": [
    "In this paper, we introduce an efficient algorithm for the exhaustive search of N best sentence hypotheses in a word graph. The search procedure is based on a two-pass algorithm. In the first pass, a word graph is constructed with standard time-synchronous beam search. The actual extraction of N best word sequences from the word graph takes place during the second pass. With our implementation of a tree-organized N-Best list, the search is performed directly on the resulting word graph. Therefore, the parallel bookkeeping of N hypotheses at each processing step during the search is not necessary. It is important to point out that the proposed N-Best search algorithm produces an exact N-Best list as defined by the word graph structure. Possible errors can only result from pruning during the construction of the word graph. In a postprocessing step, the N candidates can be rescored with a more complex language model with highly reduced computational cost. This algorithm is also applied in speech understanding to select the most likely sentence hypothesis that satisfies some additional constraints.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-539"
  },
  "goblirsch96_icslp": {
   "authors": [
    [
     "David M.",
     "Goblirsch"
    ]
   ],
   "title": "Viterbi beam search with layered bigrams",
   "original": "i96_2131",
   "page_count": 4,
   "order": 544,
   "p1": "2131",
   "pn": "2134",
   "abstract": [
    "We outline an implementation of Viterbi beam search that incorporates layered bigrams. Layered bigrams are class bigrams in which some nodes are themselves bigrams, resulting in a recursive structure. The implementation is in C++ and involves a hierarchy of classes. The paper outlines the main concepts and the corresponding C++ classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-540"
  },
  "burhke96_icslp": {
   "authors": [
    [
     "Eric",
     "Burhke"
    ],
    [
     "Wu",
     "Chou"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "A wave decoder for continuous speech recognition",
   "original": "i96_2135",
   "page_count": 4,
   "order": 545,
   "p1": "2135",
   "pn": "2138",
   "abstract": [
    "In this paper, a wave decoder based on the general re-entrant network for continuous speech recognition is described. The decoder design is based on the concept of self-adjusting decoding graph in which the decoding network is expanded and released frame-synchronously. The fast network expansion and release are made possible by utilizing a novel dynamic network scaffolding layer. The self-adjusting decoding graph is obtained by slicing the traditional decoding network horizontally for separation of different knowledge sources and vertically according to each time instant in search. A two layer hashing structure and an admissible arc predication scheme are described. These methods significantly reduce the arc mortality rate, a problem which plagues the efficiency of the dynamic decoder. Experimental results demonstrate that an order of magnitude reduction of decoding resources can be achieved based on the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-541"
  },
  "thelen96_icslp": {
   "authors": [
    [
     "Eric",
     "Thelen"
    ]
   ],
   "title": "Long term on-line speaker adaptation for large vocabulary dictation",
   "original": "i96_2139",
   "page_count": 4,
   "order": 546,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "On-line speaker adaptation is desirable for speech recognition dictation applications, because it offers the possibility to improve the system with the speaker-specific data obtained from the user. Since the user will work with such a device over a long period, for a dictation system the long term adaptation performance is more important than the adaptation speed. In contrast to speaker-dependent re-training, the speaker-specific speech data does not need to be stored for on-line speaker adaptation and each adaptation step does not require a large computational effort. In this paper we describe our way of performing online Bayesian speaker adaptation using partial traceback. We compare supervised with unsupervised adaptation and speaker adaptation with speaker-dependent training using the adaptation material. Compared to the speaker-independent startup models, the error rate was divided by two after five hours of supervised adaptation in our experiments. In the long term experiments, supervised on-line adaptation performed similar to speaker-dependent training using the adaptation material.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-542"
  },
  "sagerer96_icslp": {
   "authors": [
    [
     "Gerhard",
     "Sagerer"
    ],
    [
     "Heike",
     "Rautenstrauch"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Bernd",
     "Hildebrandt"
    ],
    [
     "A.",
     "Jusek"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Incremental generation of word graphs",
   "original": "i96_2143",
   "page_count": 4,
   "order": 547,
   "p1": "2143",
   "pn": "2146",
   "abstract": [
    "We present an algorithm for the incremental generation of word graphs. Incremental means that the speech signal is processed left-to-right by a time synchronous Viterbi algorithm and word hypotheses are generated with some delay to Viterbi decoding. The incrementally generated word hypotheses can be used for early interaction between linguistic analysis and acoustic recognition. Therefore, it is possible to derive acoustic constraints from linguistic restrictions dynamically.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-543"
  },
  "illina96b_icslp": {
   "authors": [
    [
     "Irina",
     "Illina"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Improvement in n-best search for continuous speech recognition",
   "original": "i96_2147",
   "page_count": 4,
   "order": 548,
   "p1": "2147",
   "pn": "2150",
   "abstract": [
    "In this paper, several techniques for reducing the search complexity of beam search for continuous speech recognition task are proposed. Six heuristic methods for pruning are described and the parameters of the pruning are adjusted to keep constant the word error rate while reducing the computational complexity and memory demand. The evaluation of the effect of each pruning method is performed in Mixture Stochastic Trajectory Model (MSTM). MSTM is a segment-based model using phonemes as the speech units. The set of tests in a speaker-dependent continuous speech recognition task shows that using the pruning methods, a substantial reduction of 67% of search effort is obtained in term of number of hypothesised phonemes during the search. All proposed techniques are independent of the acoustic models and therefore are applicable to other acoustic modeling techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-544"
  },
  "bonafonte96d_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "José B.",
     "Mariño"
    ],
    [
     "Albino",
     "Nogueiras"
    ]
   ],
   "title": "Sethos: the UPC speech understanding system",
   "original": "i96_2151",
   "page_count": 4,
   "order": 549,
   "p1": "2151",
   "pn": "2154",
   "abstract": [
    "In EuroSpeech95, we presented the first version of Sethos, the speech understanding system which has been developed at the UPC. In this paper some improvements are incorporated at different levels of Sethos: language model, models of the semantic units and acoustic models. These improvements increase the percentage of correctly decoded sentences from 60% to 80%. Some experiments are presented to evaluate the influence of each information source on the final performance. Furthermore, the computational cost is analyzed arriving to an important conclusion: the configuration which gives the best performance is also the less expensive. The reason is that as better is the modeling, narrower is the beam of the search.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-545"
  },
  "laface96_icslp": {
   "authors": [
    [
     "Pietro",
     "Laface"
    ],
    [
     "Luciano",
     "Fissore"
    ],
    [
     "A.",
     "Maro"
    ],
    [
     "Franco",
     "Ravera"
    ]
   ],
   "title": "Segmental search for continuous speech recognition",
   "original": "i96_2155",
   "page_count": 4,
   "order": 550,
   "p1": "2155",
   "pn": "2158",
   "abstract": [
    "The paper illustrates a search strategy for continuous speech recognition based on the recently developed Fast Segmental Viterbi Algorithm (FSVA) [5], a new search strategy particularly effective for very large vocabulary word recognition. The FSVA search has been extended to deal with continuous speech using a network that merges a general lexical tree and a set of bigram subtrees generated on demand during the search. Results are given for a 751-words speaker independent spontaneous speech recognizer of a railway timetable inquiry application, managed by a dialog system. Preliminary tests have been performed on the Wall Street Journal 5K words 1992 evaluation set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-546"
  },
  "breen96_icslp": {
   "authors": [
    [
     "A. P.",
     "Breen"
    ],
    [
     "E.",
     "Bowers"
    ],
    [
     "W.",
     "Welsh"
    ]
   ],
   "title": "An investigation into the generation of mouth shapes for a talking head",
   "original": "i96_2159",
   "page_count": 4,
   "order": 551,
   "p1": "2159",
   "pn": "2162",
   "abstract": [
    "BT is currently developing a low computation, real time, talking head as an adjunct to the Laureate text-to-speech system[1]. Research into the development of a talking head may be divided into two components; image generation, and face and head movement control. This paper concentrates on the last of the two. A significant aspect of this work is research into methods of generating convincing mouth shapes when the head is talking. The paper describes a real time method of visual speech generation, which takes into consideration major coarticulation effects. It provides a detailed description of the generation process and compares this with a method of visual speech generation proposed by Cohen and Massaro[2].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-547"
  },
  "goff96_icslp": {
   "authors": [
    [
     "Bertrand Le",
     "Goff"
    ],
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "A text-to-audiovisual-speech synthesizer for French",
   "original": "i96_2163",
   "page_count": 4,
   "order": 552,
   "p1": "2163",
   "pn": "2166",
   "abstract": [
    "An audiovisual speech synthesizer from unlimited French text is here presented. It uses a 3-D parametric model of the face. The facial model is controlled by eight parameters. Target values have been assigned to the parameters, for each French viseme, based upon measurements made on a human speaker. Parameter trajectories are modeled by means of dominance functions associated with each parameter and each viseme. A dominance function is characterized by three coefficients so that coarticulation finally depends on the phonetic context, the speech rate, and an \"hypo-hyper articulation\" coefficient adjustable by the user. Finally, the visual and audiovisual intelligibility of our visual synthesizer has been evaluated in its first version, and compared to that of the acoustic synthesizer on which it was implemented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-548"
  },
  "iwano96_icslp": {
   "authors": [
    [
     "Yuri",
     "Iwano"
    ],
    [
     "Shioya",
     "Kageyama"
    ],
    [
     "Emi",
     "Morikawa"
    ],
    [
     "Shu",
     "Nakazato"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Analysis of head movements and its role in spoken dialogue",
   "original": "i96_2167",
   "page_count": 4,
   "order": 553,
   "p1": "2167",
   "pn": "2170",
   "abstract": [
    "In this research, we analyzed the relationship between semantics of utterances and movements of head in a natural dialogue and a task oriented one in Japanese. We are going to show that visual information such as head movement will be useful for managing a dialogue and reducing the vagueness of semantics. First we extracted the head movements calculated automatically in a natural conversation and going to indicate the role of it. After this we will show an analysis of head movements during a cooperative problem solving task to construct a natural dialogue system in which initiative of the conversation moves. We will show the effectiveness of using visual information in a multimodal dialogue system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-549"
  },
  "hayamizu96_icslp": {
   "authors": [
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Osamu",
     "Hasegawa"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Katuhiko",
     "Sakaue"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shigeki",
     "Nagaya"
    ],
    [
     "Masayuki",
     "Nakazawa"
    ],
    [
     "T.",
     "Endoh"
    ],
    [
     "Fumio",
     "Togawa"
    ],
    [
     "Kenji",
     "Sakamoto"
    ],
    [
     "Kazuhiko",
     "Yamamoto"
    ]
   ],
   "title": "RWC multimodal database for interactions by integration of spoken language and visual information",
   "original": "i96_2171",
   "page_count": 7,
   "order": 554,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "This paper describes our design policy and prototype data collection of RWC (Real World Computing Program) multimodal database. The database is intended for research and development on the integration of spoken language and visual information for human computer interactions. The interactions are supposed to use image recognition, image synthesis, speech recognition, and speech synthesis. Visual information also includes non-verbal communication such as interactions using hand gestures and facial expressions be- tween human and a human-like CG (Computer Graphics) agent with a face and hands. Based on the experiments of interactions with these modes, specifications of the database are discussed from the viewpoint of controlling the variability and cost for the collection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-550"
  },
  "cave96_icslp": {
   "authors": [
    [
     "Christian",
     "Cavé"
    ],
    [
     "Isabelle",
     "Guaïtella"
    ],
    [
     "Roxane",
     "Bertrand"
    ],
    [
     "Serge",
     "Santi"
    ],
    [
     "Françoise",
     "Harlay"
    ],
    [
     "Robert",
     "Espesser"
    ]
   ],
   "title": "About the relationship between eyebrow movements and F0 variations",
   "original": "i96_2175",
   "page_count": 4,
   "order": 555,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "Speech production is always accompanied by facial and gestural activity. The present study is part of a broader research project on how head movements and facial expressions are related to voice variations in different speech situations. Ten normal subjects were recorded while reading aloud, answering yes/no questions, and dialoguing with an interviewer. Rapid rising-falling eyebrow movements produced by the subjects as they spoke were associated with F0 rises in only 71% of the cases. This suggests that eyebrow movements and fundamental frequency changes are not automatically linked (i.e., they are not the result of muscular synergy), but are more a consequence of linguistic and communicational choices. Note also that 38% of the eyebrow movements were produced while the subject was not speaking. Thus, eyebrow movements may also serve as back-channel signals or play a role in turn-taking during conversation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-551"
  },
  "fais96_icslp": {
   "authors": [
    [
     "Laurel",
     "Fais"
    ],
    [
     "Kyung-ho",
     "Loken-Kim"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "How many words is a picture really worth?",
   "original": "i96_2179",
   "page_count": 4,
   "order": 556,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "Subjects communicating in telephone and multimedia setting do not replace speech with visual images in the multimedia setting. Instead, they use more words in this environment. We discuss the trade-off between words and images, addressing this surprising result. Several factors are involved: use of redundant visual information; \"meta-media\" conversation; and a slightly greater amount of information conveyed in the multimedia setting. Suggestions concerning integration of a multimedia interface with automatic translation are made.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-552"
  },
  "lagana96_icslp": {
   "authors": [
    [
     "A.",
     "Lagana"
    ],
    [
     "F.",
     "Lavagetto"
    ],
    [
     "A.",
     "Storace"
    ]
   ],
   "title": "Visual synthesis of source acoustic speech through kohonen neural networks",
   "original": "i96_2183",
   "page_count": 4,
   "order": 557,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "The objective of bimodal (audio-video) synthesis of acoustic speech has been addressed through the use of Kohonen neural architectures encharged of associating acoustic input parameters (cepstrum coeffcients) to articulatory estimates. This association is done in real-time allowing the synchronized presentation of source acoustic speech together with coherent articulatory visualization. Different architectural solutions have been investigated and compared in terms of objective measures (estimation distortion) as well as of subjective evaluation (perception experiments).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-553"
  },
  "saldana96_icslp": {
   "authors": [
    [
     "Helena M.",
     "Saldaña"
    ],
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Jennifer M.",
     "Fellowes"
    ],
    [
     "Robert E.",
     "Remez"
    ]
   ],
   "title": "Audio-visual speech perception without speech cues",
   "original": "i96_2187",
   "page_count": 5,
   "order": 558,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "A series of experiments was conducted in which listeners were presented with audio-visual sentences in a transcription task. The visual components of the stimuli consisted of a male talkers face. The acoustic components consisted of: (1) natural speech (2) envelope-shaped noise which preserved the duration and amplitude of the original speech waveform and (3) various types of sinewave speech signals that followed the formant frequencies of a natural utterance. Sinewave speech is a skeletonized version of a natural utterance which contains frequency and amplitude variation of the formants, but lacks any fine-grained acoustic structure of speech. Intelligibility of the present set of sinewave sentences was relatively low in contrast to previous findings (Remez, Rubin, Pisoni, & Carrell 1981). However, intelligibility was greatly increased when visual information from a talkers face was presented along with the auditory stimuli. Further experiments demonstrated that the intelligibility of single tones increased differentially depending on which formant analog was presented. It was predicted that the increase in intelligibility for the sinewave speech with an added video display would be greater than the gain observed with envelope-shaped noise. This prediction is based on the assumption that the information-bearing phonetic properties of spoken utterances are preserved in the audio+visual sine-wave conditions. This prediction was borne out for the tonal analog of the second formant (T2), but not the tonal analog of the first formant (T1) or third formant (T3), suggesting that the information contained in the T2 analog is relevant for audiovisual integration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-554"
  },
  "barnett96_icslp": {
   "authors": [
    [
     "Jim",
     "Barnett"
    ],
    [
     "A.",
     "Corrada"
    ],
    [
     "G.",
     "Gao"
    ],
    [
     "Larry",
     "Gillick"
    ],
    [
     "Yoshiko",
     "Ito"
    ],
    [
     "S.",
     "Lowe"
    ],
    [
     "L.",
     "Manganaro"
    ],
    [
     "Barbara",
     "Peskin"
    ]
   ],
   "title": "Multilingual speech recognition at dragon systems",
   "original": "i96_2191",
   "page_count": 4,
   "order": 559,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "This paper reports on some Dragon Systems' experiments with multilingual large vocabulary speech recognition, both for its discrete-word product Dragon Dictate (R) for Windows Version 1.0 and for its speaker-independent continuous speech research systems. The experiments in discrete word recognition involve English, French, German, Italian and Spanish. The tests show significant, but not overwhelming, differences between the languages, with French being the hardest language to recognize and Italian being the easiest. The continuous speech experiments involve the Ricardo and CallHome corpora of conversational telephone speech, and show such high word error rates that no language-specific differences emerge. However, experiments with English switchboard and CallHome recognition indicate that improved recognition technology and larger amounts of training data can improve accuracy substantially. We therefore expect that future multilingual LVCSR experiments will be more illuminating.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-555"
  },
  "kohler96b_icslp": {
   "authors": [
    [
     "Joachim",
     "Köhler"
    ]
   ],
   "title": "Multi-lingual phoneme recognition exploiting acoustic-phonetic similarities of sounds",
   "original": "i96_2195",
   "page_count": 4,
   "order": 560,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "The aim of this work is to exploit the acoustic-phonetic similarities between several languages. In recent work cross-language HMM-based phoneme models have been used only for bootstrapping the language-dependent models and the multi-lingual approach has been investigated only on very small speech corpora. In this paper, we introduce a statistical distance measure to determine the similarities of sounds. Further, we present a new technique to model multi-lingual phonemes. The experiments are conducted with the OGI Multi-Language Telephone Speech Corpus for the languages American English, German and Spanish. In the first experiment phoneme recognition rates between 39.0% and 53.9% are achieved using language-dependent models. Using cross-language models yields for some phonemes improvement, but in average a degradation of recognition performance is observed. However, cross-language models speeds up the cross-language transfer and reduces the size of the phoneme inventory of multi-lingual speech recognition systems. Finally, a new method of modelling multi-lingual phonemes, which can be used for a variety of language, is presented. This technique reduces the number of phoneme-based units in a multi-lingual speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-556"
  },
  "nakamura96_icslp": {
   "authors": [
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Tohru",
     "Shimizu"
    ],
    [
     "Masahiro",
     "Tonomura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Japanese speech databases for robust speech recognition",
   "original": "i96_2199",
   "page_count": 4,
   "order": 561,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "At ATR, a next-generation speech translation system is under development towards natural trans-language communication. To cope with the various requirements to speech recognition technology for the new system, further research efforts should emphasize the robustness for large vocabulary, speaking variations often found in fast spontaneous speech and speaker variances. These are key problems to be solved not only for speech translation but also for the general use of speech recognition in real environments. In this paper, three large speech databases are designed to cope with these problems in speech recognition and the current status of data collection is reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-557"
  },
  "lamel96b_icslp": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Maqrtine",
     "Adda-Decker"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "Spoken language processing in a multilingual context",
   "original": "i96_2203",
   "page_count": 4,
   "order": 562,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "In this paper we overview the spoken language processing activities at LIMSI, which are carried out in a multilingual framework. These activities include speech-to-text conversion, spoken language systems for information retrieval, speaker and language recognition, and speech response. The Spoken Language Processing Group has also been actively involved in corpora development and evaluation. The group has regularly participated in evaluations organized by ARPA, in the LE-SQALE project, and in the AUPELF-UREF program for provision of linguistic resources and evaluation tests for French.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-558"
  },
  "zue96_icslp": {
   "authors": [
    [
     "Victor",
     "Zue"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Multilingual human-computer interactions: from information access to language learning",
   "original": "i96_2207",
   "page_count": 4,
   "order": 563,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "This paper describes our recent work in developing multilingual conversational systems that support human-computer interactions. Our approach is based on the premise that a common semantic representation can be extracted from the input for all languages, at least within the context of restricted domains. In our design of such systems, language dependent information is separated from the system kernel as much as possible, and encoded in external data structures. The internal system manager, discourse and dialogue component, and database are all maintained in a language transparent form. We will describe two possible application areas for such multilingual capabailities: on-line information access using multilingual spoken dialogue, and the learning and maintenance of a foreign language using a multilingual conversational system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-559"
  },
  "ackermann96_icslp": {
   "authors": [
    [
     "U.",
     "Ackermann"
    ],
    [
     "B.",
     "Angelini"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "R.",
     "Gretter"
    ],
    [
     "G.",
     "Lazzari"
    ],
    [
     "H.",
     "Niemann"
    ]
   ],
   "title": "Speedata: multilingual spoken data entry",
   "original": "i96_2211",
   "page_count": 4,
   "order": 564,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "In this paper we present a multilingual application for speech technology. The SpeeData project aims at building a demonstrator that provides a user-friendly interface for spoken data-entry in two languages: Italian and German. The application domain is the land register of an Italian region in which both languages are officially spoken. The considered data-entry task is particularly challenging as it considers many different types of data - e.g. long texts, numbers, proper names, tables, etc.- and a variety of of pronunciations, since dialects are present and users will not always speak in their native language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-560"
  },
  "alshawi96_icslp": {
   "authors": [
    [
     "Hiyan",
     "Alshawi"
    ]
   ],
   "title": "Head automata for speech translation",
   "original": "i96_2360",
   "page_count": 4,
   "order": 565,
   "p1": "2360",
   "pn": "2363",
   "abstract": [
    "This paper presents statistical language and translation models based on collections of small finite state machines we call \"head automata\". The models are intended to capture the lexical sensitivity of N-gram models and direct statistical translation models, while at the same time taking account of the hierarchical phrasal structure of language. Two types of head automata are defined: relational head automata suitable for translation by transfer of dependency trees, and head transducers suitable for direct recursive lexical translation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-561"
  },
  "wang96i_icslp": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "John",
     "Lafferty"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Word clustering with parallel spoken language corpora",
   "original": "i96_2364",
   "page_count": 4,
   "order": 566,
   "p1": "2364",
   "pn": "2367",
   "abstract": [
    "In this paper we introduce a word clustering algorithm which uses a bilingual, parallel corpus to group together words in the source and target language. Our method generalizes previous mutual information clustering algorithms for monolingual data by incorporating a statistical translation model. Preliminary experiments have shown that the algorithm can effectively employ the constraints implicit in bilingual data to extract classes which are well-suited to machine translation tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-562"
  },
  "yang96h_icslp": {
   "authors": [
    [
     "Jae-Woo",
     "Yang"
    ],
    [
     "Youngjik",
     "Lee"
    ]
   ],
   "title": "Toward translating Korean speech into other languages",
   "original": "i96_2368",
   "page_count": 3,
   "order": 567,
   "p1": "2368",
   "pn": "2370",
   "abstract": [
    "This paper describes research activities of ETRI in multi-lingual spontaneous speech translation. We have developed Korean-to-English, Korean-to-Japanese speech translation system prototype that includes 5,000 word spontaneous Korean speech recognizer, Korean-English and Korean-Japanese translators, and Korean speech synthesizer with spontaneous prosody in the travel planning task. We utilize multimedia communication to increase the performance of the spoken language translation system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-563"
  },
  "bub96b_icslp": {
   "authors": [
    [
     "Thomas",
     "Bub"
    ],
    [
     "Johannes",
     "Schwinn"
    ]
   ],
   "title": "VERBMOBIL: the evolution of a complex large speech-to-speech translation system",
   "original": "i96_2371",
   "page_count": 4,
   "order": 568,
   "p1": "2371",
   "pn": "2374",
   "abstract": [
    "The ambitious goal of the project Verbmobil is the development of a portable speech-to-speech translation system dealing with face-to-face dialogs. It constitutes a new generation of translation systems in which spontaneously spoken language, speaker independence and speaker adaptability are among the main features. Verbmobil brings together researchers from the fields of speech processing, computational linguistics and artificial intelligence and goes beyond the state of the art in these areas. Besides the speech and language processing issues, the specific constraints of the project represent an extreme challenge on the part of project management, software engineering and test and evaluation of the system: - size and complexity: 150 researchers from 29 organizations at different sites on three continents are involved in the software development, - integration of heterogeneous software: in order to reuse existing software, hardware and know-how, only a few restrictions were given to the partners. In the following article we describe the Verbmobil scenario, the system architecture and the system evolution.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-564"
  },
  "lavie96b_icslp": {
   "authors": [
    [
     "Alon",
     "Lavie"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "Lori",
     "Levin"
    ],
    [
     "Donna",
     "Gates"
    ],
    [
     "Marsal",
     "Gavaldà"
    ],
    [
     "Torsten",
     "Zeppenfeld"
    ],
    [
     "Puming",
     "Zhan"
    ],
    [
     "Oren",
     "Glickman"
    ]
   ],
   "title": "Translation of conversational speech with JANUS-II",
   "original": "i96_2375",
   "page_count": 4,
   "order": 569,
   "p1": "2375",
   "pn": "2378",
   "abstract": [
    "In this paper we investigate the possibility of translating continuous spoken conversations in a cross-talk environment. This is a task known to be difficult for human translators due to several factors. It is characterized by rapid and even overlapping turn-taking, a high degree of co-articulation, and fragmentary language. We describe experiments using both push-to-talk as well as cross-talk recording conditions. Our results indicate that conversational speech recognition and translation is possible, even in a free crosstalk environment. To date, our system has achieved performances of over 80% acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 70-80% word accuracy. The systems performance on spontaneous conversations recorded in a cross-talk environment is shown to be as good and even slightly superior to the simpler and easier push-to-talk scenario.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-565"
  },
  "edmondson96_icslp": {
   "authors": [
    [
     "William H.",
     "Edmondson"
    ],
    [
     "Jon P.",
     "Iles"
    ],
    [
     "Dorota J.",
     "Iskra"
    ]
   ],
   "title": "Pseudo-articulatory representations in speech synthesis and recognition",
   "original": "i96_2215",
   "page_count": 4,
   "order": 570,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "Pseudo-Articulatory Representations are increasingly being used in work on speech synthesis and recognition. The value of such representations lies in their derivation from linguistic abstractions - they are based on articulatory idealizations used by linguists to describe speech. Iles [4] has demonstrated that using these representations it is possible to overcome the many-to-one problem in mapping articulatory configuration to acoustic signal. In this paper we show how the representations facilitate the details of speech processing, for both synthesis and recognition, and we give details of work in progress on recognition. The role of Pseudo-Articulatory Representations in the development of an integrated approach to synthesis and recognition is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-566"
  },
  "williams96_icslp": {
   "authors": [
    [
     "David R.",
     "Williams"
    ]
   ],
   "title": "Synthesis of initial (/s/-) stop-liquid clusters using HLsyn",
   "original": "i96_2219",
   "page_count": 4,
   "order": 571,
   "p1": "2219",
   "pn": "2222",
   "abstract": [
    "This paper describes synthesis of English syllable-initial stop-liquid and /s/-stop-liquid clusters using the HLsyn speech synthesis program. The articulo-acoustic parameters of HLsyn permit efficient synthesis of most consonant types; the parameter specifications also capture important generalizations about how related sets of consonants are produced. Here, we discuss settings of a small number of parameters that permit synthesis of 60 different phonetic sequences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-567"
  },
  "shih96_icslp": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Synthesis of trill",
   "original": "i96_2223",
   "page_count": 4,
   "order": 572,
   "p1": "2223",
   "pn": "2226",
   "abstract": [
    "Trill is one of the most difficult sound for speech synthesis due to the complexity of the speech signal. The problem need to be addressed since it is a popular sound in world's languages. Several languages in the multi-language text-to-speech system of Bell Laboratories have this sound in the inventory. This paper reports a simple method that greatly improve the quality of trill for the Italian speech synthesizer which does not require any change in the existing synthesis platform.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-568"
  },
  "lo96_icslp": {
   "authors": [
    [
     "W. K.",
     "Lo"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Phone-based speech synthesis with neural network and articulatory control",
   "original": "i96_2227",
   "page_count": 4,
   "order": 573,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "This paper presents a novel method for synthesizing speech signal using a phone-based concatenation approach. Neural network is employed for the generalization of the phone templates during synthesis. Simplified articulatory space input parameters based on a modified vowel diagram are used to provide flexible and effective articulatory control. It also enables the design of an articulatory control model for allophonic variations in speech signal. The network approach is chosen for its non-linear mapping of the relationship between the articulatory space parameters and the spectral information of speech signal. In addition, non-linear approximation for phone template transitions is facilitated. The phone templates of the synthesizer are implicitly stored as network parameters of a medium size network. The performance of this new speech synthesis technique is demonstrated with a prototype system specifically designed for Cantonese (a common Chinese dialect) and the synthetic speech quality is assessed by informal listening tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-569"
  },
  "martland96b_icslp": {
   "authors": [
    [
     "P.",
     "Martland"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Steve W.",
     "Beet"
    ],
    [
     "L.",
     "Baghai-Ravary"
    ]
   ],
   "title": "Analysis of ten vowel sounds across gender and regional/cultural accent",
   "original": "i96_2231",
   "page_count": 4,
   "order": 574,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "ABSTRACT This paper compares ten vowels sounds across gender and accent. Each formant for each vowel was analysed individually across data sets, but no comparison was drawn directly between the formant relationship within each vowel. The objective here was to examine the formants individually across gender and accent to establish a method for transforming vowel quality in a rule-based synthesis system and thus increase its range of voices. Further, it was hoped that it would make the comparison of English formant data across differing accents more simple. Three sets of American English data were utilised in the analysis, and compared against two British English accents - Received Pronunciation (RP) and a General Northern accent (GN). Initial findings suggest that the relative positions of certain vowel formants are particularly static across gender, least variation being found with the second formant frequency. When accent was considered, a greater degree of variation occurred, this being predominantly found with mid-open and mid-closed vowel lasses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-570"
  },
  "abe96b_icslp": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ]
   ],
   "title": "Speech morphing by gradually changing spectrum parameter and fundamental frequency",
   "original": "i96_2235",
   "page_count": 4,
   "order": 575,
   "p1": "2235",
   "pn": "2238",
   "abstract": [
    "This paper proposes a new application of speech modification called \"speech morphing\". In image processing, morphing is a well known technique that gradually changes one person's face to that of someone else. Speech morphing produces similar results for speech; i.e., one person's speech is gradually changed to that of someone else. Speech morphing makes it possible to create movies or multi-media entertainment together with image morphing. The proposed algorithm pitch-synchronously modifies fundamental frequency(F0) and DFT spectrum and outputs high quality speech. To clarify the balance of F0 modification and spectrum modification, listening tests were carried out using 20 male speakers. The results yielded the relationship between the amount of modification and speaker identity. In terms of overall performance, listening tests show that the proposed algorithm successfully generates smooth, high quality voice changes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-571"
  },
  "geoffrois96_icslp": {
   "authors": [
    [
     "Edouard",
     "Geoffrois"
    ]
   ],
   "title": "The multi-lag-window method for robust extended-range F0 determination",
   "original": "i96_2239",
   "page_count": 5,
   "order": 576,
   "p1": "2239",
   "pn": "2242",
   "abstract": [
    "This paper addresses the problem of the fundamental frequency (F0) determination of a speech signal, and proposes four improvements to conventional frequency-domain methods. The major improvement is a multi-scale analysis which extends the range of F0 that can be correctly processed. It builds on the lag-window method proposed by Sagayama (1978), hence the name \"multi-lag-window\". Secondly, a modification of the lag-window method itself improves its robustness to periodic noises (while loosing its gain-independence property). Thirdly, a rescaling is introduced to permit a full Dynamic Programming search for the optimal F0 curve. Finally, a mathematically justified peak interpolation is proposed for replacing the conventional, inaccurate parabolic interpolation. These four improvements result in an accurate, robust, extended range F0 determination method, which was tested on spontaneous speech from 20 speakers, ranging from less than 50 Hz to more than 600 Hz.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-572"
  },
  "barner96_icslp": {
   "authors": [
    [
     "Kenneth E.",
     "Barner"
    ]
   ],
   "title": "Nonlinear estimation of DEGG signals with applications to speech pitch detection",
   "original": "i96_2243",
   "page_count": 4,
   "order": 577,
   "p1": "2243",
   "pn": "2246",
   "abstract": [
    "Speech pitch detection remains a fundamental problem due to importance in numerous aspects of speech processing. Current pitch detectors focus on determining the Glottal Closure Instant (GCI). Accurate GCI measures can be obtained from the Differentiated Electroglottograph (DEGG) signal. Unfortunately, DEGG signals are not available in most practical applications. A novel method of pitch detection is proposed here based on the nonlinear estimation of DEGG signals from the acoustic speech waveform. This method requires the DEGG signals only during optimization. In operation, the proposed pitch detector marks glottal closures based strictly on the acoustical speech waveform. In addition to the algorithm development, performance comparison results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-573"
  },
  "maidment96_icslp": {
   "authors": [
    [
     "John A.",
     "Maidment"
    ],
    [
     "M. Luisa",
     "Garcia-Lecumberri"
    ]
   ],
   "title": "Pitch analysis methods for cross-speaker comparison",
   "original": "i96_2247",
   "page_count": 3,
   "order": 578,
   "p1": "2247",
   "pn": "2249",
   "abstract": [
    "A system of fundamental frequency analysis and normalisation is described for obtaining pitch data and comparing them across speakers.  This system was used for the analysis of English and Spanish speakers' productions in order to compare the realization of accentual focus in the two languages. The system is based on the simultaneous recording of speech and the laryngeal signal. The latter is monitored by means of an electrolaryngograph. The analysis is done in three stages: (a) auditory analysis, (b) PCLX analysis obtaining measures in Hz for peaks, troughs and other relevant points in the contour and also fundamental frequency statistics from the complete data set for each speaker, (c) a detailed analysis using SFS with simultaneous display of speech pressure waveform, Lx waveform, excitation period measurements and fundamental frequency trace and with playback facilities for speech and F0. The analysis described at (c) is used for segmentation, to filter out nodes which are due to micro-intonation, and to pinpoint problem areas in the F0 trace. Outlying and anomalous period measurements may be replaced by a five-point median value. The resulting contours are normalised by converting Hz measures to percentage values (positive or negative) of the speaker's mean F0 which is obtained from the analysis described at (b).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-574"
  },
  "beet96_icslp": {
   "authors": [
    [
     "Steve W.",
     "Beet"
    ],
    [
     "L.",
     "Baghai-Ravary"
    ]
   ],
   "title": "Continuous adaptation of linear models with impulsive excitation",
   "original": "i96_2250",
   "page_count": 4,
   "order": 579,
   "p1": "2250",
   "pn": "2253",
   "abstract": [
    "This paper presents a new approach to continuously-adaptive system modelling, designed for the analysis of autoregressive (AR) systems excited by signals including an impulsive component. Voiced speech is well represented by such a model, and is used to demonstrate the advantages of the new- approach These include: 1. AR model parameter estimates are more stable in the region of pitch events. 2. A faster adaptation rate can be used, reducing the recovery time after plosives or other sudden changes in signal statistics. The new method is based on multiple simultaneous estimates of each sample, using separate but related estimators. The general concept is illustrated here using a linear prediction (LP) approach to continuously-adaptive autoregressive (AR) modelling, based on the least mean square (LMS) algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-575"
  },
  "ohno96_icslp": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Masamichi",
     "Fukumiya"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Quantitative analysis of the local speech rate and its application to speech synthesis",
   "original": "i96_2254",
   "page_count": 4,
   "order": 580,
   "p1": "2254",
   "pn": "2257",
   "abstract": [
    "On the basis of the short-time relative speech rate defined by the authors, this paper examines the optimum width of the smoothing window by perceptual experiments on the naturalness of re-synthesized speech. With the optimum window of 270 ms, relative speech rates are obtained both for fast and slow utterances of the same sentence, using an utterance produced at a normal speech rate. The averaged results show that the speech rate control function for an utterance can be approximately decomposed into a global component for each sentence and local components for each bunsetsu and each major syntactic boundary. Based on these results, a scheme is presented for controlling the local speech rate of a reference utterance to obtain a synthetic utterance of an arbitrary global speech rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-576"
  },
  "verhasselt96_icslp": {
   "authors": [
    [
     "Jan P.",
     "Verhasselt"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "A fast and reliable rate of speech detector",
   "original": "i96_2258",
   "page_count": 4,
   "order": 581,
   "p1": "2258",
   "pn": "2261",
   "abstract": [
    "In this paper, we present a new rate of speech (ROS) detector that operates independently of the recognition process. This detector is evaluated on the TIMIT corpus and positioned with respect to other ROS detectors. The ROS estimate is subsequently used to compensate for the effects of unusual speech rates on continuous speech recognition. We report on results obtained with two ROS compensation techniques on a speaker independent acoustic phonetic decoding task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-577"
  },
  "zhan96_icslp": {
   "authors": [
    [
     "Puming",
     "Zhan"
    ],
    [
     "Klaus",
     "Ries"
    ],
    [
     "Marsal",
     "Gavaldà"
    ],
    [
     "Donna",
     "Gates"
    ],
    [
     "Alon",
     "Lavie"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "JANUS-II: towards spontaneous Spanish speech recognition",
   "original": "i96_2285",
   "page_count": 4,
   "order": 582,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "JANUS-II is a research system for investigating various issues in speech-to-speech translations and has been implemented for speech-to-speech translations on many languages [1]. In this paper, we address the Spanish speech recognition part of JANUS-II. First, we report the bootstrap and optimization of the recognition system. Then we investigate the difference between push-to-talk and cross-talk dialogs, which are two different kinds of data in our database. We give a detail noise analysis for the push-to-talk and cross-talk dialogs and present some recognition results for the comparison. We have observed that the cross-talk dialogs are harder than the push-to-talk dialogs for speech recognition, because they are more noisy than the latter. Currently, the error rate of our Spanish recognizer is 27% for push-to-talk test set and 32% for cross-talk test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-578"
  },
  "demuynck96_icslp": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Dirk van",
     "Compernolle"
    ]
   ],
   "title": "Reduced semi-continuous models for large vocabulary continuous speech recognition in Dutch",
   "original": "i96_2289",
   "page_count": 4,
   "order": 583,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "Semi-continuous Density HMMs have - due to the decoupling between the set of gaussians and the other HMM-parameters - more possibilities than Continuous Density HMMs to match the number of parameters in the model to the available train data. The computational load of the SC-HMMs however is huge compared to the load of their continuous counterparts, because of the large mixture weighting vector and because of the fact that for each frame all gaussians have to be evaluated. This paper describes the different steps taken to reduce the computational load of the SC-HMMs, resulting in faster and better models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-579"
  },
  "constantinescu96_icslp": {
   "authors": [
    [
     "Andrei",
     "Constantinescu"
    ],
    [
     "Olivier",
     "Bornet"
    ],
    [
     "Gilles",
     "Caloz"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Validating different flexible vocabulary approaches on the Swiss French Polyphone and Polyvar databases",
   "original": "i96_2293",
   "page_count": 4,
   "order": 584,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "In this paper, we attempt to validate the flexible vocabulary approach for speaker independent isolated word and connected words recognition. We compare the performance of classical whole word HMMs against different sets of subword units. For this purpose, we model phonemes, diphones and words of the (Swiss) French language. The recognition rates obtained with phoneme models are monitored as we increase the amount of training data. The results of the described experiments validate the flexible vocabulary approach and show advantages and disadvantages of both proposed subword units against common whole word HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-580"
  },
  "yoma96_icslp": {
   "authors": [
    [
     "Nestor Becérra",
     "Yoma"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Use of a reliability coefficient in noise cancelling by neural net and weighted matching algorithms",
   "original": "i96_2297",
   "page_count": 4,
   "order": 585,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "The problems of efficacy estimation in noise cancelling by a neural net (LIN-Lateral Inhibition Net [5]) and the use of this information in weighting matching algorithms are focused. Since the effect of noise on the speech signal is variable and the backpropagation training algorithm is essentially stochastic (most common patterns have more influence in the weights re-estimation process), it is reasonable to suppose that the LIN efficacy depends on the input and each noisy frame could be associated to a reliability coefficient that attempts to measure how reliable is the result of the neural net processing. Isolated word recognition experiments have shown that reliability weighting can result in a mean error rate reduction as high as 96, 80, 58 and 36 % at SNR=12, 6, 3 and 0dB, respectively, when the noise is white Gaussian.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-581"
  },
  "ozeki96_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Likelihood normalization using an ergodic HMM for continuous speech recognition",
   "original": "i96_2301",
   "page_count": 4,
   "order": 586,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "In recent speech recognition technology, the score of a hypothesis is often defined on the basis of HMM likelihood. As is well known, however, direct use of the likelihood as a scoring function causes difficult problems especially when the length of a speech segment varies depending on the hypothesis as in word-spotting, and some kind of normalization is indispensable. In this paper, a new method of likelihood normalization using an ergodic HMM is presented, and its performance is compared with those of conventional ones. The comparison is made from three points of view: recognition rate, word-end detection power, and the mean hypothesis length. It is concluded that the proposed method gives the best overall performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-582"
  },
  "candille96_icslp": {
   "authors": [
    [
     "Laurence",
     "Candille"
    ],
    [
     "Henri",
     "Méloni"
    ]
   ],
   "title": "Dynamic control of a production model",
   "original": "i96_2305",
   "page_count": 4,
   "order": 587,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "A number of experiments have shown that it is possible to use production models for speech recognition tasks [6] and [2]. We present here the first results of an adaptation of Maeda's statistic model. We have also demonstrated the importance of taking into account the static and dynamic characteristics of the speaker. Some preliminary results for the identification of V1-V2 sequences are also provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-583"
  },
  "hattori96_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Hattori"
    ],
    [
     "Eiko",
     "Yamada"
    ]
   ],
   "title": "Speech recognition using sub-word units dependent on phonetic contexts of both training and recognition vocabularies",
   "original": "i96_2309",
   "page_count": 4,
   "order": 588,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "This paper proposes a new speech recognition algorithm using a new context-dependent recognition unit design method for efficient and precise acoustic modeling. This algorithm uses both training and recognition vocabularies to select context-dependent units which precisely represent acoustic variations due to phonetic contexts in a recognition vocabulary. An efficient training algorithm for selected context-dependent units is also proposed. In speaker-independent isolated-word recognition experiments, the proposed algorithm gave a 11% error reduction for 5000 word recognition, and gave a 43% error reduction for 10 digit recognition. These results confirmed the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-584"
  },
  "jacob96_icslp": {
   "authors": [
    [
     "Bruno",
     "Jacob"
    ],
    [
     "Christine",
     "Senac"
    ]
   ],
   "title": "Hidden Markov models merging acoustic and articulatory information to automatic speech recognition",
   "original": "i96_2313",
   "page_count": 3,
   "order": 589,
   "p1": "2313",
   "pn": "2315",
   "abstract": [
    "This paper describes a new scheme for robust speech recognition systems where visual information and acoustic features are merged. Using as robust unit the « pseudo-diphone », we compare a global Hidden Markov Model (HMM) and a Master/Slave HMM through a centisecond preprocessing and through a segmental one. We confirm by experimentation the importance of articulatory features in clean and noisy environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-585"
  },
  "blomberg96_icslp": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Kjell",
     "Elenius"
    ]
   ],
   "title": "Creation of unseen triphones from diphones and monophones using a speech production approach",
   "original": "i96_2316",
   "page_count": 4,
   "order": 590,
   "p1": "2316",
   "pn": "2319",
   "abstract": [
    "With limited training data, infrequent triphone models for speech recognition will not be observed in sufficient number. In this report, a speech production approach is used to predict the characteristics of unseen triphones by concatenating diphones and/or monophones in the parametric representation of a formant speech synthesiser. The parameter trajectories are estimated by interpolation between the endpoints of the original units. The spectral states of the created triphone are generated by the speech synthesiser. Evaluation of the proposed technique has been performed using spectral error measurements and recognition candidate rescoring of N-best lists. In both cases, the created triphones are shown to perform better than the shorter units from which they were constructed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-586"
  },
  "xu96_icslp": {
   "authors": [
    [
     "Bo",
     "Xu"
    ],
    [
     "Bing",
     "Ma"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Fei",
     "Qu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Speaker-independent dictation of Chinese speech with 32k vocabulary",
   "original": "i96_2320",
   "page_count": 4,
   "order": 591,
   "p1": "2320",
   "pn": "2323",
   "abstract": [
    "While early machines adopted isolated syllable as input units and needed boring enrollment, our research focus on the speaker-independent, word-based dictation. A deliberately designed 120-speaker database was built for training ; inter-syllable context ,tonal and endpoint dependent acoustic model are applied with promising MFCC feature; Two-pass acoustic matching accelerates the recognition making fully advantage of the monosyllabic structure of Chinese speech; A complete word bigram and trigram serve as language processing module. With all efforts, the system reaches 90% character accuracy performing in almost real-time on Pentium PC without DSP help.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-587"
  },
  "humphries96_icslp": {
   "authors": [
    [
     "J. J.",
     "Humphries"
    ],
    [
     "P. C.",
     "Woodland"
    ],
    [
     "D.",
     "Pearce"
    ]
   ],
   "title": "Using accent-specific pronunciation modelling for robust speech recognition",
   "original": "i96_2324",
   "page_count": 4,
   "order": 592,
   "p1": "2324",
   "pn": "2327",
   "abstract": [
    "A method of modelling accent-specific pronunciation variations is presented. Speech from an unseen accent group is phonetically transcribed such that pronunciation variations may be derived. These context-dependent variations are clustered in a decision tree which is used as a model of the pronunciation variation associated with this new accent group. The tree is then used to build a new pronunciation dictionary for use during the recognition process. Experiments are presented for the recognition of Lancashire&Yorkshire accented speech using a recognizer trained on London& South East England speakers. The results show that the addition of accent-specific pronunciations can reduce the error rate by almost 20% for cross accent recognition. It is also shown that worthwhile gains in performance can be obtained using only a small amount of accent-specific data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-588"
  },
  "sloboda96_icslp": {
   "authors": [
    [
     "Tilo",
     "Sloboda"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Dictionary learning for spontaneous speech recognition",
   "original": "i96_2328",
   "page_count": 4,
   "order": 593,
   "p1": "2328",
   "pn": "2331",
   "abstract": [
    "Spontaneous speech adds a variety of phenomena to a speech recognition task: false starts, human and nonhuman noises, new words, and alternative pronunciations. All of these phenomena have to be tackled when adapting a speech recognition system for spontaneous speech. In this paper we will focus on how to automatically expand and adapt phonetic dictionaries for spontaneous speech recognition. Especially for spontaneous speech it is important to choose the pronunciations of a word according to the frequency in which they appear in the database rather than the \\correct\" pronunciation as might be found in a lexicon. Therefore, we proposed a data-driven approach to add new pronunciations to a given phonetic dictionary [1] in a way that they model the given occurrences of words in the database. We will show how this algorithm can be extended to produce alternative pronunciations for word tuples and frequently misrecognized words. We will also discuss how further knowledge can be incorporated into the phoneme recognizer in a way that it learns to generalize from pronunciations which were found previously. The experiments have been performed on the German Spontaneous Scheduling Task (GSST), using the speech recognition engine of JANUS 2, the spontaneous speech-to-speech translation system of the Interactive Systems Laboratories at Carnegie Mellon and Karlsruhe University [2, 3].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-589"
  },
  "veth96_icslp": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Comparison of channel normalisation techniques for automatic speech recognition over the phone",
   "original": "i96_2332",
   "page_count": 4,
   "order": 594,
   "p1": "2332",
   "pn": "2335",
   "abstract": [
    "We compared three different channel normalisation (CN) methods in the context of a connected digit recognition task over the phone: ceptrum mean substraction (CMS), RASTA filtering and the Gaussian dynamic cepstrum representation (GDCR). Using a small set of context-independent (CI) continuous Gaussian mixture hidden Markov models (HMMs) we found that CMS and RASTA outperformed the GDCR technique. We show that the main cause for the superiority of CMS compared to RASTA is the phase distortion introduced by the RASTA filter. Recognition results for a phase-corrected RASTA technique are identical to those of CMS. Our results indicate that an ideal cepstrum based CN method should (1) effectively remove the DC-component, (2) at least preserve modulation frequencies in the range 2-16 Hz and (3) introduce no phase distortion in case CI HMMs are used for recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-590"
  },
  "leandro96_icslp": {
   "authors": [
    [
     "Manuel A.",
     "Leandro"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Anchor point detection for continuous speech recognition in Spanish: the spotting of phonetic events",
   "original": "i96_2336",
   "page_count": 4,
   "order": 595,
   "p1": "2336",
   "pn": "2339",
   "abstract": [
    "Several techniques have been used to constraint the search space in an HMM based continuous speech recognition system (grammar, beam search, etc.) in order to reduce computation without significant lose in performance [1]. The use of anchor points is a state-of-the-art technique that has already been used in some systems [2][3][4]. This approach has lead us to a bottom-up strategy in a continuous speech recognition system in which the first module performs the spotting of predefined phonetic events and the second module uses them as anchor points in order to guide the HMM-based recognition task. This paper describes the definition of phonetic events for Spanish (based on expert knowledge of language) and the algorithms used for their detection and classification. Figures of performance are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-591"
  },
  "raj96_icslp": {
   "authors": [
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Evandro Bacci",
     "Gouvêa"
    ],
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Cepstral compensation by polynomial approximation for environment-independent speech recognition",
   "original": "i96_2340",
   "page_count": 4,
   "order": 596,
   "p1": "2340",
   "pn": "2343",
   "abstract": [
    "Speech recognition systems perform poorly on speech degraded by even simple effects such as linear filtering and additive noise. One possible solution to this problem is to modify the probability density function (PDF) of clean speech to account for the effects of the degradation. However, even for the case of linear filtering and additive noise, it is extremely difficult to do this analytically. Previously attempted analytical solutions to the problem of noisy speech recognition have either used an overly-simplified mathematical description of the effects of noise on the statistics of speech, or they have relied on the availability of large environment-specific adaptation sets. Some of the previous methods required the use of adaptation data that consists of simultaneously-recorded or \"stereo\" recordings of clean and degraded speech. In this paper we introduce an approximation-based method to compute the effects of the environment on the parameters of the PDF of clean speech. In this work, we perform compensation by Vector Polynomial approximationS (VPS) for the effects of linear filtering and additive noise on the clean speech. We also estimate the parameters of the environment, namely the noise and the channel, by using piecewiselinear approximations of these effects. We evaluate the performance of this method (VPS) using the CMU SPHINX-II system and the 100-word alphanumeric CENSUS database. Performance is evaluated at several SNRs, with artificial white Gaussian noise added to the database. VPS provides improvements of up to 15 percent in relative recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-592"
  },
  "lilly96_icslp": {
   "authors": [
    [
     "B. T.",
     "Lilly"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Effect of speech coders on speech recognition performance",
   "original": "i96_2344",
   "page_count": 4,
   "order": 597,
   "p1": "2344",
   "pn": "2347",
   "abstract": [
    "Speech coders with bitrates as low as 2.4 kbits/s are now being developed for speech transmission in the telecommunications industry. For speech coders to work at this reduced bitrate, some speech information has to be removed and it is only natural to expect that the performance of speech recognition systems will deteriorate when coded speech is applied as input to a recognition system. In this paper, the results of a study to examine the effects speech coders have on speech recogntion are presented. Six different speech coders ranging from 4.8 kbits/s to 40 kbits/s are used with two different speech recognition systems 1) isolated word recogntion and 2) phoneme recogntion from continuous speech. The effects on speech recognition performance by tandeming each of the speech coders are also presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-593"
  },
  "janer96b_icslp": {
   "authors": [
    [
     "Léonard",
     "Janer"
    ],
    [
     "Josep",
     "Martí"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Eduardo",
     "Lleida-Solano"
    ]
   ],
   "title": "Wavelet transforms for non-uniform speech recogntion systems",
   "original": "i96_2348",
   "page_count": 4,
   "order": 598,
   "p1": "2348",
   "pn": "2351",
   "abstract": [
    "A new algorithm for non-uniform speech segmentation and its application in speech recognition systems is presented. A new method based on the Modulated Gaussian Wavelet Transform based Speech Analyser (MGWTSA) and the subsequent Parametrization block is used to transform a uniformly signal into a set of non-uniformly separated frames, with the accurate information to be fed to our speech recognition system. Our algorithm wants to have a frame characterizing the signal where it is necessary, trying to reduce as much as possible the number of frames per signal, without an appreciable reduction in the recognition rate of the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-594"
  },
  "usagawa96_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Usagawa"
    ],
    [
     "Markus",
     "Bodden"
    ],
    [
     "Klaus",
     "Rateitschek"
    ]
   ],
   "title": "A binaural model as a front-end for isolated word recognition",
   "original": "i96_2352",
   "page_count": 4,
   "order": 599,
   "p1": "2352",
   "pn": "2355",
   "abstract": [
    "Small vocabulary isolated word speech recognition can be implemented on relative small hardware. Although the recognition problem is more or less solved in noise-free situations, the general application is hindered because of the dramatic decrease of performance in noisy environments, especially for hands-free applications. In this paper a binaural front-end for speech recognition is presented. This binaural model, which was originally developed at Ruhr-University of Bochum in Germany, allows for an effective reduction of interfering noises of any kind. Besides stationary noises also concurrent speech signals can be suppressed. The original model was designed as a precise computer model of the human binaural auditory system and can explain a variety of psycho-acoustical phenomenon. Besides those abilities the model offers sharp directional selectivity which is superior to those obtained with directional microphones. We simplified this sophisticated model by adapting it to the specific task and use the peak position and the peak level of the binaural activity pattern for each frequency band as a parameter for pattern matching. The performance was evaluated in the form of recognition rates for a variety of difference noisy environments. The results show that the binaural front-end leads to a significant improvement in recognition rates corresponding to an enhancement of over 20dB in SNR in most cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-595"
  },
  "okuno96_icslp": {
   "authors": [
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "A new speech enhancement: speech stream segregation",
   "original": "i96_2356",
   "page_count": 4,
   "order": 600,
   "p1": "2356",
   "pn": "2359",
   "abstract": [
    "Speech stream segregation is presented as a new speech enhancement for automatic speech recognition. Two issues are addressed: speech stream segregation from a mixture of sounds, and interfacing speech stream segregation with automatic speech recognition. Speech stream segregation is modeled as a process of extracting harmonic fragments, grouping these extracted harmonic fragments, and substituting non-harmonic residue for non-harmonic parts of groups. The main problem in interfacing speech stream segregation with HMM-based speech recognition is how to improve the degradation of recognition performance due to spectral distortion of segregated sounds, which is caused mainly by transfer function of a binaural input. Our solution is to re-train the parameters of HMM with training data binauralized for four directions. Experiments with 500 mixtures of two women's utterances of a word showed that the cumulative accuracy of word recognition up to the 10th candidate of each woman's utterance is, on average, 75%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-596"
  },
  "slater96_icslp": {
   "authors": [
    [
     "Andrew",
     "Slater"
    ],
    [
     "John",
     "Coleman"
    ]
   ],
   "title": "Non-segmental analysis and synthesis based on a speech database",
   "original": "i96_2379",
   "page_count": 4,
   "order": 601,
   "p1": "2379",
   "pn": "2382",
   "abstract": [
    "This paper reports on experiments in non-segmental speech analysis and synthesis using parameters derived from a speech database of British English monosyllables. The database includes almost every onset, nucleus and coda, and almost all onset-nucleus and nucleus-consonant combinations occurring in English. Acoustic parameters including f0, formant frequencies and bandwidths, and amplitude of voicing were determined for each token in the database. Fine duration differences within minimal pairs are analyzed using dynamic time warping techniques, avoiding the need for manual segmentation. For each parameter, a matrix of distances between all samples of the two words is calculated, together with a minimal path through the matrix (the warp path). The set of warp paths for all parameters identifies the nature and location of acoustic differences between the words, including locations of temporal expansion and compression. Preliminary experiments using dynamic time warping for non-segmental synthesis are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-597"
  },
  "benzmuller96_icslp": {
   "authors": [
    [
     "Ralf",
     "Benzmüller"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "Microsegment synthesis - economic principles in a low-cost solution",
   "original": "i96_2383",
   "page_count": 4,
   "order": 602,
   "p1": "2383",
   "pn": "2386",
   "abstract": [
    "A low-cost concatenation based speech synthesis system for German is described which combines the advantage of minimal memory requirements with good intelligibility and high segmental and prosodic acceptability. This is achieved by the multiple use of \"microsegments\", stretches of speech signal varying in length from demi-phone to phone size. All prosodic structuring is carried out in the time domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-598"
  },
  "huang96_icslp": {
   "authors": [
    [
     "X. D.",
     "Huang"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "J.",
     "Adcock"
    ],
    [
     "H. W.",
     "Hon"
    ],
    [
     "J.",
     "Goldsmith"
    ],
    [
     "J.",
     "Liu"
    ],
    [
     "Mike",
     "Plumpe"
    ]
   ],
   "title": "Whistler: a trainable text-to-speech system",
   "original": "i96_2387",
   "page_count": 4,
   "order": 603,
   "p1": "2387",
   "pn": "2390",
   "abstract": [
    "We introduce Whistler, a trainable Text-to-Speech (TTS) system, that automatically learns the model parameters from a corpus. Both prosody parameters and concatenative speech units are derived through the use of probabilistic learning methods that have been successfully used for speech recognition. Whistler can produce synthetic speech that sounds very natural and resembles the acoustic and prosodic characteristics of the original speaker. The underlying technologies used in Whistler can significantly facilitate the process of creating generic TTS systems for a new language, a new voice, or a new speech style.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-599"
  },
  "portele96b_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Karl-Heinz",
     "Stöber"
    ],
    [
     "Horst",
     "Meyer"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "Generation of multiple synthesis inventories by a bootstrapping procedure",
   "original": "i96_2391",
   "page_count": 4,
   "order": 604,
   "p1": "2391",
   "pn": "2394",
   "abstract": [
    "In concatenative speech synthesis systems the generation of a unit inventory is a tedious task. However, some applications demand multiple voices.\n",
    "A semiautomatic method to generate unit inventories is proposed. The units are segmented out of carrier phrases by means of dynamic time warping alignment with a synthesized utterance. This requires at least one existing inventory. The availability of several existing inventories will improve the likelihood of finding one with similar voice characteristics, which will improve the accuracy of results. The method is a bootstrapping procedure. To choose the best segmentation out of a set (e.g. aligned with each voice already implemented) a penalty system was developed that uses timing constraints. The results were compared with manually corrected segmentations and show the validity of this approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-600"
  },
  "mobius96_icslp": {
   "authors": [
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Modeling segmental duration in German text-to-speech synthesis",
   "original": "i96_2395",
   "page_count": 4,
   "order": 605,
   "p1": "2395",
   "pn": "2398",
   "abstract": [
    "This paper reports on the construction of a model for segmental duration in German. The model predicts the durations of speech sounds in various textual, prosodic, and segmental contexts. It has been implemented in the German version of the Bell Labs text-to-speech system [18, 12]. The construction of the duration system was made efficient by the use of an interactive statistical analysis package that incorporates the approach outlined in [23]. The results are stored in tables in a format that can be directly interpreted by the TTS duration module. Tables are constructed in two phases: inferential-statistical analysis of the speech corpus, and parameter estimation. The overall correlation between observed and predicted segmental durations is .896.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-601"
  },
  "campbell96c_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Autolabelling Japanese ToBI",
   "original": "i96_2399",
   "page_count": 4,
   "order": 606,
   "p1": "2399",
   "pn": "2402",
   "abstract": [
    "Focussing on the prosodic labelling of Japanese as an example, this paper describes the application of speech synthesis technology in a variety of speech processing tasks. It discusses first the use of synthesised utterances in the forced alignment and segmentation of a speech corpus, then the use of generated prosodic contours to determine the prosodic phrasing of an utterance, and finally the comparison with speech resynthesised using the prosodic transcription of the original utterance in order to check the transcription. It closes with an analysis of results from an auto-transcription of Japanese ToBI, and discusses some limitations of the proposed J-ToBI system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-602"
  },
  "parthasarathy96_icslp": {
   "authors": [
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "Aaron E.",
     "Rosenberg"
    ]
   ],
   "title": "General phrase speaker verification using sub-word background models and likelihood-ratio scoring",
   "original": "i96_2403",
   "page_count": 4,
   "order": 607,
   "p1": "2403",
   "pn": "2406",
   "abstract": [
    "We present a design and study the performance of a text-dependent speaker verification system using general phrase passwords. The text of the password utterance and its phone transcription are assumed to be available. The problems that are addressed include the appropriate choice of units for building target speaker models and the choice of background models for likelihood-ratio scoring.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-603"
  },
  "murakami96_icslp": {
   "authors": [
    [
     "J.",
     "Murakami"
    ],
    [
     "M.",
     "Sugiyama"
    ],
    [
     "H.",
     "Watanabe"
    ]
   ],
   "title": "Unknown-multiple signal source clustering problem using ergodic HMM and applied to speaker classification",
   "original": "i96_2407",
   "page_count": 4,
   "order": 608,
   "p1": "2407",
   "pn": "2410",
   "abstract": [
    "In this paper, we consider signals originated from a sequence of sources. More specifically, the problems of segmenting such signals and relating the segments to their sources are addressed. This issue has wide applications in many fields. This report describes a resolution method that is based on an Ergodic Hidden Markov Model (HMM), in which each HMM state corresponds to a signal source. The signal source sequence can be determined by using a decoding procedure (Viterbi algorithm or Forward algorithm) over the observed sequence. Baum-Welch training is used to estimate HMM parameters from the training material. As an example of the multiple signal source classification problem, an experiment is performed on unknown speaker classification. The results show a classification rate of 79% for 4 male speakers. The results also indicate that the model is sensitive to the initial values of the Ergodic HMM and that employing the long-distance LPC cepstrum is effective for signal preprocessing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-604"
  },
  "floch96_icslp": {
   "authors": [
    [
     "J.-L. Le",
     "Floch"
    ],
    [
     "C.",
     "Montacié"
    ],
    [
     "M.-J.",
     "Caraty"
    ]
   ],
   "title": "GMM and ARVM cooperation and competition for text-independent speaker recognition on telephone speech",
   "original": "i96_2411",
   "page_count": 4,
   "order": 609,
   "p1": "2411",
   "pn": "2414",
   "abstract": [
    "In order to improve the performances of speaker recognition on telephone speech, we investigate the ability to cooperate of two different natures modelizations: the GMM and the ARVM. For the cooperation and competition of the GMM and ARVM modelizations, we used normalized measures. We develop two approaches for these cooperation and competition: a global approach and an analytical approach. We investigate experiments on whole sentences or selected phonetic segments. Theses approaches allow us to obtain performances improvements for both cooperation and competition, and good results on 168 speakers of the NTIMIT database (GMM: 61.7 %, ARVM: 78.1 %, cooperation: 79.9 % and competition: 82.6 %).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-605"
  },
  "lin96_icslp": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "ChiWei",
     "Che"
    ],
    [
     "Dong-Suk",
     "Yuk"
    ],
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "Selective use of the speech spectrum and a VQGMM method for speaker identification",
   "original": "i96_2415",
   "page_count": 4,
   "order": 610,
   "p1": "2415",
   "pn": "2418",
   "abstract": [
    "This paper describes two separate sets of speaker identification experiments. In the first set of experiments, the speech spectrum is selectively used for speaker identification. The results show that the higher portion of the speech spectrum contains more reliable idiosyncratic information on speakers than does the lower portion of equal bandwidth. In the second set of experiments, a vector-quantization based Gaussian mixture models (VQGMMs) is developed for text-independent speaker identification. The system has been evaluated in the recent speaker identification evaluation organized by NIST. In this paper, details of the system design are given and the evaluation results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-606"
  },
  "newman96_icslp": {
   "authors": [
    [
     "Michael",
     "Newman"
    ],
    [
     "Larry",
     "Gillick"
    ],
    [
     "Yoshiko",
     "Ito"
    ],
    [
     "Don",
     "McAllaster"
    ],
    [
     "Barbara",
     "Peskin"
    ]
   ],
   "title": "Speaker verification through large vocabulary continuous speech recognition",
   "original": "i96_2419",
   "page_count": 4,
   "order": 611,
   "p1": "2419",
   "pn": "2422",
   "abstract": [
    "We present a study of a speaker verification system for telephone data based on large-vocabulary speech recognition. After describing the recognition engine, we give details of the verification algorithm and draw comparisons with other systems. The system has been tested on a test set taken from the Switchboard corpus of conversational telephone speech, and we present results showing how performance varies with length of test utterance, and whether or not the training data has been transcribed. The dominant factor in performance appears to be channel or handset mismatch between training and testing data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-607"
  },
  "paoloni96_icslp": {
   "authors": [
    [
     "Andrea",
     "Paoloni"
    ],
    [
     "Susanna",
     "Ragazzini"
    ],
    [
     "G.",
     "Ravaioli"
    ]
   ],
   "title": "Predictive neural networks in text independent speaker verification: an evaluation on the SIVA database",
   "original": "i96_2423",
   "page_count": 4,
   "order": 612,
   "p1": "2423",
   "pn": "2426",
   "abstract": [
    "In this paper we propose a system which combines the use of predictive neural networks and the statistical approach in the task of text-independent speaker verification through the telephone line. The system is composed by a predictive neural network for every reference speaker, which is trained with the backpropagation algorithm and the maximum likelihood criterion, in order to obtain the highest probability when the input to the network belongs to the reference speaker. We also consider a global network trained on the whole training set whose likelihood gives a measure of the predictability of a given input with the aim to eliminate the strong dependence of the score from the particular input considered. The evaluation of the system is carried out on a subset of the Italian telephonic database SIVA, purposely collected for the considered task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-608"
  },
  "shrotriya96_icslp": {
   "authors": [
    [
     "Nisheeth",
     "Shrotriya"
    ],
    [
     "Rajesh",
     "Verma"
    ],
    [
     "Sunil K.",
     "Gupta"
    ],
    [
     "S. S.",
     "Agrawal"
    ]
   ],
   "title": "Durational characterstics of hindi consonant clusters",
   "original": "i96_2427",
   "page_count": 5,
   "order": 613,
   "p1": "2427",
   "pn": "2430",
   "abstract": [
    "In the present study various durations of closure, preceding vowel etc. have been studied in meaningful Hindi two consonant cluster words with stop consonants ( such as /s ptah/ (week) and / bd (word)). The data included 80 most frequently occurring clusters of Hindi language. All these words were recorded by five male speakers and analysed using the Sensimetrics speech station software package. The analysis showed some very interesting features of the clusters such as closure duration of the clusters is found to play a very important role for the different categories of stop consonants. Further the duration of the voice bar and the vowel preceding the cluster (-C1C2) is shortened in a cluster words as compared to that for a non-cluster word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-609"
  },
  "tan96_icslp": {
   "authors": [
    [
     "Beng T.",
     "Tan"
    ],
    [
     "Minyue",
     "Fu"
    ],
    [
     "Andrew",
     "Spray"
    ],
    [
     "Phillip",
     "Dermody"
    ]
   ],
   "title": "The use of wavelet transforms in phoneme recognition",
   "original": "i96_2431",
   "page_count": 4,
   "order": 614,
   "p1": "2431",
   "pn": "2434",
   "abstract": [
    "This study investigates the usefulness of wavelet transforms in phoneme recognition. Both discrete wavelet transforms (DWT) and sampled continuous wavelet transforms (SCWT) are tested. The wavelet transform is used as a part of the front-end processor which extracts feature vectors for a speaker-independent HMM-based phoneme recognizer. The results are evaluated on a portion of TIMIT corpus consisting of 30293 phoneme tokens for training and 14489 phoneme tokens for testing. The test results suggest that SCWT gives considerably better recognition rate than DWT. On the other hand, the improvement of SCWT over Mel-scale cepstral coefficients appears to be marginal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-610"
  },
  "kuwabara96_icslp": {
   "authors": [
    [
     "Hisao",
     "Kuwabara"
    ]
   ],
   "title": "Acoustic properties of phonemes in continuous speech for different speaking rate",
   "original": "i96_2435",
   "page_count": 4,
   "order": 615,
   "p1": "2435",
   "pn": "2438",
   "abstract": [
    "An investigation has been made for individual phonemes focusing mainly on their duration in continuous speech spoken in different rates: fast, normal, and slow. Fifteen short sentences uttered by four male speakers have been used as the speech material which comprises a total of 291 morae. Normal speaking rate (n-speech) is, on average, 150 milliseconds/mora (or 400 morae/minute) and the four speakers have been asked to read the sentences twice as fast as (f-speech) and 1/2 limes as slow as (s-speech) the normal speed in reference to the n-speech. Among consonants, the greatest influence has been found to occur on the syllabic nasal /N/ and the least on the voiceless stop /t/ in f-speech. For the s-speech, /N/ has also been found to be the greatest but the least is voiced stop /d/. The ratio of duration between consonant and vowel of a CV-syllable in the f-speech is kept almost the same as that in the n-speech while vowel lengthening becomes significantly large in the s-speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-611"
  },
  "fujisaki96b_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "Prosodic parameterization of spoken Japanese based on a model of the generation process of F0 contours",
   "original": "i96_2439",
   "page_count": 4,
   "order": 616,
   "p1": "2439",
   "pn": "2442",
   "abstract": [
    "The process of generating an F0 contour from a small number of linguistically meaningful parameters, has been modeled quite accurately, and the model has been used extensively in speech synthesis. The present study deals with the inverse problem, i.e., that of extracting the model parameters from a given contour, which can only be solved by successive approximation. This paper presents a method for deriving a first-order approximation to a given F0 contour from the linguistic information of the utterance, and refining the approximation by Analysis-by-Synthesis. The validity of the method has been confirmed experimentally.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-612"
  },
  "maghbouleh96_icslp": {
   "authors": [
    [
     "Arman",
     "Maghbouleh"
    ]
   ],
   "title": "A logistic regression model for detecting prominences",
   "original": "i96_2443",
   "page_count": 3,
   "order": 617,
   "p1": "2443",
   "pn": "2445",
   "abstract": [
    "This paper describes the development of a model for identifying points of prominence in speech. This model can be used as a first step in intonational labeling of corpora that are used in some speech synthesis systems (Black and Taylor, 1995). The working definition of prominence is that starred ToBI accents (Silverman et al., 1992), that is, H*, L*, L*+H, L+H*, and H+!H*, are prominent. The prominence detection model developed here is based on the sums-of-products vowel duration model (van Santen, 1992). The model was trained and tested on different portions of the Boston University Radio News corpus and achieves accuracy results of 86.3% correct identification with 12.5% false detection. The results are comparable to those of previous work (Wightman and Campbell, 1995): 85.9% correct identification with 10.7% false detection. The advantage of this model is that it can be trained quickly on as few as 600 data points, reducing the need for large corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-613"
  },
  "pfister96_icslp": {
   "authors": [
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "High-quality prosodic modification of speech signals",
   "original": "i96_2446",
   "page_count": 4,
   "order": 618,
   "p1": "2446",
   "pn": "2449",
   "abstract": [
    "The aim of this work was to develop a procedure that allows prosodic modifications of speech signals without impairing the quality. The developed procedure is based on the Fourier analysis/synthesis technique with several improvements on the analysis side, such as the analysis of signals with rapidly changing F0 and the analysis of weak spectral components. Also for the modification of the short-time spectrum and for the reconstruction of the speech signal some new methods have been introduced. The most important one, in terms of speech quality, is the way of phase compensation that limits the absolute time shift to half the pitch period. The developed procedure is used in our high-quality text-to-speech synthesis system that is based on concatenation of prosodically modified diphones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-614"
  },
  "zhang96c_icslp": {
   "authors": [
    [
     "Jialu",
     "Zhang"
    ]
   ],
   "title": "On the syllable structures of Chinese relating to speech recognition",
   "original": "i96_2450",
   "page_count": 4,
   "order": 619,
   "p1": "2450",
   "pn": "2453",
   "abstract": [
    "It is well known that Chinese is a tone language with multi-tone system, but the distinctive syllable structures relating to speech recognition have not brought to phoneticians' attention yet. The syllable structures, the phonotactic rules were discussed and the joint probability of the initials and the finals were given in this paper. A comparative study of the relative information transmitted by the place channel between Chinese and English shows that the syllable structrues of Chinese are advantage to increasing the place recognition rate and the syllable intelligibility. It was shown that perceiving a phoneme is based on a syllable in which it exits.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-615"
  },
  "otake96_icslp": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "Can a moraic nasal occur word-initially in Japanese?",
   "original": "i96_2454",
   "page_count": 4,
   "order": 620,
   "p1": "2454",
   "pn": "2457",
   "abstract": [
    "In this study two experiments were conducted to examine whether a moraic nasal could occur word-initially in Japanese, manipulating duration of an onset nasal. In Experiment I, the duration of word-initial nasals, /n/ and /m/, in two CVCV non-words were prolonged up to 300% in steps of one-tenth of the original duration. 10 Japanese college students were presented with the stimuli and asked to write them in Roman letters. The results showed that both nasals, irrespective of place of articulation, were recognized as N+n and N+m respectively when the word-initial nasal exceeded 1.8 times longer than the original duration. In Experiment II, the word initial CV syllable in two non-words, /kaNnaka/ and /KaNmaka/, was spliced off, leaving a long word-initial nasal and the nasal duration (both N+n and N+m) was decreased up to 10% in steps of one-tenth of the original duration. 10 Japanese college students were presented with them to dictate in Roman letters. The results showed that both nasals were recognized as N+n and N+m respectively as long as a certain duration was maintained. The results of the two experiments suggest that a word-initial nasal, just like a word-medial one, can be recognized as moraic as long as it achieves a certain duration in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-616"
  },
  "strange96_icslp": {
   "authors": [
    [
     "Winifred",
     "Strange"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "B. H.",
     "Fitzgerald"
    ],
    [
     "R.",
     "Kubo"
    ]
   ],
   "title": "Perceptual assimilation of american English vowels by Japanese listeners",
   "original": "i96_2458",
   "page_count": 4,
   "order": 621,
   "p1": "2458",
   "pn": "2461",
   "abstract": [
    "To assess cross-language patterns of perceptual assimilation directly, 24 Japanese (J) listeners were presented American English (AE) vowels produced in citation-form /hVbA/ bisyllables and in a carrier sentence by 4 adult male speakers. They selected the J katakana character(s) which contained the vowel most similar to the AE vowel and rated the goodness-of-fit on a 7-point scale. Results showed that, as expected, AE vowels were judged as most similar to J vowels which were adjacent in acoustic-articulatory \"vowel space.\" However, the consistency of assimilation and goodness of fit of AE vowels to J categories varied considerably with speech style (citation vs sentence). Assimilation of long AE vowels to two-mora response categories was much more consistent for target syllables produced in sentences. Acoustical analysis of stimuli suggested that listeners judged the duration of target vowels in citation bisyllables with respect to the following (constant) vowel. Other differences in perceptual assimilation patterns as a function of speech style could not easily be attributed to differences in speakers productions. These results have implications for theories of L2 speech learning and for training studies of non-native speech sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-617"
  },
  "strange96b_icslp": {
   "authors": [
    [
     "Winifred",
     "Strange"
    ],
    [
     "Ocke-Schwen",
     "Bohn"
    ],
    [
     "S. A.",
     "Trent"
    ],
    [
     "M. C.",
     "McNair"
    ],
    [
     "K. C.",
     "Bielec"
    ]
   ],
   "title": "Context and speaker effects in the perceptual assimilation of German vowels by american listeners",
   "original": "i96_2462",
   "page_count": 4,
   "order": 622,
   "p1": "2462",
   "pn": "2465",
   "abstract": [
    "To directly assess the influence of consonantal context and speaker differences on cross-language perceptual similarity of vowels, speakers of American English (AE) were asked to categorize and rate the goodness of fit of North German (NG) vowels to native categories. Four speakers produced the 14 NG monophthongs in 5 CVC contexts in a carrier sentence. Twelve listeners were presented each speakers utterances with vowels and consonantal contexts randomly sequenced. Overall perceptual assimilation patterns showed large variations in the perceived similarity of NG vowels even for those vowels which are considered phonetically similar across languages. The front rounded NG vowels, which do not occur as distinctive phonemes in AE, were almost always assimilated to back rounded AE vowels. Significant context and speaker effects were shown for most of the NG vowels. This suggests that context-free descriptions of cross-language phonetic similarity of vowels will not be adequate in predicting relative perceptual difficulty in learning to differentiate non-native vowels. These results also have implications for theories about the nature of the representation of native-language phonetic categories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-618"
  },
  "zahid96_icslp": {
   "authors": [
    [
     "Mohamed",
     "Zahid"
    ]
   ],
   "title": "Examination of a perceptual non-native speech contrast: pharyngealized/non-pharyngealized discrimination by French-speaking adults",
   "original": "i96_2466",
   "page_count": 4,
   "order": 623,
   "p1": "2466",
   "pn": "2469",
   "abstract": [
    "This study investigates the discrimination of a synthesized non-pharyngealized/pharyngealized place-of-articulation contrast in Arabic [si]-[s»i], presented to Arabic-speaking subjects and French-speaking subjects. An identification task showed group differences in the location of die category boundary, reflecting language-specific experience. An AX discrimination task revealed cross-language differences in the phonemic status of the contrast and that levels of discrimination performance were strongly associated with their assimilation patterns.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-619"
  },
  "smits96_icslp": {
   "authors": [
    [
     "Roel",
     "Smits"
    ]
   ],
   "title": "Context-dependent relevance of burst and transitions for perceived place in stops: it's in production, not perception",
   "original": "i96_2470",
   "page_count": 4,
   "order": 624,
   "p1": "2470",
   "pn": "2473",
   "abstract": [
    "Several studies on place perception of prevocalic stop consonants have shown that the apparent perceptual weight of release burst and formant transitions depends on the vowel context: bursts carry higher perceptual weight in high front vowel contexts like /i/ than in low non-front vowel contexts like /a/, while the reverse holds for formant transitions. This finding is generally interpreted as reflecting a context-dependent \"reweighting\" of burst and transition cues by the perceptual system. In this paper it is shown that the observed effect can be entirely accounted for by contextual variation of the distributions of the relevant cues themselves: Naturally produced stop bursts appear to be acoustically more distinctive in high front vowel contexts than in low non-front vowel contexts, while the reverse is true for formant transitions. The apparently context-dependent perceptual weight of burst and transitions can be reproduced with such cue distributions, even if the classification model itself contains fixed, context-independent linear boundaries. This claim is supported with acoustical and perceptual data of a burst-splicing experiment involving burst-spliced stop-vowel utterances containing the stops /p, t, k/ and vowels /a, i, y, u/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-620"
  },
  "baba96_icslp": {
   "authors": [
    [
     "Ryoji",
     "Baba"
    ],
    [
     "Kaori",
     "Omuro"
    ],
    [
     "Hiromitsu",
     "Miyazono"
    ],
    [
     "Tsuyoshi",
     "Usagawa"
    ],
    [
     "Masahiko",
     "Higuchi"
    ]
   ],
   "title": "The perception of morae in long vowels comparison among Japanese, Korean and English speakers",
   "original": "i96_2474",
   "page_count": 4,
   "order": 625,
   "p1": "2474",
   "pn": "2477",
   "abstract": [
    "There are three kinds of Tokushuhaku (the specific timing morae) in the Japanese language such as the moraic nasal, the non-nasal consonant and the long vowel. Even though Japanese native speakers can perceive them perfectly, it is difficult for Japanese learners from abroad to perceive and produce them. To make it more efficient for Japanese learners to acquire them, we need to investigate how Japanese native speakers perceive Tokushuhaku. This study examines the perception of a mora or morae of Japanese language specifically in the long vowel. Previous researchers (1)Hiroya Fujisaki, Miyoko Sugito) have assumed that duration contrasts alone are involved in perception. However we hypothesize that the accentual change in the midst of a long vowel would mark the boundary between the morae and would affect the perception of morae by native speakers of standard Japanese. Based on the hypothesis three kinds of perception tests with computer-edited sound stimuli were given to native Japanese, Korean and English speakers. The results of these tests suggest that native Japanese speakers count the number of morae not only by the duration of vowels, but also by the accentual change which indicates the boundary between morae. On the other hand, nonnative speakers count the number of morae only by the duration of vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-621"
  },
  "lickley96b_icslp": {
   "authors": [
    [
     "Robin J.",
     "Lickley"
    ]
   ],
   "title": "Juncture cues to disfluency",
   "original": "i96_2478",
   "page_count": 4,
   "order": 626,
   "p1": "2478",
   "pn": "2481",
   "abstract": [
    "This paper describes properties of normal disfluent speech which help listeners to distinguish disfluent from fluent strings of speech. It focusses on juncture phenomena in cases where there is no clear silent pause at the interruption point. Recent attempts to define acoustically identifiable features of speech which can be seen as reliable indicators of disfluency have produced several suggestions. But studies of silent pause, (pre-)pausal lengthening, glottalisation and measurements of F0 have all failed to provide any reliable means of distinguishing fluent from disfluent continuations. This paper introduces into the discussion a phonological feature of speech which has been overlooked in previous work and which could prove to be a reliable indicator of disfluency, especially in mid-clause disfluencies where no pause is present at the interruption. In normal fluent continuous speech, words are not usually separated by silent pause into discrete units, but have their boundaries obscured or linked by processes like assimilation, liaison, elision and so on. The hypothesis examined by the present study is that such juncture phenomena are blocked by disfluency. Evidence from perceptual experiments suggests that this phenomenon may be used by human listeners in early detection of disfluency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-622"
  },
  "sawusch96_icslp": {
   "authors": [
    [
     "James R.",
     "Sawusch"
    ]
   ],
   "title": "Effects of duration and formant movement on vowel perception",
   "original": "i96_2482",
   "page_count": 4,
   "order": 627,
   "p1": "2482",
   "pn": "2485",
   "abstract": [
    "Acoustical analysis of speech and perceptual studies indicate that the dominant acoustic correlates of vowel perception are the frequencies of the first three formants. However, most vowels are not completely steady-state (even in isolation) and formant frequencies change with variation in the surrounding consonantal context, prosodic influences, speaking rate, and vocal tract length of the talker. In the present studies, both natural and synthetic syllables (\"head\" and \"had\") were used to explore the relative potency of average formant frequencies, vocalic duration, and formant frequency movement in vowel perception. A male talker was identified whose formant frequencies, at the midpoint of the words \"had\" and \"head\", were identical. However, these tokens differed in their voiced duration and movement of the first three formants and were also highly intelligible. Since the formant frequencies at midpoint could not distinguish these two words, listeners were clearly using different/additional information to guide perception. In the first study, vowel duration was varied. Digital waveform editing was used to generate two series, one based on \"had\" and the other based on \"head\". Overall, duration had little effect on listeners' classification of the stimuli. The second study employed synthetic series in which the formant movements of the first three formants were varied between those of the natural \"had\" and those of the natural \"head\". Here duration played a much larger role in listeners responses. Together, these data are a step toward uncovering the relative roles of formant frequencies, formant movement, and duration in vowel perception within fluent syllables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-623"
  },
  "deshmukh96_icslp": {
   "authors": [
    [
     "N.",
     "Deshmukh"
    ],
    [
     "R. J.",
     "Duncan"
    ],
    [
     "A.",
     "Ganapathiraju"
    ],
    [
     "J.",
     "Picone"
    ]
   ],
   "title": "Benchmarking human performance for continuous speech recognition",
   "original": "i96_2486",
   "page_count": 4,
   "order": 628,
   "p1": "2486",
   "pn": "2489",
   "abstract": [
    "It is a well-established fact that human performance exceeds that of computers by orders of magnitude on a wide range of speech recognition tasks. However, there is widespread belief that the gap between human and machine performance has narrowed considerably on restricted problems. Yet, there are few extensive comparisons of performance on tasks involving large vocabulary continuous speech recognition (LVCSR) and low signal-to-noise ratios (SNRs). Human evaluations on LVCSR tasks highlight a number of interesting issues. For example, familiarity with the domain plays a crucial role in human performance. We conducted several experiments that extensively characterize human performance on LVCSR tasks over two standard evaluation corpora - ARPAs CSR94 Spoke 10 and CSR95 Hub 3. We demonstrate that human performance is at least an order of magnitude better than the best machine performance, and that human performance is fairly robust to a number of factors that typically degrade machine performance: SNR, speaking rate and style, microphone and ambient noise. In fact, human performance remained remarkably consistent across evaluation paradigms, and to some extent was artificially limited by a listeners attention span.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-624"
  },
  "arai96b_icslp": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Misha",
     "Pavel"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Carlos",
     "Avendano"
    ]
   ],
   "title": "Intelligibility of speech with filtered time trajectories of spectral envelopes",
   "original": "i96_2490",
   "page_count": 4,
   "order": 629,
   "p1": "2490",
   "pn": "2493",
   "abstract": [
    "The effect of filtering the time trajectories of spectral envelopes on speech intelligibility was investigated. Since LPC cepstrum forms the basis of many automatic speech recognition systems,we filtered time trajectories of LPC cepstrum of speech sounds, and the modified speech was reconstructed after the filtering. For processing, we applied low-pass, high-pass and band-pass filters. The results of the accuracy from the perceptual experiments for Japanese syllables show that speech intelligibility is not severely impaired as long as the filtered spectral components have 1) a rate of change faster than 1 Hz when high-pass filtered, 2) a rate of change slower than 24 Hz when low-pass filtered, and 3) a rate of change between 1 and 16 Hz when band-pass filtered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-625"
  },
  "whalen96_icslp": {
   "authors": [
    [
     "Douglas H.",
     "Whalen"
    ],
    [
     "Sonya M.",
     "Sheffert"
    ]
   ],
   "title": "Perceptual use of vowel and speaker information in breath sounds",
   "original": "i96_2494",
   "page_count": 4,
   "order": 630,
   "p1": "2494",
   "pn": "2497",
   "abstract": [
    "Inspiration sounds, being shaped by the vocal tract, are a potential source of speech information. Previous work found that both speaker sex and coarticulatory information in the breath sound could influence vowel perception [1], Speaker sex affected the perception of one /bVd/ continuum while the coarticulatory information affected a second continuum. The present study extends those results by using an /hVd/ context for the inspiration sound and for the synthetic continuum. Breaths were produced before \"heed\", \"hid\", \"head\", \"had\" and \"Hud\". Two continua, from /as/ to /©/ and from /e/ to III were presented to listeners for forced-choice identification. Preliminary results suggested that both vowel information and speaker sex affected the perception. Final results showed, instead, that the information did not affect perceptual boundaries between vowels. Several sources for the lack of an effect could exist, including variability in the amount of information in particular breath sounds and differences in the amount of ambiguity in the continua. Further work is called for.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-626"
  },
  "mousty96_icslp": {
   "authors": [
    [
     "Philippe",
     "Mousty"
    ],
    [
     "Monique",
     "Radeau"
    ],
    [
     "Ronald",
     "Peereman"
    ],
    [
     "Paul",
     "Bertelson"
    ]
   ],
   "title": "The role of neighborhood relative frequency in spoken word recognition",
   "original": "i96_2498",
   "page_count": 4,
   "order": 631,
   "p1": "2498",
   "pn": "2501",
   "abstract": [
    "According to current models of word recognition, the time to recognize a word would be affected by the frequency of its neighbors. The present study examined the role of neighborhood frequency in spoken word recognition, using a definition of the neighborhood in terms of the last candidates of the cohort aligned from onset with the input [8]. Two sets of disyllabic CVCV words were selected which differed by the relative frequency of occurrence of their more frequent neighbor (one set of words having no more frequent neighbor, and the other at least one). They were compared in three different tasks: lexical decision, repetition and gating. No effect of relative frequency was obtained either with the lexical decision task, or the repetition task. The gating task also failed to show reliable evidence for the influence of neighborhood frequency on both recognition points and isolation points (for which subjects' confidence is not taken into account). The results are discussed in terms of the nature of the competitor space of a spoken word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-627"
  },
  "mcqueen96_icslp": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Mark A.",
     "Pitt"
    ]
   ],
   "title": "Transitional probability and phoneme monitoring",
   "original": "i96_2502",
   "page_count": 4,
   "order": 632,
   "p1": "2502",
   "pn": "2505",
   "abstract": [
    "Two phoneme monitoring experiments examined the influence of Transitional Probability (TP) on phoneme recognition. Target phonemes appeared at the end of Consonant-Vowel-Consonant (CVC) syllables, or as the first element of coda clusters in CVCC syllables. Reliable TP effects were found only for targets in CVCC syllables. The TPs both into and out of the targets influenced listeners' ability to detect them in CVCCs. Furthermore, targets were more difficult to detect in CVCCs than in CVCs. TP may only influence segment recognition when that segment is more difficult to recognise, as when it occurs in a cluster.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-628"
  },
  "bonneau96_icslp": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Identification of vowel features from French stop bursts",
   "original": "i96_2506",
   "page_count": 4,
   "order": 633,
   "p1": "2506",
   "pn": "2509",
   "abstract": [
    "This paper deals with the perception of vowels from French stop bursts. The corpus was made up of 90 stimuli of 20-25 ms duration extracted from natural CVC and CV words. The syllables combined the initial stops /p,t,k/ with the vowels /i,a,u/. In order to cut off all traces of vocalic segment, bursts whose duration was too short were lengthened. Eight native speakers of French served as listeners in the experiment. Results showed that a burst onset which did not contain any traces of vocalic segment provided substantial vocalic information (the overall identification rate was 80%). The vowel /i/ was clearly identified from /t/ and /k/, and the vowel /u/ very clearly identified from /k/. The vowel /a/, with high identification rate, was often chosen in the absence of a clear vocalic timbre.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-629"
  },
  "bond96_icslp": {
   "authors": [
    [
     "Z. S.",
     "Bond"
    ],
    [
     "Thomas J.",
     "Moore"
    ],
    [
     "Beverley",
     "Gable"
    ]
   ],
   "title": "Listening in a second language",
   "original": "i96_2510",
   "page_count": 4,
   "order": 634,
   "p1": "2510",
   "pn": "2513",
   "abstract": [
    "Native and non-native listeners identified English words and sentences in six different listening conditions. When they heard speech mixed with noise or when they had to use linguistic knowledge to respond, non-native listeners suffered greater performance decrements than native listeners. Their performance appears to be data-driven ('bottom-up') requiring full specification of the acoustic-phonetic information relevant for selecting a particular word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-630"
  },
  "burnham96_icslp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Elizabeth",
     "Francis"
    ],
    [
     "Di",
     "Webster"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ],
    [
     "Chayada",
     "Attapaiboon"
    ],
    [
     "Francisco",
     "Lacerda"
    ],
    [
     "Peter",
     "Keller"
    ]
   ],
   "title": "Perception of lexical tone across languages: evidence for a linguistic mode of processing",
   "original": "i96_2514",
   "page_count": 4,
   "order": 635,
   "p1": "2514",
   "pn": "2517",
   "abstract": [
    "Pairs of Thai tones were presented for perceptual discrimination in three linguistic contexts (normal speech, low-pass filtered speech, and as musical (violin) sounds) to tonal language speakers, Thai and Cantonese, and non-tonal (English) language speakers. English speakers discriminated the tonal contrasts significantly better in the musical context than in filtered speech, and in filtered speech better than in full speech. On the other hand, both Thai and Cantonese speakers perceived the tonal contrasts equally well in all three contexts. Thus developmental absence of exposure to lexical tone results in a linguistic mode of processing which involves the attenuation of a basic psychoacoustic ability, pitch discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-631"
  },
  "magnuson96_icslp": {
   "authors": [
    [
     "James S.",
     "Magnuson"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ]
   ],
   "title": "Acoustic correlates to the effects of talker variability on the perception of English /r/ and /l/ by Japanese listeners",
   "original": "i96_2518",
   "page_count": 4,
   "order": 636,
   "p1": "2518",
   "pn": "2521",
   "abstract": [
    "It is often reported that for non-native listeners of a language, some native speakers' productions of non-native contrasts are easier to understand than others' (e.g., [1]). However, these effects are not well-understood, as acoustic correlates to the effects have proven difficult to establish. We report analyses of subject differences and acoustic measurements which may help to describe the acoustic phenomena underlying one class of talker effects that we have reported previously; specifically, the interaction of talker and talker condition (the number of talkers heard within a block of trials -- one or several) [2]. Correlations between response measures and acoustic measures suggest that when stimuli from several talkers are mixed randomly in a block of trials, subjects without well-formed categories for /r/ and /l/ attempt to use the duration of the initial steady-state portion of an /r/ or /l/ stimulus (an unreliable cue) for categorization, whereas native speakers use F3 [6]. It also appears that they use this cue to establish criteria for \"R\"-\"L\" decisions, which they apply to the overall range of durations across all talkers in one block of trials.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1996-632"
  }
 },
 "sessions": [
  {
   "title": "Plenary Lectures",
   "papers": [
    "cutler96_icslp",
    "flanagan96_icslp"
   ]
  },
  {
   "title": "Large Vocabulary",
   "papers": [
    "li96_icslp",
    "lamel96_icslp",
    "fetter96_icslp",
    "aubert96_icslp",
    "valtchev96_icslp",
    "matsuoka96_icslp",
    "carter96_icslp",
    "maciasguarasa96_icslp"
   ]
  },
  {
   "title": "Multimodal ASR (Face and Lips)",
   "papers": [
    "alissali96_icslp",
    "matthews96_icslp",
    "su96_icslp",
    "schumeyer96_icslp",
    "chandramohan96_icslp",
    "cosi96_icslp",
    "luettin96_icslp",
    "luettin96b_icslp"
   ]
  },
  {
   "title": "Perception of Words",
   "papers": [
    "gowjr96_icslp",
    "kuijk96_icslp",
    "meftah96_icslp",
    "haveman96_icslp",
    "vitevitch96_icslp",
    "auerjr96_icslp",
    "riele96_icslp",
    "donselaar96_icslp"
   ]
  },
  {
   "title": "Phonetics, Transcription, and Analysis",
   "papers": [
    "juola96_icslp",
    "ran96_icslp",
    "kipp96_icslp",
    "seneff96_icslp",
    "yang96_icslp",
    "lee96_icslp",
    "boudelaa96_icslp",
    "wesenick96_icslp",
    "wesenick96b_icslp",
    "petek96_icslp",
    "robbe96_icslp",
    "erickson96_icslp",
    "herman96_icslp",
    "kuijpers96_icslp"
   ]
  },
  {
   "title": "Spoken Language Processing for Special Populations",
   "papers": [
    "mahshie96_icslp",
    "oster96_icslp",
    "hazan96_icslp",
    "hazan96b_icslp",
    "paarmann96_icslp",
    "baraccikoja96_icslp",
    "russell96_icslp",
    "campbell96_icslp",
    "portele96_icslp"
   ]
  },
  {
   "title": "Dialogue Special Sessions",
   "papers": [
    "shirai96_icslp",
    "seneff96b_icslp",
    "kita96_icslp",
    "heisterkamp96_icslp",
    "oviatt96_icslp",
    "sarukkai96_icslp",
    "hubener96_icslp",
    "taylor96_icslp",
    "heeman96_icslp",
    "sagawa96_icslp",
    "bruce96_icslp",
    "pan96_icslp",
    "hirose96_icslp",
    "tanaka96_icslp",
    "niimi96_icslp",
    "rudnicky96_icslp",
    "meng96_icslp",
    "sadek96_icslp",
    "bennacef96_icslp",
    "lavie96_icslp"
   ]
  },
  {
   "title": "Language Modeling",
   "papers": [
    "niesler96_icslp",
    "valverdealbacete96_icslp",
    "gallwitz96_icslp",
    "seymore96_icslp",
    "iyer96_icslp",
    "federico96_icslp",
    "siu96_icslp",
    "miller96_icslp",
    "bonafonte96_icslp",
    "ries96_icslp",
    "geutner96_icslp",
    "hu96_icslp"
   ]
  },
  {
   "title": "Feature Extraction for Speech Recognition",
   "papers": [
    "sun96_icslp",
    "hubener96b_icslp",
    "eisele96_icslp",
    "milner96_icslp",
    "wassner96_icslp",
    "long96_icslp",
    "drygajlo96_icslp",
    "umesh96_icslp",
    "kobayashi96_icslp",
    "kajita96_icslp",
    "bourlard96_icslp",
    "nadeu96_icslp"
   ]
  },
  {
   "title": "Speech Production - Measurement and Modeling",
   "papers": [
    "laprie96_icslp",
    "demolin96_icslp",
    "gleason96_icslp",
    "lobo96_icslp",
    "morlec96_icslp",
    "kawahara96_icslp"
   ]
  },
  {
   "title": "Speech Coding / HMMs and NNs in ASR",
   "papers": [
    "burnett96_icslp",
    "pan96b_icslp",
    "cawley96_icslp",
    "murgia96_icslp",
    "ribeiro96_icslp",
    "yonezaki96_icslp",
    "kohata96_icslp",
    "koishida96_icslp",
    "chan96_icslp",
    "koizumi96_icslp",
    "mokhtar96_icslp",
    "gong96_icslp",
    "moudenc96_icslp",
    "illina96_icslp",
    "hild96_icslp",
    "boulianne96_icslp",
    "choi96_icslp",
    "kurimo96_icslp"
   ]
  },
  {
   "title": "Vowels",
   "papers": [
    "lang96_icslp",
    "syrdal96_icslp",
    "willerman96_icslp",
    "ingram96_icslp",
    "kewleyport96_icslp",
    "yoneyama96_icslp"
   ]
  },
  {
   "title": "NNs and Stochastic Modeling",
   "papers": [
    "lee96b_icslp",
    "hermansky96_icslp",
    "gish96_icslp",
    "knill96_icslp",
    "hogberg96_icslp",
    "lleidasolano96_icslp",
    "ma96_icslp",
    "itoh96_icslp",
    "ito96_icslp",
    "kneser96_icslp",
    "johansen96_icslp",
    "wei96_icslp",
    "yu96_icslp",
    "bosch96_icslp",
    "ramsay96_icslp",
    "yang96b_icslp",
    "freitag96_icslp",
    "lee96c_icslp",
    "wakita96_icslp"
   ]
  },
  {
   "title": "Neural Models of Speech Processing",
   "papers": [
    "aleksandrovsky96_icslp",
    "tang96_icslp",
    "vereecken96_icslp",
    "hant96_icslp",
    "hunke96_icslp",
    "aikawa96_icslp"
   ]
  },
  {
   "title": "Utterance Verification and Word Spotting",
   "papers": [
    "rose96_icslp",
    "gelin96_icslp",
    "meliani96_icslp",
    "spina96_icslp",
    "kubala96_icslp",
    "setlur96_icslp"
   ]
  },
  {
   "title": "Acquisition/Learning Training L2 Learners",
   "papers": [
    "akahaneyamada96_icslp",
    "ueyama96_icslp",
    "yamada96_icslp",
    "varden96_icslp",
    "archambault96_icslp",
    "martland96_icslp"
   ]
  },
  {
   "title": "Focus, Stress and Accent",
   "papers": [
    "sluijter96_icslp",
    "fujisaki96_icslp",
    "thibault96_icslp",
    "elsner96_icslp",
    "nishinuma96_icslp",
    "shriberg96_icslp"
   ]
  },
  {
   "title": "Spoken Language Dialogue and Conversation",
   "papers": [
    "reithinger96_icslp",
    "muller96_icslp",
    "norton96_icslp",
    "seneff96c_icslp",
    "kumamoto96_icslp",
    "hong96_icslp",
    "carlson96_icslp",
    "kameyama96_icslp",
    "hildebrandt96_icslp",
    "kakita96_icslp",
    "brandtpook96_icslp",
    "takagi96_icslp",
    "goddeau96_icslp",
    "whittaker96_icslp",
    "sutton96_icslp",
    "yang96c_icslp",
    "kenne96_icslp",
    "kenne96b_icslp",
    "venditti96_icslp",
    "bernsen96_icslp",
    "jenkin96_icslp"
   ]
  },
  {
   "title": "Speech Disorders",
   "papers": [
    "jamieson96_icslp",
    "vieira96_icslp",
    "vieira96b_icslp",
    "plante96_icslp",
    "schoentgen96_icslp",
    "gavidiaceballos96_icslp",
    "kuehn96_icslp",
    "espywilson96_icslp",
    "deng96_icslp",
    "endo96_icslp",
    "wallen96_icslp",
    "cairns96_icslp"
   ]
  },
  {
   "title": "Vocal Tract Geometry",
   "papers": [
    "honda96_icslp",
    "davis96_icslp",
    "bangayan96_icslp",
    "narayanan96_icslp",
    "yang96d_icslp",
    "elmasri96_icslp",
    "bailly96_icslp",
    "bavegard96_icslp",
    "dang96_icslp",
    "blackburn96_icslp"
   ]
  },
  {
   "title": "Prosody in ASR and Segmentation",
   "papers": [
    "oviatt96b_icslp",
    "potisuk96_icslp",
    "hsieh96_icslp",
    "rao96_icslp",
    "sakurai96_icslp",
    "pagel96_icslp"
   ]
  },
  {
   "title": "Acquisition and Learning by Machine",
   "papers": [
    "bub96_icslp",
    "cole96_icslp",
    "kobayashi96b_icslp",
    "yoshimura96_icslp",
    "amengual96_icslp",
    "cettolo96_icslp"
   ]
  },
  {
   "title": "Dialogue Systems",
   "papers": [
    "gauvain96_icslp",
    "strik96_icslp",
    "marcus96_icslp",
    "kawahara96b_icslp",
    "suhm96_icslp",
    "issar96_icslp"
   ]
  },
  {
   "title": "Speech Enhancement and Robust Processing",
   "papers": [
    "shen96_icslp",
    "vaseghi96_icslp",
    "shen96b_icslp",
    "power96_icslp",
    "avendano96_icslp",
    "brants96_icslp",
    "ringger96_icslp",
    "yasukawa96_icslp",
    "matsumoto96_icslp",
    "woods96_icslp",
    "petek96b_icslp",
    "miglietta96_icslp",
    "darlington96_icslp",
    "openshaw96_icslp",
    "ortegagarcia96_icslp",
    "harte96_icslp",
    "koizumi96b_icslp",
    "salavedra96_icslp",
    "zhou96_icslp"
   ]
  },
  {
   "title": "Speaker Adaptation and Normalization I",
   "papers": [
    "matsui96_icslp",
    "montacie96_icslp",
    "yao96_icslp",
    "huo96_icslp",
    "strom96_icslp",
    "ariki96_icslp",
    "zhang96_icslp",
    "woodland96_icslp",
    "anastasakos96_icslp",
    "homma96_icslp",
    "burnett96b_icslp",
    "ishii96_icslp"
   ]
  },
  {
   "title": "Spoken Language and NLP",
   "papers": [
    "schwartz96_icslp",
    "gorin96_icslp",
    "stolcke96_icslp",
    "boros96_icslp",
    "minker96_icslp",
    "seide96_icslp",
    "buo96_icslp",
    "antoine96_icslp",
    "alvarezcercadillo96_icslp",
    "berton96_icslp",
    "batliner96_icslp",
    "srinivas96_icslp"
   ]
  },
  {
   "title": "Spoken Discourse Analysis/Synthesis",
   "papers": [
    "chino96_icslp",
    "duff96_icslp",
    "donzel96_icslp",
    "swerts96_icslp",
    "seong96_icslp",
    "okane96_icslp"
   ]
  },
  {
   "title": "Acoustic Modeling",
   "papers": [
    "westendorf96_icslp",
    "rathinavelu96_icslp",
    "lazarides96_icslp",
    "li96b_icslp",
    "ming96_icslp",
    "pachesleal96_icslp",
    "okawa96_icslp",
    "wang96_icslp",
    "fukada96_icslp",
    "vergin96_icslp",
    "yang96e_icslp",
    "simonin96_icslp",
    "nitta96_icslp",
    "bonafonte96b_icslp",
    "cordoba96_icslp",
    "zeljkovic96_icslp",
    "minami96_icslp",
    "ramsay96b_icslp",
    "hu96b_icslp",
    "junqua96_icslp",
    "deng96b_icslp",
    "oppizzi96_icslp",
    "kirchhoff96_icslp",
    "glass96_icslp",
    "wu96_icslp"
   ]
  },
  {
   "title": "Physics and Simulation of the Vocal Tract",
   "papers": [
    "coker96_icslp",
    "badin96_icslp",
    "suzuki96_icslp",
    "liljencrants96_icslp"
   ]
  },
  {
   "title": "Duration and Rhythm",
   "papers": [
    "ouellet96_icslp",
    "wang96b_icslp",
    "dahan96_icslp",
    "mcrobbieutasi96_icslp",
    "lee96d_icslp",
    "hermes96_icslp"
   ]
  },
  {
   "title": "Acoustic Analysis",
   "papers": [
    "ying96_icslp",
    "soquet96_icslp",
    "janer96_icslp",
    "stylianou96_icslp",
    "jo96_icslp",
    "wang96c_icslp",
    "eberman96_icslp",
    "zolfaghari96_icslp",
    "richards96_icslp",
    "sharma96_icslp",
    "ohmura96_icslp",
    "namba96_icslp",
    "esposito96_icslp",
    "ananthakrishnan96_icslp",
    "ding96_icslp",
    "pfitzinger96_icslp",
    "wang96d_icslp",
    "bonafonte96c_icslp",
    "mousset96_icslp",
    "abe96_icslp",
    "moreno96_icslp"
   ]
  },
  {
   "title": "Speech Recognition Using HMMs and NNs",
   "papers": [
    "neto96_icslp",
    "suh96_icslp",
    "bilmes96_icslp",
    "cook96_icslp",
    "pican96_icslp",
    "waterhouse96_icslp"
   ]
  },
  {
   "title": "Adverse Environments and Multiple Microphones",
   "papers": [
    "yamada96b_icslp",
    "jan96_icslp",
    "kershaw96_icslp",
    "giuliani96_icslp",
    "gonzalezrodriguez96_icslp",
    "yen96_icslp"
   ]
  },
  {
   "title": "Prosodic Synthesis in Dialogue",
   "papers": [
    "lindstrom96_icslp",
    "asano96_icslp",
    "prevost96_icslp",
    "tsukada96_icslp",
    "galanis96_icslp",
    "heuft96_icslp"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "sproat96_icslp",
    "ooyama96_icslp",
    "magata96_icslp",
    "lopezgonzalo96_icslp",
    "lee96e_icslp",
    "black96_icslp",
    "wang96e_icslp",
    "dutoit96_icslp",
    "hashimoto96_icslp",
    "lee96f_icslp",
    "baudoin96_icslp",
    "delogu96_icslp",
    "zhu96_icslp",
    "campos96_icslp",
    "hwang96_icslp",
    "edgington96_icslp",
    "heggtveit96_icslp",
    "pollard96_icslp",
    "arai96_icslp",
    "wang96f_icslp",
    "rinscheid96_icslp"
   ]
  },
  {
   "title": "Instructional Technology for Spoken Language",
   "papers": [
    "yoram96_icslp",
    "jamieson96b_icslp",
    "neumeyer96_icslp",
    "simoes96_icslp",
    "eskenazi96_icslp",
    "mixdorff96_icslp",
    "markham96_icslp"
   ]
  },
  {
   "title": "Multimodal Spoken Language Processing",
   "papers": [
    "bernstein96_icslp",
    "sekiyama96_icslp",
    "vatikiotisbateson96_icslp",
    "reed96_icslp",
    "campbell96b_icslp",
    "green96_icslp",
    "brooke96_icslp",
    "remez96_icslp",
    "pisoni96_icslp"
   ]
  },
  {
   "title": "Prosody - Phonological/Phonetic Measures",
   "papers": [
    "strom96b_icslp",
    "swerts96b_icslp",
    "kim96_icslp",
    "yamashita96_icslp",
    "makarova96_icslp",
    "eklund96_icslp"
   ]
  },
  {
   "title": "Phonetics and Perception",
   "papers": [
    "shadle96_icslp",
    "warner96_icslp",
    "son96_icslp",
    "archambault96b_icslp",
    "francis96_icslp",
    "appelbaum96_icslp"
   ]
  },
  {
   "title": "Language Acquisition",
   "papers": [
    "andruski96_icslp",
    "clement96_icslp",
    "halle96_icslp",
    "jusczyk96_icslp",
    "hayashi96_icslp"
   ]
  },
  {
   "title": "Production and Prosody Posters",
   "papers": [
    "alku96_icslp",
    "marasek96_icslp",
    "boyce96_icslp",
    "masaki96_icslp",
    "kuijk96b_icslp",
    "wrench96_icslp",
    "carre96_icslp",
    "mair96_icslp",
    "hashi96_icslp",
    "znagui96_icslp",
    "arnfield96_icslp",
    "ying96b_icslp",
    "wang96g_icslp",
    "minematsu96_icslp",
    "chou96_icslp",
    "hamagami96_icslp",
    "hoskins96_icslp",
    "rapp96_icslp",
    "straub96_icslp",
    "ni96_icslp",
    "fellbaum96_icslp"
   ]
  },
  {
   "title": "User-Machine Interfaces",
   "papers": [
    "mellor96_icslp",
    "life96_icslp",
    "karaorman96_icslp",
    "basson96_icslp",
    "bayer96_icslp",
    "isobe96_icslp"
   ]
  },
  {
   "title": "TTS Systems and Rules",
   "papers": [
    "lee96g_icslp",
    "karn96_icslp",
    "andersen96_icslp",
    "adamson96_icslp",
    "albano96_icslp",
    "yoshida96_icslp"
   ]
  },
  {
   "title": "Prosody and Labeling",
   "papers": [
    "grice96_icslp",
    "batliner96b_icslp",
    "koopmansvanbeinum96_icslp",
    "ward96_icslp",
    "mast96_icslp",
    "kuijk96c_icslp"
   ]
  },
  {
   "title": "Speaker/Language Identification and Verification",
   "papers": [
    "kumpf96_icslp",
    "korkmazskiy96_icslp",
    "stockmal96_icslp",
    "yu96b_icslp",
    "genoud96_icslp",
    "martindelalamo96_icslp",
    "markov96_icslp",
    "thymegobbel96_icslp",
    "kitamura96_icslp",
    "kwan96_icslp",
    "hieronymus96_icslp",
    "teixeira96_icslp",
    "vuuren96_icslp",
    "yang96f_icslp",
    "berkling96_icslp",
    "carey96_icslp",
    "monte96_icslp",
    "dalsgaard96_icslp"
   ]
  },
  {
   "title": "Emotion in Recognition and Synthesis",
   "papers": [
    "scherer96_icslp",
    "ohala96_icslp",
    "murray96_icslp",
    "dellaert96_icslp",
    "heuft96b_icslp",
    "arnfield96b_icslp",
    "banbrook96_icslp",
    "johnstone96_icslp",
    "cowie96_icslp"
   ]
  },
  {
   "title": "Stochastic Techniques in Robust Speech Recognition",
   "papers": [
    "lee96h_icslp",
    "rahim96_icslp",
    "gupta96_icslp",
    "gales96_icslp",
    "surendran96_icslp",
    "chien96_icslp"
   ]
  },
  {
   "title": "Prosodic Synthesis in Text to Speech",
   "papers": [
    "johnson96_icslp",
    "horne96_icslp",
    "katae96_icslp",
    "tatham96_icslp",
    "boughazale96_icslp",
    "dobnikar96_icslp"
   ]
  },
  {
   "title": "Dialogue Events",
   "papers": [
    "shriberg96b_icslp",
    "yang96g_icslp",
    "lickley96_icslp",
    "garner96_icslp",
    "traum96_icslp",
    "novick96_icslp"
   ]
  },
  {
   "title": "Databases and Tools",
   "papers": [
    "roach96_icslp",
    "wang96h_icslp",
    "hurley96_icslp",
    "falcone96_icslp",
    "draxler96_icslp",
    "halstead96_icslp",
    "altosaar96_icslp",
    "langmann96_icslp",
    "muller96b_icslp",
    "parlangeau96_icslp",
    "hahn96_icslp",
    "boldea96_icslp",
    "kohler96_icslp",
    "hetherington96_icslp",
    "kiyama96_icslp",
    "bai96_icslp",
    "melin96_icslp",
    "bard96_icslp",
    "menendezpidal96_icslp",
    "hennebert96_icslp"
   ]
  },
  {
   "title": "Robust Speech Processing",
   "papers": [
    "zhang96b_icslp",
    "claes96_icslp",
    "chou96b_icslp",
    "mak96_icslp",
    "wakao96_icslp",
    "chi96_icslp"
   ]
  },
  {
   "title": "Dialects and Speaking Styles",
   "papers": [
    "huggins96_icslp",
    "kudo96_icslp",
    "miller96b_icslp",
    "kvale96_icslp",
    "torre96_icslp"
   ]
  },
  {
   "title": "Production and Perception of Prosody",
   "papers": [
    "cummins96_icslp",
    "vogel96_icslp",
    "fant96_icslp",
    "house96_icslp",
    "vainio96_icslp",
    "minematsu96b_icslp"
   ]
  },
  {
   "title": "Topics in ASR and Search",
   "papers": [
    "ueberla96_icslp",
    "yonezawa96_icslp",
    "junkawitsch96_icslp",
    "lacouture96_icslp",
    "brugnara96_icslp",
    "takagi96b_icslp",
    "vanhamme96_icslp",
    "avendano96b_icslp",
    "ortmanns96_icslp",
    "ortmanns96b_icslp",
    "husson96_icslp",
    "yamada96c_icslp",
    "nouza96_icslp",
    "caminerogil96_icslp",
    "placeway96_icslp",
    "iwahashi96_icslp",
    "minamino96_icslp",
    "tran96_icslp",
    "goblirsch96_icslp",
    "burhke96_icslp",
    "thelen96_icslp",
    "sagerer96_icslp",
    "illina96b_icslp",
    "bonafonte96d_icslp",
    "laface96_icslp"
   ]
  },
  {
   "title": "Multimodal Dialogue/HCI",
   "papers": [
    "breen96_icslp",
    "goff96_icslp",
    "iwano96_icslp",
    "hayamizu96_icslp",
    "cave96_icslp",
    "fais96_icslp",
    "lagana96_icslp",
    "saldana96_icslp"
   ]
  },
  {
   "title": "Multilingual Speech Processing",
   "papers": [
    "barnett96_icslp",
    "kohler96b_icslp",
    "nakamura96_icslp",
    "lamel96b_icslp",
    "zue96_icslp",
    "ackermann96_icslp",
    "alshawi96_icslp",
    "wang96i_icslp",
    "yang96h_icslp",
    "bub96b_icslp",
    "lavie96b_icslp"
   ]
  },
  {
   "title": "Acoustics in Synthesis",
   "papers": [
    "edmondson96_icslp",
    "williams96_icslp",
    "shih96_icslp",
    "lo96_icslp",
    "martland96b_icslp",
    "abe96b_icslp"
   ]
  },
  {
   "title": "Pitch and Rate",
   "papers": [
    "geoffrois96_icslp",
    "barner96_icslp",
    "maidment96_icslp",
    "beet96_icslp",
    "ohno96_icslp",
    "verhasselt96_icslp"
   ]
  },
  {
   "title": "General ASR Posters",
   "papers": [
    "zhan96_icslp",
    "demuynck96_icslp",
    "constantinescu96_icslp",
    "yoma96_icslp",
    "ozeki96_icslp",
    "candille96_icslp",
    "hattori96_icslp",
    "jacob96_icslp",
    "blomberg96_icslp",
    "xu96_icslp",
    "humphries96_icslp",
    "sloboda96_icslp",
    "veth96_icslp",
    "leandro96_icslp",
    "raj96_icslp",
    "lilly96_icslp",
    "janer96b_icslp",
    "usagawa96_icslp",
    "okuno96_icslp"
   ]
  },
  {
   "title": "Data-based Synthesis",
   "papers": [
    "slater96_icslp",
    "benzmuller96_icslp",
    "huang96_icslp",
    "portele96b_icslp",
    "mobius96_icslp",
    "campbell96c_icslp"
   ]
  },
  {
   "title": "Speaker Identification and Verification",
   "papers": [
    "parthasarathy96_icslp",
    "murakami96_icslp",
    "floch96_icslp",
    "lin96_icslp",
    "newman96_icslp",
    "paoloni96_icslp"
   ]
  },
  {
   "title": "Acoustic Phonetics",
   "papers": [
    "shrotriya96_icslp",
    "tan96_icslp",
    "kuwabara96_icslp",
    "fujisaki96b_icslp",
    "maghbouleh96_icslp",
    "pfister96_icslp"
   ]
  },
  {
   "title": "Perception of Vowels and Consonants",
   "papers": [
    "zhang96c_icslp",
    "otake96_icslp",
    "strange96_icslp",
    "strange96b_icslp",
    "zahid96_icslp",
    "smits96_icslp",
    "baba96_icslp",
    "lickley96b_icslp",
    "sawusch96_icslp",
    "deshmukh96_icslp",
    "arai96b_icslp",
    "whalen96_icslp",
    "mousty96_icslp",
    "mcqueen96_icslp",
    "bonneau96_icslp",
    "bond96_icslp",
    "burnham96_icslp",
    "magnuson96_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.1996"
}