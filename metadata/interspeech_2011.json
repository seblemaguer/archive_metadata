{
 "title": "Interspeech 2011",
 "location": "Florence, Italy",
 "startDate": "27/8/2011",
 "endDate": "31/8/2011",
 "chair": "General Chairs: Piero Cosi, Renato De Mori",
 "conf": "Interspeech",
 "year": "2011",
 "name": "interspeech_2011",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2011",
 "date": "27-31 August 2011",
 "booklet": "interspeech_2011.pdf",
 "papers": {
  "hirschberg11_interspeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Speaking more like you: entrainment in conversational speech",
   "original": "i11_4001",
   "page_count": 0,
   "order": 1,
   "p1": "4001",
   "pn": "",
   "abstract": [
    "When people engage in conversation, they adapt their speaking style of their conversational partner in a number of ways. They have been shown to adopt their interlocutor's way of describing objects and to align their accent, their syntax, their pitch range, and their speaking rate to their partner's -- as well as their gestures. They also adapt their turn-taking style and use of cue phrases to match their partner's. These types of entrainment have been shown to correlate with various measures of task success and dialogue naturalness. While there is considerable evidence for lexical entrainment from laboratory experiments, less is known about other types of acoustic-prosodic and discourselevel entrainment and little work has been done to examine entrainments in multiple modalities for the same dialogue. I will discuss research in entrainment in multiple dimensions on the Columbia Games Corpus and the Switchboard Corpus. Our goal is to understand how the different varieties of entrainment correlate with one another and to determine which types of entrainment will be both useful and feasible to model in Spoken Dialogue Systems.. Presenter Prof. Julia Hirschberg is professor of Computer Science at Columbia University, working on prosody, emotional speech and dialogue systems. She was editor-in-chief of Computational Linguistics from 1993-2003 and co-editor-in-chief of Speech Communication from 2003-2006. She served on the Executive Board of the Association for Computational Linguistics (ACL) from 1993-2003, on the Permanent Council of International Conference on Spoken Language Processing (ICSLP) since 1996, on the board of the International Speech Communication Association (ISCA) from 1999- 2007 (as President 2005-2007), and is currently on the board of the CRA-W. She has been a fellow of the American Association for Artificial Intelligence since 1994 and an ISCA Fellow since 2008. She received an honorary doctorate from KTH in 2007 and the James L. Flanagan Speech and Audio Processing Award in 2011.\n",
    ""
   ]
  },
  "mitchell11_interspeech": {
   "authors": [
    [
     "Tom M.",
     "Mitchell"
    ]
   ],
   "title": "Neural representations of word meanings",
   "original": "i11_4002",
   "page_count": 0,
   "order": 2,
   "p1": "4002",
   "pn": "",
   "abstract": [
    "How does the human brain represent meanings of words and pictures in terms of neural activity? This talk will present our research addressing this question, by applying machine learning algorithms to fMRI and MEG brain image data. One line of our research involves training classifiers that identify which word a person is thinking about, based on their observed neural activity. A second line involves training computational models that predict the neural activity associated with arbitrary English words, including words for which we do not yet have brain image data. A third line of work involves examining neural activity at millisecond time resolution during the comprehension of words and phrases. Presenter Professor Tom M. Mitchell is a professor in the E. Fredkin University and head of the Machine Learning Department at Carnegie Mellon University. His research interests lie in cognitive neuroscience, machine learning, natural language processing, and artificial intelligence. Mitchell is a member of the US National Academy of Engineering, a Fellow of the American Association for the Advancement of Science (AAAS), and Fellow and Past President of the Association for the Advancement of Artificial Intelligence (AAAI). Mitchell believes the field of machine learning will be the fastest growing branch of computer science during the 21st century.\n",
    ""
   ]
  },
  "pentland11_interspeech": {
   "authors": [
    [
     "Alex",
     "Pentland"
    ]
   ],
   "title": "Signals and speech",
   "original": "i11_0001",
   "page_count": 4,
   "order": 3,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "How did humans coordinate before we had sophisticated language capabilities? Pre-linguistic social species coordinate by signaling, and in particular `honest signals' which actually cause changes in the listener. I will present examples of human behaviors that are likely honest signals, and show that they can be used to predict the outcomes of dyadic interactions (dating, negotiation, trust assessment, etc.) with an average accuracy of 80%. Patterns of signaling also allow accurate identification of social and task roles in small groups, predict task performance in small groups, guide team formation, and understand aspects of organizational performance. These experiments suggest that modern language evolved `on top' of ancient signaling mechanisms, and that today linguistic and signaling mechanisms operate in parallel. Presenter Professor Alex 'Sandy' Pentland is a pioneer in computational social science, organizational engineering, and mobile information systems. He directs the MIT Human Dynamics Lab, developing computational social science and using this new science to guide organizational engineering. He also directs the Media Lab Entrepreneurship Program, spinning off companies to bring MIT technologies into the real world. He is among the most-cited computer scientists in the world. His most recent book is `Honest Signals' published by MIT Press.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-1"
  },
  "matza11_interspeech": {
   "authors": [
    [
     "Avi",
     "Matza"
    ],
    [
     "Yuval",
     "Bistritz"
    ]
   ],
   "title": "Skew Gaussian mixture models for speaker recognition",
   "original": "i11_0005",
   "page_count": 4,
   "order": 4,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "The current paper proposes skew Gaussian mixture models for speaker recognition and an associated algorithm for its training from experimental data. Speaker identification experiments were conducted, in which speakers were modeled using the familiar Gaussian mixture models (GMM), and the new skew-GMM. Each model type was evaluated using two sets of feature vectors, the mel-frequency cepstral coefficients (MFCC), that are widely used in speaker recognition applications, and line spectra frequencies (LSF), that are used in many low bit rate speech coders but were not that successful in speech and speaker recognition. Results showed that the skew-GMM, with LSF, compares favorably with the GMM-MFCC pair (under fair comparison conditions). They indicate that skew-Gaussians are better suited for capturing the relatively highly non-symmetrical shapes of the LSF distribution. Thus the skew-GMM with LSF offers a worthy alternative to the GMM-MFCC pair for speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-2"
  },
  "toledoronen11_interspeech": {
   "authors": [
    [
     "Orith",
     "Toledo-Ronen"
    ],
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "Towards goat detection in text-dependent speaker verification",
   "original": "i11_0009",
   "page_count": 4,
   "order": 5,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "We present a method that identifies speakers that are likely to have a high false-reject rate in a text-dependent speaker verification system (\"goats\"). The method normally uses only the enrollment data to perform this task. We begin with extracting an appropriate feature from each enrollment session. We then rank all the enrollment sessions in the system based on this feature. The lowest-ranking sessions are likely to have a high false-reject rate. We explore several features and show that the 1% lowest-ranking enrollments have a false reject rate of up to 7.8%, compared to our system's overall rate of 2.0%. Furthermore, when using a single additional verification score from the true speaker for ranking, the false-reject of the 1% lowest-ranking sessions rises up to 33%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-3"
  },
  "bonastre11_interspeech": {
   "authors": [
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Gabriel H.",
     "Sierra"
    ],
    [
     "Pierre-Michel",
     "Bousquet"
    ]
   ],
   "title": "Speaker modeling using local binary decisions",
   "original": "i11_0013",
   "page_count": 4,
   "order": 6,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "Achieving an accurate speaker modeling is a crucial step in any speaker-related algorithm. Many statistical speaker modeling techniques that deviate from the classical GMM/UBM approach have been proposed for some time now that can accurately discriminate between speakers. Although many of them imply the evaluation of high dimensional feature vectors and represent a speaker with a single vector, therefore not using any temporal information. In addition, they place most emphasis on modeling the most recurrent acoustic events, instead of less occurring speaker discriminant information. In this paper we explain the main benefits of our recently proposed binary speaker modeling technique and show its benefits in two particular applications, namely for speaker recognition and speaker diarization. Both applications achieve near to state-of-the-art results while benefiting from performing most processing in the binary space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-4"
  },
  "aronowitz11_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "New developments in voice biometrics for user authentication",
   "original": "i11_0017",
   "page_count": 4,
   "order": 7,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "Voice biometrics for user authentication is a task in which the object is to perform convenient, robust and secure authentication of speakers. In this work we investigate the use of state-of-theart text-independent and text-dependent speaker verification technology for user authentication. We evaluate four different authentication conditions: speaker specific digit stings, global digit strings, prompted digit strings, and text-independent. Harnessing the characteristics of the different types of conditions can provide benefits such as authentication transparent to the user (convenience), spoofing robustness (security) and improved accuracy (reliability). The systems were evaluated on a corpus collected by Wells Fargo Bank which consists of 750 speakers. We show how to adapt techniques such as joint factor analysis (JFA), Gaussian mixture models with nuisance attribute projection (GMM-NAP) and hidden Markov models with NAP (HMM-NAP) to obtain improved results for new authentication scenarios and environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-5"
  },
  "mandasari11_interspeech": {
   "authors": [
    [
     "Miranti Indar",
     "Mandasari"
    ],
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Evaluation of i-vector speaker recognition systems for forensic application",
   "original": "i11_0021",
   "page_count": 4,
   "order": 8,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "This paper contributes a study on i-vector based speaker recognition systems and their application to forensics. The sensitivity of i-vector based speaker recognition is analyzed with respect to the effects of speech duration. This approach is motivated by the potentially limited speech available in a recording for a forensic case. In this context, the classification performance and calibration costs of the i-vector system are analyzed along with the role of normalization in the cosine kernel. Evaluated on the NIST SRE-2010 dataset, results highlight that normalization of the cosine kernel provided improved performance across all speech durations compared to the use of an unnormalized kernel. The normalized kernel was also found to play an important role in reducing miscalibration costs and providing well-calibrated likelihood ratios with limited speech duration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-6"
  },
  "senoussaoui11_interspeech": {
   "authors": [
    [
     "Mohammed",
     "Senoussaoui"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Niko",
     "Brümmer"
    ],
    [
     "Edward de",
     "Villiers"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Mixture of PLDA models in i-vector space for gender-independent speaker recognition",
   "original": "i11_0025",
   "page_count": 4,
   "order": 9,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "The Speaker Recognition community that participates in NIST evaluations has concentrated on designing gender- and channelconditioned systems. In the real word, this conditioning is not feasible. Our main purpose in this work is to propose a mixture of Probabilistic Linear Discriminant Analysis models (PLDA) as a solution for making systems independent of speaker gender. In order to show the effectiveness of the mixture model, we first experiment on 2010 NIST telephone speech (det5), where we prove that there is no loss of accuracy compared with a baseline gender-dependent model. We also test with success the mixture model on a more realistic situation where there are cross-gender trials. Furthermore, we report results on microphone speech for the det1, det2, det3 and det4 tasks to confirm the effectiveness of the mixture model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-7"
  },
  "iyer11_interspeech": {
   "authors": [
    [
     "Nandini",
     "Iyer"
    ],
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Brian D.",
     "Simpson"
    ]
   ],
   "title": "Segregation of whispered speech interleaved with noise or speech maskers",
   "original": "i11_0029",
   "page_count": 4,
   "order": 10,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "Some listening environments require listeners to segregate a whispered target talker from a background of other talkers. In this experiment, a whispered speech signal was presented continuously in the presence of a continuous masker (noise, voiced speech or whispered speech) or alternated with the masker at an 8-Hz rate. Performance was near ceiling in the alternated whisper and noise condition, suggesting that harmonic structure due to voicing is not necessary to segregate a speech signal from an interleaved random-noise masker. Indeed, when whispered speech was interleaved with voiced speech, performance decreased relative to the continuous condition when the target talker was voiced but not when it was whispered, suggesting that listeners are better at selectively attending to unvoiced intervals and ignoring voiced intervals than the converse.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-8"
  },
  "kliper11_interspeech": {
   "authors": [
    [
     "Roi",
     "Kliper"
    ],
    [
     "Hendrik",
     "Kayser"
    ],
    [
     "Daphna",
     "Weinshall"
    ],
    [
     "Israel",
     "Nelken"
    ],
    [
     "Jörn",
     "Anemüller"
    ]
   ],
   "title": "Monaural azimuth localization using spectral dynamics of speech",
   "original": "i11_0033",
   "page_count": 4,
   "order": 11,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "We tackle the task of localizing speech signals on the horizontal plane using monaural cues. We show that monaural cues as incorporated in speech are efficiently captured by amplitude modulation spectra patterns. We demonstrate that by using these patterns, a linear Support Vector Machine can use directionality-related information to learn to discriminate and classify sound location at high resolution. We propose a straightforward and robust way of integrating information from two ears. Each ear is treated as an independent processor and information is integrated at the decision level thus resolving, to a large extent, ambiguity in location.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-9"
  },
  "rennies11_interspeech": {
   "authors": [
    [
     "Jan",
     "Rennies"
    ],
    [
     "Thomas",
     "Brand"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Prediction of binaural intelligibility level differences in reverberation",
   "original": "i11_0037",
   "page_count": 4,
   "order": 12,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "Speech intelligibility can be substantially improved when speech and interfering noise are spatially separated. This spatial unmasking is commonly attributed to effects of head shadow and binaural auditory processing. In reverberant rooms spatial unmasking is generally reduced. In this study spatial unmasking is systematically measured in reverberant conditions for several configurations of binaural, diotic and monaural speech signals. The data are compared to predictions of a recently developed binaural speech intelligibility model. The high prediction accuracy (R2 > 0.97) indicates that the model is applicable in real rooms and may serve as a tool in room acoustical design.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-10"
  },
  "gautreau11_interspeech": {
   "authors": [
    [
     "Aurore",
     "Gautreau"
    ],
    [
     "Michel",
     "Hoen"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Let's all speak together! exploring the impact of various languages on the comprehension of speech in multi-linguistic babble",
   "original": "i11_0041",
   "page_count": 4,
   "order": 13,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "Our research aims at exploring psycholinguistic processes implicated in the speech-in-speech situation. Our studies focused on the interferences observed during speech-in-speech comprehension. Our goal is to clarify if interferences exist only on an acoustical level or if there are clear psycholinguistic interferences. In 3 experiments, we used 4 talkers cocktail-party signals using different world languages: French, Breton, Irish and Italian. Participants had to identify French words inserted in a babble. Results first confirmed that it is more difficult to understand a French word in a French background than in a babble composed of unknown languages. This result demonstrates that the interference effect is not purely acoustic but rather linguistic. Results also showed differences in the observed performances depending on the unknown language spoken in the background and demonstrated that some languages interfered more with French than some others.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-11"
  },
  "shafiro11_interspeech": {
   "authors": [
    [
     "Valeriy",
     "Shafiro"
    ],
    [
     "Stanley",
     "Sheft"
    ],
    [
     "Robert",
     "Risley"
    ]
   ],
   "title": "Cross-rate variation in the intelligibility of dual-rate gated speech in older listeners",
   "original": "i11_0045",
   "page_count": 4,
   "order": 14,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "Intelligibility of sentences gated with a single primary rate (0.5.8 Hz, 25.75 duty cycle) or gated with an additional concurrent rate of 24 Hz and a 50% duty cycle was examined in older normal-hearing and hearing-impaired listeners. With a stronger effect of age than hearing loss, intelligibility tended to increase with primary rate and duty cycle, but varied for dual-rate gating. Reduction in the total amount of speech due to concurrent 24 Hz gating had little effect on the intelligibility for the lowest and highest primary rates, but was detrimental for rates between 2 to 4 Hz, mimicking the pattern previously obtained from young normal-hearing listeners. The dual-rate intelligibility decrement with a 2 Hz primary rate significantly correlated with speech intelligibility in multi-talker babble, suggesting overlap of perceptual processes. Overall, findings reflect interaction of central and peripheral processing of speech occurring on different time scales.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-12"
  },
  "lee11_interspeech": {
   "authors": [
    [
     "Chia-ying",
     "Lee"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Oded",
     "Ghitza"
    ]
   ],
   "title": "An efferent-inspired auditory model front-end for speech recognition",
   "original": "i11_0049",
   "page_count": 4,
   "order": 15,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "In this paper, we investigate a closed-loop auditory model and explore its potential as a feature representation for speech recognition. The closed-loop representation consists of an auditory-based, efferent-inspired feedback mechanism that regulates the operating point of a filter bank, thus enabling it to dynamically adapt to changing background noise. With dynamic adaptation, the closedloop representation demonstrates an ability to compensate for the effects of noise on speech, and generates a consistent feature representation for speech when contaminated by different kinds of noises. Our preliminary experimental results indicate that the efferent-inspired feedback mechanism enables the closed-loop auditory model to consistently improve word recognition accuracies, when compared with an open-loop representation, for mismatched training and test noise conditions in a connected digit recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-13"
  },
  "ali11_interspeech": {
   "authors": [
    [
     "Faten Ben",
     "Ali"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Sonia Djaziri",
     "Larbi"
    ]
   ],
   "title": "A long-term harmonic plus noise model for speech signals",
   "original": "i11_0053",
   "page_count": 4,
   "order": 16,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "The harmonic plus noise model (HNM) is widely used for spectral modeling of mixed harmonic/noise speech sounds. In this paper, we present an analysis/synthesis system based on a long-term two-band HNM. \"Long-term\" means that the time-trajectories of the HNM parameters are modeled using \"smooth\" (discrete cosine) functions depending on a small set of parameters. The goal is to capture and exploit the long-term correlation of spectral components on time segments of up to several hundreds of ms. The proposed long-term HNM enables joint compact representation of signals (thus a potential for low bit-rate coding) and easy signal transformation (e.g. time stretching) directly from the long-term parameters. Experiments show that it can be compared favourably with the short-term version in terms of parameter rates and signal quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-14"
  },
  "cinneide11_interspeech": {
   "authors": [
    [
     "Alan Ó",
     "Cinnéide"
    ],
    [
     "David",
     "Dorran"
    ],
    [
     "Mikel",
     "Gainza"
    ],
    [
     "Eugene",
     "Coyle"
    ]
   ],
   "title": "A frequency domain approach to ARX-LF voiced speech parameterization and synthesis",
   "original": "i11_0057",
   "page_count": 4,
   "order": 17,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The ARX-LF model interprets voiced speech as the an LF derivative glottal pulse exciting an all-pole vocal tract filter with an additional exogenous residual signal. It fully parameterizes the voice and has been shown to be useful for voice modification. Because time domain methods to determine the ARX-LF parameters from speech are very sensitive to the time placement of the analysis frame and not robust to phase distortion from e.g. recording equipment, a magnitude-only spectral approach to ARX-LF parameterization was recently developed.\n",
    "This paper describes extensions to this frequency domain approach to obtain continuous robust ARX-LF parameters for voiced speech segments. A listening test of 50 participants comparing synthetic speech produced by this method with a time domain ARX-LF parameterization approach under real and simulated recording conditions was conducted and it was found that the frequency domain approach was generally preferred.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-15"
  },
  "ramanarayanan11_interspeech": {
   "authors": [
    [
     "Vikram",
     "Ramanarayanan"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Automatic data-driven learning of articulatory primitives from real-time MRI data using convolutive NMF with sparseness constraints",
   "original": "i11_0061",
   "page_count": 4,
   "order": 18,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "We present a procedure to automatically derive interpretable dynamic articulatory primitives in a data-driven manner from image sequences acquired through real-time magnetic resonance imaging (rt-MRI). More specifically, we propose a convolutive Nonnegative Matrix Factorization algorithm with sparseness constraints (cNMFsc) to decompose a given set of image sequences into a set of basis image sequences and an activation matrix. We use a recentlyacquired rt-MRI corpus of read speech (460 sentences from 4 speakers) as a test dataset for this procedure. We choose the free parameters of the algorithm empirically by analyzing algorithm performance for different parameter values. We then validate the extracted basis sequences using an articulatory recognition task and finally present an interpretation of the extracted basis set of image sequences in a gesture-based Articulatory Phonology framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-16"
  },
  "wang11_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "Online pattern learning for non-negative convolutive sparse coding",
   "original": "i11_0065",
   "page_count": 4,
   "order": 19,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "The unsupervised learning of spectro-temporal speech patterns is relevant in a broad range of tasks. Convolutive non-negative matrix factorization (CNMF) and its sparse version, convolutive non-negative sparse coding (CNSC), are powerful, related tools. A particular difficulty of CNMF/CNSC, however, is the high demand on computing power and memory, which can prohibit their application to large scale tasks. In this paper, we propose an online algorithm for CNMF and CNSC, which processes input data piece-by-piece and updates the learned patterns after the processing of each piece by using accumulated sufficient statistics. The online CNSC algorithm remarkably increases converge speed of the CNMF/CNSC pattern learning, thereby enabling its application to large scale tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-17"
  },
  "malyska11_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Malyska"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Robert",
     "Dunn"
    ]
   ],
   "title": "Sinewave representations of nonmodality",
   "original": "i11_0069",
   "page_count": 4,
   "order": 20,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "Regions of nonmodal phonation, exhibiting deviations from uniform glottal-pulse periods and amplitudes, occur often and convey information about speaker- and linguistic-dependent factors. Such waveforms pose challenges for speech modeling, analysis/synthesis, and processing. In this paper, we investigate the representation of nonmodal pulse trains as a sum of harmonically-related sinewaves with time-varying amplitudes, phases, and frequencies. We show that a sinewave representation of any impulsive signal is not unique and also the converse, i.e., frame-based measurements of the underlying sinewave representation can yield different impulse trains. Finally, we argue how this ambiguity may explain addition, deletion, and movement of pulses in sinewave synthesis and a specific illustrative example of time-scale modification of a nonmodal case of diplophonia.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-18"
  },
  "raj11_interspeech": {
   "authors": [
    [
     "Ch. Srikanth",
     "Raj"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Time-varying signal adaptive transform and IHT recovery of compressive sensed speech",
   "original": "i11_0073",
   "page_count": 4,
   "order": 21,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "Compressive Sensing (CS) signal recovery has been formulated for signals sparse in a known linear transform domain. We consider the scenario in which the transformation is unknown and the goal is to estimate the transform as well as the sparse signal from just the CS measurements. Specifically, we consider the speech signal as the output of a time-varying AR process, as in the linear system model of speech production, with the excitation being sparse. We propose an iterative algorithm to estimate both the system impulse response and the excitation signal from the CS measurements. We show that the proposed algorithm, in conjunction with a modified iterative hard thresholding, is able to estimate the signal adaptive transform accurately, leading to much higher quality signal reconstruction than the codebook based matching pursuit approach. The estimated time-varying transform is better than a 256 size codebook estimated from original speech. Thus, we are able to get near \"toll quality\" speech reconstruction from sub-Nyquist rate CS measurements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-19"
  },
  "wollmer11_interspeech": {
   "authors": [
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Felix",
     "Weninger"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Acoustic-linguistic recognition of interest in speech with bottleneck-BLSTM nets",
   "original": "i11_0077",
   "page_count": 4,
   "order": 22,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "This paper proposes a novel technique for speech-based interest recognition in natural conversations. We introduce a fully automatic system that exploits the principle of bidirectional Long Short-Term Memory (BLSTM) as well as the structure of so-called bottleneck networks. BLSTM nets are able to model a self-learned amount of context information, which was shown to be beneficial for affect recognition applications, while bottleneck networks allow for efficient feature compression within neural networks. In addition to acoustic features, our technique considers linguistic information obtained from a multi-stream BLSTM-HMM speech recognizer. Evaluations on the TUM AVIC corpus reveal that the bottleneck-BLSTM method prevails over all approaches that have been proposed for the Interspeech 2010 Paralinguistic Challenge task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-20"
  },
  "erden11_interspeech": {
   "authors": [
    [
     "Mustafa",
     "Erden"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "Automatic detection of anger in human-human call center dialogs",
   "original": "i11_0081",
   "page_count": 4,
   "order": 23,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "Automatic emotion recognition can enhance evaluation of customer satisfaction and detection of customer problems in call centers. For this purpose emotion recognition is defined as binary classification for angry and non-angry on Turkish human-human call center conversations. We investigated both acoustic and language models for this task. Support Vector Machines (SVM) resulted in 82.9% accuracy whereas Gaussian Mixture Models (GMM) gave a slightly worse performance with 77.9%. In terms of the language modeling we compared word based, stem-only and stem+ending structures. Stem+ending based system resulted in higher accuracy with 72% using manual transcriptions. This can be mainly attributed to the agglutinative nature of Turkish language. When we fused the acoustic and LM classifiers using a Multi Layer Perceptron (MLP) we could achieve a 89% correct detection of both angry and non-angry classes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-21"
  },
  "chang11_interspeech": {
   "authors": [
    [
     "Keng-hao",
     "Chang"
    ],
    [
     "Howard",
     "Lei"
    ],
    [
     "John",
     "Canny"
    ]
   ],
   "title": "Improved classification of speaking styles for mental health monitoring using phoneme dynamics",
   "original": "i11_0085",
   "page_count": 4,
   "order": 24,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "This paper investigates the usefulness of segmental phonemedynamics for classification of speaking styles. We modeled transition details based on the phoneme sequences emitted by a speech recognizer, using data obtained from a recording of 39 depressed patients with 7 different speaking styles.normal, pressured, slurred, stuttered, flat, slow and fast speech. We designed and compared two set of phoneme models: a language model treating each phoneme as a word unit (one for each style) and a context-dependent phoneme duration model based on Gaussians for each speaking style considered. The experiments showed that language modeling at the phoneme level performed better than the duration model. We also found that better performance can be obtained by user normalization. To see the complementary effect of the phoneme-based models, the classifiers were combined at a decision level with a Hidden Markov Model (HMM) classifier built from spectral features. The improvement was 5.7% absolute (10.4% relative), reaching 60.3% accuracy in 7-class and 71.0% in 4-class classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-22"
  },
  "black11_interspeech": {
   "authors": [
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Brian R.",
     "Baucom"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "“you made me do it”: classification of blame in married couples' interactions by fusing automatically derived speech and language information",
   "original": "i11_0089",
   "page_count": 4,
   "order": 25,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "One of the goals of behavioral signal processing is the automatic prediction of relevant high-level human behaviors from complex, realistic interactions. In this work, we analyze dyadic discussions of married couples and try to classify extreme instances (low/high) of blame expressed from one spouse to another. Since blame can be conveyed through various communicative channels (e.g., speech, language, gestures), we compare two different classification methods in this paper. The first classifier is trained with the conventional static acoustic features and models \"how\" the spouses spoke. The second is a novel automatic speech recognition-derived classifier, which models \"what\" the spouses said. We get the best classification performance (82% accuracy) by exploiting the complementarity of these acoustic and lexical information sources through score-level fusion of the two classification methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-23"
  },
  "goudbeek11_interspeech": {
   "authors": [
    [
     "Martijn",
     "Goudbeek"
    ],
    [
     "Marie",
     "Nilsenová"
    ]
   ],
   "title": "Context and priming effects in the recognition of emotion of old and young listeners",
   "original": "i11_0093",
   "page_count": 4,
   "order": 26,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "The development of our ability to recognize (vocal) emotional expression has been relatively understudied. Even less studied is the effect of linguistic (spoken) context on emotion perception. In this study we investigate the performance of young (18.25) and old (60.85) listeners on two tasks: an emotion recognition task where emotions expressed in a sustained vowel (/a/) had to be recognized and an emotion attribution task where listeners had to judge a neutral fragment that was preceded by a phrase that varied in speech rate and/or loudness. The results of the recognition task showed that old and young participants do not differ in their recognition accuracy. The emotion attribution task showed that young listeners are more likely to interpret neutral stimuli as emotional when the preceding speech is emotionally colored. The results are interpreted as evidence for diminished plasticity later in life.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-24"
  },
  "gravano11_interspeech": {
   "authors": [
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Rivka",
     "Levitan"
    ],
    [
     "Laura",
     "Willson"
    ],
    [
     "Štefan",
     "Beňuš"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Ani",
     "Nenkova"
    ]
   ],
   "title": "Acoustic and prosodic correlates of social behavior",
   "original": "i11_0097",
   "page_count": 4,
   "order": 27,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "We describe acoustic/prosodic and lexical correlates of social variables annotated on a large corpus of task-oriented spontaneous speech. We employ Amazon Mechanical Turk to label the corpus with a large number of social behaviors, examining results of three of these here. We find significant differences between male and female speakers for perceptions of attempts to be liked, likeability, speech planning, that also differ depending upon the gender of their conversational partners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-25"
  },
  "oh11_interspeech": {
   "authors": [
    [
     "Kyung Hwan",
     "Oh"
    ],
    [
     "June Sig",
     "Sung"
    ],
    [
     "Doo Hwa",
     "Hong"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Decision tree-based clustering with outlier detection for HMM-based speech synthesis",
   "original": "i11_0101",
   "page_count": 4,
   "order": 28,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "In order to express natural prosodic variations in continuous speech, sophisticated speech units such as the context-dependent phone models are usually employed in HMM-based speech synthesis techniques. Since the training database cannot practically cover all possible context factors, decision tree-based HMM states clustering is commonly applied. One of the serious problems in a decision tree-based method is that the criterion used for node splitting and stopping is sensitive to irrelevant outlier data. In this paper, we propose a novel approach to removing outliers during the decision tree growing phase. Experimental results show that removing of outlying models improves the quality of the synthesized speech, especially for sentences which originally demonstrated poor quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-26"
  },
  "silen11_interspeech": {
   "authors": [
    [
     "Hanna",
     "Silén"
    ],
    [
     "Elina",
     "Helander"
    ],
    [
     "Moncef",
     "Gabbouj"
    ]
   ],
   "title": "Prediction of voice aperiodicity based on spectral representations in HMM speech synthesis",
   "original": "i11_0105",
   "page_count": 4,
   "order": 29,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "In hidden Markov model-based speech synthesis, speech is typically parameterized using source-filter decomposition. A widely used analysis/synthesis framework, STRAIGHT, decomposes the speech waveform into a framewise spectral envelope and a mixed mode excitation signal. Inclusion of an aperiodicity measure in the model enables synthesis also for signals that are not purely voiced or unvoiced. In the traditional approach employing hidden Markov modeling and decision tree-based clustering, the connection between speech spectrum and aperiodicities is not taken into account. In this paper, we take advantage of this dependency and predict voice aperiodicities afterwards based on synthetic spectral representations. The evaluations carried out for English data confirm that the proposed approach is able to provide prediction accuracy that is comparable to the traditional approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-27"
  },
  "nose11_interspeech": {
   "authors": [
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A perceptual expressivity modeling technique for speech synthesis based on multiple-regression HSMM",
   "original": "i11_0109",
   "page_count": 4,
   "order": 30,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "This paper describes a technique for modeling and controlling emotional expressivity of speech in HMM-based speech synthesis. A problem of conventional emotional speech synthesis based on HMM is that the intensity of an emotional expression appearing in synthetic speech completely depends on the database used for model training. To take into account the emotional expressivity that listeners actually perceive, the perceptual expressivity scores are introduced into a style control technique based on multipleregression hidden semi-Markov model (MRHSMM). The objective and subjective evaluation results show that the proposed technique works well when there is a large bias of emotional expressivity in the training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-28"
  },
  "hashimoto11_interspeech": {
   "authors": [
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Multi-speaker modeling with shared prior distributions and model structures for Bayesian speech synthesis",
   "original": "i11_0113",
   "page_count": 4,
   "order": 31,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This paper investigates a multi-speaker modeling technique with shared prior distributions and model structures for Bayesian speech synthesis. The quality of synthesized speech is improved by selecting appropriate model structures in HMM-based speech synthesis. Bayesian approach is known to work for such model selection. However, the result is strongly affected by prior distributions of model parameters. Therefore, determination of prior distributions and selection of model structures should be performed simultaneously. This paper investigates prior distributions and model structures in the situation where training data of multiple speakers are available. The prior distributions and model structures which represent acoustic features common to every speakers can be obtained by sharing them between multiple speaker-dependent models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-29"
  },
  "ling11_interspeech": {
   "authors": [
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Feature-space transform tying in unified acoustic-articulatory modelling for articulatory control of HMM-based speech synthesis",
   "original": "i11_0117",
   "page_count": 4,
   "order": 32,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "In previous work, we have proposed a method to control the characteristics of synthetic speech flexibly by integrating articulatory features into hidden Markov model (HMM) based parametric speech synthesis. A unified acoustic-articulatory model was trained and a piecewise linear transform was adopted to describe the dependency between these two feature streams. The transform matrices were trained for each HMM state and were tied based on each state's context. In this paper, an improved acoustic-articulatory modelling method is proposed. A Gaussian mixture model (GMM) is introduced to model the articulatory space and the cross-stream transform matrices are trained for each Gaussian mixture instead of context-dependently. This means the dependency relationship can vary with the change of articulatory features flexibly. Our results show this method improves the effectiveness of control over vowel quality by modifying articulatory trajectories without degrading naturalness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-30"
  },
  "shannon11_interspeech": {
   "authors": [
    [
     "Matt",
     "Shannon"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "The effect of using normalized models in statistical speech synthesis",
   "original": "i11_0121",
   "page_count": 4,
   "order": 33,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "The standard approach to HMM-based speech synthesis is inconsistent in the enforcement of the deterministic constraints between static and dynamic features. The trajectory HMM and autoregressive HMM have been proposed as normalized models which rectify this inconsistency. This paper investigates the practical effects of using these normalized models, and examines the strengths and weaknesses of the different models as probabilistic models of speech. The most striking difference observed is that the standard approach greatly underestimates predictive variance. We argue that the normalized models have better predictive distributions than the standard approach, but that all the models we consider are still far from satisfactory probabilistic models of speech. We also present evidence that better intra-frame correlation modelling goes some way towards improving existing normalized models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-31"
  },
  "picart11_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Picart"
    ],
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "Continuous control of the degree of articulation in HMM-based speech synthesis",
   "original": "i11_1797",
   "page_count": 4,
   "order": 34,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "This paper focuses on the implementation of a continuous control of the degree of articulation (hypo/hyperarticulation) in the framework of HMM-based speech synthesis. The adaptation of a neutral speech synthesizer to generate hypo and hyperarticulated speech using a limited amount of speech data is first studied. This is done using inter-speaker voice adaptation techniques, applied here to intra-speaker voice adaptation. The implementation of a continuous control of the degree of articulation is then proposed in a second step. Finally, a subjective evaluation shows that good quality neutral/hypo/hyperarticulated speech, and also any intermediate, interpolated or extrapolated articulation degrees, can be obtained from an HMM-based speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-32"
  },
  "chen11_interspeech": {
   "authors": [
    [
     "Ling-Hui",
     "Chen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Estimation of window coefficients for dynamic feature extraction for HMM-based speech synthesis",
   "original": "i11_1801",
   "page_count": 4,
   "order": 35,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "In standard approaches to hidden Markov model (HMM)-based speech synthesis, window coefficients for calculating dynamic features are pre-determined and fixed. This may not be optimal to capture various context-dependent dynamic characteristics in speech signals. This paper proposes a data-driven technique to estimate the window coefficients. They are optimized so as to maximize the likelihood of trajectory HMMs given data. Experimental results show that the proposed technique can achieve a comparable performance with the mean- and variance-updated trajectory HMMs in the naturalness of synthesized speech, while offering significantly lower computational cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-33"
  },
  "wen11_interspeech": {
   "authors": [
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Inverse filtering based harmonic plus noise excitation model for HMM-based speech synthesis",
   "original": "i11_1805",
   "page_count": 4,
   "order": 36,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "In this paper, a new Voicing Cut-Off Frequency (VCO) estimation method based on inverse filtering is presented. The spectrum of residual signal got from inverse filtering is split into sub-bands which are clustered into two classes by using K-means algorithm. And then, the Viterbi algorithm is used to search a smoothed VCO contour. Based on this new VCO estimation method, an adaptation of Harmonic Noise Model is also proposed to reconstruct the residual signal with both harmonic and noise components. The proposed excitation model can reduce the buzziness of speech generated by normal vocoders using simple pulse train, and has been integrated into a HMM-based speech synthesis system (HTS). The listening test showed that the HTS with our new method gives better quality of synthesized speech than the traditional HTS which only uses simple pulse train excitation model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-34"
  },
  "erro11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Erro"
    ],
    [
     "Iñaki",
     "Sainz"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inma",
     "Hernáez"
    ]
   ],
   "title": "Improved HNM-based vocoder for statistical synthesizers",
   "original": "i11_1809",
   "page_count": 4,
   "order": 37,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "Statistical parametric synthesizers have achieved very good performance scores during the last years. Nevertheless, as they require the use of vocoders to parameterize speech (during training) and to reconstruct waveforms (during synthesis), the speech generated from statistical models lacks some degree of naturalness. In previous works we explored the usefulness of the harmonics plus noise model in the design of a high-quality speech vocoder. Quite promising results were achieved when this vocoder was integrated into a synthesizer. In this paper, we describe some recent improvements related to the excitation parameters, particularly the so called maximum voiced frequency. Its estimation and explicit modelling leads to an even better synthesis performance as confirmed by subjective comparisons with other well-known methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-35"
  },
  "anumanchipalli11_interspeech": {
   "authors": [
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ],
    [
     "Luís C.",
     "Oliveira"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "A statistical phrase/accent model for intonation modeling",
   "original": "i11_1813",
   "page_count": 4,
   "order": 38,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "This paper proposes a statistical phrase/accent model of voice fundamental frequency (F0) for speech synthesis. It presents an approach for automatic extraction and modeling of phrase and accent phenomena from F0 contours by taking into account their overall trends in the training data. An iterative optimization algorithm is described to extract these components, minimizing the reconstruction error of the F0 contour. This method of modeling local and global components of F0 separately is shown to be better than conventional F0 models used in Statistical Parametric Speech Synthesis (SPSS). Perceptual evaluations confirm that the proposed model is significantly better than baseline SPSS F0 models in 3 prosodically diverse tasks . read speech, radio broadcast speech and audio book speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-36"
  },
  "henter11_interspeech": {
   "authors": [
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Intermediate-state HMMs to capture continuously-changing signal features",
   "original": "i11_1817",
   "page_count": 4,
   "order": 39,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "Traditional discrete-state HMMs are not well suited for describing steadily evolving, path-following natural processes like motion capture data or speech. HMMs cannot represent incremental progress between behaviors, and sequences sampled from the models have unnatural segment durations, unsmooth transitions, and excessive rapid variation. We propose to address these problems by permitting the state variable to occupy positions between the discrete states, and present a concrete left-right model incorporating this idea. We call this intermediate-state HMMs. The state evolution remains Markovian. We describe training using the generalized EM-algorithm and present associated update formulas. An experiment shows that the intermediate-state model is capable of gradual transitions, with more natural durations and less noise in sampled sequences compared to a conventional HMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-37"
  },
  "braunschweiler11_interspeech": {
   "authors": [
    [
     "Norbert",
     "Braunschweiler"
    ],
    [
     "Sabine",
     "Buchholz"
    ]
   ],
   "title": "Automatic sentence selection from speech corpora including diverse speech for improved HMM-TTS synthesis quality",
   "original": "i11_1821",
   "page_count": 4,
   "order": 40,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "Using publicly available audiobooks for HMM-TTS poses new challenges. This paper addresses the issue of diverse speech in audiobooks. The aim is to identify diverse speech likely to have a negative effect on HMM-TTS quality. Manual removal of diverse speech was found to yield better synthesis quality despite halving the training corpus. To handle large amounts of data an automatic approach is proposed. The approach uses a small set of acoustic and text based features. A series of listening tests showed that the manual selection is most preferred, while the automatic selection showed significant preference over the full training set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-38"
  },
  "liang11_interspeech": {
   "authors": [
    [
     "Hui",
     "Liang"
    ],
    [
     "John",
     "Dines"
    ]
   ],
   "title": "Phonological knowledge guided HMM state mapping for cross-lingual speaker adaptation",
   "original": "i11_1825",
   "page_count": 4,
   "order": 41,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "Within the HMM state mapping-based cross-lingual speaker adaptation framework, the minimum Kullback-Leibler divergence criterion has been typically employed to measure the similarity of two average voice state distributions from two respective languages for state mapping construction. Considering that this simple criterion doesn't take any language-specific information into account, we propose a data-driven, phonological knowledge guided approach to strengthen the mapping construction . state distributions from the two languages are clustered according to broad phonetic categories using decision trees and mapping rules are constructed only within each of the clusters. Objective evaluation of our proposed approach demonstrates reduction of mel-cepstral distortion and that mapping rules derived from a single training speaker generalize to other speakers, with subtle improvement being detected during subjective listening tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-39"
  },
  "obin11_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Pierre",
     "Lanchantin"
    ],
    [
     "Anne",
     "Lacheret"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Reformulating prosodic break model into segmental HMMs and information fusion",
   "original": "i11_1829",
   "page_count": 4,
   "order": 42,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "In this paper, a method for prosodic break modelling based on segmental-HMMs and Dempster-Shafer fusion for speech synthesis is presented, and the relative importance of linguistic and metric constraints in prosodic break modelling is assessed1. A context-dependent segmental-HMM is used to explicitly model the linguistic and the metric constraints. Dempster-Shafer fusion is used to balance the relative importance of the linguistic and the metric constraints into the segmental-HMM. A linguistic processing chain based on surface and deep syntactic parsing is additionally used to extract linguistic informations of different nature. An objective evaluation proved evidence that the optimal combination of the linguistic and the metric constraints significantly outperforms both the conventional HMM (linguistic information only) and segmental-HMM (equal balance of linguistic and metric constraints), and confirmed that the linguistic constraint is prior to the metric.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-40"
  },
  "maia11_interspeech": {
   "authors": [
    [
     "Ranniery",
     "Maia"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Sabine",
     "Buchholz"
    ]
   ],
   "title": "Multipulse sequences for residual signal modeling",
   "original": "i11_1833",
   "page_count": 4,
   "order": 43,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "In source-filter models of speech production, the residual signal . what remains after passing the speech signal through the inverse filter . contains important information for the generation of naturally sounding re-synthesized speech. Typically, the voiced regions of residual signals are regarded as a mixture of glottal pulse and noise. This paper introduces a novel approach to represent the noise component of voiced regions of residual signals through autoregressive filtering of multipulse sequences. The positions and amplitudes of the non-zero samples of these multipulse signals are optimized through a closed-loop procedure. The method in question is applied to excitation modeling in statistical parametric synthesis. Experimental results indicate that the use of multipulse-based noise component construction eliminates the necessity of run-time ad hoc procedures such as high-pass filtering and time modulation, common on excitation models for statistical parametric synthesizers, with no loss of synthesized speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-41"
  },
  "valentinibotinhao11_interspeech": {
   "authors": [
    [
     "Cassia",
     "Valentini-Botinhao"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Can objective measures predict the intelligibility of modified HMM-based synthetic speech in noise?",
   "original": "i11_1837",
   "page_count": 4,
   "order": 44,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "Synthetic speech can be modified to improve intelligibility in noise. In order to perform modifications automatically, it would be useful to have an objective measure that could predict the intelligibility of modified synthetic speech for human listeners. We analysed the impact on intelligibility . and on how well objective measures predict it.when we separately modify speaking rate, fundamental frequency, line spectral pairs and spectral peaks. Shifting LSPs can increase intelligibility for human listeners; other modifications had weaker effects. Among the objective measures we evaluated, the Dau model and the Glimpse proportion were the best predictors of human performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-42"
  },
  "nitta11_interspeech": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Takayuki",
     "Onoda"
    ],
    [
     "Masashi",
     "Kimura"
    ],
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Kouichi",
     "Katsurada"
    ]
   ],
   "title": "Speech synthesis based on articulatory-movement HMMs with voice-source codebooks",
   "original": "i11_1841",
   "page_count": 4,
   "order": 45,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "Speech synthesis based on one-model of articulatory movement HMMs, that are commonly applied to both speech recognition (SR) and speech synthesis (SS), is described. In an SS module, speaker-invariant HMMs are applied to generate an articulatory feature (AF) sequence, and then, after converting AFs into vocal tract parameters by using a multilayer neural network (MLN), a speech signal is synthesized through an LSP digital filter. The CELP coding technique is applied to improve voice-sources when generating these sources from embedded codes in the corresponding state of HMMs. The proposed SS module separates phonetic information and the individuality of a speaker. Therefore, the targeted speaker's voice can be synthesized with a small amount of speech data. In the experiments, we carried out listening tests for ten subjects and evaluated both of sound quality and individuality of synthesized speech. As a result, we confirmed that the proposed SS module could produce good quality speech of the targeted speaker even when the training was done with the data set of two-sentences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-43"
  },
  "kato11_interspeech": {
   "authors": [
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Makoto",
     "Yamada"
    ],
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Keiichiro",
     "Oura"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Large-scale subjective evaluations of speech rate control methods for HMM-based speech synthesizers",
   "original": "i11_1845",
   "page_count": 4,
   "order": 46,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "Three speech rate control methods for HMM-based speech synthesis were compared by large-scale subjective evaluations. The methods are 1) synthesizing speech sounds based on HMMs trained from corpora at a target speech rate, 2) stretching or shrinking utterance durations proportionally in waveform generation, and 3) determining state durations based on ML criterion under a restriction of utterance duration. The results indicated that the proportional shrinking had significant advantages for fast rate, whereas HMMs trained from slow speech sounds had a slight advantage for slow rate. We also found an advantage of proportionally shrunk speech from a synthesizer trained from slow speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-44"
  },
  "maeno11_interspeech": {
   "authors": [
    [
     "Yu",
     "Maeno"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Hideharu",
     "Nakajima"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Osamu",
     "Yoshioka"
    ]
   ],
   "title": "HMM-based emphatic speech synthesis using unsupervised context labeling",
   "original": "i11_1849",
   "page_count": 4,
   "order": 47,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "This paper describes an approach to HMM-based expressive speech synthesis which does not require any supervised labeling process for emphasis context. We use appealing-style speech whose sentences were taken from real domains. To reduce the cost for labeling speech data with an emphasis context for the model training, we propose an unsupervised labeling technique of the emphasis context based on the difference between original and generated F0 patterns of training sentences. Although the criterion for the emphasis labeling is quite simple, subjective evaluation results reveal that the unsupervised labeling is comparable to the labeling conducted carefully by a human in terms of speech naturalness and emphasis reproducibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-45"
  },
  "zhang11_interspeech": {
   "authors": [
    [
     "Ce",
     "Zhang"
    ],
    [
     "Rong",
     "Zheng"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Restoring the residual speaker information in total variability modeling for speaker verification",
   "original": "i11_0125",
   "page_count": 4,
   "order": 48,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "In this paper, we introduce the residual space into the Total Variability Modeling by assuming that the speaker super-vectors are not totally contained in a linear subspace of low dimension. Thus the feature reduction carried out by Probabilistic Principal Component Analysis (PPCA) leads to information loss including information of speaker as well as channel. We add the residual factor to restore the missing speaker information which is lost during the PPCA process. To utilize the recovered information effectively, we propose two fusion methods that combine the principal components with the residual factor. We compare the fusion results that are obtained with direct scoring and Support Vector Machines for classification, respectively. The experiments on NIST SRE 2006 show that the performance can be improved consistently by involving the residual factor, e.g. the best result achieves 6% relative improvement on Equal Error Rate(EER) compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-46"
  },
  "aronowitz11b_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Oren",
     "Barkan"
    ]
   ],
   "title": "New developments in joint factor analysis for speaker verification",
   "original": "i11_0129",
   "page_count": 4,
   "order": 49,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Joint factor analysis (JFA) is widely used by state-of-the-art speech processing systems for tasks such as speaker verification, language identification and emotion detection. In this paper we introduce new developments for the JFA framework which we validate empirically for the speaker verification task but in principle may be beneficial for other tasks too. We first propose a method for obtaining improved recognition accuracy by better modeling supervector estimation uncertainty. We then propose a novel approach we name JFAlight for extremely efficient approximated estimation of speaker, common and channel factors. Using JFAlight we are able to efficiently score a given test session with a very small degradation in accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-47"
  },
  "gonzalezrodriguez11_interspeech": {
   "authors": [
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Speaker recognition using temporal contours in linguistic units: the case of formant and formant-bandwidth trajectories",
   "original": "i11_0133",
   "page_count": 4,
   "order": 50,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "We describe a new approach to automatic speaker recognition based in explicit modeling of temporal contours in linguistic units (TCLU). Inspired in successful work in forensic speaker identification, we extend the approach to design a fully automatic system, with a high potential for combination with spectral systems. Using SRI's Decipher phone, word and syllabic labels, we have tested up to 468 unit-based subsystems from 6 groups of lexically-determined units, namely phones, diphones, triphones, center phone in triphones, syllables and words, subsystems being combined at the score level. Evaluating with NIST SRE04 English-only 1s1s, their hierarchical fusion gives an EER of 4.20% (minDCF=0.018) from automatic formant tracking of conversational telephone speech. Combining extremely well with a Joint Factor Analysis system (from JFA EER of 4.25% to 2.47%, minDCF from 0.020 to 0.012), extensions as more robust prosodic or spectral features are likely to further improve this approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-48"
  },
  "glembek11_interspeech": {
   "authors": [
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Niko",
     "Brümmer"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Pavel",
     "Matějka"
    ]
   ],
   "title": "Discriminatively trained i-vector extractor for speaker verification",
   "original": "i11_0137",
   "page_count": 4,
   "order": 51,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "We propose a strategy for discriminative training of the i-vector extractor in speaker recognition. The original i-vector extractor training was based on the maximum-likelihood generative modeling, where the EM algorithm was used. In our approach, the i-vector extractor parameters are numerically optimized to minimize the discriminative cross-entropy error function. Two versions of the i-vector extraction are studied - the original approach as defined for Joint Factor Analysis, and the simplified version, where orthogonalization of the i-vector extractor matrix is performed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-49"
  },
  "sanchez11_interspeech": {
   "authors": [
    [
     "Michelle Hewlett",
     "Sanchez"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Constrained cepstral speaker recognition using matched UBM and JFA training",
   "original": "i11_0141",
   "page_count": 4,
   "order": 52,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "We study constrained speaker recognition systems, or systems that model standard cepstral features that fall within particular types of speech regions. A question in modeling such systems is whether to constrain universal background model (UBM) training, joint factor analysis (JFA), or both. We explore this question, as well as how to optimize UBM model size, using a corpus of Arabic male speakers. Over a large set of phonetic and prosodic constraints, we find that the performance of a system using constrained JFA and UBM is on average 5.24% better than when using constraint-independent (all frames) JFA and UBM. We find further improvement from optimizing UBM size based on the percentage of frames covered by the constraint.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-50"
  },
  "mccree11_interspeech": {
   "authors": [
    [
     "Alan",
     "McCree"
    ],
    [
     "Douglas",
     "Sturim"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "A new perspective on GMM subspace compensation based on PPCA and wiener filtering",
   "original": "i11_0145",
   "page_count": 4,
   "order": 53,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "We present a new perspective on the subspace compensation techniques that currently dominate the field of speaker recognition using Gaussian Mixture Models (GMMs). Rather than the traditional factor analysis approach, we use Gaussian modeling in the sufficient statistic supervector space combined with Probabilistic Principal Component Analysis (PPCA) within-class and shared across class covariance matrices to derive a family of training and testing algorithms. Key to this analysis is the use of two noise terms for each speech cut: a random channel offset and a length dependent observation noise. Using the Wiener filtering perspective, formulas for optimal train and test algorithms for Joint Factor Analysis (JFA) are simple to derive. In addition, we can show that an alternative form of Wiener filtering results in the i-vector approach, thus tying together these two disparate techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-51"
  },
  "zhang11b_interspeech": {
   "authors": [
    [
     "Ce",
     "Zhang"
    ],
    [
     "Rong",
     "Zheng"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Data-driven Gaussian component selection for fast GMM-based speaker verification",
   "original": "i11_0245",
   "page_count": 4,
   "order": 54,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "In this paper, a fast likelihood calculation of Gaussian mixture model (GMM) is presented, by means of dividing the acoustic space into disjoint subsets and then assigning the most relevant Gaussians to each of them. The data-driven approach is explored to select Gaussian component which guarantees that the loss, brought by pre-discarding most useless Gaussians, can be easily controlled by a manual set parameter. To avoid the rapid growth of the index table size, a two level index scheme is proposed. We adjust several set of parameters to validate our work which is expected to speed up the computation while maintaining the performance. The results of the experiments on the female part of the telephone condition of NIST SRE 2006 indicate that the speed can be improved up to 5 times over the GMM-UBM baseline system without performance loss.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-52"
  },
  "garciaromero11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Analysis of i-vector length normalization in speaker recognition systems",
   "original": "i11_0249",
   "page_count": 4,
   "order": 55,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "We present a method to boost the performance of probabilistic generative models that work with i-vector representations. The proposed approach deals with the non- Gaussian behavior of i-vectors by performing a simple length normalization. This nonlinear transformation allows the use of probabilistic models with Gaussian assumptions that yield equivalent performance to that of more complicated systems based on Heavy-Tailed assumptions. Significant performance improvements are demonstrated on the telephone portion of NIST SRE 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-53"
  },
  "jiang11_interspeech": {
   "authors": [
    [
     "Weiwu",
     "Jiang"
    ],
    [
     "Zhifeng",
     "Li"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "An analysis framework based on random subspace sampling for speaker verification",
   "original": "i11_0253",
   "page_count": 4,
   "order": 56,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "Using Joint Factor Analysis (JFA) supervector for subspace analysis has many problems, such as high processing complexity and over-fitting. We propose an analysis framework based on random subspace sampling to address these problems. In this framework, JFA supervectors are first partitioned equally and each partitioned subvector is projected on to a subspace by PCA. All projected subvectors are then concatenated and PCA is applied again to reduce the dimension by projection onto a low-dimensional feature space. Finally, we randomly sample this feature space and build classifiers for the sampled features. The classifiers are fused to produce the final classification output. Experiments on NIST SRE08 prove the effectiveness of the proposed framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-54"
  },
  "scheffer11_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Yun",
     "Lei"
    ],
    [
     "Luciana",
     "Ferrer"
    ]
   ],
   "title": "Factor analysis back ends for MLLR transforms in speaker recognition",
   "original": "i11_0257",
   "page_count": 4,
   "order": 57,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "The purpose of this work is to show how recent developments in cepstral-based systems for speaker recognition can be leveraged for the use of Maximum Likelihood Linear Regression (MLLR) transforms. Speaker recognition systems based on MLLR transforms have shown to be greatly beneficial in combination with standard systems, but most of the advances in speaker modeling techniques have been implemented for cepstral features. We show how these advances, based on Factor Analysis, such as eigenchannel and ivector, can be easily employed to achieve very high accuracy. We show that they outperform the current state-of-the-art MLLR-SVM system that SRI submitted during the NIST SRE 2010 evaluation. The advantages of leveraging the new approaches are manyfold: the ability to process a large amount of data, working in a reduced dimensional space, importing any advances made for cepstral systems to the MLLR features, and the potential for system combination at the i-vector level.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-55"
  },
  "greenberg11_interspeech": {
   "authors": [
    [
     "Craig S.",
     "Greenberg"
    ],
    [
     "Alvin F.",
     "Martin"
    ],
    [
     "Bradford N.",
     "Barr"
    ],
    [
     "George R.",
     "Doddington"
    ]
   ],
   "title": "Report on performance results in the NIST 2010 speaker recognition evaluation",
   "original": "i11_0261",
   "page_count": 4,
   "order": 58,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "In the spring of 2010, the National Institute of Standards and Technology organized a Speaker Recognition Evaluation in which several factors believed to affect the performance of speaker recognition systems were explored. Among the factors considered in the evaluation were channel conditions, duration of training and test segments, number of training segments, and level of vocal effort. New cost function parameters emphasizing lower false alarm rates were used for two of the tests in the evaluation, and the reduction in false alarm rates exhibited by many of the systems suggests that the new measure may have helped to focus research on the low false alarm region of operation, which is important in many applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-56"
  },
  "kockmann11_interspeech": {
   "authors": [
    [
     "Marcel",
     "Kockmann"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "ivector fusion of prosodic and cepstral features for speaker verification",
   "original": "i11_0265",
   "page_count": 4,
   "order": 59,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "In this paper we apply the promising iVector extraction technique followed by PLDA modeling to simple prosodic contour features. With this procedure we achieve results comparable to a system that models much more complex prosodic features using our recently proposed SMM-based iVector modeling technique. We then propose a combination of both prosodic iVectors by joint PLDA modeling that leads to significant improvements over individual systems with an EER of 5.4% on NIST SRE 2008 telephone data. Finally, we can combine these two prosodic iVector front ends with a baseline cepstral iVector system to achieve up to 21% relative reduction in new DCF.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-57"
  },
  "kanagasundaram11_interspeech": {
   "authors": [
    [
     "Ahilan",
     "Kanagasundaram"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Michael",
     "Mason"
    ]
   ],
   "title": "i-vector based speaker recognition on short utterances",
   "original": "i11_2341",
   "page_count": 4,
   "order": 60,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "Robust speaker verification on short utterances remains a key consideration when deploying automatic speaker recognition, as many real world applications often have access to only limited duration speech data. This paper explores how the recent technologies focused around total variability modeling behave when training and testing utterance lengths are reduced. Results are presented which provide a comparison of Joint Factor Analysis (JFA) and i-vector based systems including various compensation techniques; Within-Class Covariance Normalization (WCCN), LDA, Scatter Difference Nuisance Attribute Projection (SDNAP) and Gaussian Probabilistic Linear Discriminant Analysis (GPLDA). Speaker verification performance for utterances with as little as 2 sec of data taken from the NIST Speaker Recognition Evaluations are presented to provide a clearer picture of the current performance characteristics of these techniques in short utterance conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-58"
  },
  "sun11_interspeech": {
   "authors": [
    [
     "Hanwu",
     "Sun"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Study of overlapped speech detection for NIST SRE summed channel speaker recognition",
   "original": "i11_2345",
   "page_count": 4,
   "order": 61,
   "p1": "2345",
   "pn": "2348",
   "abstract": [
    "This paper studies the overlapped speech detection for improving the performance of the summed channel speaker recognition system in NIST Speaker Recognition Evaluation (SRE). The speaker recognition system includes four main modules: voice activity detection, speaker diarization, overlapped speaker detection and speaker recognition. We adopt a GMM based overlapped speaker detection system, by using entropy, MFCC and LPC features, to remove the overlapped segments in summed channel test condition. With the overlapped speech detection, the speaker diarization achieves a relative 18% diarization error rate reduction for the 2008 NIST SRE summed channel test set, and we obtain relative equal error rate reductions of 13.3% and 9.4% in speaker recognition on the 1conv-summed task and 8conv-summed task, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-59"
  },
  "ma11_interspeech": {
   "authors": [
    [
     "Zhanyu",
     "Ma"
    ],
    [
     "Arne",
     "Leijon"
    ]
   ],
   "title": "Super-dirichlet mixture models using differential line spectral frequencies for text-independent speaker identification",
   "original": "i11_2349",
   "page_count": 4,
   "order": 62,
   "p1": "2349",
   "pn": "2352",
   "abstract": [
    "A new text-independent speaker identification (SI) system is proposed. This system utilizes the line spectral frequencies (LSFs) as alternative feature set for capturing the speaker characteristics. The boundary and ordering properties of the LSFs are considered and the LSF are transformed to the differential LSF (DLSF) space. Since the dynamic information is useful for speaker recognition, we represent the dynamic information of the DLSFs by considering two neighbors of the current frame, one from the past frames and the other from the following frames. The current frame with the neighbor frames together are cascaded into a supervector. The statistical distribution of this supervector is modelled by the so-called super-Dirichlet mixture model, which is an extension from the Dirichlet mixture model. Compared to the conventional SI system, which is using the mel-frequency cepstral coefficients and based on the Gaussian mixture model, the proposed SI system shows a promising improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-60"
  },
  "yu11_interspeech": {
   "authors": [
    [
     "Hon-Bill",
     "Yu"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "Comparison of voice activity detectors for interview speech in NIST speaker recognition evaluation",
   "original": "i11_2353",
   "page_count": 4,
   "order": 63,
   "p1": "2353",
   "pn": "2356",
   "abstract": [
    "Interview speech has become an important part of the NIST Speaker Recognition Evaluations (SREs). Unlike telephone speech, interview speech has substantially lower signal-to-noise ratio, which necessitates robust voice activity detection (VAD). This paper highlights the characteristics of interview speech files in NIST SREs and discusses the difficulties in performing speech/nonspeech segmentation in these files. To overcome these difficulties, this paper proposes using speech enhancement techniques as a preprocessing step for enhancing the reliability of energy-based and statistical-model-based VADs. It was found that spectral subtraction can make better use of the background spectrum than the likelihood-ratio tests in statistical-model-based VADs. A decision strategy is also proposed to overcome the undesirable effects caused by impulsive signals and sinusoidal background signals. Results on NIST 2010 SRE show that the proposed VAD outperforms the statistical-model-based VAD, the ETSI-AMR speech coder, and the ASR transcripts provided by NIST SRE Workshop.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-61"
  },
  "sarkar11_interspeech": {
   "authors": [
    [
     "A. K.",
     "Sarkar"
    ],
    [
     "S.",
     "Umesh"
    ]
   ],
   "title": "Eigen-voice based anchor modeling system for speaker identification using MLLR super-vector",
   "original": "i11_2357",
   "page_count": 4,
   "order": 64,
   "p1": "2357",
   "pn": "2360",
   "abstract": [
    "In this paper, we propose an anchor modeling scheme where instead of conventional \"anchor\" speakers, we use eigenvectors that span the Eigen-voice space. The computational advantage of conventional Anchor-modeling based speaker identification system comes from representing all speakers in a space spanned by a small number of anchor speakers instead of having separate speaker models. The conventional \"anchor\" speakers are usually chosen using data-driven clustering and the number of such speakers are also empirically determined. The use of proposed eigen-voice based anchors provide a more systematic way of spanning the speaker-space and in determining the optimal number of anchors. In our proposed method, the eigenvector space is built using the Maximum Likelihood Linear Regression (MLLR) super-vectors of non-target speakers. Further, the proposed method does not require calculation of the likelihood with respect to anchor speaker models to create the speaker-characterization vector as done in conventional anchor systems. Instead, speakers are characterized with respect to eigen-space by projecting the speaker's MLLR-super vector onto the eigen-voice space. This makes the method computationally efficient. Experimental results show that the proposed method consistently performs better than conventional anchor modeling technique for different number of anchor speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-62"
  },
  "wang11b_interspeech": {
   "authors": [
    [
     "Wen",
     "Wang"
    ],
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "Automatic detection of speaker attributes based on utterance text",
   "original": "i11_2361",
   "page_count": 4,
   "order": 65,
   "p1": "2361",
   "pn": "2364",
   "abstract": [
    "In this paper, we present models for detecting various attributes of a speaker based on uttered text alone. These attributes include whether the speaker is speaking his/her native language, the speaker's age and gender, and the regional information reported by the speakers. We explore various lexical features as well as features inspired by Linguistic Inquiry and Word Count and Dictionary of Affect in Language. Overall, results suggest that when audio data is not available, by exploring effective feature sets only from uttered text and system combinations of multiple classification algorithms, we can build high quality statistical models to detect these attributes of speakers, comparable to systems that can exploit the audio data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-63"
  },
  "cumani11_interspeech": {
   "authors": [
    [
     "Sandro",
     "Cumani"
    ],
    [
     "Pier Domenico",
     "Batzu"
    ],
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Claudio",
     "Vair"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Vasileios",
     "Vasilakakis"
    ]
   ],
   "title": "Comparison of speaker recognition approaches for real applications",
   "original": "i11_2365",
   "page_count": 4,
   "order": 66,
   "p1": "2365",
   "pn": "2368",
   "abstract": [
    "This paper describes the experimental setup and the results obtained using several state-of-the-art speaker recognition classifiers. The comparison of the different approaches aims at the development of real world applications, taking into account memory and computational constraints, and possible mismatches with respect to the training environment. The NIST SRE 2008 database has been considered our reference dataset, whereas nine commercially available databases of conversational speech in languages different form the ones used for developing the speaker recognition systems have been tested as representative of an application domain. Our results, evaluated on the two domains, show that the classifiers based on i-vectors obtain the best recognition and calibration accuracy. Gaussian PLDA and a recently introduced discriminative SVM together with an adaptive symmetric score normalization achieve the best performance using low memory and processing resources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-64"
  },
  "polzehl11_interspeech": {
   "authors": [
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Modeling speaker personality using voice",
   "original": "i11_2369",
   "page_count": 4,
   "order": 67,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "In this paper, we validate the application of an established personality assessment and modeling paradigm to speech input, and extend earlier work towards text independent speech input. We show that human labelers can consistently label acted speech data generated across multiple recording sessions, and investigate further which of the 5 scales in the NEO-FFI scheme can be assessed from speech, and how a manipulation of one scale influences the perception of another. Finally, we present a clustering of human labels of perceived personality traits, which will be useful in future experiments on automatic classification and generation of personality traits from speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-65"
  },
  "ferras11_interspeech": {
   "authors": [
    [
     "Marc",
     "Ferràs"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Structural joint factor analysis for speaker recognition",
   "original": "i11_2373",
   "page_count": 4,
   "order": 68,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "In recent years, adaptation techniques have been given a special focus in speaker recognition tasks. Addressing the separation of speaker and session variation effects, Joint Factor Analysis (JFA) has been consolidated as a powerful adaptation framework and has become ubiquitous in the last NIST Speaker Recognition Evaluations (SRE). However, its global parameter sharing strategy is not necessarily optimal when a small amount of adaptation data is available. In this paper, we address this issue by resorting to a regularization approach such as structural MAP. We introduce two variants of structural JFA (SJFA) that, depending on the amount of data, use coarser or finer parameter approximations in the adaptation process. One of these variants is shown to considerably outperform JFA. We report relative gains over 25% EER on the 2006 NIST SRE data for GMM-SVM systems using SJFA over systems using JFA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-66"
  },
  "biswas11_interspeech": {
   "authors": [
    [
     "Sangeeta",
     "Biswas"
    ],
    [
     "Marc",
     "Ferràs"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Acoustic forest for SMAP-based speaker verification",
   "original": "i11_2377",
   "page_count": 4,
   "order": 69,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "In speaker verification, structural maximum-a-posteriori (SMAP) adaptation for Gaussian mixture model (GMM) has been proven effective, especially when the speech segment is very short. In SMAP adaptation, an acoustic tree of Gaussian components is constructed to represent the hierarchical acoustic space. Until now, however, there has been no clear way to automatically find the optimal tree structure for a given speaker. In this paper, we propose using an acoustic forest, which is a set of trees, for SMAP adaptation, instead of a single tree. In this approach, we combine the results of SMAP adaptation systems with different acoustic trees. A key issue is how to combine the trees. We explore three score fusion techniques, and evaluate our approach in the text-independent speaker verification task of the NIST 2006 SRE plan using 10-second speech segments. Our proposed method decreased EER by 3.2% from the relevant MAP adaptation and by 1.6% from the conventional SMAP with a single tree.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-67"
  },
  "sivaram11_interspeech": {
   "authors": [
    [
     "G. S. V. S.",
     "Sivaram"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Mixture of auto-associative neural networks for speaker verification",
   "original": "i11_2381",
   "page_count": 4,
   "order": 70,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "The paper introduces a mixture of auto-associative neural networks for speaker verification. A new objective function based on posterior probabilities of phoneme classes is used for training the mixture. This objective function allows each component of the mixture to model part of the acoustic space corresponding to a broad phonetic class. This paper also proposes how factor analysis can be applied in this setting. The proposed techniques show promising results on a subset of NIST-08 speaker recognition evaluation (SRE) and yield about 10% relative improvement when combined with the state-of-the-art Gaussian Mixture Model i-vector system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-68"
  },
  "scharenborg11_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Holger",
     "Mitterer"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Perceptual learning of liquids",
   "original": "i11_0149",
   "page_count": 4,
   "order": 71,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "Previous research on lexically-guided perceptual learning has focussed on contrasts that differ primarily in local cues, such as plosive and fricative contrasts. The present research had two aims: to investigate whether perceptual learning occurs for a contrast with non-local cues, the /l/-/r/ contrast, and to establish whether STRAIGHT can be used to create ambiguous sounds on an /l/-/r/ continuum. Listening experiments showed lexically-guided learning about the /l/-/r/ contrast. Listeners can thus tune in to unusual speech sounds characterised by non-local cues. Moreover, STRAIGHT can be used to create stimuli for perceptual learning experiments, opening up new research possibilities.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-69"
  },
  "tuinman11_interspeech": {
   "authors": [
    [
     "Annelie",
     "Tuinman"
    ],
    [
     "Holger",
     "Mitterer"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "The efficiency of cross-dialectal word recognition",
   "original": "i11_0153",
   "page_count": 4,
   "order": 72,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Dialects of the same language can differ in the casual speech processes they allow; e.g., British English allows the insertion of [r] at word boundaries in sequences such as saw ice, while American English does not. In two speeded word recognition experiments, American listeners heard such British English sequences; in contrast to non-native listeners, they accurately perceived intended vowel-initial words even with intrusive [r]. Thus despite input mismatches, cross-dialectal word recognition benefits from the full power of native-language processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-70"
  },
  "tsuzaki11_interspeech": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Jinfu",
     "Ni"
    ]
   ],
   "title": "Estimation of perceptual spaces for speaker identities based on the cross-lingual discrimination task",
   "original": "i11_0157",
   "page_count": 4,
   "order": 73,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "This paper reconfirms that talker identity can be transmitted across languages. Talker discrimination was examined in the ABX paradigm, where the stimuli A and B were utterances by different talkers in the same language and the stimulus X was an utterance by either of A or B in the different language. The average hit rate of this discrimination task was as high as 0.89. The mutual distance matrices were generated using the discrimination index, d\u0002. By applying the multidimensional scaling, three-dimensional perceptual spaces were estimated. The features related with loudness and spectral centroid had high contribution to the perceptual dimensions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-71"
  },
  "peperkamp11_interspeech": {
   "authors": [
    [
     "Sharon",
     "Peperkamp"
    ],
    [
     "Camillia",
     "Bouchon"
    ]
   ],
   "title": "The relation between perception and production in L2 phonological processing",
   "original": "i11_0161",
   "page_count": 4,
   "order": 74,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "Seventeen French-English bilinguals read aloud a set of English sentences and performed an ABX discrimination task that assessed their perception of the English /I/-/i/ contrast. Global nativelikeness in production correlated with pronunciation accuracy for the vowels /I/ and /i/, and both production measures correlated with self-estimated pronunciation skills. However, performance on the perception task did not correlate with either global nativelikeness or /I,i/ pronunciation accuracy. These results are discussed in light of theories about the relation between perception and production in L2 phonological processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-72"
  },
  "bissiri11_interspeech": {
   "authors": [
    [
     "Maria Paola",
     "Bissiri"
    ],
    [
     "Maria Luisa",
     "Garcia Lecumberri"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Jan",
     "Volín"
    ]
   ],
   "title": "The role of word-initial glottal stops in recognizing English words",
   "original": "i11_0165",
   "page_count": 4,
   "order": 75,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "English word-initial vowels in natural continuous speech are optionally preceded by glottal stops or functionally equivalent glottalizations. It may be claimed that these glottal elements disturb the smooth flow of speech. However, they clearly mark word boundaries, which may potentially facilitate speech processing in the brain of the listener. The present study utilizes the word-monitoring paradigm to determine whether listeners react faster to words with or without glottalizations. Three groups of subjects were compared: Czech and Spanish learners of English and native English speakers. The results indicate that perceptual use of glottalization for word segmentation is not entirely governed by universal rules and reflects the mother tongue of the listener as well as the status (L1/L2) of the target language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-73"
  },
  "zhang11c_interspeech": {
   "authors": [
    [
     "Caicai",
     "Zhang"
    ],
    [
     "Gang",
     "Peng"
    ],
    [
     "William S.-Y.",
     "Wang"
    ]
   ],
   "title": "Effect of language experience on the categorical perception of Cantonese vowel duration",
   "original": "i11_0169",
   "page_count": 4,
   "order": 76,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This study investigated the effect of language experience on the categorical perception of Cantonese vowel duration distinction. By comparing Cantonese and Mandarin listeners' performances, we found that: (1) duration change elicited categorical perception in the performance of Cantonese listeners, but not in Mandarin listeners; (2) Cantonese listeners were affected by the vowel quality differences, whereas Mandarin subjects were generally unbiased towards the quality differences; (3) effect of duration was overridden by the vowel quality [a] condition in the performance of Cantonese listeners. Our findings suggested that vowel quality is incorporated as a phonological cue in Cantonese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-74"
  },
  "pedersen11_interspeech": {
   "authors": [
    [
     "C. F.",
     "Pedersen"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Adaptive estimation of zeros of time-varying z-transforms",
   "original": "i11_0173",
   "page_count": 4,
   "order": 77,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "In the present paper, a method is proposed for adaptive estimation and tracking of roots of time-varying, complex, and univariate polynomials, e.g. z-transform polynomials that arise from finite signal sequences. The objective with the method is to alleviate the computational burden induced by factorization. The estimation is done by solving a set of linear equations; the number of equations equals the order of the polynomial. To avoid potential drifting of the estimations, it is proposed to verify with Aberth-Ehrlich's factorization method at given intervals.\n",
    "A numerical experiment supplements theory by estimating roots of time-varying polynomials of different order. As a function of order, the proposed method has a lower run time than Lindsey-Fox and computing eigenvalues of companion matrices. The estimations are quite accurate, but tend to drift slightly in response to increasing coefficient perturbation lengths.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-75"
  },
  "kane11_interspeech": {
   "authors": [
    [
     "John",
     "Kane"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Identifying regions of non-modal phonation using features of the wavelet transform",
   "original": "i11_0177",
   "page_count": 4,
   "order": 78,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "The present study proposes a new parameter for identifying breathy to tense voice qualities in a given speech segment using measurements from the wavelet transform. Techniques that can deliver robust information on the voice quality of a speech segment are desirable as they can help tune analysis strategies as well as provide automatic voice quality annotation in large corpora. The method described here involves wavelet-based decomposition of the speech signal into octave bands and then fitting a regression line to the maximum amplitudes at the different scales. The slope coefficient is then evaluated in terms of its ability to differentiate voice qualities compared to other parameters in the literature. The new parameter (named here Peak Slope) was shown to have robustness to babble noise added with signal to noise ratios as low as 10 dB. Furthermore, the proposed parameter was shown to provide better differentiation of breathy to tense voice qualities in both vowels and running speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-76"
  },
  "fan11_interspeech": {
   "authors": [
    [
     "Xing",
     "Fan"
    ],
    [
     "Keith W.",
     "Godin"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Acoustic analysis of whispered speech for phoneme and speaker dependency",
   "original": "i11_0181",
   "page_count": 4,
   "order": 79,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "Whisper is used by speakers in certain circumstances to protect personal information. Due to the differences in production mechanisms between neutral and whispered speech, there are considerable differences between the spectral structure of neutral and whispered speech, such as formant shifts and shifts in spectral slope. This study analyzes the dependency of these differences on speakers and phonemes by applying a Vector Taylor Series (VTS) approximation to a model of the transformation of neutral speech into whispered speech, and estimating the parameters of this model using an Expectation Maximization (EM) algorithm. The results from this study shed light on the speaker and phoneme dependency of the shifts of neutral to whisper speech, and suggest that similarly derived model adaptation or compensation schemes for whisper speech/speaker recognition will be highly speaker dependent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-77"
  },
  "asaei11_interspeech": {
   "authors": [
    [
     "Afsaneh",
     "Asaei"
    ],
    [
     "Mohammad J.",
     "Taghizadeh"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Volkan",
     "Cevher"
    ]
   ],
   "title": "Multi-party speech recovery exploiting structured sparsity models",
   "original": "i11_0185",
   "page_count": 4,
   "order": 80,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "We study the sparsity of spectro-temporal representation of speech in reverberant acoustic conditions. This study motivates the use of structured sparsity models for efficient speech recovery. We formulate the underdetermined convolutive speech separation in spectro-temporal domain as the sparse signal recovery where we leverage model-based recovery algorithms. To tackle the ambiguity of the real acoustics, we exploit the Image Model of the enclosures to estimate the room impulse response function through a structured sparsity constraint optimization. The experiments conducted on real data recordings demonstrate the effectiveness of the proposed approach for multi-party speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-78"
  },
  "mallidi11_interspeech": {
   "authors": [
    [
     "Sri Harish",
     "Mallidi"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Modulation spectrum analysis for recognition of reverberant speech",
   "original": "i11_0189",
   "page_count": 4,
   "order": 81,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "Recognition of reverberant speech constitutes a challenging problem for typical speech recognition systems. This is mainly due to the conventional short-term analysis/compensation techniques. In this paper, we present a feature extraction technique based on modeling long segments of temporal envelopes of the speech signal in narrow sub-bands using frequency domain linear prediction (FDLP). FDLP provides an all-pole approximation of the Hilbert envelope of the signal by linear prediction on cosine transform of the signal. We show that the FDLP modulation spectrum plays an important role in the robustness of the proposed feature extraction. Automatic speech recognition (ASR) experiments on speech data degraded with a number of room impulse responses (with varying degrees of distortion) show significant performance improvements for the proposed FDLP features when compared to other robust feature extraction techniques (average relative reduction of 40% in word error rate). Similar improvements are also obtained for far-field data which contain natural reverberation in background noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-79"
  },
  "petkov11_interspeech": {
   "authors": [
    [
     "Petko N.",
     "Petkov"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Bert de",
     "Vries"
    ]
   ],
   "title": "Discrete choice models for non-intrusive quality assessment",
   "original": "i11_0193",
   "page_count": 4,
   "order": 82,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "Non-intrusive signal quality assessment in general, and its application to speech signal processing, in particular, builds extensively upon statistical regression models. Commonly, the raw preference scores used for fitting these models belong to a categorical scale. Averaging the scores over a number of test subjects results in smooth, close-to-continuous ratings, thus justifying the use of regression as opposed to classification models. A form of marginalization, averaging subjective ratings takes away useful information about the reliability of individual test points. Using a model tailored to the raw data achieves highly competitive performance in terms of conventional performance measures while providing the additional advantage of identifying the usability of individual test points. In this paper, we consider the application of discrete choice models to non-intrusive quality assessment of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-80"
  },
  "kinoshita11_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Mehrez",
     "Souden"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Single channel dereverberation using example-based speech enhancement with uncertainty decoding technique",
   "original": "i11_0197",
   "page_count": 4,
   "order": 83,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "A speech signal captured by a distant microphone is generally contaminated by reverberation, which severely degrades the audible quality and intelligibility of the observed speech. In this paper, we investigate the single channel dereverberation which has been considered as one of the most challenging tasks. We propose an example-based speech enhancement approach used in combination with non-example-based (conventional) blind dereverberation algorithm, that would complement each other. The term, example-based, refers to the method which has exact (not brief and statistical) information about the clean speech as its model. It is important to note that the combination of two algorithms is formulated utilizing the uncertainty decoding technique, thereby achieving the smooth and theoretical interconnection. Experimental results show that the proposed method achieves better dereverberation in severe reverberant environments than the conventional methods in terms of objective quality measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-81"
  },
  "erkelens11_interspeech": {
   "authors": [
    [
     "Jan S.",
     "Erkelens"
    ],
    [
     "Richard",
     "Heusdens"
    ]
   ],
   "title": "A statistical room impulse response model with frequency dependent reverberation time for single-microphone late reverberation suppression",
   "original": "i11_0201",
   "page_count": 4,
   "order": 84,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "Single-channel late reverberation suppression algorithms need estimates of the late reverberance spectral variance (LRSV) in order to suppress the late reverberance. Often the LRSV estimators are derived from a statistical room impulse response (RIR) model. Usually the late reverberation is modeled as a white Gaussian noise sequence with exponentially decaying variance. The whiteness assumption means that the same decay constant is assumed for all frequencies. Since there is generally more absorption of sound energy with increasing frequency, there is a need for RIR models that take this into account. We propose a new statistical time-varying RIR model that consists of a sum of decaying cosine functions with random phases, with a frequency dependent decay constant. We show that the resulting LRSV estimators have the same form as existing ones, but with an inherent frequency dependency of the decay constant. Experiments with real measured RIRs, however, indicate that, for the purpose of reverberation suppression, using a frequency independent decay constant is often sufficiently good. A common assumption in the derivation of LRSV estimators is that the direct signal and early reflections are uncorrelated with the late reverberation. We verify this assumption experimentally on measured RIRs and conclude that it is accurate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-82"
  },
  "zheng11_interspeech": {
   "authors": [
    [
     "Chenxi",
     "Zheng"
    ],
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "An assessment of the improvement potential of time-frequency masking for speech dereverberation",
   "original": "i11_0205",
   "page_count": 4,
   "order": 85,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "The effect of ideal time-frequency masking (ITFM) on the intelligibility of reverberated speech is tested using objective measurement, namely STI and PESQ scores. The best choice of ITFM threshold is determined for a range of reverberation times (RTs). Four existing dereverberation algorithms are also assessed. Objective test results and informal subjective listening show that IFTM provides great intelligibility improvement for all RTs and outperforms the existing dereverberation algorithms, one of which assumes perfect knowledge of the room impulse response. While ITFM provides only a best possible performance bound, our results demonstrate the potential improvement that could be obtained using time-frequency masking for speech dereverberation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-83"
  },
  "prego11_interspeech": {
   "authors": [
    [
     "Thiago de M.",
     "Prego"
    ],
    [
     "Amaro A. de",
     "Lima"
    ],
    [
     "Sergio L.",
     "Netto"
    ]
   ],
   "title": "Perceptual improvement of a two-stage algorithm for speech dereverberation",
   "original": "i11_0209",
   "page_count": 4,
   "order": 86,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "This paper presents three effective proposals for a two-stage algorithm for one-microphone reverberant speech enhancement. The original algorithm is divided into two blocks: one that deals with the coloration effect, due to the early reflections, and the other for reducing the long-term reverberation. The proposed modifications consider changing the linear-prediction model order, the adaptation stepsize and stop criterion for the first algorithm stage. All the modifications are evaluated by a perceptual-quality measure specific for the speech-reverberation context. Experimental results for a 200-signal database show that the proposed improvements yielded an increase of 12% in perceptual measure and a reduction of about 96% in the computation cost when compared to the original framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-84"
  },
  "hadir11_interspeech": {
   "authors": [
    [
     "Najib",
     "Hadir"
    ],
    [
     "Friedrich",
     "Faubel"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "A model-based spectral envelope wiener filter for perceptually motivated speech enhancement",
   "original": "i11_0213",
   "page_count": 4,
   "order": 87,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "In this work, we present a model-based Wiener filter whose frequency response is optimized in the dimensionally reduced log-Mel domain. That is achieved by making use of a reasonably novel speech feature enhancement approach that has originally been developed in the area of speech recognition. Its combination with Wiener filtering is motivated by the fact that signal reconstruction from log-Mel features sounds very unnatural. Hence, we correct only the spectral envelope and preserve the fine spectral structure of the noisy signal. Experiments on a Wall Street Journal corpus showed a relative improvement of up to 24% relative in PESQ and 45% relative in log spectral distance (LSD), compared to Ephraim and Mallah's log spectral amplitude estimator.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-85"
  },
  "marinhurtado11_interspeech": {
   "authors": [
    [
     "Jorge I.",
     "Marin-Hurtado"
    ],
    [
     "Devangi N.",
     "Parikh"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Binaural noise-reduction method based on blind source separation and perceptual post processing",
   "original": "i11_0217",
   "page_count": 4,
   "order": 88,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "Binaural hearing aids include a wireless link to exchange the signals received at each side, allowing the implementation of more efficient noise-reduction algorithms for hostile environments such as babble noise. Although several binaural noise-reduction techniques have been proposed in the literature, only a few of them preserve localization cues of the target and interfering signals simultaneously without degrading the SNR improvement. This paper proposes a novel binaural noise-reduction method based on blind source separation (BSS) and a perceptual post-processing technique. Objective and subjective tests under four different scenarios were performed. The method showed good output sound quality, high SNR improvement at very low input SNR conditions, and preservation of localization cues for the signal and noise - outperforming both an existing BSS-based method and a multichannel Wiener filter (MWF).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-86"
  },
  "ng11_interspeech": {
   "authors": [
    [
     "Tim",
     "Ng"
    ],
    [
     "Bing",
     "Zhang"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Long",
     "Nguyen"
    ]
   ],
   "title": "Region dependent transform on MLP features for speech recognition",
   "original": "i11_0221",
   "page_count": 4,
   "order": 89,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "In this work, Region Dependent Transform (RDT) is used as a feature extraction process to combine the traditional short-term acoustic features with the features derived from Multi-Layer Perceptrons (MLP) which is trained from the long-term features. When compared to the conventional feature augmentation approach, substantial improvement is obtained. Moreover, an improved RDT training procedure in which speaker dependent transforms are take into account is proposed for feature combination in the Speaker Adaptive Training. By incorporating the higher dimensional features output from the layer prior to the bottleneck layer into our Speech-to-Text (STT) system using RDT, significant improvement is achieved as compared to using the conventional bottleneck features. In summary, by using the features derived from MLP with RDT, 8.2% to 11.4% relative reduction in Character Error Rate is achieved for our Mandarin STT systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-87"
  },
  "heckmann11_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Claudius",
     "Gläser"
    ]
   ],
   "title": "Discriminant sub-space projection of spectro-temporal speech features based on maximizing mutual information",
   "original": "i11_0225",
   "page_count": 4,
   "order": 90,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "We previously developed noise robust Hierarchical Spectro- Temporal (Hist) speech features. The learning of the features was performed in an unsupervised way with unlabeled speech data. In a final stage we deployed Principal Component Analysis (PCA) to reduce the feature dimensions and to diagonalize them. In this paper we investigate if a discriminant projection can further increase the performance. We maximize the mutual information between the features and the phoneme categories using a procedure known as Maximizing Renyi's Mutual Information (MRMI) and also compare it to Linear Discriminant Analysis (LDA). Based on recognition tests in clean and in noise, i.e. in matching and mismatching conditions, we show that the discriminant projections increases recognition scores compared to PCA in matching conditions. However, this improvement does not transfer to the mismatching, i.e. noisy, conditions. We discuss measures to alleviate this problem. Overall MRMI performs better than LDA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-88"
  },
  "fukuda11_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Osamu",
     "Ichikawa"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Combining feature space discriminative training with long-term spectro-temporal features for noise-robust speech recognition",
   "original": "i11_0229",
   "page_count": 4,
   "order": 91,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "Discriminative training of feature space using maximum mutual information (fMMI) objective function has been shown to yield remarkable accuracy improvements. For noisy environments, fMMI can be regarded as an effective noise compensation algorithm and can play a significant role for noise robustness. Feature space speaker adaptation techniques such as feature space maximum likelihood linear regression (fMLLR) are also well known, suitable for mismatched test data. These feature space transform algorithms are essential for modern speech recognition but still need further improvement against low SNR conditions. In contrast, longterm spectro-temporal information has also received attention to support traditional short-term features. We previously proposed long-term temporal features to improve ASR accuracy for low SNR speech. In this paper, we show that long-term temporal features can be combined with fMMI to build more discriminative models for noisy speech and the proposed method performed favorably at low SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-89"
  },
  "chopra11_interspeech": {
   "authors": [
    [
     "Sumit",
     "Chopra"
    ],
    [
     "Patrick",
     "Haffner"
    ],
    [
     "Dimitrios",
     "Dimitriadis"
    ]
   ],
   "title": "Combining frame and segment level processing via temporal pooling for phonetic classification",
   "original": "i11_0233",
   "page_count": 4,
   "order": 92,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "We propose a simple, yet novel, multi-layer model for the problem of phonetic classification. Our model combines a frame level transformation of the acoustic signal with a segment level phone classification. Our key contribution is the study of new temporal pooling strategies that interface these two levels, determining how frame scores are converted into segment scores. On the TIMIT benchmark, we match the best performance obtained using a single classifier. Diversity in pooling strategies is further used to generate candidate classifiers with complementary performance characteristics, which perform even better as an ensemble. Without the use of any phonetic knowledge, our ensemble model achieves a 16.96% phone classification error. While our data-driven approach is exhaustive, the combinatorial inflation is limited to the smaller segmental half of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-90"
  },
  "yu11b_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Michael L.",
     "Seltzer"
    ]
   ],
   "title": "Improved bottleneck features using pretrained deep neural networks",
   "original": "i11_0237",
   "page_count": 4,
   "order": 93,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems. Conventionally, bottleneck features are extracted from a multilayer perceptron (MLP) trained to predict context-independent monophone states. The MLP typically has three hidden layers and is trained using the backpropagation algorithm. In this paper, we propose two improvements to the training of bottleneck features motivated by recent advances in the use of deep neural networks (DNNs) for speech recognition. First, we show how the use of unsupervised pretraining of a DNN enhances the network's discriminative power and improves the bottleneck features it generates. Second, we show that a neural network trained to predict context-dependent senone targets produces better bottleneck features than one trained to predict monophone states. Bottleneck features trained using the proposed methods produced a 16% relative reduction in sentence error rate over conventional bottleneck features on a large vocabulary business search task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-91"
  },
  "liao11_interspeech": {
   "authors": [
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Chia-Hsing",
     "Lin"
    ],
    [
     "We-Der",
     "Fang"
    ]
   ],
   "title": "Minimum classification error based spectro-temporal feature extraction for robust audio classification",
   "original": "i11_0241",
   "page_count": 4,
   "order": 94,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "Mel-frequency cepstral coefficients (MFCCs) are the most popular features for automatic audio classification (AAC). However, MFCCs are often not robust in adverse environment. In this paper, a minimum classification error (MCE)-based method is proposed to extract new and robust spectro-temporal features as alternatives to MFCCs. The robustness of the proposed new features is evaluated on noisy non-speech sound of RWCP Sound Scene Database in Real Acoustic Environment database with Aurora 2 multi-condition training task-like settings. Experimental results show the proposed new features achieved the lowest average recognition error rate of 3.17% which is much better than state-of-the-art MFCCs plus mean subtraction, variance normalization and ARMA filtering (MFCC+MVA, 4.31%), Gabor filters with principle component analysis (Gabor+PCA, 4.43%) and linear discriminant analysis (LDA, 4.20%) features. We thus confirm the robustness of the proposed spectro-temporal feature extraction approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-92"
  },
  "grezl11_interspeech": {
   "authors": [
    [
     "František",
     "Grézl"
    ],
    [
     "Martin",
     "Karafiát"
    ]
   ],
   "title": "Integrating recent MLP feature extraction techniques into TRAP architecture",
   "original": "i11_1229",
   "page_count": 4,
   "order": 95,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "This paper is focused on the incorporation of recent techniques for multi-layer perceptron (MLP) based feature extraction in Temporal Pattern (TRAP) and Hidden Activation TRAP (HATS) feature extraction scheme. The TRAP scheme has been origin of various MLP-based features some of which are now indivisible part of state-of-the-art LVCSR systems. The modifications which brought most improvement - sub-phoneme targets and Bottle-Neck technique - are introduced into original TRAP scheme. Introduction of sub-phoneme targets uncovered the hidden danger of having too many classes in TRAP/HATS scheme. On the other hand, Bottle-Neck technique improved the TRAP/HATS scheme so its competitive with other approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-93"
  },
  "wollmer11b_interspeech": {
   "authors": [
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Feature frame stacking in RNN-based tandem ASR systems - learned vs. predefined context",
   "original": "i11_1233",
   "page_count": 4,
   "order": 96,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "As phoneme recognition is known to profit from techniques that consider contextual information, neural networks applied in Tandem automatic speech recognition (ASR) systems usually employ some form of context modeling. While approaches based on multi-layer perceptrons or recurrent neural networks (RNN) are able to model a predefined amount of context by simultaneously processing a stacked sequence of successive feature vectors, bidirectional Long Short-Term Memory (BLSTM) networks were shown to be well-suited for incorporating a self-learned amount of context for phoneme prediction. In this paper, we evaluate combinations of BLSTM modeling and frame stacking to determine the most efficient method for exploiting context in RNN-based Tandem systems. Applying the COSINE corpus and our recently introduced multi-stream BLSTM-HMM decoder, we provide empirical evidence for the intuition that BLSTM networks redundantize frame stacking while RNNs profit from predefined feature-level context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-94"
  },
  "plahl11_interspeech": {
   "authors": [
    [
     "Christian",
     "Plahl"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improved acoustic feature combination for LVCSR by neural networks",
   "original": "i11_1237",
   "page_count": 4,
   "order": 97,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "This paper investigates the combination of different acoustic features. Several methods to combine these features such as concatenation or LDA are well known. Even though LDA improves the system, feature combination by LDA has been shown to be suboptimal. We introduce a new method based on neural networks. The posterior estimates derived from the NN lead to a significant improvement and achieve a 6% relative better word error rate (WER). Results are also compared to system combination. While system combination has been reported to outperform all other combination techniques, in this work the proposed NN-based combination outperforms system combination. We achieve a 2% relative better WER, resulting in an improvement of 7% relative to the baseline system.\n",
    "In addition to giving better recognition performance w.r.t. WER, NN-based combination reduces both, training and testing complexity. Overall, we use a single set of acoustic models, together with the training of the NN.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-95"
  },
  "pinto11_interspeech": {
   "authors": [
    [
     "Joel",
     "Pinto"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Hierarchical tandem features for ASR in Mandarin",
   "original": "i11_1241",
   "page_count": 4,
   "order": 98,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "We apply multilayer perceptron (MLP) based hierarchical Tandem features to large vocabulary continuous speech recognition in Mandarin. Hierarchical Tandem features are estimated using a cascade of two MLP classifiers which are trained independently. The first classifier is trained on perceptual linear predictive coefficients with a 90 ms temporal context. The second classifier is trained using the phonetic class conditional probabilities estimated by the first MLP, but with a relatively longer temporal context of about 150 ms. Experiments on the Mandarin DARPA GALE eval06 dataset show significant reduction (7.6% relative) in character error rates by using hierarchical Tandem features over conventional Tandem features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-96"
  },
  "valente11_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Wen",
     "Wang"
    ]
   ],
   "title": "Analysis and comparison of recent MLP features for LVCSR systems",
   "original": "i11_1245",
   "page_count": 4,
   "order": 99,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "MLP based front-ends have evolved in different ways in recent years beyond the seminal TANDEM-PLP features. This paper aims at providing a fair comparison of these recent progresses including the use of different long/short temporal inputs (PLP,MRASTA,wLP-TRAPS,DCT-TRAPS) and the use of complex architectures (bottleneck, hierarchy, multistream) that go beyond the conventional three layer MLP. Furthermore, the paper identifies which of these actually provide advantages over the conventional TANDEM-PLP. The investigation is carried on an LVCSR task for recognition of Mandarin Broadcast speech and results are analyzed in terms of Character Error Rate and phonetic confusions. Results reveal that as stand alone features, multistream front-ends can outperform by 10% conventional MFCC while TANDEM-PLP only improve by 1%. On the other hand, when used in concatenation with MFCC features, hierarchical/bottleneck front-ends reduce the character error rate by +18% relative compared to +14% relative from TANDEM-PLP. The various input long-term representations recently developed provide comparable performances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-97"
  },
  "lee11b_interspeech": {
   "authors": [
    [
     "Jaehyung",
     "Lee"
    ],
    [
     "Soo-Young",
     "Lee"
    ]
   ],
   "title": "Deep learning of speech features for improved phonetic recognition",
   "original": "i11_1249",
   "page_count": 4,
   "order": 100,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "Recently, a remarkable performance result of 23.0% Phone Error Rate (PER) on the TIMIT core test set was reported by applying Deep Belief Network (DBN) on phonetic recognition [1]. Despite the good performance reported, there is still substantial room for improvement in the reported design in order to achieve optimal results. In this letter, we present an improved but simple architecture for phonetic recognition which uses logMel spectrum directly instead of MelFrequency Cepstral Coefficient (MFCC), and combines Deep Learning with conventional BaumWelch reestimation for subphoneme alignment. Experiments performed on TIMIT speech corpus show that the proposed method outperforms most of the conventional methods, yielding 21.4% PER on the complete test set of TIMIT and 22.1% on the core test set.\n",
    "",
    "",
    "A.R. Mohamed, G. Dahl, and G. Hinton, Deep belief networks for phone recognition, in NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec. 2009.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-98"
  },
  "huang11_interspeech": {
   "authors": [
    [
     "Heyun",
     "Huang"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Globality-locality consistent discriminant analysis for phone classification",
   "original": "i11_1253",
   "page_count": 4,
   "order": 101,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "Concatenating sequences of feature vectors helps to capture essential information about articulatory dynamics, at the cost of increasing the number of dimensions in the feature space, which may be characterized by the presence of manifolds. Existing supervised dimensionality reduction methods such as Linear Discriminant Analysis may destroy part of that manifold structure. In this paper, we propose a novel supervised dimensionality reduction algorithm, called Globality-Locality Consistent Discriminant Analysis (GLCDA), which aims to preserve global and local discriminant information simultaneously. Because it allows finding the optimal trade-off between global and local structure of data sets, GLCDA can provide a more faithful compact representation of high-dimensional observations than entirely global approaches or heuristic approaches aimed to preserve local information. Experimental results on the TIMIT phone classification task show the effectiveness of the proposed algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-99"
  },
  "boril11_interspeech": {
   "authors": [
    [
     "Hynek",
     "Bořil"
    ],
    [
     "František",
     "Grézl"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Front-end compensation methods for LVCSR under lombard effect",
   "original": "i11_1257",
   "page_count": 4,
   "order": 102,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "This study analyzes the impact of noisy background variations and Lombard effect (LE) on large vocabulary continuous speech recognition (LVCSR). Robustness of several front-end feature extraction strategies combined with state-of-the-art feature distribution normalizations is tested on neutral and Lombard speech from the UT-Scope database presented in two types of background noise at various levels of SNR. An extension of a bottleneck (BN) front-end utilizing normalization of both critical band energies (CRBE) and BN outputs is proposed and shown to provide a competitive performance compared to the best MFCC-based system. A novel MFCC-based BN front-end is introduced and shown to outperform all other systems in all conditions considered (average 4.1% absolute WER reduction over the second best system). Additionally, two phenomena are observed: (i) combination of cepstral mean subtraction and recently established RASTALP filtering significantly reduces transient effects of RASTA band-pass filtering and increases ASR robustness to noise and LE; (ii) histogram equalization may benefit from utilizing reference distributions derived from pre-normalized rather than raw training features, and also from adopting distributions from different front-ends.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-100"
  },
  "lee11c_interspeech": {
   "authors": [
    [
     "Jung-Won",
     "Lee"
    ],
    [
     "Jeung-Yoon",
     "Choi"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Classification of fricatives using feature extrapolation of acoustic-phonetic features in telephone speech",
   "original": "i11_1261",
   "page_count": 4,
   "order": 103,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "This paper proposes a classification module for fricative consonants in telephone speech using an acoustic-phonetic feature extrapolation technique. In channel-deteriorated telephone speech, acoustic cues of fricative consonants are expected to be degraded or missing due to limited bandwidth. This paper applies an extrapolation technique to acoustic-phonetic features based on Gaussian mixture models, which uses a statistical learning of the correspondence between acoustic-phonetic features of wideband speech and the spectral characteristics of telephone bandwidth speech. Experimental results with NTIMIT database verify that feature extrapolation improves the performance of fricative classification module for all unvoiced fricatives by around 10% (relative error) compared to the performance obtained by only acoustic-phonetic features extracted from the narrowband signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-101"
  },
  "keronen11_interspeech": {
   "authors": [
    [
     "Sami",
     "Keronen"
    ],
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Noise robust feature extraction based on extended weighted linear prediction in LVCSR",
   "original": "i11_1265",
   "page_count": 4,
   "order": 104,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "This paper introduces extended weighted linear prediction (XLP) to noise robust short-time spectrum analysis in the feature extraction process of a speech recognition system. XLP is a generalization of standard linear prediction (LP) and temporally weighted linear prediction (WLP) which have already been applied to noise robust speech recognition with good results. With XLP, higher controllability to the temporal weighting of different parts of the noisy speech is gained by taking the lags of the signal into account in prediction. Here, the performance of XLP is put up against WLP and conventional spectrum analysis methods FFT and LP on a large vocabulary continuous speech recognition (LVCSR) scheme using real world noisy data containing additive and convolutive noise. The results show improvements over the reference methods in several cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-102"
  },
  "meyer11_interspeech": {
   "authors": [
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Suman V.",
     "Ravuri"
    ],
    [
     "Marc René",
     "Schädler"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Comparing different flavors of spectro-temporal features for ASR",
   "original": "i11_1269",
   "page_count": 4,
   "order": 105,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "In the last decade, several studies have shown that the robustness of ASR systems can be increased when 2D Gabor filters are used to extract specific modulation frequencies from the input pattern. This paper analyzes important design parameters for spectro-temporal features based on a Gabor filter bank: We perform experiments with filters that exhibit different phase sensitivity. Further, we analyze if non-linear weighting with a multi-layer perceptron (MLP) and a subsequent concatenation with mel-frequency cepstral coefficients (MFCCs) has beneficial effects. For the Aurora2 noisy digit recognition task, the use of phase sensitive filters improved the MFCC baseline, whereas using filters that neglect phase information did not. While MLP processing alone did not have a large effect on the overall performance, the best results were obtained for MLP-processed phase sensitive filters and added MFCCs, with relative error reductions of over 40% for both noisy and clean training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-103"
  },
  "variani11_interspeech": {
   "authors": [
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "VTLN in the MFCC domain: band-limited versus local interpolation",
   "original": "i11_1273",
   "page_count": 4,
   "order": 106,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "We propose a new easy-to-implement method to compute a Linear Transform (LT) to perform Vocal Tract Length Normalization (VTLN) on truncated Mel Frequency Cepstral Coefficients (MFCCs) normally used in distributed speech recognition. The method is based on a Local Interpolation which is independent of the Mel filter design. Local Interpolation (LILT) VTLN is theoretically and experimentally compared to a global scheme based on band-limited interpolation (BLI-VTLN) and the conventional frequency warping scheme (FFT-VTLN). Investigating the interoperability of these methods shows that the performance of LILT-VTLN is on par with FFT-VTLN and BLI-VTLN. The statistical significance test also shows that there are no significant differences between FFT-VTLN, LILT-VTLN, and BLI-VTLN, even if the models and front ends do not match.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-104"
  },
  "nemala11_interspeech": {
   "authors": [
    [
     "Sridhar Krishna",
     "Nemala"
    ],
    [
     "Kailash",
     "Patil"
    ],
    [
     "Mounya",
     "Elhilali"
    ]
   ],
   "title": "Multistream bandpass modulation features for robust speech recognition",
   "original": "i11_1277",
   "page_count": 4,
   "order": 107,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "Current understanding of speech processing in the brain suggests dual streams of processing of temporal and spectral information, whereby slow vs. fast modulations are analyzed along parallel paths that encode various scales of information in speech signals. This unique way for the biology to analyze the multiplicity of information in speech signals along parallel paths can bare great lessons for feature extraction front-ends in speech processing systems, particularly for dealing with extrinsic degradations and unseen noise distortions. Here, we propose a multistream approach to feature analysis for robust speaker-independent phoneme recognition in presence of nonstationary background noises. The scheme presented here centers around a multi-path bandpass modulation analysis of speech sounds with each stream covering an entire range of temporal and spectral modulations. By performing bandpass operations of slow vs. fast information along the spectral and temporal dimensions, the proposed scheme avoids the classic feature explosion problem of previous multistream approaches while maintaining the advantage of parallelism and localized feature analysis. The proposed architecture results in substantial improvements over standard baseline features and two state-of-the-art noise robust feature schemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-105"
  },
  "marino11_interspeech": {
   "authors": [
    [
     "Davide",
     "Marino"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "An analysis of automatic speech recognition with multiple microphones",
   "original": "i11_1281",
   "page_count": 4,
   "order": 108,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "Automatic speech recognition in real world situations often requires the use of microphones distant from speaker's mouth. One or several microphones are placed in the surroundings to capture many versions of the original signal. Recognition with a single far field microphone yields considerably poorer performance than with person-mounted devices (headset, lapel), with the main causes being reverberation and noise. Acoustic beam-forming techniques allow significant improvements over the use of a single microphone, although the overall performance still remains well above the close-talking results. In this paper we investigate the use of beam-forming in the context of speaker movement, together with commonly used adaptation techniques and compare against a naive multi-stream approach. We show that even such a simple approach can yield equivalent results to beam-forming, allowing for far more powerful integration of multiple microphone sources in ASR systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-106"
  },
  "kim11_interspeech": {
   "authors": [
    [
     "Yoon-Chul",
     "Kim"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Krishna S.",
     "Nayak"
    ]
   ],
   "title": "Visualization of vocal tract shape using interleaved real-time MRI of multiple scan planes",
   "original": "i11_0269",
   "page_count": 4,
   "order": 109,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "Conventional real-time magnetic resonance imaging (RT-MRI) of the upper airway typically acquires information about the vocal tract from a single midsagittal scan plane. This provides insights into the dynamics of all articulators, but does not allow for visualization of several important features in vocal tract shaping, such as grooving/doming of the tongue, asymmetries in tongue shape, and lateral shaping of the pharyngeal airway. In this paper, we present an approach to RT-MRI of multiple scan planes of interest using time-interleaved acquisition, in which temporal resolution is compromised for greater spatial coverage. We demonstrate simultaneous visualization of vocal tract dynamics from midsagittal, coronal, and axial scan planes in the articulation of English fricatives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-107"
  },
  "winkler11_interspeech": {
   "authors": [
    [
     "Ralf",
     "Winkler"
    ],
    [
     "Susanne",
     "Fuchs"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Mark",
     "Tiede"
    ]
   ],
   "title": "Biomechanical tongue models: an approach to studying inter-speaker variability",
   "original": "i11_0273",
   "page_count": 4,
   "order": 110,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "Speakers of a given language vary with respect to their acoustics, articulation, and motor commands. This variation is driven by a variety of influences, such as emotional states, communicative interaction, and individual properties of the vocal tract. In this work we focus on the latter. First, we build speaker-specific biomechanical tongue models. Second, we discuss the impact of the relative position of the bending in the vocal tract on the basis of extensive simulations with two different models. We focus on /i,a,u/ by defining target regions in the acoustic space, and discuss the corresponding speaker-specific articulatory and motor command variability observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-108"
  },
  "wang11c_interspeech": {
   "authors": [
    [
     "Jun",
     "Wang"
    ],
    [
     "Jordan R.",
     "Green"
    ],
    [
     "Ashok",
     "Samal"
    ],
    [
     "David B.",
     "Marx"
    ]
   ],
   "title": "Quantifying articulatory distinctiveness of vowels",
   "original": "i11_0277",
   "page_count": 4,
   "order": 111,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "The articulatory distinctiveness among vowels has been frequently characterized descriptively based on tongue height and front-back position; however, very few empirical methods have been proposed to characterize vowels based on time-varying articulatory characteristics. Such information is not only needed to improve knowledge about the articulation of vowels but also to determine the contribution of articulatory imprecision to poor speech intelligibility. In this paper, a novel statistical shape analysis was used to derive a vowel space that depicted the quantified articulatory distinctiveness among vowels based on tongue and lip movements. The effectiveness of the approach was supported by vowel classification accuracy of up to 91.7%. The theoretical relevance and clinical implication of the derived vowel space were discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-109"
  },
  "proctor11_interspeech": {
   "authors": [
    [
     "Michael",
     "Proctor"
    ],
    [
     "Adam",
     "Lammert"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Christina",
     "Hagedorn"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Direct estimation of articulatory kinematics from real-time magnetic resonance image sequences",
   "original": "i11_0281",
   "page_count": 4,
   "order": 112,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "A method of rapid, automatic extraction of consonantal articulatory trajectories from real-time magnetic resonance image sequences is described. Constriction location targets are estimated by identifying regions of maximally-dynamic correlated pixel activity along the palate, the alveolar ridge, and at the lips. Tissue movement into and out of the constriction location is estimated by calculating the change in mean pixel intensity in a circle located at the center of the region of interest. Closure and release gesture timings are estimated from landmarks in the velocity profile derived from the smoothed intensity function. We demonstrate the utility of the technique in the analysis of Italian intervocalic consonant production.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-110"
  },
  "birkholz11_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Christiane",
     "Neuschaefer-Rube"
    ]
   ],
   "title": "Combined optical distance sensing and electropalatography to measure articulation",
   "original": "i11_0285",
   "page_count": 4,
   "order": 113,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "We present the first prototype of a new optoelectronic instrument for the combined real-time measurement of the tongue contour in the mid-sagittal plane, the contact pattern between the tongue and the palate, and the position of the lips. The instrument consists of a thin acrylic pseudopalate with embedded contact sensors, as for electropalatography, and optical distance sensors to measure tongue-palate distances, as for glossometry. One additional distance sensor is located at the anterior side of the upper incisors to register the degree of opening and protrusion of the lips. Together, the sensors provide complementary information about the articulation of vowels and consonants, which was verified in initial experiments. The instrument offers new perspectives for the study of normal and disordered speech production, as well as for silent speech interfaces and speech prostheses for laryngectomees.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-111"
  },
  "promon11_interspeech": {
   "authors": [
    [
     "Santitham",
     "Prom-on"
    ],
    [
     "Yi",
     "Xu"
    ],
    [
     "Fang",
     "Liu"
    ]
   ],
   "title": "Simulating post-l F0 bouncing by modeling articulatory dynamics",
   "original": "i11_0289",
   "page_count": 4,
   "order": 114,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "Post-L F0 bouncing (post-L bouncing for short) is a prosodic phenomenon whereby F0 is temporarily raised following a very low pitch. The phenomenon is quite robust, but is not widely known, and it has never been computationally modeled. This paper presents the results of our simulation of the phenomenon by modeling articulatory dynamics. Using the quantitative Target Approximation (qTA) model, we were able to simulate the F0 rise after the Mandarin L tone by adding an acceleration adjustment to the initial state of the first post-L Neutral tone. Furthermore, a linear relationship was found between the added acceleration and the amount of F0 lowering in the L tone. We interpreted the results as evidence that post-L bouncing is directly related to the articulatory mechanism of producing a very low pitch.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-112"
  },
  "geiger11_interspeech": {
   "authors": [
    [
     "Jürgen T.",
     "Geiger"
    ],
    [
     "Mohamed Anouar",
     "Lakhal"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Learning new acoustic events in an HMM-based system using MAP adaptation",
   "original": "i11_0293",
   "page_count": 4,
   "order": 115,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "In this paper, we present a system for the recognition of acoustic events suited for a robotic application. HMMs are used to model different acoustic event classes. We are especially looking at the open-set case, where a class of acoustic events occurs that was not included in the training phase. It is evaluated how newly occuring classes can be learnt using MAP adaptation or conventional training methods. A small database of acoustic events was recorded with a robotic platform to perform the experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-113"
  },
  "leng11_interspeech": {
   "authors": [
    [
     "Yi Ren",
     "Leng"
    ],
    [
     "Huy Dat",
     "Tran"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Alternative frequency scale cepstral coefficient for robust sound event recognition",
   "original": "i11_0297",
   "page_count": 4,
   "order": 116,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "There are two issues when applying MFCC for sound event recognition: 1) sound events have a broader spectral range than speech thus the log-frequency scale is less informative; 2) low frequency noise is more prevalent thus the log-frequency scale captures more noise. To address these issues, we study two alternative frequency scales and show that they outperform MFCCs for sound event recognition under mismatch conditions using Support Vector Machines (SVMs) without the need for complex algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-114"
  },
  "ito11_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Akihito",
     "Aiba"
    ],
    [
     "Masashi",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Evaluation of abnormal sound detection using multi-stage GMM in various environments",
   "original": "i11_0301",
   "page_count": 4,
   "order": 117,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "We have developed a method to automatically detect incidents by detecting abnormal sound events from audio signals recorded in real environments. The proposed method uses the multi-stage Gaussian Mixture Model (GMM), which learns rare sounds using multiple GMMs. In this work, we investigated the relationship between sound environment and detection performance, and found that the performance deteriorates in noisy environments, and that the performance largely depends on the SN ratio of the abnormal sounds. Next, we investigated methods for determining hyperparameters of the multi-stage GMM, which involves intermediate thresholds, numbers of mixtures of GMMs and the detection threshold. The experimental results showed that the combination of percentile-based threshold determination and Bayesian information criterion (BIC)-based mixture determination was most effective. However, when using the automatically-determined parameters, the detection performance deteriorated by up to 20%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-115"
  },
  "schmalenstroeer11_interspeech": {
   "authors": [
    [
     "Joerg",
     "Schmalenstroeer"
    ],
    [
     "Markus",
     "Bartek"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Unsupervised learning of acoustic events using dynamic time warping and hierarchical k-means++ clustering",
   "original": "i11_0305",
   "page_count": 4,
   "order": 118,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "In this paper we propose to jointly consider Segmental Dynamic Time Warping and distance clustering for the unsupervised learning of acoustic events. As a result, the computational complexity increases only linearly with the database size compared to a quadratic increase in a sequential setup, where all pairwise SDTW distances between segments are computed prior to clustering. Further, we discuss options for seed value selection for clustering and show that drawing seeds with a probability proportional to the distance from the already drawn seeds, known as K-means++ clustering, results in a significantly higher probability of finding representatives of each of the underlying classes, compared to the commonly used draws from a uniform distribution. Experiments are performed on an acoustic event classification and an isolated digit recognition task, where on the latter the final word accuracy approaches that of supervised training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-116"
  },
  "mejianavarrete11_interspeech": {
   "authors": [
    [
     "David",
     "Mejía-Navarrete"
    ],
    [
     "Ascensión",
     "Gallardo-Antolín"
    ],
    [
     "Carmen",
     "Peláez-Moreno"
    ],
    [
     "Francisco J.",
     "Valverde-Albacete"
    ]
   ],
   "title": "Feature extraction assessment for an acoustic-event classification task using the entropy triangle",
   "original": "i11_0309",
   "page_count": 4,
   "order": 119,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "We assess the behaviour of 5 different feature extraction methods for an acoustic event classification task.built using the same SVM underlying technology - by means of two different techniques: accuracy and the entropy triangle. The entropy triangle is able to find a classifier instance whose relatively high accuracy stems from an attempt to specialize in some classes to the detriment of the overall behaviour. On all other cases, fair classifiers, accuracy and entropy triangle agree.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-117"
  },
  "natarajan11_interspeech": {
   "authors": [
    [
     "Pradeep",
     "Natarajan"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Vasant",
     "Manohar"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Premkumar",
     "Natarajan"
    ]
   ],
   "title": "Unsupervised audio analysis for categorizing heterogeneous consumer domain videos",
   "original": "i11_0313",
   "page_count": 4,
   "order": 120,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "The ever increasing volume of consumer domain videos on the Internet has led to a surge in interest in automatically analyzing such content. The audio signal in these videos contains salient information, but applying current automatic speech recognition (ASR) techniques is not viable due to high variability, noise and multilingual content. We present two unsupervised techniques which do not rely on ASR to address these challenges. The first method involves learning an unsupervised codebook by clustering audio features, and the second involves directly matching low-level features using the pyramid match kernel (PMK). Experimental results on a .200 hour audio corpus downloaded from YouTube show that both our approaches significantly outperform the traditional approach of first segmenting the audio stream to a set of mid-level classes (e.g. speech, non-speech, music, silence) and using the duration statistics of these classes to train high-level classifiers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-118"
  },
  "sridhar11_interspeech": {
   "authors": [
    [
     "Vivek Kumar Rangarajan",
     "Sridhar"
    ],
    [
     "Ann",
     "Syrdal"
    ],
    [
     "Alistair D.",
     "Conkie"
    ],
    [
     "Srinivas",
     "Bangalore"
    ]
   ],
   "title": "Enriching text-to-speech synthesis using automatic dialog act tags",
   "original": "i11_0317",
   "page_count": 4,
   "order": 121,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "We present an approach for enriching dialog based text-to-speech (TTS) synthesis systems by explicitly controlling the expressiveness through the use of dialog act tags. The dialog act tags in our framework are automatically obtained by training a maximum entropy classifier on the Switchboard-DAMSL data set, unrelated to the TTS database. We compare the voice quality produced by exploiting automatic dialog act tags with that using human annotations of dialog acts, and with two forms of reference databases. Even though the inventory of tags is different for the automatic tagger and human annotation, exploiting either form of dialog markup generates better voice quality in comparison with the reference voices in subjective evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-119"
  },
  "latacz11_interspeech": {
   "authors": [
    [
     "Lukas",
     "Latacz"
    ],
    [
     "Wesley",
     "Mattheyses"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Joint target and join cost weight training for unit selection synthesis",
   "original": "i11_0321",
   "page_count": 4,
   "order": 122,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "One of the key challenges of optimizing a unit selection voice is obtaining suitable target and join cost weights. In this paper we investigate several strategies to train these weights automatically. Two training algorithms are tested, which are based on an acoustic distance that approximates human perception: a modified version of the well-known linear regression training and an iterative algorithm that tries to minimize a selection error. Since a single, global set of weights might not result in selecting all the time the best sequence of units, we investigate whether using multiple weight sets could improve the synthesis quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-120"
  },
  "windmann11_interspeech": {
   "authors": [
    [
     "Andreas",
     "Windmann"
    ],
    [
     "Igor",
     "Jauk"
    ],
    [
     "Fabio",
     "Tamburini"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Prominence-based prosody prediction for unit selection speech synthesis",
   "original": "i11_0325",
   "page_count": 4,
   "order": 123,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "This paper describes the development and evaluation of a prosody prediction module for unit selection speech synthesis that is based on the notion of perceptual prominence. We outline the design principles of the module and describe its implementation in the Bonn Open Synthesis System (BOSS). Moreover, we report results of perception experiments that have been conducted in order to evaluate prominence prediction. The paper is concluded by a general discussion of the approach and a sketch of perspectives for further work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-121"
  },
  "pammi11_interspeech": {
   "authors": [
    [
     "Sathish",
     "Pammi"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "Evaluating the meaning of synthesized listener vocalizations",
   "original": "i11_0329",
   "page_count": 4,
   "order": 124,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "Spoken and multimodal dialogue systems start to use listener vocalizations for more natural interaction. In a unit selection framework, using a finite set of recorded listener vocalizations, synthesis quality is high but the acoustic variability is limited. As a result, many combinations of segmental form and intended meaning cannot be synthesized.\n",
    "This paper presents an algorithm in the unit selection domain for increasing the range of vocalizations that can be synthesized with a given set of recordings. We investigate whether the approach makes the synthesized vocalizations convey a meaning closer to the intended meaning, using a pairwise comparison perception test. The results partially confirm the hypothesis, indicating that in many cases, the algorithm makes available more appropriate alternatives to the available set of recorded listener vocalizations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-122"
  },
  "sainz11_interspeech": {
   "authors": [
    [
     "Iñaki",
     "Sainz"
    ],
    [
     "Daniel",
     "Erro"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inma",
     "Hernáez"
    ]
   ],
   "title": "A hybrid TTS approach for prosody and acoustic modules",
   "original": "i11_0333",
   "page_count": 4,
   "order": 125,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "Unit selection (US) TTSs generate quite natural speech but highly variable in quality. Statistical parametric (SP) systems offer far more consistent quality but reduced naturalness due to its vocoding nature. We present a hybrid approach (HA) that tries to improve the overall naturalness combining both synthesis methods. Contrary to other works, the fusion of methods is performed both in prosody and acoustic modules yielding a more robust prosody prediction and achieving greater naturalness. Objective and subjective experiments show the validity of our procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-123"
  },
  "sorin11_interspeech": {
   "authors": [
    [
     "Alexander",
     "Sorin"
    ],
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Vincent",
     "Pollet"
    ]
   ],
   "title": "Uniform speech parameterization for multi-form segment synthesis",
   "original": "i11_0337",
   "page_count": 4,
   "order": 126,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "In multi-form segment synthesis speech is constructed by sequencing speech segments of different nature: model segments, i.e. mathematical abstractions of speech and template segments, i.e. speech waveform fragments. These multi-form segments can have shared, layered or alternate speech parameterization schemes. This paper introduces an advanced uniform speech parameterization scheme for statistical model segments and waveform segments employed in our multi-form segment synthesis system. Mel-Regularized Cepstrum derived from amplitude and phase spectra forms its basic framework. Furthermore, a new adaptive enhancement technique for model segments is presented that reduces the perceived gap in quality and similarity between model and template segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-124"
  },
  "miyazaki11_interspeech": {
   "authors": [
    [
     "Ryoichi",
     "Miyazaki"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Theoretical analysis of musical noise and speech distortion in structure-generalized parametric blind spatial subtraction array",
   "original": "i11_0341",
   "page_count": 4,
   "order": 127,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "In this paper, we propose a structure-generalized parametric blind spatial subtraction array (BSSA), and the theoretical analysis of the amounts of musical noise and speech distortion is conducted via higher-order statistics. We theoretically prove a tradeoff between the amounts of musical noise and speech distortion in various BSSA structures. From the analysis and experimental evaluations, we reveal that the structure should be carefully selected according to the application, i.e., a channel-wise BSSA structure is recommended for listening but a normal BSSA is more suitable for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-125"
  },
  "tang11_interspeech": {
   "authors": [
    [
     "Yan",
     "Tang"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Subjective and objective evaluation of speech intelligibility enhancement under constant energy and duration constraints",
   "original": "i11_0345",
   "page_count": 4,
   "order": 128,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Speakers appear to adopt strategies to improve speech intelligibility for interlocutors in adverse acoustic conditions. Generated speech, whether synthetic, recorded or live, may also benefit from context-sensitive modifications in challenging situations. The current study measured the effect on intelligibility of six spectral and temporal modifications operating under global constraints of constant input-output energy and duration. Reallocation of energy from mid-frequency regions with high local SNR produced the largest intelligibility benefits, while other approaches such as pause insertion or maintenance of a constant segmental SNR actually led to a deterioration in intelligibility. Listener scores correlated only moderately well with recent objective intelligibility estimators, suggesting that further development of intelligibility models is required to improve predictions for modified speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-126"
  },
  "muraka11_interspeech": {
   "authors": [
    [
     "Nagarjuna Reddy",
     "Muraka"
    ],
    [
     "Chandra Sekhar",
     "Seelamantula"
    ]
   ],
   "title": "A risk-estimation-based comparison of mean square error and itakura-saito distortion measures for speech enhancement",
   "original": "i11_0349",
   "page_count": 4,
   "order": 129,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "The goal of speech enhancement algorithms is to provide an estimate of clean speech starting from noisy observations. In general, the estimate is obtained by minimizing a chosen distortion metric. The often-employed cost is the mean-square error (MSE), which results in a Wiener-filter solution. Since the ground truth is not available in practice, the practical utility of the optimal estimators is limited. Alternative, one can optimize an unbiased estimate of the MSE. This is the key idea behind Stein's unbiased risk estimation (SURE) principle. Within this framework, we derive SURE solutions for the MSE and Itakura-Saito (IS) distortion measures. We also propose parametric versions of the corresponding SURE estimators, which give additional flexibility in controlling the attenuation characteristics for maximum signal-to-noise-ratio (SNR) gain. We compare the performance of the two distortion measures in terms of attenuation profiles, average segmental SNR, global SNR, and spectrograms. We also include a comparison with the standard power spectral subtraction technique. The results show that the IS distortion consistently gives better performance gain in all respects. The perceived quality of the enhanced speech is also better in case of the IS metric.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-127"
  },
  "triki11_interspeech": {
   "authors": [
    [
     "Mahdi",
     "Triki"
    ]
   ],
   "title": "On noise tracking for noise floor estimation",
   "original": "i11_0353",
   "page_count": 4,
   "order": 130,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "Various speech enhancement techniques (e.g. noise suppression, dereverberation) rely on the knowledge of the statistics of the clean signal and the noise process. In practice, however, these statistics are not explicitly available, and the overall enhancement accuracy critically depends on the estimation quality of the unknown statistics. With this respect, subspace based approaches have shown to allow for reduced estimation delay and perform a good tracking vs. final misadjustment tradeoff [1,2]. For an accurate noise non-stationarity tracking, these schemes have the challenge to estimate the correlation matrix of the observed signal from a limited number of samples. In this paper, we investigate the effect of the covariance estimation artifacts on the noise PSD tracking. We show that the estimation downsides could be alleviated using an appropriate selection scheme.\n",
    "s R. C. Hendriks, J. Jensen and R. Heusdens, Noise Tracking using DFT Domain Subspace Decompositions, IEEE Trans. on Audio, Speech, and Language Processing, Mar. 2008. M. Triki and K. Janse, Minimum Subspace Noise Tracking for Noise Power Spectral Density Estimation, In Proc of ICASSP, Apr. 2009.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-128"
  },
  "milner11_interspeech": {
   "authors": [
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Maximum a posteriori estimation of noise from non-acoustic reference signals in very low signal-to-noise ratio environments",
   "original": "i11_0357",
   "page_count": 4,
   "order": 131,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "This paper examines whether non-acoustic noise reference signals can provide accurate estimates of noise at very low signal-to-noise ratios (SNRs) where conventional estimation methods are less effective. The environment chosen for the investigation is Formula 1 motor racing where SNRs are as low as -15dB and the non-acoustic reference signals are engine speed, road speed and throttle measurements. Noise is found to relate closely to these reference signals and a maximum a posteriori method (MAP) is proposed to estimate airflow and tyre noise from these parameters. Objective tests show MAP estimation to be more accurate than a range of conventional noise estimation methods. Subjective listening tests then compare speech enhancement using the proposed MAP estimation to conventional methods with the former found to give significantly higher speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-129"
  },
  "wakisaka11_interspeech": {
   "authors": [
    [
     "Ryo",
     "Wakisaka"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Tomoya",
     "Takatani"
    ]
   ],
   "title": "Blind speech prior estimation for generalized minimum mean-square error short-time spectral amplitude estimator",
   "original": "i11_0361",
   "page_count": 4,
   "order": 132,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "In this paper, to achieve high-quality speech enhancement, we introduce the generalized minimum mean-square error short-time spectral amplitude estimator with a new blind prior estimation of the speech probability density function (p.d.f.). To deal with various types of speech signals with different p.d.f., we propose an algorithm of speech kurtosis estimation based on moment-cumulant transformation for blind adaptation to the shape parameter of speech p.d.f. From the objective and subjective evaluation experiments, we show the improved noise reduction performance of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-130"
  },
  "laskowski11_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "Harmonic structure transform for speaker recognition",
   "original": "i11_0365",
   "page_count": 4,
   "order": 133,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "We evaluate a new filterbank structure, yielding the harmonic structure cepstral coefficients (HSCCs), on a mismatched-session closed-set speaker classification task. The novelty of the filterbank lies in its averaging of energy at frequencies related by harmonicity rather than by adjacency. Improvements are presented which achieve a 37%rel reduction in error rate under these conditions. The improved features are combined with a similar Mel-frequency cepstral coefficient (MFCC) system to yield error rate reductions of 32%rel, suggesting that HSCCs offer information which is complimentary to that available to today's MFCC-based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-131"
  },
  "patil11_interspeech": {
   "authors": [
    [
     "Hemant A.",
     "Patil"
    ],
    [
     "Maulik C.",
     "Madhavi"
    ],
    [
     "Keshab K.",
     "Parhi"
    ]
   ],
   "title": "Combining evidence from spectral and source-like features for person recognition from humming",
   "original": "i11_0369",
   "page_count": 4,
   "order": 134,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "In this paper, hum of a person is used in voice biometric system. In addition, recently proposed feature set, i.e., Variable length Teager Energy Based Mel Frequency Cepstral Coefficients (VTMFCC), is found to capture perceptually meaningful source-like information from hum signal. For person recognition, MFCC gives EER of 13.14% and %ID of 64.96%. A reduction in equal error rate (EER) by 0.2% and improvement in identification rate by 7.3% is achieved when a score-level fusion system is employed by combining evidence from MFCC (system) and VTMFCC (source-like features) than MFCC alone. Results are reported for various feature dimensions and population sizes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-132"
  },
  "long11_interspeech": {
   "authors": [
    [
     "Yanhua",
     "Long"
    ],
    [
     "Zhi-Jie",
     "Yan"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Li-Rong",
     "Dai"
    ],
    [
     "Wu",
     "Guo"
    ]
   ],
   "title": "Improvements in speaker characterization using spectral subband energy based on harmonic plus noise model",
   "original": "i11_0373",
   "page_count": 4,
   "order": 135,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "We previously proposed the use of Spectral Subband Energy Ratio (SSER) as speaker features in a speaker verification system [1]. Those SSER features were derived from two distinct componentsthe harmonic and noise speech parts, which were decomposed by the Harmonic plus Noise Model(HNM) from the original speech. In this paper, we report several recent improvements to this approach. First, we go into the details of the two distinct speech components and achieve a surprising better performance by only extracting the separate Spectral Subband Energy features from each component. Second, we propose a soft unvoiced/voiced (U/V) decision method to preserve more speech data during HNM analysis and feature extraction. Greatly improved experiment results have shown the efficiency of this soft U/V decision. Finally, a further preliminary attempt to extract features from linear frequency domain to mel-frequency domain has also been examined.\n",
    "",
    "",
    "Long, Y., Yan, Z-J., Soong, F. K., Dai, L. and Guo, W., Speaker Characterization Using Spectral Subband Energy Ratio Based on Harmonic Plus Noise Model, in Proc. ICASSP, 2011\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-133"
  },
  "solewicz11_interspeech": {
   "authors": [
    [
     "Yosef A.",
     "Solewicz"
    ],
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Implicit segmentation in two-wire speaker recognition",
   "original": "i11_0377",
   "page_count": 4,
   "order": 136,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "This paper presents a novel self-contained two-wire speaker recognition framework. The classical approach to two-wire speaker recognition usually requires a preliminary explicit speaker segmentation stage in order to extract audio files for the two hypothesized speakers. We propose an implicit speaker segmentation method implemented at the supervector level of speaker recognition systems. By periodically extracting successive supervectors from the two-wire audio it is possible to further associate them to each of the hypothesized speakers before scoring both streams. We show that the proposed technique leads to recognition performance comparable to standard approaches while requiring substantially less resources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-134"
  },
  "yaman11_interspeech": {
   "authors": [
    [
     "Sibel",
     "Yaman"
    ],
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "Boosting speaker recognition performance with compact representations",
   "original": "i11_0381",
   "page_count": 4,
   "order": 137,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "This paper describes a speaker recognition system combination approach in which the compact forms of MAP adapted GMM supervectors are used to boost the performance of a high-dimensional supervector-based system or a combination of multiple systems. The compact supervector representations are subjected to a diagonal transformation to emphasize those dimensions that describe significant speaker information and to de-emphasize noisy dimensions. Scores obtained from these representations are then combined with the scores obtained from high-dimensional supervector representations. The transformation parameters and the combination weights are estimated by minimizing a discriminative training objective function that approximates a minimum detection cost function. We carried out experiments on two NIST 2008 Speaker Recognition Evaluation English telephony tasks to compare the proposed approach with direct score combination obtained from low- and high-dimensional supervector representations. We have found that the proposed approach yields up to 18% relative gain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-135"
  },
  "vaquero11_interspeech": {
   "authors": [
    [
     "Carlos",
     "Vaquero"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Partitioning of two-speaker conversation datasets",
   "original": "i11_0385",
   "page_count": 4,
   "order": 138,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "We address the speaker partitioning problem on datasets composed of two-speaker conversations. In such a situation, it is desirable to obtain a good overall diarization performance but even in that case, the performance of the partitioning problem can be severely degraded if some of the recordings are incorrectly segmented. We show that the performance of a bottom-up speaker clustering approach for the partitioning of two-speaker conversation datasets is sensitive to errors in the diarization, up to a point that the Diarization Error Rate for every recording should be as low as 1% to avoid degradation in performance due to the diarization process. Finally we propose a set of confidence measures along with a logistic regression approach to detect those conversations whose segmentation hypothesis is reliable enough to perform speaker clustering, showing that it enables an improvement in clustering performance at the expense of missing a small portion of the speakers in the dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-136"
  },
  "bousquet11_interspeech": {
   "authors": [
    [
     "Pierre-Michel",
     "Bousquet"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Intersession compensation and scoring methods in the i-vectors space for speaker recognition",
   "original": "i11_0485",
   "page_count": 4,
   "order": 139,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "The total variability factor space in speaker verification system architecture based on Factor Analysis (FA) has greatly improved speaker recognition performances. Carrying out channel compensation in a low dimensional total factor space, rather than in the GMM supervector space, allows for the application of new techniques. We propose here new intersession compensation and scoring methods. Furthermore, this new approach contributes to a better understanding of the session variability characteristics in the total factor space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-137"
  },
  "drgas11_interspeech": {
   "authors": [
    [
     "Szymon",
     "Drgas"
    ],
    [
     "Adam",
     "Dabrowski"
    ]
   ],
   "title": "Kernel alignment maximization for speaker recognition based on high-level features",
   "original": "i11_0489",
   "page_count": 4,
   "order": 140,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "In this paper text-independent automatic speaker verification based on support vector machines is considered. A generalized linear kernel training method based on kernel alignment maximization is proposed. First, kernel matrix decomposition into a sum of maximally aligned directions in the input space is performed and this decomposition is spectrally optimized. The method was evaluated for high-level speaker features: prosodic, articulatory and lexical. The experiments were undertaken employing Switchboard corpus. The proposed algorithm gave equal error rate (EER) reduction up to 23%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-138"
  },
  "srinivasan11_interspeech": {
   "authors": [
    [
     "Balaji Vasan",
     "Srinivasan"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Dmitry N.",
     "Zotkin"
    ],
    [
     "Ramani",
     "Duraiswami"
    ]
   ],
   "title": "Kernel partial least squares for speaker recognition",
   "original": "i11_0493",
   "page_count": 4,
   "order": 141,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "I-vectors are a concise representation of speaker characteristics. Recent advances in speaker recognition have utilized their ability to capture speaker and channel variability to develop efficient recognition engines. Inter-speaker relationships in the i-vector space are non-linear. Accomplishing effective speaker recognition requires a good modeling of these non-linearities and can be cast as a machine learning problem. In this paper, we propose a kernel partial least squares (kernel PLS, or KPLS) framework for modeling speakers in the i-vectors space. The resulting recognition system is tested across several conditions of the NIST SRE 2010 extended core data set and compared against state-of-the-art systems: Joint Factor Analysis (JFA), Probabilistic Linear Discriminant Analysis (PLDA), and Cosine Distance Scoring (CDS) classifiers. Improvements are shown.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-139"
  },
  "omar11_interspeech": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ],
    [
     "Jason",
     "Pelecanos"
    ]
   ],
   "title": "Conversational-side-specific inter-session variability compensation",
   "original": "i11_0497",
   "page_count": 4,
   "order": 142,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "General techniques for inter-session variability compensation may not capture session and channel information specific to a given conversational side. This paper investigates three methods for estimating a conversational-side-specific projection or affine transform to compensate for session and channel effects. In the first, we estimate the projection based on an estimate of the within-class covariance matrix using a conversational-side-specific subset of the development data. In the second, we use a discriminative objective function to estimate the projection parameters. We present an iterative algorithm similar to the expectation maximization (EM) algorithm to estimate the projection parameters which maximize this objective function. An affine transform of the observation vectors of each conversational side is estimated using maximum likelihood estimation in the third method. The maximum likelihood objective function is estimated on a selected subset of the development data. We present several experiments that show how these three techniques perform compared to our baseline system on the interview tasks of the NIST 2008 and the NIST 2010 speaker recognition evaluations. The best method of these techniques gives a performance improvement of up to 20% relative compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-140"
  },
  "leeuwen11_interspeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Niko",
     "Brümmer"
    ]
   ],
   "title": "A speaker line-up for the likelihood ratio",
   "original": "i11_0501",
   "page_count": 4,
   "order": 143,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "We propose an analogy to eye witness line-up in order to compute calibrated likelihood ratios for speaker recognition, by including the target model in an identification trial with a cohort of foils. Expressions for the likelihood ratio as a function of cohort size, identification rank and system ROC performance are derived, and some properties of the likelihood ratio function are discussed. The line-up procedure is used as a method to calibrate recognition scores. Using NIST SRE 2010, we find calibration loss comparable to linear calibration (FoCal), while the proposed method gives improved discrimination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-141"
  },
  "villalba11_interspeech": {
   "authors": [
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Niko",
     "Brümmer"
    ]
   ],
   "title": "Towards fully Bayesian speaker recognition: integrating out the between-speaker covariance",
   "original": "i11_0505",
   "page_count": 4,
   "order": 144,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "We propose a variational Bayes solution to integrate out the model parameters in a generative i-vector speaker recognizer. The existing state-of-the-art in generative i-vector modelling plugs in fixed maximum-likelihood point-estimates of model parameters. This recipe may suffer from over-fitting of especially the between-speaker covariance. We show how to integrate out the between-speaker covariance and demonstrate dramatic improvements on NIST SRE 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-142"
  },
  "pekhovsky11_interspeech": {
   "authors": [
    [
     "Timur",
     "Pekhovsky"
    ],
    [
     "Alexandra",
     "Lokhanova"
    ]
   ],
   "title": "Variational Bayesian model selection for GMM-speaker verification using universal background model",
   "original": "i11_2705",
   "page_count": 4,
   "order": 145,
   "p1": "2705",
   "pn": "2708",
   "abstract": [
    "In this paper we propose to use Variational Bayesian Analysis (VBA) instead of Maximum Likelihood (ML) estimation for Universal Background Model (UBM) building in GMM text independent speaker verification systems. Using VBA estimation solves the problem of the optimal choice of the UBM mixture dimensionality for the training data set, as well as the problem of noise Gaussians which are typical for ML estimation. Experiments using the NIST 2006 and 2008 SRE datasets (cellular channels only) demonstrate superior efficiency of baseline verification systems with a UBM trained using the VBA method compared to standard ML training. Verification error was reduced by almost 8%, compared to a baseline system with standard ML training for the UBM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-143"
  },
  "mclaren11_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "To weight or not to weight: source-normalised LDA for speaker recognition using i-vectors",
   "original": "i11_2709",
   "page_count": 4,
   "order": 146,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "Source-normalised Linear Discriminant Analysis (SNLDA) was recently introduced to improve speaker recognition using i-vectors extracted from multiple speech sources. SNLDA normalises for the effect of speech source in the calculation of the between-speaker covariance matrix. Source-normalised-and-weighted (SNAW) LDA computes a weighted average of source-normalised covariance matrices to better exploit available information. This paper investigates the statistical significance of performance gains offered by SNAW-LDA over SN-LDA. An exhaustive search for optimal scatter weights was conducted to determine the potential benefit of SNAW-LDA. When evaluated on both NIST 2008 and 2010 SRE datasets, scatter-weighting in SNAW-LDA tended to overfit the LDA transform to the evaluation dataset while offering few statistically significant performance improvements over SN-LDA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-144"
  },
  "huang11b_interspeech": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Maximum entropy based data selection for speaker recognition",
   "original": "i11_2713",
   "page_count": 4,
   "order": 147,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "This paper presents the data selection method for speaker recognition. Since there is no promise that more data guarantee better results, the way of data selection becomes important. In the GMM-UBM speaker recognition, the UBM is trained to represent the speaker-independent distribution of acoustic features while the GMM speaker model is tailored for a specific speaker. In this study of data selection for speaker recognition, we apply the maximum entropy criterion to remove the redundant feature frames in the UBM training and to select the discriminative feature frames in the GMM speaker modeling. The conducted experiments on the 2008 NIST Speaker Recognition Evaluation corpus show that the proposed method outperforms the baseline system without the data selection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-145"
  },
  "rao11_interspeech": {
   "authors": [
    [
     "Wei",
     "Rao"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "Addressing the data-imbalance problem in kernel-based speaker verification via utterance partitioning and speaker comparison",
   "original": "i11_2717",
   "page_count": 4,
   "order": 148,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "GMM-SVM has become a promising approach to text-independent speaker verification. However, a problematic issue of this approach is the extremely serious imbalance between the numbers of speaker-class and impostor-class utterances available for training the speaker-dependent SVMs. This data-imbalance problem can be addressed by (1) creating more speaker-class supervectors for SVM training through utterance partitioning with acoustic vector resampling (UP-AVR) and (2) avoiding the SVM training so that speaker scores are formulated as an inner product discriminant function (IPDF) between the target-speaker's supervector and test supervector. This paper highlights the differences between these two approaches and compares the effect of using different kernels . including the KL divergence kernel, GMM-UBM mean interval (GUMI) kernel and geometric-mean-comparison kernel . on their performance. Experiments on the NIST 2010 Speaker Recognition Evaluation suggest that GMM-SVM with UP-AVR is superior to speaker comparison and that the GUMI kernel is slightly better than the KL kernel in speaker comparison.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-146"
  },
  "takashima11_interspeech": {
   "authors": [
    [
     "Ryoichi",
     "Takashima"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Single-channel head orientation estimation based on discrimination of acoustic transfer function",
   "original": "i11_2721",
   "page_count": 4,
   "order": 149,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "This paper presents a talker's head orientation estimation method using only a single microphone, where phoneme HMMs (Hidden Markov Models) of clean speech are introduced to separate the acoustic transfer function at the user's position and head orientation. The frame sequence of the acoustic transfer function is estimated by maximizing the likelihood of training data uttered from a given position with a given head orientation. Using the separated frame sequence data, the user's position and the head orientation are trained by Support Vector Machine (SVM) in advance. Then, for each test utterance, the frame sequence of the acoustic transfer function is separated based on the maximum likelihood estimation using the label sequence obtained from the phoneme recognition, and the user's position and head orientation are estimated by discriminating the separated acoustic transfer function using SVM. The effectiveness of this method has been confirmed by talker localization and head orientation estimation experiments performed in a real environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-147"
  },
  "lei11_interspeech": {
   "authors": [
    [
     "Zhenchun",
     "Lei"
    ],
    [
     "Yingchun",
     "Yang"
    ]
   ],
   "title": "Maximum likelihood i-vector space using PCA for speaker verification",
   "original": "i11_2725",
   "page_count": 4,
   "order": 150,
   "p1": "2725",
   "pn": "2728",
   "abstract": [
    "This paper proposes a new approach to training the i-vector space using a variant of PCA with the Baum-Welch statistics for speaker verification. In eigenvoice the rank of variability space is bounded by the number of training speakers, so a variant of the probabilistic PCA approach is introduced for estimating the parameters. But this constraint doesn't exist in i-vector model because the number of utterances is much bigger than the rank of total variability space. We adopt the EM algorithm for PCA with the statistics to train the total variability space, and the maximum likelihood criterion is used. After WCCN, the cosine similarity scoring is used for decision. These two total variability spaces will be fused at feature-level and score-level. The experiments have been run on the NIST SRE 2008 data, and the results show that the performances in two total variability spaces are comparable. The performance can be improved obviously after feature fusion and score fusion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-148"
  },
  "li11_interspeech": {
   "authors": [
    [
     "Ming",
     "Li"
    ],
    [
     "Xiang",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Speaker verification using sparse representations on total variability i-vectors",
   "original": "i11_2729",
   "page_count": 4,
   "order": 151,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "In this paper, the sparse representation computed by l1- minimization with quadratic constraints is employed to model the i-vectors in the low dimensional total variability space after performing the Within-Class Covariance Normalization and Linear Discriminate Analysis channel compensation. First, we propose the background normalized l2 residual as a scoring criterion. Second, we demonstrate that the Tnorm can be efficiently achieved by using the Tnorm data as the non-target samples in the over-complete dictionary. Finally, by fusing with the conventional i-vector based support vector machine (SVM) and cosine distance scoring system, we demonstrate overall system performance improvement. Exper- imental results show that the proposed fusion system achieved 4.05% (male) and 5.25% (female) equal error rate (EER) after Tnorm on the single-single multi-language handheld telephone task of NIST SRE 2008 and outperformed the SVM baseline by yielding 7.1% and 4.9% relative EER reduction for the male and female tasks, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-149"
  },
  "hasan11_interspeech": {
   "authors": [
    [
     "Taufiq",
     "Hasan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Robust speaker recognition in non-stationary room environments based on empirical mode decomposition",
   "original": "i11_2733",
   "page_count": 4,
   "order": 152,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "In this study, we consider the problem of speaker recognition in a non-stationary room/channel mismatched condition. In such circumstances, cepstral coefficients are affected in a way that the short-term stationarity assumption, on which conventional feature normalization methods are based on, may not be valid. We observe that the empirical mode decomposition (EMD) applied to the cepstral feature stream can partially separate out the nonstationary channel components, if present, into its residual signal and other lower order intrinsic mode functions (IMFs), which leads us to develop a filtering scheme based on this decomposition. The proposed method works in the time domain making use of the instantaneous frequency function obtained through Hilbert spectral analysis of the IMFs. Experimental evaluations on the TIMIT database with added non-stationary room channels in test demonstrate the superiority of the proposed scheme compared to conventional feature normalization schemes. Additional experiments performed on the newly released noisy robust open set speaker identification (ROSSI) and NIST SRE corpora also confirm the effectiveness of the proposed method in stationary room/channel mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-150"
  },
  "even11_interspeech": {
   "authors": [
    [
     "Jani",
     "Even"
    ],
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Range based multi microphone array fusion for speaker activity detection in small meetings",
   "original": "i11_2737",
   "page_count": 4,
   "order": 153,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "This paper presents a method for speaker activity detection in small meetings. The activity of the participants is deduced from audio streams obtained by multiple microphone arrays. One of the novelty of the proposed approach is that it uses a human tracker that relies on scanning laser range finders to localize the participants. First, this additional information is exploited by the beamforming algorithm creating the audio streams for each of the microphone arrays. Then, at each array, the speaker activity detection is performed using Gaussian mixture models that were trained before hand. Finally, a fusion procedure, that also uses the location information, combines the detection results of the different microphone arrays. An experiment reproducing a meeting configuration demonstrates the effectiveness of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-151"
  },
  "ogawa11_interspeech": {
   "authors": [
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Hideitsu",
     "Hino"
    ],
    [
     "Noboru",
     "Murata"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Speaker verification robust to talking style variation using multiple kernel learning based on conditional entropy minimization",
   "original": "i11_2741",
   "page_count": 4,
   "order": 154,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "We developed a new speaker verification system that is robust to intra-speaker variation. There is a strong likelihood that intraspeaker variations will occur due to changes in talking styles, the periods when an individual speaks, and so on. It is well known that such variation generally degrades the performance of speaker verification systems. To solve this problem, we applied multiple kernel learning (MKL) based on conditional entropy minimization, which impose the data to be compactly aggregated for each speaker class and ensure that the different speaker classes were far apart from each other. Experimental results showed that the proposed speaker verification system achieved a robust performance to intra-speaker variation derived from changes in the talking styles compared to the conventional maximum margin-based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-152"
  },
  "hautamaki11_interspeech": {
   "authors": [
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Regularized logistic regression fusion for speaker verification",
   "original": "i11_2745",
   "page_count": 4,
   "order": 155,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "Fusion of the base classifiers is seen as the way to achieve state-ofthe art performance in the speaker verification systems. Standard approach is to pose the fusion problem as the linear binary classification task. Most successful loss function in speaker verification fusion has been the weighted logistic regression popularized by the FoCal toolkit. However, it is known that optimizing logistic regression can overfit severely without appropriate regularization. In addition, subset classifier selection can be achieved by using an external 0/1 loss function on the best subset. In this work, we propose to use LASSO based regularization on the FoCal cost function to achieve improved performance and classifier subset selection method integrated into one optimization task. Proposed method is able to achieve 51% relative improvement in Actual DCF over the FoCal baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-153"
  },
  "jafari11_interspeech": {
   "authors": [
    [
     "Ayeh",
     "Jafari"
    ],
    [
     "Ramji",
     "Srinivasan"
    ],
    [
     "Danny",
     "Crookes"
    ],
    [
     "Ji",
     "Ming"
    ]
   ],
   "title": "A longest matching segment approach with Bayesian adaptation - application to noise-robust speaker recognition",
   "original": "i11_2749",
   "page_count": 4,
   "order": 156,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "Temporal dynamics is an important feature of speech that distinguishes speech from noise, as well as distinguishing between different speakers. In this paper, we present an approach to extract long-range temporal dynamics of speech for text-independent speaker recognition. We aim to maximize the noise immunity arising from the distinct temporal dynamics of speech. The new approach achieves this by identifying the longest matching segments between the training data and test data for recognition. Additionally, the new approach combines Bayesian adaptation, multicondition training and missing-feature theory to further advance the ability to model noisy speech. Experiments have been conducted on the NIST 2002 SRE database in the presence of various types of noise including fast-varying song and music. The new approach has shown improved performance over conventional noise-robust techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-154"
  },
  "lei11b_interspeech": {
   "authors": [
    [
     "Howard",
     "Lei"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Data selection with kurtosis and nasality features for speaker recognition",
   "original": "i11_2753",
   "page_count": 4,
   "order": 157,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "We propose new data selection approaches based on speaker discriminability features, including kurtosis and a set of nasality features which exploit spectral properties of nasal speech sounds. Data selected based on the speaker discriminability features are used to implement end-to-end speaker recognition systems, which produce significant improvements when combined with the baseline system (which uses the speech-only data regions determined by a speech/non-speech detector), where the optimal combination of systems produces roughly a 24% improvement over the baseline. Results suggest that focusing the modeling power on data regions selected via the kurtosis and nasality speaker discriminability features, part of which are often discarded in the speech/non-speech detection process, can improvement speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-155"
  },
  "hernaez11_interspeech": {
   "authors": [
    [
     "Inma",
     "Hernáez"
    ],
    [
     "Ibon",
     "Saratxaga"
    ],
    [
     "Jon",
     "Sanchez"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Iker",
     "Luengo"
    ]
   ],
   "title": "Use of the harmonic phase in speaker recognition",
   "original": "i11_2757",
   "page_count": 4,
   "order": 158,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "In this paper a novel set of features with a promising ability to identify speakers is presented. These features are based on the harmonic phase of the speech signal and have been previously used successfully in an ASR task. Using the SI-284 subset of the WSJ database, a GMM has been trained for each of the 283 speakers and several speaker identification experiments have been performed, with a high level of success. The feature extraction method and the performed experiments are described. The results show that the features present excellent identification performance, very close to the performance of the MFCC parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-156"
  },
  "benus11_interspeech": {
   "authors": [
    [
     "Štefan",
     "Beňuš"
    ],
    [
     "Marianne",
     "Pouplier"
    ]
   ],
   "title": "Jaw movement in vowels and liquids forming the syllable nucleus",
   "original": "i11_0389",
   "page_count": 4,
   "order": 159,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "This paper investigates jaw movements in the production of Slovak syllables with and without vowels. We test the hypothesis that /l, r/ in the syllable nucleus position show a degree of jaw opening comparable to vowels, therefore providing a rising-falling sonority profile even in syllables lacking vowels. We also investigate whether the phonemic length distinction occurring for both vowels and syllabic consonants is implemented in a similar fashion for the different nucleus types. Our articulatory data show that the jaw activity during syllabic liquids is indeed comparable to that of vowels, and that the jaw is recruited to help maintain the main lingual articulation. This became evident in particular in an interaction between nucleus type and phonemic length effects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-157"
  },
  "fivela11_interspeech": {
   "authors": [
    [
     "Barbara Gili",
     "Fivela"
    ],
    [
     "Antonio",
     "Stella"
    ],
    [
     "Sonia",
     "D'Apolito"
    ],
    [
     "Francesco",
     "Sigona"
    ]
   ],
   "title": "Coarticulation across prosodic domains in Italian: an ultrasound investigation",
   "original": "i11_0393",
   "page_count": 4,
   "order": 160,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "This work aims at exploring the phasing of vowels across prosodic boundaries, by analyzing ultrasound data relating to the production of V(#)CV (/i#ba/) sequences by three speakers of Italian. Sequences are inserted in sentences in such a way that C corresponds to the beginning of prosodic domains of various levels and it is strengthened depending on the type of boundary. The influence of prosodic boundaries is investigated on postboundary vowels by means of ultrasound data, which show that a smaller degree of (preservative) coarticulation is found when strong rather than weak prosodic boundaries are realized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-158"
  },
  "simko11_interspeech": {
   "authors": [
    [
     "Juraj",
     "Šimko"
    ],
    [
     "Fred",
     "Cummins"
    ],
    [
     "Štefan",
     "Beňuš"
    ]
   ],
   "title": "Investigating the stability of intergestural timing relations",
   "original": "i11_0397",
   "page_count": 4,
   "order": 161,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "An articulatory analysis of lip and tongue coordination in VCV sequences is presented for four Slovak speakers. Lip and tongue movements are obtained for many utterances elicited in a manner that ensures great variation in both rate and in articulatory precision. Theory and models suggest that gestures might not be sequenced in simple linear order, but that medial consonant timing may be tied to the evolution of the following vowel gesture. We find that the relative timing of the consonant and second vowel gestures is most stable and exhibits least variability when the second vowel gesture provides the temporal reference frame. This work contributes to our understanding of non-linear coarticulatory effects in continuous, and variable, speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-159"
  },
  "zmarich11_interspeech": {
   "authors": [
    [
     "Claudio",
     "Zmarich"
    ],
    [
     "Barbara Gili",
     "Fivela"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Graziano",
     "Tisato"
    ]
   ],
   "title": "Speech timing organization for the phonological length contrast in Italian consonants",
   "original": "i11_0401",
   "page_count": 4,
   "order": 162,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "In Italian, length contrast is exploited in the consonant system. Previous articulatory studies have focused on the temporal organization of gestures in Italian geminates and on the lower lip kinematics of the singleton/geminate distinction, and have showed that the time interval between the nuclei of two successive syllables does not depend on the number of intervening consonants (Ohman's Vowel-to-Vowel model) - In this paper, data on lip and tongue gestures from four Italian subjects saying \"mima\" and \"mimma\" at fast and comfortable rate of delivery are discussed in order to directly test the validity of the Ohman's model for the gestural organization of Italian geminate consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-160"
  },
  "celata11_interspeech": {
   "authors": [
    [
     "Chiara",
     "Celata"
    ],
    [
     "Silvia",
     "Calamai"
    ]
   ],
   "title": "Timing in Italian VNC sequences at different speech rates",
   "original": "i11_0405",
   "page_count": 4,
   "order": 163,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "This study addresses the question of temporal cohesion in Italian word-medial VNC (vowel-nasal-obstruent) sequences varying in the laryngeal status of the post-nasal consonant, for two classes of obstruents distinct in terms of place and at three different speech rates. The temporal relations among the obstruent and the two preceding sonorant segments are examined, and variations in speaking tempo are shown to affect the timing pattern of different speech units in different ways. These results support a view of speech timing control in which temporal effects over constituents spanning syllable boundaries are to be combined with the effects observed over traditional syllable-sized units.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-161"
  },
  "hagedorn11_interspeech": {
   "authors": [
    [
     "Christina",
     "Hagedorn"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Louis",
     "Goldstein"
    ]
   ],
   "title": "Automatic analysis of singleton and geminate consonant articulation using real-time magnetic resonance imaging",
   "original": "i11_0409",
   "page_count": 4,
   "order": 164,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "We explore robust methods of automatically quantifying constriction location, constriction degree and gestural kinematics of Italian short and long consonants using direct image analysis techniques applied to rtMRI data. Articulatory kinematics are estimated from correlated regional changes in pixel intensity. We demonstrate that these methods are capable of quantifying differences in constriction duration exhibited by short and long Italian consonants for labial, coronal and dorsal segments, and differences in constriction degree for labial and coronal consonants. No difference in constriction location is observed for geminates and singletons, while systematic differences in constriction location are observed between (i) coronal oral stops and coronal sonorants and (ii) dorsal stops flanked by vowels differing in backness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-162"
  },
  "wang11d_interspeech": {
   "authors": [
    [
     "Yih-Ru",
     "Wang"
    ]
   ],
   "title": "A two-stage sample-based phone boundary detector using segmental similarity features",
   "original": "i11_0413",
   "page_count": 4,
   "order": 165,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "In this paper, a two-stage sample-based phone boundary detection algorithm is proposed. In the first stage, some local sample-based acoustic parameters are used to pre-select some phone boundary candidates. Then, in the second stage, some high-order statistics of the log-likelihood differences of two adjacent speech segments around each boundary candidate are calculated to serve as similarity measure for candidate verification. Experimental results on the TIMIT speech corpus showed that EERs of 8.6% and 7.6% were achieved for one-stage and two-stage sample-based phone boundary detections, respectively. Moreover, for the two-stage system, 42.1% and 81.9% of boundaries detected were within 5- and 15-sample error tolerance from manual labeling results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-163"
  },
  "huang11c_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huang"
    ],
    [
     "Stephen J.",
     "Cox"
    ]
   ],
   "title": "Iterative improvement of speaker segmentation in a noisy environment using high-level knowledge",
   "original": "i11_0417",
   "page_count": 4,
   "order": 166,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "Our goal is to process the soundtrack of a sports game (tennis) to understand the progress of the game and ultimately, infer its rules. The chair umpire's speech is one of the most useful sources of information, and we focus on identifying the locations of this signal on the soundtrack. Although current techniques for audio segmentation can work well on this task when the acoustics of the training- and test- data are well-matched, they fail when there is a mismatch, which occurs when the chair umpire, the microphone placement, the environmental noise etc. are different in the testand training-data. Our technique uses high-level knowledge of the syntax of the audio events (derived from the training data) to make a coarse estimate of the location of the umpire's speech. The data gathered from these locations is then iteratively refined by contrasting it with data that is believed to belong to another audio class (also gathered using the technique described above). A model is built from this data that enables a more accurate determination of the location of the speech segments to be made. Our approach is applied to three different tennis games: all three have different umpires and different commentators. The results obtained show that it reaches almost the same performance level as that obtained using supervised methods, in which models for the speech are built using prior knowledge of their locations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-164"
  },
  "castan11_interspeech": {
   "authors": [
    [
     "Diego",
     "Castán"
    ],
    [
     "Carlos",
     "Vaquero"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "David",
     "Martínez"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Hierarchical audio segmentation with HMM and factor analysis in broadcast news domain",
   "original": "i11_0421",
   "page_count": 4,
   "order": 167,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "This paper investigates the performance of a Factor Analysis stage in audio segmentation systems. The system described here is designed to segment and classify the audio files coming from broadcast programs into five different classes: speech, speech with noise, speech with music, music or others. This task was recently proposed as a competitive evaluation organized by the Spanish Network on Speech Technologies as part of the conference FALA 2010. The system proposed here makes use of a hierarchical structure in two steps with two different acoustic features. First, the system decides among music, speech with music or the rest of the classes by using HMM/GMM and a smoothed combination of MFCC and Chroma as feature vectors. Next, the system classifies speech and speech with noise by using FA and MFCC as acoustic features. The results shows that, with this configuration, the error rate achieved is lower than the one obtained by the best system presented in the FALA 2010 evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-165"
  },
  "kalinli11_interspeech": {
   "authors": [
    [
     "Ozlem",
     "Kalinli"
    ]
   ],
   "title": "Syllable segmentation of continuous speech using auditory attention cues",
   "original": "i11_0425",
   "page_count": 4,
   "order": 168,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "Segmentation of speech into syllables is beneficial for many spoken language processing applications since it provides information about phonological and rhythmic aspects of speech. Traditional methods usually detect syllable nuclei using features such as energies in critical bands, linear predictive coding spectra, pitch, voicing, etc. Here, a novel system that uses auditory attention cues is proposed for predicting syllable boundaries. The auditory attention cues are biologically inspired and capture changes in sound characteristic by using 2D spectro-temporal receptive filters. When tested on TIMIT, it is shown that the proposed method successfully predicts syllable boundaries and performs as good as or better than the state-of-the art syllable nucleus detection methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-166"
  },
  "peddinti11_interspeech": {
   "authors": [
    [
     "Vijayaditya",
     "Peddinti"
    ],
    [
     "Kishore",
     "Prahallad"
    ]
   ],
   "title": "Exploiting phone-class specific landmarks for refinement of segment boundaries in TTS databases",
   "original": "i11_0429",
   "page_count": 4,
   "order": 169,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "High accuracy speech segmentation methods invariably depend on manually labelled data. However under-resourced languages do not have annotated speech corpora required for training these segmentors. In this paper we propose a boundary refinement technique which uses knowledge of phone-class specific sub-band energy events, in place of manual labels, to guide the refinement process. The use of this knowledge enables proper placement of boundaries in regions with multiple spectral discontinuities in close proximity. It also helps in the correction of large alignment errors. The proposed refinement technique provides boundaries with an accuracy of 82% within 20ms of actual boundary. Combining the proposed technique with iterative isolated HMM training technique boosts the accuracy to 89%, without the use of any manually labelled data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-167"
  },
  "pedone11_interspeech": {
   "authors": [
    [
     "Agnès",
     "Pedone"
    ],
    [
     "Juan José",
     "Burred"
    ],
    [
     "Simon",
     "Maller"
    ],
    [
     "Pierre",
     "Leveau"
    ]
   ],
   "title": "Phoneme-level text to audio synchronization on speech signals with background music",
   "original": "i11_0433",
   "page_count": 4,
   "order": 170,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "We address the task of synchronizing a given phoneme transcription with the corresponding speech signal, when the latter is linearly mixed with background music. To that end, we propose a new method based on Non-negative Matrix Factorization in the time-frequency domain, which models the speech as a source-filter factorization that includes a synchronization parameter matrix. Phoneme models, which consist of collections of basic spectral envelopes, are learned from a training set of isolated speech. The model is subjected to an iterative Maximum Likelihood optimization that concurrently estimates pitch, synchronization parameters and the contribution of the music part. Results show the feasibility of the system for application in text-informed audio processing and automatic subtitle synchronization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-168"
  },
  "seide11_interspeech": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Gang",
     "Li"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Conversational speech transcription using context-dependent deep neural networks",
   "original": "i11_0437",
   "page_count": 4,
   "order": 171,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "We apply the recently proposed Context-Dependent Deep-Neural-Network HMMs, CD-DNN-HMMs, to speech-to-text transcription. For single-pass speaker-independent recognition on the RT03S Fisher portion of phone-call transcription benchmark (Switchboard), the word-error rate is reduced from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs, to 18.5%.a 33% relative improvement.\n",
    "CD-DNN-HMMs combine classic artificial-neural-network HMMs with traditional tied-state triphones and deep-belief-network pretraining. They had previously been shown to reduce errors by 16% relatively when trained on tens of hours of data using hundreds of tied states. This paper takes CD-DNN-HMMs further and applies them to transcription using over 300 hours of training data, over 9000 tied states, and up to 9 hidden layers, and demonstrates how sparseness can be exploited.\n",
    "On four less well-matched transcription tasks, we observe relative error reductions of 22.28%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-169"
  },
  "wang11e_interspeech": {
   "authors": [
    [
     "Guangsen",
     "Wang"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Sequential classification criteria for NNs in automatic speech recognition",
   "original": "i11_0441",
   "page_count": 4,
   "order": 172,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "Neural networks (NNs) are discriminative classifiers which have been successfully integrated with hidden Markov models (HMMs), either in the hybrid NN/HMM or tandem connectionist systems. Typically, the NNs are trained with the frame-based cross-entropy criterion to classify phonemes or phoneme states. However, for word recognition, the word error rate is more closely related to the sequence classification criteria, such as maximum mutual information and minimum phone error. In this paper, the lattice-based sequence classification criteria are used to train the NNs in the hybrid NN/HMM system and the tandem system. A product-ofexpert- based factorization and smoothing scheme is proposed for the hybrid system to scale the lattice-based NN training up to 6000 triphone states. Experimental results on the WSJCAM0 reveal that the NNs trained with the sequential classification criterion yield a 24.2% relative improvement compared to the cross-entropy trained NNs for the hybrid system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-170"
  },
  "magimaidoss11_interspeech": {
   "authors": [
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Ramya",
     "Rasipuram"
    ],
    [
     "Guillermo",
     "Aradilla"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Grapheme-based automatic speech recognition using KL-HMM",
   "original": "i11_0445",
   "page_count": 4,
   "order": 173,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "The state-of-the-art automatic speech recognition (ASR) systems typically use phonemes as subword units. In this work, we present a novel grapheme-based ASR system that jointly models phoneme and grapheme information using Kullback-Leibler divergencebased HMM system (KL-HMM). More specifically, the underlying subword unit models are grapheme units and the phonetic information is captured through phoneme posterior probabilities (referred as posterior features) estimated using a multilayer perceptron (MLP). We investigate the proposed approach for ASR on English language, where the correspondence between phoneme and grapheme is weak. In particular, we investigate the effect of contextual modeling on grapheme-based KL-HMM system and the use of MLP trained on auxiliary data. Experiments on DARPA Resource Management corpus have shown that the grapheme-based ASR system modeling longer subword unit context can achieve same performance as phoneme-based ASR system, irrespective of the data on which MLP is trained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-171"
  },
  "keshet11_interspeech": {
   "authors": [
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Chih-Chieh",
     "Cheng"
    ],
    [
     "Mark",
     "Stoehr"
    ],
    [
     "David",
     "McAllester"
    ]
   ],
   "title": "Direct error rate minimization of hidden Markov models",
   "original": "i11_0449",
   "page_count": 4,
   "order": 174,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "We explore discriminative training of HMM parameters that directly minimizes the expected error rate. In discriminative training one is interested in training a system to minimize a desired error function, like word error rate, phone error rate, or frame error rate. We review a recent method (McAllester, Hazan and Keshet, 2010), which introduces an analytic expression for the gradient of the expected error-rate. The analytic expression leads to a perceptron-like update rule, which is adapted here for training of HMMs in an online fashion. While the proposed method can work with any type of the error function used in speech recognition, we evaluated it on phoneme recognition of TIMIT, when the desired error function used for training was frame error rate. Except for the case of GMM with a single mixture per state, the proposed update rule provides lower error rates, both in terms of frame error rate and phone error rate, than other approaches, including MCE and large margin.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-172"
  },
  "sun11b_interspeech": {
   "authors": [
    [
     "Xie",
     "Sun"
    ],
    [
     "Xin",
     "Chen"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "On the effectiveness of statistical modeling based template matching approach for continuous speech recognition",
   "original": "i11_0453",
   "page_count": 4,
   "order": 175,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "In this work, we validate the effectiveness of our recently proposed integrated template matching and statistical modeling approach on four baseline systems with increasing phone recognition accuracies in the range of 73% to 78% for the TIMIT task. The four baselines were generated using the methods of 1) Discriminative Training (DT) of Minimum Phone Error (MPE), 2) MFCC concatenated with ensemble Multiple Layer Perceptron (MFCC+EMLP) features, 3) DT combined with the MFCC+EMLP features, and 4) data sampling based ensemble acoustic models integrated with DT and MFCC+EMLP features. Experimental results obtained from template matching based rescoring on the phone lattices generated by the baseline models have shown that our template matching approach has produced consistent and significant improvements over the four baselines, and the highest recognition accuracy was 79.55% obtained from rescoring the phone lattices produced by the ensemble acoustic model baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-173"
  },
  "wang11f_interspeech": {
   "authors": [
    [
     "Guangsen",
     "Wang"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Comparison of smoothing techniques for robust context dependent acoustic modelling in hybrid NN/HMM systems",
   "original": "i11_0457",
   "page_count": 4,
   "order": 176,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "Hybrid Neural Network/Hidden Markov Model (NN/HMM) systems have been found to yield high quality phone recognition performance. One issue with modelling the Context Dependent (CD) NN/HMM is the robust estimation of the NN parameters to reliably predict the large number of CD state posteriors. Previously, factorization based on conditional probabilities has been commonly adopted to circumvent this problem. This paper proposes two factorization schemes based on the product-of-expert framework, depending on the choice of the experts. In addition, smoothing and interpolation schemes were introduced to improve robustness. Experimental results on the WSJCAM0 reveal that the proposed CD NN/HMM parameter estimation techniques achieved consistent improvement compared to CI hybrid systems. The best hybrid system achieves a 21.7% relative phone error rate reduction and a 17.6% word error reduction compared to a discriminative trained context dependent triphone GMM/HMM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-174"
  },
  "hsiao11_interspeech": {
   "authors": [
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Generalized Baum-welch algorithm and its implication to a new extended Baum-welch algorithm",
   "original": "i11_0773",
   "page_count": 4,
   "order": 177,
   "p1": "773",
   "pn": "776",
   "abstract": [
    "This paper describes how we can use the generalized Baum-Welch (GBW) algorithm to develop better extended Baum-Welch (EBW) algorithms. Based on GBW, we show that the backoff term in the EBW algorithm comes from KL-divergence which is used as a regularization function. This finding allows us to develop a fast EBW algorithm, which can reduce the time of model space discriminative training by half, without incurring any degradation on recognition accuracy. We compare the performance of the new EBW algorithm with the original one on various large scale systems including Farsi, Iraqi and modern standard Arabic ASR systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-175"
  },
  "diehl11_interspeech": {
   "authors": [
    [
     "F.",
     "Diehl"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "X.",
     "Liu"
    ],
    [
     "M.",
     "Tomalin"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Word boundary modelling and full covariance Gaussians for Arabic speech-to-text systems",
   "original": "i11_0777",
   "page_count": 4,
   "order": 178,
   "p1": "777",
   "pn": "780",
   "abstract": [
    "This paper describes recent improvements to the Cambridge Arabic Large Vocabulary Continuous Speech Recognition (LVCSR) Speech-to-Text (STT) system. It is shown that word-boundary context markers provide a powerful method to enhance graphemic systems by implicit phonetic information, improving the modelling capability of graphemic systems. In addition, a robust technique for full covariance Gaussian modelling in the Minimum Phone Error (MPE) training framework is introduced. This reduces the full covariance training to a diagonal covariance training problem, thereby solving related robustness problems. The full system results show that the combined use of these and other techniques within a multi-branch combination framework reduces the Word Error Rate (WER) of the complete system by up to 5.9% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-176"
  },
  "ko11_interspeech": {
   "authors": [
    [
     "Tom",
     "Ko"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "A fully automated derivation of state-based eigentriphones for triphone modeling with no tied states using regularization",
   "original": "i11_0781",
   "page_count": 4,
   "order": 179,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "to solve the data insufficiency problem in triphone acoustic modeling without the need of state tying. The idea is to treat the acoustic modeling problem of infrequent triphones (\"poor triphones\") as an adaptation problem from the more frequent triphones (\"rich triphones\"): firstly, an eigenbasis is developed over the rich triphones that have sufficient training data and the eigenvectors are called eigentriphones; then the poor triphones are adapted in a fashion similar to eigenvoice adaptation. Since, in general, no states are tied in our method, all triphones (states) are distinct so that they can be more discriminative than tied-state triphones. In our previous work, the number of eigentriphones was determined in advance with a set of development data. In this paper, we investigate simply using all of them with the help of regularization to naturally penalize the less important ones. In addition, the model-based eigenbasis is replaced by three state-based eigenbases. Experimental evaluation on the WSJ 5K task shows that triphone models trained using our new eigentriphone approach without state tying perform at least as well as the common tiedstate\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-177"
  },
  "sainath11_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "David",
     "Nahamoo"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ]
   ],
   "title": "Reducing computational complexities of exemplar-based sparse representations with applications to large vocabulary speech recognition",
   "original": "i11_0785",
   "page_count": 4,
   "order": 180,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "Recently, exemplar-based sparse representation phone identification features (Spif ) have shown promising results on large vocabulary speech recognition tasks. However, one problem with exemplar-based techniques is that they are computationally expensive. In this paper, we present two methods to speed up the creation of Spif features. First, we explore a technique to quickly select a subset of informative exemplars among millions of training examples. Secondly, we make approximations to the sparse representation computation such that a matrix-matrix multiplication is reduced to a matrix-vector product. We present results on four large vocabulary tasks, including Broadcast News where acoustic models are trained with 50 and 400 hours, and a Voice Search task, where models are trained with 160 and 1000 hours. Results on all tasks indicate improvements in speedup by a factor of four relative to the original Spif features, as well as improvements in word error rate (WER) in combination with a baseline HMM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-178"
  },
  "zhang11d_interspeech": {
   "authors": [
    [
     "Yu",
     "Zhang"
    ],
    [
     "Jian",
     "Xu"
    ],
    [
     "Zhi-Jie",
     "Yan"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "An i-vector based approach to training data clustering for improved speech recognition",
   "original": "i11_0789",
   "page_count": 4,
   "order": 181,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "We present a new approach to clustering training data for improved speech recognition. Given a training corpus, a so-called i-vector is extracted from each training utterance. A hierarchical divisive clustering algorithm is then used to cluster the training i-vectors into multiple clusters. For each cluster, an acoustic model (AM) is trained accordingly. Such trained multiple AMs can then be used in recognition stage to improve recognition accuracy. The proposed approach is very efficient therefore can deal with very large scale training corpus on current mainstream computing platforms. We report experimental results on a voice search task with 7,500 hours of speech training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-179"
  },
  "buthpitiya11_interspeech": {
   "authors": [
    [
     "Senaka",
     "Buthpitiya"
    ],
    [
     "Ian",
     "Lane"
    ],
    [
     "Jike",
     "Chong"
    ]
   ],
   "title": "Rapid training of acoustic models using graphics processing unit",
   "original": "i11_0793",
   "page_count": 4,
   "order": 182,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "Robust and accurate speech recognition systems can only be realized with adequately trained acoustic models. For common languages, state-of-the-art systems are now trained on thousands of hours of speech data. Even with a large cluster of machines the entire training process can take many weeks. To overcome this development bottleneck we propose a new framework for rapid training of acoustic models using highly parallel graphics processing units (GPUs). In this paper we focus on Viterbi training and describe the optimizations required for effective throughput on GPU processors. Using a single NVIDIA GTX580 GPU our proposed approach is shown to be 51#215; faster than a sequential CPU implementation, enabling a moderately sized acoustic model to be trained on 1000 hours of speech data in just over 9 hours. Moreover, we show that our implementation on a two-GPU system can perform 67% faster than a standard parallel reference implementation on a high-end 32-core Xeon server. Our GPU-based training platform empowers research groups to rapidly evaluate new ideas and build accurate and robust acoustic models on very large training corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-180"
  },
  "alessandrini11_interspeech": {
   "authors": [
    [
     "Michele",
     "Alessandrini"
    ],
    [
     "Giorgio",
     "Biagetti"
    ],
    [
     "Alessandro",
     "Curzi"
    ],
    [
     "Claudio",
     "Turchetti"
    ]
   ],
   "title": "Semi-automatic acoustic model generation from large unsynchronized audio and text chunks",
   "original": "i11_1681",
   "page_count": 4,
   "order": 183,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "In this paper an effective technique to train an acoustic model from large and unsynchronized audio and text chunks is presented. Given such a speech corpus, an algorithm to automatically segment each chunk into smaller fragments and to synchronize those to the corresponding text is defined. These smaller fragments are more suitable to be used in standard model training algorithms for usage in automatic speech recognition systems. The proposed approach is particularly suitable to bootstrap language models without relying neither on specialized training material nor borrowing from models trained for other similar languages. Extensive experimentation using the CMU Sphinx 4 recognizer and the SphinxTrain model generator in a setting designed for large-vocabulary continuous speech recognition shows the effectiveness of the approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-181"
  },
  "strope11_interspeech": {
   "authors": [
    [
     "Brian",
     "Strope"
    ],
    [
     "Doug",
     "Beeferman"
    ],
    [
     "Alexander",
     "Gruenstein"
    ],
    [
     "Xin",
     "Lei"
    ]
   ],
   "title": "Unsupervised testing strategies for ASR",
   "original": "i11_1685",
   "page_count": 4,
   "order": 184,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "This paper describes unsupervised strategies for estimating relative accuracy differences between acoustic models or language models used for automatic speech recognition. To test acoustic models, the approach extends ideas used for unsupervised discriminative training to include a more explicit validation on held out data. To test language models, we use a dual interpretation of the same process, this time allowing us to measure differences by exploiting expected 'truth gradients' between strong and weak acoustic models. The paper shows correlations between supervised and unsupervised measures across a range of acoustic model and language model variations. We also use unsupervised tests to assess the non-stationary nature of mobile speech input.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-182"
  },
  "kurata11_interspeech": {
   "authors": [
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Nobuyasu",
     "Itoh"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Acoustic model training with detecting transcription errors in the training data",
   "original": "i11_1689",
   "page_count": 4,
   "order": 185,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "As the target of Automatic Speech Recognition (ASR) has moved from clean read speech to spontaneous conversational speech, we need to prepare orthographic transcripts of spontaneous conversational speech to train acoustic models (AMs). However, it is expensive and slow to manually transcribe such speech word by word. We propose a framework to train an AM based on easy-to-make rough transcripts in which fillers and small word fragments are not precisely transcribed and some transcription errors are included. By focusing on the phone duration in the result of forced alignment between the rough transcripts and the utterances, we can automatically detect the erroneous parts in the rough transcripts. A preliminary experiment showed that we can detect the erroneous parts with moderately high recall and precision. Through ASR experiments with conversational telephone speech, we confirmed that automatic detection helped improve the performance of the AM trained with both conventional ML criteria and state-of-the-art boosted MMI criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-183"
  },
  "jansen11_interspeech": {
   "authors": [
    [
     "Aren",
     "Jansen"
    ],
    [
     "Kenneth",
     "Church"
    ]
   ],
   "title": "Towards unsupervised training of speaker independent acoustic models",
   "original": "i11_1693",
   "page_count": 4,
   "order": 186,
   "p1": "1693",
   "pn": "1696",
   "abstract": [
    "Can we automatically discover speaker independent phoneme-like subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-184"
  },
  "cui11_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Xin",
     "Chen"
    ],
    [
     "Jian",
     "Xue"
    ],
    [
     "Peder A.",
     "Olsen"
    ],
    [
     "John R.",
     "Hershey"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Acoustic modeling with bootstrap and restructuring based on full covariance",
   "original": "i11_1697",
   "page_count": 4,
   "order": 187,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "Bootstrap and restructuring (BSRS) has been shown in our previous work to be superior over the conventional acoustic modeling approach when dealing with low-resourced languages. This paper presents a full covariance based BSRS scheme, which is an extension of our previous work on diagonal covariance based BSRS acoustic modeling. Since full covariance provides richer structural information of acoustic model compared to its diagonal counterpart, it is advantageous for both model clustering and refinement. Therefore, in this work, full covariance is employed in BSRS to keep the structural information until the last step before being converted to diagonal covariance for practical applications. We show that using full covariance further improves the performance over diagonal covariance in the BSRS acoustic modeling framework under the same model size without increasing computational cost in decoding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-185"
  },
  "xu11_interspeech": {
   "authors": [
    [
     "Jian",
     "Xu"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Zhi-Jie",
     "Yan"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "An i-vector based approach to acoustic sniffing for irrelevant variability normalization based acoustic model training and speech recognition",
   "original": "i11_1701",
   "page_count": 4,
   "order": 188,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "This paper presents a new approach to acoustic sniffing for irrelevant variability normalization (IVN) based acoustic model training and speech recognition. Given a training corpus, a socalled i-vector is extracted from each training speech segment. A clustering algorithm is used to cluster the training i-vectors into multiple clusters, each corresponding to an acoustic condition. The acoustic sniffing can then be implemented as finding the most similar cluster by comparing the i-vector extracted from a speech segment with the centroid of each cluster. Experimental results on Switchboard-1 conversational telephone speech transcription task suggest that the i-vector based acoustic sniffing outperforms our previous Gaussian mixture model (GMM) based approach. The proposed approach is very efficient therefore can deal with very large scale training corpus on current mainstream computing platforms, yet has very low run-time cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-186"
  },
  "tahir11_interspeech": {
   "authors": [
    [
     "Muhammad Ali",
     "Tahir"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Log-linear optimization of second-order polynomial features with subsequent dimension reduction for speech recognition",
   "original": "i11_1705",
   "page_count": 4,
   "order": 189,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "Second order polynomial features are useful for speech recognition because they can be used to model class specific covariance even with a pooled covariance acoustic model. Previous experiments with second order features have shown word error rate improvements. However, the improvement comes at the price of a large increase in the number of parameters. This paper investigates the discriminative training of second order features, with a subsequent dimension reduction transform to limit the increase in number of parameters. The acoustic model parameters and the transformation matrix parameters are modeled log-linearly and optimized using maximum mutual information criterion. The advantage of log-linear optimization lies in its ability to robustly combine different kinds of features. Experiments are performed for second order MFCC features on the EPPS large vocabulary task and have resulted in a decrease in word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-187"
  },
  "zhang11e_interspeech": {
   "authors": [
    [
     "Qingqing",
     "Zhang"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Genre categorization and modeling for broadcast speech transcription",
   "original": "i11_1709",
   "page_count": 4,
   "order": 190,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "Broadcast News (BN) speech recognition transcription has attracted research due to the challenges of the task since the mid 1990's. More recently, research has been moving towards more spontaneous broadcast data, commonly called Broadcast Conversation (BC) speech. Considering the large style difference between BN and BC genres, specific modeling of genres should intuitively result in improved system performance. In this paper BN- and BC-style speech recognition has been explored by designing genre-specific systems. In order to separate the training data, an automatic genre categorization with two novel features is proposed. Experiments showed that automatic categorization of genre labels of the training data compared favorably to the original manually specified genre labels provided with corpora. When test data sets were classified into BN or BC genres and tested by the corresponding genre-specific speech recognition systems, modest but consistent error reductions were achieved compared to the baseline genreindependent systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-188"
  },
  "shin11_interspeech": {
   "authors": [
    [
     "Sunghwan",
     "Shin"
    ],
    [
     "Ho-Young",
     "Jung"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Individual error minimization learning framework and its applications to speech recognition and utterance verification",
   "original": "i11_1713",
   "page_count": 4,
   "order": 191,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "In this paper, we extend the individual recognition error minimization criteria, MDE/MIE/MSE [1] in word-level and apply them to word recognition and verification tasks, respectively. In order to effectively reduce potential errors in word-level, we expand the training token selection scheme to be more appropriate for word-level learning framework, by taking into account neighboring words and by covering internal phonemes in each training word. Then, we examine the proposed word-level learning criteria on the TIMIT word recognition task and further investigate individual rejection performance of the recognition errors in utterance verification (UV). Experimental results confirm that each of the word-level objective criteria results in primarily reducing the corresponding target error type, respectively. The rejection rates of insertion and substitution errors are also improved within MIE and MSE criteria, which lead to additional word error rate reduction after the rejection.\n",
    "",
    "",
    "Shin, S., Jung, H.-Y. and Juang, B.-H., Discriminative Training for Direct Minimization of Deletion, Insertion and Substitution Errors, in ICASSP 2011, pp. 5328-5331, May 2011\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-189"
  },
  "darjaa11_interspeech": {
   "authors": [
    [
     "Sakhia",
     "Darjaa"
    ],
    [
     "Miloš",
     "Cerňak"
    ],
    [
     "Marián",
     "Trnka"
    ],
    [
     "Milan",
     "Rusko"
    ],
    [
     "Róbert",
     "Sabo"
    ]
   ],
   "title": "Effective triphone mapping for acoustic modeling in speech recognition",
   "original": "i11_1717",
   "page_count": 4,
   "order": 192,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "This paper presents effective triphone mapping for acoustic models training in automatic speech recognition, which allows the synthesis of unseen triphones. The description of this data-driven model clustering, including experiments performed using 350 hours of a Slovak audio database of mixed read and spontaneous speech, are presented. The proposed technique is compared with tree-based state tying, and it is shown that for bigger acoustic models, at a size of 4000 states and more, a triphone mapped HMM system achieves better performance than a tree-based state tying system. The main gain in performance is due to latent application of triphone mapping on monophones with multiple Gaussian pdfs, so the cloned triphones are initialized better than with single Gaussians monophones. Absolute decrease of word error rate was 0.46% (5.73% relatively) for models with 7500 states, and decreased to 0.4% (5.17% relatively) gain at 11500 states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-190"
  },
  "nallasamy11_interspeech": {
   "authors": [
    [
     "Udhyakumar",
     "Nallasamy"
    ],
    [
     "Michael",
     "Garbus"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Thomas",
     "Schaaf"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Analysis of dialectal influence in pan-Arabic ASR",
   "original": "i11_1721",
   "page_count": 4,
   "order": 193,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "In this paper, we analyze the impact of five Arabic dialects on the front-end and pronunciation dictionary components of an Automatic Speech Recognition (ASR) system. We use ASR's phonetic decision tree as a diagnostic tool to compare the robustness of MFCC and MLP front-ends to dialectal variations in the speech data and found that MLP Bottle-Neck features are less robust to such variations. We also perform a rule-based analysis of the pronunciation dictionary, which enables us to identify dialectal words in the vocabulary and automatically generate pronunciations for unseen words. We show that our technique produces pronunciations with an average phone error rate 9.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-191"
  },
  "jalalvand11_interspeech": {
   "authors": [
    [
     "Azarakhsh",
     "Jalalvand"
    ],
    [
     "Fabian",
     "Triefenbach"
    ],
    [
     "David",
     "Verstraeten"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Connected digit recognition by means of reservoir computing",
   "original": "i11_1725",
   "page_count": 4,
   "order": 194,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "Most automatic speech recognition systems employ Hidden Markov Models with Gaussian mixture emission distributions to model the acoustics. There have been several attempts however to challenge this approach, e.g. by introducing a neural network (NN) as an alternative acoustic model. Although the performance of these so-called hybrid systems is actually quite good, their training is often problematic and time consuming. By using a reservoir . this is a recurrent NN with only the output weights being trainable . we can overcome this disadvantage and yet obtain good accuracy. In this paper, we propose the first reservoir-based connected digit recognition system, and we demonstrate good performance on the Aurora-2 testbed. Since RC is a new technology, we anticipate that our present system is still sub-optimal, and further improvements are possible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-192"
  },
  "ratnagiri11_interspeech": {
   "authors": [
    [
     "Madhavi V.",
     "Ratnagiri"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Lawrence",
     "Rabiner"
    ]
   ],
   "title": "Large margin - minimum classification error using sum of shifted sigmoids as the loss function",
   "original": "i11_1729",
   "page_count": 4,
   "order": 195,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "Mon-Ses1-P4.13, #13 We have developed a novel loss function that embeds large-margin classification into Minimum Classification Error (MCE) training. Unlike previous efforts this approach employs a loss function that is bounded, does not require incremental adjustment of the margin or prior MCE training. It extends the Bayes risk formulation of MCE using Parzen Window estimation to incorporate large-margin classification and develops a loss function that is a sum of shifted sigmoids. Experimental results show improvement in recognition performance when evaluated on the TIDigits database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-193"
  },
  "olaso11_interspeech": {
   "authors": [
    [
     "Javier M.",
     "Olaso"
    ],
    [
     "M. Inés",
     "Torres"
    ],
    [
     "Raquel",
     "Justo"
    ]
   ],
   "title": "Representing phonological features through a two-level finite state model",
   "original": "i11_1733",
   "page_count": 4,
   "order": 196,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "Articulatory information has demonstrated to be useful to improve phone recognition performance in ASR systems, being the use of Neural Networks the most successful method to detect articulatory gestures from the speech signal. On the other hand, Stochastic Finite State Automata (SFSA) have been effectively used in many speech-input natural language tasks. In this work SFSA are used to represent phonological features. A hierarchical model able to consider sequences of acoustic observations along with sequences of phonological features is defined. From this formulation a classifier of articulatory features has been derived and then evaluated over a Spanish phonetic corpus. Experimental results show that this is a promising framework to detect and include phonological\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-194"
  },
  "vanek11_interspeech": {
   "authors": [
    [
     "Jan",
     "Vaněk"
    ],
    [
     "Jan",
     "Trmal"
    ],
    [
     "Josef V.",
     "Psutka"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Optimization of the Gaussian mixture model evaluation on GPU",
   "original": "i11_1737",
   "page_count": 4,
   "order": 197,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "In this paper we present a highly optimized implementation of Gaussian mixture acoustic model evaluation algorithm. Evaluation of these likelihoods is one of the most computationally intensive parts of automatics speech recognizers but it can be well-parallelized and offloaded to GPU devices. Our approach offers significant speed-up compared to the recently published approaches, since it exploits the GPU architecture better. All the recent implementations were programmed either in CUDA or OpenCL GPU programming frameworks. We present results for both; CUDA as well as OpenCL.\n",
    "Results suggest that even very large acoustic models can be utilized in real-time speech recognition engines on computers and laptops equipped with a low-end GPU. Optimization of acoustic likelihoods computation on GPU enables to use the remaining GPU resources for offloading of other compute-intensive parts of LVCSR decoder. Other possible use of the freed GPU resources is to evaluate several acoustic models at the same time and use fusion techniques or model selection techniques to improve the quality of resulting conditional likelihoods under diverse conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-195"
  },
  "astudillo11_interspeech": {
   "authors": [
    [
     "Ramón Fernandez",
     "Astudillo"
    ],
    [
     "João Paulo da Silva",
     "Neto"
    ]
   ],
   "title": "Propagation of uncertainty through multilayer perceptrons for robust automatic speech recognition",
   "original": "i11_0461",
   "page_count": 4,
   "order": 198,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "Observation uncertainty techniques offer a way to dynamically compensate automatic speech recognizers to account for the information missing in real world scenarios. These techniques have been demonstrated to effectively be able to compensate multiple environment distortions and improve the integration of ASR systems with speech enhancement pre-processing through uncertainty propagation. Unfortunately observation uncertainty techniques rely on statistical methods and as such are limited to GMM-HMM architectures. In this paper we explore the application of observation uncertainty and uncertainty propagation techniques to multi-layer perceptrons (MLPs). We develop solutions for propagation through a generic MLP and exemplify potential gains with an large vocabulary robust ASR experiment on the AURORA4 database using an Hybrid MLP-HMM recognizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-196"
  },
  "mahkonen11_interspeech": {
   "authors": [
    [
     "Katariina",
     "Mahkonen"
    ],
    [
     "Antti",
     "Hurmalainen"
    ],
    [
     "Tuomas",
     "Virtanen"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ]
   ],
   "title": "Mapping sparse representation to state likelihoods in noise-robust automatic speech recognition",
   "original": "i11_0465",
   "page_count": 4,
   "order": 199,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "This paper proposes learning-based methods for mapping a sparse representation of noisy speech to state likelihoods in an automatic speech recognition system. We represent speech as a sparse linear combination of exemplars extracted from training data. The weights of exemplars are mapped to speech state likelihoods using Ordinary Least Squares (OLS) and Partial Least Squares (PLS) regression. Recognition experiments are conducted using the CHiME noisy speech database. According to the results, both algorithms can be successfully used for training the mapping. We achieve improvements over the previous binary labeling system, and recognition scores close to 70% at -6 dB SNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-197"
  },
  "kallasjoki11_interspeech": {
   "authors": [
    [
     "Heikki",
     "Kallasjoki"
    ],
    [
     "Ulpu",
     "Remes"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Tuomas",
     "Virtanen"
    ],
    [
     "Kalle J.",
     "Palomäki"
    ]
   ],
   "title": "Uncertainty measures for improving exemplar-based source separation",
   "original": "i11_0469",
   "page_count": 4,
   "order": 200,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "This work studies the use of observation uncertainty measures for improving the speech recognition performance of an exemplar-based source separation based front end. To generate the observation uncertainty estimates for the enhanced features, we propose the use of heuristic methods based on the sparse representation of the noisy signal in the exemplar-based source separation algorithm. The effectiveness of the proposed measures is evaluated in a large vocabulary noisy speech recognition task. The best proposed measure achieved relative error reductions up to 18% over the baseline feature enhancement method without uncertainty measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-198"
  },
  "liao11b_interspeech": {
   "authors": [
    [
     "Hsien-Cheng",
     "Liao"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Maximum confidence measure based interaural phase difference estimation for noise masking in dual-microphone robust speech recognition",
   "original": "i11_0473",
   "page_count": 4,
   "order": 201,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "A new one-stage maximum confidence measure (MCM) based interaural phase difference estimation framework for noise masking is proposed to closely integrate the underline speech models into dual-microphone array noise filtering for robust speech recognition. The main ideas are: (1) utilizing both the speech and filler models of the recognizer to feedback confidence measures (CMs) that indicate the degree of separation between filtered speech and interference noises, and (2) automatically optimizing the parameters of the microphone array with an expectation maximization (EM) algorithm based on the proposed MCM criterion. Experimental results on a Mandarin voice command task show that the proposed approach significantly improves the final speech recognition rates. Moreover the observed performance degradation is usually graceful under low signal-to-noise ratios (SNRs) and close interference noises conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-199"
  },
  "badiezadegan11_interspeech": {
   "authors": [
    [
     "Shirin",
     "Badiezadegan"
    ],
    [
     "Richard",
     "Rose"
    ]
   ],
   "title": "A performance monitoring approach to fusing enhanced spectrogram channels in robust speech recognition",
   "original": "i11_0477",
   "page_count": 4,
   "order": 202,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "An implementation of a performance monitoring approach to feature channel integration in robust automatic speech recognition is presented. Motivated by psychophysical evidence in human speech perception, the approach combines multiple feature channels using a closed loop criterion relating to the overall performance of the system. The multiple feature channels correspond to an ensemble of reconstructed spectrograms generated by applying multiresolution discrete wavelet transform analysis-synthesis filter-banks to corrupted speech spectrograms. The spectrograms associated with these feature channels differ in the degree to which information has been suppressed in multiple scales and frequency bands. The performance of this approach is evaluated in both the Aurora 2 and the Aurora 3 speech in noise task domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-200"
  },
  "cheng11_interspeech": {
   "authors": [
    [
     "Ning",
     "Cheng"
    ],
    [
     "X.",
     "Liu"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Generalized variable parameter HMMs for noise robust speech recognition",
   "original": "i11_0481",
   "page_count": 4,
   "order": 203,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "Handling variable ambient noise is a challenging task for automatic speech recognition (ASR) systems. To address this issue, multi-style, noise condition independent (CI) model training using speech data collected in diverse noise environments, or uncertainty decoding techniques can be used. An alternative approach is to explicitly approximate the continuous trajectory of Gaussian component mean and variance parameters against the varying noise level, for example, using variable parameter HMMs (VP-HMM). This paper investigates a more generalized form of variable parameter HMMs (GVP-HMM). In addition to Gaussian component means and variances, it can also provide a more compact trajectory modelling for tied linear transformations. An alternative noise condition dependent (CD) training algorithm is also proposed to handle the bias to training noise condition distribution. Consistent error rate gains were obtained over conventional VP-HMM mean and variance only trajectory modelling on a medium vocabulary Mandarin Chinese in-car navigation command recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-201"
  },
  "mowlaee11_interspeech": {
   "authors": [
    [
     "P.",
     "Mowlaee"
    ],
    [
     "R.",
     "Saeidi"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "M. G.",
     "Christensen"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "P.",
     "Fränti"
    ],
    [
     "S. H.",
     "Jensen"
    ]
   ],
   "title": "Sinusoidal approach for the single-channel speech separation and recognition challenge",
   "original": "i11_0677",
   "page_count": 4,
   "order": 204,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "Most of the single-channel speech separation (SCSS) systems use the short-time Fourier transform as their parametric features. Recent studies have shown that employing sinusoidal features for the SCSS application results in a high perceived speech quality. In this paper, we make a systematic study on automatic speech recognition results for a SCSS system that uses sinusoidal features composed of amplitude and frequency. We compare the speech recognition results with those already reported by other participants in the single-channel speech separation and recognition challenge. Our results show that a newly proposed system achieves an overall recognition accuracy of 52.3%, ranges at the median over all other participants in the challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-202"
  },
  "demir11_interspeech": {
   "authors": [
    [
     "Cemil",
     "Demir"
    ],
    [
     "A. Taylan",
     "Cemgil"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Semi-supervised single-channel speech-music separation for automatic speech recognition",
   "original": "i11_0681",
   "page_count": 4,
   "order": 205,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "In this study, we propose a semi-supervised speech-music separation method which uses the speech, music and speech-music segments in a given segmented audio signal to separate speech and music signals from each other in the mixed speech-music segments. In this strategy, we assume, the background music of the mixed signal is partially composed of the repetition of the music segment in the audio. Therefore, we used a mixture model to represent the music signal. The speech signal is modeled using Non-negative Matrix Factorization (NMF) model. The prior model of the template matrix of the NMF model is estimated using the speech segment and updated using the mixed segment of the audio. The separation performance of the proposed method is evaluated in automatic speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-203"
  },
  "maganti11_interspeech": {
   "authors": [
    [
     "HariKrishna",
     "Maganti"
    ],
    [
     "Marco",
     "Matassoni"
    ]
   ],
   "title": "A level-dependent auditory filter-bank for speech recognition in reverberant environments",
   "original": "i11_0685",
   "page_count": 4,
   "order": 206,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "Distortions due to reverberation have detrimental effect on the performance of automatic speech recognition (ASR). In this work, an auditory filter-bank based feature is presented to improve the ASR in reverberant conditions. The proposed technique is based on the gammachirp filter bank which provides level dependent frequency response to emulate mechanisms performed in the human auditory system, particularly basilar membrane filtering aimed to improve robustness of the ear. The low frequency tail of gammachirp filter which is unaffected by bandwidth parameters due to level dependency frequency resolution is effective in reducing the reverberation distortions. Experiments are performed on the Aurora-5 meeting recorder digit task recorded with four different microphones in hands-free mode at a real meeting room. The ASR experiments using the proposed gammachirp based features show reliable and consistent improvements when compared to other conventional feature extraction techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-204"
  },
  "souden11_interspeech": {
   "authors": [
    [
     "Mehrez",
     "Souden"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "A multichannel feature-based processing for robust speech recognition",
   "original": "i11_0689",
   "page_count": 4,
   "order": 207,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "We propose a new approach for multichannel robust speech recognition. This approach extends the vector Taylor series (VTS)-based feature compensation from the single channel to the multichannel case. Precisely, we use the first order VTS to approximate each of the microphone feature vectors. Afterwards, these features are jointly processed to estimate the acoustic channel and noise statistics via expectation maximization (EM). Experimental results with TI-Digits and measured impulse responses show that the proposed method can achieve significant gains in terms of word recognition accuracy in different noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-205"
  },
  "xiao11_interspeech": {
   "authors": [
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Feature normalization using structured full transforms for robust speech recognition",
   "original": "i11_0693",
   "page_count": 4,
   "order": 208,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "Classical mean and variance normalization (MVN) uses a diagonal transform and a bias vector to normalize the mean and variance of noisy features to reference values. As MVN uses diagonal transform, it ignores correlation between feature dimensions. Although full transform is able to make use of feature correlation, its large amount of parameters may not be estimated reliably from a short observation, e.g. 1 utterance. We propose a novel structured full transform that has the same amount of free parameters as diagonal transform while being able to capture correlation between feature dimensions. The proposed structured transform can be estimated reliably from one utterance by maximizing the likelihood of the normalized features on a reference Gaussian mixture model. Experimental results on Aurora- 4 task show that the structured transform produces consistently better speech recognition results than diagonal transform and also outperforms advanced frontend (AFE) feature extractor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-206"
  },
  "fujimoto11_interspeech": {
   "authors": [
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "A robust estimation method of noise mixture model for noise suppression",
   "original": "i11_0697",
   "page_count": 4,
   "order": 209,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "Vector Taylor series (VTS)-based noise suppression usually employs a single Gaussian distribution for the noise model. However, it is insufficient for non-stationary noise which has a multi-peak distribution. It is very complex to estimate multi-peak distribution of the noise, when we deal with the noise as random variables or hidden variables. To solve these problems, we investigate a way of estimating the noise mixture model by using a minimum mean squared error (MMSE) estimate of the noise. By iterating the MMSE estimation of noise and noise model estimation, the proposed method realizes the simultaneous optimization of both the observed signal model and the noise model. The proposed method significantly outperformed the VTS-based approach, and the maximum improvement in the word error rate was about 12%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-207"
  },
  "leutnant11_interspeech": {
   "authors": [
    [
     "Volker",
     "Leutnant"
    ],
    [
     "Alexander",
     "Krueger"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "A versatile Gaussian splitting approach to non-linear state estimation and its application to noise-robust ASR",
   "original": "i11_1641",
   "page_count": 4,
   "order": 210,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "In this work, a splitting and weighting scheme that allows for splitting a Gaussian density into a Gaussian mixture density (GMM) is extended to allow the mixture components to be arranged along arbitrary directions. The parameters of the Gaussian mixture are chosen such that the GMM and the original Gaussian still exhibit equal central moments up to an order of four. The resulting mixtures' covariances will have eigenvalues that are smaller than those of the covariance of the original distribution, which is a desirable property in the context of nonlinear state estimation, since the underlying assumptions of the extended Kalman filter are better justified in this case. Application to speech feature enhancement in the context of noise-robust automatic speech recognition reveals the beneficial properties of the proposed approach in terms of a reduced word error rate on the Aurora 2 recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-208"
  },
  "pardede11_interspeech": {
   "authors": [
    [
     "Hilman F.",
     "Pardede"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "Generalized-log spectral mean normalization for speech recognition",
   "original": "i11_1645",
   "page_count": 4,
   "order": 211,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "Most compensation methods for robust speech recognition against noise assume independency between speech, additive and convolutive noise. However, the nonlinear nature distortion caused by noise may introduce correlation between noise and speech. To tackle this issue, we propose generalized-log spectral mean normalization (GLSMN) in which log spectral mean normalization (LSMN) is carried out in the q-logarithmic domain. Experiments on the Aurora-2 database show that GLSMN improved speech recognition accuracies by 20% compared to cepstral mean normalization (CMN) in mel-frequency domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-209"
  },
  "kim11b_interspeech": {
   "authors": [
    [
     "Young-Ik",
     "Kim"
    ],
    [
     "Hoon-Young",
     "Cho"
    ],
    [
     "Sang-Hun",
     "Kim"
    ]
   ],
   "title": "Zero-crossing-based channel attentive weighting of cepstral features for robust speech recognition: the ETRI 2011 CHiME challenge system",
   "original": "i11_1649",
   "page_count": 4,
   "order": 212,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "We present a practical and noise-robust speech recognition system which estimates a target-to-interferers power ratio using a zerocrossing- based binaural model and applies the power ratio to a channel attentive missing feature decoder in the cepstral domain. In a natural multisource environment, our binaural model extracts spatial cues at each zero-crossing of a filterbank output signal to localize multiple sound sources and estimates a ratio mask reliably which segregates target speech from interfering noises. Our system uses gammatone filterbank cepstral coefficients (GFCCs) for the recognition and the channel attentive decoder utilizes the ratio mask on weighting the cepstral features when calculating the output probability in the Viterbi decoding. On the experiments of CHiME final testset, our channel attentive GFCC system improves the baseline recognition result 12.2% on average, and with noisy training condition, the average improvement amounts to 18.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-210"
  },
  "kim11c_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Feature compensation for speech recognition in severely adverse environments due to background noise and channel distortion",
   "original": "i11_1653",
   "page_count": 4,
   "order": 213,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "This paper proposes an effective feature compensation scheme to address severely adverse environments for robust speech recognition, where background noise and channel distortion are simultaneously involved. An iterative channel estimation method is integrated into the framework of our Parallel Combined Gaussian Mixture Model (PCGMM) based feature compensation algorithm [1]. A new speech corpus is generated which reflects both additive and convolutional noise corruption. The channel distortion effects are obtained from the NTIMIT and CTIMIT corpora. Evaluation of objective speech quality measures including STNR, PESQ, and speech recognition shows that the generated speech corpus represents highly challenging acoustic conditions for speech recognition. Performance evaluation of the proposed system over the obtained speech corpus demonstrates that the proposed feature compensation scheme is significantly effective at improving speech recognition performance with presence of both background noise and channel distortion, comparing to the conventional methods including the ETSI AFE.\n",
    "",
    "",
    "W. Kim and J.H.L. Hansen, Feature Compensation in the Cepstral Domain Employing Model Combination, Speech Comm., 51(2), pp.83-96, 2009.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-211"
  },
  "ma11b_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Phil D.",
     "Green"
    ]
   ],
   "title": "Binaural cues for fragment-based speech recognition in reverberant multisource environments",
   "original": "i11_1657",
   "page_count": 4,
   "order": 214,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "This paper addresses the problem of speech recognition using distant binaural microphones in reverberant multisource noise conditions. Our scheme employs a two stage fragment decoding approach: first spectro-temporal acoustic source fragments are identified using signal level cues, and second, a hypothesisdriven stage simultaneously searches for the most probable speech/background fragment labelling and the corresponding acoustic model state sequence. The paper reports the first successful attempt to use binaural localisation cues within this framework. By integrating binaural cues and acoustic models in a consistent probabilistic framework, the decoder is able to derive significant recognition performance benefits from fragment location estimates despite their inherent unreliability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-212"
  },
  "joshi11_interspeech": {
   "authors": [
    [
     "Vikas",
     "Joshi"
    ],
    [
     "Raghavendra",
     "Bilgi"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "L.",
     "Garcia"
    ],
    [
     "C.",
     "Benitez"
    ]
   ],
   "title": "Sub-band level histogram equalization for robust speech recognition",
   "original": "i11_1661",
   "page_count": 4,
   "order": 215,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "This paper describes a novel modification of Histogram Equalization (HEQ) approach to robust speech recognition. We propose separate equalization of the high frequency (HF) and low frequency (LF) bands. We study different combinations of the sub-band equalization and obtain best results when we perform a two-stage equalization. First, conventional HEQ is performed on the cepstral features, which does not completely equalize HF and LF bands, even though the overall histogram equalization is good. In the second stage, an equalization is done separately on the HF and the LF components of the above equalized cepstra. We refer to this approach as Sub-band Histogram Equalization (S-HEQ). The new set of features has better equalization of the sub-bands as well as the overall cepstral histogram. Recognition results show a relative improvement of 12% and 15% over conventional HEQ in WER on Aurora-2 and Aurora-4 databases respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-213"
  },
  "remes11_interspeech": {
   "authors": [
    [
     "Ulpu",
     "Remes"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "GMM-based missing-feature reconstruction on multi-frame windows",
   "original": "i11_1665",
   "page_count": 4,
   "order": 216,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "Methods for missing-feature reconstruction substitute noisecorrupted features with clean-speech estimates calculated based on reliable information found in the noisy speech signal. Gaussian mixture model (GMM) based reconstruction has conventionally focussed on reliable information present in a single frame. In this work, GMM-based reconstruction is applied on windows that span several time frames. Mixtures of factor analysers (MFA) are used to limit the number of model parameters needed to describe the feature distribution as window width increases. Using the window-based MFA in noisy speech recognition task resulted in relative error reductions up to 52% compared to frame-based GMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-214"
  },
  "sun11c_interspeech": {
   "authors": [
    [
     "Yang",
     "Sun"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Improvements of a dual-input DBN for noise robust ASR",
   "original": "i11_1669",
   "page_count": 4,
   "order": 217,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "In previous work we have shown that an ASR system consisting of a dual-input Dynamic Bayesian Network (DBN) which simultaneously observes MFCC acoustic features and an exemplar-based Sparse Classification (SC) phoneme predictor stream can achieve better word recognition accuracies in noise than a system that observes only one input stream. This paper explores three modifications of SC input to further improve the noise robustness of the dual-input DBN system: 1) using state likelihoods instead of phonemes, 2) integrating more contextual information and 3) using a complete set of likelihood distribution. Experiments on AURORA-2 reveal that the combination of the first two approaches significantly improves the recognition results, achieving up to 29% (absolute) accuracy gain at SNR -5 dB. In the dual-input system using the full likelihood vector does not outperform using the best state prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-215"
  },
  "gomez11_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Denoising using optimized wavelet filtering for automatic speech recognition",
   "original": "i11_1673",
   "page_count": 4,
   "order": 218,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "We present an improved denoising method based on filtering of the noisy wavelet coefficients using a Wiener gain for automatic speech recognition (ASR). We optimize the wavelet parameters for speech and different noise profiles to achieve a better estimate of the Wiener gain for effective filtering. Moreover, we introduce a scaling parameter in the Wiener gain to minimize mismatch caused by distortion during the denoising process. Experimental results in large vocabulary continuous speech recognition (LVCSR) show that the proposed method is effective and robust to different noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-216"
  },
  "muller11_interspeech": {
   "authors": [
    [
     "Florian",
     "Müller"
    ],
    [
     "Alfred",
     "Mertins"
    ]
   ],
   "title": "Noise robust speaker-independent speech recognition with invariant-integration features using power-bias subtraction",
   "original": "i11_1677",
   "page_count": 4,
   "order": 219,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "This paper presents new results about the robustness of invariantintegration features (IIF) in noisy conditions. Furthermore, it is shown that a feature-enhancement method known as \"power-bias subtraction\" for noisy conditions can be combined with the IIF approach to improve its performance in noisy environments while keeping the robustness of the IIFs to mismatching vocal-tract length training-testing conditions. Results of experiments with training on clean speech only as well as experiments with matchedcondition training are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-217"
  },
  "patil11b_interspeech": {
   "authors": [
    [
     "Hemant A.",
     "Patil"
    ],
    [
     "Pallavi N.",
     "Baljekar"
    ]
   ],
   "title": "Novel VTEO based mel cepstral features for classification of normal and pathological voices",
   "original": "i11_0509",
   "page_count": 4,
   "order": 220,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "In this paper, novel Variable length Teager Energy Operator (VTEO) based Mel cepstral features, viz., VTMFCC are proposed for automatic classification of normal and pathological voices. Experiments have been carried out using this proposed feature set, MFCC and their score-level fusion. Classification was performed using a 2nd order polynomial classifier on a subset of the MEEI database. The equal error rate (EER) on fusion was 3.2% less than EER of MFCC alone which was used as the baseline. Effectiveness of the proposed feature-set was also investigated under degraded conditions using the NOISEX-92 database for babble and high frequency channel noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-218"
  },
  "shimura11_interspeech": {
   "authors": [
    [
     "Eiji",
     "Shimura"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ]
   ],
   "title": "Temporal performance of dysarthric patients in speech and tapping tasks",
   "original": "i11_0513",
   "page_count": 4,
   "order": 221,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "Dysarthria is defined as a locomotor disorder of the vocal speech organ due to a pathological change of nerve and muscle systems. Several methods of speaking rate control have been widely used for the rehabilitation of dysarthria. However, these methods are not always effective depending on the condition of the dysarthric patient. In this study, we investigated the performance of tempo perception of dysarthrias, which has not yet been fully studied. Several types of experiments were conducted for both dysarthric patients and normal subjects. The experiments included speech production and tapping tasks with and without reference samples of utterances or tapping.\n",
    "The experimental results showed that some of the dysarthric subjects exhibited disorders both in the locomotor of the vocal speech organ and in their memory of tempo and rhythm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-219"
  },
  "zhou11_interspeech": {
   "authors": [
    [
     "Xinhui",
     "Zhou"
    ],
    [
     "Maureen",
     "Stone"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "A comparative acoustic study on speech of glossectomy patients and normal subjects",
   "original": "i11_0517",
   "page_count": 4,
   "order": 222,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Tongue cancer patients are treated by glossectomy, a surgical procedure to remove the cancerous tumor. As a result, the tongue properties such as volume, shape, muscle structure, and motility are affected. As a result, the vocal tract acoustics are affected too. This study compares the speech acoustics between normal subjects and partial glossectomy patients with T1 or T2 tumors. The acoustic signal of four vowels (/iy/, /uw/, /eh/, and /ah/) and two fricatives (/s/ and /sh/) were analyzed. Our results show that, while the average formants (F1-F3) for the four vowels between the normal subjects and the glossectomy patients are very similar, the average centers of gravity for the two fricatives differ significantly. These differences in fricatives can be explained by the more posterior constriction in patients due to the glossectomy (or the cancer tumor) and its resulting longer front cavity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-220"
  },
  "alpan11_interspeech": {
   "authors": [
    [
     "Ali",
     "Alpan"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Dysperiodicity analysis of perceptually assessed synthetic speech stimuli",
   "original": "i11_0521",
   "page_count": 4,
   "order": 223,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "The objective is to analyze vocal dysperiodicities in perceptually assessed synthetic speech sounds. The analysis involves a variogram-based method that enables tracking instantaneous vocal dysperiodicities. The dysperiodicity trace is summarized by means of the signal-to-dysperiodicity ratio, which has been shown to correlate strongly with the perceived degree of hoarseness of the speaker. The stimuli have been generated by a synthesizer of disordered voices that has been shown to generate natural-sounding speech fragments comprising diverse vocal perturbations. The speech stimuli have been perceptually assessed by nine listeners according to grade, breathiness and roughness. In previous studies, signal-to-dysperiodicity ratios have been correlated with perceived degrees of hoarseness. The objective here is to extend the analysis to roughness and breathiness. A second objective is to analyze the dependance of the signal-to-dysperiodicity ratio on the signal properties fixed by the synthesizer parameters. Results show a good correlation between signal-to-dysperiodicity ratios and perceptual scores. At most two frequency bands are necessary to predict the perceptual scores. Additive noise contributes most followed by jitter. The interaction between noise parameters, vocal frequency and vowel category contribute moderately or feebly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-221"
  },
  "ghio11_interspeech": {
   "authors": [
    [
     "Alain",
     "Ghio"
    ],
    [
     "Frédérique",
     "Weisz"
    ],
    [
     "Giovanna",
     "Baracca"
    ],
    [
     "Giovanna",
     "Cantarella"
    ],
    [
     "Danièle",
     "Robert"
    ],
    [
     "Virginie",
     "Woisard"
    ],
    [
     "Franco",
     "Fussi"
    ],
    [
     "Antoine",
     "Giovanni"
    ]
   ],
   "title": "Is the perception of voice quality language-dependent? a comparison of French and Italian listeners and dysphonic speakers",
   "original": "i11_0525",
   "page_count": 4,
   "order": 224,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "We present an experiment where voice quality of French and Italian dysphonic speakers was evaluated by French and Italian listeners, specialists in phoniatrics. Results showed that both groups of speakers were perceived in the same way by the two groups of listeners in term of overall severity and breathiness. But the perception of roughness is clearly language dependant. Italian listeners underestimate roughness compare to French listeners. If we link these results obtained in perception with measures obtained in speech production, we can make the hypothesis that it is a case of perception/production adaptation process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-222"
  },
  "orozcoarroyave11_interspeech": {
   "authors": [
    [
     "J. R.",
     "Orozco-Arroyave"
    ],
    [
     "S.",
     "Murillo-Rendón"
    ],
    [
     "A. M.",
     "Álvarez-Meza"
    ],
    [
     "J. D.",
     "Arias-Londoño"
    ],
    [
     "E.",
     "Delgado-Trejos"
    ],
    [
     "J. F.",
     "Vargas-Bonilla"
    ],
    [
     "C. G.",
     "Castellanos-Domínguez"
    ]
   ],
   "title": "Automatic selection of acoustic and non-linear dynamic features in voice signals for hypernasality detection",
   "original": "i11_0529",
   "page_count": 4,
   "order": 225,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "Automatic detection of hypernasality in voices of children with Cleft Lip and Palate (CLP) is made considering two characterization techniques, one based on acoustic, noise and cepstral analysis and other based on nonlinear dynamic features. Besides characterization, two automatic feature selection techniques are implemented in order to find optimal sub-spaces to better discriminate between healthy and hypernasal voices. Results indicate that nonlinear dynamic features are valuable tool for automatic detection of hypernasality; additionally both feature selection techniques show stable and consistent results, achieving accuracy levels of up to 93.73%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-223"
  },
  "reddy11_interspeech": {
   "authors": [
    [
     "Sravana",
     "Reddy"
    ],
    [
     "Evandro",
     "Gouvêa"
    ]
   ],
   "title": "Learning from mistakes: expanding pronunciation lexicons using word recognition errors",
   "original": "i11_0533",
   "page_count": 4,
   "order": 226,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "We introduce the problem of learning pronunciations of out-ofvocabulary words from word recognition mistakes made by an automatic speech recognition (ASR) system. This question is especially relevant in cases where the ASR engine is a black box. meaning that the only acoustic cues about the speech data come from the word recognition outputs. This paper presents an expectation maximization approach to inferring pronunciations from ASR word recognition hypotheses, which outperforms pronunciation estimates of a state of the art grapheme-to-phoneme system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-224"
  },
  "imseng11_interspeech": {
   "authors": [
    [
     "David",
     "Imseng"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Philip N.",
     "Garner"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ]
   ],
   "title": "Improving non-native ASR through stochastic multilingual phoneme space transformations",
   "original": "i11_0537",
   "page_count": 4,
   "order": 227,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "We propose a stochastic phoneme space transformation technique that allows the conversion of conditional source phoneme posterior probabilities (conditioned on the acoustics) into target phoneme posterior probabilities. The source and target phonemes can be in any language and phoneme format such as the International Phonetic Alphabet. The novel technique makes use of a Kullback- Leibler divergence based hidden Markov model and can be applied to non-native and accented speech recognition or used to adapt systems to under-resourced languages. In this paper, and in the context of hybrid HMM/MLP recognizers, we successfully apply the proposed approach to non-native English speech recognition on the HIWIRE dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-225"
  },
  "novotney11_interspeech": {
   "authors": [
    [
     "Scott",
     "Novotney"
    ],
    [
     "Rich",
     "Schwartz"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Unsupervised Arabic dialect adaptation with self-training",
   "original": "i11_0541",
   "page_count": 4,
   "order": 228,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "Useful training data for automatic speech recognition systems of colloquial speech is usually limited to expensive in-domain transcription. Broadcast news is an appealing source of easily available data to bootstrap into a new dialect. However, some languages, like Arabic, have deep linguistic differences resulting in poor cross domain performance. If no in-domain transcripts are available, but a large amount of in-domain audio is, self-training may be a suitable technique to bootstrap into the domain. In this work, we attempt to adapt Modern Standard Arabic (MSA) models to Levantine Arabic without any in-domain manual transcription. We contrast with varying amounts of in-domain transcription and show that 1) Self-training is effective with only one hour of in-domain transcripts. 2) Self-training is not a suitable solution to improve strong MSA models on Levantine. 3) Two metrics that quantify model bias predict self-training success. 4) Model bias explains the failure of self-training to adapt across strong domain mismatch.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-226"
  },
  "seppi11_interspeech": {
   "authors": [
    [
     "Dino",
     "Seppi"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Template-based automatic speech recognition meets prosody",
   "original": "i11_0545",
   "page_count": 4,
   "order": 229,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "In this paper, we use prosodic information to improve the accuracy of our template-based automatic speech recognizer. Prosodic information is harvested adopting a data-driven approach. A number of prosodic features is extracted, then combined into major groups, and finally studied separately and together. All acoustic evidence, both segmental and suprasegmental, is modelled non-parametrically. The different sources of information are conveniently combined with segmental conditional random fields. Prosody enhances the accuracy of the state-of-the-art baseline by reducing the word error rate by 7% relative on the nov92, 20k trigram, Wall Street Journal task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-227"
  },
  "badr11_interspeech": {
   "authors": [
    [
     "Ibrahim",
     "Badr"
    ],
    [
     "Ian",
     "McGraw"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Pronunciation learning from continuous speech",
   "original": "i11_0549",
   "page_count": 4,
   "order": 230,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "This paper explores the use of continuous speech data to learn stochastic lexicons. Building on previous work in which we augmented graphones with acoustic examples of isolated words, we extend our pronunciation mixture model framework to two domains containing spontaneous speech: a weather information retrieval spoken dialogue system and the academic lectures domain. We find that our learned lexicons out-perform expert, hand-crafted lexicons in each domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-228"
  },
  "qian11_interspeech": {
   "authors": [
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "State-level data borrowing for low-resource speech recognition based on subspace GMMs",
   "original": "i11_0553",
   "page_count": 4,
   "order": 231,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "Large vocabulary continuous speech recognition is always a difficult task, and it is particularly so for low-resource languages. The scenario we focus on here is having only 1 hour of acoustic training data in the \"target\" language. This paper presents work on a data borrowing strategy combined with the recently proposed Subspace Gaussian Mixture Model (SGMM). We developed data borrowing strategies based on two approaches: one based on minimizing K-L Divergence, and one that also takes into account state occupation counts. We demonstrate improvements versus the baseline SGMM setup, which itself is better than a conventional HMM-GMM system. The SGMMs are more robustly estimated by borrowing data from the non-target language at the acoustic-state level. Although we tested the approach for SGMMs, we expect the general idea of borrowing data from a non-target language to be applicable for conventional GMMs as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-229"
  },
  "benabderrahmane11_interspeech": {
   "authors": [
    [
     "Y.",
     "Benabderrahmane"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Blind speech separation in multiple environments using a frequency oriented PCA method for convolutive mixtures",
   "original": "i11_0557",
   "page_count": 4,
   "order": 232,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "This paper reports the results of a comparative study on blind speech separation (BSS) of two types of convolutive mixtures. The separation criterion is based on Frequency Oriented Principal Components Analysis (FOPCA). This method is compared to two other well-known methods: the Degenerate Unmixing Evaluation Technique (DUET) and Convolutive Fast Independent Component Analysis (C-FICA). The efficiency of FOPCA is exploited to derive a BSS algorithm for the under-determined case (more speakers than microphones). The FOPCA method is objectively compared in terms of signal-to-interference ratio (SIR) and the Perceptual Evaluation of Speech Quality (PESQ) criteria and subjectively by the Mean Opinion Score (MOS). Usually, the conventional algorithms in the frequency domain are subject to permutation problems. On the other hand, the proposed algorithm has the attractive feature that this inconvenience usually arising does not occur.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-230"
  },
  "koldovsky11_interspeech": {
   "authors": [
    [
     "Zbyněk",
     "Koldovský"
    ],
    [
     "Jiří",
     "Málek"
    ],
    [
     "Petr",
     "Tichavský"
    ]
   ],
   "title": "Blind speech separation in time-domain using block-toeplitz structure of reconstructed signal matrices",
   "original": "i11_0561",
   "page_count": 4,
   "order": 233,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "Methods for Blind Source Separation (BSS) aim at recovering signals from their mixture without prior knowledge about the signals and the mixing system. Among others, they provide tools for enhancing speech signals when they are disturbed by unknown noise or other interfering signals in the mixture. This paper considers a recent time-domain BSS method that is based on a complete decomposition of a signal subspace into components that should be independent. The components are used to reconstruct images of original signals using an ad hoc weighting, which influences the final performance of the method markedly. We propose a novel weighting scheme that utilizes block-Toeplitz structure of signal matrices and relies thus on an established property. We provide experiments with blind speech separation and speech recognition that prove the better performance of the modified BSS method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-231"
  },
  "sarmiento11_interspeech": {
   "authors": [
    [
     "Auxiliadora",
     "Sarmiento"
    ],
    [
     "Iván",
     "Durán"
    ],
    [
     "Sergio",
     "Cruces"
    ],
    [
     "Pablo",
     "Aguilera"
    ]
   ],
   "title": "Generalized method for solving the permutation problem in frequency-domain blind source separation of convolved speech signals",
   "original": "i11_0565",
   "page_count": 4,
   "order": 234,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "The blind speech separation of convolutive mixtures can be performed in the time-frequency domain. The separation problem becomes to a set of instantaneous mixing problems, one for each frequency bin, that can be solved independently by any appropriate instantaneous ICA algorithm. However, the arbitrary order of the estimated sources in each frequency, known as permutation problem, has to be solved to successfully recover the original sources. This paper deals with the permutation problem in the general case of N sources and N observations. The proposed method combines a correlation approach based on the amplitude correlation property of speech signals, and an optimal pairing scheme to align the permuted solutions. Our method is robust to artificially permuted speech signals. Experimental results on simulated convolutive mixtures show the effectiveness of the proposed method in terms of quality of separated signals by objective and perceptual measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-232"
  },
  "grais11_interspeech": {
   "authors": [
    [
     "Emad M.",
     "Grais"
    ],
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Adaptation of speaker-specific bases in non-negative matrix factorization for single channel speech-music separation",
   "original": "i11_0569",
   "page_count": 4,
   "order": 235,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "This paper introduces a speaker adaptation algorithm for nonnegative matrix factorization (NMF) models. The proposed adaptation algorithm is a combination of Bayesian and subspace model adaptation. The adapted model is used to separate speech signal from a background music signal in a single record. Training speech data for multiple speakers is used with NMF to train a set of basis vectors as a general model for speech signals. The probabilistic interpretation of NMF is used to achieve Bayesian adaptation to adjust the general model with respect to the actual properties of the speech signals that is observed in the mixed signal. The Bayesian adapted model is adapted again by a linear transform, which changes the subspace that the Bayesian adapted model spans to better match the speech signal that is in the mixed signal. The experimental results show that combining Bayesian with linear transform adaptation improves the separation results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-233"
  },
  "zhang11f_interspeech": {
   "authors": [
    [
     "Shuhua",
     "Zhang"
    ],
    [
     "Laurent",
     "Girin"
    ]
   ],
   "title": "An informed source separation system for speech signals",
   "original": "i11_0573",
   "page_count": 4,
   "order": 236,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "In two previous papers, we proposed an audio Informed Source Separation (ISS) system which can achieve the separation of I > 2 musical sources from linear instantaneous stationary stereo (2-channel) mixtures, based on audio signal's natural sparsity, pre-mix source signals analysis, and side-information embedding (within the mix signal). In the present paper and for the first time, we apply this system to mixtures of (up to seven) simultaneous speech signals. Compared to the reference MPEG-4 Spatial Audio Object Coding system, our system provides much cleaner separated speech signals (consistently 10.20 dB higher Signal to Interference Ratios), revealing strong potential for audio conference applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-234"
  },
  "tran11_interspeech": {
   "authors": [
    [
     "Ngoc Thuy",
     "Tran"
    ],
    [
     "William",
     "Cowley"
    ],
    [
     "André",
     "Pollok"
    ]
   ],
   "title": "Adaptive blocking beamformer for speech separation",
   "original": "i11_0577",
   "page_count": 4,
   "order": 237,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "This paper tackles the speech separation problem in a meeting room using a new acoustic beamforming method - adaptive blocking (AB) beamformer. The proposed method is an optimum beamforming with a structure similar to the generalized sidelobe canceller (GSC) structure, but simpler. Thus, it inherits the flexibility of GSC and functions well in dynamic environments. We investigate the performance of the proposed method through different experiments and compare the results with a GSC beamformer for minimum variance distortionless response (MVDR). The experimental setups include one wanted speaker, two interferers, air conditioner noise and uncorrelated sensor noise. AB provides improvement over MVDR-GSC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-235"
  },
  "kristensson11_interspeech": {
   "authors": [
    [
     "Per Ola",
     "Kristensson"
    ],
    [
     "Keith",
     "Vertanen"
    ]
   ],
   "title": "Asynchronous multimodal text entry using speech and gesture keyboards",
   "original": "i11_0581",
   "page_count": 4,
   "order": 238,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "We propose reducing errors in text entry by combining speech and gesture keyboard input. We describe a merge model that combines recognition results in an asynchronous and flexible manner. We collected speech and gesture data of users entering both short email sentences and web search queries. By merging recognition results from both modalities, word error rate was reduced by 53% relative for email sentences and 29% relative for web searches. For email utterances with speech errors, we investigated providing gesture keyboard corrections of only the erroneous words. Without the user explicitly indicating the incorrect words, our model was able to reduce the word error rate by 44% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-236"
  },
  "mclaughlin11_interspeech": {
   "authors": [
    [
     "Niall",
     "McLaughlin"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Danny",
     "Crookes"
    ]
   ],
   "title": "Robust bimodal person identification using face and speech with limited training data and corruption of both modalities",
   "original": "i11_0585",
   "page_count": 4,
   "order": 239,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "This paper presents a novel method of audio-visual fusion for person identification where both the speech and facial modalities may be corrupted, and there is a lack of prior knowledge about the corruption. Furthermore, we assume there is a limited amount of training data for each modality (e.g., a short training speech segment and a single training facial image for each person). A new representation and a modified cosine similarity are introduced for combining and comparing bimodal features with limited training data as well as vastly differing data rates and feature sizes. Optimal feature selection and multicondition training are used to reduce the mismatch between training and testing, thereby making the system robust to unknown bimodal corruption. Experiments have been carried out on a bimodal data set created from the SPIDRE and AR databases with variable noise corruption of speech and occlusion in the face images. The new method has demonstrated improved recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-237"
  },
  "youssef11_interspeech": {
   "authors": [
    [
     "Atef Ben",
     "Youssef"
    ],
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Toward a multi-speaker visual articulatory feedback system",
   "original": "i11_0589",
   "page_count": 4,
   "order": 240,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "In this paper, we present recent developments on the HMM-based acoustic-to-articulatory inversion approach that we develop for a \"visual articulatory feedback\" system. In this approach, multistream phoneme HMMs are trained jointly on synchronous streams of acoustic and articulatory data, acquired by electromagnetic articulography (EMA). Acoustic-to-articulatory inversion is achieved in two steps. Phonetic and state decoding is first performed. Then articulatory trajectories are inferred from the decoded phone and state sequence using the maximum-likelihood parameter generation algorithm (MLPG). We introduce here a new procedure for the re-estimation of the HMM parameters, based on the Minimum Generation Error criterion (MGE). We also investigate the use of model adaptation techniques based on maximum likelihood linear regression (MLLR), as a first step toward a multi-speaker visual articulatory feedback system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-238"
  },
  "hueber11_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Elie-Laurent",
     "Benaroya"
    ],
    [
     "Bruce",
     "Denby"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Statistical mapping between articulatory and acoustic data for an ultrasound-based silent speech interface",
   "original": "i11_0593",
   "page_count": 4,
   "order": 241,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "This paper presents recent developments on our \"silent speech interface\" that converts tongue and lip motions, captured by ultrasound and video imaging, into audible speech. In our previous studies, the mapping between the observed articulatory movements and the resulting speech sound was achieved using a unit selection approach. We investigate here the use of statistical mapping techniques, based on the joint modeling of visual and spectral features, using respectively Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM). The prediction of the voiced/unvoiced parameter from visual articulatory data is also investigated using an artificial neural network (ANN). A continuous speech database consisting of one-hour of high-speed ultrasound and video sequences was specifically recorded to evaluate the proposed mapping techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-239"
  },
  "schmalenstroeer11b_interspeech": {
   "authors": [
    [
     "Joerg",
     "Schmalenstroeer"
    ],
    [
     "Florian",
     "Jacob"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Marius H.",
     "Hennecke"
    ],
    [
     "Gernot A.",
     "Fink"
    ]
   ],
   "title": "Unsupervised geometry calibration of acoustic sensor networks using source correspondences",
   "original": "i11_0597",
   "page_count": 4,
   "order": 242,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "In this paper we propose a procedure for estimating the geometric configuration of an arbitrary acoustic sensor placement. It determines the position and the orientation of microphone arrays in 2D while locating a source by direction-of-arrival (DoA) estimation. Neither artificial calibration signals nor unnatural user activity are required. The problem of scale indeterminacy inherent to DoA-only observations is solved by adding time difference of arrival (TDoA) measurements. The geometry calibration method is numerically stable and delivers precise results in moderately reverberated rooms. Simulation results are confirmed by laboratory experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-240"
  },
  "wand11_interspeech": {
   "authors": [
    [
     "Michael",
     "Wand"
    ],
    [
     "Matthias",
     "Janke"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Investigations on speaking mode discrepancies in EMG-based speech recognition",
   "original": "i11_0601",
   "page_count": 4,
   "order": 243,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "In this paper we present our recent study on the impact of speaking mode variabilities on speech recognition by surface electromyography (EMG). Surface electromyography captures the electric potentials of the human articulatory muscles, which enables a user to communicate naturally without making any audible sound. Our previous experiments have shown that the EMG signal varies greatly between different speaking modes, like audibly uttered speech and silently articulated speech. In this study we extend our previous research and quantify the impact of different speaking modes by investigating the amount of mode-specific leaves in phonetic decision trees. We show that this measure correlates highly with discrepancies in the spectral energy of the EMG signal, as well as with differences in the performance of a recognizer on different speaking modes. We furthermore present how EMG signal adaptation by spectral mapping decreases the effect of the speaking mode.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-241"
  },
  "mikolov11_interspeech": {
   "authors": [
    [
     "Tomáš",
     "Mikolov"
    ],
    [
     "Anoop",
     "Deoras"
    ],
    [
     "Stefan",
     "Kombrink"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Empirical evaluation and combination of advanced language modeling techniques",
   "original": "i11_0605",
   "page_count": 4,
   "order": 244,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "We present results obtained with several advanced language modeling techniques, including class based model, cache model, maximum entropy model, structured language model, random forest language model and several types of neural network based language models. We show results obtained after combining all these models by using linear interpolation. We conclude that for both small and moderately sized tasks, we obtain new state of the art results with combination of models, that is significantly better than performance of any individual model. Obtained perplexity reductions against Good-Turing trigram baseline are over 50% and against modified Kneser-Ney smoothed 5-gram over 40%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-242"
  },
  "zweig11_interspeech": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "Personalizing model M for voice-search",
   "original": "i11_0609",
   "page_count": 4,
   "order": 245,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "Model M is a recently proposed class based exponential n-gram language model. In this paper, we extend it with personalization features, address the scalability issues present with large data sets, and test its effectiveness on the Bing Mobile voice-search task. We find that Model M by itself reduces both perplexity and word error rate compared with a conventional model, and that the personalization features produce a further significant improvement. The personalization features provide a very large improvement when the history contains a relevant query; thus the overall effect is gated by the number of times a user re-queries a past request.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-243"
  },
  "shinozaki11_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Yu",
     "Kubota"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Eiji",
     "Utsunomiya"
    ],
    [
     "Yasutaka",
     "Shindoh"
    ]
   ],
   "title": "Sentence selection by direct likelihood maximization for language model adaptation",
   "original": "i11_0613",
   "page_count": 4,
   "order": 246,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "A general framework of language model task adaptation is to select documents in a large training set based on a language model estimated on a development data. However, this strategy has a deficiency that the selected documents are biased to the most frequent patterns in the development data. To address this problem, a new task adaptation method is proposed that selects documents in the training set so as to directly reduce the perplexity on the development set. Moreover, a weighting method to modify the perplexity objective function is proposed to improve the generalization to unseen data. The proposed adaptation methods are evaluated by large vocabulary speech recognition experiments. It is shown that the proposed adaptation with the weighting term produces a compact-size model that gives consistently lower word error rates for different tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-244"
  },
  "arsoy11_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arısoy"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ]
   ],
   "title": "Feature combination approaches for discriminative language models",
   "original": "i11_0617",
   "page_count": 4,
   "order": 247,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "This paper focuses on feature combination approaches for discriminative language models (DLMs). DLM is a feature-based log-linear language modeling approach where the feature parameters are estimated discriminatively. DLM allows for easy integration of various knowledge sources into language modeling. Choosing the proper strategy when combining features coming from different information sources is important. We investigated three approaches for combining lexical, word class, and acoustic features in DLMs. The three approaches are joint parameter estimation, cascade training, and model score combination. The cascade approach is an interesting approach that finally gave the best test set performance, improving the word error rate by 0.49% absolute (3% relative) on transcription of English Broadcast News. The word class features and state duration features were found to be very complementary, and their combination provided most of the improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-245"
  },
  "ananthakrishnan11_interspeech": {
   "authors": [
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Premkumar",
     "Natarajan"
    ]
   ],
   "title": "On-line language model biasing for multi-pass automatic speech recognition",
   "original": "i11_0621",
   "page_count": 4,
   "order": 248,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "a probability distribution over the hypothesis space. In typical use, the LM is trained off-line and remains static at run-time. While cache LMs, dialogue/style adaptation, and information retrieval-based biasing provide some ability for modifying the LM at run-time, they are limited in scope, susceptible to recognition error, place restrictions on the training data and/or test sets, or cannot be implemented for on-line, interactive systems. In this paper, we describe a novel LM biasing method suitable for multi-pass ASR systems. We use k-best lists from the initial recognition pass to obtain a confidence-weighted biasing of the LM training corpus. The latter is used to train a LM biased to the test input. The biased LM is used in the second pass to obtain refined hypotheses either by re-decoding or by re-ranking the k-best list. We sketch an on-line implementation of this scheme that lends itself to integration within low-latency systems. The proposed method is robust to recognition error, and operates on individual utterances without the need for dialogue context. The biased LMs provide significant reduction in perplexity and consistent improvement in word error rate (WER) over unbiased, state-of-the-art, large-vocabulary baseline ASR systems. On the Farsi and English test sets, we obtained relative reductions in perplexity of 24.5% and 31.6%, respectively. Additionally, relative reductions of 1.6% and 1.8% in WER were obtained for large-vocabulary Farsi and English ASR, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-246"
  },
  "kang11_interspeech": {
   "authors": [
    [
     "Moonyoung",
     "Kang"
    ],
    [
     "Tim",
     "Ng"
    ],
    [
     "Long",
     "Nguyen"
    ]
   ],
   "title": "Mandarin word-character hybrid-input neural network language model",
   "original": "i11_0625",
   "page_count": 4,
   "order": 249,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "We applied neural network language model (NNLM) on Chinese by training and testing it on 2011 GALE Mandarin evaluation task. Exploiting the fact that there are no word boundaries in written Chinese, we trained various NNLMs using either word, character, or both, including a word-character hybrid-input NNLM which accepts both word and character as input. Our best result showed up to 0.6% absolute (6.3% relative) Character Error Rate (CER) reduction compared to an un-pruned 4-gram standard language model and 0.2% absolute (2.6% relative) CER reduction compared to a word-based NNLM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-247"
  },
  "sorensen11_interspeech": {
   "authors": [
    [
     "Jeffrey",
     "Sorensen"
    ],
    [
     "Cyril",
     "Allauzen"
    ]
   ],
   "title": "Unary data structures for language models",
   "original": "i11_1425",
   "page_count": 4,
   "order": 250,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "Language models are important components of speech recognition and machine translation systems. Trained on billions of words, and consisting of billions of parameters, language models often are the single largest components of these systems. There have been many proposed techniques to reduce the storage requirements for language models. A technique based upon pointer-free compact storage of ordinal trees shows compression competitive with the best proposed systems, while retaining the full finite state structure, and without using computationally expensive block compression schemes or lossy quantization techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-248"
  },
  "allauzen11_interspeech": {
   "authors": [
    [
     "Cyril",
     "Allauzen"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Bayesian language model interpolation for mobile speech input",
   "original": "i11_1429",
   "page_count": 4,
   "order": 251,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "This paper explores various static interpolation methods for approximating a single dynamically-interpolated language model used for a variety of recognition tasks on the Google Android platform. The goal is to find the statically-interpolated first-pass LM that best reduces search errors in a two-pass system or that even allows eliminating the more complex dynamic second pass entirely. Static interpolation weights that are uniform, prior-weighted, and the maximum likelihood, maximum a posteriori, and Bayesian solutions are considered. Analysis argues and recognition experiments on Android test data show that a Bayesian interpolation approach performs best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-249"
  },
  "sundermeyer11_interspeech": {
   "authors": [
    [
     "Martin",
     "Sundermeyer"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "On the estimation of discount parameters for language model smoothing",
   "original": "i11_1433",
   "page_count": 4,
   "order": 252,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "The goal of statistical language modeling is to find probability estimates for arbitrary word sequences. To obtain non-zero values, the probability distributions found in the training data need to be smoothed. In the widely-used Kneser-Ney family of smoothing algorithms, this is achieved by absolute discounting. The discount parameters can be computed directly using some approximation formulas minimizing the leaving-one-out log-likelihood of the training data.\n",
    "In this work, we outline several shortcomings of the standard estimators for the discount parameters. We propose an efficient method for computing the discount values on held-out data and analyze the resulting parameter estimates. Experiments on large English and French corpora show consistent improvements in perplexity and word error rate over the baseline method. At the same time, this approach can be used for language model pruning, leading to slightly better results than standard pruning algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-250"
  },
  "lehnen11_interspeech": {
   "authors": [
    [
     "Patrick",
     "Lehnen"
    ],
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "N-grams for conditional random fields or a failure-transition(ϕ) posterior for acyclic FSTs",
   "original": "i11_1437",
   "page_count": 4,
   "order": 253,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "Freely available software packages for the training of Conditional Random Fields, e.g. CRF++, do not support longer n-grams than bigram, which can be attributed to the fact that training CRFs in original notation has a polynomial computation time dependence on the target vocabulary size and an exponential dependence on the n-gram size. We transfer the backing-off idea from language models to CRFs. We realized the software with Finite State Transducers, where we modified the calculation of the posterior algorithm. To implement the backing-off scheme, we applied Failure-transitions(Ó) known from Open-FST. Proof of concept is given on the semantic tagging task MEDIA and on the grapheme-tophoneme (G2P) conversion tasks NETtalk and Celex, showing that computational time increases much below the size of the target vocabulary and showing error rate reduction on the G2P tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-251"
  },
  "shaik11_interspeech": {
   "authors": [
    [
     "M. Ali Basha",
     "Shaik"
    ],
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Hybrid language models using mixed types of sub-lexical units for open vocabulary German LVCSR",
   "original": "i11_1441",
   "page_count": 4,
   "order": 254,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "German is a highly inflected language with a large number of words derived from the same root. It makes use of a high degree of word compounding leading to high Out-of-vocabulary (OOV) rates, and Language Model (LM) perplexities. For such languages the use of sub-lexical units for Large Vocabulary Continuous Speech Recognition (LVCSR) becomes a natural choice. In this paper, we investigate the use of mixed types of sub-lexical units in the same recognition lexicon. Namely, morphemic or syllabic units combined with pronunciations called graphones, normal graphemic morphemes or syllables along with full-words. This mixture of units is used for building hybrid LMs suitable for open vocabulary LVCSR where the system operates over an open, constantly changing vocabulary like in broadcast news, political debates, etc. A relative reduction of around 5.0% in Word Error Rate (WER) is obtained compared to a traditional full-words system. Moreover, around 40% of the OOVs are recognized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-252"
  },
  "mousa11_interspeech": {
   "authors": [
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "M. Ali Basha",
     "Shaik"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Morpheme based factored language models for German LVCSR",
   "original": "i11_1445",
   "page_count": 4,
   "order": 255,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "German is a highly inflectional language, where a large number of words can be generated from the same root. It makes a liberal use of compounding leading to high Out-of-vocabulary (OOV) rates, and poor Language Model (LM) probability estimates. Therefore, the use of morphemes for language modeling is considered a better choice for Large Vocabulary Continuous Speech Recognition (LVCSR) than the full-words. Thereby, better lexical coverage and less LM perplexities are achieved. On the other side, the use of Factored Language Models (FLMs) is considered a successful approach that allows the integration of many information sources to get better LM probability estimates. In this paper, we try a combined methodology for language modeling where both morphological decomposition and factored language modeling are used in one model called morpheme based FLM. Finally, we obtain around 2.5% relative reduction in Word Error Rate (WER) with respect to a traditional full-words system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-253"
  },
  "nubaumthom11_interspeech": {
   "authors": [
    [
     "Markus",
     "Nußbaum-Thom"
    ],
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Compound word recombination for German LVCSR",
   "original": "i11_1449",
   "page_count": 4,
   "order": 256,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "Compound words are a difficulty for German speech recognition systems since they cause high out-of-vocabulary and word error rates. State of the art approaches augment the language model by the fragments of compounds in order to increase lexical coverage, lower the perplexity and out-of-vocabulary rate. The fragments are tagged in order to concatenate subsequent equally tagged fragments in the recognition result, but this does not guarantee the recombination of proper words. Such recombination techniques neglect the large vocabulary of the language model training data for recombination although most compounds are covered by it. In this paper, we investigate the use of this vocabulary for the recombination of compound words from the recognition result. The approach is tested on two large vocabulary tasks on top of full-word and fragment based language models and achieves good improvements of 3.7% relative over the baseline compound-sensitive word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-254"
  },
  "kobayashi11_interspeech": {
   "authors": [
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Takahiro",
     "Oku"
    ],
    [
     "Shinichi",
     "Homma"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Lattice-based risk minimization training for unsupervised language model adaptation",
   "original": "i11_1453",
   "page_count": 4,
   "order": 257,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "This paper describes a lattice-based risk minimization training method for unsupervised language model (LM) adaptation. In a broadcast archiving system, unsupervised LM adaptation using transcriptions generated by speech recognition is considered to be useful for improving the performance. However, conventional linear interpolation methods occasionally degrade the performance because of incorrect words in the training transcriptions. Accordingly, we propose a new adaptation method aiming to reflect error information among training lattices. The method minimizes the whole risk of training lattices to yield a log-linear model, which consists of a set of linguistic features. The advantage of the method is that the model parameters can be obtained efficiently in an unsupervised manner. Experimental results obtained in transcribing Japanese broadcast news showed significant word error rate reduction for those of conventional mixture LMs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-255"
  },
  "gillot11_interspeech": {
   "authors": [
    [
     "Christian",
     "Gillot"
    ],
    [
     "Christophe",
     "Cerisara"
    ]
   ],
   "title": "Similarity language model",
   "original": "i11_1457",
   "page_count": 4,
   "order": 258,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "The similarity language model is a statistical model that makes efficient use of long distance information when possible and falls back to standard ngram language model when not. To estimate the probability distribution of a given target context, each training example of the ngram model is retrieved and its similarity to the target context is estimated. In this work, this is done by performing a string alignment and training the system to estimate the similarity of each possible alignment. Whereas in the ngram model all such examples are deemed equal, the more similar an example is to the current context, the more weight it is given in the estimation of the probability distribution. The proposed model outperforms a modified Knener-Ney 4-gram model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-256"
  },
  "dikici11_interspeech": {
   "authors": [
    [
     "Erinç",
     "Dikici"
    ],
    [
     "Murat",
     "Semerci"
    ],
    [
     "Murat",
     "Saraçlar"
    ],
    [
     "Ethem",
     "Alpaydın"
    ]
   ],
   "title": "Data sampling and dimensionality reduction approaches for reranking ASR outputs using discriminative language models",
   "original": "i11_1461",
   "page_count": 4,
   "order": 259,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "This paper investigates various approaches to data sampling and dimensionality reduction for discriminative language models (DLM). Being a feature based language modeling approach, the aim of DLM is to rerank the ASR output with discriminatively trained feature parameters. Using a Turkish morphology based feature set, we examine the use of online Principal Component Analysis (PCA) as a dimensionality reduction method. We exploit ranking perceptron and ranking SVM as two alternative discriminative modeling techniques, and apply data sampling to improve their efficiency. We obtain a reduction in word error rate (WER) of 0.4%, significant at p < 0.001 over the baseline perceptron result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-257"
  },
  "masumura11_interspeech": {
   "authors": [
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Seongjun",
     "Hahm"
    ],
    [
     "Akinori",
     "Ito"
    ]
   ],
   "title": "Training a language model using webdata for large vocabulary Japanese spontaneous speech recognition",
   "original": "i11_1465",
   "page_count": 4,
   "order": 260,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "This paper describes a language modeling method using large-scale spoken language data retrieved from the Web for spontaneous speech recognition. We downloaded 15 million Web pages on a comprehensive range topics. Next, spoken language-like texts were selected from the downloaded Web data using the naive Bayes classifier, and typical linguistic phenomena such as fillers and pauses were added using simulation models. A language model trained by the generated data gave as high performance as the large-scale spontaneous speech corpus (Corpus of Spontaneous Japanese, CSJ). By combining the generated data and CSJ, we improved word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-258"
  },
  "le11_interspeech": {
   "authors": [
    [
     "Hai-Son",
     "Le"
    ],
    [
     "Ilya",
     "Oparin"
    ],
    [
     "Abdel",
     "Messaoudi"
    ],
    [
     "Alexandre",
     "Allauzen"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "François",
     "Yvon"
    ]
   ],
   "title": "Large vocabulary SOUL neural network language models",
   "original": "i11_1469",
   "page_count": 4,
   "order": 261,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "This paper presents continuation of research on Structured OUtput Layer Neural Network language models (SOUL NNLM) for automatic speech recognition. As SOUL NNLMs allow estimating probabilities for all in-vocabulary words and not only for those pertaining to a limited shortlist, we investigate its performance on a large-vocabulary task. Significant improvements both in perplexity and word error rate over conventional shortlist-based NNLMs are shown on a challenging Arabic GALE task characterized by a recognition vocabulary of about 300k entries. A new training scheme is proposed for SOUL NNLMs that is based on separate training of the out-of-shortlist part of the output layer. It enables using more data at each iteration of a neural network without any considerable slow-down in training and brings additional improvements in speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-259"
  },
  "mamou11_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Mamou"
    ],
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Paul",
     "Vozila"
    ]
   ],
   "title": "Improved spoken query transcription using co-occurrence information",
   "original": "i11_1473",
   "page_count": 4,
   "order": 262,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "Spoken queries are a natural medium for searching the Mobile Web. Language modeling for voice search recognition offers different challenges compared to more conventional speech applications. The challenges arise from the fact that spoken queries are usually a set of keywords and do not have a syntactic and grammatical structure. This paper describes a co-occurrence based approach to improve the accuracy of voice queries automatic transcription. With the right choice of scoring function and co-occurrence level, we show that co-occurrence information gives a 2% relative accuracy improvement over a state of the art system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-260"
  },
  "tam11_interspeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Paul",
     "Vozila"
    ]
   ],
   "title": "Unsupervised latent speaker language modeling",
   "original": "i11_1477",
   "page_count": 4,
   "order": 263,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "In commercial speech applications, millions of speech utterances from the field are collected from millions of users, creating a challenge to best leverage the user data to enhance speech recognition performance. Motivated by an intuition that similar users may produce similar utterances, we propose a latent speaker model for unsupervised language modeling. Inspired by latent semantic analysis (LSA), an unsupervised method to extract latent topics from document corpora, we view the accumulated unsupervised text from a user as a document in the corpora. We employ latent Dirichlet-Tree allocation, a tree-based LSA, to organize the latent speakers in a tree hierarchy in an unsupervised fashion. During speaker adaptation, a new speaker model is adapted via a linear interpolation of the latent speaker models. On an in-house evaluation, the proposed method reduces the word error rates by 1.4% compared to a well-tuned baseline with speaker-independent and speaker-dependent adaptation. Compared to a competitive document clustering approach based on the exchange algorithm, our model yields slightly better recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-261"
  },
  "sadeghi11_interspeech": {
   "authors": [
    [
     "Vahid",
     "Sadeghi"
    ]
   ],
   "title": "Laryngealization and breathiness in persian",
   "original": "i11_0629",
   "page_count": 4,
   "order": 264,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "Persian has sequences of two vowels separated by an intervening glottal consonant (/h/ or /P/). The VG(lottal)V sequence becomes reduced in certain occurrences, with the perceptual effect of the loss of the glottal consonant. The purpose of this study is to provide an acoustic description of VGV sequences in reduced forms. A production study examined three acoustic measurements of phonation types: H1-H2, H1-F1, and F0. The measurements were made at 15 ms time intervals throughout the second vowel to determine the time course of phonation effect. The issue of interest is what properties of VGV remain where G is lost. It is shown that /P/ will be preserved as vowel laryngealization and /h/ as breathiness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-262"
  },
  "muller11b_interspeech": {
   "authors": [
    [
     "Viola",
     "Müller"
    ],
    [
     "Jonathan",
     "Harrington"
    ],
    [
     "Felicitas",
     "Kleber"
    ],
    [
     "Ulrich",
     "Reubold"
    ]
   ],
   "title": "Age-dependent differences in the neutralization of the intervocalic voicing contrast: evidence from an apparent-time study on east franconian",
   "original": "i11_0633",
   "page_count": 4,
   "order": 265,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "The main aim of the present study was to investigate the extent to which East Franconian speakers neutralize the voicing opposition in intervocalic stops when they produce a variety of Standard German. A second aim was to test whether young and old speakers differ in their extent of neutralization and tend to a more standard-like pronunciation. We analyzed contrast maintenance by means of the vowel-to-stop duration ratio. An acoustic analysis of leiden-leiten revealed that old East Franconian speakers neutralized the voicing contrast either completely or to a greater extent than young East Franconian speakers. Young East Franconian speakers preserved the voicing contrast, although to a lesser extent than the Standard German speakers. A forced choice perception experiment showed that young but not old East Franconians perceived the lenis/fortis contrast. The results point to a sound change in progress in which a phonemic [± voice] stop distinction is developing in East Franconian.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-263"
  },
  "samlowski11_interspeech": {
   "authors": [
    [
     "Barbara",
     "Samlowski"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Comparing syllable frequencies in corpora of written and spoken language",
   "original": "i11_0637",
   "page_count": 4,
   "order": 266,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "In this study, various German language corpora were compared in order to discover the extent to which syllable frequencies remain stable across different contexts and modalities. Although considerable differences in relative frequency were found among the more common syllables, rank numbers proved to be more robust. Variation across corpora was mostly due to vocabulary characteristics of particular corpus domains rather than to systematic differences between spoken and written language. The results indicate that syllable frequencies in written corpora can be taken as a rough estimate for their frequency in spoken language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-264"
  },
  "iacoponi11_interspeech": {
   "authors": [
    [
     "Luca",
     "Iacoponi"
    ],
    [
     "Renata",
     "Savy"
    ]
   ],
   "title": "Sylli: automatic phonological syllabification for Italian",
   "original": "i11_0641",
   "page_count": 4,
   "order": 267,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "We will present a complete syllabifier for Italian (Sylli), that is based on phonological principles, flexible and easy to adapt for other uses, alphabets and languages. Crucial concepts regarding syllabification principles in modern phonological theory will be discussed (§1.1); specific issues concerning Italian syllabification will then be summarised (§1.2) and an overview of the available automatic syllabification models will be provided (§1.3). We will then move on to describe the program structure, the syllabification algorithm and two particular issues concerning syllabification in Italian ($2). Finally, we will illustrate the results of a manual syllabification test carried out by linguists to verify the accuracy of the algorithm (§3).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-265"
  },
  "xavier11_interspeech": {
   "authors": [
    [
     "André N.",
     "Xavier"
    ],
    [
     "Plínio A.",
     "Barbosa"
    ]
   ],
   "title": "A preliminary study on the production of signs in brazilian sign language when one of the manual articulators is unavailable",
   "original": "i11_0645",
   "page_count": 4,
   "order": 268,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "This paper aims at discussing the realization of some Brazilian Sign Language signs, articulated with both hands, when one of them is unavailable. As will be discussed, this unavailability is caused by extra-linguistic factors, as well as by a linguistic one. The data considered here were collected through the observation of spontaneous signing and discussed with three subjects. Their analysis revealed that the production of two-handed signs when one of the hands is not available does not simply consist of realizing them with only one hand, but alternatively employing other strategies, such as using a one-handed sign equivalent in meaning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-266"
  },
  "pan11_interspeech": {
   "authors": [
    [
     "Ho-hsien",
     "Pan"
    ],
    [
     "Mao-Hsu",
     "Chen"
    ],
    [
     "Shao-Ren",
     "Lyu"
    ]
   ],
   "title": "Electroglottograph and acoustic cues for phonation contrasts in taiwan min falling tones",
   "original": "i11_0649",
   "page_count": 4,
   "order": 269,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "This study explored the effective articulatory and acoustic parameters for distinguishing Taiwan Min falling unchecked tones 53 and 31 and checked tones 5 and 3. Data were collected from Zhangzhou, Quanzhou, and mixed accents in northern, central, and southern Taiwan. Results showed that EGG parameters, Contact Quotients (CQ) and Peak Increase in Contact (PIC) were not effective in distinguishing checked from unchecked tones across speakers. In contrast, f0 contour and Cepstral Peak Prominence (CPP) consistently distinguished checked tones from unchecked tones across speakers. The f0 onset was highest for tone 53, followed in order by tone 3, 5 and 31. The f0 contours of tone 5 were the highest in the later half of the vowels. CPP measures of checked tones were higher than those of unchecked tones in the latter portion of vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-267"
  },
  "saito11_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Keisuke",
     "Yamamoto"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "One-to-many voice conversion based on tensor representation of speaker space",
   "original": "i11_0653",
   "page_count": 4,
   "order": 270,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "This paper describes a novel approach to flexible control of speaker characteristics using tensor representation of speaker space. In voice conversion studies, realization of conversion from/to an arbitrary speaker's voice is one of the important objectives. For this purpose, eigenvoice conversion (EVC) based on an eigenvoice Gaussian mixture model (EV-GMM) was proposed. In the EVC, similarly to speaker recognition approaches, a speaker space is constructed based on GMM supervectors which are high-dimensional vectors derived by concatenating the mean vectors of each of the speaker GMMs. In the speaker space, each speaker is represented by a small number of weight parameters of eigen-supervectors. In this paper, we revisit construction of the speaker space by introducing the tensor analysis of training data set. In our approach, each speaker is represented as a matrix of which the row and the column respectively correspond to the Gaussian component and the dimension of the mean vector, and the speaker space is derived by the tensor analysis of the set of the matrices. Our approach can solve an inherent problem of supervector representation, and it improves the performance of voice conversion. Experimental results of one-to-many voice conversion demonstrate the effectiveness of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-268"
  },
  "qiao11_interspeech": {
   "authors": [
    [
     "Yu",
     "Qiao"
    ],
    [
     "Tong",
     "Tong"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "A study on bag of Gaussian model with application to voice conversion",
   "original": "i11_0657",
   "page_count": 4,
   "order": 271,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "The GMM based mapping techniques proved to be an efficient method to find nonlinear regression function between two spaces, and found success in voice conversion. In these methods, a linear transformation is estimated for each Gaussian component, and the final conversion function is a weighted summation of all linear transformations. These linear transformations fit well for the samples near to the center of at least one Gaussian component, but may not deal well with the samples far from the centers of all Gaussian distributions. To overcome this problem, this paper proposes Bag of Gaussian Model (BGM). BGM model consists of two types of Gaussian distributions, namely basic and complex distributions. Compared with classical GMM, BGM is adaptive for samples. That is for a sample, BGM can select a set of Gaussian distributions which fit the sample best. We develop a data-driven method to construct BGM model and show how to estimate regression function with BGM. We carry out experiment on voice conversion tasks. The experimental results exhibit the usefulness of BGM based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-269"
  },
  "li11b_interspeech": {
   "authors": [
    [
     "Lei",
     "Li"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "A Bayesian approach to voice conversion based on GMMs using multiple model structures",
   "original": "i11_0661",
   "page_count": 4,
   "order": 272,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "A spectral conversion method using multiple Gaussian Mixture Models (GMMs) based on the Bayesian framework is proposed. A typical spectral conversion framework is based on a GMM. However, in this conventional method, a GMM-appropriate number of mixtures is dependent on the amount of training data, and thus the number of mixtures should be determined beforehand. In the proposed method, the variational Bayesian approach is applied to GMM-based voice conversion, and multiple GMMs are integrated as a single statistical model. Appropriate model structures are stochastically selected for each frame based on the Bayesian frame work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-270"
  },
  "eslami11_interspeech": {
   "authors": [
    [
     "Mahdi",
     "Eslami"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Abolghasem",
     "Sayadiyan"
    ]
   ],
   "title": "Quality improvement of voice conversion systems based on trellis structured vector quantization",
   "original": "i11_0665",
   "page_count": 4,
   "order": 273,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "Common voice conversion systems employ a spectral / time domain mapping to convert speech from one speaker to another. The speech quality of conversion methods does not sound natural because the spectral / time domain patterns of two speakers' speech do not match completely. In this paper we propose a method that uses inter-frame (dynamic) characteristics in addition to intra-frame characteristics to find the converted speech frames. This method is based on VQ and uses a trellis structure to find the best conversion function. The proposed method provides high quality converted voice, low computational complexity and small trained model size in contrast to other common methods. Subjective and objective evaluations are employed to demonstrate the superiority of the proposed method over the VQ-based and GMM-based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-271"
  },
  "benisty11_interspeech": {
   "authors": [
    [
     "Hadas",
     "Benisty"
    ],
    [
     "David",
     "Malah"
    ]
   ],
   "title": "Voice conversion using GMM with enhanced global variance",
   "original": "i11_0669",
   "page_count": 4,
   "order": 274,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "The goal of voice conversion is to transform a sentence said by one speaker, to sound as if another speaker had said it. The classical conversion based on a Gaussian Mixture Model and several other schemes suggested since, produce muffled sounding outputs, due to excessive smoothing of the spectral envelopes. To reduce the muffling effect, enhancement of the Global Variance (GV) of the spectral features was recently suggested. We propose a different approach for GV enhancement, based on the classical conversion formalized as a GV-constrained minimization. Listening tests show that an improvement in quality is achieved by the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-272"
  },
  "godoy11_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Godoy"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Thierry",
     "Chonavel"
    ]
   ],
   "title": "Spectral envelope transformation using DFW and amplitude scaling for voice conversion with parallel or nonparallel corpora",
   "original": "i11_0673",
   "page_count": 4,
   "order": 275,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "Dynamic Frequency Warping (DFW) offers an appealing alternative to GMM-based voice conversion, which suffers from \"over-smoothing\" that hinders speech quality. However, to adjust spectral power after DFW, previous work returns to GMMtransformation. This paper proposes a more effective DFW with amplitude scaling (DFWA) that functions on the acoustic class level and is independent of GMM-transformation. The amplitude scaling compares average target and warped source log amplitude spectra for each class. DFWA outperforms the GMM in terms of both speech quality and timbre conversion, as confirmed in objective and subjective testing. Moreover, DFWA performance is equivalent using parallel or nonparallel corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-273"
  },
  "li11c_interspeech": {
   "authors": [
    [
     "Xiao",
     "Li"
    ],
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "Multi-task learning for spoken language understanding with shared slots",
   "original": "i11_0701",
   "page_count": 4,
   "order": 276,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "This paper addresses the problem of learning multiple spoken language understanding (SLU) tasks that have overlapping sets of slots. In such a scenario, it is possible to achieve better slot filling performance by learning multiple tasks simultaneously, as opposed to learning them independently. We focus on presenting a number of simple multi-task learning algorithms for slot filling systems based on semi-Markov CRFs, assuming the knowledge of shared slots. Furthermore, we discuss an intra-domain clustering method that automatically discovers shared slots from training data. The effectiveness of our proposed approaches is demonstrated in an SLU application that involves three different yet related tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-274"
  },
  "hillard11_interspeech": {
   "authors": [
    [
     "Dustin",
     "Hillard"
    ],
    [
     "Asli",
     "Celikyilmaz"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "Learning weighted entity lists from web click logs for spoken language understanding",
   "original": "i11_0705",
   "page_count": 4,
   "order": 277,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "Named entity lists provide important features for language understanding, but typical lists can contain many ambiguous or incorrect phrases. We present an approach for automatically learning weighted entity lists by mining user clicks from web search logs. The approach significantly outperforms multiple baseline approaches and the weighted lists improve spoken language understanding tasks such as domain detection and slot filling. Our methods are general and can be easily applied to large quantities of entities, across any number of lists.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-275"
  },
  "hakkanitur11_interspeech": {
   "authors": [
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Bootstrapping domain detection using query click logs for new domains",
   "original": "i11_0709",
   "page_count": 4,
   "order": 278,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "Domain detection in spoken dialog systems is usually treated as a multi-class, multi-label classification problem, and training of domain classifiers requires collection and manual annotation of example utterances. In order to extend a dialog system to new domains in a way that is seamless for users, domain detection should be able to handle utterances from the new domain as soon as it is introduced. In this work, we propose using web search query logs, which include queries entered by users and the links they subsequently click on, to bootstrap domain detection for new domains. While sampling user queries from the query click logs to train new domain classifiers, we introduce two types of measures based on the behavior of the users who entered a query and the form of the query. We show that both types of measures result in reductions in the error rate as compared to randomly sampling training queries. In controlled experiments over five domains, we achieve the best gain from the combination of the two types of sampling criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-276"
  },
  "celikyilmaz11_interspeech": {
   "authors": [
    [
     "Asli",
     "Celikyilmaz"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "Approximate inference for domain detection in spoken language understanding",
   "original": "i11_0713",
   "page_count": 4,
   "order": 279,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "This paper presents a semi-latent topic model for semantic domain detection in spoken language understanding systems. We use labeled utterance information to capture latent topics, which directly correspond to semantic domains. Additionally, we introduce an 'informative prior' for Bayesian inference that can simultaneously segment utterances of known domains into classes and divide them from out-of-domain utterances. We show that our model generalizes well on the task of classifying spoken language utterances and compare its results to those of an unsupervised topic model, which does not use labeled information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-277"
  },
  "huang11d_interspeech": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Speech indexing using semantic context inference",
   "original": "i11_0717",
   "page_count": 4,
   "order": 280,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "This study presents a novel approach to spoken document retrieval based on semantic context inference for speech indexing. Each recognized term in a spoken document is mapped onto a semantic inference vector containing a bag of semantic terms through a semantic relation matrix. The semantic context inference vector is then constructed by summing up all the semantic inference vectors. Such a semantic term expansion and re-weighting make the semantic context inference vector a suitable representation for speech indexing. The experiments were conducted on 1550 anchor news stories collected from Mandarin Chinese broadcast news of 198 hours. The experimental results indicate that the proposed speech indexing using the semantic context inference contributes to a substantial performance improvement of spoken document retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-278"
  },
  "ju11_interspeech": {
   "authors": [
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Jasha",
     "Droppo"
    ]
   ],
   "title": "Automatically optimizing utterance classification performance without human in the loop",
   "original": "i11_0721",
   "page_count": 4,
   "order": 281,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "The Utterance Classification (UC) method has become a developer's choice over traditional Context Free Grammars (CFGs) for voice menus in telephony applications. This data driven method achieves higher accuracy and has great potential to utilize a huge amount of labeled training data. But, having a human manually label the training data can be expensive. This paper provides a robust recipe for training a UC system using inexpensive acoustic data with limited transcriptions or semantic labels. It also describes two new algorithms that use caller confirmation, which naturally occurred within a dialog, to generate pseudo semantic labels. Experimental results show that, after having sufficient labeled data to achieve a reasonable accuracy, both of our algorithms can use unlabeled data to achieve the same performance as a system trained with labeled data, while completely eliminating the need for human supervision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-279"
  },
  "boulademareuil11_interspeech": {
   "authors": [
    [
     "Philippe",
     "Boula de Mareüil"
    ],
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Manuela",
     "Yapomo"
    ]
   ],
   "title": "In search of cues discriminating West-african accents in French",
   "original": "i11_0725",
   "page_count": 4,
   "order": 282,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "This study investigates to what extent West-African French accents can be distinguished, based on recordings made in Burkina Faso, Ivory Coast, Mali and Senegal. First, a perceptual experiment was conducted, suggesting that these accents are well identified by West-African listeners (especially the Senegal and Ivory Coast accents). Second, prosodic and segmental cues were studied by using speech processing methods such as automatic phoneme alignment. Results show that the Senegal accent (with a tendency toward word-initial stress followed by a falling pitch movement) and the Ivory Coast accent (with a tendency to delete/vocalise the /R/ consonant) are most distinct from standard French and among the West-African accents under investigation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-280"
  },
  "hanani11_interspeech": {
   "authors": [
    [
     "Abualsoud",
     "Hanani"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Michael J.",
     "Carey"
    ]
   ],
   "title": "Computer and human recognition of regional accents of british English",
   "original": "i11_0729",
   "page_count": 4,
   "order": 283,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "This paper is concerned with classification of the 14 regional accents of British English in the ABI (Accents of the British Isles) speech corpus. Results are reported using a state-of-the-art Language Identification system, variants of Huckvale's ACCDIST system, and human listeners. The best performance, 95.18% accuracy, is obtained using the text-dependent ACCDIST measure. The performance of a conventional (text-independent) acoustic Language Identification system is poor, but is improved significantly (89.6% accuracy) by the addition of phone sequence information. Human performance (58.25% accuracy) is much lower than expected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-281"
  },
  "tong11_interspeech": {
   "authors": [
    [
     "Rong",
     "Tong"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Target-aware lattice rescoring for dialect recognition",
   "original": "i11_0733",
   "page_count": 4,
   "order": 284,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "We observed that human listeners distinguish one dialect from another by paying special attention to some particular phonetic and/or phonotactic patterns. Motivated by this observation, we propose a technique that emulates this process. We explore a target-aware lattice rescoring (TALR) process that revises the n-gram statistics in a lattice with target dialect information. We then derive n-gram statistics as the phonotactic features from the lattice and develop a system under the vector space modeling framework. The experiment results show that the proposed technique consistently improves dialect recognition performance on 30-second test utterances. We achieved equal error rates (EERs) of 4.57% and 13.28% with 3-gram statistics for Chinese and English dialect recognition in 2007 NIST Language Recognition Evaluation 30-second closed test sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-282"
  },
  "akbacak11_interspeech": {
   "authors": [
    [
     "Murat",
     "Akbacak"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Arindam",
     "Mandal"
    ]
   ],
   "title": "Effective Arabic dialect classification using diverse phonotactic models",
   "original": "i11_0737",
   "page_count": 4,
   "order": 285,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "We study the effectiveness of recently developed language recognition techniques based on speech recognition models for the discrimination of Arabic dialects. Specifically, we investigate dialect-specific and cross-dialectal phonotactic models, using both language models and support vector machines (SVMs). Techniques are evaluated both alone and in combination with a cepstral system with joint factor analysis (JFA), using a four-dialect data set employing 30-second telephone speech samples. We find good complementarity from different features and modeling paradigms, and achieve 2% average equal error rate for pairwise classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-283"
  },
  "chen11b_interspeech": {
   "authors": [
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Joseph P.",
     "Campbell"
    ]
   ],
   "title": "Characterizing deletion transformations across dialects using a sophisticated tying mechanism",
   "original": "i11_0741",
   "page_count": 4,
   "order": 286,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "In this work, we propose extensions of our Phone-based Pronunciation Model (PPM) for analyzing dialect differences. We compared these systems using 3 metrics and 2 datasets of English. Empirical results suggest that (1) sophisticated tying is suitable in modeling deletion transformations across dialects, beating standard tying by 33% relative, and (2) APM (Acoustic-based Pronunciation Model) improves performance in generating dialect-specific pronunciations, dialect identification and rule retrieval, achieving relative gains beyond 34%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-284"
  },
  "biadsy11_interspeech": {
   "authors": [
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Dialect and accent recognition using phonetic-segmentation supervectors",
   "original": "i11_0745",
   "page_count": 4,
   "order": 287,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "We describe a new approach to automatic dialect and accent recognition which exceeds state-of-the-art performance in three recognition tasks. This approach improves the accuracy and substantially lower the time complexity of our earlier phoneticbased kernel approach for dialect recognition. In contrast to state-of-the-art acoustic-based systems, our approach employs phone labels and segmentation to constrain the acoustic models. Given a speaker's utterance, we first obtain phone hypotheses using a phone recognizer and then extract GMM-supervectors for each phone type, effectively summarizing the speaker's phonetic characteristics in a single vector of phone-type supervectors. Using these vectors, we design a kernel function that computes the phonetic similarities between pairs of utterances to train SVM classifiers to identify dialects. Comparing this approach to the state-of-the-art, we obtain a 12.9% relative improvement in EER on Arabic dialects, and a 17.9% relative improvement for American vs. Indian English dialects. We also see a 53.5% relative improvement over a GMM-UBM on American Southern vs. Non-Southern English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-285"
  },
  "miyazawa11_interspeech": {
   "authors": [
    [
     "Kouki",
     "Miyazawa"
    ],
    [
     "Hideaki",
     "Miura"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Reiko",
     "Mazuka"
    ]
   ],
   "title": "The multi timescale phoneme acquisition model of the self-organizing based on the dynamic features",
   "original": "i11_0749",
   "page_count": 4,
   "order": 288,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "It is unclear as to how infants learn the acoustic expression of each phoneme of their native languages. In recent studies, researchers have inspected phoneme acquisition by using a computational model. However, these studies have used a limited vocabulary as input and do not handle a continuous speech that is almost comparable to a natural environment. Therefore, we use a natural continuous speech and build a self-organization model that simulates the cognitive ability of the humans, and we analyze the quality and quantity of the speech information that is necessary for the acquisition of the native phoneme system. Our model is designed to learn values of the acoustic features of a continuous speech and to estimate the number and boundaries of the phoneme categories without using explicit instructions. In a recent study, our model could acquire the detailed vowels of the input language. In this study, we examined the mechanism necessary for an infant to acquire all the phonemes of a language, including consonants. In natural speech, vowels have a stationary feature; hence, our recent model is suitable for learning them. However, learning consonants through the past model is difficult because most consonants have more dynamic features than vowels. To solve this problem, we designed a method to separate \"stable\" and \"dynamic\" speech patterns using a feature-extraction method based on the auditory expressions used by human beings. Using this method, we showed that the acquisition of an unstable phoneme was possible without the use of instructions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-286"
  },
  "brown11_interspeech": {
   "authors": [
    [
     "Helen",
     "Brown"
    ],
    [
     "M. Gareth",
     "Gaskell"
    ]
   ],
   "title": "The time-course of talker-specificity effects for newly-learned pseudowords: evidence for a hybrid model of lexical representation",
   "original": "i11_0753",
   "page_count": 4,
   "order": 289,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "Whilst research shows that talker information affects recognition of recently studied words, it remains unclear whether this information is stored in long-term memory. Three experiments explored whether talker-specificity effects (TSEs) for pseudowords changed over time and were affected by within- and between-talker variability during study. Results showed TSEs immediately after study in all experiments, consistent with episodic models, but TSEs remained a week later only for pseudowords studied in a single voice. Furthermore, source memory data suggested that talker information becomes less accessible over time, supporting hybrid models that incorporate aspects of both episodic and abstract lexical representation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-287"
  },
  "lintfert11_interspeech": {
   "authors": [
    [
     "Britta",
     "Lintfert"
    ],
    [
     "Antje",
     "Schweitzer"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "A parametric approach to intonation acquisition research: validation on child-directed speech data",
   "original": "i11_0757",
   "page_count": 4,
   "order": 290,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "This paper validates a parametric approach to intonation acquisition research [1] using child-directed speech data. An advantage of this approach is that it can be used for studying child speech as well as adult speech. Within the field of prosody acquisition it reconciles independent approaches to child prosody with ToBI-based approaches. In this paper we substantiate this claim by showing that clusters of parameterized contours obtained from German child-directed speech correlate with GToBI(S) categories, and by elaborating how, alternatively, the parameters can be mapped to properties that are relevant in independent approaches.\n",
    "",
    "",
    "B. Lintfert, A. Schweitzer, L. Wolski, and B. M¨obius, Quantifying developmental changes of prosodic categories, in Proceedings of Speech Prosody 2010, Chicago, Illinois, 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-288"
  },
  "versteegh11_interspeech": {
   "authors": [
    [
     "Maarten",
     "Versteegh"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Modelling novelty preference in word learning",
   "original": "i11_0761",
   "page_count": 4,
   "order": 291,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "This paper investigates the effects of novel words on a cognitively plausible computational model of word learning. The model is first familiarized with a set of words, achieving high recognition scores and subsequently offered novel words for training. We show that the model is able to recognize the novel words as different from the previously seen words, based on a measure of novelty that we introduce. We then propose a procedure analogous to novelty preference in infants. Results from simulations of word learning show that adding this procedure to our model speeds up training and helps the model attain higher recognition rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-289"
  },
  "ananthakrishnan11b_interspeech": {
   "authors": [
    [
     "G.",
     "Ananthakrishnan"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Using imitation to learn infant-adult acoustic mappings",
   "original": "i11_0765",
   "page_count": 4,
   "order": 292,
   "p1": "765",
   "pn": "768",
   "abstract": [
    "This paper discusses a model which conceptually demonstrates how infants could learn the normalization between infant-adult acoustics. The model proposes that the mapping can be inferred from the topological correspondences between the adult and infant acoustic spaces, that are clustered separately in an unsupervised manner. The model requires feedback from the adult in order to select the right topology for clustering, which is a crucial aspect of the model. The feedback is in terms of an overall rating of the imitation effort by the infant, rather than a frame-by-frame correspondence. Using synthetic, but continuous speech data, we demonstrate that clusters, which have a good topological correspondence, are perceived to be similar by a phonetically trained listener.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-290"
  },
  "bergmann11_interspeech": {
   "authors": [
    [
     "Christina",
     "Bergmann"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Thresholding word activations for response scoring - modelling psycholinguistic data",
   "original": "i11_0769",
   "page_count": 4,
   "order": 293,
   "p1": "769",
   "pn": "772",
   "abstract": [
    "In the present paper we replicate simulations of infant word learning and the effect of variation in the input. We then investigate to what extent the results are influenced by the way in which the continuous response functions are treated and what effects the use of thresholds can have on the data. Our results show that the underlying response pattern, as uncovered by different thresholds, varies greatly. Nonetheless, the overall output of the model is often correct and able to generalise to unseen data. Thus, we show that the model can give correct responses even in uncertain circumstances. Links of this finding to language acquisition research are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-291"
  },
  "misu11_interspeech": {
   "authors": [
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Kiyonori",
     "Ohtake"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "User study of spoken decision support system",
   "original": "i11_0797",
   "page_count": 4,
   "order": 294,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "This paper presents the results of the user evaluation of spoken decision support dialogue systems, which help users select from a set of alternatives. Thus far, we have modeled this decision support dialogue as a partially observable Markov decision process (POMDP), and optimized its dialogue strategy to maximize the value of the user's decision. In this paper, we present a comparative evaluation of the optimized dialogue strategy with several baseline strategies, and demonstrate that the optimized dialogue strategy that was effective in user simulation experiments works well in an evaluation by real users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-292"
  },
  "raux11_interspeech": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ],
    [
     "Yi",
     "Ma"
    ]
   ],
   "title": "Efficient probabilistic tracking of user goal and dialog history for spoken dialog systems",
   "original": "i11_0801",
   "page_count": 4,
   "order": 295,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "In this paper, we describe Dynamic Probabilistic Ontology Trees, a new probabilistic model to track dialog state in a dialog system. Our model captures both the user goal and the history of user dialog acts using a unified Bayesian Network. We perform efficient inference using a form of blocked Gibbs sampling designed to exploit the structure of the model. Evaluation on a corpus of dialogs from the CMU Let's Go system shows that our approach significantly outperforms a deterministic baseline, exploiting long N-best lists without loss of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-293"
  },
  "schmitt11_interspeech": {
   "authors": [
    [
     "Alexander",
     "Schmitt"
    ],
    [
     "Alexander",
     "Zgorzelski"
    ],
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Tackling a shilly-shally classifier for predicting task success in spoken dialogue interaction",
   "original": "i11_0805",
   "page_count": 4,
   "order": 296,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "Statistical models, which predict that a task with a telephone-based Spoken Dialogue System (SDS) is unlikely to be completed, can be useful to adapt dialogue strategies. They can also trigger the decision to route callers directly to human assistance once it is clear that the SDS cannot automate the call. This paper addresses a number of issues that arise when deploying such models. We show that the predictions of a model are subject to strong variations between several adjacent dialogue steps. As a consequence, we show that the accuracy can be significantly risen when using sequences of equal predictions as basis of the decision-making. Furthermore, we implement a confidence metric that takes into account the certainty of the classifier to determine the optimum decision point.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-294"
  },
  "meguro11_interspeech": {
   "authors": [
    [
     "Toyomi",
     "Meguro"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Ryuichiro",
     "Higashinaka"
    ],
    [
     "Kohji",
     "Dohsaka"
    ]
   ],
   "title": "Evaluation of listening-oriented dialogue control rules based on the analysis of HMMs",
   "original": "i11_0809",
   "page_count": 4,
   "order": 297,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "We have been working on listening-oriented dialogues for the purpose of building listening agents. In our previous work [1], we trained hidden Markov models (HMMs) from listening-oriented dialogues (LoDs) between humans, and by analyzing them, discovered a distinguishing dialogue flow of LoD. For example, listeners suppress their information giving and self-disclosure, and instead, increase acknowledgments and questions to elicit speakers' utterances. As an initial step for building listening agents, we decided to create dialogue control rules based on our analysis of the HMMs. We built our rule-based system and compared it with three other systems by a Wizard of Oz (WoZ) experiment. As a result, we found that our rule-based system achieved as much user satisfaction as human listeners.\n",
    "",
    "",
    "T. Meguro, R. Higashinaka, K. Dohsaka, Y. Minami, and H. Isozaki, Analysis of listening-oriented dialogue for building listening agents, in Proc. SIGDIAL, 2009, pp. 124127.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-295"
  },
  "suendermann11_interspeech": {
   "authors": [
    [
     "D.",
     "Suendermann"
    ],
    [
     "J.",
     "Liscombe"
    ],
    [
     "J.",
     "Bloom"
    ],
    [
     "G.",
     "Li"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Large-scale experiments on data-driven design of commercial spoken dialog systems",
   "original": "i11_0813",
   "page_count": 4,
   "order": 298,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "The design of commercial spoken dialog systems is most commonly based on hand-crafting call flows. Voice interaction designers write prompts, predict caller responses, set speech recognition parameters, implement interaction strategies, all based on \"best design practices\". Recently, we presented the mathematical framework \"Contender\" (similar to reinforcement learning) that allows for replacing manual decisions made during system design by datadriven soft decisions made at system run time optimizing the cumulative reward of an application. The current paper reports on the results of 26 Contenders implemented in commercial applications processing a total of about 15 million calls.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-296"
  },
  "kronlid11_interspeech": {
   "authors": [
    [
     "Fredrik",
     "Kronlid"
    ],
    [
     "Jessica",
     "Villing"
    ],
    [
     "Alexander",
     "Berman"
    ],
    [
     "Staffan",
     "Larsson"
    ]
   ],
   "title": "Comparing system-driven and free dialogue in in-vehicle interaction",
   "original": "i11_0817",
   "page_count": 4,
   "order": 299,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "It is widely held that a free, natural dialogue model is more efficient and less distracting than system-initiative, state based dialogue. This paper describes an evaluation of two systems - one using system-directed dialogue and one using a more \"free\" dialogue . focusing on distraction and efficiency. The level of distraction is measured using an automotive industry standard test (LCT), and the efficiency is measured by counting the number of completed tasks. The efficiency is increased by 42% using the free, natural dialogue model while the LCT results are unclear. Using a free dialogue model increases the efficiency and reduces the distraction in some cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-297"
  },
  "cuayahuitl11_interspeech": {
   "authors": [
    [
     "Heriberto",
     "Cuayáhuitl"
    ],
    [
     "Nina",
     "Dethlefs"
    ]
   ],
   "title": "Optimizing situated dialogue management in unknown environments",
   "original": "i11_1009",
   "page_count": 4,
   "order": 300,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "We present a conversational learning agent that helps users navigate through complex and challenging spatial environments. The agent exhibits adaptive behaviour by learning spatially-aware dialogue actions while the user carries out the navigation task. To this end, we use Hierarchical Reinforcement Learning with relational representations to efficiently optimize dialogue actions tightly-coupled with spatial ones, and Bayesian networks to model the user's beliefs of the navigation environment. Since these beliefs are continuously changing, we induce the agent's behaviour in real time. Experimental results, using simulation, are encouraging by showing efficient adaptation to the user's navigation knowledge, specifically to the generated route and the intermediate locations to negotiate with the user.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-298"
  },
  "deshmukh11_interspeech": {
   "authors": [
    [
     "Om D.",
     "Deshmukh"
    ],
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Ashish",
     "Verma"
    ],
    [
     "Etienne",
     "Marcheret"
    ]
   ],
   "title": "Acoustic-similarity based technique to improve concept recognition",
   "original": "i11_1013",
   "page_count": 4,
   "order": 301,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "In this work we propose an acoustic-similarity based technique to improve the recognition of in-grammar utterances in typical directed-dialog applications where the Automatic Speech Recognition (ASR) system consists of one or more class-grammars embedded in the Language Model (LM). The proposed technique increases the transition cost of LM paths by a value proportional to the average acoustic similarity between that LM path and all the in-grammar utterances. Proposed modifications improve the in-grammar concept recognition rate by 0.5% absolute at lower grammar fanouts and by about 2% at higher fanouts as compared to a technique which reduces the probability of entering all the LM paths by a uniform value. The improvements are more pronounced as the fanout size of the grammar is increased and especially at operating points corresponding to lower False Accept (FA) values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-299"
  },
  "peters11_interspeech": {
   "authors": [
    [
     "Doug",
     "Peters"
    ],
    [
     "Peter",
     "Stubley"
    ]
   ],
   "title": "Dialog methods for improved alphanumeric string capture",
   "original": "i11_1017",
   "page_count": 4,
   "order": 302,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "In this paper, we consider advances in automated over-the-phone alphanumeric string capture. For this task, acoustic confusions typically result in significant error rates. Of course, confusions also exist in human-to-human communication. However, humans employ dialog-level strategies with which to disambiguate confusions and correct errors - allowing high-fidelity transmission of alphanumeric strings across all but the noisiest of channels. These human strategies are examined and a subset amenable to automation is identified. The resulting automated error-correction dialog achieves 30% dialog error rate reduction compared to a conventional application in a high-volume commercial deployment. Further, the fact that there are many recognition errors in the context of a structurally simple dialog recommends this task for dialog optimization. We present an example of offline optimization and discuss the potential for online learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-300"
  },
  "devault11_interspeech": {
   "authors": [
    [
     "David",
     "DeVault"
    ],
    [
     "Kenji",
     "Sagae"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Detecting the status of a predictive incremental speech understanding model for real-time decision-making in a spoken dialogue system",
   "original": "i11_1021",
   "page_count": 4,
   "order": 303,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "We explore the potential for a responsive spoken dialogue system to use the real-time status of an incremental speech understanding model to guide its incremental decision-making about how to respond to a user utterance that is still in progress. Spoken dialogue systems have a range of potentially useful real-time response options as a user is speaking, such as providing acknowledgments or backchannels, interrupting the user to ask a clarification question or to initiate the system's response, or even completing the user's utterance at appropriate moments. However, implementing such incremental response capabilities seems to require that a system be able to assess its own level of understanding incrementally, so that an appropriate response can be selected at each moment. In this paper, we use a data-driven classification approach to explore the trade-offs that a virtual human dialogue system faces in reliably identifying how its understanding is progressing during a user utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-301"
  },
  "chandramohan11_interspeech": {
   "authors": [
    [
     "Senthilkumar",
     "Chandramohan"
    ],
    [
     "Matthieu",
     "Geist"
    ],
    [
     "Fabrice",
     "Lefèvre"
    ],
    [
     "Olivier",
     "Pietquin"
    ]
   ],
   "title": "User simulation in dialogue systems using inverse reinforcement learning",
   "original": "i11_1025",
   "page_count": 4,
   "order": 304,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "Spoken Dialogue Systems (SDS) are man-machine interfaces which use natural language as the medium of interaction. Dialogue corpora collection for the purpose of training and evaluating dialogue systems is an expensive process. User simulators aim at simulating human users in order to generate synthetic data. Existing methods for user simulation mainly focus on generating data with the same statistical consistency as in some reference dialogue corpus. This paper outlines a novel approach for user simulation based on Inverse Reinforcement Learning (IRL). The task of building the user simulator is perceived as a task of imitation learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-302"
  },
  "crook11_interspeech": {
   "authors": [
    [
     "Paul A.",
     "Crook"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "Lossless value directed compression of complex user goal states for statistical spoken dialogue systems",
   "original": "i11_1029",
   "page_count": 4,
   "order": 305,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "This paper presents initial results in the application of Value Directed Compression (VDC) to spoken dialogue management belief states for reasoning about complex user goals. On a small but realistic SDS problem VDC generates a lossless compression which achieves a 6-fold reduction in the number of dialogue states required by a Partially Observable Markov Decision Process (POMDP) dialogue manager (DM). Reducing the number of dialogue states reduces the computational power, memory, and storage requirements of the hardware used to deploy such POMDP SDSs, thus increasing the complexity of the systems which could theoretically be deployed. In addition, in the case when on-line reinforcement learning is used to learn the DM policy, it should lead to, in this case, a 6-fold reduction in policy learning time. These are the first automatic compression results that have been presented for POMDP SDS states which represent user goals as sets over possible domain objects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-303"
  },
  "carlin11_interspeech": {
   "authors": [
    [
     "Michael A.",
     "Carlin"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Rapid evaluation of speech representations for spoken term discovery",
   "original": "i11_0821",
   "page_count": 4,
   "order": 306,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "Acoustic front-ends are typically developed for supervised learning tasks and are thus optimized to minimize word error rate, phone error rate, etc. However, in recent efforts to develop zero-resource speech technologies, the goal is not to use transcribed speech to train systems but instead to discover the acoustic structure of the spoken language automatically. For this new setting, we require a framework for evaluating the quality of speech representations without coupling to a particular recognition architecture. Motivated by the spoken term discovery task, we present a dynamic time warping-based framework for quantifying how well a representation can associate words of the same type spoken by different speakers. We benchmark the quality of a wide range of speech representations using multiple frame-level distance metrics and demonstrate that our performance metrics can also accurately predict phone recognition accuracies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-304"
  },
  "hixon11_interspeech": {
   "authors": [
    [
     "Ben",
     "Hixon"
    ],
    [
     "Eric",
     "Schneider"
    ],
    [
     "Susan L.",
     "Epstein"
    ]
   ],
   "title": "Phonemic similarity metrics to compare pronunciation methods",
   "original": "i11_0825",
   "page_count": 4,
   "order": 307,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "As grapheme-to-phoneme methods proliferate, their careful evaluation becomes increasingly important. This paper explores a variety of metrics to compare the automatic pronunciation methods of three freely-available grapheme-to-phoneme packages on a large dictionary. Two metrics, presented here for the first time, rely upon a novel weighted phonemic substitution matrix constructed from substitution frequencies in a collection of trusted alternate pronunciations. These new metrics are sensitive to the degree of mutability among phonemes. An alignment tool uses this matrix to compare phoneme substitutions between pairs of pronunciations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-305"
  },
  "skowronek11_interspeech": {
   "authors": [
    [
     "Janto",
     "Skowronek"
    ],
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "Investigating the effect of number of interlocutors on the quality of experience for multi-party audio conferencing",
   "original": "i11_0829",
   "page_count": 4,
   "order": 308,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "Towards an assessment method for multi-party audio conferencing systems, we investigated in a pilot study the influence of the number of interlocutors and the audio reproduction method on the quality of experience. Despite some room for improving the sensitivity of our experimental method, the results show that the number of interlocutors and the audio reproduction method influence at least partially the cognitive effort required in conferencing situations and the perceived speech transmission quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-306"
  },
  "kolar11_interspeech": {
   "authors": [
    [
     "Jáchym",
     "Kolář"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "On development of consistently punctuated speech corpora",
   "original": "i11_0833",
   "page_count": 4,
   "order": 309,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "Punctuation of automatically recognized speech is important to enhance readability of transcripts and to aid downstream NLP processing. This paper is concerned with issues involved in developing training and test corpora for automatic punctuation systems. Punctuation annotation in speech transcripts is difficult since there are numerous cases for which no standard punctuation rules exist. Special punctuation annotation guidelines tailored to spoken language were developed. Using these guidelines, almost 100 hours of broadcast news and conversation data in English and French have been punctuated by trained annotators. Measures of inter-annotator agreement are provided for both languages and differences between languages and genre are analyzed and discussed, along with some of the most frequent disagreements between annotators. Overall, using the guidelines, the annotation consistency has been significantly improved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-307"
  },
  "narayanan11_interspeech": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Erik",
     "Bresch"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Yoon",
     "Kim"
    ],
    [
     "Adam",
     "Lammert"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ],
    [
     "Yinghua",
     "Zhu"
    ]
   ],
   "title": "A multimodal real-time MRI articulatory corpus for speech research",
   "original": "i11_0837",
   "page_count": 4,
   "order": 310,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "We present MRI-TIMIT: a large-scale database of synchronized audio and real-time magnetic resonance imaging (rtMRI) data for speech research. The database currently consists of speech data acquired from two male and two female speakers of American English. Subjects' upper airways were imaged in the midsagittal plane while reading the same 460 sentence corpus used in the MOCHA-TIMIT corpus [1]. Accompanying acoustic recordings were phonemically transcribed using forced alignment. Vocal tract tissue boundaries were automatically identified in each video frame, allowing for dynamic quantification of each speaker's midsagittal articulation. The database and companion toolset provide a unique resource with which to examine articulatory-acoustic relationships in speech production.\n",
    "",
    "",
    "A. Wrench and W. Hardcastle, A multichannel articulatory speech database and its application for automatic speech recognition, in Proc. 5th SSP, Kloster Seeon, 2000, pp. 305308.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-308"
  },
  "burnham11_interspeech": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Dominique",
     "Estival"
    ],
    [
     "Steven",
     "Fazio"
    ],
    [
     "Jette",
     "Viethen"
    ],
    [
     "Felicity",
     "Cox"
    ],
    [
     "Robert",
     "Dale"
    ],
    [
     "Steve",
     "Cassidy"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Michael",
     "Wagner"
    ],
    [
     "Yuko",
     "Kinoshita"
    ],
    [
     "Roland",
     "Göcke"
    ],
    [
     "Joanne",
     "Arciuli"
    ],
    [
     "Marc",
     "Onslow"
    ],
    [
     "Trent",
     "Lewis"
    ],
    [
     "Andrew",
     "Butcher"
    ],
    [
     "John",
     "Hajek"
    ]
   ],
   "title": "Building an audio-visual corpus of Australian English: large corpus collection with an economical portable and replicable black box",
   "original": "i11_0841",
   "page_count": 4,
   "order": 311,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "The Big Australian Speech Corpus project incorporates the strategic goals of 30 Chief Investigators from various speech science areas. Speech from 1000 geographically and socially diverse speakers is being recorded using a uniform and automated protocol plus standardized hardware and software to produce a widely applicable and extensible database - AusTalk. Here we describe the project's major components and organization; share the lessons learnt from difficulties and challenges; and present the results achieved so far.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-309"
  },
  "minematsu11_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Koji",
     "Okabe"
    ],
    [
     "Keisuke",
     "Ogaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Measurement of objective intelligibility of Japanese accented English using ERJ (English read by Japanese) database",
   "original": "i11_1481",
   "page_count": 4,
   "order": 312,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "In many schools, English is taught as international communication tool and the goal of English pronunciation training is generally to acquire intelligible enough pronunciation, which is not always native-sounding pronunciation. However, the definition of the intelligible pronunciation is not easy because it depends on the speaking skill of a speaker, the predictability of a content, and the language background of a listener. One kind of accented pronunciation, which is intelligible enough for some listeners, is often less intelligible for others. This paper focuses on objective intelligibility of Japanese English through the ears of American English speakers with little exposure to Japanese English. A large listening test was conducted using ERJ (English Read by Japanese) database. A balanced subset of this database were presented over a telephone line to the American listeners who were asked to repeat what they heard. Totally, 17,416 repetitive responses were collected and they were transcribed manually. This paper describes the design of this experiment and some results of analyzing the results of transcription.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-310"
  },
  "moller11_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Chihuy",
     "Bang"
    ],
    [
     "Teele",
     "Tamme"
    ],
    [
     "Markus",
     "Vaalgamaa"
    ],
    [
     "Benjamin",
     "Weiss"
    ]
   ],
   "title": "From single-call to multi-call quality: a study on long-term quality integration in audio-visual speech communication",
   "original": "i11_1485",
   "page_count": 4,
   "order": 313,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "Speech quality is commonly assumed to be the most important factor for the quality of a speech communication service and solution. However, little is known about how the quality experienced during individual calls forms the quality perception of an entire service or solution. Taking the example of an audio-visual IP-based communication solution, a long-term study is presented in which we analyze this relationship in a controlled setting. Results show temporal integration effects in the users' response to time-varying quality levels and prove that simple averaging of call quality scores does not provide sufficiently accurate estimations of service quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-311"
  },
  "lin11_interspeech": {
   "authors": [
    [
     "Hui",
     "Lin"
    ],
    [
     "Jeff",
     "Bilmes"
    ]
   ],
   "title": "Optimal selection of limited vocabulary speech corpora",
   "original": "i11_1489",
   "page_count": 4,
   "order": 314,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "We address the problem of finding a subset of a large speech data corpus that is useful for accurately and rapidly prototyping novel and computationally expensive speech recognition architectures. To solve this problem, we express it as an optimization problem over submodular functions. Quantities such as vocabulary size (or quality) of a set of utterances, or quality of a bundle of word types are submodular functions which make finding the optimal solutions possible. We, moreover, are able to express our approach using graph cuts leading to a very fast implementation even on large initial corpora. We show results on the Switchboard-I corpus, demonstrating improved results over previous techniques for this purpose. We also demonstrate the variety of the resulting corpora that may be produced using our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-312"
  },
  "zahorian11_interspeech": {
   "authors": [
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Jiang",
     "Wu"
    ],
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Chandra",
     "SekharVootkuri"
    ],
    [
     "Brian",
     "Wong"
    ],
    [
     "Andrew",
     "Hwang"
    ],
    [
     "Eldar",
     "Tokhtamyshev"
    ]
   ],
   "title": "Open source multi-language audio database for spoken language processing applications",
   "original": "i11_1493",
   "page_count": 4,
   "order": 315,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "Over the past few decades, research in automatic speech recognition and automatic speaker recognition has been greatly facilitated by the sharing of large annotated speech databases such as those distributed by the Linguistic Data Consortium (LDC). Open sources, particularly web sites such as YouTube, contain vast and varied speech recordings in a variety of languages. However, these \"open sources\" for speech data are largely untapped as resources for speech research. In this paper, a project to collect, organize, and annotate a large group of this speech data is described. The data consists of approximately 30 hours of speech in each of three languages, English, Mandarin Chinese, and Russian. Each of 900 recordings has been orthographically transcribed at the sentence/phrase level by human listeners. Some of the issues related to working with this low quality, varied, noisy speech data in three languages are described.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-313"
  },
  "black11b_interspeech": {
   "authors": [
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Daniel",
     "Bone"
    ],
    [
     "Marian E.",
     "Williams"
    ],
    [
     "Phillip",
     "Gorrindo"
    ],
    [
     "Pat",
     "Levitt"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "The USC CARE corpus: child-psychologist interactions of children with autism spectrum disorders",
   "original": "i11_1497",
   "page_count": 4,
   "order": 316,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "We introduce the USC CARE Corpus, comprised of spontaneous and standardized child-psychologist interactions of children with a diagnosis of an autism spectrum disorder (ASD). The audio-video data is collected in the context of the Autism Diagnostic Observation Schedule (ADOS), which is a tool used by psychologists for a research-level diagnosis of ASD for children. The interaction consists of developmentally appropriate semi-structured social activities, providing the psychologist with a sample of behavior used to rate the child on a series of autism-relevant symptoms. Our goal with this multimodal corpus is to investigate how analytical technology (e.g., speech and language processing) can enhance this observational rating task and provide greater insight into social behavior and communication. We provide demographic statistics on the recruited children (60 to date), describe the multimodal recording set-up, and discuss current and future work for this novel corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-314"
  },
  "barbot11_interspeech": {
   "authors": [
    [
     "Nelly",
     "Barbot"
    ],
    [
     "Vincent",
     "Barreaud"
    ],
    [
     "Olivier",
     "Boëffard"
    ],
    [
     "Laure",
     "Charonnat"
    ],
    [
     "Arnaud",
     "Delhay"
    ],
    [
     "Sébastien Le",
     "Maguer"
    ],
    [
     "Damien",
     "Lolive"
    ]
   ],
   "title": "Towards a versatile multi-layered description of speech corpora using algebraic relations",
   "original": "i11_1501",
   "page_count": 4,
   "order": 317,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "This paper presents a software library, namely ROOTS for Rich Object Oriented Transcription System, that helps to describe spoken messages in a coherent manner linking sequences of items on numerous levels (linguistic, phonological, or acoustic). The proposed representation is incremental and can thus describe any or all parts of an utterance. In order to link different levels of description, algebraic relations are used. Instead of relying solely on fixed, pre-determined relations, algebraic composition operators are proposed that can create a missing relation on demand. In terms of software architecture, object classes are defined based on a well-grounded theoretical representation of speech (text, syntax, phonology and acoustics), without particular dependences on an annotation system (e.g. IPA is fully implemented). The API documentation for this software is available online [7].\n",
    "",
    "",
    "ROOTS homepage, http://www.irisa.fr/cordial/roots, 2011.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-315"
  },
  "richmond11_interspeech": {
   "authors": [
    [
     "Korin",
     "Richmond"
    ],
    [
     "Phil",
     "Hoole"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Announcing the electromagnetic articulography (day 1) subset of the mngu0 articulatory corpus",
   "original": "i11_1505",
   "page_count": 4,
   "order": 318,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "This paper serves as an initial announcement of the availability of a corpus of articulatory data called mngu0. This corpus will ultimately consist of a collection of multiple sources of articulatory data acquired from a single speaker: electromagnetic articulography (EMA), audio, video, volumetric MRI scans, and 3D scans of dental impressions. This data will be provided free for research use. In this first stage of the release, we are making available one subset of EMA data, consisting of more than 1,300 phonetically diverse utterances recorded with a Carstens AG500 electromagnetic articulograph. Distribution of mngu0 will be managed by a dedicated \"forum-style\" web site. This paper both outlines the general goals motivating the distribution of the data and the creation of the mngu0 web forum, and also provides a description of the EMA data contained in this initial release.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-316"
  },
  "pirker11_interspeech": {
   "authors": [
    [
     "Gregor",
     "Pirker"
    ],
    [
     "Michael",
     "Wohlmayr"
    ],
    [
     "Stefan",
     "Petrik"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "A pitch tracking corpus with evaluation on multipitch tracking scenario",
   "original": "i11_1509",
   "page_count": 4,
   "order": 319,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "In this paper, we introduce a novel pitch tracking database (PTDB) including ground truth signals obtained from a laryngograph. The database, referenced as PTDB-TUG, consists of 2342 phonetically rich sentences taken from the TIMIT corpus. Each sentence was at least recorded once by a male and a female native speaker. In total, the database contains 4720 recordings from 10 male and 10 female speakers. Furthermore, we evaluated two multipitch tracking systems on a subset of speakers to provide a benchmark for further research activities. The database can be downloaded at http://www.spsc.tugraz.at/tools.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-317"
  },
  "butko11_interspeech": {
   "authors": [
    [
     "Taras",
     "Butko"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "On building and evaluating a broadcast-news audio segmentation system",
   "original": "i11_1513",
   "page_count": 4,
   "order": 320,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "Audio segmentation is useful in diverse applications like audio indexing and retrieval, subtitling, monitoring of acoustic scenes, etc. Also, an initial audio segmentation stage may help to improve the robustness of speech technologies like automatic speech recognition and speaker diarization. In this paper, firstly, the Albayzin-2010 audio segmentation evaluation is reported, including some conclusions drawn from the analysis of the set of eight submitted systems and their results. Then an audio segmentation system build in agreement with those conclusions is described and tested. Finally, by using the gained experience, the initial design of both the acoustic classes and the detection scoring rules is refined aiming to obtain a more meaningful error rate measurement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-318"
  },
  "dobrisek11_interspeech": {
   "authors": [
    [
     "Simon",
     "Dobrišek"
    ],
    [
     "France",
     "Mihelič"
    ]
   ],
   "title": "Time- and acoustic-mediated alignment algorithms for speech recognition evaluation",
   "original": "i11_1517",
   "page_count": 4,
   "order": 321,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "The paper investigates the time- and acoustic-mediated alignment algorithms that can be used for better speech recognition evaluation. The edit-cost function, which weights the cost of speech unit matches, substitutions, deletions and insertions, is defined as a function of timed symbols or even as a function of speech signal segments. The algorithms are compared using several classical statistical measures of different types that are derived from speech recognition confusion matrices and are normally used to measure the agreement between different classifications of the same set of objects. These measures provide a reasonable indication that the investigated algorithms provide more relevant speech recognition error statistics than the algorithms that are commonly used for this purpose.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-319"
  },
  "niemann11_interspeech": {
   "authors": [
    [
     "Julia",
     "Niemann"
    ],
    [
     "Kati",
     "Schulz"
    ],
    [
     "Ina",
     "Wechsung"
    ]
   ],
   "title": "Effects of shortening speech prompts of in-car voice user interfaces on users mental models",
   "original": "i11_1521",
   "page_count": 4,
   "order": 322,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "Shortening speech prompts reduce attention allocation display; but might on the other hand deteriorate the users' mental model. Thus the paper investigates effects of reducing time effort of speech via a transfer task, retrieval tasks and navigation-orientation tasks for three different strategies: (1) using earcons for menu orientation, (2) using commando based speech for interaction options, and (3) using uptempo speech for content based information. Results show that earcons are well qualified to not impair navigationorientation performance. Commando based speech leads to even better retrieval performance than sentence based representation of interaction. Solely uptempo speech decreased retrieval performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-320"
  },
  "werff11_interspeech": {
   "authors": [
    [
     "Laurens van der",
     "Werff"
    ],
    [
     "Wessel",
     "Kraaij"
    ],
    [
     "Franciska de",
     "Jong"
    ]
   ],
   "title": "Speech transcript evaluation for information retrieval",
   "original": "i11_1525",
   "page_count": 4,
   "order": 323,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "Speech recognition transcripts are being used in various fields of research and practical applications, putting various demands on their accuracy. Traditionally ASR research has used intrinsic evaluation measures such as word error rate to determine transcript quality. In non-dictation-type applications such as speech retrieval, it is better to use extrinsic (or task specific) measures. Indexation and the associated processing may eliminate certain errors, whereas the search query may reveal others. In this work, we argue that the standard extrinsic speech retrieval measure average precision is unpractical for ASR evaluation. As an alternative we propose the use of ranked correlation measures on the output of the speech retrieval task, with the goal of predicting relative mean average precision. The measures we used showed a reasonably high correlation with average precision, but require much less human effort to calculate and can be more easily deployed in a variety of real-life settings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-321"
  },
  "rodriguezfuentes11_interspeech": {
   "authors": [
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Germán",
     "Bordel"
    ]
   ],
   "title": "The Albayzin 2010 language recognition evaluation",
   "original": "i11_1529",
   "page_count": 4,
   "order": 324,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "The Albayzin 2010 Language Recognition Evaluation (LRE), carried out from June to October 2010, was the second effort made by the Spanish/Portuguese community for benchmarking language recognition technology. As the Albayzin 2008 LRE, it was coordinated by the Software Technology Working Group of the University of the Basque Country, with the support of the Spanish Thematic Network on Speech Technology. A speech database was created for system development and evaluation. Speech signals were recorded from TV broadcasts, including clean and noisy speech. The task consisted in deciding whether or not a target language was spoken in a test utterance, and involved 6 target languages: English, Portuguese and the four official languages in Spain (Basque, Catalan, Galician and Spanish), other (Out-Of-Set) languages being also recorded to allow open-set verification tests. This paper presents the main features of the evaluation, analyses system performance on different conditions, including the confusion among languages, and gives hints for future evaluations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-322"
  },
  "moore11_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Progress and prospects for speech technology: results from three sexennial surveys",
   "original": "i11_1533",
   "page_count": 4,
   "order": 325,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "In 1997, and again in 2003, the author was invited to conduct a survey at the IEEE workshop on 'Automatic Speech Recognition and Understanding' (ASRU) in which attendees were offered a set of statements about putative future events relating to progress in various aspects of speech technology R&D. The task of the respondents was to assign a date to each possible event. The 1997 and 2003 results were published at INTERSPEECH 2005 in Lisbon. Six years later, the author was invited by the organisers of ASRU'2009 to repeat the survey for a third time, and this paper presents the combined results from all three 1997, 2003 and 2009 surveys. The overall conclusion is that, over the twelve year period progress is perceived as slow, and the future appears to be generally no nearer than it has been in the past. However, on a positive note, the 2009 survey confirmed that the market for speech technology applications on mobile devices would be highly attractive over the next ten or so years.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-323"
  },
  "novak11_interspeech": {
   "authors": [
    [
     "Josef R.",
     "Novak"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Painless WFST cascade construction for LVCSR - transducersaurus",
   "original": "i11_1537",
   "page_count": 4,
   "order": 326,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "This paper introduces the Transducersaurus toolkit which provides a set of classes for generating each of the fundamental components of a typical WFST ASR cascade, including a Context-dependency transducer, a Lexicon, a stochastic language model and an optional silence class model. The toolkit further implements a simple scripting language in order to facilitate the construction of cascades with a variety of popular combination and optimization methods and provides integrated support for the T3 and Juicer WFST decoders, and both Sphinx and HTK format acoustic models. New results for two standard WSJ tasks are also provided, comparing a variety of cascade construction and optimization algorithms. These results illustrate the flexibility of the toolkit as well as the tradeoffs inherent in various build algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-324"
  },
  "zheng11b_interspeech": {
   "authors": [
    [
     "Rong",
     "Zheng"
    ],
    [
     "Ce",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Data-driven UBM generation via tied Gaussians for GMM-supervector based accent identification",
   "original": "i11_0845",
   "page_count": 4,
   "order": 327,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "This paper presents a new approach to exploit data-driven universal background model (UBM) generation using tied Gaussians for accent identification (AID). The motivation of the proposed algorithm is to potentially utilize broad phonetic-specific accent characteristics by Gaussian mixture model (GMM) and examine data-driven phonetically-inspired UBM creation for GMM-supervector based accent classification. In this work, we discuss the issues involved in applying cumulative posterior probability based Gaussian selection and tree structure based UBM parameter estimation. Derivation and validation of the UBM refined by tied Gaussians are reported in this paper. Performance evaluations comparing our system with other well-known techniques for AID are also provided. Better performance is further achieved by fusing these acoustic-based accent classifiers. Comparison experiments conducted on the CSLU foreign-accented English (FAE) dataset show the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-325"
  },
  "martinez11_interspeech": {
   "authors": [
    [
     "David",
     "Martínez"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "I3a language recognition system for albayzin 2010 LRE",
   "original": "i11_0849",
   "page_count": 4,
   "order": 328,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "This paper describes the two systems submitted to the Albayzin 2010 Language Recognition Evaluation by I3A. This evaluation is similar to the one organized by NIST every 2 years, but the languages to be recognized are those spoken in the Iberian peninsula (Spanish, Catalan, Basque, Galician and Portuguese) plus English. Both submissions are a fusion of five phonotactic and three acoustic subsystems. The only difference between them is the normalization and fusion of the scores. State-of-the-art methods for Language Recognition are adapted to and investigated in the KALAKA-2 database. Our primary system was ranked in the first position of the evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-326"
  },
  "penagarikano11_interspeech": {
   "authors": [
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Germán",
     "Bordel"
    ]
   ],
   "title": "Dimensionality reduction for using high-order n-grams in SVM-based phonotactic language recognition",
   "original": "i11_0853",
   "page_count": 4,
   "order": 329,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "SVM-based phonotactic language recognition is state-of-the-art technology. However, due to computational bounds, phonotactic information is usually limited to low-order phone n-grams (up to n = 3). In a previous work, we proposed a feature selection algorithm, based on n-gram frequencies, which allowed us work successfully with high-order n-grams on the NIST 2007 LRE database. In this work, we use two feature projection methods for dimensionality reduction of feature spaces including up to 4-grams: Principal Component Analysis (PCA) and Random Projection. These methods allow us to attain competitive performance even for small feature sets (e.g. of size 500). Systems were built by means of open software (BUT phone decoders, HTK, SRILM, LIBLINEAR and FoCal) and experiments were carried out on the NIST 2009 LRE database. Best performance was attained by using the feature selection algorithm to get around 11500 features: 1.93% EER and CLLR = 0.413. When considering smaller sets of features, PCA provided best performance. For instance, using PCA to get a 500-dimensional feature subspace yielded 2.15% EER and CLLR = 0.457 (25% improvement with regard to using feature selection).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-327"
  },
  "dehak11_interspeech": {
   "authors": [
    [
     "Najim",
     "Dehak"
    ],
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Douglas",
     "Reynolds"
    ],
    [
     "Reda",
     "Dehak"
    ]
   ],
   "title": "Language recognition via i-vectors and dimensionality reduction",
   "original": "i11_0857",
   "page_count": 4,
   "order": 330,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "In this paper, a new language identification system is presented based on the total variability approach previously developed in the field of speaker identification. Various techniques are employed to extract the most salient features in the lower dimensional i-vector space and the system developed results in excellent performance on the 2009 LRE evaluation set without the need for any postprocessing or backend techniques. Additional performance gains are observed when the system is combined with other acoustic systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-328"
  },
  "martinez11b_interspeech": {
   "authors": [
    [
     "David",
     "Martínez"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Pavel",
     "Matějka"
    ]
   ],
   "title": "Language recognition in ivectors space",
   "original": "i11_0861",
   "page_count": 4,
   "order": 331,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "The concept of so called iVectors, where each utterance is represented by fixed-length low-dimensional feature vector, has recently become very successfully in speaker verification. In this work, we apply the same idea in the context of Language Recognition (LR). To recognize language in the iVector space, we experiment with three different linear classifiers: one based on a generative model, where classes are modeled by Gaussian distributions with shared covariance matrix, and two discriminative classifiers, namely linear Support Vector Machine and Logistic Regression. The tests were performed on the NIST LRE 2009 dataset and the results were compared with state-of-the-art LR based on Joint Factor Analysis (JFA). While the iVector system offers better performance, it also seems to be complementary to JFA, as their fusion shows another improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-329"
  },
  "qian11b_interspeech": {
   "authors": [
    [
     "Xiaojun",
     "Qian"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "On mispronunciation lexicon generation using joint-sequence multigrams in computer-aided pronunciation training (CAPT)",
   "original": "i11_0865",
   "page_count": 4,
   "order": 332,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "We investigate the use of joint-sequence multigrams to generate L2 mispronunciation lexicons for mispronunciation detection and diagnosis. In the joint-sequence framework, a pair of parallel strings (namely, the input string of either graphemes or phonemes of the canonical pronunciation and the phonetic string of the mispronunciation) are aligned to form joint units for probabilistic estimation. We compare results on lexicons produced by phoneme-to-mispronunciation conversion and those by graphemeto- mispronunciation conversion. Results reflect the hypothesized advantage (1.1% reduction in expected miss rate) in unifying phonetic confusion due to L1 negative transfer with those due to grapheme-to-phoneme errors. The impact of mispronunciation by mis-use of analogy is also studied. Recognition results show the benefit of a lexicon with proper priors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-330"
  },
  "sisinni11_interspeech": {
   "authors": [
    [
     "Bianca",
     "Sisinni"
    ],
    [
     "Mirko",
     "Grimaldi"
    ]
   ],
   "title": "Validating a second language perception model for classroom context - a longitudinal study within the perceptual assimilation model",
   "original": "i11_0869",
   "page_count": 4,
   "order": 333,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "The present study verified whether adult listeners retain the ability to improve non-native speech perception and if it can be significantly enhanced in the formal context, a very impoverished context with respect to the natural one. We tested (i) whether perceptual learning is possible for adults in a classroom context during focused phonetic lessons, and (ii) whether it follows the pattern predicted for natural acquisition by the PAM-L2 [1]. The results showed that adult listeners are still able to improve foreign sound perception and this ability seems to occur also in formal contexts in line with the PAM-L2 predictions.\n",
    "",
    "",
    "Best, C. T., Tyler, M. D., Nonnative and second language speech perception: Commonalities and complementarities, In M.J. Munro & O.-S. Bohn [Eds], Second Language speech learning: the role of language experience in speech perception and production, 13-34, Amsterdam: John Benjamins, 2007.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-331"
  },
  "sadakata11_interspeech": {
   "authors": [
    [
     "Makiko",
     "Sadakata"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "The role of variability in non-native perceptual learning of a Japanese geminate-singleton fricative contrast",
   "original": "i11_0873",
   "page_count": 4,
   "order": 334,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "The current study reports the enhancing effect of a high variability training procedure in the learning of a Japanese geminate-singleton fricative contrast. Dutch natives took part in a five-day training procedure in which they identified geminate and singleton variants of the Japanese fricative /s/. They heard either many repetitions of a limited set of words recorded by a single speaker (simple training) or fewer repetitions of a more variable set of words recorded by multiple speakers (variable training). Pre-post identification evaluations and a transfer test indicated clear benefits of the variable training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-332"
  },
  "bernstein11_interspeech": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Jian",
     "Cheng"
    ],
    [
     "Masanori",
     "Suzuki"
    ]
   ],
   "title": "Fluency changes with general progress in L2 proficiency",
   "original": "i11_0877",
   "page_count": 4,
   "order": 335,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "Second language (L2) learners tend to speak slower at every level of linguistic analysis, often in an uneven tempo, with longer pauses at the start and before some words and constructions, than is typical of native speech. As noted by Zhang&Elder [1], native listeners focus on phonological fluency in making judgments about L2 proficiency. Improved understanding of how fluency grows with progress in overall oral proficiency may lead to measures of fluency that would be useful for measuring proficiency itself. Spontaneous speech sampled from populations of L2 speakers of English and Spanish showed orderly, seemingly linear increments in the rates at which words and larger constituents are spoken as a function of human-judged general proficiency level. Results suggest that unit/time fluency measures match native expert perception of oral proficiency, supporting the hypothesis that performance-in-time is a core attribute of speaking proficiency and efficient spoken communication.\n",
    "",
    "",
    "Y. Zhang and C. Elder, Judgments of oral proficiency by nonnative and native English speaking teacher raters: Competing or complementary constructs?, Language Testing, vol. 28, no. 1, pp. 3150, 2011.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-333"
  },
  "ouni11_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Tongue gestures awareness and pronunciation training",
   "original": "i11_0881",
   "page_count": 4,
   "order": 336,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "Pronunciation training based on speech production techniques illustrating tongue movements is gaining popularity. However, there is not sufficient evidence that learners can imitate some tongue animation. In this paper, we investigated human awareness of controlling their tongue body gestures. In a first experiment, participants were asked to perform some tongue movements. This task was evaluated by observing ultrasound imaging of the tongue recorded during the experiment. No feedback was provided. In a second experiment, a short session of training was added where participants can observe ultrasound imaging in real-time of their own tongue movements. The goal was to increase their awareness of their tongue gestures. A pre-test and post-test were carried out without any feedback. The results suggest that without a priori knowledge, it is not easy to finely control tongue body gestures; and that we gain in performance after a short training session which suggests that providing visual feedback, even a short one, improves tongue gesture awareness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-334"
  },
  "dommelen11_interspeech": {
   "authors": [
    [
     "Wim A. van",
     "Dommelen"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Impact of speaker variability on speech perception in non-native listeners",
   "original": "i11_0885",
   "page_count": 4,
   "order": 337,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "This study investigates the perception of English words produced by 45 native talkers presented in moderate noise to native Norwegian listeners. The relative intelligibility of individual talkers is compared with that obtained for native listeners in order to determine whether inherent talker clarity is determined by global acoustic-phonetic characteristics. Talker intelligibility was strongly correlated across native and non-native listeners; there was also strong correlation across groups as to the lexical items most often misperceived. Word intelligibility for both was correlated with certain acoustic-phonetic characteristics of the talker's productions, including amount of energy in the mid-frequency region and mean word duration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-335"
  },
  "ordin11_interspeech": {
   "authors": [
    [
     "Mikhail",
     "Ordin"
    ],
    [
     "Leona",
     "Polyanskaya"
    ],
    [
     "Christiane",
     "Ulbrich"
    ]
   ],
   "title": "Acquisition of timing patterns in second language",
   "original": "i11_1129",
   "page_count": 4,
   "order": 338,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "The paper presents an analysis of speech rhythm development in second language. 51 German learners of English with varying degrees of proficiency were recorded producing 33 identical sentences of quasi-spontaneous speech. Durational characteristics of syllables, consonantal and vocalic intervals were calculated to allow for analysis of timing patterns on different proficiency levels. We found that 1) durational characteristics in second language depend on the proficiency level of the speaker and can be modeled to predict the proficiency level of the language learner; 2) durational characteristics become more consistent throughout the progress of second language mastery; 3) multiple rhythms operate on multiple timescales.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-336"
  },
  "li11d_interspeech": {
   "authors": [
    [
     "Hongyan",
     "Li"
    ],
    [
     "Shen",
     "Huang"
    ],
    [
     "Shijin",
     "Wang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Context-dependent duration modeling with backoff strategy and look-up tables for pronunciation assessment and mispronunciation detection",
   "original": "i11_1133",
   "page_count": 4,
   "order": 339,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "This paper makes an intensive study on the contextual modeling methods of duration information, for the purpose of improving the performance of pronunciation assessment and mispronunciation detection. The main ideas include: 1) we extend the relations among duration sequence with different level of contextual constraints, and bring them into a unified framework. 2) A backoff mechanism is introduced to resolve the problem of data sparseness and unbalanced distribution. 3) Rather than the traditional parametric functions, we use the discrete modeling for empirical duration distributions based on look-up tables, which can improve the model precision and accelerate the computation speed. The experimental results show the effectiveness of the above methods. The proposed word-dependent duration models can yield 0.0782 in absolute CC (correlation coefficient) improvement and 4.58% in absolute EER (equal error rate) reduction for the tasks of pronunciation assessment and mispronunciation detection respectively, both compared with the baseline method with conventional context-independent case.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-337"
  },
  "sonu11_interspeech": {
   "authors": [
    [
     "Mee",
     "Sonu"
    ],
    [
     "Keiichi",
     "Tajima"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Perceptual training of vowel length contrast of Japanese by L2 listeners: effects of an isolated word versus a word embedded in sentences",
   "original": "i11_1137",
   "page_count": 4,
   "order": 340,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "In an attempt to improve the perception of vowel length contrasts in Japanese by L2 learners (L1 Korean), we compared two different training methods. The first one involved training L2 learners with sets of isolated words contrasting the vowels (Word training), whereas the other training involved presenting same words within sentences (Sentence training). Word training and sentence training both led to significant improvement in the learners' ability to identify vowel length contrasts in Japanese, and, both types of training improved the listeners' ability to perceive consonant length contrasts in Japanese, which listeners were not trained to identify. Although the amount of overall improvement was not significantly different between training methods, sentence training showed improvement in a wider range of conditions than word training. These results indicate that word training and sentence training are both effective at improving perception of length contrasts in Japanese, but that sentence training may have some advantage over word training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-338"
  },
  "wu11_interspeech": {
   "authors": [
    [
     "E-Chin",
     "Wu"
    ]
   ],
   "title": "Similar vowels in L1/L2 production: confused or discerned in early L2 English learners with different amount of exposure",
   "original": "i11_1141",
   "page_count": 4,
   "order": 341,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "The degree of similarity between L1 and L2 sounds is said to be crucial for L1/L2 sound distinction. Similar L1 and L2 sounds, according to the Speech Learning Model (SLM), tend to be undistinguished. Yet, much has been left undetermined of what counts as \"similar\". In this study, the relation between phonetic similarity and L1/L2 sound distinction is explored by examining whether different acoustic cues were produced for Mandarin and English when the English vowels tested were similar to the Mandarin ones (i.e. /i/, /a/, and /u/). The effect of exposure was also investigated. The results showed that Mandarin children made distinctions for the similar vowels of Mandarin and English in the dimension expected and exposure did effect how well certain vowels were discerned for the group with less exposure. The fact that L2 learners were able to pick out the differences even between similar vowels found in L1 and L2 implies a need to clarify the concept of similarity in L2 learning models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-339"
  },
  "meister11_interspeech": {
   "authors": [
    [
     "Lya",
     "Meister"
    ],
    [
     "Einar",
     "Meister"
    ]
   ],
   "title": "Production and perception of estonian vowels by native and non-native speakers",
   "original": "i11_1145",
   "page_count": 4,
   "order": 342,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "The aim of the paper is to study the production of Estonian vowel categories by L2 speakers of Estonian with a Russian-language background and to compare the results with perception data from the same subjects. Ten native Estonian subjects and ten L2 speakers participated in both the reading of an Estonian text corpus and the perception experiment.\n",
    "It was found that mostly the production and perception results show similar patterns and thus lend support to the common standpoint that L2 perception predicts the accuracy of L2 production. However, evidence was found that despite the correct perceptual identification of L2 vowels, in L2 production the native categorical vowel representation outweighs the newer L2 category pattern.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-340"
  },
  "kibishi11_interspeech": {
   "authors": [
    [
     "Hiroshi",
     "Kibishi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "New feature parameters for pronunciation evaluation in English presentations at international conferences",
   "original": "i11_1149",
   "page_count": 4,
   "order": 343,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "We have previously proposed a statistical method for estimating the pronunciation proficiency and intelligibility of presentations made in English by non-native speakers. To investigate the relationship between various acoustic measures and the pronunciation score and intelligibility, we statistically analyzed the speaker's actual utterances to find combinations of acoustic features with a high correlation between the score estimated by a linear regression model and the score perceived by native English teachers. In this paper, we examined the quality of new acoustic features that are useful when used in combination with the system's estimates of pronunciation score and intelligibility. Results showed that the best combination of acoustic features produced correlation coefficients of 0.929 and 0.753 for pronunciation and intelligibility, respectively, using open data for speakers at the 10-sentence level.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-341"
  },
  "bailly11_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Will",
     "Barbour"
    ]
   ],
   "title": "Synchronous reading: learning French orthography by audiovisual training",
   "original": "i11_1153",
   "page_count": 4,
   "order": 344,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "We assess here the potential benefit of a karaoke-style reading system for learning sound-to-letter mapping in irregular languages. We have developed a framework that eases the development of interactive systems exploiting the alignment of text with audio at various levels (letters, phones syllables, words, chunks, etc). Synchronous reading consists of using time-aligned text with speech at the phone level to displace a cursor - here a virtual finger - on the text in synchrony with its verbalization. We demonstrate here that this bimodal reading implicitly facilitates the learning of the correspondence between sounds and letters in French for native and foreign subjects. Native subjects are shown to benefit more strongly from synchronous reading.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-342"
  },
  "koniaris11_interspeech": {
   "authors": [
    [
     "Christos",
     "Koniaris"
    ],
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Phoneme level non-native pronunciation analysis by an auditory model-based native assessment scheme",
   "original": "i11_1157",
   "page_count": 4,
   "order": 345,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "We introduce a general method for automatic diagnostic evaluation of the pronunciation of individual non-native speakers based on a model of the human auditory system trained with native data stimuli. For each phoneme class, the Euclidean geometry similarity between the native perceptual domain and the non-native speech power spectrum domain is measured. The problematic phonemes for a given second language speaker are found by comparing this measure to the Euclidean geometry similarity for the same phonemes produced by native speakers only. The method is applied to different groups of non-native speakers of various language backgrounds and the experimental results are in agreement with theoretical findings of linguistic studies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-343"
  },
  "sturm11_interspeech": {
   "authors": [
    [
     "Pavel",
     "Šturm"
    ],
    [
     "Radek",
     "Skarnitzl"
    ]
   ],
   "title": "The open front vowel /æ/ in the production and perception of Czech students of English",
   "original": "i11_1161",
   "page_count": 4,
   "order": 346,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "This study addresses the acquisition of the English open front vowel by Czech learners of English, who are known to experience difficulties in both its production and perception. Secondary school students and university students of English judged the acceptability of the open front vowel as pronounced by other Czech learners of English. Their evaluations were plotted against acoustic measurements (F1, F2, and vowel duration) and linguistically relevant variables. The evaluations varied as a function of F1 and L2 experience. The experienced subjects perceived the vowel more accurately and consistently than did the relatively inexperienced subjects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-344"
  },
  "cucchiarini11_interspeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Eric",
     "Sanders"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Error selection for ASR-based English pronunciation training in `my pronunciation coach'",
   "original": "i11_1165",
   "page_count": 4,
   "order": 347,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "In this paper we report on a study of pronunciation errors that was conducted within the framework of the project \"My Pronunciation Coach\", which is aimed at developing an ASR-based system for pronunciation training for learners of English with Dutch as their mother tongue. The aim of this study was to obtain quantitative data on the occurrence of pronunciation errors in Dutch English speech. We present the results of this study and compare them to those of previous investigations. Finally, we discuss the implications of these results for the development of My Pronunciation Coach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-345"
  },
  "nariai11_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Nariai"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "An experimental analysis of pitch patterns in Japanese speakers of English with verification by speech re-synthesis",
   "original": "i11_1169",
   "page_count": 4,
   "order": 348,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "Certain irregularities in utterances of a word or phrase occur in English as spoken by Japanese native subjects (Japanese English, henceforth). This study considers such pitch patterns as one of the most common causes of deficiencies in Japanese English, and that Japanese English would have better pitch patterns if its peculiarities are modified. Firstly, pitch patterns of Japanese English are statistically analyzed. The analytical results provide a rule for modifying the pitch patterns of Japanese English, in order to improve naturalness. To check the appropriateness of the rule, the pitch patterns of several samples of Japanese English are acoustically modified and re-synthesized. The modified speeches are evaluated in a listening experiment taken by native English speakers. Averagely, over threefold subjects support the proposed modification against original speeches. Therefore, the results indicate practical verification of modifying ways of Japanese English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-346"
  },
  "nariai11b_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Nariai"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Yoshiaki",
     "Ito"
    ]
   ],
   "title": "An analysis of word duration in native speakers and Japanese speakers of English",
   "original": "i11_1173",
   "page_count": 4,
   "order": 349,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "An analysis of word duration in English sentences uttered by native speakers of Japanese is made, in which the difference in prosodic patterns between the English and Japanese languages is taken into account. The durations of Japanese speakers are compared with those of English speakers in regard to a percentage distribution of an individual word relative to all words in a sentence. The results of the statistical analysis revealed that nouns and words at the ends of sentences in Japanese speakers were shorter for English speakers. The former result suggests that English speakers put prominence on nouns, whereas Japanese speakers tend not to have the same rhythm as English speakers. The latter result suggests that phrase-final lengthening is insufficient in Japanese speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-347"
  },
  "kurniawati11_interspeech": {
   "authors": [
    [
     "Evelyn",
     "Kurniawati"
    ],
    [
     "Samsudin",
     "Ng"
    ],
    [
     "Karthik",
     "Muralidhar"
    ],
    [
     "Sapna",
     "George"
    ]
   ],
   "title": "A template based voice trigger system using bhattacharyya edit distance",
   "original": "i11_0889",
   "page_count": 4,
   "order": 350,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "Dynamic Time Warping (DTW) is frequently used in isolated word recognition system due to their simplicity and robustness to noise. However, the computational effort required by DTW based solution is proportional to the number of words registered in the system. Vector Quantization (VQ) is employed to alleviate this by converting the spoken input to a sequence of discrete symbols to be matched with the stored word template. In this paper, we propose the use of Bhattacharyya distance as the cost function for this pattern matching problem. The template used is a string of discrete symbols, each modeled by Gaussian Mixture Model (GMM) representing context dependent sub-word unit. The system is tested on 100 template matching task from two registrations of 50 cable TV channel names to simulate voice-triggered remote control. An average of 92% accuracy is obtained. A scheme is also proposed to enable guest user without registration data to use the system efficiently.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-348"
  },
  "nolden11_interspeech": {
   "authors": [
    [
     "D.",
     "Nolden"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Acoustic look-ahead for more efficient decoding in LVCSR",
   "original": "i11_0893",
   "page_count": 4,
   "order": 351,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "In this paper we propose novel approximations of a generalized acoustic look-ahead to speed up the search process in large vocabulary continuous speech recognition (LVCSR). Unlike earlier methods, we do not employ any phoneme- or syllable level heuristics. First we define and analyze the perfect acoustic look-ahead as a simple pre-evaluation of the original acoustic models into the future. This method is very slow, but reveals the best possible impact on the search space that can be achieved through acoustic look-ahead. In a second step, we derive efficient and simple approximative look-ahead models from the perfect models. We show that the approximative models compare well to the perfect models regarding the search space, and that the approximative models significantly improve the efficiency in comparison to the baseline, without any negative effect on the precision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-349"
  },
  "duckhorn11_interspeech": {
   "authors": [
    [
     "Frank",
     "Duckhorn"
    ],
    [
     "Matthias",
     "Wolff"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "A new epsilon filter for efficient composition of weighted finite-state transducers",
   "original": "i11_0897",
   "page_count": 4,
   "order": 352,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "In this paper we propose a new composition algorithm for weighted finite-states transducers that are more and more used for speech and pattern recognition applications. Composition joins multiple transducers into one. We have implemented an embedded speech based dialog system for steering applications. Therefore regular grammars are very useful, but they may enlarge strongly by determinization. Composition using the sequential or the matching epsilon-filter does not perform optimal without determinization. Our new algorithm combines the advantages of these two epsilonfilters for size reduction. So composition and decoding time can be saved. It can be applied to many current algorithms including on-the-fly ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-350"
  },
  "siniscalchi11_interspeech": {
   "authors": [
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A bottom-up stepwise knowledge-integration approach to large vocabulary continuous speech recognition using weighted finite state machines",
   "original": "i11_0901",
   "page_count": 4,
   "order": 353,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "A bottom-up, stepwise, knowledge integration framework is proposed to realize detection-based, large vocabulary continuous speech recognition (LVCSR) with a weighted finite state machine (WFSM). The WFSM framework offers a flexible architecture for different types of knowledge network compositions, each of them can be built and optimized independently. Speech attribute detectors are used as an intermediate block to obtain phoneme posterior probabilities over which a phoneme recognition network is designed. Lexical access and syntax knowledge integration over this phoneme network are then performed to deliver the decoded sentences. Experimental evidence illustrates that the proposed system outperforms several hybrid HMM/ANN systems with different configurations on the Wall Street Journal task while it is competitive with conventional LVCSR technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-351"
  },
  "seigel11_interspeech": {
   "authors": [
    [
     "M. S.",
     "Seigel"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Combining information sources for confidence estimation with CRF models",
   "original": "i11_0905",
   "page_count": 4,
   "order": 354,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "Obtaining accurate confidence measures for automatic speech recognition (ASR) transcriptions is an important task which stands to benefit from the use of multiple information sources. This paper investigates the application of conditional random field (CRF) models as a principled technique for combining multiple features from such sources. A novel method for combining suitably defined features is presented, allowing for confidence annotation using lattice-based features of hypotheses other than the lattice 1-best. The resulting framework is applied to different stages of a state-of-the-art large vocabulary speech recognition pipeline, and consistent improvements are shown over a sophisticated baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-352"
  },
  "katsurada11_interspeech": {
   "authors": [
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Shinta",
     "Sawada"
    ],
    [
     "Shigeki",
     "Teshima"
    ],
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Evaluation of fast spoken term detection using a suffix array",
   "original": "i11_0909",
   "page_count": 4,
   "order": 355,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "We previously proposed [1] fast spoken term detection that uses a suffix array as a data structure for searching a large-scale speech documents. In this method, a keyword is divided into sub-keywords, and the phoneme sequences that contain two or more sub-keywords are output as results. Although the search is executed very quickly on a 10,000-h speech database, we only proposed a variety of matching procedures in [1]. In this paper, we compare different varieties of matching procedures in which the number of phonemes in a sub-keyword and the required number of sub-keywords to be contained in a search result are different. We also compare the performance and the process time of our method with typical spoken term detection using an inverted index.\n",
    "",
    "",
    "Katsurada, K., Teshima, S. and Nitta, T., Fast Keyword Detection Using Suffix Array, InterSpeech2009, pp.2147-2150, 2009\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-353"
  },
  "kintzley11_interspeech": {
   "authors": [
    [
     "Keith",
     "Kintzley"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Event selection from phone posteriorgrams using matched filters",
   "original": "i11_1905",
   "page_count": 4,
   "order": 356,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "In this paper we address the issue of how to select a minimal set of phonetic events from a phone posteriorgram while minimizing the loss of information. We derive phone posteriorgrams from two sources, Gaussian mixture models and sparse multilayer perceptrons, and apply phone-specific matched filters to the posteriorgrams to yield a smaller set of phonetic events. We introduce a mutual information based performance measure to compare phonetic event selection techniques and demonstrate that events extracted using matched filters can reduce input data while significantly improving performance of an event-based keyword spotting system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-354"
  },
  "zhang11g_interspeech": {
   "authors": [
    [
     "Yaodong",
     "Zhang"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "A piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping",
   "original": "i11_1909",
   "page_count": 4,
   "order": 357,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "In this paper, we propose a novel lower-bound estimate for dynamic time warping (DTW) methods that use an inner product distance on multi-dimensional posterior probability vectors known as posteriorgrams. Compared to our previous work, the new lower-bound estimate uses piecewise aggregate approximation (PAA) to reduce the time required for calculating the lower-bound estimate. We describe the PAA lower-bound construction process and prove that it can be efficiently used in an admissible K nearest neighbor (KNN) search. The amount of computational savings is quantified by a set of unsupervised spoken keyword spotting experiments. The results show that the newly proposed PAA lower-bound is able to speed up DTW-KNN search by 28% without affecting the keyword spotting performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-355"
  },
  "qin11_interspeech": {
   "authors": [
    [
     "Long",
     "Qin"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "OOV detection and recovery using hybrid models with different fragments",
   "original": "i11_1913",
   "page_count": 4,
   "order": 358,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "In this paper, we address the out-of-vocabulary (OOV) detection and recovery problem by developing three different fragment-word hybrid systems. A fragment language model (LM) and a word LM were trained separately and then combined into a single hybrid LM. Using this hybrid model, the recognizer can recognize any OOVs as fragment sequences. Different types of fragments, such as phones, subwords, and graphones were tested and compared on the WSJ 5k and 20k evaluation sets. The experiment results show that the subword and graphone hybrid systems perform better than the phone hybrid system in both 5k and 20k tasks. Furthermore, given less training data, the subword hybrid system is more preferable than the graphone hybrid system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-356"
  },
  "li11e_interspeech": {
   "authors": [
    [
     "Haiyang",
     "Li"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Tieran",
     "Zheng"
    ]
   ],
   "title": "AUC optimization based confidence measure for keyword spotting",
   "original": "i11_1917",
   "page_count": 4,
   "order": 359,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "Confidence measure plays an important role in keyword spotting. To enhance the effectiveness of the confidence measure, we propose a novel method which improves the performance of keyword spotting by directly maximizing the area under the ROC curve (AUC). Firstly, we approximate the AUC as an objective function with the weighted mean confidence measure. Then, we optimize the objective function by training the weighting factors with the generalized probabilistic descent algorithm. Compared with the current method based on minimum classification error (MCE) criterion, the proposed method makes a global enhancement of ROC curve and does not need to train any threshold. The experiments conducted on the King-ASR-023 database show that the proposed method outperforms both the method averaging phone-level confidences and the method based on MCE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-357"
  },
  "ma11c_interspeech": {
   "authors": [
    [
     "Zejun",
     "Ma"
    ],
    [
     "Xiaorui",
     "Wang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "An empirical study of multilingual spoken term detection",
   "original": "i11_1921",
   "page_count": 4,
   "order": 360,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "This paper introduces the design of multilingual spoken term detection (STD) system using CALLHOME and CALLFRIEND multilingual databases published by Linguistic Data Consortium. For our experiments seven languages namely Arabic, English, German, Japanese, Korean, Chinese Mandarin and Spanish, are used to train and evaluate the STD system.\n",
    "As the core module of our language general STD system, the multilingual automatic speech recogniser combines the acoustic and language models of seven languages into an uniform model set. A lot of our works are focused on the comparison of multilingual acoustic models . the conventional global phoneme set (GPS) based method and the recently proposed subspace GMM (SGMM) method [1] are investigated in detail. The experimental results demonstrate the viability of our multilingual STD system. It is shown that the resulting multilingual system not only supports seven different languages but also gives satisfying performance gains over the monolingual systems.\n",
    "",
    "",
    "D. Povey, L. Burget, M. Agarwal, P. Akyazi, et al., Subspace Gaussian Mixture Models for Speech Recognition, in Proc. ICASSP10, Dallas, Texas, USA, March 2010, pp. 4330-4333.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-358"
  },
  "ma11d_interspeech": {
   "authors": [
    [
     "Zejun",
     "Ma"
    ],
    [
     "Xiaorui",
     "Wang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Fusing multiple confidence measures for Chinese spoken term detection",
   "original": "i11_1925",
   "page_count": 4,
   "order": 361,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "In spoken term detection (STD) task, the confidence measure is used to assess the reliability of detected terms. The widely used confidence measure in STD is based on the normalized lattice posterior probability. In this paper, however, several distinct confidence estimation methods are investigated to improve the baseline lattice confidence: the acoustic and duration confidences are estimated by hybrid Hidden Markov Model / Artificial Neural Network (HMM/ANN) and phonetic duration model respectively. These two confidences plus lattice confidence are linearly interpolated to produce a more reliable confidence measure. The experimental results show the feasibility and effectiveness of our combination approach. The proposed method substantially improves the STD performance, for a 4.8%.11.1% relative equal error rate (EER) reduction on three evaluation sets compared with the baseline lattice confidence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-359"
  },
  "yang11_interspeech": {
   "authors": [
    [
     "Zhanlei",
     "Yang"
    ],
    [
     "Hao",
     "Chao"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Response probability based decoding algorithm for large vocabulary continuous speech recognition",
   "original": "i11_1929",
   "page_count": 4,
   "order": 362,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "Acoustic space is made up of phonemes, and it can be modeled using universal background model (UBM). Therefore, there are some relations between the phonemes and Gaussian mixture components of the UBM. This paper represents these relations by proposing a response probability (RP) model, which describes the location information of speech observations within the whole acoustic space. At decoding stage, proposed RP model is fused with traditional acoustic model (AM) and language model (LM). After integrating RP, the decoder is guided to weaken or enhance different path candidates respectively and directed to extend the most promising paths. Experiments conducted on Mandarin broadcasting speech show that character error rate is relatively reduced by 9.15% when RP model is used and by 11.89% when an improved RP model is used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-360"
  },
  "shan11_interspeech": {
   "authors": [
    [
     "Yuxiang",
     "Shan"
    ],
    [
     "Yan",
     "Deng"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Combining lattice-based language dependent and independent approaches for out-of-language detection in LVCSR",
   "original": "i11_1933",
   "page_count": 4,
   "order": 363,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "In this paper, Out-Of-Language (OOL) detection problem is handled by both language dependent (LD) and language independent (LI) approaches. In the LD approach, a novel speech content and language joint recognition algorithm is proposed, which integrates a phone lattice-based vector space modeling language recognition (LRE) backend into the conventional speech decoding procedure. In the LI approach, lattice derived confidence measures are used. Since these two approaches reflect two different dimensions of uncertainties encoded in lattices, combining them improves both the LRE and OOL detection performance. Experiments also show that for LD approach the detection accuracies can be significantly increased by applying heuristic phone lattice reconstruction. Evaluated on a Mandarin/English mixed conversational telephone speech corpus with a Mandarin speech recognizer, the proposed method achieves an EER of 12.68% in OOL detection, and reduces the recognition error by 33.06%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-361"
  },
  "ito11b_interspeech": {
   "authors": [
    [
     "Naoaki",
     "Ito"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Evaluation of tree-trellis based decoding in over-million LVCSR",
   "original": "i11_1937",
   "page_count": 4,
   "order": 364,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "Very large vocabulary continuous speech recognition (CSR) that can recognize every sentence is one of important goals in speech recognition. Several attempts have been made to achieve very large vocabulary CSR. However, very large vocabulary CSR using a tree-trellis based decoder has not been reported. We report the performance evaluation and improvement of the \"Julius\" treetrellis based decoder in large vocabulary CSR (LVCSR) involving more than one million vocabulary, referred to here as over-million LVCSR. Experiments indicated that Julius achieved a word accuracy of about 91% and a real time factor of about 2 in over-million LVCSR for Japanese newspaper speech transcription.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-362"
  },
  "huang11e_interspeech": {
   "authors": [
    [
     "Hao",
     "Huang"
    ],
    [
     "Bing Hu",
     "Li"
    ]
   ],
   "title": "Lattice based discriminative model combination using automatically induced phonetic contexts",
   "original": "i11_1941",
   "page_count": 4,
   "order": 365,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "Discriminative model combination is to integrate several model scores using discriminatively trained weighting factors. In recent research, context-dependent scaling is often applied. One limitation of this approach is a large number of parameters will be introduced. The large parameter set with limited training data might introduce training instability. In this paper, we propose to use automatically induced contexts modeled by phonetic decision trees. Questions in the tree nodes are chosen to maximize the minimum phone error criterion. First order approximation of objective increase is used for question selection to make tree growing efficient. Experimental results on continuous speech recognition show the method is capable of inducing crucial phonetic contexts and obtains error reduction with many fewer parameters, compared with the results from manually selected phonetic contexts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-363"
  },
  "mishra11_interspeech": {
   "authors": [
    [
     "Taniya",
     "Mishra"
    ],
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Mazin",
     "Gilbert"
    ]
   ],
   "title": "Predicting human perceived accuracy of ASR systems",
   "original": "i11_1945",
   "page_count": 4,
   "order": 366,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "Word error rate (WER), which is the most commonly used method of measuring automatic speech recognition (ASR) accuracy, penalizes all types of ASR errors equally. However, humans differentially weigh different types of ASR errors. They judge ASR errors that distort the meaning of the spoken message more harshly than those that do not. Aiming to align more closely with human perception of ASR accuracy, we developed a new metric HPA (Human Perceived Accuracy) that predicts the subjective perceived accuracy of ASR transcriptions. HPA is computed based on the central idea of differential weighting of different ASR errors. Applied to the particular task of automatically recognizing voicemails, we found that the correlation between HPA and the human judgement of ASR accuracy was significantly higher (r-value=0.91) than the correlation between WER and human judgement (r-value=0.65).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-364"
  },
  "vasilescu11_interspeech": {
   "authors": [
    [
     "I.",
     "Vasilescu"
    ],
    [
     "D.",
     "Yahia"
    ],
    [
     "N.",
     "Snoeren"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Cross-lingual study of ASR errors: on the role of the context in human perception of near-homophones",
   "original": "i11_1949",
   "page_count": 4,
   "order": 367,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "It is widely acknowledged that human listeners significantly outperform machines when it comes to transcribing speech. This paper presents a paradigm for perceptual experiments that aims to increase our understanding of human and automatic speech recognition errors. The role of the context length is investigated through perceptual recovery of small homophonic words or near-homophones yielding frequent automatic transcription errors. The same experimental protocol of varied size speech stimuli transcription is applied to both French and English. Our hypothesis is that ambiguity due to homophonic words reduces with context size for both languages, which in turn should entail reduced perception and transcription errors. The results show that context plays a central role as the human word error rate decreases significantly with increasing context. The long-term aim is to improve the modelling of such ambiguous items to reduce automatic errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-365"
  },
  "saito11b_interspeech": {
   "authors": [
    [
     "Tatsuhiko",
     "Saito"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Yohei",
     "Okato"
    ],
    [
     "Akio",
     "Horii"
    ]
   ],
   "title": "Performance prediction of speech recognition using average-voice-based speech synthesis",
   "original": "i11_1953",
   "page_count": 4,
   "order": 368,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "This paper describes a performance prediction technique of a speech recognition system using a small amount of target speakers' data. In the conventional HMM-based technique, a speaker-dependent model was used and thus a considerable amount of training data was needed. To reduce the amount of training data, we introduce an average voice model as a prior knowledge for the target speakers' acoustic models, and adapt it to the target speakers' ones using speaker adaptation. Experimental results show that the use of average voice model effectively save the amount of training data of the target speakers, and the prediction accuracy is significantly improved compared to the conventional technique especially when a smaller amount of training data is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-366"
  },
  "haznedaroglu11_interspeech": {
   "authors": [
    [
     "Ali",
     "Haznedaroglu"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "Confidence measures for turkish call center conversations",
   "original": "i11_1957",
   "page_count": 4,
   "order": 369,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "Automatic speech recognition accuracies of call canter conversations are still below intended levels due to harsh conditions such as channel distortions, external noises, co-articulated speech, etc. Agglutinative and free word order nature of Turkish degrades the recognition performances further; therefore the usage of confidence measures (CMs) is inevitable to retrieve correct information from the calls. In this paper, two conversational CMs, namely speech overlap ratio and opposite party energy level, are proposed, and tested together with single-channel confidence measures on Turkish stereo call center recordings. Experimental results show that conversational CMs improve the rating accuracies of the utterances with respect to their recognition rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-367"
  },
  "asami11_interspeech": {
   "authors": [
    [
     "Taichi",
     "Asami"
    ],
    [
     "Narichika",
     "Nomoto"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Satoshi",
     "Takahashi"
    ]
   ],
   "title": "Spoken document confidence estimation using contextual coherence",
   "original": "i11_1961",
   "page_count": 4,
   "order": 370,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "Selecting well-recognized transcripts is critical if information retrieval systems are to extract business intelligence from massive spoken document databases. To achieve this goal, we target spoken document confidence measures that represent the recognition rates of each document. We focus on the incoherent word occurrences over several utterances in ill-recognized transcripts of spoken documents. The proposed method uses contextual coherence as a measure of spoken document confidence. The contextual coherence is formulated as the mean of pointwise mutual information (PMI). We also propose a smoothing method of PMI, which deals with the data sparseness problem. Compared to the conventional method, our smoothing technique offers improved correlation coefficients between spoken document confidence scores and recognition rates from 0.573 to 0.672. Moreover, an even higher correlation coefficient, 0.710, is achieved by combining the contextual-based and decoder-based confidence measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-368"
  },
  "hazen11_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "Latent topic modeling for audio corpus summarization",
   "original": "i11_0913",
   "page_count": 4,
   "order": 371,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "This work presents techniques for automatically summarizing the topical content of an audio corpus. Probabilistic latent semantic analysis (PLSA) is used to learn a set of latent topics in an unsupervised fashion. These latent topics are ranked by their relative importance in the corpus and a summary of each topic is generated from signature words that aptly describe the content of that topic. This paper presents techniques for producing a high quality summarization. An example summarization of conversational data from the Fisher corpus that demonstrates the effectiveness of our approach is presented and evaluated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-369"
  },
  "dufour11_interspeech": {
   "authors": [
    [
     "Richard",
     "Dufour"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Paul",
     "Deléglise"
    ]
   ],
   "title": "Investigation of spontaneous speech characterization applied to speaker role recognition",
   "original": "i11_0917",
   "page_count": 4,
   "order": 372,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "Extracting information from large data is a challenging task. In this paper, we investigate the link between speech spontaneity levels and speaker roles, and the relevance to use an automatic spontaneous speech characterization as a speaker role identification feature. Applying this automatic spontaneous speech characterization system to a broadcast news corpus containing ten manually labeled speaker roles allowed us to highlight this relationship. So, we propose to directly apply the spontaneous speech characterization approach in order to automatically recognize speaker roles. Experimental results show that characteristics used to detect speech spontaneity could be very useful to recognize speaker roles, as we reached an overall classification precision of 74.4%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-370"
  },
  "muscariello11_interspeech": {
   "authors": [
    [
     "Armando",
     "Muscariello"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Zero-resource audio-only spoken term detection based on a combination of template matching techniques",
   "original": "i11_0921",
   "page_count": 4,
   "order": 373,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "Spoken term detection is a well-known information retrieval task that seeks to extract contentful information from audio by locating occurrences of known query words of interest. This paper describes a zero-resource approach to such task based on pattern matching of spoken term queries at the acoustic level. The template matching module comprises the cascade of a segmental variant of dynamic time warping and a self-similarity matrix comparison to further improve robustness to speech variability. This solution notably differs from more traditional train and test methods that, while shown to be very accurate, rely upon the availability of large amounts of linguistic resources. We evaluate our framework on different parameterizations of the speech templates: raw MFCC features and Gaussian posteriorgrams, French and English phonetic posteriorgrams output by two different state of the art phoneme recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-371"
  },
  "kim11d_interspeech": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "David C.",
     "Gibbon"
    ]
   ],
   "title": "Automatic learning in content indexing service using phonetic alignment",
   "original": "i11_0925",
   "page_count": 4,
   "order": 374,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "Content indexing has become necessary, not just optional, in the era where broadcast, cable and Internet produce huge amounts of media daily. Text information from spoken audio is still a key feature to understand content along with other meta-data and video features. In this paper, a new method is introduced to improve transcription quality, which allows more accurate content indexing. Our method finds phonetic similarities between two imperfect sources, closed captions and ASR outputs, and aligns them together to make quality transcriptions. In the process, even out-of-vocabulary words could be learned automatically. Given broadcast news audio and closed captions, our experimental results show that the proposed method, on average, improves word correct rates 11% from the ASR output using the baseline language model and 6% from the one using the adapted language model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-372"
  },
  "chen11c_interspeech": {
   "authors": [
    [
     "Pei-Ning",
     "Chen"
    ],
    [
     "Kuan-Yu",
     "Chen"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Leveraging relevance cues for improved spoken document retrieval",
   "original": "i11_0929",
   "page_count": 4,
   "order": 375,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "Spoken document retrieval (SDR) has emerged as an active area of research in the speech processing community. The fundamental problems facing SDR are generally three-fold: 1) a query is often only a vague expression of an underlying information need, 2) there probably would be word usage mismatch between a query and a spoken document even if they are topically related to each other, and 3) the imperfect speech recognition transcript carries wrong information and thus deviates somewhat from representing the true theme of a spoken document. To mitigate the above problems, in this paper, we study a novel use of a relevance language modeling framework for SDR. It not only inherits the merits of several existing techniques but also provides a flexible but systematic way to render the lexical and topical relationships between a query and a spoken document. Moreover, we also investigate representing the query and documents with different granularities of index features to work in conjunction with the various relevance cues. Experiments conducted on the TDT SDR task show promise of the methods deduced from our retrieval framework when compared with a few existing retrieval methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-373"
  },
  "chen11d_interspeech": {
   "authors": [
    [
     "Yun-Nung",
     "Chen"
    ],
    [
     "Yu",
     "Huang"
    ],
    [
     "Ching-Feng",
     "Yeh"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms",
   "original": "i11_0933",
   "page_count": 4,
   "order": 376,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "This paper proposes an improved approach for spoken lecture summarization, in which random walk is performed on a graph constructed with automatically extracted key terms and probabilistic latent semantic analysis (PLSA). Each sentence of the document is represented as a node of the graph and the edge between two nodes is weighted by the topical similarity between the two sentences. The basic idea is that sentences topically similar to more important sentences should be more important. In this way all sentences in the document can be jointly considered more globally rather than individually. Experimental results showed significant improvement in terms of ROUGE evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-374"
  },
  "claveau11_interspeech": {
   "authors": [
    [
     "Vincent",
     "Claveau"
    ],
    [
     "Sébastien",
     "Lefèvre"
    ]
   ],
   "title": "Topic segmentation of TV-streams by mathematical morphology and vectorization",
   "original": "i11_1105",
   "page_count": 4,
   "order": 377,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "A fine-grained segmentation of Radio or TV broadcasts is an essential step for most multimedia processings. Applying segmentation algorithms to the speech transcripts seems straightforward. Yet, most of these algorithms are not suited when dealing with short segments or noisy data. In this paper, we propose a new segmentation technique inspired from the image segmentation field and relying on a new way to compute similarities between candidate segments. This new topic segmentation technique is evaluated on two corpora of French TV broadcasts on which it largely outperforms other existing approaches from the state-of-the-art.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-375"
  },
  "lu11_interspeech": {
   "authors": [
    [
     "Mimi",
     "Lu"
    ],
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Probabilistic latent semantic analysis for broadcast news story segmentation",
   "original": "i11_1109",
   "page_count": 4,
   "order": 378,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "This paper proposes to perform probabilistic latent semantic analysis (PLSA) for broadcast news (BN) story segmentation. PLSA exploits a deeper underlying relation among terms beyond their occurrences thus conceptual matching can be employed to replace literal term matching. Different from text segmentation, lexical based BN story segmentation has to be carried out over LVCSR transcripts, where the incorrect recognition of out-of-vocabulary words inevitably impacts the semantic relation. We use phoneme subwords as the basic term units to address this problem. We integrate a cross entropy measurement with PLSA to depict lexical cohesion and compare its performance with the widely used cosine similarity metric. Furthermore, we evaluate two approaches, namely TextTiling and dynamic programming (DP), for story boundary identification. Experimental results show that the PLSA based methods bring a significant performance boost to story segmentation and the cross entropy based DP approach provides the best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-376"
  },
  "gouvea11_interspeech": {
   "authors": [
    [
     "Evandro",
     "Gouvêa"
    ]
   ],
   "title": "Hybrid speech recognition for voice search: a comparative study",
   "original": "i11_1113",
   "page_count": 4,
   "order": 379,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "We compare different systems for use in information retrieval of items by voice. These systems differ only in the unit they use: words, a subwords, a combination of these into a hybrid, and phones. The subword set is derived by splitting words using a Minimum Description Length (MDL) criterion. In general, we convert an index written in terms of words into an index written in terms of these different units. A speech recognition engine that uses a language model and pronunciation dictionary built from each such an inventory of units is completely independent from the information retrieval task, and can, therefore, remain fixed, making this approach ideal for resource constrained systems. We demonstrate that recognition accuracy and recall results at higher OOV rates are much superior for the hybrid system than the alternatives. On a music lyrics task at 80% OOV, the hybrid system has a recall of 82.9%, compared to 75.2% for the subword-based one and 47.4% for a word system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-377"
  },
  "peng11_interspeech": {
   "authors": [
    [
     "Bo",
     "Peng"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Bo",
     "Zhang"
    ]
   ],
   "title": "A new phonetic candidate generator for improving search query efficiency",
   "original": "i11_1117",
   "page_count": 4,
   "order": 380,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "Misspelled query due to homophones or mispronunciation is difficult to be corrected in the conventional spelling correction methods. In phonetic candidate generation, the generator is to produce candidates which are phonetically similar to a given query. In this paper, we present a new phonetic candidate generator for improving the search efficiency of a query. The proposed generator consists of three modules: letter-to-sound (LTS) conversion, phonetic \"trie\" and phonetic similarity estimator based upon Levenshtein distance and Kullback-Leibler Divergence (KLD) between phones. This generator yields a significant improvement over Double-metaphone in terms of candidate accuracy and effective candidate set size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-378"
  },
  "suzuki11_interspeech": {
   "authors": [
    [
     "Yukiko",
     "Suzuki"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Towards voice-input symbolic pattern retrieval using parameter-based search",
   "original": "i11_1121",
   "page_count": 4,
   "order": 381,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "This paper proposes a symbolic pattern retrieval method using emotional feature vectors. Queries and symbolic patterns are represented by emotional vectors composed of eight numerical parameters. Since the proposed method uses numerical vectors close to raw data instead of recognized text, the information loss by data conversion is small. This point is advantageous compared with conventional text-based search such as recent spoken document retrieval approach. Five similarity measures were compared on a test collection. The cos similarity and the Euclidean distance showed the best performance among five similarity measures. OOV analysis clarified several problems for achieving voice-input symbolic pattern retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-379"
  },
  "gupta11_interspeech": {
   "authors": [
    [
     "Vikram",
     "Gupta"
    ],
    [
     "Jitendra",
     "Ajmera"
    ],
    [
     "Arun",
     "Kumar"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "A language independent approach to audio search",
   "original": "i11_1125",
   "page_count": 4,
   "order": 382,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "In this paper, we propose an approach towards audio search where no language specific resources are required. This approach is most useful in those scenarios where no training data exists to create an automatic speech recognition (ASR) system for a language, e.g. in the case of most regional languages or dialects. In this approach, a Multilayer perceptron (MLP) is trained for a language where the training data exists, e.g. English. This MLP estimates a sequence of probability vectors for an audio segment, which is referred to as the posteriorgram representation for that segment. Components of the probability vector are posterior probabilities of English phonemes at any given frame of speech. Template matching technique is then used to compare the query-posteriorgram against the contentposteriorgram over the searchable audio-content. We present experiments in this paper to show that, even for other language like Hindi, the probabilities obtained from the neural network trained on English provide a characteristic representation for a word. A dynamic time warping algorithm with appropriate modifications is applied and encouraging P@N performance of 46.24% for Hindi and 65.22% for English for the task of audio search is reported while using the same MLP trained using English data in both the cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-380"
  },
  "aronowitz11c_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Speaker diarization using a priori acoustic information",
   "original": "i11_0937",
   "page_count": 4,
   "order": 383,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "Speaker diarization is usually performed in a blind manner without using a priori knowledge about the identity or acoustic characteristics of the participating speakers. In this paper we propose a novel framework for incorporating available a priori knowledge such as potential participating speakers, channels, background noise and gender, and integrating these knowledge sources into blind speaker diarization-type algorithms. We demonstrate this framework on two tasks. The first task is agent-customer speaker diarization for call-center phone calls and the second task is speaker-diarization for a PDA recorder which is part of an assistive living system for the elderly. For both of these tasks, incorporating the a priori information into our blind speaker diarization systems significantly improves diarization accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-381"
  },
  "boakye11_interspeech": {
   "authors": [
    [
     "Kofi",
     "Boakye"
    ],
    [
     "Oriol",
     "Vinyals"
    ],
    [
     "Gerald",
     "Friedland"
    ]
   ],
   "title": "Improved overlapped speech handling for speaker diarization",
   "original": "i11_0941",
   "page_count": 4,
   "order": 384,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "We present our ongoing work in addressing the issue of overlapped speech in speaker diarization through the use of overlap segmentation, overlapped speech exclusion, and overlap segment labeling. Using feature analysis, we identify the most salient features from a candidate list including those from our previous system and a set of newly proposed features. In addition, through independent optimization of overlap exclusion and labeling, we obtain a relative diarization error rate improvement of 15.1% on a sampled subset of the AMI Meeting Corpus, more than double our previous result. When analyzed independently, we show that the performance improvement due to overlapped speech exclusion now rivals that of an oracle system using reference overlap segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-382"
  },
  "shum11_interspeech": {
   "authors": [
    [
     "Stephen",
     "Shum"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Ekapol",
     "Chuangsuwanich"
    ],
    [
     "Douglas",
     "Reynolds"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Exploiting intra-conversation variability for speaker diarization",
   "original": "i11_0945",
   "page_count": 4,
   "order": 385,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "In this paper, we propose a new approach to speaker diarization based on the Total Variability approach to speaker verification. Drawing on previous work done in applying factor analysis priors to the diarization problem, we arrive at a simplified approach that exploits intra-conversation variability in the Total Variability space through the use of Principal Component Analysis (PCA). Using our proposed methods, we demonstrate the ability to achieve state-of-the-art performance (0.9% DER) in the diarization of summed-channel telephone data from the NIST 2008 SRE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-383"
  },
  "nishida11_interspeech": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Speaker clustering based on non-negative matrix factorization",
   "original": "i11_0949",
   "page_count": 4,
   "order": 386,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "This paper addresses unsupervised speaker clustering for multiparty conversations. Hierarchical clustering methods were mainly used in previous studies. However, these methods require many processes, such as distance calculation and cluster merging, when there are many utterances in conversation data. We propose a clustering method based on non-negative matrix factorization. The proposed method can perform fast and robust clustering by decomposing a matrix consisting of distances between models. We conducted speaker clustering experiments using a Bayesian information criterion based method, a method based on the likelihood ratio between Gaussian mixture models, and the proposed method. Experimental results showed that the proposed method achieves higher clustering accuracy than these conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-384"
  },
  "yella11_interspeech": {
   "authors": [
    [
     "Sree Harsha",
     "Yella"
    ],
    [
     "Fabio",
     "Valente"
    ]
   ],
   "title": "Information bottleneck features for HMM/GMM speaker diarization of meetings recordings",
   "original": "i11_0953",
   "page_count": 4,
   "order": 387,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "Improved diarization results can be obtained through combination of multiple systems. Several combination techniques have been proposed based on output voting, initialization and also integrated approaches. This paper proposes and investigates a novel approach to combine diarization systems through the use of features. A first diarization system, based on the Information Bottleneck, is used to generate a set of features that contain information relevant to the clustering. Those features are later used in conjunction with conventional MFCC in a second diarization system. This method is inspired from the TANDEM framework in ASR. While being fully integrated, the approach does not need modifications to any of the two systems in order to integrate the information. Experiments on 24 recordings from the NIST RT06/RT07/RT09 evaluations collected in five meeting rooms reveal that when the IB features are used together with MFCC, the total speaker error is reduced from\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-385"
  },
  "wang11g_interspeech": {
   "authors": [
    [
     "D.",
     "Wang"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "David",
     "Dean"
    ]
   ],
   "title": "Cross likelihood ratio based speaker clustering using eigenvoice models",
   "original": "i11_0957",
   "page_count": 4,
   "order": 388,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "This paper proposes the use of eigenvoice modeling techniques with the Cross Likelihood Ratio (CLR) as a criterion for speaker clustering within a speaker diarization system. The CLR has previously been shown to be a robust decision criterion for speaker clustering using Gaussian Mixture Models. Recently, eigenvoice modeling techniques have become increasingly popular, due to its ability to adequately represent a speaker based on sparse training data, as well as an improved capture of differences in speaker characteristics. This paper hence proposes that it would be beneficial to capitalize on the advantages of eigenvoice modeling in a CLR framework. Results obtained on the 2002 Rich Transcription (RT- 02) Evaluation dataset show an improved clustering performance, resulting in a 35.1% relative improvement in the overall Diarization Error Rate (DER) compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-386"
  },
  "zibert11_interspeech": {
   "authors": [
    [
     "Janez",
     "Žibert"
    ],
    [
     "France",
     "Mihelič"
    ]
   ],
   "title": "Prosodic and phonetic features for speaker clustering in speaker diarization systems",
   "original": "i11_1033",
   "page_count": 4,
   "order": 389,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "This work is focused on speaker clustering methods that are used in speaker diarization systems. The purpose of speaker clustering is to associate together segments that belong to the same speaker and is usually applied in the last stage of the speaker-diarization process. We concentrate on developing proper representations of speaker segments for clustering. We realize two different speaker clustering systems. The first is a standard approach using a bottomup agglomerative clustering principle with the Bayesian Information Criterion as a merging criterion. In the second system we developed a fusion-based speaker-clustering, where speaker segments are modeled by acoustic and prosodic representations. In this way we additionally model the speaker prosodic and phonetic characteristics and combine them with the basic acoustic information of speakers. This leads to improved clustering of the segments in the case of similar speaker acoustic properties and poor acoustic conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-387"
  },
  "huijbregts11_interspeech": {
   "authors": [
    [
     "Marijn",
     "Huijbregts"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Diarization-based speaker retrieval for broadcast television archives",
   "original": "i11_1037",
   "page_count": 4,
   "order": 390,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "In this study we extend a query-by-example diarization-based speaker retrieval system to a full speaker retrieval system for broadcast television. The envisioned system is capable of finding all speakers in an archive using their names instead of example speech fragments. Information extracted from a television guide is used to label speaker clusters that most likely correspond to the found names. As part of the labeling process, all speaker clusters are first classified automatically based on their role in the programs they appear in. The role classification accuracy is 64% on our evaluation set. Speaker names can automatically be attributed to a fraction of the speaker clusters with an accuracy of 70%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-388"
  },
  "zelenak11_interspeech": {
   "authors": [
    [
     "Martin",
     "Zelenák"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "The detection of overlapping speech with prosodic features for speaker diarization",
   "original": "i11_1041",
   "page_count": 4,
   "order": 391,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "Overlapping speech is responsible for a certain amount of errors produced by standard speaker diarization systems in meeting environment. We are investigating a set of prosody-based long-term features as a potential complement to our overlap detection system relying on short-term spectral parameters. The most relevant features are selected in a two-step process. They are firstly evaluated and sorted according to mRMR criterion and then the optimal number is determined by iterative wrapper approach. We show that the addition of prosodic features decreased overlap detection error. Detected overlap segments are used in speaker diarization to recover missed speech by assigning multiple speaker labels and to increase the purity of speaker clusters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-389"
  },
  "parthasarathi11_interspeech": {
   "authors": [
    [
     "Sree Hari Krishnan",
     "Parthasarathi"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Daniel",
     "Gatica-Perez"
    ]
   ],
   "title": "LP residual features for robust, privacy-sensitive speaker diarization",
   "original": "i11_1045",
   "page_count": 4,
   "order": 392,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "We present a comprehensive study of linear prediction residual for speaker diarization on single and multiple distant microphone conditions in privacy-sensitive settings, a requirement to analyze a wide range of spontaneous conversations. Two representations of the residual are compared, namely real-cepstrum and MFCC, with the latter performing better. Experiments on RT06eval show that residual with subband information from 2.5 kHz to 3.5 kHz and spectral slope yields a performance close to traditional MFCC features. As a way to objectively evaluate privacy in terms of linguistic information, we perform phoneme recognition. Residual features yield low phoneme accuracies compared to traditional MFCC features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-390"
  },
  "ghaemmaghami11_interspeech": {
   "authors": [
    [
     "Houman",
     "Ghaemmaghami"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Extending the task of diarization to speaker attribution",
   "original": "i11_1049",
   "page_count": 4,
   "order": 393,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "In this paper we extend the concept of speaker annotation within a single-recording, or speaker diarization, to a collection wide approach we call speaker attribution. Accordingly, speaker attribution is the task of clustering expectantly homogenous intersession clusters obtained using diarization according to common crossrecording identities. The result of attribution is a collection of spoken audio across multiple recordings attributed to speaker identities. In this paper, an attribution system is proposed using meanonly MAP adaptation of a combined-gender UBM to model clusters from a perfect diarization system, as well as a JFA-based system with session variability compensation. The normalized crosslikelihood ratio is calculated for each pair of clusters to construct an attribution matrix and the complete linkage algorithm is employed to conduct clustering of the inter-session clusters. A matched cluster purity and coverage of 87.1% was obtained on the NIST 2008 SRE corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-391"
  },
  "tran11b_interspeech": {
   "authors": [
    [
     "Viet-Anh",
     "Tran"
    ],
    [
     "Viet Bac",
     "Le"
    ],
    [
     "Claude",
     "Barras"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Comparing multi-stage approaches for cross-show speaker diarization",
   "original": "i11_1053",
   "page_count": 4,
   "order": 394,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "Acoustic speaker diarization is investigated for situations where a collection of shows from the same source needs to be processed. In this case, the same speaker should receive the same label across all shows. We compare different architectures for cross-show speaker diarization: the obvious concatenation of all shows, a hybrid system combining first a local clustering stage followed by a global clustering stage, and an incremental system which processes the shows in a predefined order and updates the speaker models accordingly. This latter system being best suited to real applicative situations. These three strategies were compared to a baseline single-show system on a set of 46 ten-minutes samples of British English scientific podcasts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-392"
  },
  "turco11_interspeech": {
   "authors": [
    [
     "Giuseppina",
     "Turco"
    ],
    [
     "Michele",
     "Gubian"
    ],
    [
     "Jessamyn",
     "Schertz"
    ]
   ],
   "title": "A quantitative investigation of the prosody of verum focus in Italian",
   "original": "i11_0961",
   "page_count": 4,
   "order": 395,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "In this study we present a preliminary investigation of the prosodic marking of Verum focus (VF) in Italian, which is said to be realized with a pitch accent on the finite verb (e.g. A: Paul has not eaten the banana - B: (No), Paul HAS eaten the banana!). We tried to discover whether and how Italian speakers prosodically mark VF when producing full-fledged sentences using a semi-spontaneous production experiment on 27 speakers. Speech rate and f0 contours were extracted using automatic data processing tools and were subsequently analysed using Functional Data Analysis (FDA), which allowed for automatic visualization of patterns in the contour shapes. Our results show that the postfocal region of VF sentences exhibit faster speech rate and lower f0 compared to non-VF cases. However, an expected consistent difference of f0 effect on the focal region of the VF sentence was not found in this analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-393"
  },
  "dorn11_interspeech": {
   "authors": [
    [
     "Amelie",
     "Dorn"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ]
   ],
   "title": "Effects of focus on f_0 and duration in irish (gaelic) declaratives",
   "original": "i11_0965",
   "page_count": 4,
   "order": 396,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "This pilot study investigates the effects of focus (broad, narrow and contrastive) on tonal patterns, f0 scaling and duration of accented syllables and rhythmic feet for a controlled dataset in Donegal Irish. Results show differences in pre-focal tonal patterns between broad focus and the other focus types. Narrow and contrastive focus renditions are implemented by largely the same phonetic means. Focused domains are overall longer in duration and have wider f0 excursion than broad focus. Durational differences depend on sentence position.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-394"
  },
  "cole11_interspeech": {
   "authors": [
    [
     "Jennifer",
     "Cole"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "The phonology and phonetics of perceived prosody: what do listeners imitate?",
   "original": "i11_0969",
   "page_count": 4,
   "order": 397,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "An imitation experiment tests the hypothesis that when asked to reproduce a spontaneously-spoken utterance that they hear, speakers imitate the prosody of the stimulus in its phonological structure more accurately than the phonetic details. Results suggest that speakers rarely distort the presence of a pitch accent or an intonational phrase boundary, but more often change the nature of the phonetic cues, e.g. the duration of a pause or the occurrence of irregular pitch periods associated with boundaries and accents in American English. These findings argue for an encoding of phonological prosodic structure that is separate from the phonetic cues that signal that structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-395"
  },
  "michelas11_interspeech": {
   "authors": [
    [
     "Amandine",
     "Michelas"
    ],
    [
     "Noël",
     "Nguyen"
    ]
   ],
   "title": "Uncovering the effect of imitation on tonal patterns of French accentual phrases",
   "original": "i11_0973",
   "page_count": 4,
   "order": 398,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "French accentual phrases (APs) are characterized by the presence of a typical final fo rise (LH*) and an optional/additional initial fo rise (LHi). This study tested whether between-speaker speech imitation influenced the realization of APs tonal patterns. The experiment was based on APs containing a function word plus a bisyllabic content word, whose tonal patterns differed in the potential placement of an optional/initial high tone (Hi). In two shadowing tasks (without/with explicit instructions to imitate the speaker's way of pronouncing the stimuli), participants produced more initial high tones when they heard a stimulus including both initial and final high tones relative to stimuli which only a final high tone was present. Thus, imitation influences the realization of APs tonal patterns in French.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-396"
  },
  "prieto11_interspeech": {
   "authors": [
    [
     "Pilar",
     "Prieto"
    ],
    [
     "Cecilia",
     "Pugliesi"
    ],
    [
     "Joan",
     "Borràs-Comes"
    ],
    [
     "Ernesto",
     "Arroyo"
    ],
    [
     "Josep",
     "Blat"
    ]
   ],
   "title": "Crossmodal prosodic and gestural contribution to the perception of contrastive focus",
   "original": "i11_0977",
   "page_count": 4,
   "order": 399,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "Speech prosody has traditionally been analyzed in terms of acoustic features. Even though visual features and gestures have been shown to help and enhance linguistic processing, the conventional view is that facial and body gesture information in oral (non-sign) languages tends to be redundant and has the role of helping the hearer recover the meaning of an utterance. We conducted two perception experiments with a 3D animated character showing conflicting auditory and visual information to investigate two related questions regarding the importance of gestures in conveying prosodic meaning: (a) how important are facial cues with respect to auditory cues for the perception of contrastive focus?; and (b) what is the relevance of the different gestural movements (i.e., head nod and eyebrow raising) for the perception of this type of focus? Our findings reveal that the visual component is crucial in the semantic interpretation of contrastive focus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-397"
  },
  "cvejic11_interspeech": {
   "authors": [
    [
     "Erin",
     "Cvejic"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Temporal relationship between auditory and visual prosodic cues",
   "original": "i11_0981",
   "page_count": 4,
   "order": 400,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "It has been reported that non-articulatory visual cues to prosody tend to align with auditory cues, emphasizing auditory events that are in close alignment (visual alignment hypothesis). We investigated the temporal relationship between visual and auditory prosodic cues in a large corpus of utterances to determine the extent to which non-articulatory visual prosodic cues align with auditory ones. Six speakers saying 30 sentences in three prosodic conditions (#215;2 repetitions) were recorded in a dialogue exchange task, to measure how often eyebrow movements and rigid head tilts aligned with auditory prosodic cues, the temporal distribution of such movements, and the variation across prosodic conditions. The timing of brow raises and head tilts were not aligned with auditory cues, and the occurrence of visual cues was inconsistent, lending little support for the visual alignment hypothesis. Different types of visual cues may combine with auditory cues in different ways to signal prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-398"
  },
  "szaszak11_interspeech": {
   "authors": [
    [
     "György",
     "Szaszák"
    ],
    [
     "Katalin",
     "Nagy"
    ],
    [
     "András",
     "Beke"
    ]
   ],
   "title": "Analysing the correspondence between automatic prosodic segmentation and syntactic structure",
   "original": "i11_1057",
   "page_count": 4,
   "order": 401,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "Prosody and syntax are highly related, even if the prosodic structure cannot be directly mapped to the syntactic one and vice versa. This paper presents an experiment for exploring in what degree a powerful HMM-based automatic prosodic segmentation tool can recover the syntactic structure of an utterance in speech understanding systems. Results show that the approach is capable of recalling up to 92% of syntactic clause boundaries and up to 71% of embedded syntactic phrase boundaries based on the detection of phonological phrases. Recall rates do not depend further on the syntactic level (whether the phrase is multiply embedded or not), but clause boundaries can be well separated from lower level syntactic phrases based on the type of the aligned phonological phrase(s). These findings can be exploited in speech understanding systems, allowing for the recovery of the skeleton of the syntactic structure, based purely\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-399"
  },
  "tepperman11_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Emily",
     "Nava"
    ]
   ],
   "title": "Long-distance rhythmic dependencies and their application to automatic language identification",
   "original": "i11_1061",
   "page_count": 4,
   "order": 402,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "The perception of rhythmic differences among languages relies on varieties in periodicity within prominence groups. But the consensus in phonetic research on rhythm is that existing measures don't capture true rhythm by that definition - instead, they merely measure short-term timing. This work proposes a new rhythm measure, the Generalized Variability Index (GVI), that examines durational contexts over arbitrarily long linguistic distances. To evaluate this new measure, we conducted a set of experiments in automatic language identification using large amounts of data from 11 languages in the Globalphone and TIMIT corpora. When added to baseline rhythm measures, these new GVI features offer absolute improvement in 11-way language classification accuracy by as much as 12%. Moreover, the addition of wider and wider durational context in the GVI continues to contribute information useful for automatic language ID, abating in usefulness only at a distance of about 10 syllables.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-400"
  },
  "rosenberg11_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Symbolic and direct sequential modeling of prosody for classification of speaking-style and nativeness",
   "original": "i11_1065",
   "page_count": 4,
   "order": 403,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "In this paper, we explore the differences between direct and symbolic sequential modeling of prosody. We use sequential models to characterize speech in two tasks, classifying speaking-style and distinguishing native from non-native speech. We explore the use of a spike-and-slab model to directly model pitch contour data. We find in both of these tasks that sequences of symbolic prosodic events to lead to improved performance over approaches that model pitch contours directly. We also explore the use of hypothesized prosodic events in both tasks. We find the speaking-style results to be robust to automatic annotation, while, when classifying nativeness, the spike-and-slab model leads to better performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-401"
  },
  "gu11_interspeech": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Ting",
     "Zhang"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Prosodic analysis and perception of Mandarin utterances conveying attitudes",
   "original": "i11_1069",
   "page_count": 4,
   "order": 404,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "After differentiating attitudes from emotions, the present work investigates prosodic manifestations and perceptual attributes of Mandarin utterances conveying various attitudes. A speech corpus was designed to incorporate five classes of attitudes: friendly/hostile, polite/rude, serious/joking, praising/blaming, and confident/uncertain. Perceptual experiment reveals two different patterns between intended and perceived attitudes. Statistical analysis of prosodic features shows that speech rate is distinctive in all five classes, while utterance-level F0 height and F0 range are distinctive only for some classes. Moreover, F0 features in the words carrying sentential stress are more distinctive than utterance-level settings. The relation between perception and acoustics is also examined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-402"
  },
  "cheng11b_interspeech": {
   "authors": [
    [
     "Chierh",
     "Cheng"
    ],
    [
     "Michele",
     "Gubian"
    ]
   ],
   "title": "Predicting taiwan Mandarin tone shapes from their duration",
   "original": "i11_1073",
   "page_count": 4,
   "order": 405,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "A preliminary study on modelling tonal variation as a function of duration is carried out. An experimentally controlled acoustic database was utilized to construct functional linear models. In the construction of the linear models, duration was used as independent variable in predicting the shape of disyllabic pitch contours in Taiwan Mandarin, given the target tone sequences. Results showed that by moving duration values from short to long, tonal curve shapes of disyllables ranging from non-reduced to reduced were approximated with an adequate goodness-of-fit (usually below one semitone RMSE). This study provides a novel approach to examine the relation between duration and F0 realisation of small units such as disyllables and also supports the time pressure account of phonetic reduction in general.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-403"
  },
  "wollermann11_interspeech": {
   "authors": [
    [
     "Charlotte",
     "Wollermann"
    ],
    [
     "Ulrich",
     "Schade"
    ],
    [
     "Bernhard",
     "Schröder"
    ]
   ],
   "title": "Variation of accent type and of context - influences on pragmatic focus interpretation",
   "original": "i11_1077",
   "page_count": 4,
   "order": 406,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "We present an empirical study on the variation of accent type and of context on pragmatic focus interpretation. The material was based on audio-recordings of nine German speakers who were instructed to read dialogues with embedded question-answer pairs in which the answers constituted the pragmatic focus of the utterance. Different accent types occurred for marking the focus constituent. The audio-material was presented to 53 subjects. Interpretation was tested by using pictures intended to illustrate the (non-)exhaustive reading. When presenting the picture illustrating the non-exhaustive reading, the results show in general a significant influence of both context and prosody, but the contextual influence is stronger.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-404"
  },
  "sun11d_interspeech": {
   "authors": [
    [
     "Xie",
     "Sun"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "New methods for template selection and compression in continuous speech recognition",
   "original": "i11_0985",
   "page_count": 4,
   "order": 407,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "We propose a maximum likelihood method for selecting template representatives, and in order to include more information in the selected template representatives, we further propose to create compressed template representatives by Gaussian mixture model (GMM) merging algorithm. A Kullback-Leibler (KL) divergence based local distance is proposed for Dynamic Time Warping (DTW) in template matching. Experimental results on the tasks of TIMIT phone recognition and large vocabulary continuous speech recognition demonstrated that the proposed template selection method significantly improved the recognition accuracy over the HMM baseline while only 5% or 10% templates were selected from the total templates, and the template compression method has provided further recognition accuracy gains over the template selection method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-405"
  },
  "zhang11h_interspeech": {
   "authors": [
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Structured support vector machines for noise robust continuous speech recognition",
   "original": "i11_0989",
   "page_count": 4,
   "order": 408,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "The use of discriminative models is an interesting alternative to generative models for speech recognition. This paper examines one form of these models, structured support vector machines (SVMs), for noise robust speech recognition. One important aspect of structured SVMs is the form of the joint feature space. In this work features based on generative models are used, which allows model-based compensation schemes to be applied to yield robust joint features. However, these features require the segmentation of frames into words, or subwords, to be specified. In previous work this segmentation was obtained using generative models. Here the segmentations are refined using the parameters of the structured SVM. A Viterbi-like scheme for obtaining \"optimal\" segmentations, and modifications to the training algorithm to allow them to be efficiently used, are described. The performance of the approach is evaluated on a noise corrupted continuous digit task: AURORA 2.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-406"
  },
  "suzuki11b_interspeech": {
   "authors": [
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Continuous digits recognition leveraging invariant structure",
   "original": "i11_0993",
   "page_count": 4,
   "order": 409,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "Recently, an invariant structure of speech was proposed, where the inevitable acoustic variations caused by non-linguistic factors are effectively removed from speech. The invariant structure was applied to isolated word recognition and the experimental results showed good performance. However, the previous method can't apply to continuous speech recognition directly because there was no efficient decoding algorithm. In this paper, we propose a method to leverage the invariant structure in continuous digits recognition. We use a traditional HMM-based Automatic Speech Recognition (ASR) system to get N-best lists with phone alignments. Then we construct invariant structures using these phone alignments and re-rank the N-best lists by investigating which hypothesis is structurally more valid. Experimental results show a relative WER improvement of 17.4% over the baseline HMM-based ASR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-407"
  },
  "kanevsky11_interspeech": {
   "authors": [
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "David",
     "Nahamoo"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Convergence of line search a-function methods",
   "original": "i11_0997",
   "page_count": 4,
   "order": 410,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "Recently, the Line Search A-Function (LSAF) was introduced as a technique that generalizes Extended Baum-Welch (EBW) algorithm for functions of continuous probability densities. It was shown that LSAF provides a unified scheme for a large class of optimization problems that involve discriminant objective functions of different probability densities or sparse representation objective functions such as Approximate Bayesian Compressive Sensing. In this paper, we show that a discrete EBW recursion (that was initially developed to optimize functions of discrete distributions) also fits the scope of LSAF technique. We demonstrate the utility and robustness of the technique for discrete distributions thru the experimental set up of a TIMIT phone classification task using a Convex Hull Sparse Representation approach with different Lq regularization (q being any positive number).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-408"
  },
  "fujii11_interspeech": {
   "authors": [
    [
     "Yasuhisa",
     "Fujii"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Hidden boosted MMI and hierarchical state posterior feature for automatic speech recognition based on hidden conditional neural fields",
   "original": "i11_1001",
   "page_count": 4,
   "order": 411,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "We have investigated automatic speech recognition using Hidden Conditional Neural Fields (HCNF). In this paper, we propose a new objective function, Hidden Boosted MMI (HB-MMI) that considers the number of errors in the training data even if the correct state sequence is not known for training the HCNF. The experimental results show that HB-MMI can improve recognition accuracy if overfitting does not occur. We also present an automatic speech recognition method using a hierarchical state posterior feature where the output from the first stage HCNF is used as input for the second stage HCNF. The experimental results show that the feature improves recognition accuracy. By combining both of the proposed methods, we obtain further improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-409"
  },
  "cai11_interspeech": {
   "authors": [
    [
     "Jun",
     "Cai"
    ],
    [
     "Bruce",
     "Denby"
    ],
    [
     "Pierre",
     "Roussel"
    ],
    [
     "Gérard",
     "Dreyfus"
    ],
    [
     "Lise",
     "Crevier-Buchman"
    ]
   ],
   "title": "Recognition and real time performances of a lightweight ultrasound based silent speech interface employing a language model",
   "original": "i11_1005",
   "page_count": 4,
   "order": 412,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "The work presents advances in the implementation of an ultrasound based silent speech interface system. Use of a portable acquisition device, a visual speech recognizer system with a language model, and real time tests with the Julius system are described. Experiments with two types of visual feature extraction are also presented. Results show that good recognition and real time performance can be obtained with a portable silent speech interface employing a language model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-410"
  },
  "watanabe11_interspeech": {
   "authors": [
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Model adaptation for automatic speech recognition based on multiple time scale evolution",
   "original": "i11_1081",
   "page_count": 4,
   "order": 413,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "The change in speech characteristics is originated from various factors, at various (temporal) rates in a real world conversation. These temporal changes have their own dynamics and therefore, we propose to extend the single (time-) incremental adaptations to a multiscale adaptation, which has the potential of greatly increasing the model's robustness as it will include adaptation mechanism to approximate the nature of the characteristic change. The formulation of the incremental adaptation assumes a time evolution system of the model, where the posterior distributions, used in the decision process, are successively updated based on a macroscopic time scale in accordance with the Kalman filter theory. In this paper, we extend the original incremental adaptation scheme, based on a single time scale, to multiple time scales, and apply the method to the adaptation of both the acoustic model and the language model. We further investigate methods to integrate the multi-scale adaptation scheme to realize the robust speech recognition performance. Large vocabulary continuous speech recognition experiments for English and Japanese lectures revealed the importance of modeling multiscale properties in speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-411"
  },
  "breslin11_interspeech": {
   "authors": [
    [
     "C.",
     "Breslin"
    ],
    [
     "K. K.",
     "Chin"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Kate",
     "Knill"
    ]
   ],
   "title": "Integrated online speaker clustering and adaptation",
   "original": "i11_1085",
   "page_count": 4,
   "order": 414,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "For many applications, it is necessary to produce speech transcriptions in a causal fashion. To produce high quality transcripts, speaker adaptation is often used. This requires online speaker clustering and incremental adaptation techniques to be developed. This paper presents an integrated approach to online speaker clustering and adaptation which allows efficient clustering of speakers using the same accumulated statistics that are normally used for adaptation. Using a consistent criterion for both clustering and adaptation should yield gains for both stages. The proposed approach is evaluated on a meetings transcription task using audio from multiple distant microphones. Consistent gains over standard clustering and adaptation were obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-412"
  },
  "tuske11_interspeech": {
   "authors": [
    [
     "Zoltán",
     "Tüske"
    ],
    [
     "Christian",
     "Plahl"
    ],
    [
     "Ralf",
     "Schlüter"
    ]
   ],
   "title": "A study on speaker normalized MLP features in LVCSR",
   "original": "i11_1089",
   "page_count": 4,
   "order": 415,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "Different normalization methods are applied in recent Large Vocabulary Continuous Speech Recognition Systems (LVCSR) to reduce the influence of speaker variability on the acoustic models. In this paper we investigate the use of Vocal Tract Length Normalization (VTLN) and Speaker Adaptive Training (SAT) in Multi Layer Perceptron (MLP) feature extraction on an English task. We achieve significant improvements by each normalization method and we gain further by stacking the normalizations. Studying features transformed by Constrained Maximum Likelihood Linear Regression (CMLLR) based SAT as possible input for MLP, further experiments show that MLP could not consistently take advantage of SAT as it does in case of VTLN.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-413"
  },
  "jeong11_interspeech": {
   "authors": [
    [
     "Yongwon",
     "Jeong"
    ],
    [
     "Young Kuk",
     "Kim"
    ]
   ],
   "title": "Matrix-variate distribution of training models for robust speaker adaptation",
   "original": "i11_1093",
   "page_count": 4,
   "order": 416,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "In this paper, we describe a new speaker adaptation method based on the matrix-variate distribution of training models. A set of mean vectors of hidden Markov models (HMMs) is assumed to be drawn from the matrix-variate normal distribution, and bases are derived under this assumption. The resulting bases have the same dimension as that of the eigenvoice, thus adaptation can be performed using the same equation. In the isolated-word experiments, the proposed method showed a comparable performance with the eigenvoice in a clean environment, and showed better performance than the eigenvoice in both babble and factory floor noises. The experimental results demonstrated the validity of the matrix-variate normal assumption about the training models, thus the proposed method can be used for rapid speaker adaptation in noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-414"
  },
  "seltzer11_interspeech": {
   "authors": [
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Separating speaker and environmental variability using factored transforms",
   "original": "i11_1097",
   "page_count": 4,
   "order": 417,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "Two primary sources of variability that degrade accuracy in speech recognition systems are the speaker and the environment. While many algorithms for speaker or environment adaptation have been proposed to improve performance, far less attention has been paid to approaches which address for both factors. In this paper, we present a method for compensating for speaker and environmental mismatch using a cascade of CMLLR transforms. The proposed approach enables speaker transforms estimated in one environment to be effectively applied to speech from the same user in a different environment. This approach can be further improved using a new training method called speaker and environment adaptive training method. When applying speaker transforms to new environments, the proposed approach results in a 13% relative improvement over conventional CMLLR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-415"
  },
  "gilbert11_interspeech": {
   "authors": [
    [
     "Mazin",
     "Gilbert"
    ],
    [
     "Iker",
     "Arizmendi"
    ],
    [
     "Enrico",
     "Bocchieri"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Vincent",
     "Goffin"
    ],
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Mike",
     "Phillips"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Jay",
     "Wilpon"
    ]
   ],
   "title": "Your mobile virtual assistant just got smarter!",
   "original": "i11_1101",
   "page_count": 4,
   "order": 418,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "A Mobile Virtual Assistant (MVA) is a communication agent that recognizes and understands free speech, and performs actions such as retrieving information and completing transactions. One essential characteristic of MVAs is their ability to learn and adapt without supervision. This paper describes our ongoing research in developing more intelligent MVAs that recognize and understand very large vocabulary speech input across a variety of tasks. In particular, we present our architecture for unsupervised acoustic and language model adaptation. Experimental results show that unsupervised acoustic model learning approaches the performance of supervised learning when adapting on 40.50 device-specific utterances. Unsupervised language model learning results in an 8% absolute drop in word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-416"
  },
  "laaksonen11_interspeech": {
   "authors": [
    [
     "Laura",
     "Laaksonen"
    ],
    [
     "Ville",
     "Myllylä"
    ],
    [
     "Riitta",
     "Niemistö"
    ]
   ],
   "title": "Evaluating artificial bandwidth extension by conversational tests in car using mobile devices with integrated hands-free functionality",
   "original": "i11_1177",
   "page_count": 4,
   "order": 419,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "This paper describes an artificial bandwidth extension (ABE) method that generates new high frequency components to a narrowband signal by folding specifically gained subbands to frequencies from 4 kHz to 7 kHz, and improves the quality and intelligibility of narrowband speech in mobile devices. The proposed algorithm was evaluated by subjective listening tests. In addition, rarely used conversation test was constructed. Speech quality of 1) narrowband phone call, 2) wideband phone call, and 3) narrowband phone call enhanced with ABE were evaluated in conversational context using mobile devices with integrated hands-free (IHF) functionality. The results indicate that in IHF use case, ABE quality overcomes narrowband speech quality both in car noise and in quiet environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-417"
  },
  "pulakka11_interspeech": {
   "authors": [
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Ulpu",
     "Remes"
    ],
    [
     "Santeri",
     "Yrttiaho"
    ],
    [
     "Kalle J.",
     "Palomäki"
    ],
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Low-frequency bandwidth extension of telephone speech using sinusoidal synthesis and Gaussian mixture model",
   "original": "i11_1181",
   "page_count": 4,
   "order": 420,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "The limited audio bandwidth of narrowband telephone speech degrades the speech quality. This paper proposes a method that extends the bandwidth of telephone speech to the frequency range 0.300 Hz. The lowest harmonics of voiced speech are generated using sinusoidal synthesis. The energy in the extension band is estimated from spectral features using a Gaussian mixture model. The amplitudes and phases of the synthesized signal are adjusted based on the amplitudes and phases of the narrowband input speech. The proposed method was evaluated with listening tests together with a bandwidth extension method for the range 4.8 kHz. The low-frequency bandwidth extension was found to reduce dissimilarity with wideband speech but no perceived quality improvement was achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-418"
  },
  "noureldin11_interspeech": {
   "authors": [
    [
     "Amr H.",
     "Nour-Eldin"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Memory-based approximation of the Gaussian mixture model framework for bandwidth extension of narrowband speech",
   "original": "i11_1185",
   "page_count": 4,
   "order": 421,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "In this paper, we extend our previous work on exploiting speech temporal properties to improve Bandwidth Extension (BWE) of narrowband speech using Gaussian Mixture Models (GMMs). By quantifying temporal properties through information theoretic measures and using delta features, we have shown that narrowband memory significantly increases certainty about highband parameters. However, as delta features are non-invertible, they can not be directly used to reconstruct highband frequency content. In the work presented herein, we embed temporal properties indirectly into the GMM structure through a memory-dependent tree-based approach to extend representation of the narrow band. In particular, sequences of past frames are progressively used to grow the GMM in a tree-like fashion. This growth approach results in reliable estimates for the GMM parameters such that Maximum Likelihood estimation is no longer necessary, thus circumventing the complexity accompanying high-dimensionality GMM training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-419"
  },
  "harding11_interspeech": {
   "authors": [
    [
     "Philip",
     "Harding"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Speech enhancement by reconstruction from cleaned acoustic features",
   "original": "i11_1189",
   "page_count": 4,
   "order": 422,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "This paper proposes a novel method of speech enhancement that moves away from conventional filtering-based methods and instead aims to reconstruct clean speech from a set of speech features. Underlying the enhancement system is a speech model which at present is based on a sinusoidal model. This is driven by a set of speech features, comprising voicing, fundamental frequency and spectral envelope, that are extracted from the noisy speech. A maximum a posteriori approach is proposed for estimating clean spectral envelope features from the noisy spectral envelope. A set of subjective tests, measuring speech quality, noise intrusiveness and overall quality, found the proposed method to be highly effective at removing noise. Comparison against conventional speech enhancement methods found performance to be equivalent to Wiener filtering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-420"
  },
  "choi11_interspeech": {
   "authors": [
    [
     "Jae-Hun",
     "Choi"
    ],
    [
     "Sang-Kyun",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "A soft decision-based speech enhancement using acoustic noise classification",
   "original": "i11_1193",
   "page_count": 4,
   "order": 423,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "In this letter, we present a speech enhancement technique based on the ambient noise classification incorporating the Gaussian mixture model (GMM). The principal parameters of the statistical model-based speech enhancement algorithm such as the weighting parameter in the decision-directed (DD) method and the long-term smoothing parameter of the noise estimation, are chosen as different values according to the classified contexts to ensure best performance for each noise. For the real-time environment awareness, the noise classification is performed on a frame-by-frame basis using the GMM with the soft decision framework. The speech absence probability (SAP) is used in detecting the speech absence periods and updating the likelihood of the GMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-421"
  },
  "li11f_interspeech": {
   "authors": [
    [
     "Chao",
     "Li"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "A noise estimation method based on speech presence probability and spectral sparseness",
   "original": "i11_1197",
   "page_count": 4,
   "order": 424,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "This paper addresses the problem of noise power spectrum estimation. Existing noise estimation methods cannot perform quite reliably when noise level increasing abruptly (e.g., narrowband noise bursts). To overcome this problem, we improve the time-recursive averaging algorithm based on speech presence probability (SPP), by exploiting the sparseness of speech spectrum. Firstly, we utilize the SPP estimation method based on fixed priors to achieve low SPP estimates at time-frequency bins where speech is absent. Furthermore, a spectral sparseness measure is proposed to adjust the SPP estimates. Experiments show the proposed method can update the noise estimates faster than state-of-the-art approaches in both stationary and nonstationary noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-422"
  },
  "li11g_interspeech": {
   "authors": [
    [
     "Chao",
     "Li"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Improved a posteriori speech presence probability estimation based on cepstro-temporal smoothing and time-frequency correlation",
   "original": "i11_1201",
   "page_count": 4,
   "order": 425,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "In this paper, we present a novel estimator for the SPP at each time-frequency point in the short-time Fourier transform (STFT) domain. Existing speech presence probability (SPP) estimators cannot perform quite reliably in nonstationary noise environment when applied to a speech enhancement task. To overcome this limitation, we propose a novel SPP estimation method. Firstly, the spectral outliers are eliminated by selectively smoothing the maximum likelihood estimate of a priori signal-noise ratio (SNR) in the cepstral domain. Furthermore, an adaptive tracking method for a priori SPP is derived by exploiting the strong correlation of speech presence in neighboring frequency bins of consecutive frames. The proposed approach outperforms the state-of-the-art approaches, resulting in less noise leakage and low speech distortions in both stationary and nonstationary noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-423"
  },
  "chowdhury11_interspeech": {
   "authors": [
    [
     "Md Foezur Rahman",
     "Chowdhury"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A rapid adaptation algorithm for tracking highly non-stationary noises based on Bayesian inference for on-line spectral change point detection",
   "original": "i11_1205",
   "page_count": 4,
   "order": 426,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "This paper presents an innovative rapid adaptation technique for tracking highly non-stationary acoustic noises. The novelty of this technique is that it can detect the acoustic change points from the spectral characteristics of the observed speech signal in rapidly changing non-stationary acoustic environments. The proposed innovative noise tracking technique will be very suitable for joint additive and channel distortions compensation (JAC) for on-line automatic speech recognition (ASR). The Bayesian on-line change point detection (BOCPD) approach is used to implement this technique. The proposed algorithm is tested using highly non-stationary noisy speech samples from the Aurora2 speech database. Significant improvement in minimizing the delay in adaptation to new acoustic conditions is obtained for highly nonstationary noises compared to the most popular baseline noise tracking algorithm MCRA and its derivatives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-424"
  },
  "paliwal11_interspeech": {
   "authors": [
    [
     "Kuldip",
     "Paliwal"
    ],
    [
     "Belinda",
     "Schwerin"
    ],
    [
     "Kamil",
     "Wójcicki"
    ]
   ],
   "title": "Single channel speech enhancement using MMSE estimation of short-time modulation magnitude spectrum",
   "original": "i11_1209",
   "page_count": 4,
   "order": 427,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "In this paper we investigate the enhancement of speech by applying MMSE short-time spectral magnitude estimation in the modulation domain. For this purpose, the traditional analysis-modificationsynthesis framework is extended to include modulation domain processing. We compensate the noisy modulation spectrum for additive noise distortion by applying the MMSE short-time spectral magnitude estimation algorithm in the modulation domain. Subjective experiments were conducted to compare the quality of stimuli processed by the MMSE modulation magnitude estimator to those processed using the MMSE acoustic magnitude estimator and the modulation spectral subtraction method. The proposed method is shown to have better noise suppression than MMSE acoustic magnitude estimation, and improved speech quality compared to modulation domain spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-425"
  },
  "saha11_interspeech": {
   "authors": [
    [
     "Atanu",
     "Saha"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Speech enhancement using masking properties in adverse environments",
   "original": "i11_1213",
   "page_count": 4,
   "order": 428,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "In this paper, we propose a speech enhancement method by exploiting masking properties of human auditory system. The masking properties are exploited to calculate a masking threshold. The spectral components which lie above the threshold are audible to human listeners. These audible spectral components in the proposed method are suppressed as a predefined attenuation factor of the original noise. The evaluation is conducted in the experiments. The experimental results show that the proposed method provides significant performance compared to the conventional approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-426"
  },
  "raj11b_interspeech": {
   "authors": [
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Tuomas",
     "Virtanen"
    ]
   ],
   "title": "Phoneme-dependent NMF for speech enhancement in monaural mixtures",
   "original": "i11_1217",
   "page_count": 4,
   "order": 429,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "The problem of separating speech signals out of monaural mixtures (with other non-speech or speech signals) has become increasingly popular in recent times. Among the various solutions proposed, the most popular methods are based on compositional models such as non-negative matrix factorization (NMF) and latent variable models. Although these techniques are highly effective they largely ignore the inherently phonetic nature of speech. In this paper we present a phoneme-dependent NMF-based algorithm to separate speech from monaural mixtures. Experiments performed on speech mixed with music indicate that the proposed algorithm can result in significant improvement in separation performance, over conventional NMF-based separation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-427"
  },
  "leitner11_interspeech": {
   "authors": [
    [
     "Christina",
     "Leitner"
    ],
    [
     "Franz",
     "Pernkopf"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Kernel PCA for speech enhancement",
   "original": "i11_1221",
   "page_count": 4,
   "order": 430,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "In this paper, we apply kernel principal component analysis (kPCA), which has been successfully used for image denoising, to speech enhancement. In contrast to other enhancement methods which are based on the magnitude spectrum, we rather apply kPCA to complex spectral data. This is facilitated by Gaussian kernels. In the experiments, we show good noise reduction with few artifacts for noise corrupted speech at different SNR levels using additive white Gaussian noise. We compared kPCA with linear PCA and spectral subtraction and evaluated all algorithms with perceptually motivated quality measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-428"
  },
  "gomez11b_interspeech": {
   "authors": [
    [
     "Angel M.",
     "Gomez"
    ],
    [
     "Belinda",
     "Schwerin"
    ],
    [
     "Kuldip",
     "Paliwal"
    ]
   ],
   "title": "Objective intelligibility prediction of speech by combining correlation and distortion based techniques",
   "original": "i11_1225",
   "page_count": 4,
   "order": 431,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "A number of techniques based on correlation measurements have recently been proposed to provide an objective measure of intelligibility. These techniques are able to detect nonlinear distortions and provide intelligibility scores highly correlated with those given by human listeners. However, the performance of these techniques has not been found satisfactory for measuring the speech intelligibility of speech enhancement algorithms. In this paper we first investigate the different correlation-based methods, in the context of speech enhancement. We then propose to combine these correlation-based techniques with spectral distance-based ones. Results presented show that objective intelligibility prediction is significantly improved by this combination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-429"
  },
  "damnati11_interspeech": {
   "authors": [
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Delphine",
     "Charlet"
    ]
   ],
   "title": "Multi-view approach for speaker turn role labeling in TV broadcast news shows",
   "original": "i11_1285",
   "page_count": 4,
   "order": 432,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "Speaker role recognition in TV Broadcast News shows is addressed in this paper. Speaker turns are assigned a role among anchor, reporter and other. A multi-view approach is proposed exploiting the complementarities of lexical cues obtained from Automatic Speech Recognition output and acoustical cues obtained from speech signal analysis. Early and late fusions are compared. 90.1% classification accuracy is obtained on automatically segmented speaker turns for a 6.5 hours test corpus of 14 shows mixing news and conversational speech. Further analyses are provided for other speaker turns showing interesting perspectives towards finer-grained speaker role characterization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-430"
  },
  "gandhe11_interspeech": {
   "authors": [
    [
     "Sudeep",
     "Gandhe"
    ],
    [
     "Michael",
     "Rushforth"
    ],
    [
     "Priti",
     "Aggarwal"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Evaluation of an integrated authoring tool for building advanced question-answering characters",
   "original": "i11_1289",
   "page_count": 4,
   "order": 433,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "We present the evaluation of an integrated authoring tool for rapid prototyping of dialogue systems. These dialogue systems are designed to support virtual humans engaging in advanced question-answering dialogues, such as for training tactical questioning skills. The tool was designed to help non-experts, who may have little or no knowledge of linguistics or computer science, build virtual characters that can play the role of an interviewee. The tool has been successfully used by several different non-experts to create a number of virtual characters used successfully for both training and human subjects testing. We report on experiences with seven such characters, whose development time was as little as two weeks including concept development and a round of user testing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-431"
  },
  "tur11_interspeech": {
   "authors": [
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Dustin",
     "Hillard"
    ],
    [
     "Asli",
     "Celikyilmaz"
    ]
   ],
   "title": "Towards unsupervised spoken language understanding: exploiting query click logs for slot filling",
   "original": "i11_1293",
   "page_count": 4,
   "order": 434,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "In this paper, we present a novel approach to exploit user queries mined from search engine query click logs to bootstrap or improve slot filling models for spoken language understanding. We propose extending the earlier gazetteer population techniques to mine unannotated training data for semantic parsing. The automatically annotated mined data can then be used to train slot specific parsing models. We show that this method can be used to bootstrap slot filling models and can be combined with any available annotated data to improve performance. Furthermore, this approach may eliminate the need for populating and maintaining in-domain gazetteers, in addition to providing complementary information if they are already available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-432"
  },
  "lee11d_interspeech": {
   "authors": [
    [
     "Donghyeon",
     "Lee"
    ],
    [
     "Cheongjae",
     "Lee"
    ],
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Kyungduk",
     "Kim"
    ],
    [
     "Seokhwan",
     "Kim"
    ],
    [
     "Junhwi",
     "Choi"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Web-enhanced content retrieval for information access dialogue system",
   "original": "i11_1297",
   "page_count": 4,
   "order": 435,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "We consider the problem of content retrieval with complex queries for an information access dialogue system. Traditional information access dialogue systems rely on exact query matching and heuristic rules to find relevant content in a relational database. To deal with complex queries, a dialogue system is used to attain deep semantic processing such as full semantic parsing and ontology-based reasoning. However, these systems require a large amount of semantic annotation and domain expert knowledge that are often very expensive to obtain and thus have been limited in practice. In this paper, we present a simple alternative method where web-searched documents can contribute to enhanced vector space model-based content retrieval. Our model captures underlying co-occurrence patterns between the query and the contents. An efficient ranking algorithm is applied to retrieve the relevant contents. One merit of the proposed approach is that it does not require heavy semantic processing, and therefore, it results in efficient content retrieval. We demonstrate that our method is beneficial in an electronic program-guided dialogue system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-433"
  },
  "daubigney11_interspeech": {
   "authors": [
    [
     "Lucie",
     "Daubigney"
    ],
    [
     "Milica",
     "Gašić"
    ],
    [
     "Senthilkumar",
     "Chandramohan"
    ],
    [
     "Matthieu",
     "Geist"
    ],
    [
     "Olivier",
     "Pietquin"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Uncertainty management for on-line optimisation of a POMDP-based large-scale spoken dialogue system",
   "original": "i11_1301",
   "page_count": 4,
   "order": 436,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "The optimization of dialogue policies using reinforcement learning (RL) is now an accepted part of the state of the art in spoken dialogue systems (SDS). Yet, it is still the case that the commonly used training algorithms for SDS require a large number of dialogues and hence most systems still rely on artificial data generated by a user simulator. Optimization is therefore performed off-line before releasing the system to real users. Gaussian Processes (GP) for RL have recently been applied to dialogue systems. One advantage of GP is that they compute an explicit measure of uncertainty in the value function estimates computed during learning. In this paper, a class of novel learning strategies is described which use uncertainty to control exploration on-line. Comparisons between several exploration schemes show that significant improvements to learning speed can be obtained and that rapid and safe online optimisation is possible, even on a complex task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-434"
  },
  "hara11_interspeech": {
   "authors": [
    [
     "Sunao",
     "Hara"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Detection of task-incomplete dialogs based on utterance-and-behavior tag n-gram for spoken dialog systems",
   "original": "i11_1305",
   "page_count": 4,
   "order": 437,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "We propose a method of detecting \"task incomplete\" dialogs in spoken dialog systems using N-gram-based dialog models. We used a database created during a field test in which inexperienced users used a client-server music retrieval system with a spoken dialog interface on their own PCs. In this study, the dialog for a music retrieval task consisted of a sequence of user and system tags that related their utterances and behaviors. The dialogs were manually classified into two classes: the dialog either completed the music retrieval task or it didn't. We then detected dialogs that did not complete the task, using N-gram probability models or a Support Vector Machine with N-gram feature vectors trained using manually classified dialogs. Off-line and on-line detection experiments were conducted on a large amount of real data, and the results show that our proposed method achieved good classification performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-435"
  },
  "sarikaya11_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Stanley F.",
     "Chen"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Shrinkage-based features for natural language call routing",
   "original": "i11_1309",
   "page_count": 4,
   "order": 438,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "The feature set used with a classifier can have a large impact on classification performance. This paper presents a set of shrinkagebased features for Maximum Entropy and other classifiers in the exponential family. These features are inspired by the exponential class-based language model, Model M. We motivate the use of these features for the task of text classification and evaluate them on a natural language call routing task. The proposed features along with a new word clustering method result in significant improvements in action classification accuracy over typical word-based features, particularly for small amounts of training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-436"
  },
  "rachevsky11_interspeech": {
   "authors": [
    [
     "Leonid",
     "Rachevsky"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Clustering with modified cosine distance learned from constraints",
   "original": "i11_1313",
   "page_count": 4,
   "order": 439,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "In this paper we present a modified cosine similarity metric that helps to make features more discriminative. The new metric is defined via various linear transformations of the original feature space to a space in which these samples are better separated. These transformations are learned from a set of constraints representing available domain knowledge by solving related optimization problems. We present results on two natural language call routing datasets that show significant improvements ranging from 3% to 5% absolute in the purity of clusters obtained in an unsupervised fashion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-437"
  },
  "fandrianto11_interspeech": {
   "authors": [
    [
     "Andrew",
     "Fandrianto"
    ],
    [
     "Brian",
     "Langner"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Using speaker ID to discover repeat callers of a spoken dialog system",
   "original": "i11_1317",
   "page_count": 4,
   "order": 440,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "This paper describes using speaker ID techniques to identify repeat callers in a spoken dialog system, using only acoustic features. Often it is useful to know if a dialog user is a novice or is experienced, and it can be the case that identifying data such as Caller ID is either unreliable or unavailable. Our approach attempts to remedy this by determining user identity in a dialog session using the acoustic information in the dialog. We optimize the audio content of each call by removing artifacts not relevant to modeling speech. This technique is applied to finding consecutive callers and creating unique user identities over all calls over a larger time frame, with the aim of tuning or adapting the dialog system based on the user identity. Our results show that the technique is effective in recognizing consecutive callers and in identifying a unique user identities in a large set of calls.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-438"
  },
  "pinault11_interspeech": {
   "authors": [
    [
     "Florian",
     "Pinault"
    ],
    [
     "Fabrice",
     "Lefèvre"
    ]
   ],
   "title": "Semantic graph clustering for POMDP-based spoken dialog systems",
   "original": "i11_1321",
   "page_count": 4,
   "order": 441,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "Dialog managers (DM) in spoken dialogue systems make decisions in highly uncertain conditions, due to errors from the speech recognition and spoken language understanding (SLU) modules. In this work a framework to interface efficient probabilistic modeling for both the SLU and the DM modules is described and investigated. Thorough representation of the user semantics is inferred by the SLU in the form of a graph of frames and, complemented with some contextual information, is mapped to a summary space in which a stochastic POMDP dialogue manager can perform planning of actions taking into account the uncertainty on the current dialogue state. Tractability is ensured by the use of an intermediate summary space. Also to reduce the development cost of SDS an approach based on clustering is proposed to automatically derive the master-summary mapping function. A preliminary implementation is presented in the Media domain (touristic information and hotel booking) and tested with a simulated user.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-439"
  },
  "taguchi11_interspeech": {
   "authors": [
    [
     "Ryo",
     "Taguchi"
    ],
    [
     "Yuji",
     "Yamada"
    ],
    [
     "Koosuke",
     "Hattori"
    ],
    [
     "Taizo",
     "Umezaki"
    ],
    [
     "Masahiro",
     "Hoguro"
    ],
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Kotaro",
     "Funakoshi"
    ],
    [
     "Mikio",
     "Nakano"
    ]
   ],
   "title": "Learning place-names from spoken utterances and localization results by mobile robot",
   "original": "i11_1325",
   "page_count": 4,
   "order": 442,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "This paper proposes a method for the unsupervised learning of place-names from pairs of a spoken utterance and a localization result, which represents a current location of a mobile robot, without any priori linguistic knowledge other than a phoneme acoustic model. In previous work, we have proposed a lexical learning method based on statistical model selection. This method can learn the words that represent a single object, such as proper nouns, but cannot learn the words that represent classes of objects, such as general nouns. This paper describes improvements of the method for learning both a phoneme sequence of each word and a distribution of objects that the word represents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-440"
  },
  "gamback11_interspeech": {
   "authors": [
    [
     "Björn",
     "Gambäck"
    ],
    [
     "Fredrik",
     "Olsson"
    ],
    [
     "Oscar",
     "Täckström"
    ]
   ],
   "title": "Active learning for dialogue act classification",
   "original": "i11_1329",
   "page_count": 4,
   "order": 443,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "Active learning techniques were employed for classification of dialogue acts over two dialogue corpora, the English human-human Switchboard corpus and the Spanish human-machine Dihana corpus. It is shown clearly that active learning improves on a baseline obtained through a passive learning approach to tagging the same data sets. An error reduction of 7% was obtained on Switchboard, while a factor 5 reduction in the amount of labeled data needed for classification was achieved on Dihana. The passive Support Vector Machine learner used as baseline in itself significantly improves the state of the art in dialogue act classification on both corpora. On Switchboard it gives a 31% error reduction compared to the previously best reported result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-441"
  },
  "bazillon11_interspeech": {
   "authors": [
    [
     "Thierry",
     "Bazillon"
    ],
    [
     "Benjamin",
     "Maza"
    ],
    [
     "Michael",
     "Rouvier"
    ],
    [
     "Frederic",
     "Bechet"
    ],
    [
     "Alexis",
     "Nasr"
    ]
   ],
   "title": "Speaker role recognition using question detection and characterization",
   "original": "i11_1333",
   "page_count": 4,
   "order": 444,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "Speech Data Mining is an area of research dedicated to characterizing audio streams that contain speech from one or more speakers, using descriptors related to the form and the content of the speech signal. Besides the word transcription, information about the type of audio stream and the role and identity of speakers is also crucial to allow complex queries such as: \"seek debates on X\", \"find all the interviews of Y\", etc. In this framework we present a study performed on broadcast conversations that focuses on the way speakers express their questions in conversations. The initial intuition is that the type of questions asked can help identify the role (anchor, guest, expert, etc.) of a speaker in a conversation. By tagging these questions with a set of labels and using this information in addition to the commonly used descriptors to classify users' role in broadcast conversations, we improve the role classification accuracy and validate our initial intuition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-442"
  },
  "huang11f_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huang"
    ],
    [
     "Stephen J.",
     "Cox"
    ]
   ],
   "title": "Learning score structure from spoken language for a tennis game",
   "original": "i11_1337",
   "page_count": 4,
   "order": 445,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "We describe a novel approach to inferring the scoring rules of a tennis game by analysing the chair umpire's speech. In a tennis match, the chair umpire, amongst other tasks, announces the scores. Hence his or her speech is the key resource for inferring the scoring rules of tennis, a task that can be accomplished by correlating the events on the court with these score announcements. In this work, the learning procedure consists of two steps: speech recognition followed by rule inference. For speech recognition, we use a two coupled language models one for words and one for scores. The first makes use of the internal structure of a score, the second, the dependency of a score on the previous score. For rule inference, we utilize a multigram model to segment the recognised score streams into variable-length score sequences, each of them corresponding to a game in a tennis match. The approach is applied to four complete tennis matches, and shows both enhanced recognition performance, and a promising approach to inferring the scoring rules of the game.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-443"
  },
  "witt11_interspeech": {
   "authors": [
    [
     "Silke M.",
     "Witt"
    ]
   ],
   "title": "Semi-automated classifier adaptation for natural language call routing",
   "original": "i11_1341",
   "page_count": 4,
   "order": 446,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "Commercial spoken dialogue systems traditionally have been static in the sense that once deployed, these applications only get updated as part of formal releases. Also, the creation of classification grammars in natural language call routing applications requires expensive manual annotation of caller intents. The work presented here introduces a process to semi-automatically annotate new data and to use the new annotations to update the training corpus to continually improve the classification performance. This new method consists of a combination of using multiple classifiers in a voting schema to automatically classify an utterance and a boosting mechanism to continually update the classifier with the new automatically annotated training data. This method was tested with 6 weeks' worth of data from a live system. It is shown that with this approach about 94% of all new utterances can be automatically annotated. Using the iterative boosting approach increased the size of the training corpus by about 6% per iteration while at the same time slightly increasing the classification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-444"
  },
  "liang11b_interspeech": {
   "authors": [
    [
     "Wei-Bin",
     "Liang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Chih-Hung",
     "Wang"
    ],
    [
     "Jhing-Fa",
     "Wang"
    ]
   ],
   "title": "Interactional style detection for versatile dialogue response using prosodic and semantic features",
   "original": "i11_1345",
   "page_count": 4,
   "order": 447,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This work presents an approach to interactional style (IS) detection for versatile responses in spoken dialogue systems (SDSs). Since speakers generally express their intents in different styles, the responses of an SDS should be versatile instead of invariable, planned responses. Moreover, the IS of dialogue turns can be affected by dialogue topics and speakers' emotional states. In this study, three base-level classifiers are employed for preliminary detection, including latent Dirichlet allocation for dialogue topic categorization, support vector machine for prosody-based emotional state identification and maximum entropy for semantic label-based emotional state identification. Finally, an artificial neural network is adopted for IS detection considering the scores estimated from the aforementioned classifiers. To evaluate the proposed approach, an SDS in a chatting domain was constructed for evaluation. The performance of IS detection can achieve 82.67%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-445"
  },
  "kuhnel11_interspeech": {
   "authors": [
    [
     "Christine",
     "Kühnel"
    ],
    [
     "Benjamin",
     "Weiss"
    ],
    [
     "Matthias",
     "Schulz"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Quality aspects of multimodal dialog systems: identity, stimulation and success",
   "original": "i11_1349",
   "page_count": 4,
   "order": 448,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "So far, not much is known on the relationship of quality aspects of multimodal dialog systems. This paper aims at closing this gap by analyzing the influence of input and output modalities on the systems' usability. The underlying study has been carried out with a smart-home system offering speech, gesture and touch as well as the combination of these three for input and a speech-to-text system, a TV screen and a smartphone screen for output. The results indicate that the usability of a multimodal system is composed of hedonic and pragmatic aspects. The hedonic aspects are influenced by the identity transported by the output channels and the stimulation of the input modalities. A measure for task success was sufficient to describe the pragmatic aspect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-446"
  },
  "tepperman11b_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Emily",
     "Nava"
    ]
   ],
   "title": "Where should pitch accents and phrase breaks go? a syntax tree transducer solution",
   "original": "i11_1353",
   "page_count": 4,
   "order": 449,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "Motivated by a desire to assess the prosody of foreign language learners, this study demonstrates the benefit of high-level syntactic information in automatically deciding where phrase breaks and pitch accents should go in text. The connection between syntax and prosody is well-established, and naturally lends itself to tree-based probabilistic models. With automatically-derived parse trees paired to tree transducer models, we found that categorical prosody tags for unseen text can be determined with significantly higher accuracy than they can with a baseline method that uses n-gram models of part-of-speech tags. On the Boston University Radio News Corpus, the tree transducer outperformed the baseline by 14% overall for accents, and by 3% overall for breaks. These automatic results fell within this corpus's range of inter-speaker agreement in assigning accents and breaks to text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-447"
  },
  "bocci11_interspeech": {
   "authors": [
    [
     "Giuliano",
     "Bocci"
    ],
    [
     "Cinzia",
     "Avesani"
    ]
   ],
   "title": "Phrasal prominences do not need pitch movements: postfocal phrasal heads in Italian",
   "original": "i11_1357",
   "page_count": 4,
   "order": 450,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "Informationally Given phrases following an instance of focus are generally realized in a compressed pitch range and are generally assumed to lack prosodic prominences. In this paper, we address the question of the metrical representation of postfocal constituents in Tuscan Italian. The results of a production experiment show that, despite their being realized with a low and flat F0 contour, postfocal constituents are not extrametrical, but are phrased and assigned phrasal metrical prominences of phrasal level. The impact of our results on the prosodic representation of Italian and on the information structure-prosody interface are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-448"
  },
  "gac11_interspeech": {
   "authors": [
    [
     "David Le",
     "Gac"
    ],
    [
     "Hiyon",
     "Yoo"
    ]
   ],
   "title": "Intonation of left dislocated topics in modern greek",
   "original": "i11_1361",
   "page_count": 4,
   "order": 451,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "We present in this paper the results of a production experiment testing the effects of the discourse activation state on the intonation of left-dislocated topics in Modern Greek. The activation states of the topics (active, inactive and semi-active) were examined in three different sentence types, namely declaratives, WH-questions and yes-no questions. Results show that the tunes are not affected by activation state but by sentence type. This supports the idea that the intonation of these topics is rather governed by phonological process, probably grounded on perceptually oriented principles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-449"
  },
  "thompson11_interspeech": {
   "authors": [
    [
     "Laura",
     "Thompson"
    ],
    [
     "Catherine I.",
     "Watson"
    ],
    [
     "Ray",
     "Harlow"
    ],
    [
     "Jeanette",
     "King"
    ],
    [
     "Margaret",
     "Maclagan"
    ],
    [
     "Helen",
     "Charters"
    ],
    [
     "Peter",
     "Keegan"
    ]
   ],
   "title": "Phrases, pitch and perceived prominence in māori",
   "original": "i11_1365",
   "page_count": 4,
   "order": 452,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "This study explores phrase-level prosody and prominence in the Māori language. Limited existing prosodic analysis and anecdotal evidence of diachronic change have motivated the present investigation into alignment of descriptions of intonation and stress with prominence perception test results and pitch analysis of speech data. In general, we find the expected case does occur most often, but examining results across speakers with birthdates spanning a century shows conservatism in modern elders and contradictory results in younger speakers: while making 'errors' in prominence placement, they are often as faithful to the overall expected contour as their elders.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-450"
  },
  "dubeda11_interspeech": {
   "authors": [
    [
     "Tomáš",
     "Duběda"
    ]
   ],
   "title": "Perceptual sensitivity to prenuclear and nuclear intonational patterns",
   "original": "i11_1369",
   "page_count": 4,
   "order": 453,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "We describe a perceptual experiment whose goal is to compare perceptual sensitivity to pitch accent contrasts in nuclear and prenuclear positions. The material consists of Czech sentences which have been resynthesized with controlled intonation. The results show that changes in nuclear pitch accents are perceived more sharply than changes in prenuclear pitch accents, and that the H. accent is perceptually more salient than the other accent types (L.H, L. and S.). The effect of constituent edge on the perception of intonational contrasts has not been confirmed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-451"
  },
  "kalaldeh11_interspeech": {
   "authors": [
    [
     "Raya",
     "Kalaldeh"
    ]
   ],
   "title": "Tonal alignment defined: the case of southern irish English",
   "original": "i11_1373",
   "page_count": 4,
   "order": 454,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "This paper proposes to define tonal alignment features as either intrinsic; the default alignment, or extrinsic; the shifts away from the default alignment due to prosodic contextual factors. Intrinsic alignment is different for pre-nuclear (PN) and nuclear (N) accents. This distinction is illustrated for a variety of Irish English (IrE), Drogheda English (DroghE) where the PN and the N peaks of H* accents are intrinsically aligned at a time point 70%.80% and 60%.75% into the vowel of the accented syllable, respectively. Extrinsic alignment shifts of PN and N peaks are very small not exceeding the accented vowel boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-452"
  },
  "rosenberg11b_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Using mutual information to identify regions of analysis for prosodic analysis",
   "original": "i11_1377",
   "page_count": 4,
   "order": 455,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "This paper presents a novel technique for empirically identifying regions of analysis for time/value information. The technique relies on analysis of mutual information between the contour, and some variable of interest. We present the use of this technique in the analysis of prosody in American English speech, where we identify valuable regions of analysis for the classification of phrase ending intonation. We also use the technique to investigate the most informative region of analysis for pitch accent detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-453"
  },
  "tseng11_interspeech": {
   "authors": [
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Chao-yu",
     "Su"
    ],
    [
     "Chi-Feng",
     "Huang"
    ]
   ],
   "title": "Prosodic highlights in Mandarin continuous speech - cross-genre attributes and implications",
   "original": "i11_1381",
   "page_count": 4,
   "order": 456,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "The present study examines perceived prosodic highlights in three genres of fluent continuous Mandarin to test (1) whether prosodic highlights are genre related, (2) how they interact with discourse structure, (3) how they signal information status, (4) whether systematic acoustic patterns could be obtained from speech data analysis, and (5) whether prosodic highlights is layered over to discourse structure. Results demonstrate that prosodic highlighting is genre related; distribution of key information can be attributed to linguistic content and communicative needs. Prosodic highlighting is an extra layer over discourse structure, the former signals key information while the latter underlying linguistic association.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-454"
  },
  "sulpizio11_interspeech": {
   "authors": [
    [
     "Simone",
     "Sulpizio"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "When two newly-acquired words are one: new words differing in stress alone are not automatically represented differently",
   "original": "i11_1385",
   "page_count": 4,
   "order": 457,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "Do listeners use lexical stress at an early stage in word learning? Artificial-lexicon studies have shown that listeners can learn new spoken words easily. These studies used non-words differing in consonants and/or vowels, but not differing only in stress. If listeners use stress information in word learning, they should be able to learn new words that differ only in stress (e.g., BInulo-biNUlo). We investigated this issue here. When learning new words, Italian listeners relied on segmental information; they did not take stress information into account. Newly-acquired words differing in stress alone are not automatically represented as different word forms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-455"
  },
  "bu11_interspeech": {
   "authors": [
    [
     "Shehui",
     "Bu"
    ],
    [
     "Zhenjie",
     "Zhuo"
    ],
    [
     "Lingling",
     "Yang"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Automatic determination of the standard Chinese prosodic phrase boundaries by f_0 generation model",
   "original": "i11_1389",
   "page_count": 4,
   "order": 458,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "We proposed an automatic method for determining the boundaries of prosodic phrases in real speech waves. In this method, the dynamic programming (DP) and the least mean square error (LMSE) methods were implemented based on the F0 generation model. In order to evaluate the accuracy and validity of this proposed method, a set of 973 standard Chinese speech sentences was selected. The cumulative proportion of the estimated prosodic phrase boundaries approached 76% when ET(0i) was less than the average duration of the prosodic phrases. Thus, it can be concluded that this proposed method can be used in the practical application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-456"
  },
  "looze11_interspeech": {
   "authors": [
    [
     "Céline De",
     "Looze"
    ],
    [
     "Stéphane",
     "Rauzy"
    ]
   ],
   "title": "Measuring speakers' similarity in speech by means of prosodic cues: methods and potential",
   "original": "i11_1393",
   "page_count": 4,
   "order": 459,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "This study presents a method for measuring speakers similarity (the tendency for speakers to exhibit similar speech patterns) by means of prosodic cues. It shows that similarity changes throughout social interaction and that its variations can inform about speakers' attitudes, similarity being more important when speakers are more involved in the interaction. It supports the assumption that similarity is part of social interaction and may be implemented into spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-457"
  },
  "yang11b_interspeech": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Tonal variations in Mandarin: new evidence from spontaneous and read speech",
   "original": "i11_1397",
   "page_count": 4,
   "order": 460,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "It is well-known that tone variation occurs in read speech due to a number of different contextual effects, most prominently local tone sequencing and downstepping, whereas in spontaneous speech, a greater variability of tone shape has been observed as arising from global discourse context. In this paper we show comparative results on tonal realization in read and spontaneous speech. Specifically, our study confirms the effects of tonal sequencing, and shows that higher amplitude increases conformity to lexical shape in both speech modes, while spontaneous speech is both more varied and pitch reduced in the defined f0 direction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-458"
  },
  "guinaudeau11_interspeech": {
   "authors": [
    [
     "Camille",
     "Guinaudeau"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Accounting for prosodic information to improve ASR-based topic tracking for TV broadcast news",
   "original": "i11_1401",
   "page_count": 4,
   "order": 461,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "The increasing quantity of video material available on line requires improved methods to help users navigate such data, among which are topic tracking techniques. The goal of this paper is to show that prosodic information can improve an ASR-based topic tracking system for French TV Broadcast News. To this end, two kinds of prosodic information - extracted with and without a learning phase - are integrated in the system. This integration shows significant improvements in the F1-measure, by 13 and 8 points for the two techniques compared with the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-459"
  },
  "imamura11_interspeech": {
   "authors": [
    [
     "Kenji",
     "Imamura"
    ],
    [
     "Tomoko",
     "Izumi"
    ],
    [
     "Kugatsu",
     "Sadamitsu"
    ],
    [
     "Kuniko",
     "Saito"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Hirokazu",
     "Masataki"
    ]
   ],
   "title": "Morpheme conversion for connecting speech recognizer and language analyzers in unsegmented languages",
   "original": "i11_1405",
   "page_count": 4,
   "order": 462,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "Connecting automatic speech recognizers (ASRs) and language analyzers is difficult since they may be based on differences in part-of-speech (POS) systems; the latter cannot directly analyze the outputs of the former. In addition, in unsegmented languages such as Japanese, the ASR outputs are likely to have different word segmentation from that of the language analyzer inputs because they are individually developed.\n",
    "A conventional approach is to generate raw texts from the ASR outputs and re-analyze them using a morphological analyzer. However, if the ASR outputs contain recognition errors, the morphological analyzer incorrectly analyzes them even though they contain correctly recognized words.\n",
    "To avoid this problem, we propose a morpheme conversion method that directly converts ASR outputs into morpheme sequences suitable for the language analyzers. Our experiments show that morpheme conversion is more robust than the conventional approach against recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-460"
  },
  "fang11_interspeech": {
   "authors": [
    [
     "Ren-Ying",
     "Fang"
    ],
    [
     "Bo-Wei",
     "Chen"
    ],
    [
     "Jhing-Fa",
     "Wang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Emotion detection based on concept inference and spoken sentence analysis for customer service",
   "original": "i11_1409",
   "page_count": 4,
   "order": 463,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "In this study, we aim to construct an emotion detection system, which is capable of recognizing emotions of customers from product feedback forms for customer service. The proposed system integrated the ontology and the fuzzy inference algorithm for analyzing and recognizing various emotions through verbal/spoken sentences. Companies can evaluate users' degree of satisfaction from emotions and automatically find out opinions of interest, saving unnecessary human resources.\n",
    "In order to tag emotion labels and compute the relation strength between concepts, we have proposed two algorithms: One is the semantic parser based on natural language processing technique and emotion distance calculation; the other is the concept semantic knowledge database (OMCSNet) combined with fuzzy inference algorithm. Each sentence is then sent into the inference engine for determining its corresponding emotion.\n",
    "Compared with the other baseline systems, our experimental results showed that the emotion recognition rate was increased by 10% on average. The experiments also demonstrated the efficacy of our proposed approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-461"
  },
  "cerisara11_interspeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Pavel",
     "Král"
    ],
    [
     "Claire",
     "Gardent"
    ]
   ],
   "title": "Commas recovery with syntactic features in French and in Czech",
   "original": "i11_1413",
   "page_count": 4,
   "order": 464,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "Automatic speech transcripts can be made more readable and useful for further processing by enriching them with punctuation marks and other meta-linguistic information. We study in this work how to improve automatic recovery of one of the most difficult punctuation marks, commas, in French and in Czech. We show that commas detection performances are largely improved in both languages by integrating into our baseline Conditional Random Field model syntactic features derived from dependency structures. We further study the relative impact of language-independent vs. specific features, and show that a combination of both of them gives the largest improvement. Robustness of these features to speech recognition errors is finally discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-462"
  },
  "falavigna11_interspeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ]
   ],
   "title": "Redundancy reduction in ASR of spontaneous speech through statistical machine translation",
   "original": "i11_1417",
   "page_count": 4,
   "order": 465,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "This paper describes a system, based on statistical machine translation, that tries to remove from the output of an automatic audio transcription system non relevant words, such as: erroneously inserted functional words, filled pauses, interjections, word fragments, etc, as well as to repair, at a certain extent, ungrammatical pieces of sentences.\n",
    "For this work we decided to concentrate on a political speeches application domain, due to the immediate availability of a parallel corpus of automatic audio transcriptions and related proceedings, manually produced.\n",
    "The system can effectively detect and correct several errors (mainly insertions) included in the alignment between a given automatic audio transcription and a reference transcription derived from a corresponding proceeding.\n",
    "Preliminary results, expressed in terms of word error rate, show that the proposed approach allows to improve of a relative 5% with respect to the usage of the pure automatic transcription of the audio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-463"
  },
  "chiang11_interspeech": {
   "authors": [
    [
     "Chin-Chih",
     "Chiang"
    ]
   ],
   "title": "From interview to news text: a study of taiwan TV Political interviews in newspaper reports",
   "original": "i11_1421",
   "page_count": 4,
   "order": 466,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "Mode and genre transformations are main issues in the digital era, especially apparent when news interviews are adapted into written news reports. This paper investigates how interactions in a television news interview are excerpted and adapted into a written news text in newspapers. Through a text analysis of two political interviews in Taiwan on television, and five newspaper reports based on the two interviews, this study discovered that the reporter focuses on the answers of the interviewee, and tends to extract those that fit the political news narrative and are salient in the interactions. Furthermore, this paper demonstrates how the oral interview is adapted into a written news text via various discursive techniques, how interviewee answers are quoted, and how the local interview context is constructed in the news text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-464"
  },
  "oertel11_interspeech": {
   "authors": [
    [
     "Catharine",
     "Oertel"
    ],
    [
     "Stefan",
     "Scherer"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "On the use of multimodal cues for the prediction of degrees of involvement in spontaneous conversation",
   "original": "i11_1541",
   "page_count": 4,
   "order": 467,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "Quantifying the degree of involvement of a group of participants in a conversation is a task which humans accomplish every day, but it is something that, as of yet, machines are unable to do. In this study we first investigate the correlation between visual cues (gaze and blinking rate) and involvement. We then test the suitability of prosodic cues (acoustic model) as well as gaze and blinking (visual model) for the prediction of the degree of involvement by using a support vector machine (SVM).We also test whether the fusion of the acoustic and the visual model improves the prediction. We show that we are able to predict three classes of involvement with an reduction of error rate of 0.30 (accuracy =0.68).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-465"
  },
  "nomoto11_interspeech": {
   "authors": [
    [
     "Narichika",
     "Nomoto"
    ],
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Osamu",
     "Yoshioka"
    ],
    [
     "Satoshi",
     "Takahashi"
    ]
   ],
   "title": "Anger recognition in spoken dialog using linguistic and para-linguistic information",
   "original": "i11_1545",
   "page_count": 4,
   "order": 468,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "This paper proposes a method to recognize anger-dialog based on linguistic and para-linguistic information in speech. Anger is classified into two types; HotAnger (agitated) and ColdAnger (calm). Conventional prosody-features based on para-linguistic can reliably recognize the former but not the latter. To recognize anger more robustly, we apply other para-linguistic cues named dialog-features which are seen in conversational interactive situations between two speakers such as turn-taking and back-channel feedback. We also utilize linguistic-features which represent conversational emotional salience. They are acquired by Pearson's chi-square test by comparing the automatically-transcribed texts between angry and neutral dialogs. Experiments show that the proposed feature combination improves the F-measure of ColdAnger and HotAnger by 26.9 points and 16.1 points against a baseline that uses only prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-466"
  },
  "ivanov11_interspeech": {
   "authors": [
    [
     "A. V.",
     "Ivanov"
    ],
    [
     "G.",
     "Riccardi"
    ],
    [
     "A. J.",
     "Sporka"
    ],
    [
     "J.",
     "Franc"
    ]
   ],
   "title": "Recognition of personality traits from human spoken conversations",
   "original": "i11_1549",
   "page_count": 4,
   "order": 469,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "We are interested in understanding human personality and its manifestations in human interactions. The automatic analysis of such personality traits in natural conversation is quite complex due to the user-profiled corpora acquisition, annotation task and multidimensional modeling. While in the experimental psychology research this topic has been addressed extensively, speech and language scientists have recently engaged in limited experiments. In this paper we describe an automated system for speaker-independent personality prediction in the context of human-human spoken conversations. The evaluation of such system is carried out on the PersIA human-human spoken dialog corpus annotated with user self-assessments of the Big-Five personality traits. The personality predictor has been trained on paralinguistic features and its evaluation on five personality traits shows encouraging results for the conscientiousness and extroversion labels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-467"
  },
  "schuller11_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Zixing",
     "Zhang"
    ],
    [
     "Felix",
     "Weninger"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Using multiple databases for training in emotion recognition: to unite or to vote?",
   "original": "i11_1553",
   "page_count": 4,
   "order": 470,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "We present an extensive study on the performance of data agglomeration and decision-level fusion for robust cross-corpus emotion recognition. We compare joint training with multiple databases and late fusion of classifiers trained on single databases, employing six frequently used corpora of natural or elicited emotion, namely ABC, AVIC, DES, eNTERFACE, SAL, VAM, and three classifiers i. e. SVM, Random Forests, Naive Bayes to best cover for singular effects. On average over classifier and database, data agglomeration and majority voting deliver relative improvements of unweighted accuracy by 9.0% and 4.8%, respectively, over single-database cross-corpus classification of arousal, while majority voting performs best for valence recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-468"
  },
  "burkhardt11_interspeech": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Benjamin",
     "Weiss"
    ],
    [
     "Felix",
     "Weninger"
    ]
   ],
   "title": "“would you buy a car from me?” - on the likability of telephone voices",
   "original": "i11_1557",
   "page_count": 4,
   "order": 471,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "We researched how \"likable\" or \"pleasant\" a speaker appears based on a subset of the \"Agender\" database which was recently introduced at the 2010 Interspeech Paralinguistic Challenge. 32 participants rated the stimuli according to their likability on a seven point scale. An Anova showed that the samples rated are significantly different although the inter-rater agreement is not very high. Experiments with automatic regression and classification by REPTree ensemble learning resulted in a cross-correlation of up to .378 with the evaluator weighted estimator, and 67.6% accuracy in binary classification (likable / not likable). Analysis of individual acoustic feature groups reveals that for this data, auditory spectral features seem to contribute the most to reliable automatic likability analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-469"
  },
  "gibson11_interspeech": {
   "authors": [
    [
     "James",
     "Gibson"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Automatic identification of salient acoustic instances in couples' behavioral interactions using diverse density support vector machines",
   "original": "i11_1561",
   "page_count": 4,
   "order": 472,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "Behavioral coding focuses on deriving higher-level behavioral annotations using observational data of human interactions. Automatically identifying salient events in the observed signal data could lead to a deeper understanding of how specific events in an interaction correspond to the perceived high-level behaviors of the subjects. In this paper, we analyze a corpus of married couples' interactions, in which a number of relevant behaviors, e.g., level of acceptance, were manually coded at the session-level. We propose a multiple instance learning approach called Diverse Density Support Vector Machines, trained with acoustic features, to classify extreme cases of these behaviors, e.g., low acceptance vs. high acceptance. This method has the benefit of identifying salient behavioral events within the interactions, which is demonstrated by comparable classification performance to traditional SVMs while using only a subset of the events from the interactions for classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-470"
  },
  "neiberg11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Predicting speaker changes and listener responses with and without eye-contact",
   "original": "i11_1565",
   "page_count": 4,
   "order": 473,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "This paper compares turn-taking in terms of timing and prediction in human-human conversations under the conditions when participants has eye-contact versus when there is no eye-contact, as found in the HCRC Map Task corpus. By measuring between speaker intervals it was found that a larger proportion of speaker shifts occurred in overlap for the no eye-contact condition. For prediction we used prosodic and spectral features parametrized by time-varying length-invariant discrete cosine coefficients. With Gaussian Mixture Modeling and variations of classifier fusion schemes, we explored the task of predicting whether there is an upcoming speaker change (SC) or not (hold), at the end of an utterance (EOU) with a pause lag of 200 ms. The label SC was further split into LRs (listener responses, e.g. back-channels) and other turn-shifts. The prediction was found to be somewhat easier for the eye-contact condition, for which the average recall rates were 60.57%, 66.35% and 62.00% for turn-shifts, LR and SC respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-471"
  },
  "amarakeerthi11_interspeech": {
   "authors": [
    [
     "Senaka",
     "Amarakeerthi"
    ],
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Liyanage C. De",
     "Silva"
    ],
    [
     "Michael",
     "Cohen"
    ]
   ],
   "title": "Emotion classification using inter- and intra-subband energy variation",
   "original": "i11_1569",
   "page_count": 4,
   "order": 474,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "Speech is one of the most important signals that can be used to detect human emotions. Speech is modulated by different emotions by varying frequency- and energy-related acoustic parameters such as pitch, energy, and formants. In this paper, we describe research on analyzing inter- and intra-subband energy variations to differentiate five emotions. The emotions considered are anger, fear, dislike, sadness, and neutral. We employ a Two-Layered Cascaded Subband (TLCS) filter to study the energy variations for extraction of acoustic features. Experiments were conducted on the Berlin Emotional Data Corpus (BEDC). We achieve average accuracy of 76.4% and 69.3% for speaker-dependent and -independent emotion classifications, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-472"
  },
  "kitahara11_interspeech": {
   "authors": [
    [
     "K.",
     "Kitahara"
    ],
    [
     "S.",
     "Michiwiki"
    ],
    [
     "M.",
     "Sato"
    ],
    [
     "S.",
     "Matsunaga"
    ],
    [
     "M.",
     "Yamashita"
    ],
    [
     "K.",
     "Shinohara"
    ]
   ],
   "title": "Emotion classification of infants' cries using duration ratios of acoustic segments",
   "original": "i11_1573",
   "page_count": 4,
   "order": 475,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "We propose an approach to the classification of emotion clusters using prosodic features. In our approach, we use the duration ratios of specific acoustic segments . resonant cry segments and silence segments . in the infants' cries as prosodic features. We use power and pitch information to detect these segment periods and use normal distribution as a prosodic model to approximate the occurrence probability of the duration ratios of these segments. Classification experiments on two major emotion clusters are carried out using samples of recorded cries of 11 infants. When the detection performance for the segment periods is about 75%, an emotion classification rate of 70.8% is achieved. The classification performance of our approach using the segment duration ratios was significantly better than that of the classification method using power and spectral features, thereby indicating the effectiveness of using prosodic features. Furthermore, we describe a classification method using both spectral and prosodic features with a slightly better performance (71.9%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-473"
  },
  "vlasenko11_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Vlasenko"
    ],
    [
     "Dmytro",
     "Prylipko"
    ],
    [
     "David",
     "Philippou-Hübner"
    ],
    [
     "Andreas",
     "Wendemuth"
    ]
   ],
   "title": "Vowels formants analysis allows straightforward detection of high arousal acted and spontaneous emotions",
   "original": "i11_1577",
   "page_count": 4,
   "order": 476,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "The role of automatic emotion recognition from speech grows continually because of accepted importance of reacting to the emotional state of the user in human-computer interaction. Most part of state-of-the-art emotion recognition methods are based on context independent turn- and frame-level analysis. In our earlier ICME 2011 article it has been shown that robust high arousal acted emotions detection can be performed on the context dependent vowel basis. In contrast to using a HMM/GMM classification with 39-dimensional MFCC vectors, a much more convenient Neyman- Pearson criterion with the only one average F1 value is employed here. In this paper we apply the proposed method to the spontaneous emotion recognition from speech. Also, we avoid the use of speaker-dependent acoustic features in favor of gender-specific ones. Finally we compare performances of acted and spontaneous emotions for different criterion threshold values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-474"
  },
  "neiberg11b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Petri",
     "Laukka"
    ],
    [
     "Hillary Anger",
     "Elfenbein"
    ]
   ],
   "title": "Intra-, inter-, and cross-cultural classification of vocal affect",
   "original": "i11_1581",
   "page_count": 4,
   "order": 477,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "We present intra-, inter- and cross-cultural classifications of vocal expressions. Stimuli were selected from the VENEC corpus and consisted of portrayals of 11 emotions, each expressed with 3 levels of intensity. Classification (nu-SVM) was based on acoustic measures related to pitch, intensity, formants, voice source and duration. Results showed that mean recall across emotions was around 2.4.3 times higher than chance level for both intra- and inter-cultural conditions. For cross-cultural conditions, the relative performance dropped 26%, 32%, and 34% for high, medium, and low emotion intensity, respectively. This suggests that intra-cultural models were more sensitive to mismatched conditions for low emotion intensity. Preliminary results further indicated that recall rate varied as a function of emotion, with lust and sadness showing the smallest performance drops in the cross-cultural condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-475"
  },
  "shiralishahreza11_interspeech": {
   "authors": [
    [
     "Sajad",
     "Shirali-Shahreza"
    ],
    [
     "Yashar",
     "Ganjali"
    ],
    [
     "Ravin",
     "Balakrishnan"
    ]
   ],
   "title": "Verifying human users in speech-based interactions",
   "original": "i11_1585",
   "page_count": 4,
   "order": 478,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "Verifying that a live human is interacting with an automated speech based system is needed in some applications such as biometric authentication. In this paper, we present a method to verify that the user is human. Simply stated, our method asks the user to repeat a sentence. The reply is analyzed to verify that it is the requested sentence and said by a human, not a speech synthesis system. Our method is taking advantage of both speech synthesizer and speech recognizer limitations to detect computer programs, which is new, and potentially more accessible, way to develop CAPTCHA systems. Using an acoustic model trained on voices of over 1000 users, our system can verify the user's answer with 98% accuracy and with 80% success in distinguishing humans from computers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-476"
  },
  "cheng11c_interspeech": {
   "authors": [
    [
     "Jian",
     "Cheng"
    ]
   ],
   "title": "Automatic assessment of prosody in high-stakes English tests",
   "original": "i11_1589",
   "page_count": 4,
   "order": 479,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "Prosody can be used to infer whether or not candidates fully understand a passage they are reading aloud. In this paper, we focused on automatic assessment of prosody in a read-aloud section for a high-stakes English test. A new method was proposed to handle fundamental frequency (F0) of unvoiced segments that significantly improved the predictive power of F0. The k-means clustering method was used to build canonical contour models at the word level for F0 and energy. A direct comparison between the candidate's contours and ideal contours gave a strong prediction of the candidate's human prosody rating. Duration information at the phoneme level was an even better predictive feature. When the contours and duration information were combined, the correlation coefficient r = 0.80 was obtained, which exceeded the correlation between human raters (r = 0.75). The results support the use of the new methods for evaluating prosody in high-stakes assessments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-477"
  },
  "luo11_interspeech": {
   "authors": [
    [
     "Dean",
     "Luo"
    ],
    [
     "Xuesong",
     "Yang"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Improvement of segmental mispronunciation detection with prior knowledge extracted from large L2 speech corpus",
   "original": "i11_1593",
   "page_count": 4,
   "order": 480,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "In this paper, we propose novel methods that utilize prior mispronunciation knowledge extracted from large L2 speech corpus to improve segmental mispronunciation detection performance. Mispronunciation rules are categorized and the occurrence frequency of each error type is calculated from phone-level annotation of the corpora. Based on these rules and statistics of mispronunciations, we construct extended pronunciation lexicons with prior probabilities that reflect how likely each type of error might occur as language models for ASR. A two-pass confusion network based strategy, which uses posterior probability scores with optimal thresholds estimated from the L2 speech corpus, is introduced to refine phone recognition results. Experimental results show that the proposed methods can improve mispronunciation detection performance rather significantly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-478"
  },
  "cheng11d_interspeech": {
   "authors": [
    [
     "Jian",
     "Cheng"
    ],
    [
     "Jianqiang",
     "Shen"
    ]
   ],
   "title": "Off-topic detection in automated speech assessment applications",
   "original": "i11_1597",
   "page_count": 4,
   "order": 481,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "Automated L2 speech assessment applications need some mechanism for validating the relevance of user responses before providing scores. In this paper, we discuss a method for off-topic detection in an automated speech assessment application: a high-stakes English test (PTE Academic). Different from traditional topic detection techniques that use characteristics of text alone, our method mainly focused on using the features derived from speech confidence scores. We also enhanced our off-topic detection model by incorporating other features derived from acoustic likelihood, language model likelihood, and garbage modeling. The final combination model significantly outperformed classification from any individual feature. When fixing the false rejection rate at 5% in our test set, we achieved a false acceptance rate of 9.8%. a very promising result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-479"
  },
  "stuker11_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Johanna",
     "Fay"
    ],
    [
     "Kay",
     "Berkling"
    ]
   ],
   "title": "Towards context-dependent phonetic spelling error correction in children's freely composed text for diagnostic and pedagogical purposes",
   "original": "i11_1601",
   "page_count": 4,
   "order": 482,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "Reading and writing are core competencies of any society. In Germany, international and national comparative studies such as PISA or IGLU have shown that around 25% of German school children do not reach the minimal competence level necessary to function effectively in society by the age of 15. Automized diagnosis and spelling tutoring of children can play an important role in raising their orthographic level of competence. One of several necessary steps in an automatic spelling tutoring system is the automatic correction of achieved text that was freely written by children and contains errors. Based on the common knowledge that children in the first years of school write as they speak, we propose a novel, context-sensitive spelling correction algorithm that uses phonetic similarities, in order to achieve this step. We evaluate our approach on a test set of texts written by children and show that it outperforms Hunspell, a well established isolated error correction program used in text processors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-480"
  },
  "lopezludena11_interspeech": {
   "authors": [
    [
     "V.",
     "López-Ludeña"
    ],
    [
     "R.",
     "San-Segundo"
    ],
    [
     "R.",
     "Córdoba"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J. M.",
     "Pardo"
    ]
   ],
   "title": "Factored translation models for improving a speech into sign language translation system",
   "original": "i11_1605",
   "page_count": 4,
   "order": 483,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "This paper proposes the use of Factored Translation Models (FTMs) for improving a Speech into Sign Language Translation System. These FTMs allow incorporating syntactic-semantic information during the translation process. This new information permits to reduce significantly the translation error rate. This paper also analyses different alternatives for dealing with the non-relevant words. The speech into sign language translation system has been developed and evaluated in a specific application domain: the renewal of Identity Documents and Driver's License. The translation system uses a phrase-based translation system (Moses). The evaluation results reveal that the BLEU (BiLingual Evaluation Understudy) has improved from 69.1% to 73.9% and the mSER (multiple references Sign Error Rate) has been reduced from 30.6% to 24.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-481"
  },
  "abari11_interspeech": {
   "authors": [
    [
     "Kálmán",
     "Abari"
    ],
    [
     "Zsuzsanna Zsófia",
     "Rácz"
    ],
    [
     "Gábor",
     "Olaszy"
    ]
   ],
   "title": "Formant maps in Hungarian vowels - online data inventory for research, and education",
   "original": "i11_1609",
   "page_count": 4,
   "order": 484,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "This paper describes a project for creating an online system for studying the main formant movements of Hungarian vowels in spoken words, as a function of their sound environment. The speech material and the formant data corresponding to the vowels combined present research data for many other purposes as well. For efficient presentation of the data and to allow multilevel comparisons among formant features an online solution was developed. The inventory data can be regarded as a reference, because of the strict conformity between the defined formant data and the formants of the spoken words. A two-step manual verification phase after the completion of automatic formant tracking was performed. The on-line query ensures quick and wide spread studying of formant maps in vowels. The database is available at: \"http://hungarianspeech.tmit.bme.hu/formant\".\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-482"
  },
  "bordel11_interspeech": {
   "authors": [
    [
     "Germán",
     "Bordel"
    ],
    [
     "Silvia",
     "Nieto"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Amparo",
     "Varona"
    ]
   ],
   "title": "Automatic subtitling of the basque parliament plenary sessions videos",
   "original": "i11_1613",
   "page_count": 4,
   "order": 485,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "Subtitling of video contents offered in the web by Spanish administration agencies is required by law for allowing people with hearing impairments to follow them. The automatic bilingual video subtitling system described in this paper has been applied on the plenary sessions videos that the Basque Parliament posts in its web (http://www.parlamentovasco.euskolegebiltzarra.org/), and is running from September 2010. A specific characteristic of this system is the use of a simple phonetic decoder based on a joint selection of Basque and Spanish phone models, since it is not unusual for parliamentarians to make use of a mixing of the two languages. The system uses the manually transcribed Session Diaries (almost verbatim but containing some errors) as subtitles, synchronizing text and audio by means of an acoustic decoder, a multilingual orthographic-phonetic transcriber and a very-large-symbol-sequence aligner.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-483"
  },
  "iribe11_interspeech": {
   "authors": [
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Silasak",
     "Manosavanh"
    ],
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Ryoko",
     "Hayashi"
    ],
    [
     "Chunyue",
     "Zhu"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Generating animated pronunciation from speech through articulatory feature extraction",
   "original": "i11_1617",
   "page_count": 4,
   "order": 486,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "We automatically generate CG animations to express the pronunciation movement of speech through articulatory feature (AF) extraction to help learn a pronunciation. The proposed system uses MRI data to map AFs to coordinate values that are needed to generate the animations. By using magnetic resonance imaging (MRI) data, we can observe the movements of the tongue, palate, and pharynx in detail while a person utters words. AFs and coordinate values are extracted by multi-layer neural networks (MLN). Specifically, the system displays animations of the pronunciation movements of both the learner and teacher from their speech in order to show in what way the learner's pronunciation is wrong. Learners can thus understand their wrong pronunciation and the correct pronunciation method through specific animated pronunciations. Experiments to compare MRI data with the generated animations confirmed the accuracy of articulatory features. Additionally, we verified the effectiveness of using AF to generate animation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-484"
  },
  "chen11e_interspeech": {
   "authors": [
    [
     "Wei",
     "Chen"
    ],
    [
     "Jack",
     "Mostow"
    ]
   ],
   "title": "A tale of two tasks: detecting children's off-task speech in a reading tutor",
   "original": "i11_1621",
   "page_count": 4,
   "order": 487,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "How can an automated tutor detect children's off-task utterances? To answer this question, we trained SVM classifiers on a corpus of 495 children's 36,492 computer-assisted oral reading utterances. On a test set of 620 utterances by 10 held-out readers, the classifier correctly detected 88% of off-task utterances and misclassified 17% of on-task utterances as off-task. As a test of generality, we applied the same classifier to 20 children's 410 responses to vocabulary questions. The classifier detected 84% of off-task utterances but misclassified 57% of on-task utterances. Acoustic and lexical features helped detect off-task speech in both tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-485"
  },
  "iseijaakkola11_interspeech": {
   "authors": [
    [
     "Toshiko",
     "Isei-Jaakkola"
    ],
    [
     "Takatoshi",
     "Naka"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Problems encountered by Japanese EL2 with English short vowels as illustrated on a 3d vowel chart",
   "original": "i11_1625",
   "page_count": 4,
   "order": 488,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "In this study we attempted to illustrate to what extent Japanese university students who study English immediately after their enrolment have acquired English short vowels using graphs and a three-dimensional (= 3D) vowel chart, and thus to clarify what their problems are while simultaneously producing American English short vowels. There was a prediction that Japanese learners of English (JEL2) have weakness in lip-rounding and protrusion since there are no such articulatory movements in Japanese vowels. This was clarified while observing F2 and F3. JEL2 have problems with simultaneous in lip movements, the jaw movements in general in this case. Also we found that there was a difference between female and male JEL2. As far as this experiment is concerned, female JEL2's tongue and jaw movement (F2) is less stable than males'. Moreover, it may be confirmed that the 3D Vowel Chart may be more useful for EL2 than the graph.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-486"
  },
  "pellegrini11_interspeech": {
   "authors": [
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Rui",
     "Correia"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Jorge",
     "Baptista"
    ],
    [
     "Nuno",
     "Mamede"
    ]
   ],
   "title": "Automatic generation of listening comprehension learning material in european portuguese",
   "original": "i11_1629",
   "page_count": 4,
   "order": 489,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "The goal of this work is the automatic selection of materials for a listening comprehension game. We would like to select automatically transcribed sentences from recent broadcast news corpora, in order to gather material for the games with little human effort. The recognized words are used as the ground solution of the exercises, thus sentences with misrecognitions need to be filtered out. Our experiments confirmed the feasibility of the filter chain that automatically selects sentences, although harder confidence thresholds may be needed. Together with the correct words, wrong candidates, namely distractors, are also needed to build the exercises. Two techniques of distractor generation are presented, either based on the confusion networks produced by the recognizer, or on phonetic distances. The experiments confirmed the complementarity of both approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-487"
  },
  "liu11_interspeech": {
   "authors": [
    [
     "Chao-Hong",
     "Liu"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "David",
     "Sarwono"
    ],
    [
     "Jhing-Fa",
     "Wang"
    ]
   ],
   "title": "Candidate generation for ASR output error correction using a context-dependent syllable cluster-based confusion matrix",
   "original": "i11_1633",
   "page_count": 4,
   "order": 490,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "Error correction techniques have been proposed in the applications of language learning and spoken dialogue systems for spoken language understanding. These techniques include two consecutive stages: the generation of correction candidates and the selection of correction candidates. In this study, a Context-Dependent Syllable Cluster (CD-SC)-based Confusion Matrix is proposed for the generation of correction candidates. A Contextual Fitness Score, measuring the sequential relationship to the neighbors of the candidate, is proposed for corrected syllable sequence selection. Finally, the n-gram language model is used to determine the final word sequence output. Experiments show that the proposed method improved from 0.742 to 0.771 in terms of BLEU score as compared to the conventional speech recognition mechanism.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-488"
  },
  "huynh11_interspeech": {
   "authors": [
    [
     "Thai Hoa",
     "Huynh"
    ],
    [
     "Vu An",
     "Tran"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Semi-supervised tree support vector machine for online cough recognition",
   "original": "i11_1637",
   "page_count": 4,
   "order": 491,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "Pneumonia and asthma are among the top causes of death worldwide with 300 million people suffered. In the year 2005, 255,000 people died only because of asthma [1]. Good controlling requires both proper medication and continual monitoring over days and nights. In this paper, we introduce a novel classifier, namely Semi- Supervised Tree Support Vector Machine, to target the problem of cough detection and monitoring. It will adaptively analyze the distribution of samples' confidence metrics, automatically select the most informative samples and re-train the core Tree SVM classifier inside accordingly. Besides, we also introduce a new way to build Tree SVM, based on Fisher Linear Discriminant (FLD) analytic. All are meant to improve final system performance, and our proposed classifier has really demonstrated good improvement over conventional method; validated on a database consists of comprehensive body-sounds, recorded with wearable contact microphone.\n",
    "",
    "",
    "Asthma Statistics. Online: http://www.aaaai.org/media/statistics/asthma-statistics.asp, accessed on Mar. 30, 2011.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-489"
  },
  "zhang11i_interspeech": {
   "authors": [
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Monaural voiced speech segregation based on pitch and comb filter",
   "original": "i11_1741",
   "page_count": 4,
   "order": 492,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "The correlogram is an important mid-level representation for periodic sounds which is widely used in sound source separation and pitch detection. However, it is very time consuming. In this paper, we presented a novel scheme for monaural voiced speech separation without computing correlograms. The noisy speech is firstly decomposing into time-frequency units. Pitch contour of the target speech is extracted according to the zero crossing rate of the units. Then we applied a comb filter to label each unit as target speech or intrusion. Compared with previous correlogram-based method, the proposed algorithm saves computing time and also yields better performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-490"
  },
  "hirasawa11_interspeech": {
   "authors": [
    [
     "Yasuharu",
     "Hirasawa"
    ],
    [
     "Naoki",
     "Yasuraoka"
    ],
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Fast and simple iterative algorithm of lp-norm minimization for under-determined speech separation",
   "original": "i11_1745",
   "page_count": 4,
   "order": 493,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "This paper presents an efficient algorithm to solve Lp-norm minimization problem for under-determined speech separation; that is, for the case that there are more sound sources than microphones. We employ an auxiliary function method in order to derive update rules under the assumption that the amplitude of each sound source follows generalized Gaussian distribution. Experiments reveal that our method solves the L1-norm minimization problem ten times faster than a general solver, and also solves Lp-norm minimization problem efficiently, especially when the parameter p is small; when p is not more than 0.7, it runs in real-time without loss of separation quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-491"
  },
  "rabiee11_interspeech": {
   "authors": [
    [
     "Azam",
     "Rabiee"
    ],
    [
     "Saeed",
     "Setayeshi"
    ],
    [
     "Soo-Young",
     "Lee"
    ]
   ],
   "title": "Monaural speech separation based on a 2d processing and harmonic analysis",
   "original": "i11_1749",
   "page_count": 4,
   "order": 494,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "This paper proposes a new Computational Auditory Scene Analysis (CASA) approach based on a 2D spectro-temporal analysis and harmonic separation. The 2D processing, so-called Grating Compression Transform (GCT), analyzes the spectro-temporal content of the spectrogram, mimicking the processing of the primary auditory cortex. The estimated pitches from the GCT analysis are used for separation using harmonic magnitude suppression (HMS). A powerful aspect of our model is requiring no prior training on a specific training corpus. A baseline system based on the harmonic separation is designed for comparison. Since the baseline system is similar to the proposed except the auditory-cortex-like analysis, the SIR results illustrate its importance in this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-492"
  },
  "jafari11b_interspeech": {
   "authors": [
    [
     "Ingrid",
     "Jafari"
    ],
    [
     "Serajul",
     "Haque"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Sven",
     "Nordholm"
    ]
   ],
   "title": "Underdetermined blind source separation with fuzzy clustering for arbitrarily arranged sensors",
   "original": "i11_1753",
   "page_count": 4,
   "order": 495,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "Recently, the concept of time-frequency masking has developed as an important approach to the blind source separation problem, particularly when in the presence of reverberation. However, previous research has been limited by factors such as the sensor arrangement and/or the mask estimation technique implemented. This paper presents a novel integration of two established approaches to BSS in an effort to overcome such limitations. A multidimensional feature vector is extracted from a non-linear sensor arrangement, and the fuzzy c-means algorithm is then applied to cluster the feature vectors into representations of the source speakers. Fuzzy time-frequency masks are estimated and applied to the observations for source recovery. The evaluations on the proposed study demonstrated improved separation quality over all test conditions. This establishes the potential of multidimensional fuzzy c-means clustering for mask estimation in the context of blind source separation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-493"
  },
  "vu11_interspeech": {
   "authors": [
    [
     "Dang Hai Tran",
     "Vu"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "On initial seed selection for frequency domain blind speech separation",
   "original": "i11_1757",
   "page_count": 4,
   "order": 496,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "In this paper we address the problem of initial seed selection for frequency domain iterative blind speech separation (BSS) algorithms. The derivation of the seeding algorithm is guided by the goal to select samples which are likely to be caused by source activity and not by noise and at the same time originate from different sources. The proposed algorithm has moderate computational complexity and finds better seed values than alternative schemes, as is demonstrated by experiments on the database of the SiSEC2010 challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-494"
  },
  "tanaka11_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Tanaka"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Spatial filter calibration based on minimization of modified LSD",
   "original": "i11_1761",
   "page_count": 4,
   "order": 497,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "A new sound source separation method has been developed that is robust against individual variability in microphones and acoustic lines. A specific area that has a target sound source was enhanced by using a spatial filter developed by time-frequency masking. However, there is a strong likelihood that the spatial filters will be distorted due to the impact of individual variability in microphone characteristics and acoustic lines. To solve this problem, calibration of these spatial filters' shapes was attempted using a modified log-spectral distance (MLSD) minimization criterion, which uses utterances made by each individual (i.e., a sound source) at the desired positions. The effectiveness of this spatial filter calibration was experimentally verified in speech recognition experiments; MLSD-based calibration had fewer word errors than the cases without calibration and calibration using other criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-495"
  },
  "nakashika11_interspeech": {
   "authors": [
    [
     "Toru",
     "Nakashika"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Probabilistic spectrum envelope: categorized audio-features representation for NMF-based sound decomposition",
   "original": "i11_1765",
   "page_count": 4,
   "order": 498,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "NMF (Non-negative Matrix Factorization) has been one of the most useful techniques for audio signal analysis in recent years. In particular, supervised NMF, in which a large number of samples is used for analyzing a signal, is garnering much attention in sound source separation or noise reduction research. However, because such methods require all the possible samples for the analysis, it is hard to build a practical system based on this method. In this paper, we propose a novel method of signal analysis that combines the NMF and probabilistic approaches. In this approach, it is assumed that each audio-source category (such as phonemes or musical instruments) has an environment-invariant feature, called a probabilistic spectrum envelope (PSE). At the start, the PSE of each category is learned using a technique based on Gaussian Process Regression. Then, the observed spectrum is analyzed using a combination of supervised NMF and Genetic Algorithm with pre-trained PSEs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-496"
  },
  "choi11b_interspeech": {
   "authors": [
    [
     "Jinho",
     "Choi"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "A high resolution multiple source localization based on generalized cumulant structure (GCS) matrix",
   "original": "i11_1769",
   "page_count": 4,
   "order": 499,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "This paper considers a high-resolution multiple non-stationary and non-Gaussian source localization algorithm based on the proposed generalized cumulant structure (GCS) matrix that is constructed as a weighted sum of the second and fourth order cumulants of the sensor signals. The weight determines the rank and range space of the GCS matrix, and the range space of the GCS matrix should be same to the range space of the virtual array manifold matrix to estimate the true direction of arrival (DOA)s of the sources. To estimate the weight and the DOAs of sources, a rank constrained optimization problem is formulated. The optimal solution is computationally heavy, and for this reason a suboptimal solution is considered. With the weight set to an arbitrary value, singular value decomposition on the GCS matrix is performed to determine the singular matrix associated with the null space of the virtual array response matrix, and either this singular matrix or the singular matrix obtained using only the second order (SO) statistic is used to obtain the proposed spatial spectrum. Experimental results show that the proposed algorithm performs better than the recently proposed SO cumulant based algorithm for synthetic and real speech data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-497"
  },
  "grais11b_interspeech": {
   "authors": [
    [
     "Emad M.",
     "Grais"
    ],
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Single channel speech music separation using nonnegative matrix factorization with sliding windows and spectral masks",
   "original": "i11_1773",
   "page_count": 4,
   "order": 500,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "A single channel speech-music separation algorithm based on nonnegative matrix factorization (NMF) with sliding windows and spectral masks is proposed in this work. We train a set of basis vectors for each source signal using NMF in the magnitude spectral domain. Rather than forming the columns of the matrices to be decomposed by NMF of a single spectral frame, we build them with multiple spectral frames stacked in one column. After observing the mixed signal, NMF is used to decompose its magnitude spectra into a weighted linear combination of the trained basis vectors for both sources. An initial spectrogram estimate for each source is found, and a spectral mask is built using these initial estimates. This mask is used to weight the mixed signal spectrogram to find the contributions of each source signal in the mixed signal. The method is shown to perform better than the conventional NMF approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-498"
  },
  "marinhurtado11b_interspeech": {
   "authors": [
    [
     "Jorge I.",
     "Marin-Hurtado"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Perceptually-inspired processing for multichannel Wiener filter",
   "original": "i11_1777",
   "page_count": 4,
   "order": 501,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "Binaural noise-reduction techniques based on Multichannel Wiener filter (MWF) have been reported as promissory candidates to be used in binaural hearing aids because of their effective SNR improvement at any arbitrary direction of arrival of the target signal and the preservation of localization cues. There are different MWF techniques derived in the FFT domain. The use of an FFT-based processing involve two important challenges for the real-time implementation of these techniques in a digital hearing: high computational cost and processing delay. To reduce computational cost and processing delay without degrading the SNR improvement and sound quality, this paper proposes the use of an auditory representation instead of an FFT representation. The proposed processing shows significant advantages over an FFT-based processing: reduction of the computational cost and processing delay, and improvement of the output SNR and sound quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-499"
  },
  "nakano11_interspeech": {
   "authors": [
    [
     "Shoichi",
     "Nakano"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Speech recognition in mixed sound of speech and music based on vector quantization and non-negative matrix factorization",
   "original": "i11_1781",
   "page_count": 4,
   "order": 502,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "This paper describes a speech recognition method for mixed sound, consisting of speech and music, that removes the music only based on vector quantization (VQ) and non-negative matrix factorization (NMF). For isolated word recognition using the clean speech model, an improvement of about 15% was obtained compared with the case of not removing music. Furthermore, a high recognition rate of about 90% was achieved, even under the 0 dB condition using a model trained from the mixed sound after removing the music according to the VQ method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-500"
  },
  "nakatani11_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Masakiyo",
     "Fujimoto"
    ]
   ],
   "title": "Reduction of highly nonstationary ambient noise by integrating spectral and locational characteristics of speech and noise for robust ASR",
   "original": "i11_1785",
   "page_count": 4,
   "order": 503,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "This paper proposes a new multi-channel noise reduction approach that can appropriately handle highly nonstationary noise based on the spectral and locational features of speech and noise. We focus on a distant talking scenario, where a 2-ch microphone array receives a target speaker's voice from the front while it receives highly nonstationary ambient noise from any direction. To cope well with this scenario, we introduce prior training not only for the spectral features of speech and noise but also for their locational features, and utilize them in a unified manner. The proposed method can distinguish rapid changes in speech and noise based mainly on their locational features, while it can reliably estimate the spectral shapes of the speech based largely on the spectral features. A filter-bank based implementation is also discussed to enable the proposed method to work in real time. Experiments using the PASCAL CHiME separation and recognition challenge task show the superiority of the proposed method as regards both speech quality and automatic speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-501"
  },
  "drioli11_interspeech": {
   "authors": [
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Andrea",
     "Calanca"
    ]
   ],
   "title": "Voice processing by dynamic glottal models with applications to speech enhancement",
   "original": "i11_1789",
   "page_count": 4,
   "order": 504,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "We discuss the use of low-dimensional physical models of the voice source for speech coding and processing applications. A class of waveform-adaptive dynamic glottal models and parameter tracking procedures are illustrated. The model and analysis procedures are assessed by addressing speech encoding and enhancement, achievable by using a state space version of the dynamical model in a Extended Kalman filtering framework. The proposed method is shown to provide better SNR improvement if compared to a standard AR Kalman filtering scheme.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-502"
  },
  "sang11_interspeech": {
   "authors": [
    [
     "Jinqiu",
     "Sang"
    ],
    [
     "Guoping",
     "Li"
    ],
    [
     "Hongmei",
     "Hu"
    ],
    [
     "Mark E.",
     "Lutman"
    ],
    [
     "Stefan",
     "Bleeck"
    ]
   ],
   "title": "Supervised sparse coding strategy in cochlear implants",
   "original": "i11_1793",
   "page_count": 4,
   "order": 505,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "In this paper we explore how to improve a sparse coding (SC) strategy that was successfully used to improve subjective speech perception in noisy environment in cochlear implants. On the basis of the existing unsupervised algorithm, we developed an enhanced supervised SC strategy, using the SC shrinkage (SCS) principle. The new algorithm is implemented at the stage of the spectral envelopes after the signal separation in a 22-channel filter bank. SCS can extract and transmit the most important information from noisy speech. The new algorithm is compared with the unsupervised algorithm using objective evaluation for speech in babble and white noise (signal-to-noise ratios, SNR = 10dB, 5dB, 0dB) using objective measures in a cochlea implant simulation. Results show that the supervised SC strategy performs better in white noise, but not significantly better with babble noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-503"
  },
  "bertini11_interspeech": {
   "authors": [
    [
     "Chiara",
     "Bertini"
    ],
    [
     "Pier Marco",
     "Bertinetto"
    ],
    [
     "Na",
     "Zhi"
    ]
   ],
   "title": "Chinese and Italian speech rhythm: normalization and the CCI algorithm",
   "original": "i11_1853",
   "page_count": 4,
   "order": 506,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "This paper re-examines the speech rhythm of Beijing Chinese and Pisa Italian by means of the Control/Compensation Index (CCI), with a view to normalizing the speech data, in order to reduce the effect of the rate factor. Two metrics were applied: (a) DnCCI, an adaptation to the CCI model of the nPVI normalization strategy; (b) SnCCI, a z-score normalization, which takes into account the actual constitution of each V- and C-interval, by referring the individual segment's duration to the mean duration of the members of the corresponding natural phoneme class. The results indicate the advantage of the SnCCI metrics as a normalization strategy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-504"
  },
  "mairano11_interspeech": {
   "authors": [
    [
     "Paolo",
     "Mairano"
    ],
    [
     "Antonio",
     "Romano"
    ]
   ],
   "title": "Rhythm metrics on syllables and feet do not work as expected",
   "original": "i11_1857",
   "page_count": 4,
   "order": 507,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "The aim of this paper is to explore the possibility of using rhythm metrics on the traditional units of speech rhythm (the syllable and the foot), instead of applying them to consonantal and vocalic intervals. Despite [1] had already proven that the standard deviation of syllables and feet did not provide a satisfactory representation of the traditional rhythm classes, some recent studies obtained encouraging results. In particular, [2] applied the PVI to English and Estonian syllables and feet, and a similar approach is intrinsic in the YARD index (cf. [3]) though only at syllable level. We computed the deltas and the PVIs on syllables (measured as the distance between two successive vocalic onsets) and feet (measured as the distance between the onsets of two stressed vowels) for 30 samples of 14 languages.\n",
    "The results do not confirm expectations and do not seem to support the use of these units for the study of speech rhythm in these terms.\n",
    "s Roach, P. (1982) On the Distinction between Stress-timed and Syllable-timed Languages. In D. Crystal, Linguistic controversies, London: Edward Arnold, 73-79. Asu, E. L. & Nolan, F. (2006) Estonian and English rhythm: a two-dimensional quantification based on syllables and feet. Proc. of Speech Prosody 2006, Dresden (Germany). Wagner, P. & Dellwo, V. (2004) Introducing YARD (Yet Another Rhythm Determinator) and Re-Introducing Isochrony to Rhythm Research. Proc. of Speech Prosody 2004, Nara (Japan), 23-26 March 2004.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-505"
  },
  "chen11f_interspeech": {
   "authors": [
    [
     "Lei",
     "Chen"
    ],
    [
     "Klaus",
     "Zechner"
    ]
   ],
   "title": "Applying rhythm features to automatically assess non-native speech",
   "original": "i11_1861",
   "page_count": 4,
   "order": 508,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "Speech rhythm measurements have been used in a limited number of previous studies on automated speech assessment, an approach using speech recognition technology to judge non-native speakers' proficiency levels. However, one of the most problematic issues of these previous studies is a lack of a comparison of these rhythm features with other effective non-rhythm features found in decade-long previous research. In this paper, we extracted both non-rhythm and rhythm features and compared them with respect to their performances to predict proficiency scores rated by humans. We show that adding rhythm features significantly improves the performance of the scoring model based only on non-rhythm features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-506"
  },
  "vaughan11_interspeech": {
   "authors": [
    [
     "Brian",
     "Vaughan"
    ]
   ],
   "title": "Prosodic synchrony in co-operative task-based dialogues: a measure of agreement and disagreement",
   "original": "i11_1865",
   "page_count": 4,
   "order": 509,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "Prosodic synchrony has been reported to be an important aspect of conversational dyads. In this paper, synchrony in four different dyads is examined. A Time Aligned Moving Average (TAMA) procedure is used to temporally align the prosodic measurements for the detection of synchrony in the dyads. An overlapping windowed correlation procedure is used to measure synchrony for six different prosodic parameters: mean pitch, pitch range, mean intensity, intensity range, centre of gravity and spectral slope. This study shows that a windowed correlation procedure better captures the dynamic nature of speech synchrony than a single measure across a whole conversation. This method also enables points of concurrent synchrony between prosodic parameters to be detected. Moreover, the synchrony of the prosodic parameters was considered in relation to levels of agreement and disagreement in the four dyads. Results show only one parameter in one dyad to be significantly correlated with agreement/disagreement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-507"
  },
  "niebuhr11_interspeech": {
   "authors": [
    [
     "Oliver",
     "Niebuhr"
    ],
    [
     "Astrid",
     "Wolf"
    ]
   ],
   "title": "Low and high, short and long by crook or by hook?",
   "original": "i11_1869",
   "page_count": 4,
   "order": 510,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "The paper deals with perceived speech rhythm, starting from the observation that two nouns with a conjunction in between ('X and/or Y', cf. title) sound more rhythmical in a particular noun order. A perception experiment on German with real and pseudo nouns provides evidence that speech rhythm is not just created prosodically by means of high and low or long and short syllables, but that the phonetic properties of the vowel nuclei and of the consonantal onsets and offsets of the stressed syllables are separate segmental constituents of speech rhythm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-508"
  },
  "heinrich11_interspeech": {
   "authors": [
    [
     "Christian",
     "Heinrich"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Estimating speaking rate by means of rhythmicity parameters",
   "original": "i11_1873",
   "page_count": 4,
   "order": 511,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "In this paper we present a speech rate estimator based on so-called rhythmicity features derived from a modified version of the short-time energy envelope. To evaluate the new method, it is compared to a traditional speech rate estimator on the basis of semi-automatic segmentation. Speech material from the Alcohol Language Corpus (ALC) covering intoxicated and sober speech of different speech styles provides a statistically sound foundation to test upon. The proposed measure clearly correlates with the semi-automatically determined speech rate and seems to be robust across speech styles and speaker states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-509"
  },
  "arnold11_interspeech": {
   "authors": [
    [
     "Denis",
     "Arnold"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Comparing word and syllable prominence rated by naïve listeners",
   "original": "i11_1877",
   "page_count": 4,
   "order": 512,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "Prominence has been widely studied on the word level and the syllable level. An extensive study comparing the two approaches is missing in the literature. This study investigates how word and syllable prominence relate to each other in German. We find that perceptual ratings based on the word level are more extreme than those based on the syllable level. The correlations between word prominence and acoustic features are greater than the correlations between syllable prominence and acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-510"
  },
  "tokuma11_interspeech": {
   "authors": [
    [
     "Shinichi",
     "Tokuma"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "L1/L2 perception of lexical stress with F0 peak-delay: effect of an extra syllable added",
   "original": "i11_1881",
   "page_count": 4,
   "order": 513,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "This study addressed the problems of our previous study [5] and investigated the perceptual effect of F0 peak-delay on L1 / L2 perception of English lexical stress. A trisyllabic English nonsense word 'ninini' /nInInI/ whose F0 was set to reach its peak around the second syllable was embedded in a frame sentence and used as the stimulus of the perceptual experiment. Native English and Japanese speakers were asked to determine lexical stress locations in the experiment. The results showed that in the perception of English lexical stress, delayed F0 peaks which were aligned with the second syllable of the stimulus words perceptually affected both Japanese and English groups, although slightly in a different manner: the Japanese group perceived the delayed F0 peaks as a cue to lexical stress in the first syllable when the peaks were aligned with, or before, the end of /n/ in the second syllable, while the English group had the boundary shifted in an earlier temporal position. It was also discovered that the Japanese group had greater sensitivity to the delayed peak positions. The difference could be attributed to the dependency upon F0 as a perceptual cue and upon its alignment position in a constant / relative timing, as well as the speech rate difference between the two languages.\n",
    "",
    "",
    "Tokuma, S. and Xu, Y., The effect of F0 peak-delay on the L1 / L2 perception of English lexical stress. Proceedings of Interspeech 2009, Brighton, UK. pp. 1687-1690, 2009.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-511"
  },
  "seng11_interspeech": {
   "authors": [
    [
     "Kheang",
     "Seng"
    ],
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Letter-to-phoneme conversion based on two-stage neural network focusing on letter and phoneme contexts",
   "original": "i11_1885",
   "page_count": 4,
   "order": 514,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "The improvement of Letter-To-Phoneme (L2P) conversion that can output the phoneme strings corresponding to Out-Of-Vocabulary (OOV) words, especially in English language, has become one of the most important issues in Text-To-Speech (TTS) research. In this paper, we propose a Two-Stage Neural Network (NN) based approach to solve the problem of conflicting output at a phonemic level. Both Letter and Phoneme Context-Dependent models are combined and implemented in the first-stage NN to convert several letters into several phonemes. Then, the second-stage NN can predict the final output phoneme by observing on a combination of several consecutive phoneme sequences that obtained from the first-stage NN. Therefore, our L2P conversion module takes a sequence of letters as input and outputs only one phoneme at each time. By focusing mainly on the result of word accuracy of OOV words, this new approach usually provides a higher performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-512"
  },
  "orr11_interspeech": {
   "authors": [
    [
     "Rosemary",
     "Orr"
    ],
    [
     "Hugo",
     "Quené"
    ],
    [
     "Roeland van",
     "Beek"
    ],
    [
     "Thari",
     "Diefenbach"
    ],
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Marijn",
     "Huijbregts"
    ]
   ],
   "title": "An international English speech corpus for longitudinal study of accent development",
   "original": "i11_1889",
   "page_count": 4,
   "order": 515,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "If English is used intensively as a lingua franca in a multi-language community, do speakers then converge towards a single common accent? This speech corpus allows for longitudinal study to investigate the question of convergence by means of repeated speech recordings of students at an English-language college over a period of 5 years. We describe the content and collection of the corpus and the type of research that is envisaged, as well as tools used to manage and analyze the recordings, including automatic phone recognition for prosodic analyses; and intelligibility experiments using the SRT method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-513"
  },
  "kim11e_interspeech": {
   "authors": [
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Kyuwhan",
     "Lee"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "A corpus-based study of English pronunciation variations",
   "original": "i11_1893",
   "page_count": 4,
   "order": 516,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "This paper aims to present an analysis of English pronunciation variations using the TIMIT corpus of American English. The manually annotated data are analyzed by comparing the pronunciation variants to their canonical pronunciations which are defined by using the CMU Pronunciation Dictionary. Vowels and consonants are separately analyzed with respect to substitution, deletion and insertion. The results show that: i) vowels are more subject to substitution than deletion, whereas consonants are more subject to deletion than substitution; and ii) vocalic substitutions are related to the raising and the reduction of vowels, whereas consonantal substitutions are related to changes in voice, place of articulation and manner of articulation. Given that the ultimate goal of pronunciation training in the area of second language acquisition is to help students achieve a reasonably \"intelligible\" pronunciation rather than an \"accent-less\" pronunciation, the results of this study will contribute to the determination of \"comprehensible\" pronunciation of English. Furthermore, they will also contribute to the study of English phonetics and phonology as well as to the development of pronunciation modeling of English speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-514"
  },
  "stoakes11_interspeech": {
   "authors": [
    [
     "Hywel",
     "Stoakes"
    ],
    [
     "Andrew",
     "Butcher"
    ],
    [
     "Janet",
     "Fletcher"
    ],
    [
     "Marija",
     "Tabain"
    ]
   ],
   "title": "Long term average speech spectra in yolngu matha and pitjantjatjara speaking females and males",
   "original": "i11_1897",
   "page_count": 4,
   "order": 517,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "This paper provides a spectral analysis of two Australian languages Yolngu Matha (YM), Pitjantjatjara (PTJ) and Australian Aboriginal English (AAE) as spoken in two language communities. The aim of this study is to show clear quantitative spectral differences between Australian Aboriginal English and the two Aboriginal languages. Thirteen speakers of Yolngu Matha (YolNu Matha) ten male, three female and three female speakers of Pitjantjatjara were recorded reading or retelling a passage in their first language and also in English. The results show that there is a difference between the spectral averages of the two language groups with the AAE having higher amplitudes at higher frequencies when compared to the two Australian languages. The Australian Aboriginal language examples all show higher amplitudes for frequencies between 750 Hz and 2 kHz for all speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-515"
  },
  "graczi11_interspeech": {
   "authors": [
    [
     "Tekla Etelka",
     "Gráczi"
    ],
    [
     "Steven M.",
     "Lulich"
    ],
    [
     "Tamás Gábor",
     "Csapó"
    ],
    [
     "András",
     "Beke"
    ]
   ],
   "title": "Context and speaker dependency in the relation of vowel formants and subglottal resonances - evidence from Hungarian",
   "original": "i11_1901",
   "page_count": 4,
   "order": 518,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "Subglottal resonances are claimed to divide front/back vowels and low/high vowels in several languages, including Hungarian. However, some 'recalcitrant' vowels appear to resist this mould. We therefore performed a careful analysis of the role coarticulation and speaker-dependent effects might play in the recalcitrance of these vowels in Hungarian. The present analysis focused on various stop contexts in order to see the place of articulation triggered effects. It is shown that the subglottal resonances indeed divide the vowel space as claimed, and that the recalcitrance of certain vowels is due to coarticulation with specific consonants. The magnitude of the coarticulation effect is speaker dependent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-516"
  },
  "pawi11_interspeech": {
   "authors": [
    [
     "Alipah",
     "Pawi"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Seyed",
     "Ghorshi"
    ]
   ],
   "title": "Fundamental frequency estimation using modified higher order moments and multiple windows",
   "original": "i11_1965",
   "page_count": 4,
   "order": 519,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "This paper proposes a set of higher-order modified moments for estimation of the fundamental frequency of speech and explores the impact of the speech window length on pitch estimation error. The pitch extraction methods are evaluated in a range of noise types and SNRs. For calculation of errors, pitch reference values are calculated from manually-corrected estimates of the periods obtained from laryngograph signals. The results obtained for the 3rd and 4th order modified moment compare well with methods based on correlation and magnitude difference criteria and the YIN method; with improved pitch accuracy and less occurrence of large errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-517"
  },
  "wohlmayr11_interspeech": {
   "authors": [
    [
     "Michael",
     "Wohlmayr"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "EM-based gain adaptation for probabilistic multipitch tracking",
   "original": "i11_1969",
   "page_count": 4,
   "order": 520,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "We introduce an EM algorithm for automatic speaker gain adaptation, and use this approach for probabilistic multipitch tracking. We derive a lower bound on the log-likelihood of the gain parameters and use a fast pruning method to make lower bound optimization efficient. We evaluate the performance of gain adapted multipitch tracking on the GRID database, where 3000 speech mixtures were generated for each mixing level. For gain differences in the range of zero up to 18dB, the proposed method achieves almost the same performance as for the case where the gain is assumed to be known.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-518"
  },
  "drugman11_interspeech": {
   "authors": [
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Joint robust voicing detection and pitch estimation based on residual harmonics",
   "original": "i11_1973",
   "page_count": 4,
   "order": 521,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "This paper focuses on the problem of pitch tracking in noisy conditions. A method using harmonic information in the residual signal is presented. The proposed criterion is used both for pitch estimation, as well as for determining the voicing segments of speech. In the experiments, the method is compared to six stateof- the-art pitch trackers on the Keele and CSTR databases. The proposed technique is shown to be particularly robust to additive noise, leading to a significant improvement in adverse conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-519"
  },
  "govind11_interspeech": {
   "authors": [
    [
     "D.",
     "Govind"
    ],
    [
     "S. R. M.",
     "Prasanna"
    ],
    [
     "Debadatta",
     "Pati"
    ]
   ],
   "title": "Epoch extraction in high pass filtered speech using hilbert envelope",
   "original": "i11_1977",
   "page_count": 4,
   "order": 522,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "Hilbert envelope (HE) is defined as the magnitude of the analytic signal. This work proposes HE based zero frequency filtering (ZFF) approach for the extraction of epochs in high pass filtered speech. Epochs in speech correspond to instants of significant excitation like glottal closure instants. The ZFF method for epoch extraction is based on the signal energy around the impulse at zero frequency which seems to be significantly attenuated in case of high pass filtered speech. The low frequency nature of HE reinforces the signal energy around the impulse at zero frequency. This work therefore processes the HE of high pass filtered speech or its residual by zero frequency filtering for epoch extraction. The proposed approach shows significant improvement in performance for the high pass filtered speech compared to the conventional ZFF of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-520"
  },
  "pavlovets11_interspeech": {
   "authors": [
    [
     "Alexander",
     "Pavlovets"
    ],
    [
     "Alexander",
     "Petrovsky"
    ]
   ],
   "title": "Robust HNR-based closed-loop pitch and harmonic parameters estimation",
   "original": "i11_1981",
   "page_count": 4,
   "order": 523,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "An important problem in speech coding framework is model parameters estimation. In most cases parametric speech coding methods do not preserve shape of speech waveform. This fact implies straightforward parameters estimation and analysis-bysynthesis method is hardly used.\n",
    "A novel analysis-by-synthesis parameters estimation method in speech coders based on harmonic models presented. We introduce improved speech model based on robust harmonic and noise components separation. The separation is performed with usage of Pitch Tracking Modified DFT (PTDFT). Harmonic parameters and pitch frequency are estimated simultaneously in a closed-loop manner based on Harmonic-to-Noise Ratio (HNR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-521"
  },
  "prakash11_interspeech": {
   "authors": [
    [
     "Chetana",
     "Prakash"
    ],
    [
     "Dhananjaya",
     "N."
    ],
    [
     "Suryakanth V.",
     "Gangashetty"
    ]
   ],
   "title": "Exploring bessel features for detection of glottal closure instants",
   "original": "i11_1985",
   "page_count": 4,
   "order": 524,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "For voiced speech, the most significant excitation takes place around the instant of glottal closure. Glottal closure instants (GCI) information is useful for accurate speech analysis. In particular accurate spectrum analysis is performed by considering the speech in the intervals of glottal closure. In this paper we propose an approach for detection of GCI by exploring Bessel feature, and the use of AM-FM signal. Using appropriate range of Bessel coefficients, the narrow band, band limited signal is obtained for the given signal. The band limited signal is considered as AM-FM signal. The signal is band limited for 0.300 Hz to remove effect of formants. Amplitude envelope (AE) function of the AM-FM signal model has been estimated by the discrete energy separation algorithm (DESA). The performance of the method is demonstrated using CMU-Arctic database. The corresponding electro-glottograph (EGG) signals are used as a reference for the validation of the detected GCI locations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-522"
  },
  "cabral11_interspeech": {
   "authors": [
    [
     "João P.",
     "Cabral"
    ],
    [
     "John",
     "Kane"
    ],
    [
     "Christer",
     "Gobl"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Evaluation of glottal epoch detection algorithms on different voice types",
   "original": "i11_1989",
   "page_count": 4,
   "order": 525,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "According to the source-filter model of speech production, speech can be represented by passing the excitation signal through the vocal tract filter. The epoch or instant of maximum excitation corresponds to the glottal closure instant. Several speech processing applications require robust epoch detection but this can be a difficult task. Although state-of-the-art epoch estimation methods can produce reliable results, they are generally evaluated using speech recorded with a neutral voice quality (modal voice). This paper reviews and evaluates six popular algorithms for the calculation of glottal closure instants on speech spoken with modal voice and seven additional voice qualities. Results show that the performance of each method is affected by the voice type and that some methods perform better than others for each voice quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-523"
  },
  "origlia11_interspeech": {
   "authors": [
    [
     "Antonio",
     "Origlia"
    ],
    [
     "Giovanni",
     "Abete"
    ],
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Iolanda",
     "Alfano"
    ],
    [
     "Renata",
     "Savy"
    ],
    [
     "Bogdan",
     "Ludusan"
    ]
   ],
   "title": "A divide et impera algorithm for optimal pitch stylization",
   "original": "i11_1993",
   "page_count": 4,
   "order": 526,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "We present OpS, a divide et impera algorithm to address the problem of pitch stylization as an optimization process in O(N log N). We aim at balancing the quality of the stylized curve and its cost in terms of the number of control points used. We also investigate how the occurrence of prominent syllables can be exploited to obtain less expensive stylizations. Our tests show that the basic OpS algorithm performs in a similar way to the MOMEL algorithm without having to set any parameter. By introducing prominence, we show that the cost of the stylization is lowered without losing perceptual equality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-524"
  },
  "sousa11_interspeech": {
   "authors": [
    [
     "Ricardo",
     "Sousa"
    ],
    [
     "Aníbal",
     "Ferreira"
    ]
   ],
   "title": "Singing voice analysis using relative harmonic delays",
   "original": "i11_1997",
   "page_count": 4,
   "order": 527,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "In this paper we introduce new phase-related features denoting the delay between the harmonics and the fundamental frequency of a periodic signal, notably of voiced singing. These features are identified as Normalized Relative Delay (NRD) and denote the phase contribution to the shape invariance of a periodic signal. Thus, NRDs are amenable to a physical and psychophysical interpretation and are structurally independent of the overall time shift of the signal, an important property that is shared with the magnitude spectrum in the case of a locally stationary signal. We describe the NRD and report on preliminary studies testing the discrimination capability of NRDs applied to singing signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-525"
  },
  "lee11e_interspeech": {
   "authors": [
    [
     "S. W.",
     "Lee"
    ],
    [
     "Minghui",
     "Dong"
    ]
   ],
   "title": "Singing voice synthesis: singer-dependent vibrato modeling and coherent processing of spectral envelope",
   "original": "i11_2001",
   "page_count": 4,
   "order": 528,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "Pleasant singing voice is often ornamented by vibrato. This pitch fluctuation acts as a distinctive feature for singing and promotes voice quality. Nevertheless, independent pitch processing in singing voice synthesis does not guarantee the output quality. The spectral envelope actually varies with pitch during human voice production. This paper proposes a modeling technique for singers' vibratos, followed by a joint processing on vibrato and spectral envelope, such that these attributes are consistent. The performance of the proposed processing has been verified by subjective listening test. The synthetic singing outputs are found to have similar quality as the human singing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-526"
  },
  "beux11_interspeech": {
   "authors": [
    [
     "Sylvain Le",
     "Beux"
    ],
    [
     "Lionel",
     "Feugère"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Chorus digitalis: experiments in chironomic choir singing",
   "original": "i11_2005",
   "page_count": 4,
   "order": 529,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "This paper reports on experiments in real-time gestural control of voice synthesis. The ability of hand writing gestures for controlling singing intonation (chironomic singing synthesis) is studied. In a first part, the singing synthesizer and controller are described. The system is developed in an environment for multi-users music synthesis, allowing for synthetic choir singing. In a second part, performances of subjects playing with the system are analyzed. The results show that chironomic singers are able to control melody with accuracy, to perform vibrato, portamento and other types of fine-grained intonation variations, and to give convincing musical performances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-527"
  },
  "li11h_interspeech": {
   "authors": [
    [
     "Kun",
     "Li"
    ],
    [
     "Shuang",
     "Zhang"
    ],
    [
     "Mingxing",
     "Li"
    ],
    [
     "Wai-Kit",
     "Lo"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Prominence model for prosodic features in automatic lexical stress and pitch accent detection",
   "original": "i11_2009",
   "page_count": 4,
   "order": 530,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "A prominence model is proposed for enhancing prosodic features in automatic lexical stress and pitch accent detection. We make use of a loudness model and incorporate differential pitch values to improve conventional features. Experiments show that these new prosodic features can improve the detection of lexical stress and pitch accent by about 6%. We further employ a prominence model to take into account of effects from neighboring syllables. For pitch accent detection, we achieve a further performance improvement from 80.61% to 83.30%. For lexical stress detection, we achieve performance improvements in (i) classification of primary, secondary and unstressed syllables (from 76.92% to 78.64%), as well as (ii) determining the presence or absence of primary stress (from 86.99% to 89.80%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-528"
  },
  "li11i_interspeech": {
   "authors": [
    [
     "Ya",
     "Li"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Xiaoying",
     "Xu"
    ]
   ],
   "title": "Hierarchical stress modeling in Mandarin text-to-speech",
   "original": "i11_2013",
   "page_count": 4,
   "order": 531,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "Automatic stress prediction is helpful for both speech synthesis and natural speech understanding. This paper proposes a novel hierarchical Mandarin stress modeling method. The top level emphasizes stressed syllables, while the bottom level focuses on unstressed syllables for the first time due to its importance in both naturalness and expressiveness of synthetic speech. Maximum Entropy model is adopted to predict stress structure from textual features. Experiments show that the modeling method could capture the macro- and micro-characteristics of stress successfully. The F-score of two-level stress predictions are 73.3% and 78.7%, respectively, which are satisfactory compared to other prosody predictions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-529"
  },
  "ni11_interspeech": {
   "authors": [
    [
     "Chong-Jia",
     "Ni"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Automatic prosodic events detection by using syllable-based acoustic, lexical and syntactic features",
   "original": "i11_2017",
   "page_count": 4,
   "order": 532,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "Automatic prosodic events detection and annotation are important for both speech understanding and natural speech synthesis. In this paper, the complementary model method is proposed to detect prosodic events. This method discards the independent assumption between the acoustic features and the lexical and syntactic features, models not only the features of the current syllable but also the contextual features of the current syllable at the model level, and realizes the complementarities by taking the advantages of each model. The experiments on Boston University Radio News Corpus show that the complementary model can yield 91.40% pitch accent detection accuracy rate, 95.19% intonational phrase boundaries (IPB) detection accuracy rate and 93.96% break index detection accuracy rate. When compared with the previous work, the results for pitch accent, IPB and break index detection are significantly better.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-530"
  },
  "rilliard11_interspeech": {
   "authors": [
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Alexandre",
     "Allauzen"
    ],
    [
     "Philippe",
     "Boula de Mareüil"
    ]
   ],
   "title": "Using dynamic time warping to compute prosodic similarity measures",
   "original": "i11_2021",
   "page_count": 4,
   "order": 533,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "This paper presents the use of Dynamic Time Warping (DTW) for measuring prosodic differences between variable-sized sentences. This methodological study may apply to various prosodic functions, accented or expressive speech. Both the structuring and attitudinal functions of prosody are investigated here. We evaluated the relevance of three prosodic (dis)similarity measures to account for perceived variations. The importance of constraints on the DTW alignment process is highlighted, together with the possibility to use prosodic features beyond pitch. Results show the effectiveness of DTW-based measurements to capture different syntactic-prosodic structures and to cluster prosodically similar attitudinal expressions, irrespective of the utterance length.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-531"
  },
  "barbosa11_interspeech": {
   "authors": [
    [
     "Plínio A.",
     "Barbosa"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Sandra",
     "Madureira"
    ]
   ],
   "title": "Applying the quantitative target approximation model (qTA) to German and brazilian portuguese",
   "original": "i11_2025",
   "page_count": 4,
   "order": 534,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "This work is an attempt to explore a different prosodic domain for the quantitative target approximation model (qTA) model than the syllable. This is done by studying the model's ability to synthesise the melodic contours of two different languages, German and Brazilian Portuguese, in two distinct speaking styles, reading and storytelling. The connected utterances studied here present more complex material than hitherto studied using the qTA model. However, the modelling accuracy on these data is similar to that of the Fujisaki model. The results show that the word can be the domain for both prominence marking and phrase boundary type (terminal and non-terminal). By restricting the qTA parameter search space for the two mentioned functions, it is possible to develop an encoding scheme for them.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-532"
  },
  "obin11b_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Anne",
     "Lacheret"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Stylization and trajectory modelling of short and long term speech prosody variations",
   "original": "i11_2029",
   "page_count": 4,
   "order": 535,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "In this paper, a unified trajectory model based on the stylization and the modelling of f0 variations simultaneously over various temporal domains is proposed. The syllable is used as the minimal temporal domain for the description of speech prosody, and short-term and long-term f0 variations are stylized and modelled simultaneously over various temporal domains. During the training, a context-dependent model is estimated according to the joint stylized f0 contours over the syllable and a set of long-term temporal domains. During the synthesis, f0 variations are determined using the long-term variations as trajectory constraints. In a subjective evaluation in speech synthesis, the stylization and trajectory modelling of short and long term speech prosody variations is shown to consistently model speech prosody and to outperform the conventional short-term modelling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-533"
  },
  "avanzi11_interspeech": {
   "authors": [
    [
     "Mathieu",
     "Avanzi"
    ],
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Anne",
     "Lacheret-Dujour"
    ],
    [
     "Bernard",
     "Victorri"
    ]
   ],
   "title": "Toward a continuous modeling of French prosodic structure: using acoustic features to predict prominence location and prominence degree",
   "original": "i11_2033",
   "page_count": 4,
   "order": 536,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "The aim of this paper is to present a tool developed in order to generate French rhythmical structure semi-automatically, without taking grammatical cues into account. On the basis of a phonemic alignment, the software first locates prominent syllables by considering basic acoustic features such as F0, duration and silent pause. It then assigns a degree of prominence to each syllable identified. The estimation of this degree results from a computation of the values of silent pause, relative duration and height averages used for prominence detection in the first step. The second part of the article presents an experiment conducted in order to validate the algorithm's performances, by comparing the predictions of the software with a continuous manual coding carried out by four annotators on a 4-minute stretch of corpus (788 syllables) involving read aloud speech, map task and spontaneous dialogue. The performance of the algorithm is encouraging: a Fleiss' kappa calculation estimates the rate at 0.8, and a correlation agreement calculation at 91%, in the best cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-534"
  },
  "mahrt11_interspeech": {
   "authors": [
    [
     "Tim",
     "Mahrt"
    ],
    [
     "Jui-Ting",
     "Huang"
    ],
    [
     "Yoonsook",
     "Mo"
    ],
    [
     "Margaret",
     "Fleck"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "Optimal models of prosodic prominence using the Bayesian information criterion",
   "original": "i11_2037",
   "page_count": 4,
   "order": 537,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "This study investigated the relation between various acoustic features and prominence. Past research has suggested that duration, pitch, and intensity all play a role in the perception of prominence. In our past work, we found a correlation between these acoustic features and speaker agreement over the placement of prominence. The current study was motivated by a need to enrich our understanding of this correlation. Using the Bayesian information criterion, we show that the best model for a feature that cues prosody is not necessarily a single Gaussian. Rather, the best model depends on the feature. This finding has consequences for our understanding of the role of these features in the perception of prosody and for prosody recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-535"
  },
  "hussein11_interspeech": {
   "authors": [
    [
     "Hussein",
     "Hussein"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hue San",
     "Do"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Quantitative analysis of tone coarticulation in Mandarin",
   "original": "i11_2041",
   "page_count": 4,
   "order": 538,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "The current paper examines the effect of tone coarticulation in Mandarin on the amplitude and duration of tone commands of the Fujisaki model and whether declination needs to be taken into account when synthesizing F0 contours of Mandarin. Based on a corpus of short sentences mean parameters of the Fujisaki-model were calculated for the 15 combinations of Mandarin tones. The resulting smoothed F0 contours differ from the canonical shapes due to tonal coarticulation. Results of averaged parameters suggest that sequences of tone commands with the same polarity can usually be merged into a single tone command because their tone command amplitudes At are very similar. T2 for the first and T1 for the second command in these sequences are also very similar, so they can be set to the same value. As a consequence, tonal combinations can be interpreted as sequences of tone switches between high and low tones, considerably simplifying the modeling. It was also found that for most utterances phrase commands of magnitude Ap greater 0 occurred, indicating that the phrase component should be taken into account when analyzing and synthesizing of F0 contour of Mandarin.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-536"
  },
  "neiberg11c_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "G.",
     "Ananthakrishnan"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Tracking pitch contours using minimum jerk trajectories",
   "original": "i11_2045",
   "page_count": 4,
   "order": 539,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "This paper proposes a fundamental frequency tracker, with the specific purpose of comparing the automatic estimates with pitch contours that are sketched by trained phoneticians. The method uses a frequency domain approach to estimate pitch tracks that form minimum jerk trajectories. This method tries to mimic motor movements of the hand made while sketching. When the fundamental frequency tracked by the proposed method on the oral and laryngograph signals were compared using the MOCHA-TIMIT database, the correlation was 0.98 and the root mean squared error was 4.0 Hz, which was slightly better than a state-of-the-art pitch tracking algorithm included in the ESPS. We also demonstrate how the proposed algorithm could to be applied when comparing with sketches made by phoneticians for the variations in accent II among the Swedish dialects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-537"
  },
  "maza11_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Maza"
    ],
    [
     "Marc",
     "El-Beze"
    ],
    [
     "Georges",
     "Linares"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "On the use of linguistic features in an automatic system for speech analytics of telephone conversations",
   "original": "i11_2049",
   "page_count": 4,
   "order": 540,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "A research on the analysis of human/human conversations in a call centre is described. The purpose of the research is to provide short reports of each conversation with information useful for monitoring the call centre efficiency. Data from real users discussing over the telephone with agents are processed by an automatic speech recognition (ASR) system. Reports are grouped into classes by the agents based on predefined taxonomy. A train set of manually transcribed data is used for training the extraction of features relevant to the application and the classification of the conversations. The use of all the words of the application vocabulary, of automatically selected keywords, and of automatically learned sentence chunks containing semantic classes of words are compared and evaluated with a totally different test set. The results show a significant increase in performance when chunks are used even in comparison with the use of bags of words obtained with a boosting algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-538"
  },
  "kazemzadeh11_interspeech": {
   "authors": [
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Determining what questions to ask, with the help of spectral graph theory",
   "original": "i11_2053",
   "page_count": 4,
   "order": 541,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "This paper considers questions and the objects being asked about to be a graph and formulates the knowledge goal of a questionasking agent in terms of connecting this graph. The game of twenty questions can be thought of as a testbed of such a question-asking agent's knowledge. If the agent's knowledge of the domain were completely specified, the goal of question-asking would be to find the answer as quickly as possible and could follow a decision tree approach to narrow down the candidate answers. However, if the agent's knowledge is incomplete, it must have a secondary goal for the questions it plans: to complete its knowledge. We claim that this secondary goal of a question asking agent can be formulated in terms of spectral graph theory. In particular, disconnected portions of the graph must be connected in a principled way. We show how the eigenvalues of a graph Laplacian of the question-object adjacency graph can identify whether a set of knowledge contains disconnected components and the zero elements of the powers of the question-object adjacency graph provide a way to identify these questions. We illustrate the approach using an emotion description task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-539"
  },
  "buschmeier11_interspeech": {
   "authors": [
    [
     "Hendrik",
     "Buschmeier"
    ],
    [
     "Zofia",
     "Malisz"
    ],
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Stefan",
     "Kopp"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "`are you sure you're paying attention?' - `uh-huh' communicating understanding as a marker of attentiveness",
   "original": "i11_2057",
   "page_count": 4,
   "order": 542,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "We report on the first results of an experiment designed to investigate properties of communicative feedback produced by non-attentive listeners in dialogue. Listeners were found to produce less feedback when distracted by an ancillary task. A decreased number of feedback expressions communicating understanding was a particularly reliable indicator of distractedness. We argue this finding could be used to facilitate recognition of attentional states in dialogue system users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-540"
  },
  "ishimoto11_interspeech": {
   "authors": [
    [
     "Yuichi",
     "Ishimoto"
    ],
    [
     "Mika",
     "Enomoto"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "Projectability of transition-relevance places using prosodic features in Japanese spontaneous conversation",
   "original": "i11_2061",
   "page_count": 4,
   "order": 543,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "In this paper, to clarify acoustic features for predicting the ends of utterances, we investigated prosodic features that project transition relevance places in Japanese spontaneous conversation. Acoustic parameters used as the prosodic features are the fundamental frequency, power, and mora duration of accentual phrases and words. Results showed that the fundamental frequency and power at the beginning of the final accentual phrase indicate whether the utterance includes utterance-final elements, which are the syntactic cue for detecting the end-of-utterance. In addition, the mora duration lengthened in the final accentual phrase. That is, these prosodic features around the beginning of the final accentual phrase showed the characteristic changes that make hearers predict the transition relevance places.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-541"
  },
  "hjalmarsson11_interspeech": {
   "authors": [
    [
     "Anna",
     "Hjalmarsson"
    ],
    [
     "Kornel",
     "Laskowski"
    ]
   ],
   "title": "Measuring final lengthening for speaker-change prediction",
   "original": "i11_2065",
   "page_count": 4,
   "order": 544,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "We explore pre-silence syllabic lengthening as a cue for nextspeakership prediction in spontaneous dialogue. When estimated using a transcription-mediated procedure, lengthening is shown to reduce error rates by 25% relative to majority class guessing. Lengthening should therefore be exploited by dialogue systems. With that in mind, we evaluate an automatic measure of spectral envelope change, Mel-spectral flux (MSF), and show that its speaker-independent performance is at least as good as that of the transcription-mediated measure. Modeling MSF is likely to improve turn uptake in dialogue systems, and to benefit other applications needing an estimate of durational variability in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-542"
  },
  "laskowski11b_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "Incremental learning and forgetting in stochastic turn-taking models",
   "original": "i11_2069",
   "page_count": 4,
   "order": 545,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "feature is the capacity for incremental learning and forgetting. To showcase its flexibility, we design experiments answering four concrete questions about the systematics of spoken interaction. The results show that: (1) individuals are clearly affected by one another; (2) there is individual variation in interaction strategy; (3) strategies wander in time rather than converge; and (4) individuals exhibit similarity with their interlocutors. We expect the proposed framework to be capable of answering many such questions with little additional effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-543"
  },
  "georgila11_interspeech": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Reinforcement learning of argumentation dialogue Policies in negotiation",
   "original": "i11_2073",
   "page_count": 4,
   "order": 546,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "We build dialogue system policies for negotiation, and in particular for argumentation. These dialogue policies are designed for negotiation against users of different cultural norms (individualists, collectivists, and altruists). In order to learn these policies we build simulated users (SUs), i.e. models that simulate the behavior of real users, and use Reinforcement Learning (RL). The SUs are trained on a spoken dialogue corpus in a negotiation domain, and then tweaked towards a particular cultural norm using hand-crafted rules. We evaluate the learned policies in a simulation setting. Our results are consistent with our SUs, in other words, the policies learn what they are designed to learn, which shows that RL is a promising technique for learning policies in domains, such as argumentation, that are more complex than standard slot-filling applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-544"
  },
  "heinroth11_interspeech": {
   "authors": [
    [
     "Tobias",
     "Heinroth"
    ],
    [
     "Savina",
     "Koleva"
    ],
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Topic switching strategies for spoken dialogue systems",
   "original": "i11_2077",
   "page_count": 4,
   "order": 547,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "One of the most important challenges researchers are facing within the field of Spoken Dialogue Systems is that life is neither domain dependent nor driven by a single task. In the recent past various methods to handle multiple tasks or topics in parallel within spoken human-human and human-computer dialogues have been investigated. In this paper we compare several task switching approaches such as discourse markers and task recovery methods. The aim of our study is to reveal which strategies users prefer regarding metrics such as efficiency, friendliness, and reliability. Furthermore we investigate how the different strategies influence the cognitive capacity of the subjects. The dialogues used for the study have been implemented utilising the OwlSpeak Spoken Dialogue Manager, which applies ontologies as dialogue models that can be dynamically combined during runtime.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-545"
  },
  "higashinaka11_interspeech": {
   "authors": [
    [
     "Ryuichiro",
     "Higashinaka"
    ],
    [
     "Noriaki",
     "Kawamae"
    ],
    [
     "Kugatsu",
     "Sadamitsu"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Toyomi",
     "Meguro"
    ],
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Hirohito",
     "Inagaki"
    ]
   ],
   "title": "Unsupervised clustering of utterances using non-parametric Bayesian methods",
   "original": "i11_2081",
   "page_count": 4,
   "order": 548,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "Unsupervised clustering of utterances can be useful for the modeling of dialogue acts for dialogue applications. Previously, the Chinese restaurant process (CRP), a non-parametric Bayesian method, has been introduced and has shown promising results for the clustering of utterances in dialogue. This paper newly introduces the infinite HMM, which is also a nonparametric Bayesian method, and verifies its effectiveness. Experimental results in two dialogue domains show that the infinite HMM, which takes into account the sequence of utterances in its clustering process, significantly outperforms the CRP. Although the infinite HMM outperformed other methods, we also found that clustering complex dialogue data, such as human-human conversations, is still hard when compared to human-machine dialogues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-546"
  },
  "parada11_interspeech": {
   "authors": [
    [
     "Carolina",
     "Parada"
    ],
    [
     "Mark",
     "Dredze"
    ],
    [
     "Frederick",
     "Jelinek"
    ]
   ],
   "title": "OOV sensitive named-entity recognition in speech",
   "original": "i11_2085",
   "page_count": 4,
   "order": 549,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "Named Entity Recognition (NER), an information extraction task, is typically applied to spoken documents by cascading a large vocabulary continuous speech recognizer (LVCSR) and a named entity tagger. Recognizing named entities in automatically decoded speech is difficult since LVCSR errors can confuse the tagger. This is especially true of out-of-vocabulary (OOV) words, which are often named entities and always produce transcription errors. In this work, we improve speech NER by including features indicative of OOVs based on a OOV detector, allowing for the identification of regions of speech containing named entities, even if they are incorrectly transcribed. We construct a new speech NER data set and demonstrate significant improvements for this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-547"
  },
  "saers11_interspeech": {
   "authors": [
    [
     "Markus",
     "Saers"
    ],
    [
     "Dekai",
     "Wu"
    ],
    [
     "Chi-kiu",
     "Lo"
    ],
    [
     "Karteek",
     "Addanki"
    ]
   ],
   "title": "Speech translation with grammar driven probabilistic phrasal bilexica extraction",
   "original": "i11_2089",
   "page_count": 4,
   "order": 550,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "We introduce a new type of transduction grammar that allows for learning of probabilistic phrasal bilexica, leading to a significant improvement in spoken language translation accuracy. The current state-of-the-art in statistical machine translation relies on a complicated and crude pipeline to learn probabilistic phrasal bilexica . the very core of any speech translation system. In this paper, we present a more principled approach to learning probabilistic phrasal bilexica, based on stochastic transduction grammar learning applicable to speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-548"
  },
  "tillmann11_interspeech": {
   "authors": [
    [
     "Christoph",
     "Tillmann"
    ],
    [
     "Sanjika",
     "Hewavitharana"
    ]
   ],
   "title": "An efficient unified extraction algorithm for bilingual data",
   "original": "i11_2093",
   "page_count": 4,
   "order": 551,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "The paper presents a unified algorithm for aligning sentences with their translations in bilingual data. The sentence alignment problem is handled as a large-scale pattern recognition problem similar to the task of finding the word sequence that corresponds to an acoustic input signal in isolated word automatic speech recognition (ASR). The algorithm gains efficiency from related work on dynamic programming (DP) search for speech recognition ([1]): a stack-based search is parametrized in a novel way, such that the unified algorithm can be used on various types of data that have been previously handled by separate implementations: the extracted text chunk pairs can be either sub-sentential pairs, one-to-one, or many-to-many sentence-level pairs. The one-stage search algorithm is carried out in a single run over the data. With the help of a unified beam-search candidate pruning, the algorithm is very efficient: it avoids any document-level pre-filtering and uses less restrictive sentence-level filtering. Results are presented on a Russian-English and a Spanish-English extraction task. Based on a simple word-based scoring model, text chunk pairs are extracted out of several trillion candidates.\n",
    "",
    "",
    "H. Ney, The Use of a One-stage Dynamic Programming Algorithm for Connected Word Recognition, IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 263271, 1984.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-549"
  },
  "huang11g_interspeech": {
   "authors": [
    [
     "Songfang",
     "Huang"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Using features from topic models to alleviate over-generation in hierarchical phrase-based translation",
   "original": "i11_2097",
   "page_count": 4,
   "order": 552,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "In hierarchical phrase-based translation systems, the grammars (SCFG rules) have over-generation problem because we can replace the non-terminal X with almost everything without knowing the syntactic or semantic role of X. In this paper, we present an approach that uses topic models to learn the distributions for non-terminals in each SCFG rule, based on which we further derive static features for the discriminative framework of statistical machine translation. Experimental results on three corpora show that we can obtain some gains in BLEU by using these features derived from topic models to alleviate the over-generation problem in hierarchical phrase-based translation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-550"
  },
  "huang11h_interspeech": {
   "authors": [
    [
     "Songfang",
     "Huang"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "An empirical study on improving hierarchical phrase-based translation using alignment features",
   "original": "i11_2101",
   "page_count": 4,
   "order": 553,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "In this paper, we empirically investigate three new features from word alignments to improve speech-to-speech translation on mobile devices for low-resource languages. The three features include one feature about alignment for boundary words of the target side phrase, one about the balance of terminal words between the source and the target side, and another about the number of unaligned words. We carry out experiments on both directions (E2F and F2E) for Pashto and Dari, two official languages of Afghanistan. By using the proposed alignment features, we can obtain improvements (up to 1% BLEU score) on the test sets for both Pashto and Dari.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-551"
  },
  "he11_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "He"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Robust speech translation by domain adaptation",
   "original": "i11_2105",
   "page_count": 4,
   "order": 554,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "Speech translation tasks usually are different from text-based machine translation tasks, and the training data for speech translation tasks are usually very limited. Therefore, domain adaptation is crucial to achieve robust performance across different conditions in speech translation. In this paper, we study the problem of adapting a general-domain, writing-text-style machine translation system to a travel-domain, speech translation task. We study a variety of domain adaptation techniques, including data selection and incorporation of multiple translation models, in a unified decoding process. The experimental results demonstrate significant BLEU score improvement on the targeting scenario after domain adaptation. The results also demonstrate robust translation performance achieved across multiple conditions via joint data selection and model combination. We finally analyze and compare the robust techniques developed for speech recognition and speech translation, and point out further directions for robust translation via variability-adaptive and discriminatively-adaptive learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-552"
  },
  "ettelaie11_interspeech": {
   "authors": [
    [
     "Emil",
     "Ettelaie"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Enhancements to the training process of classifier-based speech translator via topic modeling",
   "original": "i11_2109",
   "page_count": 4,
   "order": 555,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "Classification of sentences based on their meaning (or concept) has been used as component in speech translation and spoken language understanding systems. Preparing training data for this type of classifiers is often a tedious task. In our previous work, we presented a method of clustering sentences as a step toward automated annotation of concepts. To measure the distance between two sentences, that method relied on the local lexical dependencies in their translations. In this work, we apply Topic Modeling to enhance the previously proposed distance metric so that it includes information from semantic associations among the words. Our experiments on the DARPA USC Transonics and BBN Transtac data sets show the advantage of incorporating this information as performance improvements in a set of clustering tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-553"
  },
  "sridhar11b_interspeech": {
   "authors": [
    [
     "Vivek Kumar Rangarajan",
     "Sridhar"
    ],
    [
     "Luciano",
     "Barbosa"
    ],
    [
     "Srinivas",
     "Bangalore"
    ]
   ],
   "title": "A scalable approach to building a parallel corpus from the web",
   "original": "i11_2113",
   "page_count": 4,
   "order": 556,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "Parallel text acquisition from the Web is an attractive way for augmenting statistical models (e.g., machine translation, cross-lingual document retrieval) with domain representative data. The basis for obtaining such data is a collection of pairs of bilingual Web sites or pages. In this work, we propose a crawling strategy that locates bilingual Web sites by constraining the visitation policy of the crawler to the graph neighborhood of bilingual sites on the Web. Subsequently, we use a novel recursive mining technique that recursively extracts text and links from the collection of bilingual Web sites obtained from the crawling. Our method does not suffer from the computationally prohibitive combinatorial matching typically used in previous work that uses document retrieval techniques to match a collection of bilingual webpages. We demonstrate the efficacy of our approach in the context of machine translation in the tourism and hospitality domain. The parallel text obtained using our novel crawling strategy results in a relative improvement of 21% in BLEU score (English-to-Spanish) over an out-of-domain seed translation model trained on the European parliamentary proceedings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-554"
  },
  "itoh11_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Kohei",
     "Iwata"
    ],
    [
     "Masaaki",
     "Ishigame"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ]
   ],
   "title": "Spoken term detection results using plural subword models by estimating detection performance for each query",
   "original": "i11_2117",
   "page_count": 4,
   "order": 557,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "The present paper proposes a new integration method of plural spoken term detection (STD) results obtained from plural subword models that we previously proposed. We confirmed that these new subword models, which are the 1/2 phone model, the 1/3 phone model, and the sub-phonetic segment (SPS) model, are effective for STD systems, which must be vocabulary-free in order to process arbitrary query words. In addition, these models are more sophisticated on the time axis than conventional phone models, such as the triphone model. In the present study, we utilize the results of the subword models explicitly when integrating the plural results. For this purpose, we introduce an STD performance index that expresses the degree of detection difficulty for each query word. The index is approximated by the recognition accuracy of the query subword sequence. We demonstrate improved performance through experiments using an actual presentation speech corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-555"
  },
  "barbosa11b_interspeech": {
   "authors": [
    [
     "Luciano",
     "Barbosa"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Amanda",
     "Stent"
    ]
   ],
   "title": "Speechforms: from web to speech and back",
   "original": "i11_2121",
   "page_count": 4,
   "order": 558,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "This paper describes SpeechForms, a system that uses novel techniques to automatically identify form element semantics and form element content, and to semi-automatically generate language models that allow users to fill out each web form element by voice. Preliminary experimental results show that simple per-element language models are faster and may be more accurate than statistical n-gram language models trained on large amounts of web text data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-556"
  },
  "noritake11_interspeech": {
   "authors": [
    [
     "Kazuyuki",
     "Noritake"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Takehiko",
     "Yoshimi"
    ]
   ],
   "title": "Image processing filters for line detection-based spoken term detection",
   "original": "i11_2125",
   "page_count": 4,
   "order": 559,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "Spoken term detection (STD) from oral presentations is addressed. Specifically, we regard STD as a line detection problem in an image file, in which each pixel holds a syllable-distance between query term and automatic speech recognition (ASR) results. Since such kind of image file essentially includes ASR errors, line detection in noisy image should be investigated. In this paper, we propose line detection-oriented image processing filters for STD. We achieved 0.39 of F-measure for low frequency term (out of vocabulary term in ASR system) detection task, and 0.69 of F-measure for known term (in-vocabulary term in ASR system) detection task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-557"
  },
  "polifroni11_interspeech": {
   "authors": [
    [
     "Joe",
     "Polifroni"
    ],
    [
     "François",
     "Mairesse"
    ]
   ],
   "title": "Using latent topic features for named entity extraction in search queries",
   "original": "i11_2129",
   "page_count": 4,
   "order": 560,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "Search is one of the most quickly growing applications in the mobile market. As people rely more on portable devices for performing search, it becomes increasingly important to analyze user queries in order to achieve more targetted results over a broad set of search entities. While most previous work has relied on lexico-syntactic features and handcrafted knowledge sources, this paper investigates methods for learning latent semantic features from unlabelled user-generated content. We extract word-topic associations by training a Latent Dirichlet Allocation model on a corpus of online reviews, and show that this information improves named-entity classification performance over broad domain search queries. We believe that topical features provide a rich source of information from data with minimal manual effort, and no dependency on a specific language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-558"
  },
  "masumura11b_interspeech": {
   "authors": [
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Seongjun",
     "Hahm"
    ],
    [
     "Akinori",
     "Ito"
    ]
   ],
   "title": "Language model expansion using webdata for spoken document retrieval",
   "original": "i11_2133",
   "page_count": 4,
   "order": 561,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "In recent years, there has been increasing demand for ad hoc retrieval of spoken documents. We can use existing text retrieval methods by transcribing spoken documents into text data using a Large Vocabulary Continuous Speech Recognizer (LVCSR). However, retrieval performance is severely deteriorated by recognition errors and out-of-vocabulary (OOV) words. To solve these problems, we previously proposed an expansion method that compensates the transcription by using text data downloaded from the Web. In this paper, we introduce two improvements to the existing document expansion framework. First, we use a large-scale sample database of webdata as the source of relevant documents, thus avoiding the bias introduced by choosing keywords in the existing methods. Next, we use a document retrieval method based on a statistical language model (SLM), which is a popular framework in information retrieval, and also propose a new smoothing method considering recognition errors and missing keywords. Retrieval experiments show that the proposed methods yield a good results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-559"
  },
  "akiba11_interspeech": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Koichiro",
     "Honda"
    ]
   ],
   "title": "Effects of query expansion for spoken document passage retrieval",
   "original": "i11_2137",
   "page_count": 4,
   "order": 562,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "One of the major challenges for spoken document retrieval is how to handle speech recognition errors within the target documents. Query expansion is promising for this challenge. In this paper, we apply relevance models, a type of query expansion method, for the spoken document passage retrieval task. We adapted the original relevance model for passage retrieval. We also extended it to benefit from massive collections of Web documents for query expansion. Through our experimental evaluation, we found that our relevance model successfully improved the retrieval performance. We also found that using Web documents was effective when the transcription of the target documents had a high word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-560"
  },
  "chan11_interspeech": {
   "authors": [
    [
     "Chun-an",
     "Chan"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Unsupervised hidden Markov modeling of spoken queries for spoken term detection without speech recognition",
   "original": "i11_2141",
   "page_count": 4,
   "order": 563,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "We propose an unsupervised technique to model the spoken query using hidden Markov model (HMM) for spoken term detection without speech recognition. By unsupervised segmentation, clustering and training, a set of HMMs, referred to as acoustic segment HMMs (ASHMMs), is generated from the spoken archive to model the signal variations and frame trajectories. An unsupervised technique is also designed for ASHMMs parameter training. A model-based approach for spoken term detection is then developed by constructing a query HMM from the ASHMMs, and then scoring the spoken documents using the query HMM. Experiments show that this model-based approach complements the feature-based dynamic time warping approach. A significant improvement on detection performance is achieved by integrating the two methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-561"
  },
  "gemello11_interspeech": {
   "authors": [
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ],
    [
     "Pier Domenico",
     "Batzu"
    ]
   ],
   "title": "Topic identification from audio recordings using rich recognition results and neural network based classifiers",
   "original": "i11_2145",
   "page_count": 4,
   "order": 564,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "This paper investigates the use of a Neural Network classifier for topic identification from conversational telephone speech, which exploits rich recognition results coming from an automatic speech recognizer. The baseline features used to feed the neural classifier are produced using the words extracted from the 1-best sequence. Rich recognition results include the word union of the first n-best sequences, the consensus hypothesis and the full or pruned Word Confusion Network generated from the n-best sequences. Different probabilistic information attached to the words, including confidence and word posterior probabilities, is investigated together with classical and probabilistic feature weighting schemes. A large experimentation on conversational telephone speech of Fisher corpus is reported, showing significant improvements when compared to the state of the art.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-562"
  },
  "parlikar11_interspeech": {
   "authors": [
    [
     "Alok",
     "Parlikar"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "A grammar based approach to style specific phrase prediction",
   "original": "i11_2149",
   "page_count": 4,
   "order": 565,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "We present an approach to style specific phrasing for Text-to- Speech (TTS) systems. We formulate the problem of phrase break prediction (or phrasing) as generation of a sequence of breaks (B) and non-breaks (NB) after each word in a sentence. We use prosodic breaks in speech data to build shallow parses over corresponding text. We then learn a grammar that can predict these shallow prosodic parses from text. We then combine this prosodic phrasing information with other word level features in a CART tree to predict where phrase breaks should be inserted in new text. We show that a model built to target a specific reading style can predict phrase breaks more accurately than the standard generic model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-563"
  },
  "watts11_interspeech": {
   "authors": [
    [
     "Oliver",
     "Watts"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Unsupervised features from text for speech synthesis in a speech-to-speech translation system",
   "original": "i11_2153",
   "page_count": 4,
   "order": 566,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "We explore the use of linguistic features for text to speech (TTS) conversion in the context of a speech-to-speech translation system that can be extracted from unannotated text in an unsupervised, language-independent fashion. The features are intended to act as surrogates for conventional part of speech (POS) features. Unlike POS features, the experimental features assume only the availability of tools and data that must already be in place for the construction of other components of the translation system, and can therefore be used for the TTS module without incurring additional TTS-specific costs. We here describe the use of the experimental features in a speech synthesiser, using six different configurations of the system to allow the comparison of the proposed features with conventional, knowledge-based POS features. We present results of objective and subjective evaluations of the usefulness of the new features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-564"
  },
  "watts11b_interspeech": {
   "authors": [
    [
     "Oliver",
     "Watts"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Unsupervised continuous-valued word features for phrase-break prediction without a part-of-speech tagger",
   "original": "i11_2157",
   "page_count": 4,
   "order": 567,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "Part of speech (POS) tags are foremost among the features conventionally used to predict intonational phrase-breaks for text to speech (TTS) conversion. The construction of such systems therefore presupposes the availability of a POS tagger for the relevant language, or of a corpus manually tagged with POS. However, such tools and resources are not available in the majority of the world's languages, and manually labelling text with POS tags is an expensive and time-consuming process. We therefore propose the use of continuous-valued features that summarise the distributional characteristics of word types as surrogates for POS features. Importantly, such features are obtained in an unsupervised manner from an untagged text corpus. We present results on the phrase-break prediction task, where use of the features closes the gap in performance between a baseline system (using only basic punctuation-related features) and a topline system (incorporating a state-of-the-art POS tagger).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-565"
  },
  "campillo11_interspeech": {
   "authors": [
    [
     "Francisco",
     "Campillo"
    ],
    [
     "Francisco",
     "Méndez"
    ],
    [
     "Montserrat",
     "Arza"
    ],
    [
     "Laura",
     "Docío"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Iñaki",
     "Sainz"
    ]
   ],
   "title": "Albayzín 2010: a Spanish text to speech evaluation",
   "original": "i11_2161",
   "page_count": 4,
   "order": 568,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "Albayzin 2010 Text-to-Speech Evaluation Campaign was the second biannual Albayzin Campaign. A Spanish corpus was provided by the Group of Multimedia Technologies of the University of Vigo, and six teams developed a total of ten systems for the evaluation. A set of test sentences was released to be synthesized, and an on-line evaluation was conducted, focusing on naturalness, similarity to the original voice, and intelligibility. In this paper the evaluation details and results are described.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-566"
  },
  "shen11_interspeech": {
   "authors": [
    [
     "Binbin",
     "Shen"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Yongxin",
     "Wang"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Combining active and semi-supervised learning for homograph disambiguation in Mandarin text-to-speech synthesis",
   "original": "i11_2165",
   "page_count": 4,
   "order": 569,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "Grapheme-to-phoneme conversion (G2P) is a crucial step for Mandarin text-to-speech (TTS) synthesis, where homograph disambiguation is the core issue. Several machine learning algorithms have been proposed to solve the issue by building models from well annotated training corpus. However, the preparation of such well annotated corpus is very laboring and time-consuming which requires lots of manual hand-label work to validate the proper pronunciations of the homographs. This work tries to cover this problem by introducing the active learning (AL) and semi-supervised learning (SSL) algorithms for the homograph disambiguation task using unlabeled data. Experiments show that the proposed framework can greatly reduce the cost of manual hand-label work while preserving the performance of the trained model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-567"
  },
  "ewender11_interspeech": {
   "authors": [
    [
     "Thomas",
     "Ewender"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Automatically creating a diphone set from a speech database",
   "original": "i11_2169",
   "page_count": 4,
   "order": 570,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "This paper presents a measure that scores various aspects of phone quality. The measure is designed to penalize phone instances with one or several characteristics that are not desirable in concatenation-based speech synthesis. Depending on the phone type, these aspects amongst others include spectrum, phase, fundamental frequency, duration, voicing and plosive quality. We applied this quality measure to select diphone sets from four different speech databases and demonstrate the quality of these diphone sets by means of synthesis examples. The quality of these examples showed that the proposed measure can be applied to select a high-quality diphone set from a speech database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-568"
  },
  "mattheyses11_interspeech": {
   "authors": [
    [
     "Wesley",
     "Mattheyses"
    ],
    [
     "Lukas",
     "Latacz"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Automatic viseme clustering for audiovisual speech synthesis",
   "original": "i11_2173",
   "page_count": 4,
   "order": 571,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "A common approach in visual speech synthesis is the use of visemes as atomic units of speech. In this paper, phoneme-based and viseme-based audiovisual speech synthesis techniques are compared in order to explore the balancing between data availability and an improved audiovisual coherence for synthesis optimization. A technique for automatic viseme clustering is described and it is compared to the standardized viseme set described in MPEG-4. Both objective and subjective testing indicated that a phoneme-based approach leads to better synthesis results. In addition, the test results improve when more different visemes are defined. This raises some questions on the widely applied viseme-based approach. It appears that a many-to-one phonemeto- viseme mapping is not capable of describing all subtle details of the visual speech information. In addition, with viseme-based synthesis the perceived synthesis quality is affected by the loss of audiovisual coherence in the synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-569"
  },
  "hinterleitner11_interspeech": {
   "authors": [
    [
     "Florian",
     "Hinterleitner"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Christoph",
     "Norrenbrock"
    ],
    [
     "Ulrich",
     "Heute"
    ]
   ],
   "title": "Perceptual quality dimensions of text-to-speech systems",
   "original": "i11_2177",
   "page_count": 4,
   "order": 572,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "The aim of this paper is to analyze the perceptual quality dimensions of state-of-the-art text-to-speech systems (TTS). Therefore, several pretests were conducted to determine a suitable set of attribute scales. The resulting 16 scales were used in a semantic differential on a diverse database containing 16 different TTS systems. A subsequent multidimensional analysis (Principal Axis Factor analysis with Promax rotation) resulted in three underlying quality dimensions. They were labeled naturalness, disturbances, and temporal distortions. A mapping of these factors onto the perceived overall quality revealed that naturalness contributes the most to the quality of TTS signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-570"
  },
  "mori11_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Graham",
     "Neubig"
    ]
   ],
   "title": "A pointwise approach to pronunciation estimation for a TTS front-end",
   "original": "i11_2181",
   "page_count": 4,
   "order": 573,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "In this paper, we propose a pointwise approach to the Japanese TTS front-end. In this approach, phoneme sequence estimation of sentences is decomposed into two tasks: word segmentation of the input sentence and phoneme estimation of each word. Then these two tasks are solved by pointwise classifiers without referring to the neighboring classification results. In contrast to existing sequence-based methods, an n-gram model based on sequences of word-phoneme pairs for example, this framework enables us to use various language resources such as sentences in which only a few words are annotated, or an unsegmented list of compound words, among others. In the experiments, we compared a joint tri-gram model with the combination of a pointwise word segmenter and a pointwise phoneme sequence estimator. The results showed that our framework successfully enables a TTS front-end to refer to a partially annotated corpus and/or a word sequence list annotated with phoneme sequences to realize a far larger improvement in accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-571"
  },
  "abouzleikha11_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Abou-Zleikha"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Correlating text with prosody",
   "original": "i11_2185",
   "page_count": 4,
   "order": 574,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "The prediction of prosody from text information has long been recognised as a requirement for natural sounding speech synthesis. While an examination of the relationship between text information and prosody typically focuses on the role of accent, duration and phrasing both from a statistical and rule-based perspective, this paper investigates the correlation between the similarities calculated with respect to text information and those calculated with respect to prosody from an exemplar-based perspective. Two text features are examined, the syntactic tree and the dependency tree, along with two prosody features, pitch and intensity. The work in this paper investigates 1) the correlation between text information and prosody information 2) the conditional membership probability between text information and prosodic information, and 3) the effect of the number of exemplars on the conditional membership probability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-572"
  },
  "rosenberg11c_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Raul",
     "Fernandez"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "“what is… dengue fever?” - modeling and predicting pronunciation errors in a text-to-speech system",
   "original": "i11_2189",
   "page_count": 4,
   "order": 575,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "We propose a system to predict baseform-generation errors in a text-to-speech (TTS) front-end, and aid in the process of customizing the synthesis engine to a novel application with a large, open-ended vocabulary. We motivate the use of the system by using data collected during the deployment of the IBM TTS engine in the Watson Deep Question-Answering system customized to play a game of Jeopardy!. We propose a set of features derived from a lexeme's orthography and candidate baseform, and use a variety of learning schemes and data sampling algorithms to address the issue of skewed class priors in the training data. We show that 1) these different approaches provide complementary information that can then be exploited by fusion schemes to improve on the baseline performances, and 2) it is possible to use these techniques to retrieve a list of likely incorrect lexemes so as to reduce the number of tokens that must be vetted before finding and fixing an error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-573"
  },
  "norrenbrock11_interspeech": {
   "authors": [
    [
     "Christoph",
     "Norrenbrock"
    ],
    [
     "Ulrich",
     "Heute"
    ],
    [
     "Florian",
     "Hinterleitner"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Aperiodicity analysis for quality estimation of text-to-speech signals",
   "original": "i11_2193",
   "page_count": 4,
   "order": 576,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "This contribution presents a new approach towards nonintrusive quality assessment of text-to-speech (TTS) signals. Perturbation measures which capture the degree of excitation-specific aperiodicity in voiced speech are investigated concerning their quality implications in synthesized speech. Based on two independent TTS databases for which formal attribute-based listening tests have been conducted, we show that perturbation measures are sensitive to quality aspects of prosody and voice characteristic. Furthermore a dominant dependency on TTS type, namely non-uniform unitselection and diphone synthesis, is identified. Yet, considerable differences between male and female TTS samples are recognized, emphasizing the need for gender-specific quality assessment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-574"
  },
  "klintfors11_interspeech": {
   "authors": [
    [
     "Eeva",
     "Klintfors"
    ],
    [
     "Ellen",
     "Marklund"
    ],
    [
     "Francisco",
     "Lacerda"
    ]
   ],
   "title": "Parallels in infants' attention to speech articulation and to physical changes in speech-unrelated objects",
   "original": "i11_2197",
   "page_count": 4,
   "order": 577,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "The mechanisms of how children develop the capacity to make use of speech articulation cues to support interpretation of the speech signal are not exhaustively explored. The purpose of this study is to investigate if there are parallels in infants' way of attending to speech articulation and their perception of physical changes in speech-unrelated objects. The current research questions grew out from an earlier study in which it was found that perception of speech in infants was based on a match between auditory and visual prominence . as opposed to a match between sound and to it corresponding face. Data suggested that speech perception in infancy may function as described by Stevens power law, and two methodological supplements to test the validness of this hypothesis were made in the current study. First, a non-articulatory test condition was added to investigate infants' perception of speech-unrelated objects. Second, amplitude manipulated stimuli were added to introduce systematic changes in loudness. Results confirmed our hypothesis; the visually prominent articulations were favored, and the same pattern was found in response to non-speech related objects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-575"
  },
  "duran11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Duran"
    ],
    [
     "Jagoda",
     "Bruni"
    ],
    [
     "Grzegorz",
     "Dogil"
    ],
    [
     "Hinrich",
     "Schütze"
    ]
   ],
   "title": "Speech events are recoverable from unlabeled articulatory data: using an unsupervised clustering approach on data obtained from electromagnetic midsaggital articulography (EMA)",
   "original": "i11_2201",
   "page_count": 4,
   "order": 578,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "Some models of speech perception/production and language acquisition make use of a quasi-continuous representation of the acoustic speech signal. We investigate whether such models could potentially profit from incorporating articulatory information in an analogous fashion. In particular, we investigate how articulatory information represented by EMA measurements can influence unsupervised phonetic speech categorization. By incorporation of the acoustic signal and non-synthetic, raw articulatory data, we present first results of a clustering procedure, which is similarly applied in numerous language acquisition and speech perception models. It is observed that non-labeled articulatory data, i.e. without previously assumed landmarks, perform fine clustering results. A more effective clustering outcome for plosives than for vowels seems to support the motor view of speech perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-576"
  },
  "strombergsson11_interspeech": {
   "authors": [
    [
     "Sofia",
     "Strömbergsson"
    ]
   ],
   "title": "Children's recognition of their own voice: influence of phonological impairment",
   "original": "i11_2205",
   "page_count": 4,
   "order": 579,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "This study explores the ability to identify the recorded voice as one's own, in three groups of children: one group of children with phonological impairment (PI) and two groups of children with typical speech and language development; 4.5 year-olds and 7.8 year-olds. High average performance rates in all three groups suggest that these children indeed recognize their recorded voice as their own, with no significant difference between the groups. Signs indicating that children with deviant speech use their speech deviance as a cue to identifying their own voice are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-577"
  },
  "kagomiya11_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Kagomiya"
    ],
    [
     "Seiji",
     "Nakagawa"
    ]
   ],
   "title": "Evaluation of bone-conducted ultrasonic hearing-aid regarding transmission of speaker discrimination information",
   "original": "i11_2209",
   "page_count": 4,
   "order": 580,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "Human listeners can perceive speech signals in a voice-modulated ultrasonic carrier from a bone-conduction stimulator, even if the listeners are patients with sensorineural hearing loss. Considering this fact, we have been developing a bone-conducted ultrasonic hearing aid (BCUHA). The purpose of this study is to evaluate the usability of BCUHA regarding transmission of speaker discrimination information. For this purpose, a prototype of speaker discrimination test was developed. The test consists of 120 pairs of 10 words spoken by 10 speakers, and examinee is requested to judge the speakers of each pair are \"same\" or \"different\". The usability of BCUHA was assessed by using the speaker discrimination test. The test was also conduced to air-conduction (AC) and cochlear implant simulator (CIsim) condition. The results show that BCUHA can transmit speaker information speaker as well as CIsim.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-578"
  },
  "herff11_interspeech": {
   "authors": [
    [
     "Christian",
     "Herff"
    ],
    [
     "Matthias",
     "Janke"
    ],
    [
     "Michael",
     "Wand"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Impact of different feedback mechanisms in EMG-based speech recognition",
   "original": "i11_2213",
   "page_count": 4,
   "order": 581,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "This paper reports on our recent research in the feedback effects of Silent Speech. Our technology is based on surface electromyography (EMG) which captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. While recognition results are good for loudly articulated speech and when experienced users speak silently, novice users usually achieve far worse results when speaking silently. Since there is no acoustic feedback when speaking silently, we investigate different kinds of feedback modes: no additional feedback except the natural somatosensory feedback (like the touching of the lips), visual feedback using a mirror and indirect acoustic feedback by speaking simultaneously to a previously recorded audio signal. In addition we examine recorded EMG data when the subject speaks audibly and silently in a loud environment to see if the Lombard effect can be observed in Silent Speech, too.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-579"
  },
  "yip11_interspeech": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "Phonotactic constraints and the segmentation of Cantonese speech",
   "original": "i11_2217",
   "page_count": 4,
   "order": 582,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "Two word-spotting experiments were conducted to examine the question of whether native Cantonese listeners are constrained by phonotactic information in the segmentation of Cantonese continuous speech. Because there are no legal consonant clusters occurred within individual Cantonese words, so this kind of phonotactic information of words may most likely cue native Cantonese listeners the locations of possible word boundaries in the continuous speech. Finally, the observed results from the two experiments confirmed this prediction. Together with other relevant studies [1,2], we argue that phonotactic constraint is one of the useful sources of information in segmenting Cantonese continuous speech.\n",
    "s Yip, M. C. W. (2000). Recognition of spoken words in continuous speech: Effects of transitional probability. Proceedings of the ICSLP'2000, 758-761. Yip, M. C. W. (2006). The role of positional probability in the segmentation of Cantonese speech. Proceedings of ICSLP2006, 865-868.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-580"
  },
  "schneider11_interspeech": {
   "authors": [
    [
     "Katrin",
     "Schneider"
    ],
    [
     "Grzegorz",
     "Dogil"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Reaction time and decision difficulty in the perception of intonation",
   "original": "i11_2221",
   "page_count": 4,
   "order": 583,
   "p1": "2221",
   "pn": "2224",
   "abstract": [
    "An experiment was carried out to test the Categorical Perception as well as possible Perceptual Magnet Effects in the two boundary tone categories L% and H% in German, corresponding to statement vs. question interpretation, respectively. Additionally, reaction times (RT) were logged during all subtests to see if they support the results. Analyses revealed that RTs always increased with rising difficulty of the perceptual task, and decreased when the decision process was easy. Task-specific results showed that RT also correlated with the number of possible answers during a perceptual decision, i.e. more answer alternatives resulted in longer RT. Furthermore, female subjects generally reacted faster during all perceptual tasks, although this did not necessarily correlate with the accuracy of the results. Nevertheless, the results confirmed the usefulness of RT to support the analyses and the interpretation of perceptual data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-581"
  },
  "honbolygo11_interspeech": {
   "authors": [
    [
     "Ferenc",
     "Honbolygó"
    ],
    [
     "Valéria",
     "Csépe"
    ]
   ],
   "title": "Processing of stress related acoustic cues as indexed by ERPs",
   "original": "i11_2225",
   "page_count": 4,
   "order": 584,
   "p1": "2225",
   "pn": "2228",
   "abstract": [
    "The present paper investigated the event-related brain potential correlates of the processing of word stress related acoustic changes. We studied the processing of non-speech stimuli containing similar intensity and f0 changes as speech stimuli in a passive oddball paradigm. Contrary to our previous results using speech stimuli with a trochaic stress pattern contrasted with a iambic stress pattern, non-speech stimuli elicited a single MMN component. This result was interpreted as showing that the processing of stress information is based on speech specific mechanisms, instead of solely acoustic mechanisms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-582"
  },
  "witteman11_interspeech": {
   "authors": [
    [
     "Marijt J.",
     "Witteman"
    ],
    [
     "Andrea",
     "Weber"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "On the relationship between perceived accentedness, acoustic similarity, and processing difficulty in foreign-accented speech",
   "original": "i11_2229",
   "page_count": 4,
   "order": 585,
   "p1": "2229",
   "pn": "2232",
   "abstract": [
    "Foreign-accented speech is often perceived as more difficult to understand than native speech. What causes this potential difficulty, however, remains unknown. In the present study, we compared acoustic similarity and accent ratings of American-accented Dutch with a cross-modal priming task designed to measure online speech processing. We focused on two Dutch diphthongs: ui and ij. Though both diphthongs deviated from standard Dutch to varying degrees and perceptually varied in accent strength, native Dutch listeners recognized words containing the diphthongs easily. Thus, not all foreign-accented speech hinders comprehension, and acoustic similarity and perceived accentedness are not always predictive of processing difficulties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-583"
  },
  "amano11_interspeech": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Yukari",
     "Hirata"
    ]
   ],
   "title": "The perception boundary between single and geminate stops in 3- and 4-mora Japanese words",
   "original": "i11_2233",
   "page_count": 4,
   "order": 586,
   "p1": "2233",
   "pn": "2236",
   "abstract": [
    "The perception boundary between single and geminate stops was examined by regression analyses in 3- and 4-mora Japanese words spoken at various speaking rates. It was found that the perception boundary is well predicted by a linear function with duration of stop closure and durations of word or disyllable which contained the single and geminate stops. However, we conclude that the disyllable duration was a better variable than the word duration because it provides a more consistent explanation for the perception boundary regardless of word length and speaking rate variations. The results support a relational acoustic invariance theory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-584"
  },
  "ijima11_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Mitsuaki",
     "Isogai"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "Correlation analysis of acoustic features with perceptual voice quality similarity for similar speaker selection",
   "original": "i11_2237",
   "page_count": 4,
   "order": 587,
   "p1": "2237",
   "pn": "2240",
   "abstract": [
    "This paper describes the correlations between various acoustic features and perceptual voice quality similarity. We focus on identifying the acoustic features that are correlated with voice quality similarity. First, a large-scale perceptual experiment using the voices of 62 speakers is conducted and perceptual similarity scores between each pair of speakers are acquired. Next, multiple linear regression analysis is carried out; it shows that five acoustic features exhibit high correlation to voice quality similarity. Last, we perform similar speaker selection based on multiple linear regression with the above features and moreover, assess its performance by classifying speakers based on the perceptual similarity. The results indicate that the combination of the five acoustic features in classifying speakers into two classes is effective in choosing speakers with similar voice quality; it reduces the error rate by about 44% compared to using just the cepstrum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-585"
  },
  "jesse11_interspeech": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "Holger",
     "Mitterer"
    ]
   ],
   "title": "Pointing gestures do not influence the perception of lexical stress",
   "original": "i11_2445",
   "page_count": 4,
   "order": 588,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "We investigated whether seeing a pointing gesture influences the perceived lexical stress. A pitch contour continuum between the Dutch words \"CAnon\" ('canon') and \"kaNON\" ('cannon') was presented along with a pointing gesture during the first or the second syllable. Pointing gestures following natural recordings but not Gaussian functions influenced stress perception (Experiment 1 and 2), especially when auditory context preceded (Experiment 2). This was not replicated in Experiment 3. Natural pointing gestures failed to affect the categorization of a pitch peak timing continuum (Experiment 4). There is thus no convincing evidence that seeing a pointing gesture influences lexical stress perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-586"
  },
  "cushing11_interspeech": {
   "authors": [
    [
     "Ian R.",
     "Cushing"
    ],
    [
     "Francis F.",
     "Li"
    ],
    [
     "Ken",
     "Worrall"
    ],
    [
     "Tim",
     "Jackson"
    ]
   ],
   "title": "Relationships between phonetic features and speech perception - a statistical investigation from a large anechoic british English corpus",
   "original": "i11_2449",
   "page_count": 4,
   "order": 589,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "This paper concerns the relationships amongst acoustic phonetic features of speech signals, perceived vocal effort, and speech clarity. It is presented from a statistical analysis of a good number of subjective testing on an anechoic speech corpus with 5 different vocal efforts, namely hushed, normal, raised, loud, and shouted, with an aim to map objective acoustic phonetic features onto subjective ratings. Results show that listeners can differentiate vocal effort from subtle acoustic phonetic variations. There is also a correlation between clarity and vocal efforts. A regression model is further established to predict vocal effort from acoustic phonetic analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-587"
  },
  "brown11b_interspeech": {
   "authors": [
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Tim",
     "Jürgens"
    ],
    [
     "Ray",
     "Meddis"
    ],
    [
     "Matthew",
     "Robertson"
    ],
    [
     "Nicholas R.",
     "Clark"
    ]
   ],
   "title": "The representation of speech in a nonlinear auditory model: time-domain analysis of simulated auditory-nerve firing patterns",
   "original": "i11_2453",
   "page_count": 4,
   "order": 590,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "A nonlinear auditory model is appraised in terms of its ability to encode speech formant frequencies in the fine time structure of its output. It is demonstrated that groups of model auditory nerve (AN) fibres with similar interpeak intervals accurately encode the resonances of synthetic three-formant syllables, in close agreement with physiological data. Acoustic features are derived from the interpeak intervals and used as the input to a hidden Markov model-based automatic speech recognition system. In a digits-in-noise recognition task, interval-based features gave a better performance than features based on AN firing rate at every signal-to-noise ratio tested.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-588"
  },
  "coelho11_interspeech": {
   "authors": [
    [
     "Luis",
     "Coelho"
    ],
    [
     "Daniela",
     "Braga"
    ],
    [
     "Miguel",
     "Sales-Dias"
    ],
    [
     "Carmen",
     "Garcia-Mateo"
    ]
   ],
   "title": "An automatic voice pleasantness classification system based on prosodic and acoustic patterns of voice preference",
   "original": "i11_2457",
   "page_count": 4,
   "order": 591,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "In the last few years the number of systems and devices that use voice based interaction has grown significantly. For a continued use of these systems the interface must be reliable and pleasant in order to provide an optimal user experience. However there are currently very few studies that try to evaluate how good is a voice when the application is a speech based interface. In this paper we present a new automatic voice pleasantness classification system based on prosodic and acoustic patterns of voice preference. Our study is based on a multi-language database composed by female voices. In the objective performance evaluation the system achieved a 7.3% error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-589"
  },
  "carre11_interspeech": {
   "authors": [
    [
     "René",
     "Carré"
    ],
    [
     "Pierre",
     "Divenyi"
    ],
    [
     "Willy",
     "Serniclaes"
    ],
    [
     "Emmanuel",
     "Ferragne"
    ],
    [
     "Egidio",
     "Marsico"
    ],
    [
     "Viet-Son",
     "Nguyen"
    ]
   ],
   "title": "Contributions of F1 and F2 (F2') to the perception of plosive consonants",
   "original": "i11_2461",
   "page_count": 4,
   "order": 592,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "This study examined the contribution of F1 and F2 alone on the perception of plosive consonants in a CV context. Applying a 3-Bark spectral integration the F2 frequency was corrected for effects of proximity either to F1 or to F3, i.e., was replaced by F2'. Subjects used a two-dimensional Method of Adjustment to select the F1 and F2 consonant onset frequencies that led to a subjectively optimal percept of a predefined target CV. Results indicate that place prototypes are guided by F2 and are largely independent of F1. Nevertheless, while F2 alone is sufficient for segregating place prototypes for some consonants and vocalic contexts, it is insufficient for explaining the perception of place.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-590"
  },
  "kim11f_interspeech": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Auditory speech processing is affected by visual speech in the periphery",
   "original": "i11_2465",
   "page_count": 4,
   "order": 593,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "Two experiments were conducted to determine whether visual speech presented in the visual periphery affects the perceived identity of speech sounds. Auditory speech targets (vCv syllables) were presented in noise (-8 dB) with congruent or incongruent visual speech presented in full-face or upper-half face conditions. Participants' eye-movements were monitored to assure that visual speech input occurred only from the periphery. In Experiment 1 participants had only to identify what they heard. The results showed that peripherally presented visual speech (full-face) facilitated identification of AV congruent stimuli compared to the upper-face control. Likewise, visual speech reduced correct identification for the incongruent stimuli. Experiment 2 was the same as the first except that in addition participants performed a central visual task. Once again significant effects of visual speech were found. These results show that peripheral visual speech affects speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-591"
  },
  "paris11_interspeech": {
   "authors": [
    [
     "Tim",
     "Paris"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Visual speech speeds up auditory identification responses",
   "original": "i11_2469",
   "page_count": 4,
   "order": 594,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "Auditory speech perception is more accurate when combined with visual speech. Recent ERP studies suggest that visual speech helps 'predict' which phoneme will be heard via feedback from visual to auditory areas, with more visual salient articulations associated with greater facilitation. Two experiments tested this hypothesis with a speeded auditory identification measure. Stimuli consisted of the sounds 'apa', 'aka' and 'ata', with matched and mismatched videos that showed the talker's whole face or upper face (control). The percentage of matched AV videos was set at 85% in Experiment 1 and 15% in Experiment 2. Results showed that responses to matched whole face stimuli were faster than both upper face and mismatched videos in both experiments. Furthermore, salient phonemes (aPa) showed a greater reduction in reaction times than ambiguous ones (aKa). The current study provides support for the proposal that visual speech speeds up processing of auditory speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-592"
  },
  "takashima11b_interspeech": {
   "authors": [
    [
     "Ryoichi",
     "Takashima"
    ],
    [
     "Tohru",
     "Nagano"
    ],
    [
     "Ryuki",
     "Tachibana"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Agglomerative hierarchical clustering of emotions in speech based on subjective relative similarity",
   "original": "i11_2473",
   "page_count": 4,
   "order": 595,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "When we humans are asked whether or not the emotions in two speech samples are in the same category, the judgment depends on the size of the target category. Hierarchical clustering is a suitable technique for simulating such perceptions by humans of relative similarities of the emotions in speech. For better reflection of subjective similarities in clustering results, we have devised a method of hierarchical clustering that uses a new type of relative similarity data based on tagging the most similar pair in sets of three samples. This type of data allowed us to create a closed-loop algorithm for feature weight learning that uses the clustering performance as the objective function. When classifying the utterances of a specific sentence in Japanese recorded at a real call center, the method reduced the errors by 15.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-593"
  },
  "mai11_interspeech": {
   "authors": [
    [
     "Guangting",
     "Mai"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Optimal syllabic rates and processing units in perceiving Mandarin spoken sentences",
   "original": "i11_2477",
   "page_count": 4,
   "order": 596,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "This paper presents our investigations on the syllable-related processing during human perception of Mandarin spoken sentences. Two behavioral perception experiments were conducted employing a signal synthesis method in a previous study [1]. We found (1) a clear relationship between speech intelligibility and syllabic rates of spoken sentences and (2) significantly higher speech intelligibility of sentences acoustically segmented at sub-syllable and syllable levels than at the level beyond one syllable. We therefore revealed the optimal syllabic rates and processing units in perceiving Mandarin continuous speech and further discussed the association between our results and the possible underlying neural mechanisms in the human brain.\n",
    "",
    "",
    "Ghitza, O. and Greenberg, S., On the possible role of brain rhythms in speech perception: intelligibility of time-compressed speech with periodic and aperiodic insertions of silence, Phonetica, 66: 113126, 2009.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-594"
  },
  "wester11_interspeech": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Hui",
     "Liang"
    ]
   ],
   "title": "Cross-lingual speaker discrimination using natural and synthetic speech",
   "original": "i11_2481",
   "page_count": 4,
   "order": 597,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "This paper describes speaker discrimination experiments in which native English listeners were presented with natural speech stimuli in English and Mandarin, synthetic speech stimuli in English and Mandarin, or natural Mandarin speech and synthetic English speech stimuli. In each experiment, listeners were asked to judge whether the sentences in a pair were spoken by the same person or not. We found that the results of Mandarin/English speaker discrimination were very similar to those found in previous work on German/English and Finnish/English speaker discrimination. We conclude from this and previous work that listeners are able to discriminate between speakers across languages or across speech types, but the combination of these two factors leads to a speaker discrimination task that is too difficult for listeners to perform successfully, given the fact that the quality of across-language speaker adapted speech synthesis at present still needs to be improved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-595"
  },
  "navarathna11_interspeech": {
   "authors": [
    [
     "Rajitha",
     "Navarathna"
    ],
    [
     "Tristan",
     "Kleinschmidt"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Patrick",
     "Lucey"
    ]
   ],
   "title": "Can audio-visual speech recognition outperform acoustically enhanced speech recognition in automotive environment?",
   "original": "i11_2241",
   "page_count": 4,
   "order": 598,
   "p1": "2241",
   "pn": "2244",
   "abstract": [
    "The use of visual features in the form of lip movements to improve the performance of acoustic speech recognition has been shown to work well, particularly in noisy acoustic conditions. However, whether this technique can outperform speech recognition incorporating well-known acoustic enhancement techniques, such as spectral subtraction, or multi-channel beamforming is not known. This is an important question to be answered especially in an automotive environment, for the design of an efficient human-vehicle computer interface. We perform a variety of speech recognition experiments on a challenging automotive speech dataset and results show that synchronous HMM-based audio-visual fusion can outperform traditional single as well as multi-channel acoustic speech enhancement techniques. We also show that further improvement in recognition performance can be obtained by fusing speech-enhanced audio with the visual modality, demonstrating the complementary nature of the two robust speech recognition approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-596"
  },
  "alabau11_interspeech": {
   "authors": [
    [
     "Vicent",
     "Alabau"
    ],
    [
     "Verónica",
     "Romero"
    ],
    [
     "Antonio-L.",
     "Lagarda"
    ],
    [
     "Carlos-D.",
     "Martínez-Hinarejos"
    ]
   ],
   "title": "A multimodal approach to dictation of handwritten historical documents",
   "original": "i11_2245",
   "page_count": 4,
   "order": 599,
   "p1": "2245",
   "pn": "2248",
   "abstract": [
    "Handwritten Text Recognition is a problem that has gained attention in the last years due to the interest in the transcription of historical documents. Handwritten Text Recognition employs models that are similar to those employed in Automatic Speech Recognition (Hidden Markov Models and n-grams). Dictation of the contents of the document is an alternative to text recognition. In this work, we explore the performance of a Handwritten Text Recognition system against that of two speech dictation systems: a non-multimodal system that only uses speech and a multimodal system that performs a text recognition which is used in the posterior speech recognition. Results show that the multimodal combination outperforms any of the other considered non-multimodal systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-597"
  },
  "toutios11_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Utpala",
     "Musti"
    ],
    [
     "Slim",
     "Ouni"
    ],
    [
     "Vincent",
     "Colotte"
    ]
   ],
   "title": "Weight optimization for bimodal unit-selection talking head synthesis",
   "original": "i11_2249",
   "page_count": 4,
   "order": 600,
   "p1": "2249",
   "pn": "2252",
   "abstract": [
    "This paper addresses talking head synthesis based on the concatenation of units comprising of both acoustic and visual information. Selection of appropriate diphone units to synthesize a given text string is based on the minimization of a weighted linear combination of four costs that reflect linguistic, acoustic, and visual considerations. We present initial work toward a method to determine automatically the weights applied to each cost, using a series of metrics that assess quantitatively the performance of synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-598"
  },
  "schaffer11_interspeech": {
   "authors": [
    [
     "Stefan",
     "Schaffer"
    ],
    [
     "Benjamin",
     "Jöckel"
    ],
    [
     "Ina",
     "Wechsung"
    ],
    [
     "Robert",
     "Schleicher"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Modality selection and perceived mental effort in a mobile application",
   "original": "i11_2253",
   "page_count": 4,
   "order": 601,
   "p1": "2253",
   "pn": "2256",
   "abstract": [
    "This paper describes a study investigating the influence of efficiency and effectiveness on modality selection and perceived mental effort. Each participant had to perform several tasks with a smart phone application offering touch screen and Wizard-of-Oz speech recognition simulation as input modalities. The results show that efficiency and effectiveness have a strong influence on modality selection. Speech usage increases with increasing efficiency of speech input. A lower effectiveness of speech input raised the threshold for changing the modality selection strategy. For effectiveness mental effort differed significantly between the groups presented with low and high speech recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-599"
  },
  "ajmera11_interspeech": {
   "authors": [
    [
     "Jitendra",
     "Ajmera"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "A cross-lingual spoken content search system",
   "original": "i11_2257",
   "page_count": 4,
   "order": 602,
   "p1": "2257",
   "pn": "2260",
   "abstract": [
    "This paper presents an approach towards enabling audio search for those languages where training an automatic speech recognition (ASR) system is difficult, owing to lack of training resources. Our work is related to previous approaches where the problem of allowing search for out-of-vocabulary terms has been addressed. A phonetic recognizer is used to convert the audio data into phonetic lattices. In the proposed approach, the acoustic models (AM) for the phonetic recognizer are trained on a base language for which training data is available and used to search the content in a similar language. A phonetic language model (PLM) is trained for each language independently using text data available from a variety of sources including the web. We have performed experiments to evaluate this approach for searching through Gujarati corpus where the AM were trained on Indian-English corpus. The experimental results show that this approach can provide a P@10 (precision at 10) accuracy of up to 0.65.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-600"
  },
  "girardi11_interspeech": {
   "authors": [
    [
     "C.",
     "Girardi"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "M.",
     "Federico"
    ]
   ],
   "title": "Nemo: a platform for multilingual news monitoring",
   "original": "i11_2261",
   "page_count": 4,
   "order": 603,
   "p1": "2261",
   "pn": "2264",
   "abstract": [
    "News Monitor (NeMo) is an environment in which the Human Language Technology research unit at FBK brings together its technologies pertaining to Automatic Speech Recognition (ASR), Machine Translation (MT) and Natural Language Processing (NLP). In this view it is a dynamic framework where we can share ideas and technologies, refine algorithms, see and discuss performance and errors of our algorithms that are daily applied on fresh data. In this paper we describe a framework in which a set of parallel news streams in different languages are automatically transcribed and translated. The architecture of the system utilizes modules that perform ASR, MT and NLP. The development of the various modules relies upon a continuous acquisition activity of parallel data (both audio and texts) in different languages. In particular, the availability of large corpora of aligned multilingual text/audio data has allowed to implement unsupervised Acoustic Model (AM) training approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-601"
  },
  "chaudhuri11_interspeech": {
   "authors": [
    [
     "Sourish",
     "Chaudhuri"
    ],
    [
     "Mark",
     "Harvilla"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Unsupervised learning of acoustic unit descriptors for audio content representation and classification",
   "original": "i11_2265",
   "page_count": 4,
   "order": 604,
   "p1": "2265",
   "pn": "2268",
   "abstract": [
    "In this paper, we attempt to represent audio as a sequence of acoustic units using unsupervised learning and use them for multi-class classification. We expect the acoustic units to represent sounds or sound sequences to automatically create a sound alphabet. We use audio from multi-class Youtube-quality multimedia data to converge on a set of sound units, such that each audio file is represented as a sequence of these units. We then try to learn category language models over sequences of the acoustic units, and use them to generate acoustic and language model scores for each category. Finally, we use a margin based classification algorithm to weight the category scores to predict the class that each test data point belongs to. We compare different settings and report encouraging results on this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-602"
  },
  "glodek11_interspeech": {
   "authors": [
    [
     "Michael",
     "Glodek"
    ],
    [
     "Stefan",
     "Scherer"
    ],
    [
     "Friedhelm",
     "Schwenker"
    ]
   ],
   "title": "Conditioned hidden Markov model fusion for multimodal classification",
   "original": "i11_2269",
   "page_count": 4,
   "order": 605,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "Classification using hidden Markov models (HMM) is in general done by comparing the model likelihoods and choosing the class more likely to have generated the data. This work investigates a conditioned HMM which additionally provides a probability for a class label and compares different fusion strategies. The notion is two-fold: on the one hand applications in affective computing might pass their uncertainty of the classification to the next processing unit, on the other hand different streams might be fused to increase the performance. The data set studied incorporates two modalities and is based on a naturalistic multiparty dialogue. The goal is to discriminate between laughter and utterances. It turned out that the conditioned HMM outperforms classical HMM using different late fusion approaches while additionally providing a certainty about class decision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-603"
  },
  "lecouteux11_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Michel",
     "Vacher"
    ],
    [
     "François",
     "Portet"
    ]
   ],
   "title": "Distant speech recognition in a smart home: comparison of several multisource ASRs in realistic conditions",
   "original": "i11_2273",
   "page_count": 4,
   "order": 606,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "While the smart home domain has become a major field of application of ICT to improve support and wellness of people in loss of autonomy, speech technology in smart home has, comparatively to other ICTs, received limited attention. This paper presents the Sweet-Home project whose aim is to make it possible for frail persons to control their domestic environment through voice interfaces. Several state-of-the-art and novel ASR techniques were evaluated on realistic data acquired in a multiroom smart home. This distant speech French corpus was recorded with 21 speakers playing scenarios including activities of daily living in a smart home equipped with several microphones. Techniques acting at the decoding stage and using a priori knowledge such as DDA give better results (WER=8.8%, Domotic F-measure=96.8%) than the baseline (WER=18.3%, Domotic F-measure=89.2%) and other approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-604"
  },
  "chen11g_interspeech": {
   "authors": [
    [
     "Jiansong",
     "Chen"
    ],
    [
     "Lei",
     "Zhu"
    ],
    [
     "Bailan",
     "Feng"
    ],
    [
     "Peng",
     "Ding"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A robust approach to mining repeated sequence in audio stream",
   "original": "i11_2277",
   "page_count": 4,
   "order": 607,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "In multimedia stream, repeated sequences, e.g., commercials, jingles, usually imply potentially significant information. Therefore, mining repeated sequence is an important approach to analyzing multimedia content. This paper reports on a robust unsupervised technique of discovering repeated sequence in audio stream. Different from former research, our approach transforms the repeated sequence detection task into a Hidden Markov Model (HMM) decoding problem in a similarity trellis. To resist the false and missing matches in real application, we present a soft definition of repeated sequence, termed as maximal loosely repeated sequence (MLRS), as the objective for detection, and use a Viterbi-like algorithm to mine all the MLRSs in the stream. In addition, we propose a novel metric to evaluate the repeated sequence detection algorithm. Experiments both on simulated data and real broadcast data demonstrate the effectiveness of our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-605"
  },
  "yu11c_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Accelerated parallelizable neural network learning algorithm for speech recognition",
   "original": "i11_2281",
   "page_count": 4,
   "order": 608,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "We describe a set of novel, batch-mode algorithms we developed recently as one key component in scalable, deep neural network based speech recognition. The essence of these algorithms is to structure the single-hidden-layer neural network so that the upper-layer's weights can be written as a deterministic function of the lower-layer's weights. This structure is effectively exploited during training by plugging in the deterministic function to the least square error objective function while calculating the gradients. Accelerating techniques are further exploited to make the weight updates move along the most promising directions. The experiments on TIMIT frame-level phone and phone-state classification show strong results. In particular, the error rate is strictly monotonically dropping as the mini-batch size increases. This demonstrates the potential for the proposed batch-mode algorithms in large scale speech recognition since they are easily parallelizable across computers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-606"
  },
  "deng11_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Deep convex net: a scalable architecture for speech pattern classification",
   "original": "i11_2285",
   "page_count": 4,
   "order": 609,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "We recently developed context-dependent DNN-HMM (Deep- Neural-Net/Hidden-Markov-Model) for large-vocabulary speech recognition. While achieving impressive recognition error rate reduction, we face the insurmountable problem of scalability in dealing with virtually unlimited amount of training data available nowadays. To overcome the scalability challenge, we have designed the deep convex network (DCN) architecture. The learning problem in DCN is convex within each module. Additional structure-exploited fine tuning further improves the quality of DCN. The full learning in DCN is batch-mode based instead of stochastic, naturally lending it amenable to parallel training that can be distributed over many machines. Experimental results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-607"
  },
  "wang11h_interspeech": {
   "authors": [
    [
     "Siwei",
     "Wang"
    ],
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Modeling broad context for tone recognition with conditional random fields",
   "original": "i11_2289",
   "page_count": 4,
   "order": 610,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "We propose a tone recognition approach that employs linear-chain Conditional Random Fields (CRF) to model tone variation due to intonation effects. We implement three linear-chain CRFs which aim at modeling intonation effects at phrase-sentence- and story-level boundaries, where we show that standard recognition techniques degrade and common normalization approaches do not improve. We show that all linear-chain CRFs outperform the baseline unigram model, and the biggest improvement is found in recognizing 3rd tones, (4%) in overall accuracy. In particular, Phrase Bigram CRFs show a drastic 39% improvement in recognizing 3rd tones located at initial boundaries. This improvement shows that the position specific modeling of initial tones in bigram CRFs captures the intonation effects better than the baseline unigram model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-608"
  },
  "li11j_interspeech": {
   "authors": [
    [
     "Shang-wen",
     "Li"
    ],
    [
     "Yow-bang",
     "Wang"
    ],
    [
     "Liang-che",
     "Sun"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Improved tonal language speech recognition by integrating spectro-temporal evidence and pitch information with properly chosen tonal acoustic units",
   "original": "i11_2293",
   "page_count": 4,
   "order": 611,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "We propose an improved Tandem system for tonal language speech recognition. Three different types of features, cepstral, spectrotemporal and pitch features, are integrated for modeling tone and phoneme variation simultaneously. Tonal phonemes (or tonemes) are used for MLP posterior estimation, and tonal acoustic units for HMM recognition. In our experiments conducted on Mandarin broadcast news, a 19.3% relative CER reduction was achieved over the conventional MFCC Tandem baseline. With different training acoustic units, we analyze the complementarity among the three types of features in tone, phoneme, and toneme classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-609"
  },
  "gouvea11b_interspeech": {
   "authors": [
    [
     "Evandro",
     "Gouvêa"
    ],
    [
     "Marelie H.",
     "Davel"
    ]
   ],
   "title": "Kullback-leibler divergence-based ASR training data selection",
   "original": "i11_2297",
   "page_count": 4,
   "order": 612,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "Data preparation and selection affects systems in a wide range of complexities. A system built for a resource-rich language may be so large as to include borrowed languages. A system built for a resource-scarce language may be affected by how carefully the training data is selected and produced. Accuracy is affected by the presence of enough samples of qualitatively relevant information. We propose a method using the Kullback-Leibler divergence to solve two problems related to data preparation: the ordering of alternate pronunciations in a lexicon, and the selection of transcription data. In both cases, we want to guarantee that a particular distribution of n-grams is achieved. In the case of lexicon design, we want to ascertain that phones will be present often enough. In the case of training data selection for scarcely resourced languages, we want to make sure that some n-grams are better represented than others. Our proposed technique yields encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-610"
  },
  "nss11_interspeech": {
   "authors": [
    [
     "Arild Brandrud",
     "Næss"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ]
   ],
   "title": "Articulatory feature classification using nearest neighbors",
   "original": "i11_2301",
   "page_count": 4,
   "order": 613,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "Recognizing aspects of articulation from audio recordings of speech is an important problem, either as an end in itself or as part of an articulatory approach to automatic speech recognition. In this paper we study the frame-level classification of a set of articulatory features (AFs) inspired by the vocal tract variables of articulatory phonology. We compare k nearest neighbor (k-NN) classifiers and multilayer perceptrons (MLPs), using different acoustic feature vectors, and classify the AFs either independently or jointly. We also consider using the MLP outputs for all of the AFs as inputs to k-NN classifiers for the individual AFs, effectively using the MLPs as a form of nonlinear dimensionality reduction and allowing the decision for each AF to be based on the MLPs for the other AFs. We find that MLPs outperform k-NN classifiers, while k-NN classifiers using MLP outputs outperform both.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-611"
  },
  "demange11_interspeech": {
   "authors": [
    [
     "Sébastien",
     "Demange"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Continuous episodic memory based speech recognition using articulatory dynamics",
   "original": "i11_2305",
   "page_count": 4,
   "order": 614,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "In this paper we present a speech recognition system based on articulatory dynamics. We do not extend the acoustic feature with any explicit articulatory measurements but instead the articulatory dynamics of speech are structurally embodied within episodic memories. The proposed recognizer is made of different memories each specialized for a particular articulator. As all the articulators do not contribute equally to the realization of a particular phoneme, the specialized memories do not perform equally regarding each phoneme. We show, through phone string recognition experiments that combining the recognition hypotheses resulting from the different articulatory specialized memories leads to significant recognition improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-612"
  },
  "li11k_interspeech": {
   "authors": [
    [
     "T.",
     "Li"
    ],
    [
     "P. C.",
     "Woodland"
    ],
    [
     "F.",
     "Diehl"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Graphone model interpolation and Arabic pronunciation generation",
   "original": "i11_2309",
   "page_count": 4,
   "order": 615,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "This paper extends n-gram graphone model pronunciation generation to use a mixture of such models. This technique is useful when pronunciation data is for a specific variant (or set of variants) of a language, such as for a dialect, and only a small amount of pronunciation dictionary training data for that specific variant is available. The performance of the interpolated n-gram graphone model is evaluated on Arabic phonetic pronunciation generation for words that can't be handled by the Buckwalter Morphological Analyser. The pronunciations produced are also used to train an Arabic broadcast audio speech recognition system. In both cases the interpolated graphone model leads to improved performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-613"
  },
  "illina11_interspeech": {
   "authors": [
    [
     "Irina",
     "Illina"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Grapheme-to-phoneme conversion using conditional random fields",
   "original": "i11_2313",
   "page_count": 4,
   "order": 616,
   "p1": "2313",
   "pn": "2316",
   "abstract": [
    "We propose an approach to grapheme-to-phoneme conversion based on a probabilistic method: Conditional Random Fields (CRF). CRF give a long term prediction, and assume a relaxed state independence condition. Moreover, we propose an algorithm to the one-to-one letter to phoneme alignment needed for CRF training. This alignment is based on discrete HMMs. The proposed system is validated on two pronunciation dictionaries. Different CRF features are studied: POS-tag, context size, unigram versus bigram. Our approach compares favorably with the performance of the state-of-the-art Joint-Multigram Models for the quality of the pronunciations, but provides better recall and precision measures for multiple pronunciation variants generation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-614"
  },
  "yeh11_interspeech": {
   "authors": [
    [
     "Ching-Feng",
     "Yeh"
    ],
    [
     "Chao-Yu",
     "Huang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Bilingual acoustic model adaptation by unit merging on different levels and cross-level integration",
   "original": "i11_2317",
   "page_count": 4,
   "order": 617,
   "p1": "2317",
   "pn": "2320",
   "abstract": [
    "This paper presents a bilingual acoustic model adaptation approach for transcribing Mandarin-English code-mixed lectures with highly unbalanced language distribution. This includes a adaptation structure, merging of Mandarin and English acoustic units on model, state and Gaussian levels, and a cross-level integration scheme. The corpora tested include two real courses, in which special terminologies were produced in the guest language of English (about 15.19%) and embedded in the utterances produced in the host language (about 81.85%). The code-mixing nature of the target corpora and the very small percentage of the English data made the task difficult. Preliminary experiments showed that unit merging was helpful, merging on lower levels offered more improvements, and cross-level integration was even better. The code-mixing situation considered is actually very natural in the spoken language of the daily lives of many people in the globalized world today.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-615"
  },
  "schraagen11_interspeech": {
   "authors": [
    [
     "Marijn",
     "Schraagen"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ]
   ],
   "title": "A qualitative evaluation of phoneme-to-phoneme technology",
   "original": "i11_2321",
   "page_count": 4,
   "order": 618,
   "p1": "2321",
   "pn": "2324",
   "abstract": [
    "Automatic speech recognition systems apply grapheme-tophoneme transcription (G2P) to model pronunciation of items in the lexicon. General purpose G2P transcriptions are not always accurate, e.g., in a multilingual environment. To improve the transcription quality, G2P transcriptions can be postprocessed using a phoneme-to-phoneme (P2P) converter. This paper discusses the applicability of P2P technology based on results of a speech recognition experiment using P2P conversion on a multilingual speech corpus. P2P conversion can be applied successfully, however the analysis also shows limitations of P2P technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-616"
  },
  "falavigna11b_interspeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "Cheap bootstrap of multi-lingual hidden Markov models",
   "original": "i11_2325",
   "page_count": 4,
   "order": 619,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "In this work we investigate the usage of TV audio data for crosslanguage training of multi-lingual acoustic models. We intend to take advantage from the availability of a training speech corpus formed by parallel news uttered in different languages and transmitted over separated audio channels.\n",
    "Spanish, French and Russian phone Hidden Markov Models (HMMs) are bootstrapped using an unsupervised training procedure starting from an Italian set of phone HMMs. The use of confidence measures in order to select the training audio data was also investigated and has proved to be effective. The usage of cross language information, i.e. exploiting the temporal alignment of news in different languages to build news-dependent Language Models (LMs), was also demonstrated to give benefits to the acoustic model training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-617"
  },
  "mesgarani11_interspeech": {
   "authors": [
    [
     "Nima",
     "Mesgarani"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Adaptive stream fusion in multistream recognition of speech",
   "original": "i11_2329",
   "page_count": 4,
   "order": 620,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "A new method to deal with variable distortions of speech during the operation of the system is proposed. First, multiple processing streams are formed by extracting different spectral and temporal modulation components from the speech signal. Information in each stream is used to estimate posterior probabilities of phonemes. Initial values for a weighted integration of these individual estimates are found by normalized cross-correlation of the estimates with the actual phoneme labels on the training data. A statistical model of the final estimated posterior probabilities is used to characterize the system performance. During the operation, the weights in the linear fusion are adapted using particle filtering to optimize the performance. Results on phoneme recognition from noisy speech indicate the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-618"
  },
  "siu11_interspeech": {
   "authors": [
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Steve",
     "Lowe"
    ],
    [
     "Arthur",
     "Chan"
    ]
   ],
   "title": "Unsupervised audio patterns discovery using HMM-based self-organized units",
   "original": "i11_2333",
   "page_count": 4,
   "order": 621,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "In our previous work [1, 2], we trained an HMM-based speech recognizer without transcription or any knowledge or resources. The trained HMM recognizer was used to transcribe audio into self-organized units (SOUs) and we evaluated its performance on the task of topic identification. In this paper, we report our work in applying SOUs to discover audio patterns in spoken documents without supervision. By recognizing audio into SOUs which are sound-like units, the discovery for common audio patterns can be carried out extremely efficiently over a large corpus, without dynamic programming comparisons as proposed by earlier work [3]. Experiments were performed on Mandarin conversational telephone speech using both the one-best SOU token sequences and SOU consensus networks. We show that using SOU as keys to audio patterns, we can discover frequently spoken words with good purity.\n",
    "s H. Gish, M. Siu, A. Chan andW. Belfield, Unsupervised training of an HMM-based speech recognition system for topic classification, Interspeech 2009. M. Siu, H Gish, A Chan and W. Belfield , Improved Topic Classification and Keyword Discovery Using an HMM-Based Speech Recognizer Trained Without Supervision, Interspeech 2010. Y. Zhang and J. Glass, Towards multi-speaker unsupervised speech pattern discovery, in ICASSP, 2010\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-619"
  },
  "labiak11_interspeech": {
   "authors": [
    [
     "John",
     "Labiak"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "Nearest neighbors with learned distances for phonetic frame classification",
   "original": "i11_2337",
   "page_count": 4,
   "order": 622,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "Nearest neighbor-based techniques provide an approach to acoustic modeling that avoids the often lengthy and heuristic process of training traditional Gaussian mixture-based models. Here we study the problem of choosing the distance metric for a k-nearest neighbor (k-NN) phonetic frame classifier. We compare the standard Euclidean distance to two learned Mahalanobis distances, based on large-margin nearest neighbors (LMNN) and locality preserving projections (LPP).We use locality sensitive hashing for approximate nearest neighbor search to reduce the test time of k-NN classification. We compare the error rates of these approaches, as well as of baseline Gaussian mixture-based and multilayer perceptron classifiers, on the task of phonetic frame classification of speech from the TIMIT database. The k-NN classifiers outperform Gaussian mixture models, but not multilayer perceptrons. We find that the best k-NN classification performance is obtained using LPP, while LMNN is close behind.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-620"
  },
  "fagerlund11_interspeech": {
   "authors": [
    [
     "Seppo",
     "Fagerlund"
    ],
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Stop consonant recognition by temporal fine structure of burst",
   "original": "i11_2385",
   "page_count": 4,
   "order": 623,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "The automatic classification of the unvoiced stop consonants is widely considered as a difficult task for traditional frequency domain and even time-frequency methods. Main reason for this is their short duration and diverse temporal structure. In this paper we present a novel method for stop consonant recognition. The method is based on statistical properties of short temporal fine structure of burst part. Classification is also evaluated with simple frequency domain method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-621"
  },
  "kirchhoff11_interspeech": {
   "authors": [
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Andrei",
     "Alexandrescu"
    ]
   ],
   "title": "Phonetic classification using controlled random walks",
   "original": "i11_2389",
   "page_count": 4,
   "order": 624,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "Recently, semi-supervised learning algorithms for phonetic classifiers have been proposed that have obtained promising results. Often, these algorithms attempt to satisfy learning criteria that are not inherent in the standard generative or discriminative training procedures for phonetic classifiers. Graph-based learners in particular utilize an objective function that not only maximizes the classification accuracy on a labeled set but also the global smoothness of the predicted label assignment. In this paper we investigate a novel graph-based semi-supervised learning framework that implements a controlled random walk where different possible moves in the random walk are controlled by probabilities that are dependent on the properties of the graph itself. Experimental results on the TIMIT corpus are presented that demonstrate the effectiveness of this procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-622"
  },
  "marujo11_interspeech": {
   "authors": [
    [
     "Luís",
     "Marujo"
    ],
    [
     "Márcio",
     "Viveiros"
    ],
    [
     "João Paulo da Silva",
     "Neto"
    ]
   ],
   "title": "Keyphrase cloud generation of broadcast news",
   "original": "i11_2393",
   "page_count": 4,
   "order": 625,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "This paper describes an enhanced automatic keyphrase extraction method applied to Broadcast News. The keyphrase extraction process is used to create a concept level for each news. On top of words resulting from a speech recognition system output and news indexation and it contributes to the generation of a tag/keyphrase cloud of the top news included in a Multimedia Monitoring Solution system for TV and Radio news/programs, running daily, and monitoring 12 TV channels and 4 Radios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-623"
  },
  "canterla11_interspeech": {
   "authors": [
    [
     "Alfonso M.",
     "Canterla"
    ],
    [
     "Magne H.",
     "Johnsen"
    ]
   ],
   "title": "Optimized feature extraction and HMMs in subword detectors",
   "original": "i11_2397",
   "page_count": 4,
   "order": 626,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "This paper presents methods and results for optimizing subword detectors in continuous speech. Speech detectors are useful within areas like detection-based ASR, pronunciation training, phonetic analysis, word spotting, etc. We build detectors for both articulatory features and phones by discriminative training of detector-specific MFCC filterbanks and HMMs. The resulting filterbanks are clearly different from each other and reflect acoustic properties of the corresponding detection classes. For the TIMIT task, our detector-specific features reduce the average detection error rate by 20% compared to standard MFCCs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-624"
  },
  "shi11_interspeech": {
   "authors": [
    [
     "Ziqiang",
     "Shi"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Tieran",
     "Zheng"
    ]
   ],
   "title": "Real-world speech/non-speech audio classification based on sparse representation features and GPCs",
   "original": "i11_2401",
   "page_count": 4,
   "order": 627,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "A novel and robust approach for content based speech/non-speech audio classification is proposed based on sparse representation (SR) features and Gaussian process classifiers (GPCs). The projections of the noise robust sparse representations for audio signals computed by L1-norm minimization are used as features. GPCs are used to learn and predict audio categories. Compare to the difficulties of Support Vector Machines (SVMs) in determining the hyperparameters, GPCs employ Bayesian selection criterion to estimate them. Experimental results on real-world audio datasets show that the SR features are more robust to audio variants than mel-frequency cepstral coefficients (MFCCs) and the proposed approach gives better performances than SVM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-625"
  },
  "pathak11_interspeech": {
   "authors": [
    [
     "Manas A.",
     "Pathak"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Privacy preserving speaker verification using adapted GMMs",
   "original": "i11_2405",
   "page_count": 4,
   "order": 628,
   "p1": "2405",
   "pn": "2408",
   "abstract": [
    "In this paper we present an adapted UBM-GMM based privacy preserving speaker verification (PPSV) system, where the system is not able to observe the speech data provided by the user and the user does not observe the models trained by the system. These privacy criteria are important in order to prevent an adversary having unauthorized access to the user's client device from impersonating a user and also from another adversary who can break into the verification system can learn about the user's speech patterns to impersonate the user in another system. We present protocols for speaker enrollment and verification which preserve privacy according to these requirements and report experiments with a prototype implementation on the YOHO dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-626"
  },
  "szekely11_interspeech": {
   "authors": [
    [
     "Éva",
     "Székely"
    ],
    [
     "João P.",
     "Cabral"
    ],
    [
     "Peter",
     "Cahill"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Clustering expressive speech styles in audiobooks using glottal source parameters",
   "original": "i11_2409",
   "page_count": 4,
   "order": 629,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "A great challenge for text-to-speech synthesis is to produce expressive speech. The main problem is that it is difficult to synthesise high-quality speech using expressive corpora. With the increasing interest in audiobook corpora for speech synthesis, there is a demand to synthesise speech which is rich in prosody, emotions and voice styles. In this work, Self-Organising Feature Maps (SOFM) are used for clustering the speech data using voice quality parameters of the glottal source, in order to map out the variety of voice styles in the corpus. Subjective evaluation showed that this clustering method successfully separated the speech data into groups of utterances associated with different voice characteristics. This work can be applied in unit-selection synthesis by selecting appropriate data sets to synthesise utterances with specific voice styles. It can also be used in parametric speech synthesis to model different voice styles separately.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-627"
  },
  "ludusan11_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Antonio",
     "Origlia"
    ],
    [
     "Francesco",
     "Cutugno"
    ]
   ],
   "title": "On the use of the rhythmogram for automatic syllabic prominence detection",
   "original": "i11_2413",
   "page_count": 4,
   "order": 630,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "In this paper we will investigate the usefulness of the rhythmogram, a speech rhythm representation based on the Auditory Primal Sketch model, for the automatic detection of prominent syllables. This representation was compared to other features usually used for this task and it showed a higher performance in the identification of prominent/non-prominent syllables. A new prominence detection algorithm is proposed, combining the rhythmogram and pitch features and tested on two corpora of Italian and French. The results obtained showed significant detection improvements with respect to other systems in the literature, 0.9% and 2.5% accuracy increase respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-628"
  },
  "sam11_interspeech": {
   "authors": [
    [
     "Sethserey",
     "Sam"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Eric",
     "Castelli"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Speech modulation features for robust nonnative speech accent detection",
   "original": "i11_2417",
   "page_count": 4,
   "order": 631,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "In this paper, we propose to use speech modulation features for robust nonnative accent detection. Modulation spectrum carries long term temporal information of speech and may discriminate accents of native and nonnative speakers. For each speech segment to be tested, we extract a 10 dimension feature vector from modulation spectrum and use it for model training and testing. The proposed modulation features are compared with other popular features such as pitch and formant on a nonnative French accent detection task. Results show that the modulation features produce good detection performance and are quite robust to channel distortions. In addition, when combine test scores of modulation features and pitch features, performance is further significantly reduced. The best equal error rate is 13.1% by fusing pitch and modulation-based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-629"
  },
  "zhang11j_interspeech": {
   "authors": [
    [
     "Chi",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Frame-level vocal effort likelihood space modeling for improved whisper-island detection",
   "original": "i11_2421",
   "page_count": 4,
   "order": 632,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "In this study, a frame-based vocal effort likelihood space modeling framework for improved whisper-island detection within normally phonated audio streams is proposed. The proposed method is based on first training a traditional Gaussian mixture model for whisper and neutral speech, which is then employed to extract a newly proposed discriminative feature set entitled Vocal Effort Likelihood (VEL), for whisper-island detection. The VEL feature set is integrated within a BIC/T2-BIC segmentation scheme for vocal effort change point (VECP) detection. With the dimension-reduced VEL 2-D feature set, the proposed framework has reduced computational costs versus prior method [1]. Experimental results using the UT-VocalEffort II corpus for whisper-island detection using the proposed framework are presented and compared with a previous algorithm introduced in [1]. The proposed algorithm is shown to improve performance in VECP detection with the lowest Multi- Error Score (MES) of 6.33. Furthermore, very accurate whisper-island detection was obtained using proposed algorithm, which is useful for sustained performance in speech systems (ASR, Speaker-ID, etc.) which might experience whisper speech. Finally, experimental performance achieves a 100% detection rate for the proposed algorithm, which represents the best whisper-island detection performance with lowest computational costs available in the literature to date.\n",
    "",
    "",
    "C. Zhang and J.H.L. Hansen, An unsupervised effective algorithm for whisper-island detection, IEEE Transactions on Audio, Speech, and Language Processing, vol. PP, no. 99, p. 1, 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-630"
  },
  "fan11b_interspeech": {
   "authors": [
    [
     "Xing",
     "Fan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speaker identification for whispered speech using a training feature transformation from neutral to whisper",
   "original": "i11_2425",
   "page_count": 4,
   "order": 633,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "A number of research studies in speaker recognition have recently focused on robustness due to microphone and channel mismatch( e.g., NIST SRE). However, changes in vocal effort, especially whispered speech, present significant challenges in maintaining system performance. Due to the mismatch spectral structure resulting from the different production mechanisms, performance of speaker identification systems trained with neutral speech degrades significantly when tested with whispered speech. This study considers a feature transformation method in the training phase that leads to a more robust speaker model for speaker ID with whispered speech. In the proposed system, a Speech Mode Independent (SMI) Universal Background Model (UBM) is built using collected real neutral features and pseudo whispered features generated with Vector Taylor Series (VTS), or via Constrained Maximum Likelihood Linear Regression (CMLLR) model adaptation. Textindependent closed set speaker ID results using the UT-VocalEffort II corpus show an accuracy of 88.87% using the proposed method, which represents a relative improvement of 46.26% compared with the 79.29% accuracy of the baseline system. This result confirms a viable approach to improving speaker ID performance for neutral and whispered speech mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-631"
  },
  "demarco11_interspeech": {
   "authors": [
    [
     "Andrea",
     "DeMarco"
    ],
    [
     "Stephen J.",
     "Cox"
    ]
   ],
   "title": "An accurate and robust gender identification algorithm",
   "original": "i11_2429",
   "page_count": 4,
   "order": 634,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "We describe a robust, unsupervised method of automatic gender identification from speech. We first design a baseline gender classifier based on MFCC features, and add a second classifier that uses context-dependent but text-independent pitch features. The results of these classifiers are then examined for disagreements in gender classification. Any disagreements are resolved by the use of a novel pitch-shifting mechanism applied to the utterances. We show how the acoustic context classifier provides very good gender identification results, and how these are further enhanced by the pitch-shifting process. Furthermore this enhancement is preserved across a set of different corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-632"
  },
  "yang11c_interspeech": {
   "authors": [
    [
     "Xiaohong",
     "Yang"
    ],
    [
     "Qingcai",
     "Chen"
    ],
    [
     "Shusen",
     "Zhou"
    ],
    [
     "Xiaolong",
     "Wang"
    ]
   ],
   "title": "Deep belief networks for automatic music genre classification",
   "original": "i11_2433",
   "page_count": 4,
   "order": 635,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "This paper proposes an approach to automatic music genre classification using deep belief networks. Based on the restricted Boltzmann machines, the deep belief networks is constructed and takes the acoustic features extracted through content-based analysis of music signals as input. The model parameters are initially determined after the deep belief network is trained by greedy layer-wise learning algorithm with feature vectors that are comprised of short-term and long-term features. Then the parameters are fine-tuned to local optimum according to back propagation algorithm. Experiments on GTZAN dataset show that the performance of music genre classification using deep belief networks is superior to those of widely used classification methods such as support vector machine, K-nearest neighbor, linear discriminant analysis and neural network.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-633"
  },
  "dennis11_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Dennis"
    ],
    [
     "Huy Dat",
     "Tran"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Image representation of the subband power distribution for robust sound classification",
   "original": "i11_2437",
   "page_count": 4,
   "order": 636,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "This paper proposes a robust sound event classification method, based on a selective image feature driven from the novel subband power distribution (SPD), which represents the distribution of power over frequency components. This method is an extension of our previous work, which was motivated by the visual perception of the spectrogram to produce a robust feature for sound classification. Unlike the conventional spectrogram, the proposed SPD representation is invariant to time-shifting and therefore suitable for real scenarios where the detected sound clips are not always balanced. Furthermore, we develop a missing feature classification method, which automatically selects the sparse, representative areas of the signal from the noisy SPD image of the sound clip. The method is tested on a large database containing 50 sound classes, under four different noise environments, varying from clean to severe noise conditions. A significant improvement in performance was obtained in mismatched conditions, producing an average classification accuracy of 87.5% in the 0dB noise condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-634"
  },
  "xiao11b_interspeech": {
   "authors": [
    [
     "Bo",
     "Xiao"
    ],
    [
     "Viktor",
     "Rozgić"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Brian R.",
     "Baucom"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Acoustic and visual cues of turn-taking dynamics in dyadic interactions",
   "original": "i11_2441",
   "page_count": 4,
   "order": 637,
   "p1": "2441",
   "pn": "2444",
   "abstract": [
    "In this paper we introduce an empirical study of multimodal cues of turn-taking dynamics in a social interaction context. We first identify pauses, gaps and overlapped speech segments in the dyadic conversation dataset. Second, we define two types of measurements, Mean Equalized Energy (MEE) and Animation Level (AL) on the audio and video channels, respectively. Then, we verify the hypothesis that the speaker with higher MEE or AL is more likely to take the floor after silence or overlapped speech. The results suggest that both the vocal and visual movement energy offer useful cues towards inferring the intention of the interlocutor to grab the floor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-635"
  },
  "shi11b_interspeech": {
   "authors": [
    [
     "Yong-zhe",
     "Shi"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Robust audio fingerprinting based on local spectral luminance maxima scheme",
   "original": "i11_2485",
   "page_count": 4,
   "order": 638,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "This paper proposes a robust audio fingerprinting system based on local spectral luminance maxima (LSLM) scheme using image processing approaches. Our approach treats spectrogram of an audio clip as a 2-D image and extracts the local luminance maxima of spectrum image as the discriminative characteristics. LSLM are selected due to resilience against quantization, compression, and noise addition, etc. Experimental results show that the proposed binary audio fingerprints outperform some of the state-of-the-art in the context of both robustness and reliability, especially in the noisy environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-636"
  },
  "laine11_interspeech": {
   "authors": [
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Entropy-rate driven inference of stochastic grammars",
   "original": "i11_2489",
   "page_count": 4,
   "order": 639,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "A new method for inferring specific stochastic grammars is presented. The process called Hybrid Model Learner (HML) applies entropy rate to guide the agglomeration process of type ab->c. Each rule derived from the input sequence is associated with a certain entropy-rate difference. A grammar automatically inferred from an example sequence can be used to detect and recognize similar structures in unknown sequences. Two important schools of thought, that of structuralism and the other of 'stochasticism' are discussed, including how these two have met and are influencing current statistical learning methods. It is argued that syntactic methods may provide universal tools to model and describe structures from the very elementary level of signals up to the highest one, that of language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-637"
  },
  "lee11f_interspeech": {
   "authors": [
    [
     "Sheng-Chieh",
     "Lee"
    ],
    [
     "K.",
     "Bharanitharan"
    ],
    [
     "Bo-Wei",
     "Chen"
    ],
    [
     "Jhing-Fa",
     "Wang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Min-Jian",
     "Liao"
    ]
   ],
   "title": "An efficient pre-processing scheme to improve the sound source localization system in noisy environment",
   "original": "i11_2493",
   "page_count": 4,
   "order": 640,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "In this study, we introduce an efficient pre-processing scheme for direction of arrival (DOA) estimation, which is capable of reducing the noise and reverberation effects in speech sound source localization. Furthermore, this presented system is also suitable for far-field speech localization. The adopted method of this proposed system can be simply subdivided into three stages: Linear phase-difference approximation, covariance matrix reconstruction, and frequency bin selection. The first two stages can initially decrease the influences of noise and reverberation; the last stage is used to filter the noise frequency bands according to the eigenvalue decomposition (EVD) of the covariance matrix. The experimental results show that our proposed system has effective performance of detecting different directions of speeches. For different signal-to-noise ratios (SNRs) speech signals, the average estimation errors can be decreased by about 5 to 7.5 degrees.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-638"
  },
  "lejan11_interspeech": {
   "authors": [
    [
     "Guylaine",
     "Le-Jan"
    ],
    [
     "Yannick",
     "Benezeth"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "A study on auditory feature spaces for speech-driven lip animation",
   "original": "i11_2497",
   "page_count": 4,
   "order": 641,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "We present in this paper a study on auditory feature spaces for speech-driven face animation. The goal is to provide solid analytic ground to underscore the description capability of some well-known features with relation to lipsync. A set of various audio features describing the temporal and spectral shape of speech signal has been computed on annotated audio extracts. The dimension of the input feature space has been reduced with PCA and the contribution of each input feature is investigated to determine the more descriptive. The resulting feature space is quantitatively and qualitatively analyzed for the description of acoustic units (phonemes, visemes, etc.) and we demonstrate that the use of some low-level features in addition to MFCC increases the relevance of the feature space. Finally, we evaluate the stability of these features w.r.t. the gender of the speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-639"
  },
  "loweimi11_interspeech": {
   "authors": [
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Seyed Mohammad",
     "Ahadi"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ]
   ],
   "title": "Phase-only speech reconstruction using very short frames",
   "original": "i11_2501",
   "page_count": 4,
   "order": 642,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "This paper aims to investigate potentials existing in speech phase spectrum. We observed that the window shape and scale incompatibility error (SIE) are two important factors which deeply influence the quality of phase-only reconstructed speech. After evaluating effects of different windows, we found Chebyshev window with dynamic range of 25 to 30 dB the best option. Inspiring from Hilbert transform relations, we removed the SIE and found the reason for quality improvement of ordinary phase-only reconstructed speech by frame length extension. Results show that phase spectrum, even in very short frame lengths such as 16 ms, can be highly informative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-640"
  },
  "skogstad11_interspeech": {
   "authors": [
    [
     "Trond",
     "Skogstad"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ]
   ],
   "title": "Frequency-warped and stabilized time-varying cepstral coefficients",
   "original": "i11_2505",
   "page_count": 4,
   "order": 643,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "This paper presents a set of cepstral parameters based on timevarying linear prediction. The lattice filter structure is utilized to accommodate efficient stabilization of models and a Bark-like warped frequency scale. As the proposed cepstral features are based on non-stationary spectral analysis there is a potential for complementary information not captured in conventional features. In classification and recognition experiments, the proposed features are shown to improve performance when augmenting MFCCs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-641"
  },
  "william11_interspeech": {
   "authors": [
    [
     "Freddy",
     "William"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Using human perception for automatic accent assessment",
   "original": "i11_2509",
   "page_count": 4,
   "order": 644,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "In this study, a new algorithm for automatic accent evaluation of native and non-native speakers is presented. The proposed system consists of two main steps: alignment and scoring. At the alignment step, the speech utterance is processed using a Weighted Finite State Transducer (WFST) based technique to automatically estimate the pronunciation errors. Subsequently, in the scoring step a Maximum Entropy (ME) based technique is employed to assign perceptually motivated scores to pronunciation errors. The combination of the two steps yields an approach that measures accent based on perceptual impact of pronunciation errors, and is termed as the Perceptual WFST (P-WFST). The P-WFST is evaluated on American English (AE) spoken by native and non-native (native speakers of Mandarin-Chinese) speakers from the CU-Accent corpus. The proposed P-WFST algorithm shows higher and more consistent correlation with human evaluated accent scores, when compared to the Goodness Of Pronunciation (GOP) algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-642"
  },
  "molina11_interspeech": {
   "authors": [
    [
     "Carlos",
     "Molina"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Néstor Becerra",
     "Yoma"
    ]
   ],
   "title": "A study of the effectiveness of articulatory strokes for phonemic recognition",
   "original": "i11_2513",
   "page_count": 4,
   "order": 645,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "This paper explores a framework to incorporate articulatory movement information into a classical ASR scheme based on the concept of articulatory stroke. Articulatory stroke is a geometrical segmental unit which corresponds to a target approaching-releasing articulatory gesture. It has been shown that critical and non-critical (i.e., secondary or dummy) articulatory gestures can be classified with about 88% accuracy using the stroke parameters. Phonetic recognition accuracy is also investigated by augmenting the conventional MFCC features with the articulatory stroke features (obtained using the MOCHA corpus). It is found that the phonetic recognition accuracy increases 15% with respect to the best result using the ordinary MFCC parameters only. This provides supporting evidence for the usefulness of the articulatory stroke representation of articulatory movements not only for speech production description but also for automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-643"
  },
  "okamoto11_interspeech": {
   "authors": [
    [
     "Erika",
     "Okamoto"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Auditory filterbank improves voice morphing",
   "original": "i11_2517",
   "page_count": 4,
   "order": 646,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "This paper presents a new method for vocal tract length (VTL) estimation and normalization based on a gammachirp auditory filterbank (GCFB) to improve the sound quality in voice morphing. VTL ratios between 28 speakers were estimated based on the spectral distances for all permutations (756 = 28P27) . The VTL estimation using the mel-frequency filterbank (MFFB), which is a preprocessor for calculating MFCCs commonly used in ASR, was also evaluated for comparison. The results of subjective listening tests of morphed voice sounds with and without VTL normalization are also reported. The objective and subjective results indicate that VTL normalization is essential for voice morphing, and the proposed GCFB-based method outperforms the MFCC-based method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-644"
  },
  "fuchs11_interspeech": {
   "authors": [
    [
     "Anna Katharina",
     "Fuchs"
    ],
    [
     "Christian",
     "Feldbauer"
    ],
    [
     "Michael",
     "Stark"
    ]
   ],
   "title": "Monaural sound localization",
   "original": "i11_2521",
   "page_count": 4,
   "order": 647,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "The principles of human sound localization imply binaural (interaural level and time difference) as well as monaural cues. The latter are captured by the head-related transfer functions (HRTFs), which describe the direction-dependent, spectral shaping of the incident sound wave, and can be exploited to determine the direction. In this paper an accurate talker localization strategy in the horizontal plane using the signal of only one microphone is presented. The sound localization method is developed based on a set of HRTF measurements taken from a dummy head and a statistical model of speech. High-dimensional spectral features (STFT coefficients) are taken and the direction of the sound source is evaluated with Gaussian mixture models (GMMs) using a maximum likelihood (ML) framework. An evaluation of the developed method in a synthetic test environment yields excellent localization results and leads to a promising approach which can be further investigated in future research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-645"
  },
  "fukui11_interspeech": {
   "authors": [
    [
     "Masahiro",
     "Fukui"
    ],
    [
     "Shigeaki",
     "Sasaki"
    ],
    [
     "Yusuke",
     "Hiwasaki"
    ],
    [
     "Kurihara",
     "Sachiko"
    ],
    [
     "Yoichi",
     "Haneda"
    ]
   ],
   "title": "Dual-mode AVQ coding based on spectral masking and sparseness detection for ITU-t g.711.1/g.722 super-wideband extensions",
   "original": "i11_2525",
   "page_count": 4,
   "order": 648,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "ITU-T Recommendations G.711.1 Annex D and G.722 Annex B, which are super-wideband (50.14,000 Hz) extensions to G.711.1 and G.722, have been recently standardized. This paper introduces a new coding method proposed and employed in the above ITU-T standards. The proposed coding method employs an adaptive spectral masking of the algebraic vector quantization (AVQ) for MDCT-domain non-sparse signals. The adaptive spectral masking is switched on and off based on MDCT-domain sparseness analysis. When the target MDCT coefficients are categorized as non-sparse, masking level of the target MDCT coefficients is adaptively controlled using spectral envelope information. The performance of the proposed method as a part of the ITU-T G.711.1 Annex D is evaluated in comparison with the ordinary AVQ. Subjective listening test results show that the proposed method improves the sound quality more than 0.1 points with a five grade scale in average of speech, music and mixed content, and the significance of the improvement is validated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-646"
  },
  "taufique11_interspeech": {
   "authors": [
    [
     "Azar",
     "Taufique"
    ],
    [
     "Kumaran",
     "Vijayasankar"
    ],
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Marco",
     "Tacca"
    ],
    [
     "Andrea",
     "Fumagalli"
    ]
   ],
   "title": "Phone impact based speech transmission technique for reliable speech recognition in poor wireless network conditions",
   "original": "i11_2529",
   "page_count": 4,
   "order": 649,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "This paper presents a preliminary study on an effective differentiable network service technique to achieve improved speech recognition under severely poor wireless channel conditions, by leveraging multiple priority levels applied to speech classes. Each speech class is assigned a different priority level based on its level of impact on speech recognition performance. Based on their priority level, frames of each speech class are given distinct levels of network quality of service (QoS) to satisfy the delay requirement and enable speech recognition at the receiver. This proposed Phone Impact (PI) based priority class is compared to the Voiced/Unvoiced (VU) based priority class in this study. The experimental results prove that the proposed scheme is effective at providing wireless network service for robust speech recognition under poor channel conditions, showing up to 2.67 dB and 5.93 dB lower Signal to Noise Ratio (SNR) operating regions compared to the VU based and plain protocols respectively. The PI based method also shows acceptable WERs at lower SNRs where VU and plain systems significantly degrade in speech recognition performance in case of retry limit of 6.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-647"
  },
  "zhou11b_interspeech": {
   "authors": [
    [
     "Jingting",
     "Zhou"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Automatic speech codec identification with applications to tampering detection of speech recordings",
   "original": "i11_2533",
   "page_count": 4,
   "order": 650,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "In this paper we explored many versions of CELP codecs and studied different codebooks they use to encode noisy part of residual. Taking advantage of noise patterns they generated, an algorithm was proposed to detect GSM-AMR,EFR,HR and SILK codecs. Then it's extended to identify subframe offset to do tampering detection of cellphone speech recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-648"
  },
  "lee11g_interspeech": {
   "authors": [
    [
     "Chang-Heon",
     "Lee"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "A hybrid quasi-harmonic/CELP wideband speech coding scheme for unit selection TTS synthesis",
   "original": "i11_2537",
   "page_count": 4,
   "order": 651,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "This paper suggests a new wideband speech coding model to efficiently compress acoustic inventories for concatenative unit selection text-to-speech (TTS) synthesis system. To fulfill the requirements of TTS synthesizer such as partial segment decoding and random access capability, a non-predictive scheme was adopted which combines the adaptive Quasi-Harmonic Model (aQHM) with the innovative codebook (ICB) model. aQHM plays a major role in modeling pitch harmonic components, and ICB compensates, in a closed-loop way, for the modeling error of aQHM. This is especially important in transient or unvoiced regions. To further improve the coding efficiency, a hybrid coding framework is also suggested. Results from a large French speech database show that the proposed algorithm provides similar speech quality to the high quality AMR-WB codec while it supports the random access capability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-649"
  },
  "ramo11_interspeech": {
   "authors": [
    [
     "Anssi",
     "Rämö"
    ],
    [
     "Henri",
     "Toukomaa"
    ]
   ],
   "title": "Voice quality characterization of IETF opus codec",
   "original": "i11_2541",
   "page_count": 4,
   "order": 652,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "This paper discusses the voice quality of Opus, IETF driven open source voice and audio codec. Opus is a newly developed hybrid codec based on SILK and CELT codec technologies. Opus construction is described shortly in this paper and more importantly its optimal operating points are found out based on the listening test results. Voice quality was evaluated with two subjective listening tests. Industry standard voice codecs: 3GPP AMR and AMR-WB, and ITU-T G.718B, G.722.1C and G.719 as well as direct signals were used as voice quality references.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-650"
  },
  "pedersen11b_interspeech": {
   "authors": [
    [
     "C. F.",
     "Pedersen"
    ]
   ],
   "title": "Leja ordering LSFs for accurate estimation of predictor coefficients",
   "original": "i11_2545",
   "page_count": 4,
   "order": 653,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "Linear prediction (LP) is the most prevalent method for spectral modelling of speech, and line spectrum pair (LSP) decomposition is the standard method to robustly represent the coefficients of LP models. Specifically, the angles of LSP polynomial roots, i.e. line spectrum frequencies (LSFs), encode exactly the same information as LP coefficients. The conversion of LP coefficients to LSFs and back, has received considerable attention since mid 1970s when LSFs were introduced.\n",
    "The present paper demonstrates how Leja ordering LSFs reduce amplification of rounding errors when converting LSFs to LP coefficients. The theory behind Leja ordering and the LSFs to LP coefficients conversion is presented. To supplement theory, numerical experiments illustrate the accuracy gain achieved by Leja ordering LSFs prior to conversion. Accuracy is measured as the root mean square deviation between estimated coefficient vectors with and without prior Leja ordering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-651"
  },
  "gong11_interspeech": {
   "authors": [
    [
     "Qipeng",
     "Gong"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Improved quality for conversational voIP using path diversity",
   "original": "i11_2549",
   "page_count": 4,
   "order": 654,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "In Voice-over-IP, the quality of interactive conversation is important to users. Quality-based playout buffering seeks an optimum balance between delay and loss. However, such a scheme still suffers when packet losses are bursty. Path diversity can alleviate the effect of losses and improve perceived quality by providing redundancy. In this paper, a new scheme is proposed which evaluates the performance of both paths. We consider three different path diversity schemes. The playout scheduling algorithms are designed based on conversational quality including both calling quality and interactivity. The simulation results show the efficacy of our algorithms in correcting for losses (isolated and burst) and improving perceived conversational quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-652"
  },
  "khan11_interspeech": {
   "authors": [
    [
     "Abdul Hannan",
     "Khan"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Tree encoding for the ITU-t g.711.1 speech coder",
   "original": "i11_2553",
   "page_count": 4,
   "order": 655,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "This paper examines enhancement to ITU-T Recommendation G.711.1 PCM wideband extension speech coder. To further improve the core lower-band coding performance the use of vector quantization and delayed decision coding is studied. A particular case of delayed decision coding, tree encoding, is implemented in the above standard. The bitstream is compatible with both the legacy G.711 and the G.711.1 decoder. PESQ (ITU-T P.862, Perceptual Evaluation of Speech Quality) is used to evaluate the performance. Both the vector quantizer and tree encoder have better performance than the original core layer encoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-653"
  },
  "wang11i_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "Parallel and hierarchical decision making for sparse coding in speech recognition",
   "original": "i11_2557",
   "page_count": 4,
   "order": 656,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "Sparse coding exhibits promising performance in speech processing, mainly due to the large number of bases that can be used to represent speech signals. However, the high demand for computational power represents a major obstacle in the case of large datasets, as does the difficulty in utilising information scattered sparsely in high dimensional features. This paper reports the use of an online dictionary learning technique, proposed recently by the machine learning community, to learn large scale bases efficiently, and proposes a new parallel and hierarchical architecture to make use of the sparse information in high dimensional features. The approach uses multilayer perceptrons (MLPs) to model sparse feature subspaces and make local decisions accordingly; the latter are integrated by additional MLPs in a hierarchical way for making global decisions. Experiments on the WSJ database show that the proposed approach not only solves the problem of prohibitive computation with large-dimensional sparse features, but also provides better performance in a frame-level phone prediction task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-654"
  },
  "chiang11b_interspeech": {
   "authors": [
    [
     "Chen-Yu",
     "Chiang"
    ],
    [
     "Jyh-Her",
     "Yang"
    ],
    [
     "Ming-Chieh",
     "Liu"
    ],
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Sin-Horn",
     "Chen"
    ]
   ],
   "title": "A new model-based Mandarin-speech coding system",
   "original": "i11_2561",
   "page_count": 4,
   "order": 657,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "In this paper, a new model-based Mandarin-speech coding system is proposed. It employs a prosody-enriched ASR with a hierarchical prosodic model (HPM) to generate from the input speech enriched transcriptions, including linguistic features, prosodic tags and spectral parameters in the encoder. By sending these features to the decoder, we can first reconstruct the prosodic-acoustic features of syllable pitch contour, syllable duration, syllable energy level, and inter-syllable pause duration by HPM using the linguistic features and prosodic tags; and then combined with spectral parameters to reconstruct the input speech signal by an HMM-based speech synthesizer. Experimental results show that the reconstructed speech has good quality at a low data rate of 543 bits/s.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-655"
  },
  "cerva11_interspeech": {
   "authors": [
    [
     "Petr",
     "Cerva"
    ],
    [
     "Karel",
     "Palecek"
    ],
    [
     "Jan",
     "Silovsky"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Using unsupervised feature-based speaker adaptation for improved transcription of spoken archives",
   "original": "i11_2565",
   "page_count": 4,
   "order": 658,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "This paper deals with unsupervised feature-based speaker adaptation techniques. The goal is to design an optimal adaptation approach for improving the recognition accuracy of a LVCSR system developed for automatic transcription of large archives of spoken Czech (e.g. the archive of the parliament talks, historical archives of Czech broadcast stations, etc.) For this purpose, several modifications of VTLN and CMLLR techniques were investigated and combined together. Our study focuses on the application of the adaptation methods in the recognition process as well as in building a normalized acoustic model within the speaker adaptive training scheme. The methods were evaluated experimentally on a large amount of various data (with total number 93k words). The resulting two-step adaptation scheme yields a significant WER reduction from 17.8% to 14.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-656"
  },
  "fischer11_interspeech": {
   "authors": [
    [
     "Volker",
     "Fischer"
    ],
    [
     "Siegfried",
     "Kunzmann"
    ]
   ],
   "title": "Online speaker adaptation with pre-computed FMLLR transformations",
   "original": "i11_2569",
   "page_count": 4,
   "order": 659,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "This paper presents a memory efficient single pass speech recognizer that makes use of pre-computed FMLLR transformations for online speaker adaptation. For that purpose we apply unsupervised segment clustering to the training corpus, create a transformation matrix for each cluster, and train a text-independent Gaussian mixture classifier for cluster selection during runtime. We use the RWTH Aachen University open source speech recognition toolkit for evaluation and compare the results to a standard speaker adaptive two pass decoding strategy. Results indicate that the method improves single pass recognition in VTLN feature space almost without overhead due to cluster selection, and show a relative improvement of up to 15 percent over speaker adaptative decoding, if only little data is available for unsupervised online adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-657"
  },
  "giuliani11_interspeech": {
   "authors": [
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "Instantaneous speaker adaptation through selection and combination of fMLLR transformation matrices",
   "original": "i11_2573",
   "page_count": 4,
   "order": 660,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "This paper addresses instantaneous speaker adaptation, based on feature-space maximum likelihood linear regression (fMLLR), in the context of an automatic transcription task. We investigate the use of fMLLR-based adaptation when the need of a preliminary decoding pass for a speech segment is removed, as sufficient statistics for adaptation parameter estimation are gathered with respect to a Gaussian mixture model. To cope with limited adaptation data, in addition of using feature-space maximum a posteriori linear regression (fMAPLR), an investigation is conducted where the transformation matrix to be applied to the speech segment is estimated through selection and combination of pre-computed fMLLR transformation matrices. For speaker adaptively trained acoustic models results of recognition experiments show that the proposed approach is moderately better than fMLLR but not as good as fMAPLR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-658"
  },
  "song11_interspeech": {
   "authors": [
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Yunkeun",
     "Lee"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Joint bilinear transformation space based maximum a posteriori linear regression adaptation using prior with variance function",
   "original": "i11_2577",
   "page_count": 4,
   "order": 661,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "This paper proposes a new joint maximum a posteriori linear regression (MAPLR) adaptation using single prior distribution with a variance function in bilinear transformation space (BITS). There are two indirect adaptation methods based on the linear transformation in BITS and these are tightly coupled by joint MAP-based estimation. The proposed method not only has the scalable parameters but also is based on only one prior distribution, unlike the conventional joint MAP-MAPLR method with two priors. Experimental results, especially for small amount of adaptation data, show the synergy between two indirect BITS-based methods over other methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-659"
  },
  "sanand11_interspeech": {
   "authors": [
    [
     "D. R.",
     "Sanand"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "A study on combining VTLN and SAT to improve the performance of automatic speech recognition",
   "original": "i11_2581",
   "page_count": 4,
   "order": 662,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "In this paper, we present ideas to combine VTLN and SAT to improve the performance of automatic speech recognition. We show that VTLN matrices can be used as SAT transformation matrices in recognition, though the training still follows conventional SAT. This will be useful when there is very little adaptation data and the SAT transformation matrix can not be estimated to perform the required adaptation. We also present a study to understand whether VTLN can be performed after SAT and whether such a combination is better than the conventional approach, where VTLN is performed before SAT. Finally, we present a novel approach to perform VTLN by using VTLN matrices in cascade. This allows us to include warping-factors that are not included in the initial search space. We show through recognition experiments that these combinations improve the performance of ASR, with major gains in the mis-matched train and test speaker conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-660"
  },
  "tsao11_interspeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Incorporating regional information to enhance MAP-based stochastic feature compensation for robust speech recognition",
   "original": "i11_2585",
   "page_count": 4,
   "order": 663,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "In this study, we propose an environment structuring framework to facilitate suitable prior density preparation for MAP-based stochastic feature matching (SFM) for robust speech recognition. We use a two-stage hierarchical structure to construct the environment structuring framework to characterize the regional information of various speaker and speaking environments. With the regional information, we derive three types of prior densities, namely clustered prior, sequential prior, and hierarchical prior densities. We also designed an integrated prior density to combine the advantages of the above three prior densities. From our experimental results on the Aurora-2 task, we confirmed that with regional information, we can obtain more suitable prior densities and thus enhance the performance of MAP-based SFM. Moreover, we found that by using the integrated prior density, which integrates multiple knowledge sources from the other three, MAP-based SFM gives the best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-661"
  },
  "ghai11_interspeech": {
   "authors": [
    [
     "Shweta",
     "Ghai"
    ],
    [
     "Rohit",
     "Sinha"
    ]
   ],
   "title": "A study on the effect of pitch on LPCC and PLPC features for children's ASR in comparison to MFCC",
   "original": "i11_2589",
   "page_count": 4,
   "order": 664,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "In this work, following our previous studies, we study and quantify the effect of pitch on LPCC and PLPC features and explore their efficacy for children's mismatched ASR in comparison to MFCC. Our analysis shows that, unlike MFCC, LPCC feature has no major influence of pitch variations. On the other hand, similar to MFCC, though PLPC is also found to be significantly effected by pitch variations but comparatively to a lesser extent. However, after explicit pitch normalization of children's speech, MFCC is found to result in the best children's speech recognition performance on adults' speech trained models in comparison to LPCC and PLPC features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-662"
  },
  "jouvet11_interspeech": {
   "authors": [
    [
     "Denis",
     "Jouvet"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "About handling boundary uncertainty in a speaking rate dependent modeling approach",
   "original": "i11_2593",
   "page_count": 4,
   "order": 665,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "Variability dependent modeling provides a way of handling the impact of some variability sources in the modeling. In many cases, the variability factor is estimated in a deterministic way, leading to a mere selection of the most adequate model. However, there are always some uncertainty in the estimation of the variability sources which may induce a sub optimal model selection. This paper considers the context of a speaking rate dependent modeling approach, and shows that the uncertainty on the speech segment boundaries, which translates in an uncertainty on the speaking rate estimation, can be handled in the training process and/or in the decoding process. Preliminary results reported here are promising for dealing with variability estimation uncertainty.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-663"
  },
  "wu11b_interspeech": {
   "authors": [
    [
     "Ji",
     "Wu"
    ],
    [
     "Zhiyang",
     "He"
    ],
    [
     "Ping",
     "Lv"
    ]
   ],
   "title": "An active learning approach to task adaptation",
   "original": "i11_2597",
   "page_count": 4,
   "order": 666,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "An active learning approach is proposed to automatically analyze speech recognition tasks and select particularly useful adaptation data. In this approach, the distribution of task data is first estimated, which is a combination of two distributions based on N-best recognition results and low confidence data. After that, a subset of adaptation data is selected in two stages using a greedy algorithm according to the estimated distribution. Low confidence data are firstly selected and manually labeled. Then, the high confidence data are selected based on the top-best recognition results, which are also used as labels for the adaptation. The experimental results of the subsequent task adaptation show that the proposed active learning approach can effectively select the useful data to improve the overall performance of the system. The word accuracy is close to, and even exceed, the performance of supervised adaptation using all of the data, when only 10%.20% of the total data need to be manually labeled.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-664"
  },
  "joshi11b_interspeech": {
   "authors": [
    [
     "Vikas",
     "Joshi"
    ],
    [
     "Raghavendra",
     "Bilgi"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "C.",
     "Benitez"
    ],
    [
     "L.",
     "Garcia"
    ]
   ],
   "title": "Efficient speaker and noise normalization for robust speech recognition",
   "original": "i11_2601",
   "page_count": 4,
   "order": 667,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "In this paper, we describe a computationally efficient approach for combining speaker and noise normalization techniques. In particular, we combine the simple yet effective Histogram Equalization (HEQ) for noise compensation with Vocal-tract length normalization (VTLN) for speaker-normalization. While it is intuitive to remove noise first and then perform VTLN, this is difficult since HEQ performs noise compensation in the cepstral domain, while VTLN involves warping in spectral domain. In this paper, we investigate the use of the recently proposed T-VTLN approach to speaker normalization where matrix transformations are directly applied on cepstral features. We show that the speaker-specific warp-factors estimated even from noisy speech using this approach closely match those from clean-speech. Further, using sub-band HEQ (S-HEQ) and T-VTLN we get a significant relative improvement of 20% and an impressive 33.54% over baseline in recognition accuracy for Aurora-2 and Aurora-4 task respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-665"
  },
  "winkler11b_interspeech": {
   "authors": [
    [
     "Thomas",
     "Winkler"
    ]
   ],
   "title": "How realistic is artificially added noise?",
   "original": "i11_2605",
   "page_count": 4,
   "order": 668,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "Evaluations of algorithms for robust automatic speech recognition (ASR) are often based on artificial noisy speech instead of realistic noisy speech. In this paper we compare the ASR performance of speech with artificial additive noise to the performance of realistic noisy speech. All data was recorded during the same recording campaign and with nearly identical channel characteristics. The simulation process takes into account all major characteristics of the noisy reference data. Clean speech, noisy speech and simulated speech are compared for different aspects of robust ASR including noise reduction by Spectral Subtraction and the ETSI robust front end. The results show, that artificial noisy speech even in very controlled simulation environments is not very similar and not a full substitute for realistic noisy data. While the tendencies of the improvement for artificial and realistic data are similar for the evaluated approaches, the magnitude can be quite different.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-666"
  },
  "unoki11_interspeech": {
   "authors": [
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Rico",
     "Petrick"
    ],
    [
     "Shota",
     "Morita"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Voice activity detection in MTF-based power envelope restoration",
   "original": "i11_2609",
   "page_count": 4,
   "order": 669,
   "p1": "2609",
   "pn": "2612",
   "abstract": [
    "This paper reports comparative evaluations of conventional voice activity detection (VAD) methods in reverberant environments. Both conventional and standard (G.729) methods are discussed. In general, these methods work well under clean conditions, but their performance is drastically affected by reverberation. Preliminary comparative evaluations showed that the false acceptance rate (FAR) is significantly increased due to the false rejection rate (FRR) being moderately increased by reverberation. We therefore developed a method using MTF-based power envelope restoration to improve the robustness of VAD in reverberant environments. This restoration method can blindly restore the power envelope of reverberant speech based on the MTF concept. The proposed method consists of an MTF-based restoration method as the front end and a conventional VAD method as the final decision. Experimental results demonstrated that the proposed method is superior to conventional methods with regard to robustness and providing accurate VAD (reducing both FAR and FRR) in reverberant environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-667"
  },
  "espi11_interspeech": {
   "authors": [
    [
     "Miquel",
     "Espi"
    ],
    [
     "Shigeki",
     "Miyabe"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Nobutaka",
     "Ono"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Using spectral fluctuation of speech in multi-feature HMM-based voice activity detection",
   "original": "i11_2613",
   "page_count": 4,
   "order": 670,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "Observation of speech spectrum leads to the fact that speech has a specific spectral fluctuation pattern both along time and frequency. In this paper, we integrate the usage of this nature in a multi-feature approach for voice activity detection. The effect of separating such specific spectral fluctuation using multi-stage HPSS (Harmonic-Percussive Sound Separation) has been analyzed over conventional features in voice activity detection, reducing frame-wise detection error by up to 78%, depending on the SNR conditions and noise type. The multi-feature approach has been tested using Hidden Markov Models to model the features stream as a sequence, which has out-performed standard and similar VAD proposals in utterance-based tests intended for automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-668"
  },
  "mehta11_interspeech": {
   "authors": [
    [
     "Kannu",
     "Mehta"
    ],
    [
     "Chau Khoa",
     "Pham"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Linear dynamic models for voice activity detection",
   "original": "i11_2617",
   "page_count": 4,
   "order": 671,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "In this paper, we propose a robust voice activity detection method based on long-term stationarity (LTS) of the speech signal. The approach is motivated by the fact that noise, in time-domain, is relatively more stationary as compared to speech. We describe the use of Linear dynamic models (LDMs) as a measure of calculating the long-term stationarity of the signal and propose a voice activity detector by comparing the degree of stationarity at different times in the signal. We evaluate the proposed approach in presence of five types of noises at various SNR levels. Comparison with G.729-Annex B, order statistics filters (OSF) VAD, windowed autocorrelation lag energy (WALE), and autocorrelation zero-crossing rate (AZR) schemes demonstrates that the accuracy of the LTSbased VAD scheme averaged over all noises and all SNRs is 3.94% better than that obtained by the best among the considered VAD schemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-669"
  },
  "pohjalainen11_interspeech": {
   "authors": [
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Detection of shouted speech in the presence of ambient noise",
   "original": "i11_2621",
   "page_count": 4,
   "order": 672,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "This study focuses on the detection of shouted speech in realistic noisy conditions. An automatic system based on modified mel frequency cepstral coefficient (MFCC) feature extraction and Gaussian mixture model (GMM) classification is developed. The performance of the automatic system is compared against human perception measured by a listening test. At moderate noise levels, the automatic system outperforms humans. In severe conditions, classification by humans is clearly better.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-670"
  },
  "fukuda11b_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Osamu",
     "Ichikawa"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Breath-detection-based telephony speech phrasing",
   "original": "i11_2625",
   "page_count": 4,
   "order": 673,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "ASR has long attracted attention for call center monitoring systems. In the ASR technology for call center conversations, the system usually divides an input signal into separate utterances and eliminates the unneeded silence parts of the signal before doing ASR processing on the detected utterances. This means the input signal should be split into utterances of the proper length for both ASR performance and readability. However, typical VAD techniques sometimes generate overly long speech segments because they are focused only on the length of the pause (non-speech) between sentences. In contrast, it is shown that speakers typically take breaths for when speaking more than one sentence or long sentences. These breaths are highly correlated with the major prosodic breaks. In this paper, we focus on the breath events in the pause intervals and attempt to split the input signal into utterances by detecting the breathing events. The proposed method leverages acoustic information that is specialized for breathing sounds, which led to a two-step approach to detect the breath events with an accuracy of 97.4%. Also, the proper speech phrasing based on breath events improved word error rate in ASR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-671"
  },
  "kim11g_interspeech": {
   "authors": [
    [
     "Gibak",
     "Kim"
    ]
   ],
   "title": "Multi-channel voice activity detection based on conic constraints",
   "original": "i11_2629",
   "page_count": 4,
   "order": 674,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "Unlike single microphone techniques for voice activity detection (VAD), multi-microphone signal processing usually exploits the spatial information of signals received at multiple microphones. In this paper, we propose a VAD algorithm based on conic constraints to achieve robustness against the direction of arrival (DOA) estimation error. The proposed algorithm uses the phase vector as feature and detects the presence of the target speech by comparing the angles between the phase vector of the multi-microphone input signal and two mean phase vectors for target speech+interference period and interference-only period. The proposed algorithm was tested with simulation data generated by real-measured impulse response for seven uniformly distributed microphones. The simulation results showed that the proposed algorithm presents a reliable VAD metric in the presence of competing speech. The results also supported the robustness of the proposed algorithm against the DOA estimation error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-672"
  },
  "petsatodis11_interspeech": {
   "authors": [
    [
     "Theodoros",
     "Petsatodis"
    ],
    [
     "Fotios",
     "Talantzis"
    ],
    [
     "Christos",
     "Boukis"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Ramjee",
     "Prasad"
    ]
   ],
   "title": "Multi-sensor voice activity detection based on multiple observation hypothesis testing",
   "original": "i11_2633",
   "page_count": 4,
   "order": 675,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "Voice Activity Detection (VAD) in acoustic environments remains a challenging task due to potentially adverse noise and reverberation conditions. The problem becomes even more difficult when the microphones used to detect speech reside far from the speaker. An unsupervised VAD scheme is presented in this paper. The system is based on processing signals captured by multiple farfield sensors in order to integrate spatial information in addition to the frequency content available at a single channel recording. To decide upon the presence or absence of speech the system employs a modified multiple observation hypothesis that tests at each sensor the probability of having an active speaker and then fuses the decisions. To minimize misdetections and enhance the performance of the hypothesis test a computationally efficient forgetting scheme is also employed. Simulations conducted in several artificial environments illustrate that significant improvements in performance can be expected from the proposed scheme when compared to systems of similar philosophy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-673"
  },
  "gao11_interspeech": {
   "authors": [
    [
     "Chao",
     "Gao"
    ],
    [
     "Guruprasad",
     "Saikumar"
    ],
    [
     "Saurabh",
     "Khanwalkar"
    ],
    [
     "Avi",
     "Herscovici"
    ],
    [
     "Anoop",
     "Kumar"
    ],
    [
     "Amit",
     "Srivastava"
    ],
    [
     "Premkumar",
     "Natarajan"
    ]
   ],
   "title": "Online speech activity detection in broadcast news",
   "original": "i11_2637",
   "page_count": 4,
   "order": 676,
   "p1": "2637",
   "pn": "2640",
   "abstract": [
    "In this paper, we investigate the important implications of realtime processing to the design of a speech activity detection (SAD) system, with a focus on the impact of the unique constraints posed by online automatic speech recognition. Our investigation is built on a real-life application of speech technology, the BBN Broadcast Monitoring System (BMS), which encapsulates a real-time automatic rich transcription system. We propose a segmentation method that is capable of variable scale speech boundary detection in an online SAD system and evaluate how different granularities of boundary detection impact the performance of speech-to-text (STT) and speaker diarization. In addition, the interactions between STT and speaker diarization are evaluated and mechanisms for trading off the performance of these two system components are studied. In our experiment, the segmentation mechanism in the proposed SAD system reduces error rates of STT and speaker diarization by 2.4% and 9.5% relatively, compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-674"
  },
  "reich11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Reich"
    ],
    [
     "Felix",
     "Putze"
    ],
    [
     "Dominic",
     "Heger"
    ],
    [
     "Joris",
     "Ijsselmuiden"
    ],
    [
     "Rainer",
     "Stiefelhagen"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "A real-time speech command detector for a smart control room",
   "original": "i11_2641",
   "page_count": 4,
   "order": 677,
   "p1": "2641",
   "pn": "2644",
   "abstract": [
    "In this work we present an always-on speech recognition system that discriminates spoken commands directed to the system from other spoken input. For discrimination we integrated various features ranging from prosodic cues and decoding features to linguistic information. The resulting \"Speech Command Detector\" provides intuitive hands-free user interaction in a Smart Control Room environment where voice commands are directed toward a large interactive display. Based on a recognition vocabulary of 259 words with more than 10k possible commands, the Speech Command Detector detected 88.3% of the commands correctly maintaining a very low False Positive Rate of 1.5%. In a crossdomain setup the system was evaluated on a Star Trek episode. With only minor adjustments, our system achieved very promising results with 91.2% command detection rate at a False Positive Rate of 1.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-675"
  },
  "chuangsuwanich11_interspeech": {
   "authors": [
    [
     "Ekapol",
     "Chuangsuwanich"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Robust voice activity detector for real world applications using harmonicity and modulation frequency",
   "original": "i11_2645",
   "page_count": 4,
   "order": 678,
   "p1": "2645",
   "pn": "2648",
   "abstract": [
    "The task of robustly detecting distant speech in low SNR environments for automatic speech recognition is examined using a two-stage approach based on two distinguishing features of speech, namely harmonicity and modulation frequency (MF). A modified metric for harmonicity is used as a gating function to a set of parallel classifiers that incorporate MFs computed on different frequency bands. Performance is evaluated on both the frame-level discriminative power and also the system level ASR results on a real-world robotic forklift task. Compared to other previously proposed features such as relative spectral entropy, and classification strategies involving MFs, the combined approach shows good generalization across different kinds of dynamic noise conditions, and obtains a significant improvement on the false alarm rate at low speech miss rate settings. The overall ASR results also improved significantly compared to the ESTI AMR-VAD2, while reducing the number of false alarms by a factor of two.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-676"
  },
  "dekens11_interspeech": {
   "authors": [
    [
     "Tomas",
     "Dekens"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "On noise robust voice activity detection",
   "original": "i11_2649",
   "page_count": 4,
   "order": 679,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "In this paper, we show that the performance of voice activity detection algorithms (VAD) can be highly dependent on the type of background noise and we introduce a new VAD algorithm that is based on relative energy measurements in different frequency bands. The obtained experimental results are compared to the results obtained with two other spectrum-based VADs and it is concluded that a VAD, configured to use around 3 frequency bands can cope best with a large variety of background sounds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-677"
  },
  "lu11b_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Ryosuke",
     "Isotani"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Adaptive regularization framework for robust voice activity detection",
   "original": "i11_2653",
   "page_count": 4,
   "order": 680,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "Traditional VAD algorithms work well under clean conditions, their performance however decreases drastically in noisy environments. We have investigated the tradeoff between false acceptance rate (FAR) and false rejection rate (FRR) in VAD with the consideration of noise reduction and speech distortion problem in speech enhancement, and proposed a regularization framework for noise reduction in designing VAD algorithms. In the framework, the balance between FAR and FRR was implicitly controlled by using a regularization parameter. In addition, the regularization was done in a reproducing kernel Hilbert space (RKHS) which made it easy to apply a nonlinear transform function via \"kernel trick\" for noise reduction. Under this framework, a better tradeoff between FAR and FRR was obtained in VAD. Considering the non-stationarity property of speech and noise, in this study, an adaptive regularization framework was further developed in which the regularization parameter was changed adaptively according to local variations of the signal to noise ratio (SNR). We tested our algorithm on VAD experiments, and compared it with several typical VAD algorithms. The results showed that the proposed algorithm could be used to improve the robustness of VAD.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-678"
  },
  "koriyama11_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "On the use of extended context for HMM-based spontaneous conversational speech synthesis",
   "original": "i11_2657",
   "page_count": 4,
   "order": 681,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "This paper addresses an issue of prosodic variability of spontaneous speech in HMM-based spontaneous conversational speech synthesis. We propose an extended context set including information peculiar to spontaneous speech derived from the annotation data embedded in a large-scale database of spontaneous Japanese. We show the effectiveness of the newly introduced contexts from the results of objective and subjective evaluation experiments. We also propose stopping criteria for decision-tree clustering to alleviate an over-fitting problem. Experimental results show that the restriction of the size of each leaf node can improve the quality of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-679"
  },
  "toutios11b_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Predicting tongue positions from acoustics and facial features",
   "original": "i11_2661",
   "page_count": 4,
   "order": 682,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "We test the hypothesis that adding information regarding the positions of electromagnetic articulograph (EMA) sensors on the lips and jaw can improve the results of a typical acoustic-to-EMA mapping system, based on support vector regression, that targets the tongue sensors. Our initial motivation is to use such a system in the context of adding a tongue animation to a talking head built on the basis of concatenating bimodal acoustic-visual units. For completeness, we also train a system that maps only jaw and lip information to tongue information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-680"
  },
  "bosch11_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Annika",
     "Hämäläinen"
    ],
    [
     "Mirjam",
     "Ernestus"
    ]
   ],
   "title": "Assessing acoustic reduction: exploiting local structure in speech",
   "original": "i11_2665",
   "page_count": 4,
   "order": 683,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "This paper presents a method to quantify the spectral characteristics of reduction in speech. Hamalainen et al. (2009) proposes a measure of spectral reduction which is able to predict a substantial amount of the variation in duration that linguistically motivated variables do not account for. In this paper, we continue studying acoustic reduction in speech by developing a new acoustic measure of reduction, based on local manifold structure in speech. We show that this measure yields significantly improved statistical models for predicting variation in duration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-681"
  },
  "andreeva11_interspeech": {
   "authors": [
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Magdalena",
     "Wolska"
    ]
   ],
   "title": "The “fortis-lenis” distinction in Bulgarian and German",
   "original": "i11_2669",
   "page_count": 4,
   "order": 684,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "The present study investigates the voicing contrast in Bulgarian and German. Analyses of two production experiments are reported. In the first experiment logatoms were constructed containing /p, t, k/ and /b, d, g/ in intervocalic position. In the second experiment one Bulgarian and one German sentence were elicited in different focus conditions resulting in different accentuation levels. Based on the obtained data we analyze the phonetic implementation of the phonological categories voiced vs. voiceless and the influence of focus condition and accentuation. It is shown, that: First, the two languages differ in the phonetic realization of /p, t, k/ but not /b, d, g/ in intervocalic position in terms of voice onset time (short lag in Bulgarian and long lag in German). Second, accentuation levels are realised in different ways in the two languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-682"
  },
  "chen11h_interspeech": {
   "authors": [
    [
     "Gang",
     "Chen"
    ],
    [
     "Jody",
     "Kreiman"
    ],
    [
     "Yen-Liang",
     "Shue"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Acoustic correlates of glottal gaps",
   "original": "i11_2673",
   "page_count": 4,
   "order": 685,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "During speech production, the vocal folds may not close completely. The resulting glottal gap (GG) or incomplete glottal closure has not been systematically studied in terms of GG acoustic and/or perceptual consequences. This paper uses high-speed imaging to investigate the relationship between GG area, source parameters, acoustic measures, and voice quality for 6 subjects. Results showed that the cepstral peak prominence (CPP) and the harmonics-tonoise ratio (HNR) are affected by GG area, indicating the presence of more spectral noise with increasing GG area. Analysis of a glide phonation from breathy to pressed for one female speaker showed that measures H1* - H2* and H1* - A3* were positively correlated with GG area under a steady fundamental frequency (F0). In some phonatory modes, increasing F0 may reduce the amplitude of vocal folds vibration, increase GG area, and produce a lower spectral tilt due to significant aspiration noise, leading to a negative correlation between GG area and the spectral tilt measure H1* - A3* .\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-683"
  },
  "bush11_interspeech": {
   "authors": [
    [
     "Brian O.",
     "Bush"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Akiko",
     "Amano-Kusumoto"
    ]
   ],
   "title": "Using a genetic algorithm to estimate parameters of a coarticulation model",
   "original": "i11_2677",
   "page_count": 4,
   "order": 686,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "We present a real-coded genetic algorithm that efficiently estimates parameters of a formant trajectory model. The genetic algorithm uses roulette-wheel selection and elitism to minimize the root mean square error between the observed formant trajectory and the model trajectory. Parameters, including vowel and consonant target values and coarticulation parameters, are estimated for a corpus of English clear and conversational CVC words. Results show consistent consonant formant targets, even when those consonants do not themselves have formant structure. We also present findings of a relationship between a coarticulation parameter and the consonant identity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-684"
  },
  "birkholz11b_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Bernd J.",
     "Kröger"
    ],
    [
     "Christiane",
     "Neuschaefer-Rube"
    ]
   ],
   "title": "Synthesis of breathy, normal, and pressed phonation using a two-mass model with a triangular glottis",
   "original": "i11_2681",
   "page_count": 4,
   "order": 687,
   "p1": "2681",
   "pn": "2684",
   "abstract": [
    "Two-mass models of the vocal folds and their variants are valuable tools for voice synthesis and analysis, but are not able to produce breathy voice qualities. The produced voice qualities usually lie between normal and pressed. The reason for this property is that the mass elements are aligned parallel to the dorso-ventral axis. Thereby, the glottis always closes simultaneously along the entire length of the vocal folds. For breathy phonation, however, the closure happens rather gradual. This article introduces a modified two-mass model with mass elements that are inclined with respect to the dorso-ventral axis as a function of the degree of abduction. In this way, the closing phase of the glottis becomes progressively more gradual when the degree of abduction is increased. This model is able to produce the continuum of voice qualities from pressed over normal to breathy voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-685"
  },
  "ghosh11_interspeech": {
   "authors": [
    [
     "Prasanta Kumar",
     "Ghosh"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Analysis of inter-articulator correlation in acoustic-to-articulatory inversion using generalized smoothness criterion",
   "original": "i11_2685",
   "page_count": 4,
   "order": 688,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "The movements of the different speech articulators are known to be correlated to various degrees during speech production. In this paper, we investigate whether the inter-articulator correlation is preserved among the articulators estimated through acoustic-toarticulatory inversion using the generalized smoothness criterion (GSC). GSC estimates each articulator separately without explicitly using any correlation information between the articulators. Theoretical analysis of inter-articulator correlation in GSC reveals that the correlation between any two estimated articulators may not be identical to that between the corresponding measured articulatory trajectories; however, based on smoothness constraints provided by the real articulatory data, we found that, in practice, the correlation among articulators is approximately preserved in GSC based inversion. To validate the theoretical analysis on inter-articulator correlation, we propose a modified version of GSC where correlations among articulators are explicitly imposed. We found that there is no significant benefit in inversion using such modified GSC, which further strengthens the conclusions drawn from the theoretical analysis of inter-articulator correlation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-686"
  },
  "kaburagi11_interspeech": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ]
   ],
   "title": "Frequency-domain representation of source-filter coupling and its effect in the production of voice",
   "original": "i11_2689",
   "page_count": 4,
   "order": 689,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "The acoustic coupling between the voice production system and the vocal tract has a significant influence on the production of voice. In this study, the coupling effect was represented using the acoustic pressure difference across the glottis, which is capable of inducing a flow, and the mean acoustic pressure in the glottis, which acts as a driving force for the vocal folds. These specific acoustic pressures were then interpreted in the frequency domain in the form of frequency responses, and incorporated into a model of the voice production system. In this framework, we were able to test the effect of source-filter coupling by filtering frequency responses. Numerical results revealed that these responses and the input impedance of the vocal tract both exhibited a dominant peak around 4 kHz. In addition, voice production simulations revealed that this high-frequency peak has a significant influence on the spatio-temporal pattern of glottal volume flow and vocal fold movements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-687"
  },
  "rasilo11_interspeech": {
   "authors": [
    [
     "Heikki",
     "Rasilo"
    ],
    [
     "Unto K.",
     "Laine"
    ],
    [
     "Okko",
     "Räsänen"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Method for speech inversion with large scale statistical evaluation",
   "original": "i11_2693",
   "page_count": 4,
   "order": 690,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "An articulatory model of speech production is created for the purpose of studying the links between speech production and perception. A computationally effective method for speech inversion in proposed, using a two-pole predictor structure in order to maintain better articulatory dynamics when compared to conventional dynamic programming methods. Preliminary tests for the effect of inversion are performed for 2500 Finnish syllables extracted from continuous speech, consisting of 125 different syllable classes. A cluster selectivity test shows that the syllables are more reliably clustered using the automatically obtained parametric representation of articulatory gestures rather than the original formant representation that is used as a starting point for the inversion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-688"
  },
  "braun11_interspeech": {
   "authors": [
    [
     "Bettina",
     "Braun"
    ],
    [
     "Sabine",
     "Geiselmann"
    ]
   ],
   "title": "Italian in the no-man's land between stress-timing and syllable-timing? speakers are more stress-timed than listeners",
   "original": "i11_2697",
   "page_count": 4,
   "order": 691,
   "p1": "2697",
   "pn": "2700",
   "abstract": [
    "How syllable-timed is Italian? We investigate two contexts for vowel reduction, unstressed syllables and syllables in polysyllabic words. In a production experiment, a large sample of speakers from Tuscany read di- and trisyllabic target words with different stress placement in a sentence context. Results showed vowel reduction in unstressed syllables, both in terms of duration and spectral quality as well as polysyllabic shortening (without spectral reduction). These temporal adjustments are of similar magnitude as reported for stress-timed languages. Results of a two-alternative forced choice task, however, showed little sensitivity to temporal patterns in monosyllabic fragments. Hence, production patterns appear to be more stress-timed than perceptual mechanisms which has implications for duration models in speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-689"
  },
  "folk11_interspeech": {
   "authors": [
    [
     "Laura",
     "Folk"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "The lombard effect in spontaneous dialog speech",
   "original": "i11_2701",
   "page_count": 4,
   "order": 692,
   "p1": "2701",
   "pn": "2704",
   "abstract": [
    "The Lombard effect - environmental noise affects speech production - has already been studied extensively for read lab speech. In this study spontaneous dialog speech produced by 24 German speakers has been recorded under noisy conditions and analysed for the Lombard effect. A sophisticated experimental setup using behind-the-ear hearing aid equipment allows us to insert real car noise into the perceived audio stream of speakers while maintaining the normal auditory feedback loop. We found that the main Lombard effects - rising fundamental frequency and intensity . can be confirmed for dialog speech. Speaking rate did not slow down although reported earlier for read speech. We also found that certain rhythmicity features regarding the dynamic of the RMS energy contour change significantly under Lombard conditions but only for the female speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-690"
  },
  "pilkington11_interspeech": {
   "authors": [
    [
     "Nicholas C. V.",
     "Pilkington"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Gaussian process experts for voice conversion",
   "original": "i11_2761",
   "page_count": 4,
   "order": 693,
   "p1": "2761",
   "pn": "2764",
   "abstract": [
    "Conventional approaches to voice conversion typically use a GMM to represent the joint probability density of source and target features. This model is then used to perform spectral conversion between speakers. This approach is reasonably effective but can be prone to overfitting and oversmoothing of the target spectra. This paper proposes an alternative scheme that uses a collection of Gaussian process experts to perform the spectral conversion. Gaussian processes are robust to overfitting and oversmoothing and can predict the target spectra more accurately. Experimental results indicate that the objective performance of voice conversion can be improved using the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-691"
  },
  "veaux11_interspeech": {
   "authors": [
    [
     "Christophe",
     "Veaux"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Intonation conversion from neutral to expressive speech",
   "original": "i11_2765",
   "page_count": 4,
   "order": 694,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "Intonation is one of the most important factors of speech expressivity. This paper presents a conversion method for the F0 contours. The F0 segments are represented with discrete cosine transform (DCT) coefficients at the syllable level. Multi-level dynamic features are added to model the temporal correlation between syllables and to constrain the F0 contour at the phrase level. Gaussian mixture models (GMM) are used to map the prosodic features between neutral and expressive speech, and the converted F0 contour is generated under the dynamic features constraints. Experimental evaluation using a database of acted emotional speech shows the effectiveness of the proposed F0 model and conversion method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-692"
  },
  "hattori11_interspeech": {
   "authors": [
    [
     "Nobuhiko",
     "Hattori"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaker-adaptive speech synthesis based on eigenvoice conversion and language-dependent prosodic conversion in speech-to-speech translation",
   "original": "i11_2769",
   "page_count": 4,
   "order": 695,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "This paper describes a novel approach based on voice conversion (VC) to speaker-adaptive speech synthesis for speech-to-speech translation. Voice quality of translated speech in an output language is usually different from that of an input speaker of the translation system since a text-to-speech system is developed with another speaker's voices in the output language. To render the input speaker's voice quality in the translated speech, we propose a voice quality control method based on one-to-many eigenvoice conversion (EVC) and language-dependent prosodic conversion. Spectral parameters of the translated speech are effectively converted by one-to-many EVC enabling unsupervised speaker adaptation. Moreover, prosodic parameters are modified considering their global differences between the input and output languages. The effectiveness of the proposed method is confirmed by experimental evaluations on cross-lingual VC among Japanese, English, and Chinese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-693"
  },
  "perez11_interspeech": {
   "authors": [
    [
     "Javier",
     "Pérez"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Adding glottal source information to intra-lingual voice conversion",
   "original": "i11_2773",
   "page_count": 4,
   "order": 696,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "This paper studies the inclusion of glottal source characteristics in voice conversion (VC) systems. We use source/filter decomposition to parametrize the vocal tract using LSF, the glottal source using the LF model, and the aspiration noise using amplitude-modulated high-pass filtered AWGN noise. To evaluate the impact of this new parametrization in VC, we use a reference conversion system that estimates a linear transformation function using a joint target/source model obtained with CART and GMM. The reference system is based on the LPC model, uses LSF to represent the vocal tract and a selection technique for the residual. We use the reference algorithm to build a VC system for each of the three parameter sets. We compared both parametrizations in the framework of an intra-lingual voice conversion task in Spanish. The results show that the new source/filter representation clearly improves the overall performance, both in terms of speaker identity transformation and voice quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-694"
  },
  "lei11c_interspeech": {
   "authors": [
    [
     "Ming",
     "Lei"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Formant-controlled HMM-based speech synthesis",
   "original": "i11_2777",
   "page_count": 4,
   "order": 697,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "This paper proposes a novel framework that enables us to manipulate and control formants in HMM-based speech synthesis. In this framework, the dependency between formants and spectral features is modelled by piecewise linear transforms; formant pa- rameters are effectively mapped by these to the means of Gaussian distributions over the spectral synthesis parameters. The spectral envelope features generated under the influence of formants in this way may then be passed to high-quality vocoders to generate the speech waveform. This provides two major advantages over conventional frameworks. First, we can achieve spectral modification by changing formants only in those parts where we want control, whereas the user must specify all formants manually in conventional formant synthesisers (e.g. Klatt). Second, this can produce high-quality speech. Our results show the proposed method can control vowels in the synthesized speech by manipulating F1 and F2 without any degradation in synthesis quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-695"
  },
  "raitio11_interspeech": {
   "authors": [
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Antti",
     "Suni"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Analysis of HMM-based lombard speech synthesis",
   "original": "i11_2781",
   "page_count": 4,
   "order": 698,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "Humans modify their voice in interfering noise in order to maintain the intelligibility of their speech.this is called the Lombard effect. This ability, however, has not been extensively modeled in speech synthesis. Here we compare several methods of synthesizing speech in noise using a physiologically based statistical speech synthesis system (GlottHMM). The results show that in a realistic street noise situation the synthetic Lombard speech is judged by listeners both as appropriate for the situation and as intelligible as natural Lombard speech. Of the different types of models, one using adaptation and extrapolation performed the best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-696"
  },
  "obin11c_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Pierre",
     "Lanchantin"
    ],
    [
     "Anne",
     "Lacheret"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Discrete/continuous modelling of speaking style in HMM-based speech synthesis: design and evaluation",
   "original": "i11_2785",
   "page_count": 4,
   "order": 699,
   "p1": "2785",
   "pn": "2788",
   "abstract": [
    "This paper assesses the ability of a HMM-based speech synthesis systems to model the speech characteristics of various speaking styles1. A discrete/continuous HMM is presented to model the symbolic and acoustic speech characteristics of a speaking style. The proposed model is used to model the average characteristics of a speaking style that is shared among various speakers, depending on specific situations of speech communication. The evaluation consists of an identification experiment of 4 speaking styles based on delexicalized speech, and compared to a similar experiment on natural speech. The comparison is discussed and reveals that discrete/continuous HMM consistently models the speech characteristics of a speaking style.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-697"
  },
  "sung11_interspeech": {
   "authors": [
    [
     "June Sig",
     "Sung"
    ],
    [
     "Doo Hwa",
     "Hong"
    ],
    [
     "Shin Jae",
     "Kang"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Factored MLLR adaptation for singing voice generation",
   "original": "i11_2789",
   "page_count": 4,
   "order": 700,
   "p1": "2789",
   "pn": "2792",
   "abstract": [
    "In our previous study, we proposed factored MLLR (FMLLR) where each MLLR parameter is defined as a function of a control vector. We presented a method to train the FMLLR parameters based on a general framework of the expectation-maximization (EM) algorithm. In this paper, we extend the FMLLR structure from diagonal to unrestricted full matrix with a sophisticated algorithm for the training of relevant parameters. In the experiments on artificial generation of singing voice, we evaluate the performance of the FMLLR technique with two matrix structures and also compare with other approaches to parameter adaptation in HMM-based speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-698"
  },
  "hirose11_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Ryusuke",
     "Mihara"
    ],
    [
     "Hiroya",
     "Hashimoto"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Adaptation of prosody in speech synthesis by changing command values of the generation process model of fundamental frequency",
   "original": "i11_2793",
   "page_count": 4,
   "order": 701,
   "p1": "2793",
   "pn": "2796",
   "abstract": [
    "A method was developed to adapt prosody to a new speaker/style in speech synthesis. It is based on predicting differences between target and original speakers/styles and applying them to the original one. Differences in fundamental frequency (F0) contours are represented in the framework of the generation process model; differences in the command magnitudes/amplitudes. While the original one requires a certain amount of training corpus, while corpus for training command differences can be small. Furthermore, in the case of style adaptation, it is not necessarily the corpus being uttered by the same speaker of the original style. Speech synthesis was conducted using HMM-based speech synthesis system, where prosody was controlled by the method. Listening experiments on synthetic speech with style adaptation and voice conversion both showed the validity of the method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-699"
  },
  "wen11b_interspeech": {
   "authors": [
    [
     "Miaomiao",
     "Wen"
    ],
    [
     "Miaomiao",
     "Wang"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Prosody conversion for emotional Mandarin speech synthesis using the tone nucleus model",
   "original": "i11_2797",
   "page_count": 4,
   "order": 702,
   "p1": "2797",
   "pn": "2800",
   "abstract": [
    "In this paper, tone nucleus model is employed to represent and convert F0 contour for synthesizing an emotional Mandarin speech from a neutral speech. Compared with previous prosody transforming methods, the proposed method 1) only converts the tone nucleus part of each syllable rather than the whole F0 contour to avoid the data sparseness problems; 2) builds mapping functions for well-chosen tone nucleus model parameters to better capture Mandarin tonal information. Using only a modest amount of training data, the perceptual accuracy achieved by our method was shown to be comparable to that obtained by a professional speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-700"
  },
  "karhila11_interspeech": {
   "authors": [
    [
     "Reima",
     "Karhila"
    ],
    [
     "Mirjam",
     "Wester"
    ]
   ],
   "title": "Rapid adaptation of foreign-accented HMM-based speech synthesis",
   "original": "i11_2801",
   "page_count": 4,
   "order": 703,
   "p1": "2801",
   "pn": "2804",
   "abstract": [
    "This paper presents findings of listeners' perception of speaker identity in synthetic speech. Specifically, we investigated what the effect is on the perceived identity of a speaker when using differently accented average voice models and limited amounts (five and fifteen sentences) of a speaker's data to create the synthetic stimuli. A speaker discrimination task was used to measure speaker identity. Native English listeners were presented with natural and synthetic speech stimuli in English and were asked to decide whether they thought the sentences were spoken by the same person or not. An accent rating task was also carried out to measure the perceived accents of the synthetic speech stimuli. The results show that listeners, for the most part, perform as well at speaker discrimination when the stimuli have been created using five or fifteen adaptation sentences as when using 105 sentences. Furthermore, the accent of the average voice model does not affect listeners' speaker discrimination performance even though the accent rating task shows listeners are perceiving different accents in the synthetic stimuli. Listeners do not base their speaker similarity decisions on perceived accent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-701"
  },
  "toth11_interspeech": {
   "authors": [
    [
     "Bálint",
     "Tóth"
    ],
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Géza",
     "Németh"
    ]
   ],
   "title": "The effects of phoneme errors in speaker adaptation for HMM speech synthesis",
   "original": "i11_2805",
   "page_count": 4,
   "order": 704,
   "p1": "2805",
   "pn": "2808",
   "abstract": [
    "In this paper the phoneme errors in adaptation data of HMM based synthesis is investigated. Phoneme errors are likely to appear in automatic speech recognition (ASR) based transcriptions. The research also investigates the perspective of merely ASR transcription based unsupervised adaptation. To achieve better quality a new method is introduced for selecting an optimal subset of ASR transcription based adaptation data. Quality evaluation of the method was also performed. The results showed that adaptation was successful even on higher than 50% phoneme error rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-702"
  },
  "berry11_interspeech": {
   "authors": [
    [
     "Jeffrey",
     "Berry"
    ],
    [
     "Sunjing",
     "Ji"
    ],
    [
     "Ian",
     "Fasel"
    ],
    [
     "Diana",
     "Archangeli"
    ]
   ],
   "title": "Articulatory reduction in Mandarin Chinese words",
   "original": "i11_2809",
   "page_count": 4,
   "order": 705,
   "p1": "2809",
   "pn": "2812",
   "abstract": [
    "We investigate the effect of reduction induced by repetition during articulation. Specifically, we report how tongue movement differs between the first mention of Mandarin words and that of later repetitions using ultrasound imaging. Two analyses were carried out in this paper: tongue deformation and timing. We used Dynamic Time Warping to measure the tongue deformation from a neutral position. We used Functional Data Analysis to measure the timing difference between the first and later repetitions. We found that the tongue deviates less from the neutral and moves faster in time for the later repetitions, namely the more reduced ones. Our study shows promise for more thorough investigation of speech reduction from the articulatory perspective, and provides insights for constructing applications for speech synthesis/recognition towards more natural speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-703"
  },
  "lammert11_interspeech": {
   "authors": [
    [
     "Adam",
     "Lammert"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Morphological variation in the adult vocal tract: a modeling study of its potential acoustic impact",
   "original": "i11_2813",
   "page_count": 4,
   "order": 706,
   "p1": "2813",
   "pn": "2816",
   "abstract": [
    "In order to fully understand inter-speaker variability in the acoustical and articulatory domains, morphological variability must be considered, as well. Human vocal tracts display substantial morphological differences, all of which have the potential to impact a speaker's acoustic output. The palate and rear pharyngeal wall, in particular, vary widely and have the potential to strongly impact the resonant properties of the vocal tract. To gain a better understanding of this impact, we combine an examination of morphological variation with acoustic modeling experiments. The goal is to show the theoretical acoustic effect of common inter-speaker differences for a set of English vowels. Modeling results indicate that the effect is indeed strong, but also surprisingly complex and context-specific, even when morphology varies in relatively straightforward ways.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-704"
  },
  "lulich11_interspeech": {
   "authors": [
    [
     "Steven M.",
     "Lulich"
    ],
    [
     "Harish",
     "Arsikere"
    ],
    [
     "John R.",
     "Morton"
    ],
    [
     "Gary K. F.",
     "Leung"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Mitchell S.",
     "Sommers"
    ]
   ],
   "title": "Analysis and automatic estimation of children's subglottal resonances",
   "original": "i11_2817",
   "page_count": 4,
   "order": 707,
   "p1": "2817",
   "pn": "2820",
   "abstract": [
    "Models and measurements of subglottal resonances are generally made from adult data, but there are several applications in which it would be useful to know about subglottal resonances in children. We therefore conducted an analysis of both new and old recordings of children's subglottal acoustics in order 1) to produce a fuller picture of the variability of children's subglottal resonances, and 2) to confirm that existing models of subglottal acoustics can be reasonably applied to children. We also tested the effectiveness of recent algorithms for estimating children's subglottal resonances from speech formants and the fundamental frequency, which were originally formulated based on adult data. It was found that these algorithms are effective for children at least 150cm tall.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-705"
  },
  "wokurek11_interspeech": {
   "authors": [
    [
     "Wolfgang",
     "Wokurek"
    ],
    [
     "Andreas",
     "Madsack"
    ]
   ],
   "title": "Acceleration sensor based estimates of subglottal resonances: short vs. long vowels",
   "original": "i11_2821",
   "page_count": 4,
   "order": 708,
   "p1": "2821",
   "pn": "2824",
   "abstract": [
    "The current version ACCV4 of our acceleration sensor device is presented and used to study the influence of vowel duration to the estimates of resonances of the subglottal system of 7 female and 9 male speakers. The sensor records movements in all three spatial directions below 5 kHz. It is gently pressed to the neck of the speaker in front of the cricothyroid ligament, a soft tissue in the lower part of the larynx. Linear prediction is used the estimate three resonances between 500 Hz and 2 kHz. Statistically significant differences in the estimates taken for the first two subglottal formants are found.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-706"
  },
  "audibert11_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Angélique",
     "Amelot"
    ]
   ],
   "title": "Comparison of nasalance measurements from accelerometers and microphones and preliminary development of novel features",
   "original": "i11_2825",
   "page_count": 4,
   "order": 709,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "This study compares four nasalance measures computed as ratios between the amplitude of signals recorded with accelerometers and microphones. Two new measures based on RMS amplitude differences between the nasal signal and either the vocal folds vibration signal (LND) or the oral acoustic signal (OND) are introduced. Measures were compared on a total of 584 utterances produced by four native French speakers. Results show that (1) all measures separate nasal from oral consonants, (2) the different experimental setups cannot be considered equivalent, (3) difference-based measures appear to better describe the time course of nasality than ratio-based measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-707"
  },
  "fitzpatrick11_interspeech": {
   "authors": [
    [
     "Michael",
     "Fitzpatrick"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "The effect of seeing the interlocutor on speech production in different noise types",
   "original": "i11_2829",
   "page_count": 4,
   "order": 710,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "Talkers modify their speech production in noisy environments partly as a reflex but also as an intentional communicative strategy to facilitate the transmission of the speech signal to the interlocutor. Previous studies have shown that the characteristics of such modifications vary depending on the type of noise. The current study examined whether speech production (and its interaction with noise type) would be affected by being able to see their interlocutor or not. Participants completed an interactive communication game in various noise conditions with/without being able to see their interlocutor. The results show that speech modifications differed with noise condition and that the speech amplitude was significantly lower when interlocutors could see each other. These results suggest that talkers actively monitor their environment and adopt appropriate speech production for efficient communication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-708"
  },
  "aubanel11_interspeech": {
   "authors": [
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Julián",
     "Villegas"
    ],
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ]
   ],
   "title": "Conversing in the presence of a competing conversation: effects on speech production",
   "original": "i11_2833",
   "page_count": 4,
   "order": 711,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "How does a background conversation affect a foreground conversation? In this scenario, and unlike traditional studies of noise-induced speech modification (Lombard speech), listeners have to cope with the additional challenge of competing speech material. In the current study, pairs of talkers engaged in natural dialogs in the absence or presence of another talker pair. Changes in speech level revealed only a small energetic masking effect of the background pair, but very large modifications in prosodic parameters (F0, speech rate) were observed during overlaps within conversations. The presence of the background pair led to increases in the numbers of dysfluencies, mistiming and interruptions, suggesting that interlocutors suffer from competing speech in ways which are not well-described by Lombard speech modifications. Longer inter-turn pauses seen in the background present condition may indicate that listeners monitor the other conversation to avoid temporally-competing speech material where possible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-709"
  },
  "heldner11_interspeech": {
   "authors": [
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "Anna",
     "Hjalmarsson"
    ],
    [
     "Kornel",
     "Laskowski"
    ]
   ],
   "title": "Very short utterances and timing in turn-taking",
   "original": "i11_2837",
   "page_count": 4,
   "order": 712,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "as well as the effects of excluding intervals adjacent to such utterances from distributions of between-speaker interval durations. The results show that very short utterances are more precisely timed to the preceding utterance than longer utterances in terms of a smaller variance and a larger proportion of no-gapno- overlaps. Excluding intervals adjacent to very short utterances furthermore results in measures of central tendency closer to zero (i.e. no-gap-no-overlaps) as well as larger variance (i.e. relatively longer gaps and overlaps).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-710"
  },
  "katsamanis11_interspeech": {
   "authors": [
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Erik",
     "Bresch"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Validating rt-MRI based articulatory representations via articulatory recognition",
   "original": "i11_2841",
   "page_count": 4,
   "order": 713,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "The large corpus of real time magnetic resonance image sequences of the vocal tract during speech production that was recently acquired and can be referred to as MRI-TIMIT, provides us with a unique platform for systematically studying articulatory dynamics. Compared to previously collected articulatory datasets, e.g., using articulography or X-rays, MRI-TIMIT is a rich source of information for the entire vocal tract and not only for certain articulatory landmarks and further has the potential to continue increasing in size covering a large variety of speakers and speaking styles. In this work, we investigate an articulatory representation based on full vocal tract shapes. We employ an articulatory recognition framework in MRI-TIMIT to analyze its merits and drawbacks. We argue that articulatory recognition can serve as a general validation tool for real-time MRI based articulatory representations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-711"
  },
  "li11l_interspeech": {
   "authors": [
    [
     "Yinghao",
     "Li"
    ],
    [
     "Jiangping",
     "Kong"
    ]
   ],
   "title": "An electropalatographic and acoustic study on anticipatory coarticulation in V1#C2V2 sequences in standard Chinese",
   "original": "i11_2845",
   "page_count": 4,
   "order": 714,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "This paper presents the data on the anticipatory coarticulation of C2 and V2 on V1 in V1#C2V2 sequences in Standard Chinese. Electropalatographic measures and F2 trajectory were obtained to define the articulatory and F2 targets for V1 as well as the displacement for articulatory and F2 transition of V1. Results show that the articulatory target is affected only by C2 place, while C2 place, C2 manner, and V2 show combined effect on the articulatory and F2 displacement of V1. Lip rounding associated with V2 is found to affect the F2 target and F2 transition of V1.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-712"
  },
  "hanique11_interspeech": {
   "authors": [
    [
     "Iris",
     "Hanique"
    ],
    [
     "Mirjam",
     "Ernestus"
    ]
   ],
   "title": "Final /t/ reduction in dutch past-participles: the role of word predictability and morphological decomposability",
   "original": "i11_2849",
   "page_count": 4,
   "order": 715,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "This corpus study demonstrates that the realization of word-final /t/ in Dutch past-participles in various speech styles is affected by a word's predictability and paradigmatic relative frequency. In particular, /t/s are shorter and more often absent if the two preceding words are more predictable. In addition, /t/s, especially in irregular verbs, are more reduced, the lower the verb's lemma frequency relative to the past-participle's frequency. Both effects are more pronounced in more spontaneous speech. These findings are expected if speech planning plays an important role in speech reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-713"
  },
  "raeesy11_interspeech": {
   "authors": [
    [
     "Zeynab",
     "Raeesy"
    ],
    [
     "Ladan",
     "Baghai-Ravary"
    ],
    [
     "John",
     "Coleman"
    ]
   ],
   "title": "Parametrising degree of articulator movement from dynamic MRI data",
   "original": "i11_2853",
   "page_count": 4,
   "order": 716,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "A new approach is proposed for quantifying the degree of articulator movement within a phoneme as a single scalar value, using vocal tract images captured using dynamic MRI. This indicates the degree of physical movement of the articulators involved in speech production, rather than the acoustic consequences of that movement.\n",
    "We go on to show that this is a valid method for characterising the overall dynamics of the vocal tract, by demonstrating a coefficient of determination (R2) of 0.61 between it and a similarly-defined scalar measure of the acoustic dynamics of the signal. The calculation of the new measure involves production of images showing the exact location of any movement within the vocal tract, and additionally shows this information separately for the initial and final segments of each phoneme.\n",
    "Finally, we demonstrate that although some sounds may involve more movement of the articulators than would be expected from the dynamics of the acoustic signal, it is rare for the degree of articulation derived from the MRI data to be significantly less than expected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-714"
  },
  "liu11b_interspeech": {
   "authors": [
    [
     "X.",
     "Liu"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Improving LVCSR system combination using neural network language model cross adaptation",
   "original": "i11_2857",
   "page_count": 4,
   "order": 717,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "State-of-the-art large vocabulary continuous speech recognition (LVCSR) systems often combine outputs from multiple subsystems developed at different sites. Cross system adaptation can be used as an alternative to direct hypothesis level combination schemes such as ROVER. The standard approach involves only cross adapting acoustic models. To fully exploit the complimentary features among sub-systems, language model (LM) cross adaptation techniques can be used. Previous research on multi-level n-gram LM cross adaptation is extended to further include the cross adaptation of neural network LMs in this paper. Using this improved LM cross adaptation framework, significant error rate gains of 4.0%.7.1% relative were obtained over acoustic model only cross adaptation when combining a range of Chinese LVCSR sub-systems used in the 2010 and 2011 DARPA GALE evaluations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-715"
  },
  "xue11_interspeech": {
   "authors": [
    [
     "Jian",
     "Xue"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Gregg",
     "Daggett"
    ],
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Towards high performance LVCSR in speech-to-speech translation system on smart phones",
   "original": "i11_2861",
   "page_count": 4,
   "order": 718,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "This paper presents the endeavors to improve the performance of large vocabulary continuous speech recognition (LVCSR) in speech-to-speech translation system on smart phones. A variety of techniques towards high LVCSR performance are investigated to achieve high accuracy and low latency given constrained resources. This includes one-pass streaming mode decoding for minimum latency, acoustic modeling with full-covariance based on bootstrap and model restructuring for improving recognition accuracy with limited training data; quantized discriminative feature space transforms and quantized Gaussian mixture model to reduce memory usage with negligible degradation on recognition accuracy. Some speed optimization methods are also discussed to increase the recognition speed. The proposed techniques evaluated on the DARPA Transtac datasets will be shown to give good overall performance under the constraints of both CPU and memory on smart phones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-716"
  },
  "sung11b_interspeech": {
   "authors": [
    [
     "Yun-Hsuan",
     "Sung"
    ],
    [
     "Martin",
     "Jansche"
    ],
    [
     "Pedro J.",
     "Moreno"
    ]
   ],
   "title": "Deploying google search by voice in Cantonese",
   "original": "i11_2865",
   "page_count": 4,
   "order": 719,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "We describe our efforts in deploying Google search by voice for Cantonese, a southern Chinese dialect widely spoken in and around Hong Kong and Guangzhou. We collected audio data from local Cantonese speakers in Hong Kong and Guangzhou by using our DataHound smartphone application. This data was used to create appropriate acoustic models. Language models were trained on anonymized query logs from Google Web Search for Hong Kong. Because users in Hong Kong frequently mix English and Cantonese in their queries, we designed our system from the ground up to handle both languages. We report on experiments with different techniques for mapping the phoneme inventories for both languages into a common space. Based on extensive experiments we report word error rates and web scores for both Hong Kong and Guangzhou data. Cantonese Google search by voice was launched in December 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-717"
  },
  "alshareef11_interspeech": {
   "authors": [
    [
     "Sarah",
     "Al-Shareef"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "An investigation in speech recognition for colloquial Arabic",
   "original": "i11_2869",
   "page_count": 4,
   "order": 720,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "This paper describes a study of grapheme-based speech recognition for colloquial Arabic. An investigation of language and acoustic model configurations is carried out to illustrate the differences between colloquial and modern standard Arabic (MSA) on the example of Levantine telephone conversations. The study defines extensive and carefully crafted data sets for different dialects and studies their overlap with MSA sources. The use of grapheme models is re-investigated, and alternative configuration for acoustic models to correct obvious shortcomings are tested. The recognition performance was analyzed on two levels: corpuslevel and dialect-level. In addition modifications of dictionaries to allow better specification of sound patterns is explored. Overall the experiments highlight the need for higher level information on acoustic model selection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-718"
  },
  "brugnara11_interspeech": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "A multithreaded implementation of Viterbi decoding on recursive transition networks",
   "original": "i11_2873",
   "page_count": 4,
   "order": 721,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "This paper describes the move to a multithreaded implementation of a Recursive Transition Network Viterbi speech decoder, undertaken with the objective of performing low-latency synchronous decoding on live audio streams to support online subtitling. The approach was meant to be independent on any specific hardware, in order to be easily exploitable on common computers, and portable to different operating systems. In the paper, the reference serial algorithm is presented, together with the modifications introduced to distribute most of the load to different threads by means of a dispatcher/collector thread and several worker threads. Results are presented, confirming a performance benefit in accordance with the design goals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-719"
  },
  "kombrink11_interspeech": {
   "authors": [
    [
     "Stefan",
     "Kombrink"
    ],
    [
     "Tomáš",
     "Mikolov"
    ],
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Lukáš",
     "Burget"
    ]
   ],
   "title": "Recurrent neural network based language modeling in meeting recognition",
   "original": "i11_2877",
   "page_count": 4,
   "order": 722,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "We use recurrent neural network (RNN) based language models to improve the BUT English meeting recognizer. On the baseline setup using the original language models we decrease word error rate (WER) more than 1% absolute by n-best list rescoring and language model adaptation. When n-gram language models are trained on the same moderately sized data set as the RNN models, improvements are higher yielding a system which performs comparable to the baseline. A noticeable improvement was observed with unsupervised adaptation of RNN models. Furthermore, we examine the influence of word history on WER and show how to speed-up rescoring by caching common prefix strings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-720"
  },
  "cossalter11_interspeech": {
   "authors": [
    [
     "Michele",
     "Cossalter"
    ],
    [
     "Priya",
     "Sundararajan"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Ad-hoc meeting transcription on clusters of mobile devices",
   "original": "i11_2881",
   "page_count": 4,
   "order": 723,
   "p1": "2881",
   "pn": "2884",
   "abstract": [
    "For all the time invested in meetings, very little of the wealth of information that is exchanged is explicitly preserved. In this paper, we propose a novel platform for meeting transcription using cellular phones for recognition. As most meeting participants carry cellular phones with them, this platform will allow meetings to be transcribed wherever they take place, without requiring any additional infrastructure. In this paper, we introduce our proposed platform, and compare three approaches for combining audio from multiple devices: microphone selection, either at signal or feature level, and combination of decoder outputs via confusion network combination. We evaluated the effectiveness of our cellular phone based platform on speech collected in a meeting environment, and found that the early microphone selection at signal level obtained a 16% improvement in speech recognition accuracy compared to using a single recording device. Moreover, this approach offered a comparable performance to multi-system confusion network combination, while requiring significantly lower computational cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-721"
  },
  "abida11_interspeech": {
   "authors": [
    [
     "Kacem",
     "Abida"
    ],
    [
     "Fakhri",
     "Karray"
    ]
   ],
   "title": "ROVER enhancement with automatic error detection",
   "original": "i11_2885",
   "page_count": 4,
   "order": 724,
   "p1": "2885",
   "pn": "2888",
   "abstract": [
    "In this paper, an approach is presented to improve the existing performance of the Recognizer Output Voting Error Reduction (ROVER) procedure used for speech decoders' combination in automatic speech transcription. A contextual analysis is injected within the ROVER process to detect and eliminate erroneous words. This filtering is carried out through the combination of automatic error detection techniques. Experiments showed it is possible to outperform the ROVER baseline, and that combining it with error detection methods leads to an even lower Word Error Rate (WER) in the final ROVER composite output.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-722"
  },
  "akita11_interspeech": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic comma insertion of lecture transcripts based on multiple annotations",
   "original": "i11_2889",
   "page_count": 4,
   "order": 725,
   "p1": "2889",
   "pn": "2892",
   "abstract": [
    "To enhance readability and usability of speech recognition results, automatic punctuation is an essential process. In this paper, we address automatic comma prediction based on conditional random fields (CRF) using lexical, syntactic and pause information. Since there is large disagreement in comma insertion between humans, we model individual tendencies of punctuation using annotations given by multiple annotators, and combine these models by voting and interpolation frameworks. Experimental evaluations on real lecture speech demonstrated that the combination of individual punctuation models achieves higher prediction accuracy for commas agreed by all annotators and those given by individual annotators.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-723"
  },
  "you11_interspeech": {
   "authors": [
    [
     "Chang Huai",
     "You"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Kong Aik",
     "Lee"
    ]
   ],
   "title": "Study on the relevance factor of maximum a posteriori with GMM for language recognition",
   "original": "i11_2893",
   "page_count": 4,
   "order": 726,
   "p1": "2893",
   "pn": "2896",
   "abstract": [
    "In this paper, the relevance factor in maximum a posteriori (MAP) adaptation of Gaussian mixture model (GMM) from universal background model (UBM) is studied for language recognition. In conventional MAP, relevance factor is typically set as a constant empirically. Knowing that relevance factor determines how much the observed training data influence the model adaptation, thus the resulting GMM models, we believe that the relevance factor should be dependent to the data for more effective modeling. We formulate the estimation of relevance factor in a systematic manner and study its role in characterizing spoken languages with supervectors. We use a Bhattacharyya-based language recognition system on National Institute of Standards and Technology (NIST) language recognition evaluation (LRE) 2009 task to investigate the validate of the data-dependent relevance factor. Experimental results show that we achieve improved performance by using the proposed relevance factor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-724"
  },
  "habib11_interspeech": {
   "authors": [
    [
     "Tania",
     "Habib"
    ],
    [
     "Harald",
     "Romsdorfer"
    ]
   ],
   "title": "Improving multiband position-pitch algorithm for localization and tracking of multiple concurrent speakers by using a frequency selective criterion",
   "original": "i11_2897",
   "page_count": 4,
   "order": 727,
   "p1": "2897",
   "pn": "2900",
   "abstract": [
    "We present an auditory inspired frequency selective extension to the multiband position-pitch (MPoPi) algorithm and a new particle filtering algorithm for localization and tracking of an arbitrary number of concurrent speakers. In the particle filtering framework, we combine standard bootstrap with importance sampling techniques. The proposed algorithm was tested on real-world recordings using a 24 channel microphone array in a meeting room for different location and speaker combinations. The results show that using the frequency selective criterion outperforms state-of-the-art and our original algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-725"
  },
  "varona11_interspeech": {
   "authors": [
    [
     "Amparo",
     "Varona"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Germán",
     "Bordel"
    ]
   ],
   "title": "On the use of lattices of time-synchronous cross-decoder phone co-occurrences in a SVM-phonotactic language recognition system",
   "original": "i11_2901",
   "page_count": 4,
   "order": 728,
   "p1": "2901",
   "pn": "2904",
   "abstract": [
    "This paper presents a simple approach to phonotactic language recognition which uses Lattices of Time-Synchronous Cross- Decoder Phone Co-occurrences at the frame level. In previous works we have successfully applied cross-decoder information, but using statistics of n-grams extracted from 1-best phone strings. In this work, the method to build and properly use lattices of cross-decoder phone co-occurrences is fully explained and developed. For evaluating the approach, a choice of open software (Brno University of Technology phone decoders, HTK, SRILM, LIBLINEAR and FoCal) was used, and experiments were carried out on the 2007 NIST LRE database. The proposed approach outperformed the baseline phonotactic systems both considering n-grams up to n=3 (yielding around 13% relative improvement) and up to n=4 (yielding around 7% relative improvement). In both cases, best results were obtained by considering the m=400 most likely cross-decoder co-occurrences: 1.29% EER and CLLR = 0.203. The fusion of the baseline system with the proposed approach yielded 1.22% EER and CLLR = 0.203 (meaning 18% and 15% relative improvements, respectively) for n=3, and 1.17% EER and CLLR = 0.197 (meaning 15% and 10% relative improvements, respectively) for n=4, outperforming state-of-the-art phonotactic systems on the same task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-726"
  },
  "tawara11_interspeech": {
   "authors": [
    [
     "Naohiro",
     "Tawara"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Speaker clustering based on utterance-oriented dirichlet process mixture model",
   "original": "i11_2905",
   "page_count": 4,
   "order": 729,
   "p1": "2905",
   "pn": "2908",
   "abstract": [
    "This paper provides the analytical solution and algorithm of UO-DPMM based on a non-parametric Bayesian manner, and thus realizes fully Bayesian speaker clustering. We carried out preliminary speaker clustering experiments by using a TIMIT database to compare the proposed method with the conventional Bayesian Information Criterion (BIC) based method, which is an approximate Bayesian approach. The results showed that the proposed method outperformed the conventional one in terms of both computational cost and robustness to changes in tuning parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-727"
  },
  "silovsky11_interspeech": {
   "authors": [
    [
     "Jan",
     "Silovsky"
    ],
    [
     "Jan",
     "Prazak"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "PLDA-based clustering for speaker diarization of broadcast streams",
   "original": "i11_2909",
   "page_count": 4,
   "order": 730,
   "p1": "2909",
   "pn": "2912",
   "abstract": [
    "This paper presents two approaches to speaker clustering based on Probabilistic Linear Discriminant Analysis (PLDA) in the speaker diarization task. We refer to the approaches as the multifold-PLDA approach and the onefold-PLDA approach. For both approaches, simple factor analysis model is employed to extract low-dimensional representation of a sequence of acoustic feature vectors . so called i-vectors . and these i-vectors are modeled using the PLDA model. Further, two-stage clustering with Bayesian Information Criterion (BIC) based approach applied in the first stage and the PLDA-based approach in the second stage is examined. We carried out our experiments using the COST278 multilingual broadcast news database. The best evaluated system yielded 42% relative improvement of the speaker error rate over a baseline BIC-based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-728"
  },
  "soufifar11_interspeech": {
   "authors": [
    [
     "Mehdi",
     "Soufifar"
    ],
    [
     "Marcel",
     "Kockmann"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ]
   ],
   "title": "ivector approach to phonotactic language recognition",
   "original": "i11_2913",
   "page_count": 4,
   "order": 731,
   "p1": "2913",
   "pn": "2916",
   "abstract": [
    "This paper addresses a novel technique for representation and processing of n-gram counts in phonotactic language recognition (LRE): subspace multinomial modelling represents the vectors of n-gram counts by low dimensional vectors of coordinates in total variability subspace, called iVector. Two techniques for iVector scoring are tested: support vector machines (SVM), and logistic regression (LR). Using standard NIST LRE 2009 task as our evaluation set, the latter scoring approach was shown to outperform phonotactic LRE system based on direct SVM classification of n-gram count vectors. The proposed iVector paradigm also shows comparable results to previously proposed PCA-based phonotactic feature extraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-729"
  },
  "alberti11_interspeech": {
   "authors": [
    [
     "Chris",
     "Alberti"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Discriminative features for language identification",
   "original": "i11_2917",
   "page_count": 4,
   "order": 732,
   "p1": "2917",
   "pn": "2920",
   "abstract": [
    "In this paper we investigate the use of discriminatively trained feature transforms to improve the accuracy of a MAP-SVM language recognition system. We train the feature transforms by alternatively solving an SVM optimization on MAP super-vectors estimated from transformed features, and performing a small step on the transforms in the direction of the antigradient of the SVM objective function. We applied this method on the LRE2003 dataset, and obtained an 5.9% relative reduction of pooled equal error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-730"
  },
  "fox11_interspeech": {
   "authors": [
    [
     "Robert Allen",
     "Fox"
    ],
    [
     "Ewa",
     "Jacewicz"
    ]
   ],
   "title": "Perceptual sensitivity to dialectal and generational variations in vowels",
   "original": "i11_2921",
   "page_count": 4,
   "order": 733,
   "p1": "2921",
   "pn": "2924",
   "abstract": [
    "Perception of dialect variation is well studied with respect to perceptual similarity of talkers based on dialectal markers. This study examines the perceptual distinctiveness of regional vowel variants in light of cross-generational changes in vowel productions. Listeners from two regional dialects of English identified the dialect of the speaker in monosyllabic words (produced by older adults, young adults and children). Differential listener sensitivity to speaker dialect was found, which was highly affected by speaker generation. This suggests that the ability to determine dialect membership is an interaction between the perceptual spaces of listeners and the acoustic variations in vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-731"
  },
  "yang11d_interspeech": {
   "authors": [
    [
     "Qian",
     "Yang"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Investigation of cross-show speaker diarization",
   "original": "i11_2925",
   "page_count": 4,
   "order": 734,
   "p1": "2925",
   "pn": "2928",
   "abstract": [
    "The goal of cross-show diarization is to index speech segments of speakers from a set of shows, with the particular challenge that reappearing speakers across shows have to be labeled with the same speaker identity. In this paper, we introduce three cross-show diarization systems namely Global-BIC-Seg, Global-BIC-Cluster, and Incremental. We compared the three systems on a set of 46 English scientific podcast shows. Among the three systems, the Global-BIC-Cluster achieves the best performance with 15.53% and 13.21% cross-show diarization error rate (DER) on the dev and test set, respectively. However, an incremental approach is more practical since data and shows are typically collected over time. By applying T-Norm on our incremental system, we obtain 13.18% and 10.97% relative improvements in terms of cross-show DER on dev and test set. We also investigate the impact of the show processing order on cross-show diarization for the incremental system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-732"
  },
  "siivola11_interspeech": {
   "authors": [
    [
     "Vesa",
     "Siivola"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Meagan",
     "Sills"
    ]
   ],
   "title": "Language identification for text chats",
   "original": "i11_2929",
   "page_count": 4,
   "order": 735,
   "p1": "2929",
   "pn": "2932",
   "abstract": [
    "This work aims to classify the language of typed messages in a text chat system used by language learners. A method for training a language classifier from unlabeled data is presented. A dictionary-based method is used to produce initial classification of the messages. Character based n-gram models of order 3 and 5 are built. A method for selectively choosing the n-grams to be modeled is used to train 15-gram models. This method produces the best-performing classifier. It has models for 57 languages and obtains over 95% accuracy on the classification of messages that are unambiguously in one language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-733"
  },
  "lee11h_interspeech": {
   "authors": [
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Chang Huai",
     "You"
    ],
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Spoken language recognition in the latent topic simplex",
   "original": "i11_2933",
   "page_count": 4,
   "order": 736,
   "p1": "2933",
   "pn": "2936",
   "abstract": [
    "This paper proposes the use of latent topic modeling for spoken language recognition, where a topic is defined as a discrete distribution over phone n-grams. The latent topics are trained in an unsupervised manner using the latent Dirichlet allocation (LDA) technique. Language recognition is then performed in a low dimensional simplex defined by the latent topics. We apply the Bhattacharyya measure to compute the n-gram similarity in the topic simplex. Our study shows that some of the latent topics are language specific while others exhibit multilingual characteristic. Experiment conducted on the NIST 2007 language detection task shows that language cues can be sufficiently preserved in the topic simplex.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-734"
  },
  "gottsmann11_interspeech": {
   "authors": [
    [
     "Frederike",
     "Gottsmann"
    ],
    [
     "Corinna",
     "Harwardt"
    ]
   ],
   "title": "Investigating robustness of spectral moments on normal- and high-effort speech",
   "original": "i11_2937",
   "page_count": 4,
   "order": 737,
   "p1": "2937",
   "pn": "2940",
   "abstract": [
    "In this paper we are looking for a robust value of the spectral moments that does not change when a speaker varies his vocal effort from normal to loud speech. To do this we first calculate the first four spectral moments for normal and loud speech. Then we compare the results for each single phoneme. After this, we do a correlation analysis to check whether normal and loud speech are linked with each other linearly. The results of the investigations show that plosives and fricatives are robust to changes of vocal effort. Vowels and sonorants demonstrate significant differences in vocal effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-735"
  },
  "harwardt11_interspeech": {
   "authors": [
    [
     "Corinna",
     "Harwardt"
    ]
   ],
   "title": "Comparing the impact of raised vocal effort on various spectral parameters",
   "original": "i11_2941",
   "page_count": 4,
   "order": 738,
   "p1": "2941",
   "pn": "2944",
   "abstract": [
    "Vocal effort changes induce various modifications to acoustic characteristics of speech. In this paper we investigate the impact of raised vocal effort on the speech spectrum. In particular, we look at different spectral parameters and compare the changes. The parameters we take into account are spectral tilt, spectral center of gravity, energy ratio and spectral moments. We carry out tests on the complete data set with all phonemes pooled into one distribution and tests with the data divided into three phoneme classes. Furthermore we run vocal effort classification tests to verify our results from statistical analysis. The results indicate significant changes for all parameters on the complete distribution as well as for vowels and sonorants. For obstruents we observe significant changes, too. But the modifications are much smaller than those for the other two phoneme classes. The parameters that are less affected by raised vocal effort are energy ratio and second spectral moment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-736"
  },
  "godin11_interspeech": {
   "authors": [
    [
     "Keith W.",
     "Godin"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Vowel context and speaker interactions influencing glottal open quotient and formant frequency shifts in physical task stress",
   "original": "i11_2945",
   "page_count": 4,
   "order": 739,
   "p1": "2945",
   "pn": "2948",
   "abstract": [
    "Physical task stress is known to affect the fundamental frequency of speech. This study of two American English vowels /IY/ and /AH/ investigates whether physical task stress affects the center frequencies of formants F1 and F2, and whether it affects the glottal open quotient, and whether these effects are different for different speakers, the different vowels, and two different vowel contexts. Formant center frequencies are measured from the acoustic waveform, and the glottal open quotient is measured from the electroglottograph signal. The study finds in general that the production of vowels is affected by physical task stress. In particular, the study finds that F1, F2, and the glottal open quotient are affected by physical task stress. It also finds that the effects of stress on F1 vary for different speakers, and that the effects of stress on the glottal open quotient vary for different combinations of speaker and vowel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-737"
  },
  "pakhomov11_interspeech": {
   "authors": [
    [
     "Serguei",
     "Pakhomov"
    ],
    [
     "Michael",
     "Kotlyar"
    ]
   ],
   "title": "Prosodic correlates of individual physiological response to stress",
   "original": "i11_2949",
   "page_count": 4,
   "order": 740,
   "p1": "2949",
   "pn": "2952",
   "abstract": [
    "Response to stress is an important health risk factor. We compared several methods based on automatic speech analysis for extracting prosodic information from spontaneous speech of 19 subjects participating in a study of the effects of bupropion (a medication that increases smoking cessation rates and treats depression) on stress response in smokers. Automatically extracted mean fundamental frequency (F0), F0 variability, and the mean duration of silent pauses significantly correlate with physiological measures of stress: plasma concentration of epinephrine (adrenaline), heart rate and blood pressure. These findings indicate that automated speech analysis may be used for non-invasive stress response measurement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-738"
  },
  "charfuelan11_interspeech": {
   "authors": [
    [
     "Marcela",
     "Charfuelan"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "The vocal effort of dominance in scenario meetings",
   "original": "i11_2953",
   "page_count": 4,
   "order": 741,
   "p1": "2953",
   "pn": "2956",
   "abstract": [
    "In this paper we address two questions about dominance in the AMI-IDIAP scenario meetings: (i) do the annotated most and least dominant utterances correlate with different levels of vocal effort? and if so (ii) how quantitatively discriminative are the vocal effort effects for prosody, voice quality and low level acoustic features? For answering these questions we perform supervised learning with dominance annotations in AMI-IDIAP meetings and vocal effort annotations in controlled data. A linear discriminant analysis (LDA) classifier is used to optimise class separability. We have found that the most and least dominant utterances are acoustically correlated with loud and soft vocal effort. We were able to quantify around 55% discrimination of equal distributions of most dominant, neutral and least dominant utterances using low level acoustic measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-739"
  },
  "patel11_interspeech": {
   "authors": [
    [
     "Sona",
     "Patel"
    ],
    [
     "Rahul",
     "Shrivastav"
    ]
   ],
   "title": "A preliminary model of emotional prosody using multidimensional scaling",
   "original": "i11_2957",
   "page_count": 4,
   "order": 742,
   "p1": "2957",
   "pn": "2960",
   "abstract": [
    "Models of emotional prosody based on perception have typically required listeners to rate emotional expressions according to the psychological dimensions (arousal, valence, and power). We propose a perception-based model without assuming that the psychological dimensions are those used by listeners to differentiate emotional prosody. Instead, multidimensional scaling is used to identify three perceptual dimensions, which are then regressed onto a dynamic feature set that does not require a training set or normalization to a speaker's \"neutral\" expression. The model predictions for Dimensions 1 and 3 closely matched the perceptual model; however, a moderately close match observed for Dimension 2.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-740"
  },
  "kim11h_interspeech": {
   "authors": [
    [
     "Jangwon",
     "Kim"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "An exploratory study of the relations between perceived emotion strength and articulatory kinematics",
   "original": "i11_2961",
   "page_count": 4,
   "order": 743,
   "p1": "2961",
   "pn": "2964",
   "abstract": [
    "Acoustic and articulatory behaviors underlying emotion strength perception are studied by analyzing acted emotional speech. Listeners evaluated emotion identity, strength and confidence. Parameters related to pitch, loudness and articulatory kinematics are associated with a 2-level (strong/weak) representation of the emotion strength. Two-class discriminant analyses show averaged leave-one-out accuracies of 65.8% and 63.8% in the acoustic and articulatory domains, respectively. Two-factor ANOVA (emotion type/strength) indicates that the listeners assess the emotion strength based on the nature of perceived emotions in the arousal dimension. Only hot anger and happiness show significant differences in pitch use in the strength contrast. Such contrasts are also observed in tongue lowering and/or advancing. The strength contrast by listeners may mainly rely upon pitch and loudness. However, interactions between the acoustic and articulatory parameters in strength perception are complex.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-741"
  },
  "ishi11_interspeech": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Improved acoustic characterization of breathy and whispery voices",
   "original": "i11_2965",
   "page_count": 4,
   "order": 744,
   "p1": "2965",
   "pn": "2968",
   "abstract": [
    "In order to improve the acoustic characterization of breathy and whispery segments, we proposed a normalized breathiness power measure (NBP) by embedding a mid-frequency voicing measure (F1F3syn) in its formulation. A partial inverse filtering preprocessing and a sub-band periodicity-based frequency boundary selection approach were also proposed for improving the performance of the F1F3syn and NBP measures. Improvements from 70 to 83% on detection of breathy/whispery segments are achieved by the proposed NBP measure relative to previous methods, for a false detection rate of 10% in modal and rough segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-742"
  },
  "govind11b_interspeech": {
   "authors": [
    [
     "D.",
     "Govind"
    ],
    [
     "S. R. M.",
     "Prasanna"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Neutral to target emotion conversion using source and suprasegmental information",
   "original": "i11_2969",
   "page_count": 4,
   "order": 745,
   "p1": "2969",
   "pn": "2972",
   "abstract": [
    "This work uses instantaneous pitch and strength of excitation along with duration of syllable-like units as the parameters for emotion conversion. Instantaneous pitch and duration of the syllable-like units of the neutral speech are modified by the prosody modification of its linear prediction (LP) residual using the instants of significant excitation. The strength of excitation is modified by scaling the Hilbert envelope (HE) of the LP residual. The target emotion speech is then synthesized using the prosody and strength modified LP residual. The pitch, duration and strength modification factors for emotion conversion are derived using the syllable-like units of initial, middle and final regions from an emotion speech database having different speakers, texts and emotions. The effectiveness of the region wise modification of source and supra segmental features over the gross level modification is confirmed by the waveforms, spectrograms and subjective evaluations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-743"
  },
  "truong11_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Ronald",
     "Poppe"
    ],
    [
     "Iwan de",
     "Kok"
    ],
    [
     "Dirk",
     "Heylen"
    ]
   ],
   "title": "A multimodal analysis of vocal and visual backchannels in spontaneous dialogs",
   "original": "i11_2973",
   "page_count": 4,
   "order": 746,
   "p1": "2973",
   "pn": "2976",
   "abstract": [
    "Backchannels (BCs) are short vocal and visual listener responses that signal attention, interest, and understanding to the speaker. Previous studies have investigated BC prediction in telephone-style dialogs from prosodic cues. In contrast, we consider spontaneous face-to-face dialogs. The additional visual modality allows speaker and listener to monitor each other's attention continuously, and we hypothesize that this affects the BC-inviting cues. In this study, we investigate how gaze, in addition to prosody, can cue BCs. Moreover, we focus on the type of BC performed, with the aim to find out whether vocal and visual BCs are invited by similar cues. In contrast to telephone-style dialogs, we do not find rising/falling pitch to be a BC-inviting cue. However, in a face-to-face setting, gaze appears to cue BCs. In addition, we find that mutual gaze occurs significantly more often during visual BCs. Moreover, vocal BCs are more likely to be timed during pauses in the speaker's speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-744"
  },
  "malandrakis11_interspeech": {
   "authors": [
    [
     "Nikos",
     "Malandrakis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Elias",
     "Iosif"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Kernel models for affective lexicon creation",
   "original": "i11_2977",
   "page_count": 4,
   "order": 747,
   "p1": "2977",
   "pn": "2980",
   "abstract": [
    "Emotion recognition algorithms for spoken dialogue applications typically employ lexical models that are trained on labeled indomain data. In this paper, we propose a domain-independent approach to affective text modeling that is based on the creation of an affective lexicon. Starting from a small set of manually annotated seed words, continuous valence ratings for new words are estimated using semantic similarity scores and a kernel model. The parameters of the model are trained using least mean squares estimation. Word level scores are combined to produce sentencelevel scores via simple linear and non-linear fusion. The proposed method is evaluated on the SemEval news headline polarity task and on the ChIMP politeness and frustration detection dialogue task, achieving state-of-the-art results on both. For politeness detection, best results are obtained when the affective model is adapted using in domain data. For frustration detection, the domain-independent model and non-linear fusion achieve the best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-745"
  },
  "sturim11_interspeech": {
   "authors": [
    [
     "Douglas",
     "Sturim"
    ],
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Nicolas",
     "Malyska"
    ],
    [
     "Alan",
     "McCree"
    ]
   ],
   "title": "Automatic detection of depression in speech using Gaussian mixture modeling with factor analysis",
   "original": "i11_2981",
   "page_count": 4,
   "order": 748,
   "p1": "2981",
   "pn": "2984",
   "abstract": [
    "Of increasing importance in the civilian and military population is the recognition of Major Depressive Disorder at its earliest stages and intervention before the onset of severe symptoms. Toward the goal of more effective monitoring of depression severity, we investigate automatic classifiers of depression state, that have the important property of mitigating nuisances due to data variability, such as speaker and channel effects, unrelated to levels of depression. To assess our measures, we use a 35-speaker free-response speech database of subjects treated for depression over a six-week duration, along with standard clinical HAMD depression ratings. Preliminary experiments indicate that by mitigating nuisances, thus focusing on depression severity as a class, we can significantly improve classification accuracy over baseline Gaussian-mixturemodel- based classifiers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-746"
  },
  "bunnell11_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Jason",
     "Lilley"
    ],
    [
     "Sigfrid D.",
     "Soli"
    ],
    [
     "Ivan",
     "Pal"
    ]
   ],
   "title": "Utterance verification for automating the hearing in noise test (HINT)",
   "original": "i11_2985",
   "page_count": 4,
   "order": 749,
   "p1": "2985",
   "pn": "2988",
   "abstract": [
    "Tests of speech intelligibility play an essential role in many audiological procedures, including diagnostic assessment, verification of hearing aid and cochlear implant fittings, outcome assessment following intervention, and screening of applicants for hearingcritical jobs. The Hearing In Noise Test (HINT) [1] is a speech intelligibility test commonly used for these purposes. A limitation of the HINT, as well as other similar tests, is that they must be administered and scored by a human observer. The present study is an evaluation of a preliminary HMM-based utterance verification system that can be used in place of a human observer to administer HINT.\n",
    "",
    "",
    "Nilsson, M., S.D. Soli, and J.A. Sullivan, Development of the Hearing in Noise Test for the measurement of speech reception thresholds in quiet and in noise. . Journal of the Acoustical Society of America, 1994. 95(6): p. 1085-1099.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-747"
  },
  "mower11_interspeech": {
   "authors": [
    [
     "Emily",
     "Mower"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "James",
     "Gibson"
    ],
    [
     "Theodora",
     "Chaspari"
    ],
    [
     "Marian E.",
     "Williams"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Analyzing the nature of ECA interactions in children with autism",
   "original": "i11_2989",
   "page_count": 4,
   "order": 750,
   "p1": "2989",
   "pn": "2992",
   "abstract": [
    "Embodied conversational agents (ECA) offer platforms for the collection of structured interaction and communication data. This paper discusses the data collected from the Rachel system, an ECA developed at the University of Southern California, for interactions with children with autism. Two dyads each composed of a child with autism and his parent participated in an experiment with two modes: interactions with and without the ECA present. The goal of this work is to assess the naturalness of the data recorded in the ECA interaction. This analysis was carried out using a classification framework with a prediction variable of the presence or absence of the ECA in the interaction. The results demonstrate that it is possible to estimate whether or not a parent is interacting with the ECA using their speech data. However, it is not generally possible to do so for the child suggesting that the Rachel system is eliciting communication data that is similar to that elicited through interactions between the child and his parent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-748"
  },
  "athanaselis11_interspeech": {
   "authors": [
    [
     "Theologos",
     "Athanaselis"
    ],
    [
     "Stelios",
     "Bakamidis"
    ],
    [
     "Ioannis",
     "Dologlou"
    ],
    [
     "Evmorfia N.",
     "Argyriou"
    ],
    [
     "Antonis",
     "Symvonis"
    ]
   ],
   "title": "Incorporating speech recognition engine into an intelligent assistive reading system for dyslexic students",
   "original": "i11_2993",
   "page_count": 4,
   "order": 751,
   "p1": "2993",
   "pn": "2996",
   "abstract": [
    "In this paper we present an approach for incorporating a state of the art speech recognition engine into a novel assistive reading system for Greek dyslexic students. This system is being developed in the framework of the AGENT-DYSL IST project, and facilitates dyslexic children in learning to read fluently. Unlike previously presented approaches, the aim of this system is to monitor the progress and perspectives of a dyslexic user and supply personalised help. The goal of this help is to gradually increase the reading capabilities of the user, gradually diminish the assistance provided, till he is able to read as a non-dyslexic reader.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-749"
  },
  "cummins11_interspeech": {
   "authors": [
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Michael",
     "Breakspear"
    ],
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "An investigation of depressed speech detection: features and normalization",
   "original": "i11_2997",
   "page_count": 4,
   "order": 752,
   "p1": "2997",
   "pn": "3000",
   "abstract": [
    "In recent years, the problem of automatic detection of mental illness from the speech signal has gained some initial interest, however questions remaining include how speech segments should be selected, what features provide good discrimination, and what benefits feature normalization might bring given the speakerspecific nature of mental disorders. In this paper, these questions are addressed empirically using classifier configurations employed in emotion recognition from speech, evaluated on a 47-speaker depressed/neutral read sentence speech database. Results demonstrate that (1) detailed spectral features are well suited to the task, (2) speaker normalization provides benefits mainly for less detailed features, and (3) dynamic information appears to provide little benefit. Classification accuracy using a combination of MFCC and formant based features approached 80% for this database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-750"
  },
  "sanchez11b_interspeech": {
   "authors": [
    [
     "Michelle Hewlett",
     "Sanchez"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Colleen",
     "Richey"
    ],
    [
     "Pablo",
     "Garcia"
    ],
    [
     "Bruce",
     "Knoth"
    ],
    [
     "William",
     "Jarrold"
    ]
   ],
   "title": "Using prosodic and spectral features in detecting depression in elderly males",
   "original": "i11_3001",
   "page_count": 4,
   "order": 753,
   "p1": "3001",
   "pn": "3004",
   "abstract": [
    "As research in speech processing has matured, there has been much interest in paralinguistic speech processing problems including the speaker's mental and psychological health. In this study, we focus on speech features that can identify the speaker's emotional health, i.e., whether the speaker is depressed or not. We use prosodic speech measurements, such as pitch and energy, in addition to spectral features, such as formants and spectral tilt, and compute statistics of these features over different regions of the speech signal. These statistics are used as input features to a discriminative classifier that predicts the speaker's depression state. We find that with an N-fold leave-one-out cross-validation setup, we can achieve a prediction accuracy of 81.3%, where random guess is 50%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-751"
  },
  "middag11_interspeech": {
   "authors": [
    [
     "Catherine",
     "Middag"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Combining phonological and acoustic ASR-free features for pathological speech intelligibility assessment",
   "original": "i11_3005",
   "page_count": 4,
   "order": 754,
   "p1": "3005",
   "pn": "3008",
   "abstract": [
    "Intelligibility is widely used to measure the severity of articulatory problems in pathological speech. Recently, a number of automatic intelligibility assessment tools have been developed. Most of them use automatic speech recognizers (ASR) to compare the patient's utterance with the target text. These methods are bound to one language and tend to be less accurate when speakers hesitate or make reading errors. To circumvent these problems, two different ASR-free methods were developed over the last few years, only making use of the acoustic or phonological properties of the utterance. In this paper, we demonstrate that these ASR-free techniques are also able to predict intelligibility in other languages. Moreover, they show to be complementary, resulting in even better intelligibility predictions when both methods are combined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-752"
  },
  "hofe11_interspeech": {
   "authors": [
    [
     "Robin",
     "Hofe"
    ],
    [
     "Stephen R.",
     "Ell"
    ],
    [
     "Michael J.",
     "Fagan"
    ],
    [
     "James M.",
     "Gilbert"
    ],
    [
     "Phil D.",
     "Green"
    ],
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Sergey I.",
     "Rybchenko"
    ]
   ],
   "title": "Speech synthesis parameter generation for the assistive silent speech interface MVOCA",
   "original": "i11_3009",
   "page_count": 4,
   "order": 755,
   "p1": "3009",
   "pn": "3012",
   "abstract": [
    "In previous publications, a silent speech interface based on permanent-magnetic articulography (PMA) has been introduced and evaluated using standard automatic speech recognition techniques. However, word recognition is a task that is computationally expensive and introduces a significant time delay between speech articulation and generation of the acoustic signal. This paper investigates a direct synthesis approach where control parameters for parametric speech synthesis are generated directly from the sensor data of the silent speech interface, without an intermediate lexical representation. Users of such a device would not be tied to the limited vocabulary of a word-based recogniser and could therefore express themselves more freely. This paper presents a feasibility study that investigates whether it is possible to infer speech synthesis parameters from PMA sensor data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-753"
  },
  "heeman11_interspeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Andy",
     "McMillin"
    ],
    [
     "J. Scott",
     "Yaruss"
    ]
   ],
   "title": "Computer-assisted disfluency counts for stuttered speech",
   "original": "i11_3013",
   "page_count": 4,
   "order": 756,
   "p1": "3013",
   "pn": "3016",
   "abstract": [
    "We present computer tools to help speech-language pathologists (SLP) in counting disfluencies, for both real-time and transcriptbased counts. The latter tend to be more precise and show which words are involved in each disfluency. Our approach allows realtime counts to be used as the basis for transcript-based counts. We employ automatic speech recognition to generate a word transcript (for read-speech samples), and then automatically merge the disfluency annotations with the word transcript, and have the SLP review parts of the audio file where a disfluency annotation was placed. This approach results in improved disfluency annotations, and can be done in 3 times real-time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-754"
  },
  "hummel11_interspeech": {
   "authors": [
    [
     "Richard",
     "Hummel"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Tiago H.",
     "Falk"
    ]
   ],
   "title": "Spectral features for automatic blind intelligibility estimation of spastic dysarthric speech",
   "original": "i11_3017",
   "page_count": 4,
   "order": 757,
   "p1": "3017",
   "pn": "3020",
   "abstract": [
    "In this paper, we explore the use of the standard ITU-T P.563 speech quality estimation algorithm for automatic assessment of dysarthric speech intelligibility. A linear mapping consisting of three salient P.563 internal features is proposed and shown to accurately estimate spastic dysarthric speech intelligibility. Delta-energy features are further proposed in order to characterize the atypical spectral dynamics and limited vowel space observed with spastic dysarthria. Experiments using the publicly-available Universal Access database (10 speaker patients) show that when salient delta-energy and internal P.563 features are used, correlations with subjective intelligibility ratings as high as 0.98 can be attained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-755"
  },
  "prudhommeaux11_interspeech": {
   "authors": [
    [
     "Emily T.",
     "Prud'hommeaux"
    ],
    [
     "Brian",
     "Roark"
    ]
   ],
   "title": "Extraction of narrative recall patterns for neuropsychological assessment",
   "original": "i11_3021",
   "page_count": 4,
   "order": 758,
   "p1": "3021",
   "pn": "3024",
   "abstract": [
    "Poor narrative memory is associated with a variety of neurodegenerative and developmental disorders, such as autism and Alzheimer's related dementia. Hence, narrative recall tasks are included in most standard neurological examinations. In this paper, we explore methods for automatically assessing the quality of retellings via alignment to the original narrative. Word alignments serve both to automate manual scoring and to derive other features related to narrative coherence that can be used for diagnostic classification. Despite relatively high word alignment error rates, the automatic alignments provide sufficient information to achieve nearly as accurate diagnostic classification as manual scores. Furthermore, additional features that become available with alignment provide utility in classifying subject groups. While the additional features we explore here did not provide additive gains in accuracy, they point the way to the development of many potentially useful features in this domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-756"
  },
  "kunikoshi11_interspeech": {
   "authors": [
    [
     "Aki",
     "Kunikoshi"
    ],
    [
     "Yu",
     "Qiao"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Gesture design of hand-to-speech converter derived from speech-to-hand converter based on probabilistic integration model",
   "original": "i11_3025",
   "page_count": 4,
   "order": 759,
   "p1": "3025",
   "pn": "3028",
   "abstract": [
    "When dysarthrics, individuals with speaking disabilities, try to communicate using speech, they often have no choice but to use speech synthesizers which require them to type word symbols or sound symbols. Input by this method often makes real-time communication troublesome and dysarthric users struggle to have smooth flowing conversations. In this study, we are developing a novel speech synthesizer where speech is generated through hand motions rather than symbol input. In recent years, statistical voice conversion techniques have been proposed based on space mapping between given parallel utterances. By applying these methods, a hand space was mapped to a vowel space and a converter from hand motions to vowel transitions was developed. It reported that the proposed method is effective enough to generate the five Japanese vowels. In this paper, we discuss the expansion of this system to consonant generation. In order to create the gestures for consonants, a Speech-to-Hand conversion system is firstly developed using parallel data for vowels, in which consonants are not included. Then, we are able to automatically search for candidates for consonant gestures for a Hand-to-Speech system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-757"
  },
  "sasou11_interspeech": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ]
   ],
   "title": "Powered wheelchair control using acoustic-based recognition of head gesture accompanying speech",
   "original": "i11_3029",
   "page_count": 4,
   "order": 760,
   "p1": "3029",
   "pn": "3032",
   "abstract": [
    "In this paper, we propose the novel interface for powered wheelchair control using the acoustic-based recognition of head gesture accompanying speech. A microphone array mounted on a wheelchair localizes the position of the user's voice. Because the localized position of the user's voice almost corresponds with that of the mouth, the tracking of the head movements accompanying speech can be achieved by means of the microphone array. The proposed interface does not require disabled people to wear any microphones or utter recognizable voice commands, but requires only two capabilities: the ability to move the head and the ability to utter an arbitrary sound. In our preliminary experiments, five subjects performed six kinds of head gestures accompanying speech. The head gestures of each subject were recognized using the models trained from the other subjects' data. The average recognition accuracy was 99.7%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-758"
  },
  "blanco11_interspeech": {
   "authors": [
    [
     "José Luis",
     "Blanco"
    ],
    [
     "Rubén",
     "Fernández"
    ],
    [
     "Doroteo",
     "Torre"
    ],
    [
     "F. Javier",
     "Caminero"
    ],
    [
     "Eduardo",
     "López"
    ]
   ],
   "title": "Analyzing training dependencies and posterior fusion in discriminant classification of apnea patients based on sustained and connected speech",
   "original": "i11_3033",
   "page_count": 4,
   "order": 761,
   "p1": "3033",
   "pn": "3036",
   "abstract": [
    "We present a novel approach using both sustained vowels and connected speech, to detect obstructive sleep apnea (OSA) cases within a homogeneous group of speakers. The proposed scheme is based on state-of-the-art GMM-based classifiers, and acknowledges specifically the way in which acoustic models are trained on standard databases, as well as the complexity of the resulting models and their adaptation to specific data. Our experimental database contains a suitable number of utterances and sustained speech from healthy (i.e control) and OSA Spanish speakers. Finally, a 25.1% relative reduction in classification error is achieved when fusing continuous and sustained speech classifiers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-759"
  },
  "parent11_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Parent"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Speaking to the crowd: looking at past achievements in using crowdsourcing for speech and predicting future challenges",
   "original": "i11_3037",
   "page_count": 4,
   "order": 762,
   "p1": "3037",
   "pn": "3040",
   "abstract": [
    "This paper examines the literature on the use of crowdsourcing for speech-related tasks: speech acquisition, transcription and annotation as well as the assessment of speech technology. 29 papers were found, representing, 37 different experiments, which were annotated and analyzed to find trends in the field. The paper focuses on the different techniques used for quality control and the variety of sources of \"crowds\". Finally, we propose several challenges for the future of crowdsourcing for speech processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-760"
  },
  "lee11i_interspeech": {
   "authors": [
    [
     "Chia-ying",
     "Lee"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "A transcription task for crowdsourcing with automatic quality control",
   "original": "i11_3041",
   "page_count": 4,
   "order": 763,
   "p1": "3041",
   "pn": "3044",
   "abstract": [
    "In this paper, we propose a two-stage transcription task design for crowdsourcing with an automatic quality control mechanism embedded in each stage. For the first stage, a support vector machine (SVM) classifier is utilized to quickly filter poor quality transcripts based on acoustic cues and language patterns in the transcript. In the second stage, word level confidence scores are used to estimate a transcription quality and provide instantaneous feedback to the transcriber. The proposed design was evaluated using Amazon Mechanical Turk (MTurk) and tested on seven hours of academic lecture speech, which is typically conversational in nature and contains technical material. Compared to baseline transcripts which were also collected from MTurk using a ROVERbased method, we observed that the new method resulted in higher quality transcripts while requiring less transcriber effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-761"
  },
  "audhkhasi11_interspeech": {
   "authors": [
    [
     "Kartik",
     "Audhkhasi"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Reliability-weighted acoustic model adaptation using crowd sourced transcriptions",
   "original": "i11_3045",
   "page_count": 4,
   "order": 764,
   "p1": "3045",
   "pn": "3048",
   "abstract": [
    "This paper focuses on adaptation of acoustic models using speech transcribed by multiple noisy experts. A simple approach involves combining multiple transcripts using word frequency based Recognizer Output Voting Error Reduction (ROVER) followed by adaptation using the combined transcripts. But this assumes that the transcripts being combined are equally reliable. To overcome this assumption, we use two sets of scores to estimate this reliability. The first set is based on answers to some questions given by the transcribers. The second set is derived in an unsupervised way using the word frequency based ROVER transcripts and baseline acoustic models. The overall confidence is a convex combination of these scores and is used to perform a confidence weighted fusion. We adapt the baseline acoustic models using these combined transcripts. Recognition results for a Mexican Spanish ASR system show an absolute improvement of 0.5% in word error rate and 0.9% in sentence error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-762"
  },
  "cooke11_interspeech": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ],
    [
     "Krzysztof",
     "Wasilewski"
    ]
   ],
   "title": "Crowdsourcing for word recognition in noise",
   "original": "i11_3049",
   "page_count": 4,
   "order": 765,
   "p1": "3049",
   "pn": "3052",
   "abstract": [
    "Access to large samples of listeners is an appealing prospect for speech perception researchers, but lack of control over key factors such as listeners' linguistic backgrounds and quality of stimulus delivery is a formidable barrier to the application of crowdsourcing. We describe the outcome of a web-based listening experiment designed to discover consistent confusions amongst words presented in noise, alongside an identical task carried out using traditional laboratory methods. Web listeners were graded based on information they provided as well as via their responses to tokens recognised robustly by a majority of participants. While overall word identification scores even for the best-performing web subset were well below those obtained in the laboratory, word confusions with high levels of cross-listener agreement were obtained nevertheless, suggesting that focused application of crowdsourcing in speech perception can provide useful data for scientific analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-763"
  },
  "buchholz11_interspeech": {
   "authors": [
    [
     "Sabine",
     "Buchholz"
    ],
    [
     "Javier",
     "Latorre"
    ]
   ],
   "title": "Crowdsourcing preference tests, and how to detect cheating",
   "original": "i11_3053",
   "page_count": 4,
   "order": 766,
   "p1": "3053",
   "pn": "3056",
   "abstract": [
    "We describe an approach to crowdsource the evaluation of TTS systems by preference tests and report on lessons learnt from running 127 real-life crowdsourced tests. We show that at least one type of cheating becomes more prevalent over time if left unchecked and develop metrics to exclude cheaters. We demonstrate that their exclusion improves test outcomes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-764"
  },
  "mcgraw11_interspeech": {
   "authors": [
    [
     "Ian",
     "McGraw"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Growing a spoken language interface on Amazon Mechanical Turk",
   "original": "i11_3057",
   "page_count": 4,
   "order": 767,
   "p1": "3057",
   "pn": "3060",
   "abstract": [
    "Typically data collection, transcription, language model generation, and deployment are separate phases of creating a spoken language interface. An unfortunate consequence of this is that the recognizer usually remains a static element of systems often deployed in dynamic environments. By providing an API for human intelligence, Amazon Mechanical Turk changes the way system developers can construct spoken language systems. In this work, we describe an architecture that automates and connects these four phases, effectively allowing the developer to grow a spoken language interface. In particular, we show that a human-in-the-loop programming paradigm, in which workers transcribe utterances behind the scenes, can alleviate the need for expert guidance in language model construction. We demonstrate the utility of these organic language models in a voice-search interface for photographs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-765"
  },
  "jurcicek11_interspeech": {
   "authors": [
    [
     "F.",
     "Jurčíček"
    ],
    [
     "S.",
     "Keizer"
    ],
    [
     "Milica",
     "Gašić"
    ],
    [
     "François",
     "Mairesse"
    ],
    [
     "B.",
     "Thomson"
    ],
    [
     "K.",
     "Yu"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Real user evaluation of spoken dialogue systems using Amazon Mechanical Turk",
   "original": "i11_3061",
   "page_count": 4,
   "order": 768,
   "p1": "3061",
   "pn": "3064",
   "abstract": [
    "This paper describes a framework for evaluation of spoken dialogue systems. Typically, evaluation of dialogue systems is performed in a controlled test environment with carefully selected and instructed users. However, this approach is very demanding. An alternative is to recruit a large group of users who evaluate the dialogue systems in a remote setting under virtually no supervision. Crowdsourcing technology, for example Amazon Mechanical Turk (AMT), provides an efficient way of recruiting subjects. This paper describes an evaluation framework for spoken dialogue systems using AMT users and compares the obtained results with a recent trial in which the systems were tested by locally recruited users. The results suggest that the use of crowdsourcing technology is feasible and it can provide reliable results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-766"
  },
  "gelas11_interspeech": {
   "authors": [
    [
     "Hadrien",
     "Gelas"
    ],
    [
     "Solomon Teferra",
     "Abate"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "François",
     "Pellegrino"
    ]
   ],
   "title": "Quality assessment of crowdsourcing transcriptions for african languages",
   "original": "i11_3065",
   "page_count": 4,
   "order": 769,
   "p1": "3065",
   "pn": "3068",
   "abstract": [
    "We evaluate the quality of speech transcriptions acquired by crowdsourcing to develop ASR acoustic models (AM) for underresourced languages. We have developed AMs using reference (REF) transcriptions and transcriptions from crowdsourcing (TRK) for Swahili and Amharic. While the Amharic transcription was much slower than that of Swahili to complete, the speech recognition systems developed using REF and TRK transcriptions have almost similar (40.1 vs 39.6 for Amharic and 38.0 vs 38.5 for Swahili) word recognition error rate. Moreover, the character level disagreement rates between REF and TRK are only 3.3% and 6.1% for Amharic and Swahili, respectively. We conclude that it is possible to acquire quality transcriptions from the crowd for under-resourced languages using Amazon's Mechanical Turk. Recognizing such a great potential of it, we recommend some legal and ethical issues to consider.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-767"
  },
  "evanini11_interspeech": {
   "authors": [
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Klaus",
     "Zechner"
    ]
   ],
   "title": "Using crowdsourcing to provide prosodic annotations for non-native speech",
   "original": "i11_3069",
   "page_count": 4,
   "order": 770,
   "p1": "3069",
   "pn": "3072",
   "abstract": [
    "We present the results of an experiment in which 2 expert and 11 naive annotators provided prosodic annotations for stress and boundary tones on a corpus of spontaneous speech produced by non-native speakers of English. The results show that agreement rates were higher for boundary tones than for stress. In addition, a crowdsourcing approach was implemented to combine the naive annotations to increase accuracy. The crowdsourcing approach was able to match expert agreement for stress (62.1%) with 3 naive annotators, and come within 7.2% of expert agreement for boundary tones (82.4%) with 11 naive annotators. This experiment also demonstrates that noticeable improvements in naive annotations can be obtained with a small amount of additional training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-768"
  },
  "goto11_interspeech": {
   "authors": [
    [
     "Masataka",
     "Goto"
    ],
    [
     "Jun",
     "Ogata"
    ]
   ],
   "title": "Podcastle: recent advances of a spoken document retrieval service improved by anonymous user contributions",
   "original": "i11_3073",
   "page_count": 4,
   "order": 771,
   "p1": "3073",
   "pn": "3076",
   "abstract": [
    "In this paper, we introduce recent advances of a speech retrieval web service, PodCastle, that collects and amplifies voluntary contributions by anonymous users. Our goal is to provide users with a public web service based on speech recognition and crowdsourcing so that they can experience state-of-the-art speech recognition performance through a useful service. PodCastle enables users to find speech data (such as podcasts and YouTube video clips) that include a search term, read full texts of their recognition results, and easily correct recognition errors by simply selecting from a list of candidates. The resulting corrections were used to improve both the speech retrieval and recognition performances. In our experiences from its practical use over the past four years (since December, 2006), over half a million recognition errors in about one hundred thousand speech data were corrected by anonymous users and we confirmed that the speech recognition performance of PodCastle was actually improved by those corrections.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-769"
  },
  "valente11b_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Alessandro",
     "Vinciarelli"
    ]
   ],
   "title": "Language-independent socio-emotional role recognition in the AMI meetings corpus",
   "original": "i11_3077",
   "page_count": 4,
   "order": 772,
   "p1": "3077",
   "pn": "3080",
   "abstract": [
    "Social roles are a coding scheme that characterizes the relationships between group members during a discussion and their roles \"oriented toward the functioning of the group as a group\". This work presents an investigation on language-independent automatic social role recognition in AMI meetings based on turns statistics and prosodic features. At first, turn-taking statistics and prosodic features are integrated into a single generative conversation model which achieves a role recognition accuracy of 59%. This model is then extended to explicitly account for dependencies (or influence) between speakers achieving an accuracy of 65%. The last contribution consists in investigating the statistical dependencies between the formal and the social role that participants have; integrating the information related to the formal role in the model, the recognition achieves an accuracy of 68%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-770"
  },
  "levitan11_interspeech": {
   "authors": [
    [
     "Rivka",
     "Levitan"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Measuring acoustic-prosodic entrainment with respect to multiple levels and dimensions",
   "original": "i11_3081",
   "page_count": 4,
   "order": 773,
   "p1": "3081",
   "pn": "3084",
   "abstract": [
    "In conversation, speakers become more like each other in various dimensions. This phenomenon, commonly called entrainment, coordination, or alignment, is widely believed to be crucial to the success and naturalness of human interactions. We investigate entrainment in four acoustic and prosodic dimensions. We explore whether speakers coordinate with each other in these dimensions over the conversation as a whole as well as on a turn-by-turn basis and in both relative and absolute terms, and whether this coordination improves over the course of the conversation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-771"
  },
  "park11_interspeech": {
   "authors": [
    [
     "Youngja",
     "Park"
    ]
   ],
   "title": "Automatic call quality monitoring using cost-sensitive classification",
   "original": "i11_3085",
   "page_count": 4,
   "order": 774,
   "p1": "3085",
   "pn": "3088",
   "abstract": [
    "In this paper, we propose advanced text analytics and cost-sensitive classification-based approaches for call quality monitoring and show that automatic quality monitoring with ASR transcripts can be achieved with a high accuracy. Our system analyzes ASR transcripts and determines if a call is a good call or a bad call. The set of features were identified through analysis of a large number of human monitoring results, which aim to estimate agent's attitude and customer's sentiment during the call. To enhance the accuracy of feature extraction, we apply various techniques to improve the quality of transcribed calls, such as sentence boundary detection and disfluency removal. We further note that quality monitoring has skewed class distribution and unequal classification error costs, and thus apply cost sensitive classification algorithms. Validation on 386 customer calls confirms the benefits of our approach. A SVM-based method produces a classification accuracy of 83.16% and 67.66% in F1 Score for identifying bad calls, which is promising. This system can therefore be used to conduct initial monitoring of all the calls in a contact center and to select calls that require human monitoring.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-772"
  },
  "iwata11_interspeech": {
   "authors": [
    [
     "Tomoharu",
     "Iwata"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Learning influences from word use in polylogue",
   "original": "i11_3089",
   "page_count": 4,
   "order": 775,
   "p1": "3089",
   "pn": "3092",
   "abstract": [
    "We propose a probabilistic model for estimating influences among speakers from conversation data with multiple people. In conversations, people tend to mimic their companions' behavior depending on their level of trust. With the proposed model, we assume that the word use of a speaker depends on the word use of previous speakers as well as their own earlier word use and the general word distribution. The influences can be efficiently estimated by using the expectation maximization (EM) algorithm. Experiments on two meeting data sets in Japanese and in English demonstrate the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-773"
  },
  "wang11j_interspeech": {
   "authors": [
    [
     "Wen",
     "Wang"
    ],
    [
     "Kristin",
     "Precoda"
    ],
    [
     "Colleen",
     "Richey"
    ],
    [
     "Geoffrey",
     "Raymond"
    ]
   ],
   "title": "Identifying agreement/disagreement in conversational speech: a cross-lingual study",
   "original": "i11_3093",
   "page_count": 4,
   "order": 776,
   "p1": "3093",
   "pn": "3096",
   "abstract": [
    "This paper presents models for detecting agreement/ disagreement between speakers in English and Arabic broadcast conversation shows. We explore a variety of features, including lexical, structural, durational, and prosodic features. We experiment with these features using Conditional Random Fields models and conduct systematic investigations on efficacy of various feature groups across languages. Sampling approaches are examined for handling highly imbalanced data. Overall, we achieved 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on English broadcast conversation data; and 89.2% (precision), 30.1% (recall), 45.1% (F1) for agreement detection and 75.9% (precision), 28.4% (recall), and 41.3% (F1) for disagreement detection, on Arabic broadcast conversation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-774"
  },
  "neiberg11d_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "A dual channel coupled decoder for fillers and feedback",
   "original": "i11_3097",
   "page_count": 4,
   "order": 777,
   "p1": "3097",
   "pn": "3100",
   "abstract": [
    "This study presents a dual channel decoder capable of modeling cross-speaker dependencies for segmentation and classification of fillers and feedbacks in conversational speech found in the DEAL corpus. For the same number of Gaussians per state, we have shown improvement in terms of average F-score for the successive addition of 1) increased frame rate from 10 ms to 50 ms 2) Joint Maximum Cross-Correlation (JMXC) features in a single channel decoder 3) a joint transition matrix which captures dependencies symmetrically across the two channels 4) coupled acoustic model retraining symmetrically across the two channels. The final step gives a relative improvement of over 100% for fillers and feedbacks compared to our previous published results. The F-scores are in the range to make it possible to use the decoder as both a voice activity detector and an illucotary act decoder for semi-automatic annotation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-775"
  },
  "lee11j_interspeech": {
   "authors": [
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Brian R.",
     "Baucom"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "An analysis of PCA-based vocal entrainment measures in married couples' affective spoken interactions",
   "original": "i11_3101",
   "page_count": 4,
   "order": 778,
   "p1": "3101",
   "pn": "3104",
   "abstract": [
    "Entrainment has played a crucial role in analyzing marital couples interactions. In this work, we introduce a novel technique for quantifying vocal entrainment based on Principal Component Analysis (PCA). The entrainment measure, as we define in this work, is the amount of preserved variability of one interlocutor's speaking characteristic when projected onto representing space of the other's speaking characteristics. Our analysis on real couples interactions shows that when a spouse is rated as having positive emotion, he/she has a higher value of vocal entrainment compared when rated as having negative emotion. We further performed various statistical analyses on the strength and the directionality of vocal entrainment under different affective interaction conditions to bring quantitative insights into the entrainment phenomenon. These analyses along with a baseline prediction model demonstrate the validity and utility of the proposed PCA-based vocal entrainment measure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-776"
  },
  "schillingmann11_interspeech": {
   "authors": [
    [
     "Lars",
     "Schillingmann"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Christian",
     "Munier"
    ],
    [
     "Britta",
     "Wrede"
    ],
    [
     "Katharina",
     "Rohlfing"
    ]
   ],
   "title": "Using prominence detection to generate acoustic feedback in tutoring scenarios",
   "original": "i11_3105",
   "page_count": 4,
   "order": 779,
   "p1": "3105",
   "pn": "3108",
   "abstract": [
    "Robots interacting with humans need to understand actions and make use of language in social interactions. Research on infant development has shown that language helps the learner to structure visual observations of action. This acoustic information typically in the form of narration overlaps with action sequences and provides infants with a bottom-up guide to find structure within them. This concept has been introduced as acoustic packaging by Hirsh-Pasek and Golinkoff. We developed and integrated a prominence detection module in our acoustic packaging system to detect semantically relevant information linguistically highlighted by the tutor. Evaluation results on speech data from adult-infant interactions show a significant agreement with human raters. Furthermore a first approach based on acoustic packages which uses the prominence detection results to generate acoustic feedback is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-777"
  },
  "otsuka11_interspeech": {
   "authors": [
    [
     "Takuma",
     "Otsuka"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Bayesian extension of MUSIC for sound source localization and tracking",
   "original": "i11_3109",
   "page_count": 4,
   "order": 780,
   "p1": "3109",
   "pn": "3112",
   "abstract": [
    "This paper presents a Bayesian extension of MUSIC-based sound source localization (SSL) and tracking method. SSL is important for distant speech enhancement and simultaneous speech separation for improving speech recognition, as well as for auditory scene analysis by mobile robots. One of the drawbacks of existing SSL methods is the necessity of careful parameter tunings, e.g., the sound source detection threshold depending on the reverberation time and the number of sources. Our contribution consists of (1) automatic parameter estimation in the variational Bayesian framework and (2) tracking of sound sources with reliability. Experimental results demonstrate our method robustly tracks multiple sound sources in a reverberant environment with RT20=840(ms).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-778"
  },
  "wollmer11c_interspeech": {
   "authors": [
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Felix",
     "Weninger"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Speech-based non-prototypical affect recognition for child-robot interaction in reverberated environments",
   "original": "i11_3113",
   "page_count": 4,
   "order": 781,
   "p1": "3113",
   "pn": "3116",
   "abstract": [
    "We present a study on the effect of reverberation on acousticlinguistic recognition of non-prototypical emotions during childrobot interaction. Investigating the well-defined Interspeech 2009 Emotion Challenge task of recognizing negative emotions in children's speech, we focus on the impact of artificial and real reverberation conditions on the quality of linguistic features and on emotion recognition accuracy. To maintain acceptable recognition performance of both, spoken content and affective state, we consider matched and multi-condition training and apply our novel multi-stream automatic speech recognition system which outperforms conventional Hidden Markov Modeling. Depending on the acoustic condition, we obtain unweighted emotion recognition accuracies of between 65.4% and 70.3% applying our multi-stream system in combination with the SimpleLogistic algorithm for joint acoustic-linguistic analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-779"
  },
  "maazaoui11_interspeech": {
   "authors": [
    [
     "Mounira",
     "Maazaoui"
    ],
    [
     "Yves",
     "Grenier"
    ],
    [
     "Karim",
     "Abed-Meraim"
    ]
   ],
   "title": "Blind source separation for robot audition using fixed beamforming with HRTFs",
   "original": "i11_3117",
   "page_count": 4,
   "order": 782,
   "p1": "3117",
   "pn": "3120",
   "abstract": [
    "We present a two stage blind source separation (BSS) algorithm for robot audition. The algorithm is based on a beamforming preprocessing and a BSS algorithm using a sparsity separation criterion. Before the BSS step, we filter the sensors outputs by beamforming filters to reduce the reverberation and the environmental noise. As we are in a robot audition context, the manifold of the sensor array in this case is hard to model, so we use pre-measured Head Related Transfer Functions (HRTFs) to estimate the beamforming filters. In this article, we show the good performance of this method as compared to a single stage BSS only method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-780"
  },
  "tahon11_interspeech": {
   "authors": [
    [
     "Marie",
     "Tahon"
    ],
    [
     "Agnes",
     "Delaborde"
    ],
    [
     "Laurence",
     "Devillers"
    ]
   ],
   "title": "Real-life emotion detection from speech in human-robot interaction: experiments across diverse corpora with child and adult voices",
   "original": "i11_3121",
   "page_count": 4,
   "order": 783,
   "p1": "3121",
   "pn": "3124",
   "abstract": [
    "We focus in this paper on the detection of the emotions in the voice of a speaker in a Human-Robot Interaction context. This work is part of the ROMEO project, which aims to design a robot for both elderly people and children. Our system offers several modules based on a multi-level processing of the audio cues. The affective markers produced by these different modules will allow to pilot the emotional behaviour of the robot. Since the models are built with recording data and the system will test real-life data, we need to estimate our emotion detection system performances in cross-corpus situations. Cross-validation experiments on a three class detection show that derivatives and energy features may be removed from our feature set for this specific task. Cross-corpora experiments on anger-positive-neutral data suggest that detection performances may be better with two different models: one for child voices, one for adult voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-781"
  },
  "attabi11_interspeech": {
   "authors": [
    [
     "Yazid",
     "Attabi"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Weighted ordered classes - nearest neighbors: a new framework for automatic emotion recognition from speech",
   "original": "i11_3125",
   "page_count": 4,
   "order": 784,
   "p1": "3125",
   "pn": "3128",
   "abstract": [
    "In this paper we present a new framework for emotion recognition from speech based on a similarity concept called Weighted Ordered Classes-Nearest Neighbors (WOC-NN). Unlike the k-nearest neighbor, an instance-similarity based method; WOC-NN computes similarities between a test instance and a class pattern of each emotion class. An emotion class pattern is a representation of its ranked neighboring classes. A Hamming distance is used as distance metric, enhanced with two improvements: i) weighting the importance of each class rank of each neighborhood pattern and ii) discarding irrelevant class ranks from the patterns. Thus, the decision process in WOC-NN exploits more information than Bayes rule which makes use only of the information about the model class that minimizes Bayes risk. This extra information allows WOC-NN to get more accurate prediction. Also, the results show that the proposed system outperforms the result of state-of-the art systems when applied to the FAU AIBO corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-782"
  },
  "doukhan11_interspeech": {
   "authors": [
    [
     "David",
     "Doukhan"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Prosodic analysis of a corpus of tales",
   "original": "i11_3129",
   "page_count": 4,
   "order": 785,
   "p1": "3129",
   "pn": "3132",
   "abstract": [
    "This paper presents a prosodic analysis of a corpus of 12 tales, read by one male speaker. The work is part of a project which aims at providing storytelling capacities to a humanoid robot. One main point is to improve text-to-speech synthesis expressivity according to a semi-automatic analysis of a given tale. Automatic tagging and prosodic stylization were applied to the corpus. The extracted parameters are described and analyzed according to relevant elements of the tales' structure. The results show that changes of pitch and intensity registers, use of glide tones or devoicing are relevant to impersonate prototypical characters and to modify expressivity according to the different structural parts of each tale. These prosodic results will contribute to enhance the expressivity of a non-uniform-unit text-to-speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-783"
  },
  "ishi11b_interspeech": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Analysis of acoustic-prosodic features related to paralinguistic information carried by interjections in dialogue speech",
   "original": "i11_3133",
   "page_count": 4,
   "order": 786,
   "p1": "3133",
   "pn": "3136",
   "abstract": [
    "Interjections are often used in dialogue communication for expressing a reaction (such as agreement, surprise and disgust) to the interlocutor. Thus, a correct interpretation of the paralinguistic information (intention, attitude or emotion) carried by interjections is important for achieving a smooth dialogue interaction between humans and robots. In the present work, analyses are conducted on several interjections appearing in spontaneous conversational speech databases to investigate the relationship between acousticprosodic features (related to intonation and voice quality) and their paralinguistic functions in dialogue speech. It is found that there are common and interjection-dependent relationships between acoustic features and paralinguistic information. Regardless of the interjection type, non-modal voice qualities, such as whispery, harsh and pressed voices, are shown to be important cues for the expression of emotions and attitudes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-784"
  },
  "heckmann11b_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Hirofumi",
     "Nakajima"
    ]
   ],
   "title": "Robust intonation pattern classification in human robot interaction",
   "original": "i11_3137",
   "page_count": 4,
   "order": 787,
   "p1": "3137",
   "pn": "3140",
   "abstract": [
    "We present a system for the classification of intonation patterns in human robot interaction. The system distinguishes questions from other types of utterances and can deal with additional reverberations, background noise, as well as music interfering with the speech signal. The main building blocks of our system are a multi channel source separation, robust fundamental frequency extraction and tracking, segmentation of the speech signal, and classification of the fundamental frequency pattern of the last speech segment. We evaluate the system with Japanese sentences which are ambiguous without intonation information in a realistic human robot interaction scenario. Despite the challenging task our system is able to classify the intonation pattern with good accuracy. With several experiments we evaluate the contribution of the different aspects of our system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-785"
  },
  "sumiyoshi11_interspeech": {
   "authors": [
    [
     "Takashi",
     "Sumiyoshi"
    ],
    [
     "Masahito",
     "Togami"
    ],
    [
     "Yasunari",
     "Obuchi"
    ]
   ],
   "title": "ASR for human-symbiotic robot “EMIEW2” with mechanical noise and floor-level noise reduction",
   "original": "i11_3141",
   "page_count": 4,
   "order": 788,
   "p1": "3141",
   "pn": "3144",
   "abstract": [
    "A human-symbiotic robot called \"EMIEW2\" and its auditory function which includes two noise reduction methods against self-generated mechanical noise and external floor-level noise is introduced. The former type of noise is produced by the robot itself, and this is a difficult problem because it can be loud, non-stationary, and have a wide frequency band. We adopt a maximized SNR technique, in which noise correlation matrix is selected from noise clusters that are learned from the pre-recorded noise signals. The latter type of noise, which can occur when robots are used in office environments, is also a problem, and we addressed it by expanding the beamforming area from one dimension (azimuth angle) to the two dimensions (azimuth and elevation angles). We evaluated these methods in a 100-word speech recognition task and we show that both methods are effective for improving the speech recognition rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-786"
  },
  "vu11b_interspeech": {
   "authors": [
    [
     "Ngoc Thang",
     "Vu"
    ],
    [
     "Franziska",
     "Kraus"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Rapid building of an ASR system for under-resourced languages based on multilingual unsupervised training",
   "original": "i11_3145",
   "page_count": 4,
   "order": 789,
   "p1": "3145",
   "pn": "3148",
   "abstract": [
    "This paper presents our work on rapid language adaptation of acoustic models based on multilingual cross-language bootstrapping and unsupervised training. We used Automatic Speech Recognition (ASR) systems in the six source languages English, French, German, Spanish, Bulgarian and Polish to build from scratch an ASR system for Vietnamese, an under-resourced language. System building was performed without using any transcribed audio data by applying three consecutive steps, i.e. cross-language transfer, unsupervised training based on the \"multilingual A-stabil\" confidence score [1], and bootstrapping. We investigated the correlation between performance of \"multilingual A-stabil\" and the number of source languages and improved the performance of \"multilingual A-stabil\" by applying it at the syllable level. Furthermore, we showed that increasing the amount of source language ASR systems for the multilingual framework results in better performance of the final ASR system in the target language Vietnamese. The final Vietnamese recognition system has a Syllable Error Rate (SyllER) of 16.8% on the development set and 16.1% on the evaluation set.\n",
    "",
    "",
    "N. T. Vu, F. Kraus and T. Schultz. Multilingual A-stabil: A new confidence score for multilingual unsupervised training. In IEEE Workshop on Spoken Language Technology, SLT 2010, Berkeley, California, USA, 2010.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-787"
  },
  "mandal11_interspeech": {
   "authors": [
    [
     "Shyamal Kr. Das",
     "Mandal"
    ],
    [
     "Somnath",
     "Chandra"
    ],
    [
     "Swaran",
     "Lata"
    ],
    [
     "A. K.",
     "Datta"
    ]
   ],
   "title": "Places and manner of articulation of Bangla consonants: an EPG based study",
   "original": "i11_3149",
   "page_count": 4,
   "order": 790,
   "p1": "3149",
   "pn": "3152",
   "abstract": [
    "Bangla phoneme inventory consists of 32 consonants out of those 16 are stop or plosive and 4 are affricate. This paper presents the detailed investigation of place and manner of articulation of Bengali phonemes. The place of articulation study of consonants is based on Electropalatography (EPG) system and the manner of articulation study is based on acoustic study of large number of well-spoken Bengali VCV sequences in which V represents the seven Bangla vowels /u/, /o/, /O/, /a/, /æ/, /e/ and /i/ while C represents all the consonants of Bangla. The study shows that in case of Bangla language plosives have three distinct places of articulation namely dental, alveolar and post alveolar and four manner of articulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-788"
  },
  "davel11_interspeech": {
   "authors": [
    [
     "Marelie H.",
     "Davel"
    ],
    [
     "Charl van",
     "Heerden"
    ],
    [
     "Neil",
     "Kleynhans"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Efficient harvesting of internet audio for resource-scarce ASR",
   "original": "i11_3153",
   "page_count": 4,
   "order": 791,
   "p1": "3153",
   "pn": "3156",
   "abstract": [
    "Spoken recordings that have been transcribed for human reading (e.g. as captions for audiovisual material, or to provide alternative modes of access to recordings) are widely available in many languages. Such recordings and transcriptions have proven to be a valuable source of ASR data in well-resourced languages, but have not been exploited to a significant extent in under-resourced languages or dialects. Techniques used to harvest such data typically assume the availability of a fairly accurate ASR system, which is generally not available when working with resource-scarce languages. In this work, we define a process whereby an ASR corpus is bootstrapped using unmatched ASR models in conjunction with speech and approximate transcriptions sourced from the Internet. We introduce a new segmentation technique based on the use of a phone-internal garbage model, and demonstrate how this technique (combined with limited filtering) can be used to develop a large, high-quality corpus in an under-resourced dialect with minimal effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-789"
  },
  "secujski11_interspeech": {
   "authors": [
    [
     "Milan",
     "Sečujski"
    ],
    [
     "Darko",
     "Pekar"
    ],
    [
     "Nikša",
     "Jakovljević"
    ]
   ],
   "title": "Automatic prosody generation for serbo-croatian speech synthesis based on regression trees",
   "original": "i11_3157",
   "page_count": 4,
   "order": 792,
   "p1": "3157",
   "pn": "3160",
   "abstract": [
    "The paper presents the module for automatic generation of prosodic features of synthesized speech, namely, f0 targets and phonetic segment durations, within the speech synthesizer AlfaNumTTS, the most sophisticated speech synthesis system for Serbo-Croatian language to date. The module is based on regression trees trained on a studio recorded single speaker database of Serbo-Croatian. The database has been annotated for phonemic identity as well as a number of prosodic events such as pitch accents, phrase breaks and prosodic prominence. Besides the traditional description of the intonational phonology of Serbo-Croatian through four distinct accent types, within this study we have examined the possibility of representing them as tonal sequences, which has been suggested in recent linguistic literature. The results obtained confirm that the four accents can indeed be reduced to sequences of high and low tones without loss of quality, provided that phonemic length contrast is preserved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-790"
  },
  "karpov11_interspeech": {
   "authors": [
    [
     "Alexey",
     "Karpov"
    ],
    [
     "Irina",
     "Kipyatkova"
    ],
    [
     "Andrey",
     "Ronzhin"
    ]
   ],
   "title": "Very large vocabulary ASR for spoken Russian with syntactic and morphemic analysis",
   "original": "i11_3161",
   "page_count": 4,
   "order": 793,
   "p1": "3161",
   "pn": "3164",
   "abstract": [
    "In this paper, we present a word-based very large vocabulary automatic speech recognition system for Russian. Some novel methods are proposed for organization of the lexicon and the language model. Two-level morpho-phonemic prefix graph that uses some information on morphemic structure of lexical units is suggested for a compact representation of the pronunciation vocabulary and search space. Such model is more compact than the lexical tree or the linearly-based vocabulary and provides speeding up the recognition process. The syntactic analysis of a training text corpus in a combination with the statistical analysis is suggested for generation of N-gram language models. The syntax-based Russian language model allows taking into account long-distance syntactic dependencies between word pairs. The results have proved that the syntactic-statistic language model gives 5% relative improvement on the word and letter error rates with respect to the baseline models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-791"
  },
  "kempton11_interspeech": {
   "authors": [
    [
     "Timothy",
     "Kempton"
    ],
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Cross-language phone recognition when the target language phoneme inventory is not known",
   "original": "i11_3165",
   "page_count": 4,
   "order": 794,
   "p1": "3165",
   "pn": "3168",
   "abstract": [
    "Cross-language speech recognition often assumes a certain amount of knowledge about the target language. However, there are hundreds of languages where not even the phoneme inventory is known. In the work reported here, phone recognisers are evaluated on a cross-language task with minimum target knowledge. A phonetic distance measure is introduced for the evaluation, allowing a distance to be calculated between any utterance of any language. This has a number of spin-off applications such as allophone detection, a phone-based ROVER approach to recognition, and cross-language forced alignment. Results show that some of these novel approaches will be of immediate use in characterising languages where there is little phonological knowledge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-792"
  },
  "chaudhuri11b_interspeech": {
   "authors": [
    [
     "Sourish",
     "Chaudhuri"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Tony",
     "Ezzat"
    ]
   ],
   "title": "A paradigm for limited vocabulary speech recognition based on redundant spectro-temporal feature sets",
   "original": "i11_3169",
   "page_count": 4,
   "order": 795,
   "p1": "3169",
   "pn": "3172",
   "abstract": [
    "Speech recognition techniques have come to rely almost completely on HMM based frameworks. In this paper, we present a novel paradigm for small-vocabulary speech recognition based on a recently proposed word spotting technique. Recent work using discriminative classifiers with ordered spectro-temporal features to detect the presence of keywords obtained encouraging improvements over HMM-based models. We propose to extend this approach to recognize continuous speech in our work. Our method uses discriminative models to predict which words are present in a speech signal and hypothesize their locations. A graph search using dynamic programming is then used to obtain the most likely sequence of words from the hypothesis set produced as a result of combining the results from the discriminative word classifiers. While this approach doesn't perform as well as state-of-the-art ASR systems, it can be particularly useful for languages with small amounts of annotated data available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-793"
  },
  "barroso11_interspeech": {
   "authors": [
    [
     "N.",
     "Barroso"
    ],
    [
     "K.",
     "López de Ipiña"
    ],
    [
     "A.",
     "Ezeiza"
    ],
    [
     "C.",
     "Hernández"
    ],
    [
     "N.",
     "Ezeiza"
    ],
    [
     "O.",
     "Barroso"
    ],
    [
     "U.",
     "Susperregi"
    ],
    [
     "S.",
     "Barroso"
    ]
   ],
   "title": "Gorup: an ontology-driven audio information retrieval system that suits the requirements of under-resourced languages",
   "original": "i11_3173",
   "page_count": 4,
   "order": 796,
   "p1": "3173",
   "pn": "3176",
   "abstract": [
    "GorUp is an Information Retrieval system that provides information about the contents of audio broadcast news in Basque, Spanish, and French. Since the resources available for Basque in general, and for this task in particular, were very few, data optimization methodologies had to be applied in various phases of the development. Moreover, the agglutinative nature of Basque required the use of morphemes and other sub-word units. Additionally, some keyword spotting and semantic methods have been also applied in the system in order to retrieve information properly. In most of the cases, the methods employed during this project could suit the requirements of many under-resourced languages, and one of these techniques could be the ontology-based approach. This paper presents the system in general for Basque and emphasizes the techniques employed in order to enhance the system using a semantic ontology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-794"
  },
  "vries11_interspeech": {
   "authors": [
    [
     "Nic J. de",
     "Vries"
    ],
    [
     "Jaco",
     "Badenhorst"
    ],
    [
     "Marelie H.",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Alta de",
     "Waal"
    ]
   ],
   "title": "Woefzela - an open-source platform for ASR data collection in the developing world",
   "original": "i11_3177",
   "page_count": 4,
   "order": 797,
   "p1": "3177",
   "pn": "3180",
   "abstract": [
    "Building transcribed speech corpora for under-resourced languages plays a pivotal role in developing speech technologies for such languages. We have developed an open-source tool for devices running the Android operating system to facilitate the efficient collection of speech data for Automatic Speech Recognition system development. The tool was designed for use in typical developingworld conditions; we present the relevant design choices and analyse the effectiveness of this tool by means of a case study. In particular, we introduce a novel semi-real-time quality monitoring system, which increases the efficiency of the data collection process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-795"
  },
  "mixdorff11_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Lehlohonolo",
     "Mohasi"
    ],
    [
     "'Malillo",
     "Machobane"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "A study on the perception of tone and intonation in Sesotho",
   "original": "i11_3181",
   "page_count": 4,
   "order": 798,
   "p1": "3181",
   "pn": "3184",
   "abstract": [
    "This paper presents a study on the perception of Sesotho, a Southern African tonal language, employing a set of recorded minimal pairs, whose F0 contours were analyzed in a previous study using the Fujisaki model and resynthesized. Sequences of prosodically modified stimuli were produced to examine the effect of these modifications on word identification, statement/question distinction, as well as focus identification. With few exceptions, results regarding word identification are in line with our expectations. F0 modifications even seem to override vowel differences between words when both affect its meaning. With respect to the statement/ question distinction, shortening of the penultimate syllable, higher speech rate and increased phrase command magnitude Ap all increase the probability of an utterance to be perceived as a question. The focus experiment, however, produced inconclusive results, possibly due to its complex setting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-796"
  },
  "wet11_interspeech": {
   "authors": [
    [
     "Febe de",
     "Wet"
    ],
    [
     "Alta de",
     "Waal"
    ],
    [
     "Gerhard B. van",
     "Huyssteen"
    ]
   ],
   "title": "Developing a broadband automatic speech recognition system for Afrikaans",
   "original": "i11_3185",
   "page_count": 4,
   "order": 799,
   "p1": "3185",
   "pn": "3188",
   "abstract": [
    "Afrikaans is one of the eleven official languages of South Africa. It is classified as an under-resourced language. No annotated broadband speech corpora currently exist for Afrikaans. This article reports on the development of speech resources for Afrikaans, specifically a broadband speech corpus and an extended pronunciation dictionary. Baseline results for an ASR system that was built using these resources are also presented. In addition, the article suggests different strategies to exploit the close relationship between Afrikaans and Dutch for the purposes of technology development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-797"
  },
  "kamper11_interspeech": {
   "authors": [
    [
     "Herman",
     "Kamper"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "Multi-accent speech recognition of Afrikaans, black and white varieties of south african English",
   "original": "i11_3189",
   "page_count": 4,
   "order": 800,
   "p1": "3189",
   "pn": "3192",
   "abstract": [
    "In this paper we investigate speech recognition performance of systems employing several accent-specific recognisers in parallel for the simultaneous recognition of multiple accents. We compare these systems with oracle systems, in which test utterances are presented to matching accent-specific recognisers, and with accent-independent systems, in which acoustic and language model training data are pooled. Our investigation is based on Afrikaans (AE), Black (BE) and White (EE) accents of South African English. We find that, when accent is classified on a per-utterance basis, parallel systems outperform oracle systems for the AE+EE accent pair while the opposite is observed for BE+EE. When accent identification is carried out on a per-speaker basis, oracle or better performance is obtained for both accent pairs. Furthermore, parallel systems based on multi-accent acoustic modelling, which allows selective cross-accent sharing of acoustic training data, outperform parallel systems using accent-specific acoustic models. The former also yields better performance than accent-independent recognition, which uses pooled acoustic and language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-798"
  },
  "tantibundhit11_interspeech": {
   "authors": [
    [
     "C.",
     "Tantibundhit"
    ],
    [
     "C.",
     "Onsuwan"
    ],
    [
     "T.",
     "Saimai"
    ],
    [
     "N.",
     "Saimai"
    ],
    [
     "S.",
     "Thatphithakkul"
    ],
    [
     "P.",
     "Chootrakool"
    ],
    [
     "K.",
     "Kosawat"
    ],
    [
     "N.",
     "Thatphithakkul"
    ]
   ],
   "title": "Perceptual representation of consonant sounds in Thai",
   "original": "i11_3193",
   "page_count": 4,
   "order": 801,
   "p1": "3193",
   "pn": "3196",
   "abstract": [
    "This work is an attempt to construct a perceptual representation of Thai consonants based on perceptual identification results (from 28 Thais) of 21 phonemes presented in noise. The experiment is designed to equally make pairwise comparisons among 21 wordinitial phonemes, which results in 210 real-word stimulus pairs. Percent correct responses and confusion matrices are obtained. Similarity score and perceptual distance for each phoneme pair are systematically derived from confusion scores based on a method proposed by Shepard (Psychological Representation of Speech Sounds, 1972). Piecing this together pair by pair, a perceptual space or representation of Thai consonants takes shape. The perceptual space could roughly be divided into 5 non-overlapping groupings: glide, glottal constriction, nasality, aspirated obstruent, and a combination of liquid and unaspirated obstruent. It is suggested that these phonological classes reflect the most distinct and relevant perceptual properties of Thai consonants. Preliminary cross-linguistic observation is addressed in light of the data of English consonants from Miller and Nicely (J. Acoust. Soc. Am., 1955).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-799"
  },
  "mustafa11_interspeech": {
   "authors": [
    [
     "Mumtaz B.",
     "Mustafa"
    ],
    [
     "Raja N.",
     "Ainon"
    ],
    [
     "Roziati",
     "Zainuddin"
    ],
    [
     "Zuraidah M.",
     "Don"
    ],
    [
     "Gerry",
     "Knowles"
    ]
   ],
   "title": "A cross-lingual approach to the development of an HMM-based speech synthesis system for malay",
   "original": "i11_3197",
   "page_count": 4,
   "order": 802,
   "p1": "3197",
   "pn": "3200",
   "abstract": [
    "This research reports the development of an HMM-based speech synthesis system for Malay, which is an under-resourced language with few resources including recorded speech and segmental labels. We propose the cross-lingual use of resources for developing a Malay HMM-based speech synthesis system. We used the Festival English speech synthesis system to generate time-aligned phone transcriptions for Malay using specially constructed Malay grapheme-to-phoneme database and English CART. These transcriptions together with Malay recorded speech databases were used for training and synthesis of Malay speech. The effectiveness of the proposed approach is confirmed by intelligibility and naturalness tests on the synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-800"
  },
  "schuller11b_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Florian",
     "Schiel"
    ],
    [
     "Jarek",
     "Krajewski"
    ]
   ],
   "title": "The INTERSPEECH 2011 speaker state challenge",
   "original": "i11_3201",
   "page_count": 4,
   "order": 803,
   "p1": "3201",
   "pn": "3204",
   "abstract": [
    "While the first open comparative challenges in the field of paralinguistics targeted more 'conventional' phenomena such as emotion, age, and gender, there still exists a multiplicity of not yet covered, but highly relevant speaker states and traits. The INTERSPEECH 2011 Speaker State Challenge thus addresses two new sub-challenges to overcome the usually low compatibility of results: In the Intoxication Sub-Challenge, alcoholisation of speakers has to be determined in two classes; in the Sleepiness Sub-Challenge, another two-class classification task has to be solved. This paper introduces the conditions, the Challenge corpora \"Alcohol Language Corpus\" and \"Sleepy Language Corpus\", and a standard feature set that may be used. Further, baseline results are given.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-801"
  },
  "montacie11_interspeech": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "Combining multiple phoneme-based classifiers with audio feature-based classifier for the detection of alcohol intoxication",
   "original": "i11_3205",
   "page_count": 4,
   "order": 804,
   "p1": "3205",
   "pn": "3208",
   "abstract": [
    "This article describes the two systems which we submitted for the Intoxication Sub-Challenge of INTERSPEECH 2011 Speaker State Challenge. At first, we developed an Extended Baseline System with a significant improvement of the unweighted accuracy compared to the Official Baseline System (OBS) on the development set. Then, we investigated the phonetic variations of speech under alcoholisation and developed gender-dependent Phoneme-based SVM classifiers. For this purpose, we selected the most relevant phonemes and investigated the combination of six Phoneme-based SVM classifiers (MP). Its results in accuracy are slightly below the OBS results. Finally, the combination of the two systems is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-802"
  },
  "biadsy11b_interspeech": {
   "authors": [
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "William Yang",
     "Wang"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Intoxication detection using phonetic, phonotactic and prosodic cues",
   "original": "i11_3209",
   "page_count": 4,
   "order": 805,
   "p1": "3209",
   "pn": "3212",
   "abstract": [
    "In this paper, we investigate multiple approaches for automatically detecting intoxicated speakers given samples of their speech. Intoxicated speech in a given language can be viewed simply as a different accent of this language; therefore we adopt our recent approach to dialect and accent recognition to detect intoxication. The system models phonetic structural differences across sober and intoxicated speakers. This approach employs SVM with a kernel function that computes similarities between adapted phone GMMs which summarize speakers' phonetic characteristics in their utterances. We also investigate additional cues, such as prosodic events, phonotactics and phonetic durations under intoxicated and sober conditions. We find that our phonetic-based system when combined with phonotactic features provides us with our best result on the official development set, an accuracy of 73% and an equal error rate of 26.3%, significantly higher than the official baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-803"
  },
  "bocklet11_interspeech": {
   "authors": [
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Drink and speak: on the automatic classification of alcohol intoxication by acoustic, prosodic and text-based features",
   "original": "i11_3213",
   "page_count": 4,
   "order": 806,
   "p1": "3213",
   "pn": "3216",
   "abstract": [
    "This paper focuses on the automatic detection of a person's blood level alcohol based on automatic speech processing approaches. We compare 5 different feature types with different ways of modeling. Experiments are based on the ALC corpus of IS2011 Speaker State Challenge. The classification task is restricted to the detection of a blood alcohol level above 0.5‰. Three feature sets are based on spectral observations: MFCCs, PLPs, TRAPS. These are modeled by GMMs. Classification is either done by a Gaussian classifier or by SVMs. In the later case classification is based on GMM-based supervectors, i.e. concatenation of GMM mean vectors. A prosodic system extracts a 292-dimensional feature vector based on a voiced-unvoiced decision. A transcription-based system makes use of text transcriptions related to phoneme durations and textual structure. We compare the stand-alone performances of these systems and combine them on score level by logistic regression. The best stand-alone performance is the transcriptionbased system which outperforms the baseline by 4.8% on the development set. A Combination on score level gave a huge boost when the spectral-based systems were added (73.6%). This is a relative improvement of 12.7% to the baseline. On the test-set we achieved an UA of 68.6% which is a significant improvement of 4.1% to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-804"
  },
  "bone11_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bone"
    ],
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Angeliki",
     "Metallinou"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Intoxicated speech detection by fusion of speaker normalized hierarchical features and GMM supervectors",
   "original": "i11_3217",
   "page_count": 4,
   "order": 807,
   "p1": "3217",
   "pn": "3220",
   "abstract": [
    "Speaker state recognition is a challenging problem due to speaker and context variability. Intoxication detection is an important area of paralinguistic speech research with potential real-world applications. In this work, we build upon a base set of various static acoustic features by proposing the combination of several different methods for this learning task. The methods include extracting hierarchical acoustic features, performing iterative speaker normalization, and using a set of GMM supervectors. We obtain an optimal unweighted recall for intoxication recognition using score-level fusion of these subsystems. Unweighted average recall performance is 70.54% on the test set, an improvement of 4.64% absolute (7.04% relative) over the baseline model accuracy of 65.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-805"
  },
  "ultes11_interspeech": {
   "authors": [
    [
     "Stefan",
     "Ultes"
    ],
    [
     "Alexander",
     "Schmitt"
    ],
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Attention, sobriety checkpoint! can humans determine by means of voice, if someone is drunk… and can automatic classifiers compete?",
   "original": "i11_3221",
   "page_count": 4,
   "order": 808,
   "p1": "3221",
   "pn": "3224",
   "abstract": [
    "This paper analyzes the human performance of recognizing drunk speakers merely by voice and compares the results with the performance of an automatic statistical classifier. The study is carried out within the Interspeech 2011 Speaker State Challenge [1] employing the Alcohol Language Corpus (ALC) [2]. The 79 subjects yielded an average performance of 55.8% unweighted accuracy on a balanced intoxicated/non-intoxicated sample set. The statistical classifier developed in this study reaches a performance of 66.6% unweighted accuracy on the test set. In comparison, the subject with the highest performance yielded 70.0%. Our classifier is based on 4368 acoustic and prosodic features. Incorporating linguistic features along with feature selection using Information Gain Ratio (IGR) ranking added 0.7% absolute improvement with resulting in a 29% smaller feature space size.\n",
    "s B. Schuller, S. Steidl, A. Batliner, F. Schiel, and J. Krajewski, The interspeech 2011 speaker state challenge, in Proc. of the International Conference on Speech and Language Processing (ICSLP), Aug. 2011. S. B. Florian Schiel, Christian Heinrich and T. Gilg, Alc: Alcohol language corpus, in Proc. of LREC. Marrakech, Morocco: European Language Resources Association (ELRA), may 2008.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-806"
  },
  "honig11_interspeech": {
   "authors": [
    [
     "Florian",
     "Hönig"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Does it groove or does it stumble - automatic classification of alcoholic intoxication using prosodic features",
   "original": "i11_3225",
   "page_count": 4,
   "order": 809,
   "p1": "3225",
   "pn": "3228",
   "abstract": [
    "This paper studies how prosodic features can help in the automatic detection of alcoholic intoxication. We compute features that have recently been proposed to model speech rhythm such as the pair-wise variability index for consonantal and vocalic segments (PVI) and study their aptness for the task. Further, we use a large prosodic feature vector modelling the usual candidates . pitch, intensity, and duration . and apply it onto different units such as words, syllables and stressed syllables to create generalizations of the rhythm features mentioned. The results show that the prosodic features computed are suitable for detecting alcoholic intoxication and add complementary information to state-of-the-art features. The database is the intoxication database provided by the organizers of the 2011 Interspeech Speaker State Challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-807"
  },
  "schiel11_interspeech": {
   "authors": [
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Perception of alcoholic intoxication in speech",
   "original": "i11_3281",
   "page_count": 4,
   "order": 810,
   "p1": "3281",
   "pn": "3284",
   "abstract": [
    "The ALC sub-challenge of the Interspeech Speaker State Challenge (ISSC) aims at the automatic classification of speech signals into intoxicated and sober speech. In this context we conducted a perception experiment on data derived from the same corpus to analyze the human performance on the same task. The results show that human still outperform comparable baseline results of ISSC. Female and male listeners perform on the same level, but there is strong evidence that intoxication in female voices is easier to be recognized than in male voices. Prosodic features contribute to the decision of human listeners but seem not to be dominant. In analogy to Doddington's zoo of speaker verification we find some evidence for the existence of lambs and goats but no wolves.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-808"
  },
  "rahman11_interspeech": {
   "authors": [
    [
     "Tauhidur",
     "Rahman"
    ],
    [
     "Soroosh",
     "Mariooryad"
    ],
    [
     "Shalini",
     "Keshavamurthy"
    ],
    [
     "Gang",
     "Liu"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Detecting sleepiness by fusing classifiers trained with novel acoustic features",
   "original": "i11_3285",
   "page_count": 4,
   "order": 811,
   "p1": "3285",
   "pn": "3288",
   "abstract": [
    "Automatic sleepiness detection is a challenging task that can lead to advances in various domains including traffic safety, medicine and human-machine interaction. This paper analyzes the discriminative power of different acoustic features to detect sleepiness. The study uses the sleepy language corpus (SLC). Along with standard acoustic features, novel features are proposed including functionals across voiced segment statistics in the F0 contour, likelihoods of reference models used to contrast non-neutral speech, and a set of robust to noise spectral features. These feature sets, which have performed well in other paralinguistic tasks such as emotion recognition, are used to train classifiers that are combined at the feature and decision levels. The best unweighted accuracy (UA) is obtained by combining the classifiers at the decision level under a maximum likelihood framework (UA = 70.97%). This performance is higher than the best results reported in the corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-809"
  },
  "rodriguez11_interspeech": {
   "authors": [
    [
     "Albino Nogueiras",
     "Rodríguez"
    ]
   ],
   "title": "An HMM-based approach to the INTERSPEECH 2011 speaker state challenge",
   "original": "i11_3289",
   "page_count": 4,
   "order": 812,
   "p1": "3289",
   "pn": "3292",
   "abstract": [
    "The current main trend in paralinguistic information recognition is the so-called static classification. In this kind of classification the low level descriptors are pooled together by means of statistical functionals and all, or almost all, information about the temporal structure and evolution of speech is lost. Although this approach represents the state-of-the-art, we believe that dynamic classification, where temporal information is kept, still deserves some attention due to its capability to handle aspects impossible to do by the static one. In this paper the INTERSPEECH 2011 Speaker State Challenged is addressed using the Automatic Speech Recognition system developed at UPC, which has already been used in a similar task: emotion recognition. Although results fall below the baseline, we believe that they are close enough to be taken into account.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-810"
  },
  "bozkurt11_interspeech": {
   "authors": [
    [
     "Elif",
     "Bozkurt"
    ],
    [
     "Engin",
     "Erzin"
    ],
    [
     "Çiğdem Eroğlu",
     "Erdem"
    ],
    [
     "A. Tanju",
     "Erdem"
    ]
   ],
   "title": "RANSAC-based training data selection for speaker state recognition",
   "original": "i11_3293",
   "page_count": 4,
   "order": 813,
   "p1": "3293",
   "pn": "3296",
   "abstract": [
    "We present a Random Sampling Consensus (RANSAC) based training approach for the problem of speaker state recognition from spontaneous speech. Our system is trained and tested with the INTERSPEECH 2011 Speaker State Challenge corpora that includes the Intoxication and the Sleepiness Sub-challenges, where each subchallenge defines a two-class classification task. We aim to perform a RANSAC-based training data selection coupled with the Support Vector Machine (SVM) based classification to prune possible outliers, which exist in the training data. Our experimental evaluations indicate that utilization of RANSAC-based training data selection provides 66.32% and 65.38% unweighted average (UA) recall rate on the development and test sets for the Sleepiness Subchallenge, respectively and a slight improvement on the Intoxication Sub-challenge performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-811"
  },
  "gajsek11_interspeech": {
   "authors": [
    [
     "Rok",
     "Gajšek"
    ],
    [
     "Simon",
     "Dobrišek"
    ],
    [
     "France",
     "Mihelič"
    ]
   ],
   "title": "University of Ljubljana system for interspeech 2011 speaker state challenge",
   "original": "i11_3297",
   "page_count": 4,
   "order": 814,
   "p1": "3297",
   "pn": "3300",
   "abstract": [
    "The paper presents our efforts in the Interspeech 2011 Speaker State Challenge. Both systems, for the Intoxication and the Sleepiness Sub-Challenge, are based on a Universal Background Model (UBM) in a form of a Hidden Markov Model (HMM), and the Maximum A Posteriori (MAP) adaptation. With the combination of our HMM-UBM-MAP derived supervectors and selected statistical functionals from the baseline feature set, we were able to surpass the baseline system in both sub-challenges. By employing majority voting fusion of best systems we were able to further improve the performance. In the Intoxication Sub-Challenge our best result on the test set is 67.46%, and in the Sleepiness Sub-Challenge 71.28%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-812"
  },
  "huang11i_interspeech": {
   "authors": [
    [
     "Dong-Yan",
     "Huang"
    ],
    [
     "Shuzhi Sam",
     "Ge"
    ],
    [
     "Zhengchen",
     "Zhang"
    ]
   ],
   "title": "Speaker state classification based on fusion of asymmetric SIMPLS and support vector machines",
   "original": "i11_3301",
   "page_count": 4,
   "order": 815,
   "p1": "3301",
   "pn": "3304",
   "abstract": [
    "This paper describes a Speaker State Classification System (SSCS) for the INTERSPEECH 2011 Speaker State Challenge. Our SSC system for the Intoxication and Sleepiness Sub-Challenges uses fusion of several individual sub-systems. We make use of three standard feature sets per corpus given by organizers. Modeling is based on our own developed classification method . Asymmetric simple partial least squares (ASIMPLS) and Support Vector Machines (SVMs), followed by the calibration and multiple fusion methods. The advantage of asymmetric SIMPLS is prone to protect the minority class from being misclassified and boosts the performance on the majority class. Our experimental results show that our SSC system performs better than baseline system. Our final fusion results in 1.8% absolute improvement on the unweighted accuracy value for the Alcohol Language Corpus (ALC) and about 0.7% for the Sleepy Language Corpus (SLC) on the development set over the baseline. On the test set, we obtain 1.1% and 1.4% absolute improvement, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-813"
  },
  "draxler11_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Mark",
     "Liberman"
    ],
    [
     "Peter",
     "Wittenburg"
    ]
   ],
   "title": "Speech processing tools - an introduction to interoperability",
   "original": "i11_3229",
   "page_count": 4,
   "order": 816,
   "p1": "3229",
   "pn": "3232",
   "abstract": [
    "Research and development in the field of spoken language depends critically on the existence of software tools. A large range of excellent tools have been developed and are widely used today. Most tools were developed by individuals who recognized the need for a given tool, had the necessary conceptual and programming skills, and were deeply rooted in the application field, namely spoken language.\n",
    "Excellent tools are a prerequisite to research. However, tool developers rarely receive academic recognition for their efforts. Journals, conferences and funding agencies are interested in the results of the work on a research question while the tools developed to achieve these results are of less interest. This makes it difficult to publish articles on tools, and it is next to impossible to obtain funding for their development.\n",
    "The Interspeech 2011 special event on speech processing tools aims to provide a forum for tool developers to improve their academic visibility and thus enhance their motivation to continue developing the software needed by the community. This year, the focus is on interoperability . how can different tools be integrated into the workflow, data be exchanged between tools, queries work across tools, and a consistent user interface be achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-814"
  },
  "goldman11_interspeech": {
   "authors": [
    [
     "Jean-Philippe",
     "Goldman"
    ]
   ],
   "title": "Easyalign: an automatic phonetic alignment tool under praat",
   "original": "i11_3233",
   "page_count": 4,
   "order": 817,
   "p1": "3233",
   "pn": "3236",
   "abstract": [
    "We provide a user-friendly automatic phonetic alignment tool for continuous speech, named EasyAlign. It is developed as a plug-in of Praat, the popular speech analysis software, and it is freely available. Its main advantage is that one can easily align speech from an orthographic transcription. It requires a few minor manual steps and the result is a multi-level annotation within a TextGrid composed of phonetic, syllabic, lexical and utterance tiers. Evaluation showed that the performances of this HTK-based aligner compare to human alignment and to other existing alignment tools. It was originally fully available for French, English. Community's interests for its extension to other languages helped to develop a straightforward methodology to add languages. While Spanish and Taiwan Min were recently added, other languages are under development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-815"
  },
  "villegas11_interspeech": {
   "authors": [
    [
     "Julián",
     "Villegas"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Marco A.",
     "Piccolino-Boniforti"
    ]
   ],
   "title": "Mtrans: a multi-channel, multi-tier speech annotation tool",
   "original": "i11_3237",
   "page_count": 4,
   "order": 818,
   "p1": "3237",
   "pn": "3240",
   "abstract": [
    "Mtrans, a freely available tool for annotating multi-channel speech is presented. This software tool is designed to provide visual and aural display flexibility required for transcribing multi-party conversations; in particular, it eases the analysis of speech overlaps by overlaying waveforms and spectrograms (with controllable transparency), and the mapping from media channels to annotation tiers by allowing arbitrary associations between them. Mtrans supports interoperability with other tools via the Open Sound Control protocol.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-816"
  },
  "cerisara11b_interspeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Claire",
     "Gardent"
    ]
   ],
   "title": "The JSafran platform for semi-automatic speech processing",
   "original": "i11_3241",
   "page_count": 4,
   "order": 819,
   "p1": "3241",
   "pn": "3244",
   "abstract": [
    "JSafran is an open-source Java platform for editing, annotating and transforming speech corpora both manually and automatically at many levels: transcription, alignment, morphosyntactic tagging, syntactic parsing and semantic roles labelling. It integrates preconfigured state-of-the-art libraries for this purpose, including the Sphinx4, TreeTagger, OpenNLP, MaltParser and MATE applications, as well as the companion JTrans software for text-to-speech alignment and transcription. Despite the complexity of such speech processing tasks, JSafran has been designed to maximize simplicity both for the end-user, thanks to an easy-to-use GUI that controls all of these automatic and manual annotation functionalities, and for the developer, thanks to well-defined interfaces and to the multilevel stand-off annotation paradigm. JSafran has been used so far for several tasks, including the creation of a new French treebank on top of the broadcast news ESTER corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-817"
  },
  "wagner11_interspeech": {
   "authors": [
    [
     "Johannes",
     "Wagner"
    ],
    [
     "Florian",
     "Lingenfelser"
    ],
    [
     "Elisabeth",
     "André"
    ]
   ],
   "title": "The social signal interpretation framework (SSI) for real time signal processing and recognition",
   "original": "i11_3245",
   "page_count": 4,
   "order": 820,
   "p1": "3245",
   "pn": "3248",
   "abstract": [
    "The construction of systems for recording, processing and recognising a human's social and affective signals is a challenging effort that includes numerous but necessary sub-tasks to be dealt with. In this article, we introduce our Social Signal Interpretation (SSI) tool, a framework dedicated to support the development of such systems. It provides a flexible architecture to construct pipelines to handle multiple modalities like audio or video and establishing on- and offline recognition tasks. The plug-in system of SSI encourages developers to integrate external code, while a XML interface allows anyone to write own applications with a simple text editor. Furthermore, data recording, annotation and classification can be done using a straightforward graphical user interface, allowing simple access to inexperienced users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-818"
  },
  "sloetjes11_interspeech": {
   "authors": [
    [
     "Han",
     "Sloetjes"
    ],
    [
     "Peter",
     "Wittenburg"
    ],
    [
     "Aarthy",
     "Somasundaram"
    ]
   ],
   "title": "ELAN - aspects of interoperability and functionality",
   "original": "i11_3249",
   "page_count": 4,
   "order": 821,
   "p1": "3249",
   "pn": "3252",
   "abstract": [
    "ELAN is a multimedia annotation tool that has been developed for roughly ten years now and is still being extended and improved in, on average, two or three major updates per year. This paper describes the current state of the application, the main areas of attention of the past few years and the plans for the near future. The emphasis will be on various interoperability issues: interoperability with other tools through file conversions, process based interoperability with other tools by means of commands send to or received from other applications, interoperability on the level of the data model and semantic interoperability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-819"
  },
  "schroder11_interspeech": {
   "authors": [
    [
     "Marc",
     "Schröder"
    ],
    [
     "Marcela",
     "Charfuelan"
    ],
    [
     "Sathish",
     "Pammi"
    ],
    [
     "Ingmar",
     "Steiner"
    ]
   ],
   "title": "Open source voice creation toolkit for the MARY TTS platform",
   "original": "i11_3253",
   "page_count": 4,
   "order": 822,
   "p1": "3253",
   "pn": "3256",
   "abstract": [
    "This paper describes an open source voice creation toolkit that supports the creation of unit selection and HMM-based voices, for the MARY (Modular Architecture for Research on speech Synthesis) TTS platform. The toolkit can be easily employed to create voices in the languages already supported by MARY TTS, but also provides the tools and generic reusable run-time system modules to add new languages. The voice creation toolkit is mainly intended to be used by research groups on speech technology throughout the world, notably those who do not have their own pre-existing technology yet. We try to provide them with a reusable technology that lowers the entrance barrier for them, making it easier to get started. The toolkit is developed in Java and includes an intuitive Graphical User Interface (GUI) for most of the common tasks in the creation of a synthetic voice. We present the toolkit and discuss a number of interoperability issues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-820"
  },
  "steidl11_interspeech": {
   "authors": [
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Florian",
     "Hönig"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Java visual speech components for rapid application development of GUI based speech processing applications",
   "original": "i11_3257",
   "page_count": 4,
   "order": 823,
   "p1": "3257",
   "pn": "3260",
   "abstract": [
    "In this paper, we describe a new Java framework for an easy and efficient way of developing new GUI based speech processing applications. Standard components are provided to display the speech signal, the power plot, and the spectrogram. Furthermore, a component to create a new transcription and to display and manipulate an existing transcription is provided, as well as a component to display and manually correct external pitch values. These Swing components can be easily embedded into own Java programs. They can be synchronized to display the same region of the speech file. The object-oriented design provides base classes for rapid development of own components.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-821"
  },
  "johnston11_interspeech": {
   "authors": [
    [
     "Michael",
     "Johnston"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Simon",
     "Urbanek"
    ]
   ],
   "title": "mtalk - a multimodal browser for mobile services",
   "original": "i11_3261",
   "page_count": 4,
   "order": 824,
   "p1": "3261",
   "pn": "3264",
   "abstract": [
    "The mTalk multimodal browser is a tool which enables rapid prototyping for research and development of mobile multimodal interfaces combining natural modalities such as speech, touch, and gesture. mTalk integrates a broad range of open standards for authoring graphical and spoken user interfaces and is supported by a cloud-based multimodal processing architecture. In this paper, we describe mTalk and illustrate its capabilities through examination of a series of sample applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-822"
  },
  "wrigley11_interspeech": {
   "authors": [
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Web-based automatic speech recognition service - webASR",
   "original": "i11_3265",
   "page_count": 4,
   "order": 825,
   "p1": "3265",
   "pn": "3268",
   "abstract": [
    "A state-of-the-art automatic speech recognition (ASR) system was developed as part of the AMIDA project whose core domain was the transcription of small to medium sized meetings. The system has performed well in recent NIST evaluations (RT'07 and RT'09). This research-grade ASR system has now been made available as a free web service (webASR) targeting non-commercial researchers. Access to the service is via and standard browser-based interface as well as an API. The service provides the facility to upload audio recordings which are then processed by the ASR system to produce a word-level transcript. Such transcripts are available in a range of formats to suite different needs and technical expertise. The API allows the core webASR functionality to be integrated seamlessly into applications and services. Detailed descriptions of the system design and user interface are provided.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-823"
  },
  "klehr11_interspeech": {
   "authors": [
    [
     "Markus",
     "Klehr"
    ],
    [
     "Andreas",
     "Ratzka"
    ],
    [
     "Thomas",
     "Roß"
    ]
   ],
   "title": "A web based speech transcription workplace",
   "original": "i11_3269",
   "page_count": 4,
   "order": 826,
   "p1": "3269",
   "pn": "3272",
   "abstract": [
    "We describe our web based speech transcription tool EML Transcription Workplace (TWP). Apart from its main purpose of annotating audio data, it also includes support for the management of transcription data, ASR based pre-transcription, assignment of work packages to specific users, user management and a correction/ verification workflow. These features help to increase the productivity for both transcriptionists and supervisors and facilitate further processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-824"
  },
  "martin11_interspeech": {
   "authors": [
    [
     "Philippe",
     "Martin"
    ]
   ],
   "title": "Winpitch: a multimodal tool for speech analysis of endangered languages",
   "original": "i11_3273",
   "page_count": 4,
   "order": 827,
   "p1": "3273",
   "pn": "3276",
   "abstract": [
    "WinPitch is a speech analysis program running on PC and Mac personal computers for acoustical analysis of speech corpora. It includes a large number of specialized functions to transcribe, align and analyze large sound and video recordings. It supports multiple hierarchical layers for segmentation (up to 96 layers), speaker lists, and overlapping speech. Various character encodings, including Unicode, are supported, with optional right to left text display for Arabic and Hebrew transcriptions. Interfaces with other popular speech analysis programs are provided, as well as standard alignment input and output in XML format. Many functions are devoted to the transcription, alignment and description of less documented languages, such as slow speed playback, programmable keyboard, automatic lexicon generation and text labeling. Various software functions are described together with their applications to the analysis of Parkateje, a Timbira language spoken in the Amazonia by about 400 speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-825"
  },
  "huckvale11_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Recording caregiver interactions for machine acquisition of spoken language using the KLAIR virtual infant",
   "original": "i11_3277",
   "page_count": 4,
   "order": 828,
   "p1": "3277",
   "pn": "3280",
   "abstract": [
    "The goals of the KLAIR project are to facilitate research into the computational modelling of spoken language acquisition. Previously we have described the KLAIR toolkit that implements a virtual infant that can see, hear and talk. In this paper we describe how the toolkit has been enhanced and extended to make it easier to build interactive applications that promote dialogues with human subjects, and also to record and document them. Primary developments are the introduction of 3D models, integration of speech recognition, real-time video recording, support for .NET languages, and additional tools for supporting interactive experiments. An example experimental configuration is described in which KLAIR appears to learn how to say the names of toys in order to encourage dialogue with caregivers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2011-826"
  },
  "burkhardt11b_interspeech": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ]
   ],
   "title": "An affective spoken storyteller",
   "original": "i11_3305",
   "page_count": 2,
   "order": 829,
   "p1": "3305",
   "pn": "3306",
   "abstract": [
    "We present a software to read texts with emotional expression. The software is developed as part of the Emofilt open source emotional speech synthesis software. The affective storyteller consists of a text editor which offers a set of emotional speaking styles that can be used to mark up the text. The system was validated in a perception experiment and, although the number of participants wasn't very large, could show the general usability of the approach.\n",
    ""
   ]
  },
  "wang11k_interspeech": {
   "authors": [
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Wei",
     "Han"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "Text driven 3d photo-realistic talking head",
   "original": "i11_3307",
   "page_count": 2,
   "order": 830,
   "p1": "3307",
   "pn": "3308",
   "abstract": [
    "We propose a new 3D photo-realistic talking head with a personalized, photo realistic appearance. Different head motions and facial expressions can be freely controlled and rendered. It extends our prior, high-quality, 2D photo-realistic talking head to 3D. Around 20-minutes of audio-visual 2D video are first recorded with read prompted sentences spoken by a speaker. We use a 2D-to-3D reconstruction algorithm to automatically adapt a general 3D head mesh model to the individual. In training, super feature vectors consisting of 3D geometry, texture and speech are formed to train a statistical, multi-streamed, Hidden Markov Model (HMM). The HMM is then used to synthesize both the trajectories of geometry animation and dynamic texture. The 3D talking head animation can be controlled by the rendered geometric trajectory while the facial expressions and articulator movements are rendered with the dynamic 2D image sequences. Head motions and facial expression can also be separately controlled by manipulating corresponding parameters. The new 3D talking head has many useful applications such as voice-agent, tele-presence, gaming, social networking, etc.\n",
    ""
   ]
  },
  "arai11_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Physical models producing vowels with pitch variation",
   "original": "i11_3309",
   "page_count": 2,
   "order": 831,
   "p1": "3309",
   "pn": "3310",
   "abstract": [
    "Physical models of the human vocal tract are useful for education in acoustics and speech science. To excite such vocal-tract models, different types of sound sources may be used. We have developed two new types of physical models which produce a glottal source with a variable fundamental frequency. Both types are based on a reed vibration, and the length of the vibratory portion can be varied manually. In the first type, the reed itself is curved, while the reed of the second type is straight but its support is curved. In each case, we can demonstrate vowel production with pitch variation by combining vocal-tract models with our proposed source models.\n",
    ""
   ]
  },
  "mieskes11_interspeech": {
   "authors": [
    [
     "Margot",
     "Mieskes"
    ]
   ],
   "title": "An engine-independent text-to-speech workplace",
   "original": "i11_3311",
   "page_count": 2,
   "order": 832,
   "p1": "3311",
   "pn": "3312",
   "abstract": [
    "workplace, a web-based graphical user interface, is intended to be engine-independent, allowing the user to not worry about the interaction with the specific engine, but to focus on his/her task and create a good synthesis result. Additionally, the workplace offers support for non-expert users in specific tuning and interaction tasks, such as phonetic transcriptions or creating a lexicon for usage during synthesis. We also present two application scenarios which were the basis for creating this workplace and the current status of the workplace.\n",
    ""
   ]
  },
  "carcone11_interspeech": {
   "authors": [
    [
     "Simone",
     "Carcone"
    ],
    [
     "Carlo",
     "Giovannella"
    ]
   ],
   "title": "An application to test the emotion conveyed by vocal and musical signals",
   "original": "i11_3313",
   "page_count": 2,
   "order": 833,
   "p1": "3313",
   "pn": "3314",
   "abstract": [
    "We present an application that allows straightforwardly to built up, administer and analyze tests designed to measure the emotion conveyed by multimodal and single modal signals, among them voice, music and sounds. The application is available either as stand-alone application and, partially, as web-service.\n",
    ""
   ]
  },
  "zioko11_interspeech": {
   "authors": [
    [
     "Mariusz",
     "Ziółko"
    ],
    [
     "Jakub",
     "Gałka"
    ],
    [
     "Bartosz",
     "Ziółko"
    ],
    [
     "Tomasz",
     "Jadczyk"
    ],
    [
     "Dawid",
     "Skurzok"
    ],
    [
     "Mariusz",
     "Masior"
    ]
   ],
   "title": "Automatic speech recognition system dedicated for Polish",
   "original": "i11_3315",
   "page_count": 2,
   "order": 834,
   "p1": "3315",
   "pn": "3316",
   "abstract": [
    "An automatic speech recognition system for Polish is demonstrated. A few layers of our system are different from popular approaches as a result of differences between Polish and English languages.\n",
    ""
   ]
  },
  "lee11k_interspeech": {
   "authors": [
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Helen",
     "Thai"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Joint application of speech and speaker recognition for automation and security in smart home",
   "original": "i11_3317",
   "page_count": 2,
   "order": 835,
   "p1": "3317",
   "pn": "3318",
   "abstract": [
    "This paper describes the deployment of speech technologies in STARHome, a fully functional smart home prototype. We make use of speech and speaker recognition technologies to provide three voice services, namely, voice command for controlling home appliances, voice biometric for entrance-door access control, and service customization (speaker-loaded command control). Voice applications for STARHome have been designed to deal with short utterances and low SNR.\n",
    ""
   ]
  },
  "larsson11_interspeech": {
   "authors": [
    [
     "Staffan",
     "Larsson"
    ],
    [
     "Alexander",
     "Berman"
    ],
    [
     "Jessica",
     "Villing"
    ]
   ],
   "title": "Adding a speech cursor to a multimodal dialogue system",
   "original": "i11_3319",
   "page_count": 2,
   "order": 836,
   "p1": "3319",
   "pn": "3320",
   "abstract": [
    "This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogue and a \"speech cursor\" which enables menu navigation as well as browsing long list using haptic input and spoken output.\n",
    ""
   ]
  },
  "christie11_interspeech": {
   "authors": [
    [
     "S. Thomas",
     "Christie"
    ],
    [
     "Serguei",
     "Pakhomov"
    ]
   ],
   "title": "Prosody toolkit: integrating HTK, praat and WEKA",
   "original": "i11_3321",
   "page_count": 2,
   "order": 837,
   "p1": "3321",
   "pn": "3322",
   "abstract": [
    "A major hurdle in computational speech analysis is the effective integration of available tools originally developed for purposes unrelated to each other. We present a Python-based tool to enable an efficient and organized processing workflow incorporating automatic speech recognition using HTK, phoneme-level prosodic feature extraction in Praat and machine learning in WEKA. Our system is extensible, customizable and organizes prosodic data by phoneme and time stamp in a tabular fashion in preparation for analysis using other utilities. Plotting of prosodic information is supported to enable visualization of prosodic features.\n",
    ""
   ]
  },
  "francesconi11_interspeech": {
   "authors": [
    [
     "F.",
     "Francesconi"
    ],
    [
     "A.",
     "Ghosh"
    ],
    [
     "G.",
     "Riccardi"
    ],
    [
     "M.",
     "Ronchetti"
    ],
    [
     "A.",
     "Vagin"
    ]
   ],
   "title": "Collecting life logs for experience-based corpora",
   "original": "i11_3323",
   "page_count": 2,
   "order": 838,
   "p1": "3323",
   "pn": "3324",
   "abstract": [
    "In this paper we propose an approach to lightweight acquisition, sharing and annotation of experience-based corpora via mobile devices. Corpora acquisition is the crucial and often costly process in speech and language science and engineering. To address this problem, we have built a system for creating a location based corpora annotated with multimedia tags (e.g. text, speech, image) generated by end-users. We describe a relevant case study for the collection of mobile user life logs. We plan to make publicly available such tools and platforms to the research community for collaborative development and distributed experiential corpora collection.\n",
    ""
   ]
  },
  "wrigley11b_interspeech": {
   "authors": [
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Making an automatic speech recognition service freely available on the web",
   "original": "i11_3325",
   "page_count": 2,
   "order": 839,
   "p1": "3325",
   "pn": "3326",
   "abstract": [
    "The state-of-the-art speech recognition system developed by the AMIDA project and which performed well in the NIST RT'09 evaluation has been made available as a web service. The service provides free access to ASR aimed specifically at the scientific community. There are two ways in which this service can be accessed: via a standard web-browser and programmatically via an API.\n",
    ""
   ]
  },
  "kim11i_interspeech": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Thomas",
     "Okken"
    ],
    [
     "Alistair D.",
     "Conkie"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ]
   ],
   "title": "AT&t voicebuilder: a cloud-based text-to-speech voice builder tool",
   "original": "i11_3327",
   "page_count": 2,
   "order": 840,
   "p1": "3327",
   "pn": "3328",
   "abstract": [
    "The AT&T VoiceBuilder provides a new tool to researchers and practitioners who want to have their voices synthesized by a high-quality, commercial-grade text-to-speech (TTS) system without the need to install, configure, or manage speech processing software and equipment. It is implemented as a web service on the AT&T Speech Mashup Portal1. The system records, processes, and validates users' utterances, and provides a web service API to make the new voice immediately available to real-time applications. All the procedures are fully-automated to avoid human intervention.\n",
    ""
   ]
  },
  "tucker11_interspeech": {
   "authors": [
    [
     "Roger",
     "Tucker"
    ],
    [
     "Dan",
     "Fry"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Extending audio notetaker to browse webASR transcriptions",
   "original": "i11_3329",
   "page_count": 2,
   "order": 841,
   "p1": "3329",
   "pn": "3330",
   "abstract": [
    "The audio annotation tool Audio Notetaker has been extended to allow browsing of transcripts produced with the WebASR system from Sheffield University. The interface has been designed to be usable with as much as 50% recognition error.\n",
    ""
   ]
  },
  "ainsley11_interspeech": {
   "authors": [
    [
     "Samantha",
     "Ainsley"
    ],
    [
     "Linne",
     "Ha"
    ],
    [
     "Martin",
     "Jansche"
    ],
    [
     "Ara",
     "Kim"
    ],
    [
     "Masayuki",
     "Nanzawa"
    ]
   ],
   "title": "A web-based tool for developing multilingual pronunciation lexicons",
   "original": "i11_3331",
   "page_count": 2,
   "order": 842,
   "p1": "3331",
   "pn": "3332",
   "abstract": [
    "We present a web-based tool for generating and editing pronunciation lexicons in multiple languages. The tool is implemented as a web application on Google App Engine and can be accessed remotely from a web browser. The client application displays to users a textual prompt and interface that reconfigures based on language and task. It lets users generate pronunciations via constrained phoneme selection, which allows users with no special training to provide phonemic transcriptions efficiently and accurately.\n",
    ""
   ]
  },
  "johnston11b_interspeech": {
   "authors": [
    [
     "Michael",
     "Johnston"
    ],
    [
     "Patrick",
     "Ehlen"
    ]
   ],
   "title": "Speak4it and the multimodal semantic interpretation system",
   "original": "i11_3333",
   "page_count": 2,
   "order": 843,
   "p1": "3333",
   "pn": "3334",
   "abstract": [
    "Multimodal interaction allows users to specify commands using combinations of inputs from multiple different modalities. For example, in a local search application, a user might say \"gas stations\" while simultaneously tracing a route on a touchscreen display. In this demonstration, we describe the extension of our cloud-based speech recognition architecture to a Multimodal Semantic Interpretation System (MSIS) that supports processing of multimodal inputs streamed over HTTP. We illustrate the capabilities of the framework using Speak4it, a deployed mobile local search application supporting combined speech and gesture input. We provide interactive demonstrations of Speak4it on the iPhone and iPad and explain the challenges of supporting true multimodal interaction in a deployed mobile service.\n",
    ""
   ]
  },
  "alumae11_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Ahti",
     "Kitsik"
    ]
   ],
   "title": "TSAB - web interface for transcribed speech collections",
   "original": "i11_3335",
   "page_count": 2,
   "order": 844,
   "p1": "3335",
   "pn": "3336",
   "abstract": [
    "This paper describes a new web interface for accessing large transcribed spoken data collections. The system uses automatic or manual time-aligned transcriptions with speaker and topic segmentation information to present structured speech data more efficiently and make accessing relevant speech data quicker. The system is independent of the underlying speech processing technology. The software is free and open-source.\n",
    ""
   ]
  },
  "ljolje11_interspeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Vincent",
     "Goffin"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Taniya",
     "Mishra"
    ],
    [
     "Mazin",
     "Gilbert"
    ]
   ],
   "title": "Visual voice mail to text on the iphone/ipad",
   "original": "i11_3337",
   "page_count": 2,
   "order": 845,
   "p1": "3337",
   "pn": "3338",
   "abstract": [
    "Our visual Voice-Mail-to-Text (VMTT) transcription system takes a conventional voice mail and converts it to formatted text following standard punctuation, capitalization and presentation conven- tions. The text can then be used in a plethora of applications, from emails, to databases, text messages etc., which in turn allow searching, classification, data extraction, statistical analyses and other processes. Here we demonstrate our fully automated VMTT application by displaying the best scoring hypotheses from various recognition passes, the addition of punctuation and capitalization, formatting by using appropriate conventions for times, dates, dollar amounts and abbreviations, and finally applying grayscaling to lower the impact of the words recognized with low confidence scores.\n",
    ""
   ]
  },
  "draxler11b_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "Percy - an HTML5 framework for media rich web experiments on mobile devices",
   "original": "i11_3339",
   "page_count": 2,
   "order": 846,
   "p1": "3339",
   "pn": "3340",
   "abstract": [
    "Percy is a small software framework for perception experiments via the WWW. It is implemented entirely in dynamic HTML and makes use of the new multimedia tags available in HTML5, eliminating the need for browser plug-ins or external players to display media content. With Percy, perception experiments can be run on any platform supporting HTML5, including tablet computers, smartphones or game consoles and thus access new participant populations.\n",
    "Percy supports touch interfaces and measures reaction times. It stores its data in a relational database system on a server. This allows immediate access to the experiment data via standard database access APIs.\n",
    "The system has been used for a number of experiments in German, Castilian Spanish and English.\n",
    ""
   ]
  },
  "huckvale11b_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "The KLAIR toolkit for recording interactive dialogues with a virtual infant",
   "original": "i11_3341",
   "page_count": 2,
   "order": 847,
   "p1": "3341",
   "pn": "3342",
   "abstract": [
    "The goals of the KLAIR project are to facilitate research into the computational modelling of spoken language acquisition. Previously we have described the KLAIR toolkit that implements a virtual infant that can see, hear and talk. In this demonstration we show how the toolkit can be used to record interactive dialogues with caregivers. The outcomes are both an audio-video recording and a log of the \"beliefs\" and \"goals\" of the infant control program. These recordings can then be analysed by machine learning systems to model spoken language acquisition. In our demonstration, visitors will be able to interact with KLAIR and try to teach it the names of some toys.\n",
    ""
   ]
  },
  "nesta11_interspeech": {
   "authors": [
    [
     "Francesco",
     "Nesta"
    ],
    [
     "Marco",
     "Matassoni"
    ],
    [
     "HariKrishna",
     "Maganti"
    ]
   ],
   "title": "Real-time prototype for integration of blind source extraction and robust automatic speech recognition",
   "original": "i11_3343",
   "page_count": 2,
   "order": 848,
   "p1": "3343",
   "pn": "3344",
   "abstract": [
    "This demo presents a real-time prototype for automatic blind source extraction and speech recognition in presence of multiple interfering noise sources. Binaural recorded mixtures are processed by a combined Blind/Semi-Blind Source Separation algorithm in order to obtain an estimation of the target signal. The recovered target signal is segmented and used as input to a real-time automatic speech recognition (ASR) system. Further, to improve the recognition performance, noise robust features based on Gammatone filters are used. The demo utilizes the data provided for the CHiME Pascal speech separation and recognition challenge and also real-time mixtures recorded on-site. Users will be able to listen to the recovered target signal and compare it with the original mixture and ASR output.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "hirschberg11_interspeech",
    "mitchell11_interspeech",
    "pentland11_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition - Modeling",
   "papers": [
    "matza11_interspeech",
    "toledoronen11_interspeech",
    "bonastre11_interspeech",
    "aronowitz11_interspeech",
    "mandasari11_interspeech",
    "senoussaoui11_interspeech"
   ]
  },
  {
   "title": "Speech Perception - Speech Intelligibility",
   "papers": [
    "iyer11_interspeech",
    "kliper11_interspeech",
    "rennies11_interspeech",
    "gautreau11_interspeech",
    "shafiro11_interspeech",
    "lee11_interspeech"
   ]
  },
  {
   "title": "Speech Representation and Modelling",
   "papers": [
    "ali11_interspeech",
    "cinneide11_interspeech",
    "ramanarayanan11_interspeech",
    "wang11_interspeech",
    "malyska11_interspeech",
    "raj11_interspeech"
   ]
  },
  {
   "title": "Emotion, Speaking Style, and Social Behavior",
   "papers": [
    "wollmer11_interspeech",
    "erden11_interspeech",
    "chang11_interspeech",
    "black11_interspeech",
    "goudbeek11_interspeech",
    "gravano11_interspeech"
   ]
  },
  {
   "title": "HMM-Based Speech Synthesis I, II",
   "papers": [
    "oh11_interspeech",
    "silen11_interspeech",
    "nose11_interspeech",
    "hashimoto11_interspeech",
    "ling11_interspeech",
    "shannon11_interspeech",
    "picart11_interspeech",
    "chen11_interspeech",
    "wen11_interspeech",
    "erro11_interspeech",
    "anumanchipalli11_interspeech",
    "henter11_interspeech",
    "braunschweiler11_interspeech",
    "liang11_interspeech",
    "obin11_interspeech",
    "maia11_interspeech",
    "valentinibotinhao11_interspeech",
    "nitta11_interspeech",
    "kato11_interspeech",
    "maeno11_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition - Modeling, Automatic Procedures, Analysis I-III",
   "papers": [
    "zhang11_interspeech",
    "aronowitz11b_interspeech",
    "gonzalezrodriguez11_interspeech",
    "glembek11_interspeech",
    "sanchez11_interspeech",
    "mccree11_interspeech",
    "zhang11b_interspeech",
    "garciaromero11_interspeech",
    "jiang11_interspeech",
    "scheffer11_interspeech",
    "greenberg11_interspeech",
    "kockmann11_interspeech",
    "kanagasundaram11_interspeech",
    "sun11_interspeech",
    "ma11_interspeech",
    "yu11_interspeech",
    "sarkar11_interspeech",
    "wang11b_interspeech",
    "cumani11_interspeech",
    "polzehl11_interspeech",
    "ferras11_interspeech",
    "biswas11_interspeech",
    "sivaram11_interspeech"
   ]
  },
  {
   "title": "Speech Perception - Perceptual Learning and Cross-Language Perception",
   "papers": [
    "scharenborg11_interspeech",
    "tuinman11_interspeech",
    "tsuzaki11_interspeech",
    "peperkamp11_interspeech",
    "bissiri11_interspeech",
    "zhang11c_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "pedersen11_interspeech",
    "kane11_interspeech",
    "fan11_interspeech",
    "asaei11_interspeech",
    "mallidi11_interspeech",
    "petkov11_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement and Dereverberation",
   "papers": [
    "kinoshita11_interspeech",
    "erkelens11_interspeech",
    "zheng11_interspeech",
    "prego11_interspeech",
    "hadir11_interspeech",
    "marinhurtado11_interspeech"
   ]
  },
  {
   "title": "ASR - Feature Extraction I, II",
   "papers": [
    "ng11_interspeech",
    "heckmann11_interspeech",
    "fukuda11_interspeech",
    "chopra11_interspeech",
    "yu11b_interspeech",
    "liao11_interspeech",
    "grezl11_interspeech",
    "wollmer11b_interspeech",
    "plahl11_interspeech",
    "pinto11_interspeech",
    "valente11_interspeech",
    "lee11b_interspeech",
    "huang11_interspeech",
    "boril11_interspeech",
    "lee11c_interspeech",
    "keronen11_interspeech",
    "meyer11_interspeech",
    "variani11_interspeech",
    "nemala11_interspeech",
    "marino11_interspeech"
   ]
  },
  {
   "title": "Speech Production - Articulatory Measurements",
   "papers": [
    "kim11_interspeech",
    "winkler11_interspeech",
    "wang11c_interspeech",
    "proctor11_interspeech",
    "birkholz11_interspeech",
    "promon11_interspeech"
   ]
  },
  {
   "title": "Acoustic Event Detection",
   "papers": [
    "geiger11_interspeech",
    "leng11_interspeech",
    "ito11_interspeech",
    "schmalenstroeer11_interspeech",
    "mejianavarrete11_interspeech",
    "natarajan11_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis - Unit Selection and Hybrid approaches",
   "papers": [
    "sridhar11_interspeech",
    "latacz11_interspeech",
    "windmann11_interspeech",
    "pammi11_interspeech",
    "sainz11_interspeech",
    "sorin11_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement Analysis and Evaluation",
   "papers": [
    "miyazaki11_interspeech",
    "tang11_interspeech",
    "muraka11_interspeech",
    "triki11_interspeech",
    "milner11_interspeech",
    "wakisaka11_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition - Analysis and Statistics I-III",
   "papers": [
    "laskowski11_interspeech",
    "patil11_interspeech",
    "long11_interspeech",
    "solewicz11_interspeech",
    "yaman11_interspeech",
    "vaquero11_interspeech",
    "bousquet11_interspeech",
    "drgas11_interspeech",
    "srinivasan11_interspeech",
    "omar11_interspeech",
    "leeuwen11_interspeech",
    "villalba11_interspeech",
    "pekhovsky11_interspeech",
    "mclaren11_interspeech",
    "huang11b_interspeech",
    "rao11_interspeech",
    "takashima11_interspeech",
    "lei11_interspeech",
    "li11_interspeech",
    "hasan11_interspeech",
    "even11_interspeech",
    "ogawa11_interspeech",
    "hautamaki11_interspeech",
    "jafari11_interspeech",
    "lei11b_interspeech",
    "hernaez11_interspeech"
   ]
  },
  {
   "title": "Speech Production - Coarticulation and Speech Timing",
   "papers": [
    "benus11_interspeech",
    "fivela11_interspeech",
    "simko11_interspeech",
    "zmarich11_interspeech",
    "celata11_interspeech",
    "hagedorn11_interspeech"
   ]
  },
  {
   "title": "Speech Segmentation",
   "papers": [
    "wang11d_interspeech",
    "huang11c_interspeech",
    "castan11_interspeech",
    "kalinli11_interspeech",
    "peddinti11_interspeech",
    "pedone11_interspeech"
   ]
  },
  {
   "title": "ASR - Acoustic Models I-III",
   "papers": [
    "seide11_interspeech",
    "wang11e_interspeech",
    "magimaidoss11_interspeech",
    "keshet11_interspeech",
    "sun11b_interspeech",
    "wang11f_interspeech",
    "hsiao11_interspeech",
    "diehl11_interspeech",
    "ko11_interspeech",
    "sainath11_interspeech",
    "zhang11d_interspeech",
    "buthpitiya11_interspeech",
    "alessandrini11_interspeech",
    "strope11_interspeech",
    "kurata11_interspeech",
    "jansen11_interspeech",
    "cui11_interspeech",
    "xu11_interspeech",
    "tahir11_interspeech",
    "zhang11e_interspeech",
    "shin11_interspeech",
    "darjaa11_interspeech",
    "nallasamy11_interspeech",
    "jalalvand11_interspeech",
    "ratnagiri11_interspeech",
    "olaso11_interspeech",
    "vanek11_interspeech"
   ]
  },
  {
   "title": "Robust Speech Recognition I-III",
   "papers": [
    "astudillo11_interspeech",
    "mahkonen11_interspeech",
    "kallasjoki11_interspeech",
    "liao11b_interspeech",
    "badiezadegan11_interspeech",
    "cheng11_interspeech",
    "mowlaee11_interspeech",
    "demir11_interspeech",
    "maganti11_interspeech",
    "souden11_interspeech",
    "xiao11_interspeech",
    "fujimoto11_interspeech",
    "leutnant11_interspeech",
    "pardede11_interspeech",
    "kim11b_interspeech",
    "kim11c_interspeech",
    "ma11b_interspeech",
    "joshi11_interspeech",
    "remes11_interspeech",
    "sun11c_interspeech",
    "gomez11_interspeech",
    "muller11_interspeech"
   ]
  },
  {
   "title": "Physiology and Pathology of Spoken Language",
   "papers": [
    "patil11b_interspeech",
    "shimura11_interspeech",
    "zhou11_interspeech",
    "alpan11_interspeech",
    "ghio11_interspeech",
    "orozcoarroyave11_interspeech"
   ]
  },
  {
   "title": "ASR - Lexical, Prosodic and Multi-Lingual Models",
   "papers": [
    "reddy11_interspeech",
    "imseng11_interspeech",
    "novotney11_interspeech",
    "seppi11_interspeech",
    "badr11_interspeech",
    "qian11_interspeech"
   ]
  },
  {
   "title": "Source Separation",
   "papers": [
    "benabderrahmane11_interspeech",
    "koldovsky11_interspeech",
    "sarmiento11_interspeech",
    "grais11_interspeech",
    "zhang11f_interspeech",
    "tran11_interspeech"
   ]
  },
  {
   "title": "Multimodal Signal Processing",
   "papers": [
    "kristensson11_interspeech",
    "mclaughlin11_interspeech",
    "youssef11_interspeech",
    "hueber11_interspeech",
    "schmalenstroeer11b_interspeech",
    "wand11_interspeech"
   ]
  },
  {
   "title": "ASR - Language Models I, II",
   "papers": [
    "mikolov11_interspeech",
    "zweig11_interspeech",
    "shinozaki11_interspeech",
    "arsoy11_interspeech",
    "ananthakrishnan11_interspeech",
    "kang11_interspeech",
    "sorensen11_interspeech",
    "allauzen11_interspeech",
    "sundermeyer11_interspeech",
    "lehnen11_interspeech",
    "shaik11_interspeech",
    "mousa11_interspeech",
    "nubaumthom11_interspeech",
    "kobayashi11_interspeech",
    "gillot11_interspeech",
    "dikici11_interspeech",
    "masumura11_interspeech",
    "le11_interspeech",
    "mamou11_interspeech",
    "tam11_interspeech"
   ]
  },
  {
   "title": "Phonology and Phonetics",
   "papers": [
    "sadeghi11_interspeech",
    "muller11b_interspeech",
    "samlowski11_interspeech",
    "iacoponi11_interspeech",
    "xavier11_interspeech",
    "pan11_interspeech"
   ]
  },
  {
   "title": "Voice Conversion",
   "papers": [
    "saito11_interspeech",
    "qiao11_interspeech",
    "li11b_interspeech",
    "eslami11_interspeech",
    "benisty11_interspeech",
    "godoy11_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "li11c_interspeech",
    "hillard11_interspeech",
    "hakkanitur11_interspeech",
    "celikyilmaz11_interspeech",
    "huang11d_interspeech",
    "ju11_interspeech"
   ]
  },
  {
   "title": "Dialect and Accent Identification",
   "papers": [
    "boulademareuil11_interspeech",
    "hanani11_interspeech",
    "tong11_interspeech",
    "akbacak11_interspeech",
    "chen11b_interspeech",
    "biadsy11_interspeech"
   ]
  },
  {
   "title": "First Language Acquisition",
   "papers": [
    "miyazawa11_interspeech",
    "brown11_interspeech",
    "lintfert11_interspeech",
    "versteegh11_interspeech",
    "ananthakrishnan11b_interspeech",
    "bergmann11_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems I, II",
   "papers": [
    "misu11_interspeech",
    "raux11_interspeech",
    "schmitt11_interspeech",
    "meguro11_interspeech",
    "suendermann11_interspeech",
    "kronlid11_interspeech",
    "cuayahuitl11_interspeech",
    "deshmukh11_interspeech",
    "peters11_interspeech",
    "devault11_interspeech",
    "chandramohan11_interspeech",
    "crook11_interspeech"
   ]
  },
  {
   "title": "Spoken Language Resources, Evaluation and Standardization I, II",
   "papers": [
    "carlin11_interspeech",
    "hixon11_interspeech",
    "skowronek11_interspeech",
    "kolar11_interspeech",
    "narayanan11_interspeech",
    "burnham11_interspeech"
   ]
  },
  {
   "title": "Spoken Language Resources, Evaluation and Standardization I",
   "papers": [
    "minematsu11_interspeech",
    "moller11_interspeech",
    "lin11_interspeech",
    "zahorian11_interspeech",
    "black11b_interspeech",
    "barbot11_interspeech",
    "richmond11_interspeech",
    "pirker11_interspeech",
    "butko11_interspeech",
    "dobrisek11_interspeech",
    "niemann11_interspeech",
    "werff11_interspeech",
    "rodriguezfuentes11_interspeech",
    "moore11_interspeech",
    "novak11_interspeech"
   ]
  },
  {
   "title": "Language Identification",
   "papers": [
    "zheng11b_interspeech",
    "martinez11_interspeech",
    "penagarikano11_interspeech",
    "dehak11_interspeech",
    "martinez11b_interspeech"
   ]
  },
  {
   "title": "Second Language Acquisition, Development and Learning I, II",
   "papers": [
    "qian11b_interspeech",
    "sisinni11_interspeech",
    "sadakata11_interspeech",
    "bernstein11_interspeech",
    "ouni11_interspeech",
    "dommelen11_interspeech",
    "ordin11_interspeech",
    "li11d_interspeech",
    "sonu11_interspeech",
    "wu11_interspeech",
    "meister11_interspeech",
    "kibishi11_interspeech",
    "bailly11_interspeech",
    "koniaris11_interspeech",
    "sturm11_interspeech",
    "cucchiarini11_interspeech",
    "nariai11_interspeech",
    "nariai11b_interspeech"
   ]
  },
  {
   "title": "ASR - Search, Keyword Spotting and Confidence Measures I, II",
   "papers": [
    "kurniawati11_interspeech",
    "nolden11_interspeech",
    "duckhorn11_interspeech",
    "siniscalchi11_interspeech",
    "seigel11_interspeech",
    "katsurada11_interspeech",
    "kintzley11_interspeech",
    "zhang11g_interspeech",
    "qin11_interspeech",
    "li11e_interspeech",
    "ma11c_interspeech",
    "ma11d_interspeech",
    "yang11_interspeech",
    "shan11_interspeech",
    "ito11b_interspeech",
    "huang11e_interspeech",
    "mishra11_interspeech",
    "vasilescu11_interspeech",
    "saito11b_interspeech",
    "haznedaroglu11_interspeech",
    "asami11_interspeech"
   ]
  },
  {
   "title": "SLP for Information Extraction and Retrieval I, II",
   "papers": [
    "hazen11_interspeech",
    "dufour11_interspeech",
    "muscariello11_interspeech",
    "kim11d_interspeech",
    "chen11c_interspeech",
    "chen11d_interspeech",
    "claveau11_interspeech",
    "lu11_interspeech",
    "gouvea11_interspeech",
    "peng11_interspeech",
    "suzuki11_interspeech",
    "gupta11_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization I, II",
   "papers": [
    "aronowitz11c_interspeech",
    "boakye11_interspeech",
    "shum11_interspeech",
    "nishida11_interspeech",
    "yella11_interspeech",
    "wang11g_interspeech",
    "zibert11_interspeech",
    "huijbregts11_interspeech",
    "zelenak11_interspeech",
    "parthasarathi11_interspeech",
    "ghaemmaghami11_interspeech",
    "tran11b_interspeech"
   ]
  },
  {
   "title": "Prosody I, II",
   "papers": [
    "turco11_interspeech",
    "dorn11_interspeech",
    "cole11_interspeech",
    "michelas11_interspeech",
    "prieto11_interspeech",
    "cvejic11_interspeech",
    "szaszak11_interspeech",
    "tepperman11_interspeech",
    "rosenberg11_interspeech",
    "gu11_interspeech",
    "cheng11b_interspeech",
    "wollermann11_interspeech"
   ]
  },
  {
   "title": "ASR - New Paradigms",
   "papers": [
    "sun11d_interspeech",
    "zhang11h_interspeech",
    "suzuki11b_interspeech",
    "kanevsky11_interspeech",
    "fujii11_interspeech",
    "cai11_interspeech"
   ]
  },
  {
   "title": "Adaptation for ASR",
   "papers": [
    "watanabe11_interspeech",
    "breslin11_interspeech",
    "tuske11_interspeech",
    "jeong11_interspeech",
    "seltzer11_interspeech",
    "gilbert11_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "laaksonen11_interspeech",
    "pulakka11_interspeech",
    "noureldin11_interspeech",
    "harding11_interspeech",
    "choi11_interspeech",
    "li11f_interspeech",
    "li11g_interspeech",
    "chowdhury11_interspeech",
    "paliwal11_interspeech",
    "saha11_interspeech",
    "raj11b_interspeech",
    "leitner11_interspeech",
    "gomez11b_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue &amp; Spoken Language Understanding Systems",
   "papers": [
    "damnati11_interspeech",
    "gandhe11_interspeech",
    "tur11_interspeech",
    "lee11d_interspeech",
    "daubigney11_interspeech",
    "hara11_interspeech",
    "sarikaya11_interspeech",
    "rachevsky11_interspeech",
    "fandrianto11_interspeech",
    "pinault11_interspeech",
    "taguchi11_interspeech",
    "gamback11_interspeech",
    "bazillon11_interspeech",
    "huang11f_interspeech",
    "witt11_interspeech",
    "liang11b_interspeech",
    "kuhnel11_interspeech"
   ]
  },
  {
   "title": "Prosodic Structure",
   "papers": [
    "tepperman11b_interspeech",
    "bocci11_interspeech",
    "gac11_interspeech",
    "thompson11_interspeech",
    "dubeda11_interspeech",
    "kalaldeh11_interspeech",
    "rosenberg11b_interspeech",
    "tseng11_interspeech",
    "sulpizio11_interspeech",
    "bu11_interspeech",
    "looze11_interspeech",
    "yang11b_interspeech"
   ]
  },
  {
   "title": "Language Processing",
   "papers": [
    "guinaudeau11_interspeech",
    "imamura11_interspeech",
    "fang11_interspeech",
    "cerisara11_interspeech",
    "falavigna11_interspeech",
    "chiang11_interspeech"
   ]
  },
  {
   "title": "Paralinguistic Information - Classification and Detection",
   "papers": [
    "oertel11_interspeech",
    "nomoto11_interspeech",
    "ivanov11_interspeech",
    "schuller11_interspeech",
    "burkhardt11_interspeech",
    "gibson11_interspeech",
    "neiberg11_interspeech",
    "amarakeerthi11_interspeech",
    "kitahara11_interspeech",
    "vlasenko11_interspeech",
    "neiberg11b_interspeech"
   ]
  },
  {
   "title": "Applications for Learning, Education, Aged and Handicapped Persons",
   "papers": [
    "shiralishahreza11_interspeech",
    "cheng11c_interspeech",
    "luo11_interspeech",
    "cheng11d_interspeech",
    "stuker11_interspeech",
    "lopezludena11_interspeech",
    "abari11_interspeech",
    "bordel11_interspeech",
    "iribe11_interspeech",
    "chen11e_interspeech",
    "iseijaakkola11_interspeech",
    "pellegrini11_interspeech",
    "liu11_interspeech",
    "huynh11_interspeech"
   ]
  },
  {
   "title": "Source Separation and Speech Enhancement",
   "papers": [
    "zhang11i_interspeech",
    "hirasawa11_interspeech",
    "rabiee11_interspeech",
    "jafari11b_interspeech",
    "vu11_interspeech",
    "tanaka11_interspeech",
    "nakashika11_interspeech",
    "choi11b_interspeech",
    "grais11b_interspeech",
    "marinhurtado11b_interspeech",
    "nakano11_interspeech",
    "nakatani11_interspeech",
    "drioli11_interspeech",
    "sang11_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology, Stress, Accent, Rhythm",
   "papers": [
    "bertini11_interspeech",
    "mairano11_interspeech",
    "chen11f_interspeech",
    "vaughan11_interspeech",
    "niebuhr11_interspeech",
    "heinrich11_interspeech",
    "arnold11_interspeech",
    "tokuma11_interspeech",
    "seng11_interspeech",
    "orr11_interspeech",
    "kim11e_interspeech",
    "stoakes11_interspeech",
    "graczi11_interspeech"
   ]
  },
  {
   "title": "Pitch Processing - Singing Voice Analysis",
   "papers": [
    "pawi11_interspeech",
    "wohlmayr11_interspeech",
    "drugman11_interspeech",
    "govind11_interspeech",
    "pavlovets11_interspeech",
    "prakash11_interspeech",
    "cabral11_interspeech",
    "origlia11_interspeech",
    "sousa11_interspeech",
    "lee11e_interspeech",
    "beux11_interspeech"
   ]
  },
  {
   "title": "Prosodic Modeling",
   "papers": [
    "li11h_interspeech",
    "li11i_interspeech",
    "ni11_interspeech",
    "rilliard11_interspeech",
    "barbosa11_interspeech",
    "obin11b_interspeech",
    "avanzi11_interspeech",
    "mahrt11_interspeech",
    "hussein11_interspeech",
    "neiberg11c_interspeech"
   ]
  },
  {
   "title": "Discourse and Dialogue",
   "papers": [
    "maza11_interspeech",
    "kazemzadeh11_interspeech",
    "buschmeier11_interspeech",
    "ishimoto11_interspeech",
    "hjalmarsson11_interspeech",
    "laskowski11b_interspeech",
    "georgila11_interspeech",
    "heinroth11_interspeech",
    "higashinaka11_interspeech"
   ]
  },
  {
   "title": "SLP for Speech Translation, Information Extraction and Retrieval",
   "papers": [
    "parada11_interspeech",
    "saers11_interspeech",
    "tillmann11_interspeech",
    "huang11g_interspeech",
    "huang11h_interspeech",
    "he11_interspeech",
    "ettelaie11_interspeech",
    "sridhar11b_interspeech",
    "itoh11_interspeech",
    "barbosa11b_interspeech",
    "noritake11_interspeech",
    "polifroni11_interspeech",
    "masumura11b_interspeech",
    "akiba11_interspeech",
    "chan11_interspeech",
    "gemello11_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis - Selected Topics",
   "papers": [
    "parlikar11_interspeech",
    "watts11_interspeech",
    "watts11b_interspeech",
    "campillo11_interspeech",
    "shen11_interspeech",
    "ewender11_interspeech",
    "mattheyses11_interspeech",
    "hinterleitner11_interspeech",
    "mori11_interspeech",
    "abouzleikha11_interspeech",
    "rosenberg11c_interspeech",
    "norrenbrock11_interspeech"
   ]
  },
  {
   "title": "Human Speech and Sound Perception I, II",
   "papers": [
    "klintfors11_interspeech",
    "duran11_interspeech",
    "strombergsson11_interspeech",
    "kagomiya11_interspeech",
    "herff11_interspeech",
    "yip11_interspeech",
    "schneider11_interspeech",
    "honbolygo11_interspeech",
    "witteman11_interspeech",
    "amano11_interspeech",
    "ijima11_interspeech",
    "jesse11_interspeech",
    "cushing11_interspeech",
    "brown11b_interspeech",
    "coelho11_interspeech",
    "carre11_interspeech",
    "kim11f_interspeech",
    "paris11_interspeech",
    "takashima11b_interspeech",
    "mai11_interspeech",
    "wester11_interspeech"
   ]
  },
  {
   "title": "Multilingual and Multimodal Approaches to Spoken Language",
   "papers": [
    "navarathna11_interspeech",
    "alabau11_interspeech",
    "toutios11_interspeech",
    "schaffer11_interspeech",
    "ajmera11_interspeech",
    "girardi11_interspeech",
    "chaudhuri11_interspeech",
    "glodek11_interspeech",
    "lecouteux11_interspeech",
    "chen11g_interspeech"
   ]
  },
  {
   "title": "ASR - New Paradigms and Other Topics",
   "papers": [
    "yu11c_interspeech",
    "deng11_interspeech",
    "wang11h_interspeech",
    "li11j_interspeech",
    "gouvea11b_interspeech",
    "nss11_interspeech",
    "demange11_interspeech",
    "li11k_interspeech",
    "illina11_interspeech",
    "yeh11_interspeech",
    "schraagen11_interspeech",
    "falavigna11b_interspeech",
    "mesgarani11_interspeech",
    "siu11_interspeech",
    "labiak11_interspeech"
   ]
  },
  {
   "title": "Speech Audio Analysis and Classification",
   "papers": [
    "fagerlund11_interspeech",
    "kirchhoff11_interspeech",
    "marujo11_interspeech",
    "canterla11_interspeech",
    "shi11_interspeech",
    "pathak11_interspeech",
    "szekely11_interspeech",
    "ludusan11_interspeech",
    "sam11_interspeech",
    "zhang11j_interspeech",
    "fan11b_interspeech",
    "demarco11_interspeech",
    "yang11c_interspeech",
    "dennis11_interspeech",
    "xiao11b_interspeech"
   ]
  },
  {
   "title": "Speech Audio Analysis",
   "papers": [
    "shi11b_interspeech",
    "laine11_interspeech",
    "lee11f_interspeech",
    "lejan11_interspeech",
    "loweimi11_interspeech",
    "skogstad11_interspeech",
    "william11_interspeech",
    "molina11_interspeech",
    "okamoto11_interspeech",
    "fuchs11_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "fukui11_interspeech",
    "taufique11_interspeech",
    "zhou11b_interspeech",
    "lee11g_interspeech",
    "ramo11_interspeech",
    "pedersen11b_interspeech",
    "gong11_interspeech",
    "khan11_interspeech",
    "wang11i_interspeech",
    "chiang11b_interspeech"
   ]
  },
  {
   "title": "Robustness and Adaptation for ASR",
   "papers": [
    "cerva11_interspeech",
    "fischer11_interspeech",
    "giuliani11_interspeech",
    "song11_interspeech",
    "sanand11_interspeech",
    "tsao11_interspeech",
    "ghai11_interspeech",
    "jouvet11_interspeech",
    "wu11b_interspeech",
    "joshi11b_interspeech",
    "winkler11b_interspeech"
   ]
  },
  {
   "title": "Voice Activity Detection",
   "papers": [
    "unoki11_interspeech",
    "espi11_interspeech",
    "mehta11_interspeech",
    "pohjalainen11_interspeech",
    "fukuda11b_interspeech",
    "kim11g_interspeech",
    "petsatodis11_interspeech",
    "gao11_interspeech",
    "reich11_interspeech",
    "chuangsuwanich11_interspeech",
    "dekens11_interspeech",
    "lu11b_interspeech"
   ]
  },
  {
   "title": "Human Speech Production I",
   "papers": [
    "koriyama11_interspeech",
    "toutios11b_interspeech",
    "bosch11_interspeech",
    "andreeva11_interspeech",
    "chen11h_interspeech",
    "bush11_interspeech",
    "birkholz11b_interspeech",
    "ghosh11_interspeech",
    "kaburagi11_interspeech",
    "rasilo11_interspeech",
    "braun11_interspeech",
    "folk11_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Speech Synthesis",
   "papers": [
    "pilkington11_interspeech",
    "veaux11_interspeech",
    "hattori11_interspeech",
    "perez11_interspeech",
    "lei11c_interspeech",
    "raitio11_interspeech",
    "obin11c_interspeech",
    "sung11_interspeech",
    "hirose11_interspeech",
    "wen11b_interspeech",
    "karhila11_interspeech",
    "toth11_interspeech"
   ]
  },
  {
   "title": "Human Speech Production II",
   "papers": [
    "berry11_interspeech",
    "lammert11_interspeech",
    "lulich11_interspeech",
    "wokurek11_interspeech",
    "audibert11_interspeech",
    "fitzpatrick11_interspeech",
    "aubanel11_interspeech",
    "heldner11_interspeech",
    "katsamanis11_interspeech",
    "li11l_interspeech",
    "hanique11_interspeech",
    "raeesy11_interspeech"
   ]
  },
  {
   "title": "Systems for LVCSR and Rich Transcription",
   "papers": [
    "liu11b_interspeech",
    "xue11_interspeech",
    "sung11b_interspeech",
    "alshareef11_interspeech",
    "brugnara11_interspeech",
    "kombrink11_interspeech",
    "cossalter11_interspeech",
    "abida11_interspeech",
    "akita11_interspeech"
   ]
  },
  {
   "title": "Language, Dialect Identification and Speaker Diarization",
   "papers": [
    "you11_interspeech",
    "habib11_interspeech",
    "varona11_interspeech",
    "tawara11_interspeech",
    "silovsky11_interspeech",
    "soufifar11_interspeech",
    "alberti11_interspeech",
    "fox11_interspeech",
    "yang11d_interspeech",
    "siivola11_interspeech",
    "lee11h_interspeech"
   ]
  },
  {
   "title": "Paralinguistic Information - Analysis and Tools",
   "papers": [
    "gottsmann11_interspeech",
    "harwardt11_interspeech",
    "godin11_interspeech",
    "pakhomov11_interspeech",
    "charfuelan11_interspeech",
    "patel11_interspeech",
    "kim11h_interspeech",
    "ishi11_interspeech",
    "govind11b_interspeech",
    "truong11_interspeech",
    "malandrakis11_interspeech"
   ]
  },
  {
   "title": "Speech and Language Processing-Based Assistive Technologies and Health Applications (Special Session)",
   "papers": [
    "sturim11_interspeech",
    "bunnell11_interspeech",
    "mower11_interspeech",
    "athanaselis11_interspeech",
    "cummins11_interspeech",
    "sanchez11b_interspeech",
    "middag11_interspeech",
    "hofe11_interspeech",
    "heeman11_interspeech",
    "hummel11_interspeech",
    "prudhommeaux11_interspeech",
    "kunikoshi11_interspeech",
    "sasou11_interspeech",
    "blanco11_interspeech"
   ]
  },
  {
   "title": "Crowdsourcing for Speech Processing (Special Session)",
   "papers": [
    "parent11_interspeech",
    "lee11i_interspeech",
    "audhkhasi11_interspeech",
    "cooke11_interspeech",
    "buchholz11_interspeech",
    "mcgraw11_interspeech",
    "jurcicek11_interspeech",
    "gelas11_interspeech",
    "evanini11_interspeech",
    "goto11_interspeech"
   ]
  },
  {
   "title": "Spoken Language Processing of Human-Human Conversations (Special Session)",
   "papers": [
    "valente11b_interspeech",
    "levitan11_interspeech",
    "park11_interspeech",
    "iwata11_interspeech",
    "wang11j_interspeech",
    "neiberg11d_interspeech",
    "lee11j_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Processing for Human-Robot Interaction (Special Session)",
   "papers": [
    "schillingmann11_interspeech",
    "otsuka11_interspeech",
    "wollmer11c_interspeech",
    "maazaoui11_interspeech",
    "tahon11_interspeech",
    "attabi11_interspeech",
    "doukhan11_interspeech",
    "ishi11b_interspeech",
    "heckmann11b_interspeech",
    "sumiyoshi11_interspeech"
   ]
  },
  {
   "title": "Speech Technology for Under-Resourced Languages (Special Session)",
   "papers": [
    "vu11b_interspeech",
    "mandal11_interspeech",
    "davel11_interspeech",
    "secujski11_interspeech",
    "karpov11_interspeech",
    "kempton11_interspeech",
    "chaudhuri11b_interspeech",
    "barroso11_interspeech",
    "vries11_interspeech",
    "mixdorff11_interspeech",
    "wet11_interspeech",
    "kamper11_interspeech",
    "tantibundhit11_interspeech",
    "mustafa11_interspeech"
   ]
  },
  {
   "title": "Speaker State Challenge - Intoxication and Sleepiness I, II (Special Session)",
   "papers": [
    "schuller11b_interspeech",
    "montacie11_interspeech",
    "biadsy11b_interspeech",
    "bocklet11_interspeech",
    "bone11_interspeech",
    "ultes11_interspeech",
    "honig11_interspeech",
    "schiel11_interspeech",
    "rahman11_interspeech",
    "rodriguez11_interspeech",
    "bozkurt11_interspeech",
    "gajsek11_interspeech",
    "huang11i_interspeech"
   ]
  },
  {
   "title": "Speech Processing Tools (Special Session)",
   "papers": [
    "draxler11_interspeech",
    "goldman11_interspeech",
    "villegas11_interspeech",
    "cerisara11b_interspeech",
    "wagner11_interspeech",
    "sloetjes11_interspeech",
    "schroder11_interspeech",
    "steidl11_interspeech",
    "johnston11_interspeech",
    "wrigley11_interspeech",
    "klehr11_interspeech",
    "martin11_interspeech",
    "huckvale11_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Demonstration - Speech Systems and Applications (Special Session)",
   "papers": [
    "burkhardt11b_interspeech",
    "wang11k_interspeech",
    "arai11_interspeech",
    "mieskes11_interspeech",
    "carcone11_interspeech",
    "zioko11_interspeech",
    "lee11k_interspeech",
    "larsson11_interspeech",
    "christie11_interspeech",
    "francesconi11_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Demonstration - Mobility and Web-Services (Special Session)",
   "papers": [
    "wrigley11b_interspeech",
    "kim11i_interspeech",
    "tucker11_interspeech",
    "ainsley11_interspeech",
    "johnston11b_interspeech",
    "alumae11_interspeech",
    "ljolje11_interspeech",
    "draxler11b_interspeech",
    "huckvale11b_interspeech",
    "nesta11_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2011"
}