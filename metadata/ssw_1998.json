{
 "title": "3rd ESCA/COCOSDA Workshop on Speech Synthesis (SSW 3)",
 "location": "Jenolan Caves House, Blue Mountains, Australia",
 "startDate": "26/11/1998",
 "endDate": "29/11/1998",
 "chair": "Dedicated to the memory of Christian Benoit",
 "conf": "SSW",
 "year": "1998",
 "name": "ssw_1998",
 "series": "SSW",
 "SIG": "SynSIG",
 "title1": "3rd ESCA/COCOSDA Workshop on Speech Synthesis",
 "title2": "(SSW 3)",
 "date": "26-29 November 1998",
 "papers": {
  "hirst98_ssw": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Comparison of subjective evaluation and an objective evaluation metric for prosody in text-to-speech synthesis",
   "original": "ssw3_001",
   "page_count": 4,
   "order": 1,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "An experimental technique is described for eliciting a subjective evaluation of the prosody of synthetic speech by untrained listeners. The technique makes use of a graphic display time-aligned with the speech signal. Subjects are asked to indicate which parts of a recording are unsatisfactory by clicking on a computer screen with a mouse. The technique was applied to two TTS systems for French. Results obtained using this technique are to be compared with those obtained using an objective evaluation metric for prosodic characteristics, comparing the synthetic versions with a number of different readings by human speakers.\n",
    ""
   ]
  },
  "sonntag98_ssw": {
   "authors": [
    [
     "Gerit P.",
     "Sonntag"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Felicitas",
     "Haas"
    ]
   ],
   "title": "Comparing the comprehensibility of different synthetic voices in a dual task experiment",
   "original": "ssw3_005",
   "page_count": 6,
   "order": 2,
   "p1": "5",
   "pn": "10",
   "abstract": [
    "We measured the comprehensibility of six German speech synthesis systems and one human voice in a dual task experiment that simulated the complexity of a real life task. PCM (Pulse Coded Modulation) and simulated GSM (Global System for Mobile communications) coding were compared. Both primary and secondary task showed significant differences in response times between voice types. Differences between the two coding conditions were significant for two of the seven voice types. Speaking rate was found to be an important factor for comprehension under increased difficulties.\n",
    ""
   ]
  },
  "dalessandro98_ssw": {
   "authors": [
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Joint evaluation of text-to-speech synthesis in French within the AUPELF ARC-B3 project",
   "original": "ssw3_011",
   "page_count": 6,
   "order": 3,
   "p1": "11",
   "pn": "16",
   "abstract": [
    "A joint international evaluations of Text-To-Speech syn- thesis (TTS) systems is being conducted for the French language. This project involves eight laboratories of French-speaking countries (Belgium, Canada, France and Switzerland), and is funded by AUPELF (Association of French Speaking Universities) The results obtained after 2 years of work are presented in this paper. The project is split into 4 tasks: 1/ evaluation of grapheme-to-phoneme conversion; 2/ evaluation of prosody; 3/ evaluation of  segments concatenation/modification; 4/ global system evalu- ation. Grapheme-phoneme conversion evaluation has now been completed, and both methodological issues and  results for the eight systems are presented at the workshop. For prosody evaluation, the problem is to study several systems that incorporate di\u000berent linguistic analyses,  different prosodic systems, di\u000berent intonation and rhythmic models. Using the same phonemic input and the same con- catenation/modification system with the same diphones, it will be possible to assess prosodic quality independently of the other modules. Perceptual evaluation is used at this stage. The degradation introduced by  concatenation/modification systems and by the quality of segment data-bases will be studied using perceptual tests.  Evaluation of the global systems will also be performed, using both intellegibility and agreement measures. Finally, one of the aims of this project is to make available corpora and evaluation paradigms that may be reused in future research. This will enable a quantitative analysis of the  results obtained, and a measurement of the progress achieved for each specific system.\n",
    ""
   ]
  },
  "campbell98_ssw": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Where is the information in speech? (and to what extent can it be modelled in synthesis?)",
   "original": "ssw3_017",
   "page_count": 4,
   "order": 4,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "Di\u000berent kinds of speech information are meaningfully used in human communication. This paper attempts to show how they can be modelled in speech synthesis and suggests that many conventional synthesis methods may fail to take into account the subtleties of human speech variation. It argues that modelling of voice  quality should be the next main goal for speech synthesis technology, and proposes that evaluations of synthesis technology should aim to include a Turing component, which measures the ability of each system to perform on a range of human-speech features.\n",
    ""
   ]
  },
  "mizuno98_ssw": {
   "authors": [
    [
     "Osamu",
     "Mizuno"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "Synthetic speech/sound control language: MSCL",
   "original": "ssw3_021",
   "page_count": 6,
   "order": 5,
   "p1": "21",
   "pn": "26",
   "abstract": [
    "The Multi-layered Speech/Sound Synthesis Control  Language (MSCL) proposed herein facilitates the synthesizing of several speech modes such as nuance, mental state and  emotion, and allows speech to be synchronized to other media easily. MSCL is a multi-layered linguistic system and  encompasses three layers: and semantic level layer (The S-layer), interpretation level layer (The I-layer), and parameter level layer (The P-layer). This multi-level description system is convenient for both laymen and professional users. MSCL also encompasses many e\u000bective prosodic feature control functions such as a time-varying pattern description  function, absolute and relative control forms, and SDS(Speaker Dependent Scale). MSCL enables more natural and  expressive synthetic speech than conventional TTS systems.  Furthermore, research was conducted into mental state tendencies using a test that examined the perceptions of the  subject's sensibility to the control of synthetic speech prosody. The results showed the relationships between prosodic  control rules and non-verbal expressions. Duration control  reflects information processing state in spoken dialogues.  Sentence final pitch contour control reflects the reliability of the information. Pitch contour dynamic range control indicates the speaker's excitement. The pitch contour control from start to peak pitch contour indicates the speaker's  requirement for attention. These relationships are of use for  constructing semantic prosody control.\n",
    "This paper describes these functions and the effective prosodic feature controls possible with MSCL.\n",
    ""
   ]
  },
  "sproat98_ssw": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ],
    [
     "Andrew",
     "Hunt"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Paul",
     "Taylor"
    ],
    [
     "Allan",
     "Black"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Mike",
     "Edgington"
    ]
   ],
   "title": "SABLE: A standard for TTS markup",
   "original": "ssw3_027",
   "page_count": 4,
   "order": 6,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "Currently, speech synthesizers are controlled by a multitude of proprietary tag sets. These tag sets vary substantially across synthesizers and are an inhibitor to the adoption of speech synthesis technology by developers. SABLE is an XML/SGML-based markup scheme for text-to-speech synthesis, developed to address the need for a common TTS control paradigm. This paper presents an overview of the SABLE v0.2 specification, and provides links to websites with further information on SABLE.\n",
    ""
   ]
  },
  "venditti98_ssw": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Modeling segmental durations for Japanese text-to-speech synthesis",
   "original": "ssw3_031",
   "page_count": 6,
   "order": 7,
   "p1": "31",
   "pn": "36",
   "abstract": [
    "Accurate estimation of segmental durations is crucial for naturalsounding text-to-speech (TTS) synthesis. This paper presents a model of segmental duration used in the Bell Labs Japanese TTS system. We describe the constraints on vowel devoicing, and effects of factors such as phone identity, surrounding phone identities, accentuation, syllabic structure, and phrasal position on the duration of both consonants and vowels. A Sum-of-Products approach is used to model key interactions observed in the data, and to predict values of factor combinations not found in the speech database. We report overall observed-predicted correlations of 0.88 for vowels (RMSdev: 16.8ms) and 0.94 for consonants (RMSdev: 12.5ms).\n",
    ""
   ]
  },
  "yang98_ssw": {
   "authors": [
    [
     "Li-Chiung",
     "Yang"
    ]
   ],
   "title": "Contextual Effects on Syllable Duration",
   "original": "ssw3_037",
   "page_count": 6,
   "order": 8,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "Recent work on duration has emphasized both higher level contextual factors as key to syllable duration and segmental characteristics as giving rise to syllable duration. fn this paper, our goal is to look at several higher level contexts, as well as specific phonemic characteristics, and investigate their respective contributions to syllable level duration. We show that the number of syllables in a name is an important determinant of syllable duration in our data. Tn addition, we find that position in sentence, stress, and phonemic complexity are important factors that help to account for variations in syllable duration, and that the duration sensitivity to stress and position are dependent on vowel characteristics, specifically, on the laxness or tenseness of the vowel.\n",
    ""
   ]
  },
  "febrer98_ssw": {
   "authors": [
    [
     "Albert",
     "Febrer"
    ],
    [
     "Jaume",
     "Padrell"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Modeling phone duration: application to Catalan TTS",
   "original": "ssw3_043",
   "page_count": 4,
   "order": 9,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "There are many exhaustive works that deal with the use of models for segmental duration. The aim of this paper is to evaluate some of the properties mentioned in literature and evaluate factorial and sum-of-products models in front of a listlike approach for Catalan language as a base for a most exhaustive study on duration in this language. Sum-of-products models for vowels and subsystems of consonants seem to be more adequate to model phone duration. The parameters for the sum-of-products models are presented in the paper. stress, phrasal position, surrounding phones, syllable length and syllable position.\n",
    ""
   ]
  },
  "trouvain98_ssw": {
   "authors": [
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "William J.",
     "Barry"
    ],
    [
     "Claus",
     "Nielsen"
    ],
    [
     "Ove",
     "Andersen"
    ]
   ],
   "title": "Implications of energy declination for speech synthesis",
   "original": "ssw3_047",
   "page_count": 6,
   "order": 10,
   "p1": "47",
   "pn": "52",
   "abstract": [
    "ABSTRACT This paper examines whether observed phenomena in energy declination can be used to improve the naturalness of synthetic speech. In two production experiments different aspects of intensity fall-off within utterances are analysed including degree of stress, phrase length, phrase boundaries. Energy manipulation was carried out using diphone synthesis as a basis for generating stimuli for perception tests in English and Danish. The results of the listening experiments, in which different versions of a paragraph were ranked for naturalness indicate that amplitude differences can contribute to greater naturalness. However, it is apparent that fine-tuning of amplitude requires good quality synthesis at the more basic prosodic levels.\n",
    ""
   ]
  },
  "damper98_ssw": {
   "authors": [
    [
     "Robert I.",
     "Damper"
    ],
    [
     "Y.",
     "Marchand"
    ],
    [
     "M. J.",
     "Adamson"
    ],
    [
     "Kjell",
     "Gustafson"
    ]
   ],
   "title": "Comparative evaluation of letter-to-sound conversion techniques for English text-to-speech synthesis",
   "original": "ssw3_053",
   "page_count": 6,
   "order": 11,
   "p1": "53",
   "pn": "58",
   "abstract": [
    "Dictionary look-up is the primary strategy for deriving pronunciations for input words in a text-to-speech (TTS) system. This strategy is accurate for dictionary words, but it is not complete: it is impossible to list exhaustively all input words. The proper treatment of 'unknown' words is currently an unsolved problem in TTS synthesis. There are many competing techniques for letter-to-sound conversion and the system developer must make a rational selection among them. However, it is unclear how di\u000berent  techniques should be properly compared. In this paper, we re- port a comparative assessment of the competitor methods of letter-to-sound rules, pronunciation by analogy,  feedforward neural networks and a k-nearest neighbour method, with respect to their success at automatic phonemisation. This is achieved by using standardised scoring methods, test lexicon and phoneme inventories. The problem of standardising the phoneme set ('harmonisation') is  deceptive: this is much harder than at first appears.  The principal finding is that (contrary to the weight of opinion expressed in the literature) data-driven techniques outperform  knowledge-based methods by a very significant margin.\n",
    ""
   ]
  },
  "mobius98_ssw": {
   "authors": [
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Word and syllable models for German text-to-speech synthesis",
   "original": "ssw3_059",
   "page_count": 6,
   "order": 12,
   "p1": "59",
   "pn": "64",
   "abstract": [
    "The correct pronunciation of unknown or novel words is one of the biggest challenges for text-to-speech systems. In this paper we describe the implementation of unknown word analysis as a central component of the text analysis module in the Bell Labs German text-to-speech system. The implementation is based on a model of the morphological structure of words and on the study of the productivity of word forming affixes. One important subcomponent of the word model is a phonotactic syllable model which enables the system to handle orthographic substrings that are unaccounted for by the explicitly listed morphemes. Finally, we discuss issues for future research.\n",
    ""
   ]
  },
  "damper98b_ssw": {
   "authors": [
    [
     "Robert I.",
     "Damper"
    ],
    [
     "Y.",
     "Marchand"
    ]
   ],
   "title": "Improving pronunciation by analogy for text-to-speech applications",
   "original": "ssw3_065",
   "page_count": 6,
   "order": 13,
   "p1": "65",
   "pn": "70",
   "abstract": [
    "This paper extends previous work on pronunciation by analogy (PbA) in several directions. PbA is a data-driven method for converting letters to sound, with potential  application to next-generation text-to-speech systems. We experiment with a range of methods for matching letter patterns in input words to those in the system dictionary when building a pronunciation lattice. We give prelimin- ary consideration to deriving lexical stress for input words. Common errors are analysed: these mostly involve vowel letters and phonemes. An output is not necessarily  guaranteed in PbA { the so-called silence problem. We report on a simple but effective strategy for silence avoidance.  Finally, we introduce the idea of using different strategies in combination to improve performance.\n",
    ""
   ]
  },
  "kiraz98_ssw": {
   "authors": [
    [
     "George Anton",
     "Kiraz"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Multilingual syllabification using weighted finite-state transducers",
   "original": "ssw3_071",
   "page_count": 6,
   "order": 14,
   "p1": "71",
   "pn": "76",
   "abstract": [
    "This paper describes an approach to syllabification that has been incorporated into the English and German text-to-speech systems at Bell Labs. Implemented as a weighted finite-state transducer, the syllabifier is easily integrated - via mathematical composition - into the finite-state based text analysis component of the text-to-speech  system. The weights are based on frequencies of onset, nucleus and coda types obtained from training data. While the training data is language-dependent, the formal approach is multilingual.\n",
    ""
   ]
  },
  "black98_ssw": {
   "authors": [
    [
     "Alan W",
     "Black"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Vincent",
     "Pagel"
    ]
   ],
   "title": "Issues in building general letter to sound rules",
   "original": "ssw3_077",
   "page_count": 4,
   "order": 15,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In general text-to-speech systems, it is not possible to guarantee that a lexicon will contain all words found in a text, therefore some system for predicting pronunciation from the word itself is necessary.\n",
    "Here we present a general framework for building letter to sound (LTS) rules from a word list in a language. The technique can be fully automatic, though a small amount of hand seeding can give better results. We have applied this technique to English (UK and US), French and German. The generated models achieve, 75%, 58%, 93% and 89%, respectively, words correct for held out data from the word lists.\n",
    "To test our models on more typical data we also analyzed general text, to find which words do not appear in our lexicon. These unknown words were used as a more realistic test corpus for our models. We also discuss the distribution and type of such unknown words.\n",
    ""
   ]
  },
  "shih98_ssw": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Contextual effects on voicing profiles of German and Mandarin consonants",
   "original": "ssw3_081",
   "page_count": 6,
   "order": 16,
   "p1": "81",
   "pn": "86",
   "abstract": [
    "In this paper we present a study of the voicing profiles of consonants in Mandarin Chinese and German. The voicing profile is defined as the frame-by-frame voicing status of a speech sound in continuous speech. We are particularly interested in discrepancies between the phonological voicing status of a speech sound and its actual phonetic realization in connected speech.We further examine the contextual factors that cause voicing variations and test the cross-language validity of these factors. The result can be used to improve speech synthesis, and to refine phone models to enhance the performance of automatic speech segmentation and recognition.\n",
    ""
   ]
  },
  "rilliard98_ssw": {
   "authors": [
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Reiterant speech for the evaluation of natural vs. synthetic prosody",
   "original": "ssw3_087",
   "page_count": 5,
   "order": 17,
   "p1": "87",
   "pn": "92",
   "abstract": [
    "This work deals with some evaluation experiments on reiterant speech using both synthetic and natural stimuli. They have been designed to test the efficiency of the described paradigm to diagnose the adequacy of synthetic prosody to syntactic structure in reference with natural performances. Following a general methodology developed for synthesis [1], experiments have been conducted on the ICP synthetic prosody with the aim of validating the original corpus from which the prosodic model was learned . perceptive information carried by prosody.\n",
    "",
    "",
    "Morlec, Y., Rilliard, A., Bailly, G. & Aubergé, V., \"Evaluating the adequacy of synthetic prosody in signalling syntactic boundaries: methodology and first results\". Proceedings of the 1st International Conference on Language Resources and Evaluation, Vol. 1, p. 647-650, 1998.\n",
    ""
   ]
  },
  "grover98_ssw": {
   "authors": [
    [
     "C.",
     "Grover"
    ],
    [
     "J.",
     "Fackrell"
    ],
    [
     "H.",
     "Vereecken"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Designing prosodic databases for automatic modelling in 6 languages",
   "original": "ssw3_093",
   "page_count": 6,
   "order": 18,
   "p1": "93",
   "pn": "98",
   "abstract": [
    "We describe the design and creation of prosodic speech databases for 6 languages. The purpose of the databases is to allow derivation of prosody models in order to improve TTS synthesis. The main prosodic variables to model were word prominence, prosodic boundary strength and phone duration. We describe the database structure and contents and the methodology for creating prosodic databases, and we present statistics for the main prosodic variables.\n",
    ""
   ]
  },
  "sannier98_ssw": {
   "authors": [
    [
     "Frédérique",
     "Sannier"
    ],
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Rabia",
     "Belrhali"
    ]
   ],
   "title": "How a French text-to-speech system can describe loanwords",
   "original": "ssw3_099",
   "page_count": 6,
   "order": 19,
   "p1": "99",
   "pn": "104",
   "abstract": [
    "We propose here an approach of the morphophonological treatment of borrowed lexical items in the general French lexicon, through the mecanims of the functionning of the TTS Toph. We will show how the methodology we adopted in this TTS allows us to distinguish two levels of processing, adapted to the nature of the items. After the display of the typology of phonetic transfers we will focus on the inflexional systems of loanwords, taking into account the graphic and the phonetic dimensions. The last part will be dedicated to the phonetic inflected forms generation, which would find applications in concept-to-speech synthesis.\n",
    ""
   ]
  },
  "shih98b_ssw": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Efficient adaptation of TTS duration model to new speakers",
   "original": "ssw3_105",
   "page_count": 5,
   "order": 20,
   "p1": "105",
   "pn": "110",
   "abstract": [
    "This paper discusses a methodology using a minimal set of sentences to adapt an existing TTS duration model to capture interspeaker variations. The assumption is that the original duration database contains information of both language-specific and speaker-specific duration characteristics. In training a duration model for a new speaker, only the speaker-specific information needs to be modeled, therefore the size of the training data can be reduced drastically. Results from several experiments are compared and discussed.\n",
    ""
   ]
  },
  "dirksen98_ssw": {
   "authors": [
    [
     "Arthur",
     "Dirksen"
    ],
    [
     "Ludmila",
     "Menert"
    ]
   ],
   "title": "Prosody control in fluent Dutch text-to-speech",
   "original": "ssw3_111",
   "page_count": 5,
   "order": 21,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "Fluent Dutch Text-To-Speech is a new, high-quality TTS system for Dutch, built on top of the MBROLA diphone synthesizer (Dutoit 1997), and commercially available for Windows 95/NT. This paper discusses two applications which use the TTS system: a talking dictionary, and a communication aid for the vocally handicapped. In the talking dictionary application, parameterized prosody rules enable the generation of various pronunciations of a word, from formal to informal. For the vocally handicapped, prosody controls are used to create different voices and moods.\n",
    ""
   ]
  },
  "jokisch98_ssw": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Diane",
     "Hirschfeld"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Creating an individual speech rhythm: a data driven approach",
   "original": "ssw3_115",
   "page_count": 5,
   "order": 22,
   "p1": "115",
   "pn": "119",
   "abstract": [
    "Generating a near-to-natural speech rhythm can greatly contribute to the user's acceptance of TTS systems. Beside common aspects of the rhythm control (correctness of the segmental durations, robust function, etc.) rhythmic flexibility for several applications and individual speaking styles are desired. This article describes a data driven concept, which aims at the generation of an individual speech rhythm for the Dresden TTS system for German (DreSS). An additional, prosodic-phonetic database has been extracted from the source speakers of the existing diphone inventories (acoustic synthesis). This database is used for adjusting rule-based and statistic models for the duration control, but also for training an alternative, neural network model (ANN). Several combinations of the models have been tested. From the current point of view, the effect of the specific model used is less than expected, but the appropriate design of the prosodic database seems to support the necessary variety of the rhythmic parameters. A limited individual modeling of the speech rhythm is possible. However, the global evaluation of the introduced approach includes some contradictions; more extensive tests are required.\n",
    ""
   ]
  },
  "cahn98_ssw": {
   "authors": [
    [
     "Janet E.",
     "Cahn"
    ]
   ],
   "title": "Generating pitch accent distributions that show individual and stylistic differences",
   "original": "ssw3_121",
   "page_count": 6,
   "order": 23,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "I describe a limited-resource approach to generating prosody that mediates text-based information through a model of attention and working memory, whose simulation parameters are quantitative. The main parameter  quantifies recall. Varying it varies what counts as given and new in a text, and therefore, the pitch accents with which the text is uttered. Currently, the system produces prosody in three different styles of read speech - child-like, adult expressive, and knowledgeable - and individual variation within each. A comparison with natural data shows clear and predictable stylistic similarities, although not at  significance. However, informal feedback is more forgiving, indicating that the prosody is both natural and expressive for consecutive phrases, but that work is still needed to make this effect consistent throughout the text.\n",
    ""
   ]
  },
  "boulademareuil98_ssw": {
   "authors": [
    [
     "Philippe",
     "Boula de Mareüil"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Text chunking for prosodic phrasing in French",
   "original": "ssw3_127",
   "page_count": 5,
   "order": 24,
   "p1": "127",
   "pn": "132",
   "abstract": [
    "In this paper, we describe experiments in text chunking  for prosodic phrasing and generation in French. We present a quick, robust and deterministic parser which uses part-of-speech information and a set of rules, to consistently  assign prosodic boundaries in Text-To-Speech synthesis.  The syntactic phrasing, consisting of segmenting sentences in non-recursive sequences, is defined in terms of sets of possible categories. The syntax-prosody interface  is presented: the sequences enable the location of potential prosodic boundaries (minor, major or intermediate).  The first results are given, including of a listening  test, which demonstrated the advantage of our chunk grammar over a simpler approach, based on function words and punctuation. Quantitative measures are made on the chunks defined between boundaries. Our model prefers structural criteria to probabilities, and an approach by  intension rather than by extension, is also compared with other models.\n",
    ""
   ]
  },
  "miller98_ssw": {
   "authors": [
    [
     "Corey",
     "Miller"
    ]
   ],
   "title": "Individuation of postlexical phonology for speech synthesis",
   "original": "ssw3_133",
   "page_count": 4,
   "order": 25,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Postlexical phonology exhibits a great deal of interspeaker variation, even within widely construed dialects such as American English. Contemporary speech synthesizers have concentrated on modeling the acoustic properties of individual speakers, but not necessarily their phonologies. We discuss a method for inferring individual postlexical phonologies from labeled corpora. Using this method, individualized phonologies can be combined with individualized acoustic models, thereby enabling a speech synthesizer to achieve a closer facsimile of an original voice.\n",
    ""
   ]
  },
  "keller98_ssw": {
   "authors": [
    [
     "Eric",
     "Keller"
    ],
    [
     "Brigitte",
     "Zellner"
    ]
   ],
   "title": "Motivations for the prosodic predictive chain",
   "original": "ssw3_137",
   "page_count": 5,
   "order": 26,
   "p1": "137",
   "pn": "142",
   "abstract": [
    "Prosodic modelling is presented as a two-part process. In the first part, predictor variables are identified on the basis of psycholinguistic factors that determine word grouping. In the second part, these and intrinsic-contextual linguistic factors generate predictions for durational and melodic assignment. It is argued that data-driven approaches can be rendered more efficient by introducing and respecting psycholinguistic principles of human prosodic processing. This study summarises thinking developed conjointly in our laboratory since 1993, as well as portions of the second authors thesis (Zellner, 1998).\n",
    "",
    "",
    "Zellner, B. (1998). Caractérisation et prédiction du débit de parole en français. Une étude de cas. Thèse de Doctorat. Faculté des Lettres, Université de Lausanne.\n",
    ""
   ]
  },
  "zellner98_ssw": {
   "authors": [
    [
     "Brigitte",
     "Zellner"
    ]
   ],
   "title": "Temporal structures for fast and slow speech rate",
   "original": "ssw3_143",
   "page_count": 4,
   "order": 27,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "The rhythmic component in speech synthesis often remains rather rudimentary, despite recent major efforts in the modeling of prosodic models. The European COST Action 258 has identified this problem as one of the next challenges for speech synthesis. This paper is a contribution to a new, promising approach that was tested on a French temporal model.\n",
    ""
   ]
  },
  "taylor98_ssw": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Richard",
     "Caley"
    ]
   ],
   "title": "The architecture of the Festival speech synthesis system",
   "original": "ssw3_147",
   "page_count": 5,
   "order": 28,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "We describe a new formalism for storing linguistic data in a text to speech system. Linguistic entities such as words and phones are stored as feature structures in a general object called an linguistic item. Items are configurable at run time and via the feature structure can contain arbitrary information. Linguistic relations are used to store the relationship between items of the same linguistic type. Relations can take any graph structure but are commonly trees or lists. Utterance structures contain all the items and relations contained in a single utterance. We first describe the design goals when building a synthesis architecture, and then describe some problems with previous architectures. We then discuss our new formalism in general along with the implementation details and consequences of our approach.\n",
    ""
   ]
  },
  "portele98_ssw": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "JUst CONcatenation - A Corpus-based Approach and its Limits",
   "original": "ssw3_153",
   "page_count": 6,
   "order": 29,
   "p1": "153",
   "pn": "158",
   "abstract": [
    "This paper describes a radical corpus-based approach to speech synthesis. No signal manipulation is performed and the synthesis becomes a mere concatenation. The feasibility of this approach is evaluated regarding corpus selection constraints and realization of different prominence patterns. A \"traditional\" concatenative system serves as a baseline. The results indicate that the size of the corpus must be rather large in order to obtain satisfying and reliable results for unlimited text-to-speech conversion.\n",
    ""
   ]
  },
  "carvalho98_ssw": {
   "authors": [
    [
     "Pedro M.",
     "Carvalho"
    ],
    [
     "Luís C.",
     "Oliveira"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "M. Céu",
     "Viana"
    ]
   ],
   "title": "Concatenative speech synthesis for European Portuguese",
   "original": "ssw3_159",
   "page_count": 6,
   "order": 30,
   "p1": "159",
   "pn": "164",
   "abstract": [
    "This paper describes our on-going work in the area of text-tospeech synthesis, specifically on concatenative techniques. Our preliminary work consisted in investigating the current trends in concatenative synthesis and the problems that could arise when we apply the existing state-of-the art solutions to the specific case of European Portuguese.\n",
    "Our ultimate goal is to develop a text-to-speech system that could be trained for any speakers voice in a fully automatic way, i.e., we would like to develop a customized text-to-speech synthesizer for any voice reading a predetermined text.\n",
    "Our first steps in this direction involved such issues as automatic segmentation and alignment of recorded speech, optimized inventory design for concatenative synthesis, unit selection and optimal coupling of the selected units.\n",
    ""
   ]
  },
  "andersen98_ssw": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "N.-J.",
     "Dyhr"
    ],
    [
     "I. S.",
     "Engberg"
    ],
    [
     "C.",
     "Nielsen"
    ]
   ],
   "title": "Synthesising short vowels from their long counterparts in a concatenative based text-to-speech system",
   "original": "ssw3_165",
   "page_count": 6,
   "order": 31,
   "p1": "165",
   "pn": "170",
   "abstract": [
    "Danish has a distinctive vowel length opposition which is realized with little differences in vowel qualities. This paper investigates the possibilities of using this fact in reducing the size of the speech unit database in a high quality concatenative based text-to-speech system for Danish. The purpose is to evaluate the concept of using long vowels for synthesizing the corresponding short vowels. If this proves successful the size of the speech unit database may be reduced by approximately 40%. An acoustic analysis of the long and short vowels in the present speech unit database was performed. The results are presented in a F1-F2 plot, and demonstrate a significant overlap between long and short vowels. Consequently, two different strategies for synthesizing the short vowels from their long counterpart were tested. The first strategy used resegmented long vowel and the second relied entirely on the time-scaling technique built into the signal generation module. The two strategies for synthesizing the short vowels were compared to using pre-recorded short vowels in a comprehensive listening test. The results of the listening test were based on 32 subjects judging intelligibility and naturalness. The results show no significant differences between the prerecorded short vowels and the resegmented long vowels synthesized as short vowels. The resegmented long vowels will be implemented in the present text-to-speech system for further testing.\n",
    ""
   ]
  },
  "bunnell98_ssw": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Steven R.",
     "Hoskins"
    ],
    [
     "Debra M.",
     "Yarrington"
    ]
   ],
   "title": "A biphone constrained concatenation method for diphone synthesis",
   "original": "ssw3_171",
   "page_count": 6,
   "order": 32,
   "p1": "171",
   "pn": "176",
   "abstract": [
    "Diphone concatenation [1] has the advantages of simplicity and a relatively small database of speech when compared to other concatenative synthesis methods (e.g., [2]). However, diphone concatenation faces two notable problems. The first is coarticulation which extends beyond the scope of a single diphone and entails some degree of contextual mismatch for virtually any diphone in at least some concatenation contexts. The second problem, which stems from the first, is computational. It is the problem of selecting, from a specific speech corpus, an optimal instance of each diphone to achieve the least amount of temporal and spectral distortion in the broadest set of concatenation contexts (e.g., [3]).\n",
    "We present a variant of diphone synthesis which addresses both problems by (a) allowing multiple tokens of diphones where needed to accommodate the effects of coarticulation, and (b) postponing diphone selection until synthesis when optimization can be constrained by known contextual factors. This method, termed Biphone Constrained Concatenation (BCC), has been implemented for use in the ModelTalker TtS system [4]. Comparisons of speech synthesized using BCC versus speech synthesized using pure diphone concatenation indicate clear improvements in naturalness for the BCC method. However, our listening experiments also demonstrated some increase in consonant confusions for the BCC method due to uncontrolled durational factors.\n",
    "s Peterson, G., Wang, W., and Siversten, E. (1958). Segmentation techniques in speech synthesis. . Journal of the Acoustical Society of America, 30, 739-742. Takeda, K., Abe, K., and Sagisaka, Y. (1992). On the basic scheme and algorithms in non-uniform unit speech synthesis. In G. Bailly, C. Benoit and T.R. Sawallis (eds.), Talking Machines: Theories, Models, and Designs. Amsterdam: Elsevier, 93-105. Conkie, A. D. and Isard, S. (1997). Optimal coupling of diphones. In Progress in Speech Synthesis, van Santen, J.P.H., Sproat, R.W., Olive, J.P., and Hirschberg, J. (eds.). Springer, New York, pp. 293-304. Bunnell, H.T., Hoskins, S. R., and Yarrington, D. M. (1998). The ModelTalker project: Software for diphone speech synthesis and automatic diphone extraction. University of Delaware, Computer and Information Sciences Technical Report, 98-13.\n",
    ""
   ]
  },
  "campbell98b_ssw": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Foreign Language Speech System",
   "original": "ssw3_177",
   "page_count": 4,
   "order": 33,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "This paper describes a method of concatenative speech synthesis for producing speech in a language other than that of the database speaker. In certain applications, such as interpreted dialogues or multi-lingual e-mail, it is necessary to synthesise words that are foreign with respect to the language of the main text. In this case, rather than switch voices, we show that the use of an  intermediate stage of synthesis improves the pronunciation and prosody of the output speech.\n",
    ""
   ]
  },
  "fujisawa98_ssw": {
   "authors": [
    [
     "Ken",
     "Fujisawa"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Prosody-based unit selection for Japanese speech synthesis",
   "original": "ssw3_181",
   "page_count": 4,
   "order": 34,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "A corpus-based concatenative speech synthesis system using no signal processing can produce intelligible  synthetic speech maintaining original voice characteristics. In such a concatenative system, it is very important to select appropriate waveform segments that are  naturally close to the target prosody. But with a limited size database it can sometimes be difficult to realize natural prosody.\n",
    "This paper describes an approach to unit (waveform segment) selection for improving the intonation. We analyzed the pitch patterns of 503 sentences of read speech spoken by a Japanese female and obtained the F0 range of natural prosody. Then we applied this restriction to the unit selection of the concatenative speech synthesizer. Through subjective experiments, we confirmed that this measure significantly improved the intonational naturalness of synthetic speech.\n",
    ""
   ]
  },
  "beutnagel98_ssw": {
   "authors": [
    [
     "Mark",
     "Beutnagel"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "Diphone synthesis using unit selection",
   "original": "ssw3_185",
   "page_count": 6,
   "order": 35,
   "p1": "185",
   "pn": "190",
   "abstract": [
    "This paper describes an experimental AT&T concatenative synthesis system using unit selection, for which the basic synthesis  units are diphones. The synthesizer may use any of the data from a large database of utterances. Since there are in general multiple instances of each concatenative unit, the system performs dynamic unit selection. Selection among candidates is done dynamically at synthesis, in a manner that is based on and extends unit selection implemented in the CHATR synthesis system [1][4]. Selected units may be either phones or diphones, and they can be synthesized by a variety of methods, including PSOLA [5], HNM [3], and simple unit concatenation. The AT&T system, with CHATR unit selection, was implemented within the framework of the Festival Speech Synthesis System [2]. The voice database amounted to approximately one and one-half hours of speech and was constructed from read text taken from three sources. The first source was a portion of the 1989 Wall Street Journal  material from the Penn Treebank Project, so that the most frequent diphones were well represented. Complete  diphone coverage was assured by the second text, which was designed for diphone databases [6]. A third set of data consisted of recorded prompts for telephone service applications.  Subjective formal listening tests were conducted to compare speech quality for several options that exist in the AT&T synthesizer, including synthesis methods and choices of fundamental units. These tests showed that unit selection techniques can be successfully applied to diphone synthesis.\n",
    "s\n",
    "A. Black. CHATR, Version 0.8, a generic speech synthe- sis. System documentation. ATR - Interpreting Telecom- munications Laboratories, Kyoto, Japan, March 1996. A. Black and P. Taylor. The Festival Speech Synthe- sis System: system documentation. Technical Report HCRC/TR-83. Human Communications Research Cen- tre, University of Edinburgh, Scotland, UK, January 1997. Y. Stylianou, T. Dutoit, and J. Schroeter. Diphones con- catenation using a harmonic plus noise model of speech. Proc. EUROSPEECH, Sept. 1997. A. Hunt and A. Black. Unit selection in a concatenative speech synthesis system using a large speech database. ICASSP, 1:373-376, 1996. E. Moulines and F. Charpentier. Pitch-synchronous waveform processing techniques for text-to-speech syn- thesis using diphones. Speech Communication, 9 (5/6):453{467, 1990. A. Syrdal. Development of a female voice for a concate- native text-to-speech synthesis system. Current Topics in Acoust. Res., 1:169-181, 1994.\n",
    ""
   ]
  },
  "ding98_ssw": {
   "authors": [
    [
     "Wen",
     "Ding"
    ],
    [
     "Ken",
     "Fujisawa"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Improving speech synthesis of CHATR using a perceptual discontinuity function and constraints of prosodic modification",
   "original": "ssw3_191",
   "page_count": 4,
   "order": 36,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "Concatenative synthesis is widely used in TTS to  generate synthetic speech with high quality and relatively natural-sounding prosody. Whatever the type of  synthesis unit used, (diphone, phoneme, etc.), a large speech database is usually needed to ensure the phonetic and phonemic variation of the units in a rich variety of  contexts. In the CHATR synthesis system, unit selection finds the most appropriate phoneme sequence for an  input text by using a criterion of minimizing a) joint  discontinuity and b) mismatch in target prosody. However, in the current unit selection module, only an objective distance function is used, and the pitch and duration are not modified to match the target prosody.\n",
    "We address two issues in this paper: (1) How to derive a perceptual discontinuity function to determine the perceptually significant amount of discontinuity  between two candidate units, while (2) taking into  account the constraints of possible prosodic modification (pitch/duration scaling using signal processing). Both the techniques are tested with the unit selection and synthesis modules and the changes in voice quality and prosody are evaluated.\n",
    ""
   ]
  },
  "macon98_ssw": {
   "authors": [
    [
     "Michael W.",
     "Macon"
    ],
    [
     "Andrew E.",
     "Cronk"
    ],
    [
     "Johan",
     "Wouters"
    ]
   ],
   "title": "Generalization and discrimination in tree-structured unit selection",
   "original": "ssw3_195",
   "page_count": 6,
   "order": 37,
   "p1": "195",
   "pn": "200",
   "abstract": [
    "Concatenative \"selection-based\" synthesis from large databases has emerged as a viable framework for TTS waveform generation. Unit selection algorithms  attempt to predict the appropriateness of a particular database speech segment using only linguistic features output by text analysis and prosody prediction  components of a synthesizer. All of these algorithms have in common a training or \"learning\" phase in which  parameters are trained to select appropriate waveform seg- ments for a given feature vector input. One approach to this step is to partition available data into clusters that can be indexed by linguistic features available at runtime. This method relies critically on two important principles: discrimination of fine phonetic details using a perceptually-motivated distance measure in training and generalization to unseen cases in selection. In this paper, we describe e\u000borts to systematically investigate and improve these parts of the process.\n",
    ""
   ]
  },
  "breen98_ssw": {
   "authors": [
    [
     "Andrew P.",
     "Breen"
    ],
    [
     "Peter",
     "Jackson"
    ]
   ],
   "title": "Non-uniform unit selection and the similarity metric within BTs Laureate TTS system",
   "original": "ssw3_201",
   "page_count": 6,
   "order": 38,
   "p1": "201",
   "pn": "206",
   "abstract": [
    "In BT's Laureate text to speech system, the process of generating natural sounding synthetic speech from text can be viewed as a three stage process. The first stage attempts to convert general text into some form of normalised textual representation. This stage may consist of a number of components which are designed to handle domain specific problems. The second stage converts the normalised linear orthographic input data into a structured linguistic description. This stage consists of a number of components comprising orthography to phoneme conversion, syntactic analysis, performance parsing, and the prediction of duration and intonation. The third and final stage uses this linguistic structure to generate synthetic speech. The nature of the production stage differs depending on the production method. The Laureate system has, over the last five years, been concentrating on a concatenative approach. Clearly the method of unit selection used within this stage contributes significantly to the eventual quality of the synthetic speech produced. This paper will describe the method of unit selection currently implemented within Laureate.\n",
    "Concatenative speech synthesis systems generate speech from a unit inventory of sounds. The phoneme has proved the most popular symbolic representation of sound in these systems, but simply storing one sample phone for each phoneme is not sufficient for good quality synthesis. Coarticulation is one reason why this is so - the production of one phone can be highly influenced by its preceding and following neighbours. The challenge for all methods of unit selection is to provide an efficient method of selecting units which, in some clearly specified way, provide the best approximation to the desired phones available within the inventory.\n",
    "The Laureate system uses mixed N-phone units. In theory such units could be of arbitrary size, but in practice, they are constrained to a maximum of three phones (triphone). In addition, unlike traditional methods of unit selection, Laureate does not attempt to find the best unit from a fixed pre-selected set. Rather, it dynamically generates a sequence of units based on a global cost. Units are selected using purely phonologically motivated criteria, without reference to any acoustic features either desired or available within the inventory.\n",
    "Details of the selection process will be provided within the paper together with a discussion on existing short falls of the method and future envisaged improvements.\n",
    ""
   ]
  },
  "torretoledano98_ssw": {
   "authors": [
    [
     "D.",
     "Torre Toledano"
    ],
    [
     "M. A.",
     "Rodríguez Crespo"
    ],
    [
     "J. G.",
     "Escalada Sardina"
    ]
   ],
   "title": "Trying to mimic human segmentation of speech using HMM and fuzzy logic post-correction rules",
   "original": "ssw3_207",
   "page_count": 7,
   "order": 39,
   "p1": "207",
   "pn": "212",
   "abstract": [
    "The process of human segmentation and labelling of speech can be seen as a two-step process. In the first step humans listen to a speech signal, recognize the word and phoneme sequence, and roughly determine the position of each phonetic boundary. In the second step humans examine several speech signal features (waveform, energy, spectrogram, etc.) to place a phonetic boundary time mark where these features best satisfy a certain set of conditions specific for that kind of phonetic boundary. In this paper an automatic two-stage system for phonetic segmentation and labelling of speech is presented. This system tries to mimic the two-step process of human segmentation and labelling of speech. The first stage of the system is a context-dependent phonetic HMM recognizer that yields the recognized phoneme sequence and a set of rough phonetic boundary time marks. The second stage extracts several speech signal features that are intended to be the counterpart of those examined by humans. These features are used to refine each rough time mark obtained in the first stage. Each time mark is moved to a near position where the degree of truthfulness of a certain set of fuzzy logic conditions (specific for that kind of phonetic boundary) is maximum. These fuzzy logic conditions are intended to be the counterpart of the conditions tested by humans.\n",
    ""
   ]
  },
  "sluijter98_ssw": {
   "authors": [
    [
     "Agaath",
     "Sluijter"
    ],
    [
     "E.",
     "Bosgoed"
    ],
    [
     "J.",
     "Kerkhoff"
    ],
    [
     "E.",
     "Meier"
    ],
    [
     "Toni",
     "Rietveld"
    ],
    [
     "A.",
     "Sanderman"
    ],
    [
     "Marc",
     "Swerts"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Evaluation of speech synthesis systems for Dutch in telecommunication applications",
   "original": "ssw3_213",
   "page_count": 6,
   "order": 40,
   "p1": "213",
   "pn": "218",
   "abstract": [
    "An evaluation was conducted to compare four TTS systems for Dutch with respect to intelligibility and acceptability, as a follow-up to an evaluation study conducted in 1997. Intelligibility was measured by the ability of listeners to write down correctly semantically unpredicatable sentences. Acceptability was tested in two ways: by asking listeners for subjective judgments on a number of semantic scales for individual TTS systems, and by asking for preference judgments in a pairwise comparison. For the acceptability tests a weather forecast and an e-mail text were used as test materials. It was found that the system giving best intelligibility performed worst in the acceptability tests. It was concluded that subjective acceptability is not a simple consequence of intelligibility, and that a distinction needs to be made between the esthetic and functional dimensions of synthetic speech. In comparing the results of the current study with those of the evaluation study conducted in 1997, it appeared fair to conclude that at least two systems are available for Dutch which constitute a substantial improvement over last years state of the art, although it is evident from the results that many aspects are still in need of improvement.\n",
    ""
   ]
  },
  "heid98_ssw": {
   "authors": [
    [
     "Sebastian",
     "Heid"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "PROCSY: A hybrid approach to high-quality formant synthesis using HLSyn",
   "original": "ssw3_219",
   "page_count": 6,
   "order": 41,
   "p1": "219",
   "pn": "224",
   "abstract": [
    "PROCSY is a hybrid method of automatically producing natural-sounding formant-based synthetic speech from an existing speech signal by using copy-synthesis and  estimated articulatory trajectories as input to the HLsynTM synthesizer (Sensimetrics Corporation). The purpose is to allow controlled manipulation of selected acoustic  parameters. Parameters for HLsyn are derived from labelled speech files in two ways. Broadly, vowels and approximants  are copy-synthesized from the acoustic signal, while obstruents and nasals are synthesized by rule: articulatory trajectories and constriction areas are estimated from the segment label and duration, and converted into HL  parameter values. HLsyn combines information from both sources to calculate parameter values for a Klatt-type  synthesizer. Strengths of the method are (i) simple HLsyn input captures acoustically complex obstruents, and (ii) HLsyn parameters automatically produce complex acoustic properties that accompany consonantal closures, especially at segment boundaries. These properties are hard to  synthesize and thus typically absent in formant TTS, yet they provide some of the systematic variability we hypothesize contributes to robust, natural-sounding synthesis.  Potential applications are discussed.\n",
    ""
   ]
  },
  "kain98_ssw": {
   "authors": [
    [
     "Alexander",
     "Kain"
    ],
    [
     "Mike",
     "Macon"
    ]
   ],
   "title": "Personalizing a speech synthesizer by voice adaptation",
   "original": "ssw3_225",
   "page_count": 6,
   "order": 42,
   "p1": "225",
   "pn": "230",
   "abstract": [
    "A voice adaptation system enables users to quickly create new voices for a text-to-speech system, allowing for the personalization of the synthesis output. The system adapts to the pitch and spectrum of the target speaker, using a probabilistic, locally linear conversion function based on a Gaussian Mixture Model. Numerical and perceptual evaluations reveal insights into the correlation between adaptation quality and the amount of training data, the number of free parameters. A new joint density estimation algorithm is compared to a previous approach. Numerical errors are studied on the basis of broad phonetic categories. A data augmentation method for training data with incomplete phonetic coverage is investigated and found to maintain high speech quality while partially adapting to the target voice.\n",
    ""
   ]
  },
  "plumpe98_ssw": {
   "authors": [
    [
     "M.",
     "Plumpe"
    ],
    [
     "S.",
     "Meredith"
    ]
   ],
   "title": "Which is more important in a concatenative text to speech system - pitch, duration, or spectral discontinuity?",
   "original": "ssw3_231",
   "page_count": 5,
   "order": 43,
   "p1": "231",
   "pn": "236",
   "abstract": [
    "This paper focuses on experimental evaluations designed to determine the relative quality of the components of the Whistler TTS engine. Eight different systems were compared pairwise to determine a rank ordering as well as a measure of the quality difference between the systems. The most interesting aspect of the results is that the simple unit duration scheme used in Whistler was found to be very good, both when it was used in combination with natural acoustics and pitch as well as when it was taken in combination with synthetic pitch. The synthetic pitch was found to be the aspect of the system that results in greatest quality degradation.\n",
    ""
   ]
  },
  "silva98_ssw": {
   "authors": [
    [
     "C.",
     "Silva"
    ],
    [
     "S.",
     "Chennoukh"
    ]
   ],
   "title": "Estimation of articulatory parameter trajectory from speech acoustic dynamics",
   "original": "ssw3_237",
   "page_count": 5,
   "order": 44,
   "p1": "237",
   "pn": "242",
   "abstract": [
    "This research aims to perform articulatory analysis as a basis for low bit-rate speech coding. The classical approach consists of gathering a large set of acoustic and articulatory vector pairs in a codebook. Then, based on some criteria, the non-uniqueness of the articulatory trajectories is solved using a dynamic optimization procedure. An articulatory codebook requires a model capable of generating shapes for all possible speech sounds. This paper reports a new  approach for incorporating the tongue tip on Ishizaka's vocal tract area function model in order to reproduce more  details of certain classes of consonants. Results are reported on the search for an optimized articulatory codebook using different methods of model parameter sampling.\n",
    ""
   ]
  },
  "charonnat98_ssw": {
   "authors": [
    [
     "L.",
     "Charonnat"
    ],
    [
     "G.",
     "Ó-Néill"
    ],
    [
     "Guy",
     "Mercier"
    ]
   ],
   "title": "An Irish speech synthesiser",
   "original": "ssw3_243",
   "page_count": 6,
   "order": 45,
   "p1": "243",
   "pn": "248",
   "abstract": [
    "In the context of the realisation of a Text-To-Speech[1] system for Irish[2], a new algorithm for speech synthesis has been developed. This algorithm, which achieves synthesis by concatenation of diphones, is based principally on two classical signal processing techniques: the linear prediction and the Overlap and Add (OLA). Unlike the well-known TD-PSOLA method, no pitch marking is required; instead, the recorded segments are modified in order to produce pitch constant signals. Thus, the OLA procedures are applied to broad windows especially during concatenation, enabling a spectral smoothing of the transition between the diphones.\n",
    "An initial pitch modification, energy equalisation and, if necessary, a lengthening of the shorter sounds are carried out. The actual synthesis then consists of two modules: concatenation and prosody matching, including pitch and duration modification.\n",
    "The pitch modification (both in the initialisation stage and in the prosody matching) is realised through a linear prediction analysis of the signal, producing estimates of the vocal tract filter and the glottal signal. In order to modify the pitch without changing the formant frequencies, an interpolation (or decimation) is applied to each period of the glottal signal according to the required pitch modification rate.\n",
    "The duration modification is based on the time-scale modification algorithm proposed by Roucos and Wilgus [3], called the Synchronous Overlap and Add algorithm. The method and the computation of its parameters have been optimised, producing a very high quality time-scale modification.\n",
    "Finally, the concatenation module consists of overlapping the common phoneme of the two diphones being concatenated. A computation of their cross correlation allows us to synchronise them avoiding phase mismatch. The constant pitch allows a large overlap of the signals. Before their addition, two half hamming windows (the first one is decreasing and the second one is increasing) are applied to the signals to generate a smooth spectral transition.\n",
    "The algorithm has been tested on Irish sentences. The diphones have been extracted from a corpus recorded by an Irish speaker, trying hard to keep a constant pitch during the pronunciation to facilitate the initial pitch modification. The prosody of the sentence have been defined from a reference pronunciation of the same sentence. The synthesised sentence is fairly clear with some degree of naturalness.\n",
    ""
   ]
  },
  "badin98_ssw": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "M.",
     "Raybaudi"
    ],
    [
     "C.",
     "Segebarth"
    ]
   ],
   "title": "A three-dimensional linear articulatory model based on MRI data",
   "original": "ssw3_249",
   "page_count": 6,
   "order": 46,
   "p1": "249",
   "pn": "254",
   "abstract": [
    "Based on a set of 3D vocal tract images obtained by MRI, a 3D statistical articulatory model has been built using guided Principal Component Analysis. It constitutes an extension to the lateral dimension of the mid-sagittal model previously developed from a radiofilm recorded on the same subject. The parameters of the 2D model have been found to be good predictors of the 3D shapes, for most configurations. A first evaluation of the model in terms of area functions and formants is presented.\n",
    ""
   ]
  },
  "funaki98_ssw": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ],
    [
     "Yoshikazu",
     "Miyanaga"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "On subband analysis based on glottal-ARMAX speech model",
   "original": "ssw3_255",
   "page_count": 6,
   "order": 47,
   "p1": "255",
   "pn": "260",
   "abstract": [
    "We have already developed a speech analysis method based on the Glottal-ARMAX (Auto Regressive and  Moving Average eXogenous) model, in which the speech  production model is supposed to be an ARMAX vocal tract model and two kinds of excitation: glottal source model excitation and white Gaussian. The speech analysis method based on the Glottal-ARMAX model can estimate the glottal source and ARMAX model parameters  simultaneously with pitch synchronous. In this paper, a subband processing with QMF filterbank or Haar filterbank is  introduced to the Glottal-ARMAX method in order to  reduce the computation. The introduction makes it possible to reduce computation since the orders of ARMAX identification  can be set smaller than that of fullband analysis. The introduction also enables to improve an estimation accuracy of the glottal source model parameters owing to the improved resolution in the frequency domain.\n",
    ""
   ]
  },
  "stylianou98_ssw": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Concatenative speech synthesis using a harmonic plus noise model",
   "original": "ssw3_261",
   "page_count": 6,
   "order": 48,
   "p1": "261",
   "pn": "266",
   "abstract": [
    "This paper describes the application of the Harmonic plus Noise Model, HNM, for concatenative Text-to-Speech (TTS) synthesis. In the context of HNM, speech signals are  represented as a time-varying harmonic component plus a  modulated noise component. The decomposition of speech signal in these two components allows for more natural-sounding modifications (e.g., source and filter modifications) of the signal. The parametric representation of speech using HNM provides a straightforward way of smoothing discontinuities of acoustic units around concatenation points. Formal  listening tests have shown that HNM provides high-quality speech synthesis while outperforming other models for  synthesis (e.g., TD-PSOLA) in intelligibility, naturalness and pleasantness.\n",
    ""
   ]
  },
  "stylianou98b_ssw": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Removing phase mismatches in concatenative speech synthesis",
   "original": "ssw3_267",
   "page_count": 6,
   "order": 49,
   "p1": "267",
   "pn": "272",
   "abstract": [
    "Concatenation of acoustic units is widely used in most of the currently available text-to-speech systems. While this approach leads to higher intelligibility and naturalness than synthesis-by-rule, it has to cope with the issues of  concatenating acoustic units that have been recorded in a different order. One important issue in concatenation is that of  synchronization of speech frames or, in other words, inter-frame coherence. This paper presents a novel method for synchronization  of signals with applications to speech synthesis. The method is based on the notion of center of gravity applied to speech signals. It is an o\u000b-line approach as this can be done during analysis with no computational burden on synthesis. The method has been tested with the Harmonic plus Noise Model, HNM, on many large speech databases. The resulting  synthetic speech is free of phase mismatch (inter-frame incoherence) problems.\n",
    ""
   ]
  },
  "tamura98_ssw": {
   "authors": [
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Speaker adaptation for HMM-based speech synthesis system using MLLR",
   "original": "ssw3_273",
   "page_count": 5,
   "order": 50,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "This paper describes a voice characteristics conversion technique for an HMM-based text-to-speech synthesis system. The system uses phoneme HMMs as the speech synthesis units, and voice characteristics conversion is achieved by changing HMM parameters appropriately. To transform the voice characteristics of synthetic speech to the target speaker, we apply anMLLR (Maximum Likelihood Linear Regression) technique, one of the speaker adaptation techniques, to the system. From the results of objective and subjective tests, it is shown that the characteristics of synthetic speech is close to target speakers voice, and the speech generated from the adapted model set using 5 sentences has almost the same DMOS score as that from the speaker dependent model set.\n",
    ""
   ]
  },
  "dalessandro98b_ssw": {
   "authors": [
    [
     "Christophe",
     "dAlessandro"
    ],
    [
     "Boris",
     "Doval"
    ]
   ],
   "title": "Experiments in voice quality modification of natural speech signals: the spectral approach",
   "original": "ssw3_277",
   "page_count": 7,
   "order": 51,
   "p1": "277",
   "pn": "282",
   "abstract": [
    "Voice quality is currently a key issue in speech synthesis research. The lack of realistic intra-speaker voice quality variation is an important source of concern for concatenation-based  synthesis methods. A challenging problem is to reproduce the voice quality changes that are occuring in natural speech when the vocal e\u000bort is varying. A new method for voice quality modification is presented. It takes advantage of a spectral theory for voice source signal  representation. An algorithm based on periodic-aperiodic  decomposition and spectral processing (using the short-term Fourier transform) is described. The use of adaptive  inverse filtering in this framework is also discussed.  Applications of this algorithm may include: pre-processing of speech corpora, modification of voice quality parameters together with intonation in synthesis, voice transformation. Some experiments are reported, showing convincing voice quality modifications for various speakers.\n",
    ""
   ]
  },
  "holzapfel98_ssw": {
   "authors": [
    [
     "Martin",
     "Holzapfel"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "A wavelet-domain PSOLA approach",
   "original": "ssw3_283",
   "page_count": 4,
   "order": 52,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "A basic problem in concatenative speech synthesis are discontinuities at the concatenation points. The units which are produced by different (independent) articulatory movements differ in their spectral characteristics even if their phonetic context is carefully chosen.\n",
    "This paper describes a wavelet transform of the spectrum of the speech concatenated within the PSOLA algorithm.\n",
    "This multiresolution analysis separates the following perceptive important spectral characteristica: the intrinsic pitch resulting in a fine-ripple of the spectrum, articulatory movements typically resulting in formant-structures and the global spectral tilt.\n",
    "In the wavelet domain each of this characteristica can be analysed and manipulated separately in a consistent and completely non paramtric way. Optimised concationation points can easly be located. Remaining spectral irregularities can be adjusted efficiently, resulting in clear and naturally sounding synthetic speech.\n",
    "Dyadic filter banks are a computational efficient implementation of the presented transform.\n",
    ""
   ]
  },
  "zhong98_ssw": {
   "authors": [
    [
     "Jialin",
     "Zhong"
    ],
    [
     "Joseph",
     "Olive"
    ]
   ],
   "title": "Cloning synthetic talking heads",
   "original": "ssw3_287",
   "page_count": 5,
   "order": 53,
   "p1": "287",
   "pn": "292",
   "abstract": [
    "The quality of Text-to-Visual-Speech synthesis is judged by how well it matches the visual perception of speech articulators with acoustic speech perception.  Concurrently, di\u000berent viewers often prefer di\u000berent head  models for subjective reasons. Traditional facial animation  approach tied the parameterization of animation directly to the model. Switching the head model is di\u000ecult because a lengthy training process is required. In this paper, we present a method that creates a new talking head from an existing one without repeating the training process. It is assumed in this work that the visible motion of speech  articulators can be described by a small set of feature points. By mapping the 3D trajectories of the feature points from the existing model to the new one, we can transfer the motion of articulators. A morphing algorithm is then used to animate a new talking head from these trajectories of feature points on the new model. The new talking head, though looking di\u000berent, preserves the perceptual quality of the original one.\n",
    ""
   ]
  },
  "santen98_ssw": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Description of the Bell Labs Intonation System",
   "original": "ssw3_293",
   "page_count": 6,
   "order": 54,
   "p1": "293",
   "pn": "298",
   "abstract": [
    "This paper describes the approach to intonation modeling currently used in the Bell Labs Multi-lingual Text-to-Speech System for English, French, German, Italian, Spanish, Russian, Romanian, and Japanese.\n",
    ""
   ]
  },
  "fujisaki98_ssw": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Changfu",
     "Wang"
    ]
   ],
   "title": "A command-response model for F<sub>0</sub> contour generation in multilingual speech synthesis",
   "original": "ssw3_299",
   "page_count": 6,
   "order": 55,
   "p1": "299",
   "pn": "304",
   "abstract": [
    "This paper describes the command-response model for F0 contour generation originally developed for the common Japanese, and demonstrates its capability of generating F0 contours of various other languages. It is shown that intonations of various languages are produced by language-specific patterns of input commands, while the response mechanisms are essentially identical in speakers of various languages, thus making the model especially useful for multilingual speech synthesis.\n",
    ""
   ]
  },
  "syrdal98_ssw": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Kurt",
     "Dusterhoff"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Three methods of intonation modeling",
   "original": "ssw3_305",
   "page_count": 6,
   "order": 56,
   "p1": "305",
   "pn": "310",
   "abstract": [
    "This paper compares di\u000berent methods of generating  intonation for an American English Text-to-Speech synthesis  system. We look at a primarily rule-based approach and two data-driven approaches.\n",
    "For data-driven modeling we used two separate data sets, each representing a somewhat di\u000berent prosodic style. One database was recordings of a portion of 1989 Wall Street Journal text from the Penn Treebank Project. The  second database was recordings of interactive prompts used in telephone network services. Both were read by the same female speaker. Approximately two and one-half hours of speech was phonetically and prosodically segmented and  labeled (first automatically, and subsequently verified manually).  The prosodic labeling used ToBI [1] tones and breaks. Three di\u000berent intonation models were compared: (1) a  predominantly rule-based model based on ToBI labels [3]; (2) a parametric model using the Tilt approach [2]; and (3) a Vector Quantized model based on an underlying parametric representation [4]. Sentences representative of both prosodic styles were synthesized with each of these models, and were presented to listeners for subjective ratings in a formal  listening test. The results of the evaluation are reported.\n",
    "s K. Silverman, M. Beckman, J. Pitrelli, M. Osten- dorf, C. Wightman, P. Price, J. Pierrehumbert, and J. Hirschberg. ToBI: a standard for labeling english prosody. ICSLP, 2:867-870, 1992. P. Taylor and A. Black. Synthesizing conversational in- tonation from a linguistically rich input. In Proc. ESCA Workshop on Speech Synthesis, pages 175-178, Mohonk, NY, 1994. Matthias Jilka. Regelbasierte Generierung naturlich klin- gender Intonationsmuster des Amerikanischen Englisch (Rule-based generation of naturally sounding intonation patterns of American English). University of Stuttgart, Institute of Natural Language Processing, University of Stuttgart, 1996. Gregor Möhler and Alistair Conkie. Parametric modeling of intonation using vector quantization. In Third Inter- national Workshop on Speech Synthesis, Jenolan Caves, Australia, 1998.\n",
    ""
   ]
  },
  "mohler98_ssw": {
   "authors": [
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Parametric modeling of intonation using vector quantization",
   "original": "ssw3_311",
   "page_count": 6,
   "order": 57,
   "p1": "311",
   "pn": "316",
   "abstract": [
    "In this study we propose a data-based approach to intonation modeling using vector quantization. The model is based on an F0 parametrization with an especially designed  approximation function. The parameter vectors found are vector quantized with varying codebook sizes. This method is  motivated by intonation theories that suggest that pitch accent and boundary phenomena can be described by a distinct number of di\u000berent types. We use classification trees to  predict the F0 movements represented in the codebook from a set of features. We assessed the quality of the model by numerical measures and perceptual testing. The tests show that our method performs well when compared with other methods of intonation modeling.\n",
    ""
   ]
  },
  "venditti98b_ssw": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Kazuaki",
     "Maeda"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Modeling Japanese boundary pitch movements for speech synthesis",
   "original": "ssw3_317",
   "page_count": 6,
   "order": 58,
   "p1": "317",
   "pn": "322",
   "abstract": [
    "This paper provides a detailed analysis of the intonational form and function of five boundary pitch movements (BPMs) in Tokyo Japanese. The perception study describes the linguistic and paralinguistic dimensions on which meanings of the BPMs are distinguished. The production study details how the F0 heights, durations, and (crucially) alignment of the F0 contour with the segments are all used to define the movement types. We suggest a quantitative model of F0 contour alignment which uses observed data to model the entire shape of F0 curves.\n",
    ""
   ]
  },
  "malfrere98_ssw": {
   "authors": [
    [
     "F.",
     "Malfrère"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Piet",
     "Mertens"
    ]
   ],
   "title": "Automatic prosody generation using suprasegmental unit selection",
   "original": "ssw3_323",
   "page_count": 6,
   "order": 59,
   "p1": "323",
   "pn": "328",
   "abstract": [
    "Text-to-Prosody systems based on the use of prosodic databases extracted from natural speech will be a key point for further development of new Text-to-Speech systems.\n",
    "This paper describes a system using such speech databases to generate the rhythm and the intonation of texts written in French. The system is based on a very crude chinks n chunks prosodic phrasing algorithm and on an automatic prosodic analysis of a natural speech database. The rhythm of the synthetic speech is generated with a CART tree trained on a large mono-speaker speech corpus. The acoustic aspect of the intonation is derived from a set of prosodic patterns automatically derived from the same speech corpus. At synthesis time, patterns are chosen on the fly from the database so as to minimize a total selection cost composed of pattern target costs and pattern concatenation costs.\n",
    ""
   ]
  },
  "santen98b_ssw": {
   "authors": [
    [
     "Jan P.H. van",
     "Santen"
    ],
    [
     "Louis C.W.",
     "Pols"
    ],
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Dan",
     "Kahn"
    ],
    [
     "Eric",
     "Keller"
    ],
    [
     "Julie",
     "Vonwiller"
    ]
   ],
   "title": "Report on the Third ESCA TTS Workshop evaluation procedure",
   "original": "ssw3_329",
   "page_count": 4,
   "order": 60,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "This paper provides a description and rationale for the Evaluation Procedure taking place at the Workshop. The procedure has three goals. First, setting a precedent of providing conference participants with a more candid and thorough picture of the quality of current TTS systems than is usually available in the form of prepared conference demonstrations. Second, providing results that will be informative for TTS systems developers. Third, stimulating a discussion and contributing to a consensus building process on text-to-speech synthesis evaluation.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Session A: Assessment and Standards",
   "papers": [
    "hirst98_ssw",
    "sonntag98_ssw",
    "dalessandro98_ssw",
    "campbell98_ssw",
    "mizuno98_ssw",
    "sproat98_ssw",
    "venditti98_ssw",
    "yang98_ssw",
    "febrer98_ssw",
    "trouvain98_ssw",
    "damper98_ssw",
    "mobius98_ssw",
    "damper98b_ssw",
    "kiraz98_ssw",
    "black98_ssw",
    "shih98_ssw",
    "rilliard98_ssw",
    "grover98_ssw",
    "sannier98_ssw",
    "shih98b_ssw",
    "dirksen98_ssw",
    "jokisch98_ssw",
    "cahn98_ssw",
    "boulademareuil98_ssw",
    "miller98_ssw",
    "keller98_ssw",
    "zellner98_ssw",
    "taylor98_ssw",
    "portele98_ssw",
    "carvalho98_ssw",
    "andersen98_ssw",
    "bunnell98_ssw",
    "campbell98b_ssw",
    "fujisawa98_ssw",
    "beutnagel98_ssw",
    "ding98_ssw",
    "macon98_ssw",
    "breen98_ssw",
    "torretoledano98_ssw",
    "sluijter98_ssw",
    "heid98_ssw",
    "kain98_ssw",
    "plumpe98_ssw",
    "silva98_ssw",
    "charonnat98_ssw",
    "badin98_ssw",
    "funaki98_ssw",
    "stylianou98_ssw",
    "stylianou98b_ssw",
    "tamura98_ssw",
    "dalessandro98b_ssw",
    "holzapfel98_ssw",
    "zhong98_ssw",
    "santen98_ssw",
    "fujisaki98_ssw",
    "syrdal98_ssw",
    "mohler98_ssw",
    "venditti98b_ssw",
    "malfrere98_ssw",
    "santen98b_ssw"
   ]
  }
 ]
}