{
 "location": "Kent Ridge, Singapore",
 "startDate": "13/12/2006",
 "endDate": "16/12/2006",
 "original_url": "http://www.isca-speech.org/archive/iscslp2006/iscslp2006_v2.html",
 "original_title": "Int'l Symp. on Chinese Spoken Language Proc. (ISCSLP 2002)",
 "logo": "top_right.jpg",
 "note": "Some of the papers presented were subsequently published as LNAI 4724 by Springer. These are not included here.",
 "conf": "ISCSLP",
 "year": "2006",
 "name": "iscslp_2006",
 "series": "ISCSLP",
 "SIG": "CSLP",
 "title": "International Symposium on Chinese Spoken Language Processing",
 "title1": "International Symposium on Chinese Spoken Language Processing",
 "date": "13-16 December 2006",
 "papers": {
  "yuan06_iscslp": {
   "authors": [
    [
     "Chu",
     "Yuan"
    ],
    [
     "Aijun",
     "Li"
    ]
   ],
   "title": " The Breath Segment in Expressive Speech",
   "original": "B1",
   "page_count": 12,
   "order": 1,
   "p1": "1",
   "pn": "12",
   "abstract": [
    "In this paper, we choose about one hour expressive speech and make a pilot study on how to use the breath segments to get more natural and expressive speech. We focus on exploring when the breath segments occur and how their acoustic features are affected by the speaker’s emotional states: valence and activation. The statistic analysis has been carried out to find the relationship between the length and intensity of the breath segments and the two state parameters. Finally, a perceptual experiment is done by employing the analysis results to the synthesized discourses, the results imply that breath segment insertion can help to improve the expressiveness and naturalness of the synthesized speech. Keywords: breath segment, expressive speech, emotion, valence, activation\n"
   ]
  },
  "zhang06_iscslp": {
   "authors": [
    [
     "Bufan",
     "Zhang"
    ],
    [
     "Zhenhua",
     "Ling"
    ],
    [
     "Long",
     "Qin"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " Applying SFC Model for Chinese Expressive Speech Synthesis",
   "original": "B14",
   "page_count": 8,
   "order": 2,
   "p1": "139",
   "pn": "146",
   "abstract": [
    "This paper presents an approach to model the pitch contour in Chinese expressive speech synthesis by using SFC (Superposition of Functional Contours) model. Some functional contours corresponding to the expressions are introduced when applying SFC for expressive speech. During implementation, both the emotion-dependent method and emotion-independent method are realized and compared. Three emotion types (neutral, happiness and sadness) and stress caused by narrow focus are studied in our experiments. The results show that the RMSE and correlation between predicted F0 and neutral one are satisfactory and the listening tests prove that the synthesized speech using proposed pitch model presents corresponding expressions as expected. Keywords: SFC model, Chinese expressive speech synthesis.\n"
   ]
  },
  "agrawal06_iscslp": {
   "authors": [
    [
     "S. S.",
     "Agrawal"
    ],
    [
     "K.",
     "Samudravijaya"
    ],
    [
     "Karunesh",
     "Arora"
    ]
   ],
   "title": " Recent Advances of Speech Databases Development Activity for Indian Languages",
   "original": "B75",
   "page_count": 6,
   "order": 3,
   "p1": "771",
   "pn": "776",
   "abstract": [
    "Development of Speech Corpora and acoustic–phonetic data bases are indispensable for any research and development work in spoken language systems. Systematic efforts have been made to create speech databases for some major languages of India. The paper attempts to present the status and the recent advancements made in corpora development for some of the Indian languages. Different types of databases developed include text corpora for speech, annotated/non-annotated speech corpora, acoustic-phonetic and labeled speech databases, special speech corpora etc. These have been developed for general purpose as well as task oriented applications. Databases of a few Indian languages have been developed in a well designed manner which includes adequate representation of textual / linguistic information, regional/dialectal variations, speaking styles and environments etc. These databases have been used for developing systems such as Text to Speech synthesis, Speech recognition, Speaker identification, speech secrecy, language translation and forensic applications etc. Keywords: Speech Corpora, Indian Languages\n"
   ]
  },
  "luong06_iscslp": {
   "authors": [
    [
     "Chi Mai",
     "Luong"
    ],
    [
     "Ngoc Duc",
     "Dang"
    ]
   ],
   "title": " Design of Vietnamese Speech Corpus and Current Status",
   "original": "B73",
   "page_count": 11,
   "order": 4,
   "p1": "748",
   "pn": "758",
   "abstract": [
    "This paper presents a current status and activities for spoken language resources for Vietnamese implemented in research institutions such as Institute of Information Technology, Vietnamese Academy of Science and Technology, and International Research Center MICA, Hanoi University of Technology. This is our first attempt of a process of building a large Vietnamese speech database and the corpora should be in a common design to make it available for researchers in Vietnamese speech processing. Keywords: Speech corpus, Design of speech corpus, General corpus, Specific corpus, Phonetic structure, Data collection, Recording, Labeling.\n"
   ]
  },
  "dawa06_iscslp": {
   "authors": [
    [
     "I.",
     "Dawa"
    ],
    [
     "",
     "Husal"
    ],
    [
     "Liu",
     "Yue"
    ],
    [
     "Yue Yao",
     "Ming"
    ],
    [
     "",
     "Uulang"
    ],
    [
     "Bai Shuang",
     "Cheng"
    ],
    [
     "",
     "Batsaihan"
    ],
    [
     "Y.",
     "Arai"
    ],
    [
     "M.",
     "Mitsunaga"
    ],
    [
     "H.",
     "Isahara"
    ],
    [
     "S.",
     "Nakamura"
    ]
   ],
   "title": " Multilingual Text - Speech Corpus of Mongolian",
   "original": "B74",
   "page_count": 12,
   "order": 5,
   "p1": "759",
   "pn": "770",
   "abstract": [
    "Abstract:In this paper, we reported a multilingual parallel electronic dictionary( called MPEDMCJKE) and a multiple speech corpus(called MDSCM) of Mongolian. MPEDMCJKE is paralleled the languages of Mongolian(including the versions of Cyrillic, traditional Mongolian and Mongolian Todo used in Mongolia, China and Russia, respectively), Chinese, Japanese, Korean and English. And It is done through the international cooperation of the National Institute of Information and Communications Technology of Japan(NICT), MENKsoft Co., Ltd. and the Mongolian Information Technology Institute of Social Science of Inner Mongolia(MIT), China, and the Korea Advanced Institute of Science and Technology of Korea(KAIST). MDSCM is a multi-dialectal speech corpus of Mongolian collected from different areas or countries, and is done supported by Shirai laboratory in waseda university and ATR of Japan during 1998-2006.  Keyword: Mongolian text-speech corpus, various versions and dialects, Multilingual dictionary.\n"
   ]
  },
  "patil06_iscslp": {
   "authors": [
    [
     "Hemant A.",
     "Patil"
    ],
    [
     "S.",
     "Ghosh"
    ],
    [
     "A.",
     "Si"
    ],
    [
     "T. K.",
     "Basu"
    ]
   ],
   "title": " Design of Cross-lingual and Multilingual Corpora for Speaker Recognition Research and Evaluation in Indian Languages",
   "original": "B72",
   "page_count": 12,
   "order": 6,
   "p1": "736",
   "pn": "747",
   "abstract": [
    "Automatic Speaker Recognition (ASR) is an economic method of biometrics because of the availability of the low cost and powerful processors. Results of ASR are highly dependent on database, i.e., the results obtained in an ASR system are meaningless if the recording conditions are not of standard. In this paper, a methodology and a typical experimental setup used for development of corpora for various ASR tasks, viz., mono-lingual, cross-lingual and multilingual speaker identification in the text-independent mode for different Indian languages, viz., Bengali, Hindi and Indian English have been described. Finally, an ASR system is presented to evaluate the developed corpora.\n"
   ]
  },
  "zu06_iscslp": {
   "authors": [
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Zhenhai",
     "Cao"
    ],
    [
     "Guilin",
     "Chen"
    ],
    [
     "Kesong",
     "Han"
    ],
    [
     "Peng",
     "Lu"
    ],
    [
     "Runqiang",
     "Yan"
    ],
    [
     "Kaizhi",
     "Wang"
    ],
    [
     "Zhenli",
     "Yu"
    ],
    [
     "Dongjian",
     "Yue"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Zhigang",
     "Yin"
    ]
   ],
   "title": " Multi-lingual TTS Speech Corpus Development",
   "original": "B71",
   "page_count": 12,
   "order": 7,
   "p1": "724",
   "pn": "735",
   "abstract": [
    "This paper presents approach of multi-lingual speech corpus design, data collection and phonetic annotation for text-to-speech (TTS) system development. Under a uniform data structure, more than 10 languages and dialects speech corpora are shared with language independent data management approaches and data processing procedures. A specifically defined super phonetic symbol set are used for all languages and related dialects. The defined data management methods enable Motorola multi-lingual TTS systems employs a uniform architecture for cost function-based unit selection strategy and speech synthesizer modules on both sever-based and embedded platforms. Keywords: Multi-lingual, TTS, speech corpus.\n"
   ]
  },
  "shao06_iscslp": {
   "authors": [
    [
     "Jian",
     "Shao"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Jiang",
     "Han"
    ],
    [
     "Jun",
     "Yang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Syllable Based Audio Search Using Confusion Network Arc as Indexing Unit",
   "original": "B61",
   "page_count": 12,
   "order": 8,
   "p1": "621",
   "pn": "632",
   "abstract": [
    "Compared to English, Chinese has a simpler and more restricted syllabic structure. In order to exploit the special characteristics of Chinese, syllable is selected as the unit for ASR lattice representation. For the sake of fast retrieval, syllable lattices are clustered into confusion network linear lattices, and then encoded into inverted index. To recover the posterior probabilities of pruned word hypotheses in confusion network, syllable confusion matrix is used to calculate relevance score of a given keyword. Experiments on the corpora for the keyword spotting task in the 2005 HTRDP ASR Evaluation show that the proposed approach not only yields a compact inverted index and supports quick keyword query, but also achieves an EER of 46.75%. Keywords: audio search, keyword spotting, spoken document retrieval, confusion network, syllable based confusion matrix, relevance score\n"
   ]
  },
  "banchs06_iscslp": {
   "authors": [
    [
     "Rafael E.",
     "Banchs"
    ],
    [
     "Josep M.",
     "Crego"
    ],
    [
     "Patrik",
     "Lambert"
    ],
    [
     "Jose B.",
     "Marino"
    ]
   ],
   "title": " A Feasibility Study for Chinese-Spanish Statistical Machine Translation",
   "original": "B67",
   "page_count": 12,
   "order": 9,
   "p1": "681",
   "pn": "692",
   "abstract": [
    "This article presents and describes an experimental prototype system for performing Chinese-to-Spanish and Spanish-to-Chinese machine translation. The system is based on the statistical machine translation (SMT) framework and, more speciﬁcally, it implements the bilingual n-gram SMT approach. Since, as far as we know, no large Chinese-Spanish parallel corpus is currently available for training purposes, an alternative experimental method for building a training corpus was used. This method is compared, in terms of translation quality, to the simpler approach of using English as a bridge language for performing Chinese-to-Spanish and Spanish-to-Chinese translations.\n"
   ]
  },
  "hu06_iscslp": {
   "authors": [
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": " Chinese Character-based Segmentation \\&amp; POS-tagging and Named Entity Identification with a CRF Chunker ",
   "original": "B68",
   "page_count": 10,
   "order": 10,
   "p1": "693",
   "pn": "702",
   "abstract": [
    "In this paper, we propose a character-based conditional random field (CRF) chunker to identify Chinese named entity words in the text files. The input for it is from a character-based tagger in which the segmentation and partof-speech (POS) tagging are conducted simultanueously. The character-based tagger is trained by using a corpus in which each character is tagged with both its position (POC) in a word and POS tag of the word. The chunker is trained by an IOB2 tagged corpus, in which each character is labelled with POC, POS and chunk tags (one of the B, I, O). 4 kinds of named entities, including personal names, location names, organization names, and other proper nouns, are assumed to be identification targets. In experiments using the People’s Daily corpus, we found the CRF chunker can obtain better results than the maximum entropy model and support vector machine model in the case of using similar features. We also confirmed that the bigram features for the CRF chunker is superior to the unigram features, and nearly 1% improvement in identification is obtained with the addition of POS information. Keywords: Chinese Segmentation & POS tagging, Named Entity Identification, Character-based Model, ME, CRF\n"
   ]
  },
  "liu06_iscslp": {
   "authors": [
    [
     "Chuanhan",
     "Liu"
    ],
    [
     "Yongcheng",
     "Wang"
    ],
    [
     "Fei",
     "Zheng"
    ],
    [
     "Derong",
     "Liu"
    ]
   ],
   "title": " Automatic Chinese Dialogue Text Summarization Based On LSA and Segmentation",
   "original": "B69",
   "page_count": 12,
   "order": 11,
   "p1": "703",
   "pn": "714",
   "abstract": [
    "Automatic Chinese text summarization for dialogue style is a relatively new research area. In this paper, Latent Semantic Analysis (LSA) is first used to extract semantic knowledge from a given document, all question paragraphs are identified, an approach of automatic text segmentation analogous to TextTiling is exploited to improve the precision of correlating question paragraphs and answer paragraphs, and finally some 'important' sentences are extracted from the generic content and the question-answer pairs to generate a complete summary. Experimental results show that our approach is high efficient and improves significantly the coherency of the summary while not compromising informativeness. Keywords: Automatic text summarization, Latent semantic analysis, Text segmentation, Dialogue style, Coherency, Question-answer pairs.\n"
   ]
  },
  "zhang06b_iscslp": {
   "authors": [
    [
     "Sheng",
     "Zhang"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Fanrang",
     "Kong"
    ]
   ],
   "title": " Acoustic Analysis of Emotional Speech in Mandarin Chinese",
   "original": "B6",
   "page_count": 10,
   "order": 12,
   "p1": "57",
   "pn": "66",
   "abstract": [
    "In this paper, the vocal expressions of human emotions of anger, fear, joy and sadness are acoustically analyzed in relation to neutral speech. The features under investigation include duration, short-time amplitude, as well as pitch at both word and sentence level. The study is specially focused on word stress in Mandarin speech signals with emotion arousal. Based on the analytical results, an overall quantitative measure of the distinctive characteristics of emotional speech will be presented. Keywords: Mandarin Emotional Speech, Acoustic Analysis, Word Stress\n"
   ]
  },
  "cui06_iscslp": {
   "authors": [
    [
     "Dandan",
     "Cui"
    ],
    [
     "Lianhong",
     "Cai"
    ],
    [
     "Yongxin",
     "Wang"
    ],
    [
     "Xiaozhou",
     "Zhang"
    ]
   ],
   "title": " Investigation on Pleasure Related Acoustic Features of Affective Speech",
   "original": "B7",
   "page_count": 12,
   "order": 13,
   "p1": "67",
   "pn": "78",
   "abstract": [
    "This paper presents our recent work on the investigation of affective speech along the pleasure-displeasure (P) dimension in the PAD emotional space. First, 76 conventional features are taken as candidates. And a novel feature, F0 Dominant Ratio, which is designed to reflect the dominant pitch, is also introduced. Correlative coefficients are calculated and a preliminary selection is done. Factor analysis and co-clustering are then employed. After another two rounds of selection, a set of 5 correlative features is selected. It includes Spectral Roll off, Spectral Low-high Ratio, Average F0, Min. F0, and the F0 Dominant Ratio, which can illustrate the acoustic differences between arousal-equaled emotion categories. Then a conversion experiment is carried out. Our model is built through stepwise linear regressions, and modifications are implemented carefully. Perceptual test shows that the conversion result of pleasure intensity is acceptable. Among the 5 features, the spectral ones show the highest favor. The dominant pitch appears relative, yet it requires further investigation. Meanwhile, intensity of arousal (A) in these sentences is agreed to be unchanged, i.e. the pleasure-oriented features are separated nicely from the arousal-oriented ones.\n"
   ]
  },
  "zou06_iscslp": {
   "authors": [
    [
     "Yu",
     "Zou"
    ],
    [
     "Xiaohua",
     "Li"
    ],
    [
     "Min",
     "Hou"
    ],
    [
     "",
     "Anna"
    ]
   ],
   "title": " Comparison of News Announcing and Talking Styles in Broadcast Speech",
   "original": "B8",
   "page_count": 10,
   "order": 14,
   "p1": "79",
   "pn": "88",
   "abstract": [
    "In broadcast speech, there are different speaking styles among different TV or radio programs. It is known that news announcing is a very distinctive style; it's quite distinct from others, e.g. talking style. What is the difference between news announcing style and talking style? In this study, we reports our studies on comparison of two speaking styles based on broadcast speech corpus, which the segmentation and labeling of the broadcast speech have been done. The purpose is to give some results from two aspects. First, in news announcing and talking styles, the mean duration of syllable in news announcing is longer than that in talking style; Second, the pitch and pitch range of news announcing style is high and has big fluctuation; that of talking style is high and has small fluctuation. Keywords: Broadcast speech; speaking styles; news announcing; talking style\n"
   ]
  },
  "guan06_iscslp": {
   "authors": [
    [
     "Yong",
     "Guan"
    ],
    [
     "Peng",
     "Li"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": " Multi-Pitch Detection for Co-Channel Speech Utilizing Frequency Channel Piecewise Integration and Morphological Feedback Verification Tracking",
   "original": "B9",
   "page_count": 9,
   "order": 15,
   "p1": "89",
   "pn": "97",
   "abstract": [
    "We propose an eﬀective algorithm to detect the multi-pitch of co-channel speech with two speakers. A discriminative scheme is exploited to gather harmonic information according to the piecewise continuity of autocorrelation peaks across diﬀerent frequency channels within a time frame. A group of adaptive morphological ﬁlters and feedback veriﬁcation are employed to coordinate and verify the periodicity information across time frames. Quantitative evaluation shows that the proposed system gains excellent results in veracity and integrality comparing with existing good algorithms of multi-pitch detection for co-channel speech. In addition, the proposed algorithm performs eﬃciently in timeconsuming as well as memory-consuming. Key words: Multi-Pitch Tracking, CASA, Piecewise Continuity of Harmonic Information, Morphological Filtering\n"
   ]
  },
  "choi06_iscslp": {
   "authors": [
    [
     "Mu-Yeol",
     "Choi"
    ],
    [
     "Seul-Han",
     "Park"
    ],
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": " A New Approach for Speech/Music Discrimination Based on Cepstral Distance",
   "original": "B10",
   "page_count": 9,
   "order": 16,
   "p1": "98",
   "pn": "106",
   "abstract": [
    "Discrimination between speech and music is important in many multimedia applications. In this paper, focusing on the spectral change characteristics of speech and music, we propose a new method based on cepstral distances to improve performance of speech/music classification. Instead of using cepstral distances between the frames with fixed interval, we employ the minimum of cepstral distances among close frames. In addition, we exclude short pause blocks from computing cepstral distances, to prevent the short pause segments in speech from being misclassified into music due to their small cepstral distances. The experimental results show that the proposed parameter outperforms other parameters. In comparison with conventional cepstral distances, taking the minimum of cepstral distances yields the error rate reduction of 60%. Also we achieve 20% additional error rate reduction by excluding short pause segments from audio signal.\n"
   ]
  },
  "liu06b_iscslp": {
   "authors": [
    [
     "Tantan",
     "Liu"
    ],
    [
     "Xiaoxing",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Speaker Diarization System Based on GMM and BIC",
   "original": "B11",
   "page_count": 7,
   "order": 17,
   "p1": "107",
   "pn": "113",
   "abstract": [
    "This paper presents an approach for speaker diarization based on a novel combination of Gaussian mixture model (GMM) and standard Bayesian information criterion (BIC). Gaussian mixture model provides a good description of feature vector distribution and BIC enables a proper merging and stopping criterion. Our system combines the advantage of these two method and yields favorable performance. Experiments carried out on mandarin broadcast news data demonstrate the advantage of the proposed approach, which shows better performance than the approach only based on GMM clustering. Keywords: speaker diarization, clustering, GMM, BIC.\n"
   ]
  },
  "qin06_iscslp": {
   "authors": [
    [
     "Shenghao",
     "Qin"
    ],
    [
     "Sha",
     "Meng"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": " A Robust Acoustic Echo Canceller for Noisy Environment",
   "original": "B25",
   "page_count": 10,
   "order": 18,
   "p1": "248",
   "pn": "257",
   "abstract": [
    "A new acoustic echo cancellation method for noisy environment is proposed and a real-time system is built. Different from the conventional acoustic echo cancellers, the proposed has a novel structure and can remove the echo effectively at low SNR. In our system, the speech with stationary noise is processed in order to separate the echo from the noise before updating the adaptive filter which is used for echo cancellation. Robust speech detector is another crucial component in the system. A module called noise generator is used for the comfort and continuity of the speech in real applications. With efficient organization and combination, the system gets excellent performance and is evaluated by statistical experiments\n"
   ]
  },
  "zhang06c_iscslp": {
   "authors": [
    [
     "Jing",
     "Zhang"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": " Short-Time ICA for Blind Separation of Noisy Speech",
   "original": "B26",
   "page_count": 10,
   "order": 19,
   "p1": "258",
   "pn": "267",
   "abstract": [
    "Noisy ICA model offers a viable solution for blind speech signal separation under real life scenarios, but it also creates new problem which need to be tackled, namely: estimating of the de-mixing matrix with non-invertible model. In this paper, a new algorithm based on short-time ICA is proposed for accurate estimation of the de-mixing matrix in the presence of noise. Without any assumption or prior knowledge of the noise covariance, the derivation of the proposed algorithm solely depends on the fact that the distribution shape remains pretty much unchanged although the estimated optima of the de-mixing matrix drift with noise contamination. In this work, we have set up a criterion on the distribution decision and signal-frame-selection for more accurate estimation. Experiment on Mandarin noisy speech showed quite satisfactory performance of matrix-estimation, thereby allowing better blind speech signal separation under noisy environment. Keywords: Blind speech signal separation, noisy ICA model, short-time local optima distribution, signal-frame-selection\n"
   ]
  },
  "wang06_iscslp": {
   "authors": [
    [
     "Jing",
     "Wang"
    ],
    [
     "Jingming",
     "Kuang"
    ],
    [
     "Shenghui",
     "Zhao"
    ]
   ],
   "title": " A Closed-loop Multimode Variable Bit Rate Characteristic Waveform Interpolation Coder",
   "original": "B23",
   "page_count": 11,
   "order": 20,
   "p1": "226",
   "pn": "236",
   "abstract": [
    "A variable bit rate characteristic waveform interpolation (VBR-CWI) speech codec with about 1.86kbps average bit rate which combines closed-loop multimode techniques is presented in this paper. Each kind of characteristic waveform (CW) surface is regarded as only rapidly evolving waveforms (REWs), only slowly evolving waveforms (SEWs) or mixed REWs plus SEWs in different cases of CWs evolving performance. A cost criterion based on weighted signal-to-noise (WSNR) value in the spectral domain is used to make the mode selection. Experiments show that the proposed closed-loop multimode VBR-CWI coder has reduced the average bit rate markedly and improved the synthesis speech quality to some extent compared to the original fixed bit rate coder. Further research can be done in order to have a more accurate perceptual objective quality measurement instead of WSNR and there is also need to pay attention to computational complexity of closed-loop method in real-time applications. Keywords: Closed-loop multimode; Variable bit rate; Cost criterion; Waveform interpolation.\n"
   ]
  },
  "qi06_iscslp": {
   "authors": [
    [
     "Fengyan",
     "Qi"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": " A Low-complexity Improved WI Speech Coding at 2kbps",
   "original": "B24",
   "page_count": 11,
   "order": 21,
   "p1": "237",
   "pn": "247",
   "abstract": [
    "The waveform interpolation (WI) speech coding presents a good performance at low bit rate. However, the algorithm has a very high complexity in computation. In this paper, a low-complexity improved waveform interpolation speech coder at 2kbps is proposed. The improved coding scheme has greatly reduced the computational complexity and improved the reconstructed speech quality by using various techniques, including FFT, cubic B-spline interpolation, nonlinear frequency resolution and so on. The algorithm complexity of the characteristic waveform (CW) representation and alignment is reduced from 43.3 to 9.2 MOPS. Furthermore, subjective listening tests show that the 2kb/s coder provides the high quality reconstructed speech. Keywords: speech coding; waveform interpolation; CW quantization.\n"
   ]
  },
  "tian06_iscslp": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Imre",
     "Kiss"
    ]
   ],
   "title": " Modular Text-to-Speech Synthesis Evaluation for Mandarin Chinese",
   "original": "B12",
   "page_count": 13,
   "order": 22,
   "p1": "114",
   "pn": "126",
   "abstract": [
    "Proper evaluation can efficiently drive the development of text-tospeech (TTS) systems. The assessment is needed to determine how well a system or technique compares to others or how it compares with the previous version of the system. In order to obtain more useful feedback for the development, we do not only evaluate the whole system but also each module of the TTS system separately. Based on the evaluation results, new progress can be achieved on individual modules. Furthermore, the evaluation allows identifying the best techniques in the different modules and comparing different modules for which we have defined a common evaluation specification. To enable the evaluation, the Mandarin TTS system is separated into three modules: text, prosody and acoustic processing modules. Furthermore, common interfaces have been designed for the communication between the TTS modules. The objective and subjective metrics have been proposed for the module-wise evaluation. This paper also presents the evaluation results that our Mandarin TTS modules achieved in the evaluation carried out in the European project on technology and corpora for speech-to-speech translation (TC-STAR). Keywords: Text-to-speech, speech synthesis, SSML, prosodic modeling, evaluation\n"
   ]
  },
  "li06_iscslp": {
   "authors": [
    [
     "Xiaocui",
     "Li"
    ],
    [
     "Heng",
     "Kang"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": " State-Correlated Duration Model for HMM-Based Speech Synthesis System ",
   "original": "B13",
   "page_count": 12,
   "order": 23,
   "p1": "127",
   "pn": "138",
   "abstract": [
    "This paper proposes a State-Correlated Duration model for HMM-based speech synthesis system. It uses an improved forward-backward algorithm to estimate the state-duration transition probability between the neighboring states. In the synthesis part, we determine the state duration taking account of the state-duration transition probability. Experiment results show that the speech we synthesized using the new duration model has a higher quality. Keywords: State-Correlated Duration model, improved forward-backward algorithm, state duration, state-duration transition probability\n"
   ]
  },
  "li06b_iscslp": {
   "authors": [
    [
     "Jian",
     "Li"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Jie",
     "Hao"
    ],
    [
     "Lifu",
     "Yi"
    ]
   ],
   "title": " A Diphone Sharing Method Towards Scalable Unit-training-based TTS",
   "original": "B15",
   "page_count": 12,
   "order": 24,
   "p1": "147",
   "pn": "158",
   "abstract": [
    "One of the most popular applications of Text to Speech (TTS) is in embedded devices. The resource limitation of embedded device requires the footprint of TTS system to be very small. Toshiba TTS for embedded device is a unit-training-based system and uses diphone as basic unit. The trained diphone inventory occupies a large part of the footprint. This paper proposes a diphone sharing method to reduce the size of trained diphone inventory. We use the phonetic knowledge of Mandarin to cluster the vowels so that the vowels in the same cluster can be shared. We also propose a method to automatically judge whether a shared diphone is good or not and only those judged as good are used in the inventory. By the diphone-sharing method, the size of trained diphone inventory is reduced by 40% while the subjective evaluation shows that the performance keeps almost the same with that of unshared diphone inventory. Keywords: TTS, embedded system, diphone sharing, unit-training-based TTS\n"
   ]
  },
  "yi06_iscslp": {
   "authors": [
    [
     "Lifu",
     "Yi"
    ],
    [
     "Jian",
     "Li"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": " A Unified Totally-Data-Driven Framework for Duration and Intonation Modeling",
   "original": "B16",
   "page_count": 11,
   "order": 25,
   "p1": "159",
   "pn": "169",
   "abstract": [
    "This paper proposes a unified framework for duration and intonation modeling in Mandarin TTS. In this framework, we design a novel parametric representation of mandarin intonation based on orthogonal polynomial approximation. By this representation, we can decompose F0 vector into 3 orthogonal polynomial parameters that are continuous scalars. Based on this vector-to-scalar decomposition, we can predict both duration and F0 representation parameters from linguistic and phonetic attributes by generalized linear models (GLM) in a unified manner. The model coefficients in GLM can be trained in a data-driven manner. Furthermore, the model structure, i.e., the significant attributes or attribute interactions in GLM, can be automatically optimized in a data-driven manner as well, rather than intuitively decided. So the proposed framework is totally-data-driven. In objective evaluation experiments, the new approach shows comparable or higher prediction performance compared with the other excellent approaches. Informal subjective perceptual experiments show that the predicted duration and intonation are quite appropriate and natural. Keywords: duration modeling, intonation modeling, F0 contour parametric representation, generalized linear model, speech synthesis, stepwise regression.\n"
   ]
  },
  "yu06_iscslp": {
   "authors": [
    [
     "Jian",
     "Yu"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Xia",
     "Wang"
    ]
   ],
   "title": " Pitch Prediction for Mandarin TTS with Mutual Prosodic Constraint",
   "original": "B17",
   "page_count": 11,
   "order": 26,
   "p1": "170",
   "pn": "180",
   "abstract": [
    "Most of current pitch prediction methods for mandarin TTS try to get pitch contours from the contextual information with a group of weights assigning. Without a good method in prosody concatenation constraint, the predicted pitch contours are not always stable because of the incomplete accordance between prosody information and text information. The paper presents a new mandarin pitch prediction method with mutual prosodic constraint between syllables. The idea of this mutual constraint is first inspired by lots of observations on corpus, but then it has been strictly proved with performance comparison and feature contribution analysis of CART-Based prosodic parameter prediction. Based on this, a reasonable definition of prosody concatenation cost is presented to measure the naturalness of pitch contours between two adjacent syllables. By minimizing this cost, the model can generate fluent pitch contours, which has been proved to be able to make the TTS system more natural than traditional systems. Keywords: Speech synthesis, TTS, Mandarin, prosody model, pitch generation, mutual constraint.\n"
   ]
  },
  "guo06_iscslp": {
   "authors": [
    [
     "Qing",
     "Guo"
    ],
    [
     "Endong",
     "Xun"
    ],
    [
     "Nobuyuki",
     "Katae"
    ]
   ],
   "title": " Prosodic Word Grouping in Mandarin TTS System",
   "original": "B18",
   "page_count": 10,
   "order": 27,
   "p1": "181",
   "pn": "190",
   "abstract": [
    "This paper reports the methodology and results of prosodic word grouping for a Mandarin TTS system developed by the Fujitsu Laboratories. In view of any inner prosodic word break will make speech unintelligible or unnatural, a new prosodic word grouping framework is proposed. The word segmentation result can be regarded as an initial prosodic word sequence with grids inserted into each word boundary. The target of prosodic word grouping is to remove those grids that are assessed needlessly in the prosodic word level. Several prosodic word grouping methods can work together because each makes its own decision. As one of three methods used in our TTS system, a binary prosodic tree method is also proposed in this paper. Lastly, experiment results and discussion are presented. Keywords: prosodic word, prosodic word grouping, binary prosodic tree.\n"
   ]
  },
  "gu06_iscslp": {
   "authors": [
    [
     "Hung-Yan",
     "Gu"
    ],
    [
     "Yan-Zuo",
     "Zhou"
    ],
    [
     "Huang-Liang",
     "Liau"
    ]
   ],
   "title": " An Initial System for Integrated Synthesis of Mandarin, Min-nan, and Hakka Speech",
   "original": "B19",
   "page_count": 11,
   "order": 28,
   "p1": "191",
   "pn": "201",
   "abstract": [
    "In this study, an integrated speech synthesis system is initially built to synthesize Mandarin, Min-nan, and Hakka speeches. By integration, only a model trained with Min-nan sentences is used to generate pitch-contours for the three languages, same rules are used to generate syllable duration and amplitude values, and a same program module implementing the method, TIPW, is used to synthesize the three languages’ speech waveforms. Also, each syllable of a language has just one recorded signal waveform, i.e. no chance of unit selection. Under such a restricted situation, the synthetic speech signals still have a noticeable level of naturalness and signal clarity. Keywords: cross-lingual speech synthesis, pitch contour, TIPW.\n"
   ]
  },
  "xu06_iscslp": {
   "authors": [
    [
     "Jun",
     "Xu"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": " Spectral Continuity Measures at Mandarin Syllable Boundaries",
   "original": "B20",
   "page_count": 8,
   "order": 29,
   "p1": "202",
   "pn": "209",
   "abstract": [
    "In Text-to-Speech (TTS) systems based on concatenative synthesis, the naturalness of synthetic speech is highly aﬀected by the spectral continuities at the concatenation point. In this paper, we focused on 4 kinds of syllable boundaries in mandarin and used several spectral distance measures combined with time derivatives distance measures to predict their audible discontinuities. A perceptual test is also proposed to evaluate the eﬃciencies of these measures.\n"
   ]
  },
  "lee06_iscslp": {
   "authors": [
    [
     "Ching-Hsien",
     "Lee"
    ],
    [
     "Ren-Jr",
     "Wang"
    ],
    [
     "Chung-Jen",
     "Chiu"
    ]
   ],
   "title": " A Multi-stage Method for Text-To-Pronunciation Conversion",
   "original": "B21",
   "page_count": 8,
   "order": 30,
   "p1": "210",
   "pn": "217",
   "abstract": [
    "Text-to-Pronunciation conversion is often used for speech synthesis and speech recognition-related systems. In this paper we present a data-driven, language-independent and multi-stage model for Text-to-Pronunciation conversion. With a Grapheme/Phoneme pair well aligned dictionary for training and utilizing a re-scoring strategy for those graphemes likely to be tagged erroneously, our model can not only increase the efficiency but also achieve a high accuracy than other data-driven approaches that have been applied to the same tasks. Keywords: Text-To-Pronunciation, Grapheme, Phoneme.\n"
   ]
  },
  "yuan06b_iscslp": {
   "authors": [
    [
     "Xiaoliang",
     "Yuan"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Dezhi",
     "Huang"
    ],
    [
     "Jun",
     "Guo"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": " Decision Tree Classification Approach for Model Selection in Segmenting Mandarin TTS Corpus",
   "original": "B22",
   "page_count": 8,
   "order": 31,
   "p1": "218",
   "pn": "225",
   "abstract": [
    "High accuracy automatic segmentation of Mandarin TTS (text to speech) corpus is vital for obtaining high quality syllable’s boundary to corpusbased speech synthesis. Among the existing methods, most studies on automatic segmentation are based upon single model, ignoring the diverse time marks gained by different models in specific Mandarin boundary environment. In this paper, three hidden Markov models (HMM) models initial-final monophonebased HMM, semi-syllable monophone-based HMM and initial-final triphonebased HMM are adopted respectively. Making use of decision tree algorithm C4.5, a classification approach is proposed to combine the advantages of the three models together by means of selecting the most appropriate one for each boundary unit. The experimental results show that the proposed method can achieve better performance than the single model method, in terms of error rate and time shift of boundaries. Keywords: automatic segmentation, Mandarin TTS, decision tree\n"
   ]
  },
  "hoshino06_iscslp": {
   "authors": [
    [
     "Akemi",
     "Hoshino"
    ],
    [
     "Akio",
     "Yasuda"
    ]
   ],
   "title": " Evaluation of Aspiration Sounds of Chinese Labial and Alveolar Diphthong Uttered by Japanese Students Using Voice Onset Time and Breathing Power",
   "original": "B2",
   "page_count": 12,
   "order": 32,
   "p1": "13",
   "pn": "24",
   "abstract": [
    "A Chinese aspiration is generally considered to be very difficult for Japanese students to reproduce. We measured the voice on set time (VOT) and mean breathing power during VOT, as evaluation parameters, of diphthong sounds of the labial of pai[p'ai], pao[p'ao], pei[p'ei], and pie[p'iε] and diphthong sounds of the alveolar of tai[t'ai], tao[t'ao], tie[t'iε], and tou[t'ou] in Chinese aspirations uttered by nine Japanese students and nine native Chinese speakers. The sounds were evaluated by the listening test of eight native Chinese speakers. We found that the VOT was not the sole measure for evaluating the pronunciations; the mean breathing power during VOT is also a useful measure for evaluation. Keywords: Chinese Aspiration, Pronunciation, VOT, Breathing Power\n"
   ]
  },
  "li06c_iscslp": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Ziyu",
     "Xiong"
    ],
    [
     "Xia",
     "Wang"
    ]
   ],
   "title": " Contrastive Study on Tonal Patterns Between Accented and Standard Chinese",
   "original": "B3",
   "page_count": 13,
   "order": 33,
   "p1": "25",
   "pn": "37",
   "abstract": [
    "This paper depicts a contrastive study on tonal patterns of monosyllables and disyllabic words between Standard Chinese (SC) and three regional accented Chinese(RAC): Shanghai, Xiamen and Taiwan. It is found that for mid level accent speakers, tonal contours of the third tone syllable uttered in isolation or at the final position of a prosodic constituent are always different for RAC from those of SC, displaying a less concave contour or even no concave at all. Moreover, even if the third tone contours are similar in isolation, they still may be different in final positions of bi-syllabic words between RAC and SC. The tonal register and range are compared among SC, RAC and dialects, showing that the RAC tends to keep the same tonal register and range as in dialects, and tonal range is harder to change. A perceptual experiment was carried out to find the third tone perceptual patterns for 3 accented Chinese. All the results imply that third tonal production is a difficult task for some dialectal speakers. Tonal acquisition of the citation form can not completely solve all tonal acquisition problems; tonal combination pattern still needs acquisition. The tonal register and range may be harder to be acquired than tonal contours. Keywords: tonal patterns, contrastive study, accented Chinese, tone production, tone acquisition\n"
   ]
  },
  "sittiprapaporn06_iscslp": {
   "authors": [
    [
     "Wichian",
     "Sittiprapaporn"
    ],
    [
     "Usanee",
     "Sotthiwat"
    ],
    [
     "Chittin",
     "Chindaduangratn"
    ],
    [
     "Naiphinich",
     "Kotchabhakdi"
    ]
   ],
   "title": " Mismatch Negativity Elicited by Non-cluster and Cluster Consonants Changes in Thai Words in Humans",
   "original": "B4",
   "page_count": 11,
   "order": 34,
   "p1": "38",
   "pn": "48",
   "abstract": [
    "Mismatch negativity (MMN) was used to investigate the processing of cluster and noncluster initial consonants in consonant-vowel syllables in the human brain. The MMN was elicited by either syllable with cluster or noncluster initial consonant, phonetic contrasts being identical in both syllables. Compared to the noncluster consonant, the cluster consonant elicited a more prominent MMN. The MMN to the cluster consonant occurred later than that of the noncluster consonant. The topography of the mismatch responses showed clear left-hemispheric laterality in both syllables. However, the syllable with an initial noncluster consonant stimulus produced MMN maximum over the middle temporal gyrus, whereas maximum of the MMN activated by the syllable with initial cluster consonant was observed over the superior temporal gyrus. We suggest that the MMN component in consonant-vowel syllables is more sensitive to cluster compared to noncluster initial consonants. Spatial and temporal features of the cluster consonant indicate delayed activation of leftlateralized perisylvian cell assemblies that function as cortical memory traces of cluster initial consonant in consonant-vowel syllables. Keywords: EEG; Speech perception; Auditory cortex; MMN; Cluster consonants\n"
   ]
  },
  "ding06_iscslp": {
   "authors": [
    [
     "Hongwei",
     "Ding"
    ],
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Ruiger",
     "Hoffmann"
    ]
   ],
   "title": " F0 Analysis of Chinese Accented German Speech",
   "original": "B5",
   "page_count": 8,
   "order": 35,
   "p1": "49",
   "pn": "56",
   "abstract": [
    "This paper aims to examine the contribution of pitch and pitch range to the perception of foreign accent produced by Chinese L2 speakers of German. A perceptual assessment of 8 Chinese speakers was conducted by a panel of 10 German native speakers. Phonetic acoustic analysis was carried out for these 8 Chinese speakers and 7 German speakers for comparison. The results show that the Chinese speakers produced higher average pitch, larger pitch range than the German speakers both on phoneme and utterance level. It was further discovered, that the pitch range at the phoneme level was highly correlated with the perceptual assessment, which can contribute to the overall perception of foreign accent. Keywords: pitch, pitch range, foreign accent, Chinese, German\n"
   ]
  },
  "dong06_iscslp": {
   "authors": [
    [
     "Bin",
     "Dong"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Automatic Scoring of Flat Tongue and Raised Tongue in Computer-assisted Mandarin Learning",
   "original": "B57",
   "page_count": 12,
   "order": 36,
   "p1": "580",
   "pn": "591",
   "abstract": [
    "In this paper, a paradigm for the automatic scoring of ﬂat tongue and raised tongue in computer-assisted Mandarin learning is presented. In this paradigm, distinctive features of Mandarin is used to distinguish the two classes consonants and score them. The confusion of them mainly results from diﬀerent articulation places. The method based on the distributing of concentrated frequency area is proposed for scoring these consonants. In this method, recursive calculation of searching maximum energy value with sliding window is used to remove the high energy area of low frequency and the ratio of average energy is used as feature to evaluate ﬂat-tongues and raised-tongues. With the method, the correct rate of scoring for the two classes consonants can reach 98.35%.\n"
   ]
  },
  "pan06_iscslp": {
   "authors": [
    [
     "Fuping",
     "Pan"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Improvements in Tone Pronunciation Scoring for Strongly Accented Mandarin Speech",
   "original": "B58",
   "page_count": 11,
   "order": 37,
   "p1": "592",
   "pn": "602",
   "abstract": [
    "This paper discusses a tone pronunciation scoring system of Mandarin. It recognizes tones of syllables by using GMM model and uses the recognition results for tone assessment. Initially, experiment results are bad on strongly accented speech. There are two reasons: one is that the inaccurate force-alignment leads to incomplete F0 contours; the other is due to the special pattern of F0 contours. We propose several measures to the problems. The first is to make the extraction of F0 contour independent of the force-alignment. The second is to base the scoring on GMM posterior probabilities. The third is to use the same accented speech to train the GMM model. And the last is to train the fractionized bi-tone GMM models to cover tone changes in the multiplecharacter words. After these measures are taken, the tone scoring correct rate is improved from 60.2% to 83.3%. Keywords: CALL, tone assessment, GMM, tone recognition, HMM, forcealignment, F0\n"
   ]
  },
  "liu06c_iscslp": {
   "authors": [
    [
     "Qingsheng",
     "Liu"
    ],
    [
     "Si",
     "Wei"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " The Application of Phone Weight in Putonghua Pronunciation Quality Assessment",
   "original": "B59",
   "page_count": 6,
   "order": 38,
   "p1": "603",
   "pn": "608",
   "abstract": [
    "This paper proposes a new phone-based Putonghua pronunciation quality assessment algorithm. This work is part of a project aimed at developing an automatic system that can assess the quality of Chinese Putonghua pronunciation. We bring up a new algorithm based on classical log-posterior probability algorithm by weighting each phone of Putonghua to calculate machine score. Experiments show that the new method improves the correlation between machine and human scores, which went up from 0.666 to 0.707. Keywords: speech recognition; pronunciation quality assessment; log-posterior probability; phone weigh\n"
   ]
  },
  "hu06b_iscslp": {
   "authors": [
    [
     "Guoping",
     "Hu"
    ],
    [
     "Dan",
     "Liu"
    ],
    [
     "Qingfeng",
     "Liu"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " SpeechQoogle: An Open-Domain Question Answering System with Speech Interface",
   "original": "B60",
   "page_count": 12,
   "order": 39,
   "p1": "609",
   "pn": "620",
   "abstract": [
    "In this paper, we propose a new and valuable research task: opendomain question answering system with speech interface, and first prototype (SpeechQoogle) is constructed with three separated modules: speech recognition, question answering (QA) and speech synthesis. Speech interface improves the utility of QA system, but also brings several new challenges, including 1) distorting effect of speech recognition error; 2) just one answer could be returned; and 3) the returned answer should be understandable just in speech. To conquer these challenges in SpeechQoogle’s construction, we first carefully choose FAQ-based question answering technique for QA module because of its inherent advances and 600,000 QA pairs are collected to support this technique. Then corresponding acoustic model and language model are particularly developed for speech recognition module which promotes the character ACC to 87.17%. Finally, in open-set testing the integrated prototype successfully answers 56.25% spoken questions, which is a quite satisfied and inspiring performance because many potential improving approaches are still unexploited. Keywords: Question Answering, Speech Interface, Speech Recognition, Speech Synthesis, Information Extraction\n"
   ]
  },
  "zhang06d_iscslp": {
   "authors": [
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Hongchen",
     "Jiang"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": " Research and Analysis of Fast Training in SVM-based Audio Classification",
   "original": "B62",
   "page_count": 9,
   "order": 40,
   "p1": "633",
   "pn": "641",
   "abstract": [
    "In this paper, we propose a new method to choose the effective samples for support vector machines (SVM) training in audio classiﬁcation task. The objective is to reduce the training time of SVM by choosing eﬀective examples from the training set of binary classes. We test the performances of our new method on a dataset composed of about 6-hour audio data which illustrate that the computation time can be signiﬁcantly reduced without a signiﬁcant decrease in the prediction accuracy.\n"
   ]
  },
  "li06d_iscslp": {
   "authors": [
    [
     "Ming",
     "Li"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " An Efficient and Robust Approach to Audio ID Identification",
   "original": "B63",
   "page_count": 8,
   "order": 41,
   "p1": "642",
   "pn": "649",
   "abstract": [
    "Identical audios may look quite differently in time and frequency domain because of various codecs, bit rates or transferring channels. Audio identification is needed for many scenarios. An enhanced audio fingerprint is proposed based on Haitsma’s approach whose robustness is improved significantly. A Beam-based search approach is also presented in this paper which prunes unpromising branches as early as possible in order to achieve high search efficiency.\n"
   ]
  },
  "huang06_iscslp": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Wei-Chuan",
     "Lee"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": " Robust Speech-Annotated Photo Retrieval Using Syllable-Transformed Patterns",
   "original": "B64",
   "page_count": 12,
   "order": 42,
   "p1": "650",
   "pn": "661",
   "abstract": [
    "This study presents a robust indexing and retrieval scheme for digital photos with speech annotations based on the syllable-transformed patterns. In speech retrieval application, out-of-vocabulary and recognition error problems are generally prone to incorrect transcription and therefore degrade the retrieval performance. In this study, the recognized n-best syllable candidates for each syllable is regarded as an ordered pattern and converted into an “image-like” pattern using the multidimensional scaling (MDS) method for indexing and retrieval. Vector quantization is then applied to cluster image vectors into the indexing codeword. Finally, a VSM-based indexing mechanism is used for photo retrieval with speech query. Experiments were conducted on the speech annotations of 1,055 collected digital photos. Compared to other conventional methods, the syllable-transformed pattern method shows a promising improvement on speech-annotated photo retrieval. Keywords: Speech retrieval, photo retrieval, multidimensional scaling.\n"
   ]
  },
  "zhang06e_iscslp": {
   "authors": [
    [
     "Feng",
     "Zhang"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " Two-layer Distance Scheme in Matching Engine for Query by Humming System",
   "original": "B65",
   "page_count": 7,
   "order": 43,
   "p1": "662",
   "pn": "668",
   "abstract": [
    "In query-by-humming system, minimizing the impact introduced by the pitch tracking errors and humming errors is always a difficult problem. In this paper, we propose a two-layer distance scheme instead of the global DTW distance in traditional QBH system, which is motivated by the people’s perception of humming. The local distance measure tries to find the correct part of humming, which is robust to the hummed error and the noise. Also, the approach to combine the local and global distance measure is proposed. The experiment of our QBH system on the mobile phone channel shows that the performance is greatly improved. Keywords: Query By Humming, Dynamic Time Warping, Two-layer Distance\n"
   ]
  },
  "wu06_iscslp": {
   "authors": [
    [
     "Xiao",
     "Wu"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Jun",
     "Yang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " A Top-down Approach to Melody Match in Pitch Contour for Query by Humming",
   "original": "B66",
   "page_count": 12,
   "order": 44,
   "p1": "669",
   "pn": "680",
   "abstract": [
    "In this paper, a novel frame-based algorithm called recursive alignment(RA) for query-by-humming(QBH) application is presented. Compared with other approaches, RA optimizes melody alignment problems in a top-down fashion which is more capable of capturing longdistance information in human singing. Three RA variations which run much faster at the expense of less accuracy are presented, and they can be used as ﬁlters of QBH systems. A QBH system built upon RA and its variations is also described brieﬂy. The experiment results show that the proposed algorithm compares favorably with other methods we have implemented.\n"
   ]
  },
  "yan06_iscslp": {
   "authors": [
    [
     "Xiang",
     "Yan"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Pei",
     "Ding"
    ],
    [
     "Rui",
     "Zhao"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": " Multi-accented Mandarin Database Construction and Benchmark Evaluations",
   "original": "B70",
   "page_count": 9,
   "order": 45,
   "p1": "715",
   "pn": "723",
   "abstract": [
    "In this paper, we describe the designing, recording and checking procedures of a multi-accented Mandarin speech database, and present benchmark evaluation of this database. The database was recorded in 6 cities in China, containing 1200 speakers’ accented Mandarin speech of continuous digits, isolated words and sentences. In total, 520k utterances (572.5 hours) were collected. We perfrom the intra-accent and cross-accent evaluations, together with the evaluation of a multi-accented acoustic model trained from the whole database. The database is a phonetically rich, gender-balanced and accent-balanced database, which could serve as the basic material for accented Mandarin recognition research, and it could also be used for creating real automatic speech recognition products for users with different accents. Keywords: Mandarin, speech database, multi-accented\n"
   ]
  },
  "han06_iscslp": {
   "authors": [
    [
     "Yan",
     "Han"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": " EM Algorithm with Split and Merge in Trajectory Clustering for Automatic Speech Recognition",
   "original": "B27",
   "page_count": 12,
   "order": 46,
   "p1": "268",
   "pn": "279",
   "abstract": [
    "In this paper, we introduce two reformulated versions of the standard EM algorithm, namely Successive Split EM and Split and Merge EM, to relax the problem of initialization dependence in datadriven Speech Trajectory Clustering. These two algorithms allow us to prevent the EM procedure in Trajectory Clustering from ending in a local maximum of the likelihood surface. Thus, the new methods will generate more coherent trajectory clusters. We applied these two methods for developing multiple parallel HMMs for a continuous digit recognition task. We compared the performance obtained with the proposed methods to the recognition performance obtained with knowledge-based contextdependent Head-Body-Tail models. The results showed that both datadriven approaches signiﬁcantly outperform the knowledge-based approach. In addition, in most cases the model based on Split and Merge EM is better than the model based on Successive Split EM.\n"
   ]
  },
  "chen06_iscslp": {
   "authors": [
    [
     "Jiang-Chun",
     "Chen"
    ],
    [
     "Chun-Jen",
     "Lee"
    ],
    [
     "Shuo-Pin",
     "Hsu"
    ],
    [
     "J.-S. Roger",
     "Jang"
    ]
   ],
   "title": " Sausage-net-based Minimum Phone Error Training for Continuous Phone Recognition",
   "original": "B28",
   "page_count": 9,
   "order": 47,
   "p1": "280",
   "pn": "288",
   "abstract": [
    "This paper describes a discriminative-based training approach to continuous phone recognition. Recently minimum phone error (MPE) training is widely used to enhance the performance of large vocabulary continuous speech recognition, but few of them applied it to continuous phone recognition. In this paper, we explore a flexible combination of the sausage net with the MPE training. Furthermore, a more effective method of MPE weight update is introduced. The best experimental result in this study indicates that our approach achieves 7% error rate reduction when comparing to the baseline system. This demonstrates the advantages of the proposed approach for the MPE training. Keywords: minimum phone error, discriminative training, continuous phone recognition, sausage, speech recognition.\n"
   ]
  },
  "yan06b_iscslp": {
   "authors": [
    [
     "Zhijie",
     "Yan"
    ],
    [
     "Peng",
     "Liu"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Frank",
     "Soong"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " Training Discriminative HMM by Optimal Allocation of Gaussian Kernels",
   "original": "B29",
   "page_count": 10,
   "order": 48,
   "p1": "289",
   "pn": "298",
   "abstract": [
    "We propose to train Hidden Markov Model (HMM) by allocating Gaussian kernels non-uniformly across states so as to optimize a selected discriminative training criterion. The optimal kernel allocation problem is ﬁrst formulated based upon a non-discriminative, Maximum Likelihood (ML) criterion and then generalized to incorporate discriminative ones. An eﬀective kernel exchange algorithm is derived and tested on TIDIGITS, a speaker-independent (man, woman, boy and girl), connected digit recognition database. Relative 46–51% word error rate reductions are obtained comparing to the conventional uniformly allocated ML baseline. The recognition performance of discriminative kernel allocation is also consistently better than the non-discriminative ML based, nonuniform kernel allocation.\n"
   ]
  },
  "gharavian06_iscslp": {
   "authors": [
    [
     "Davood",
     "Gharavian"
    ],
    [
     "S.M.",
     "Ahadi"
    ]
   ],
   "title": " Recognition of Emotional Speech and Speech Emotion in Farsi",
   "original": "B30",
   "page_count": 10,
   "order": 49,
   "p1": "299",
   "pn": "308",
   "abstract": [
    "Speech emotion can add extra information to speech in comparison with available textual information. However, it can also lead to some problems in the automatic speech recognition process. We evaluated the changes in speech parameters, i.e. formant frequencies and pitch frequency, due to anger and grief for Farsi language in a former research. Here, using those results, we try to improve emotional speech recognition accuracy using baseline models. We show that adding parameters such as formant and pitch frequencies to the speech feature vector can improve recognition accuracy. The percentage of improvement depends on parameter type, number of mixture components and the emotional condition. Identification of the emotional condition can also help in improving speech recognition accuracy. To recognize emotional condition of speech, formant and pitch frequencies were used successfully in two approaches, namely maximum likelihood and GMM. Keywords: Prosody, Speech emotion, Speech recognition, emotion recognition.\n"
   ]
  },
  "zeng06_iscslp": {
   "authors": [
    [
     "Jing-Teng",
     "Zeng"
    ],
    [
     "Cheng-Chang",
     "Lee"
    ],
    [
     "Jeng-Shien",
     "Lin"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Sen-Chia",
     "Chang"
    ]
   ],
   "title": " Monte Carlo Noisy HMM Estimation and Segmental Differential Features on the Aurora2 Clean Training Evaluation",
   "original": "B31",
   "page_count": 9,
   "order": 50,
   "p1": "309",
   "pn": "317",
   "abstract": [
    "In this paper, the compensation of mismatch between clean training hidden Markov models (HMMs) and noisy test speech is addressed. The purpose is to approach the performance of Aurora2 multi-condition training but use only clean training material. The idea is to integrate three methods including (1) mean subtraction, variance normalization and ARMA filtering (MVA) post-processing for Mel-scaled cepstral coefficients (MFCCs) normalization, (2) Monte Carlo noisy HMM estimation by adding artificial noises in the linear mel-scale filterbank parameter (MELSPEC) domain and (3) novel segmental differential features for increasing recognizer’s discriminative power. Experimental results on Aurora2 clean training corpus have shown that great performance improvement was achieved. Especially, although only clean training material was used, the performance did close to the level of Aurora2 multi-condition training. Keywords: Aurora2, Monte Carlo simulation, segmental differential feature, noise compensation.\n"
   ]
  },
  "ding06b_iscslp": {
   "authors": [
    [
     "Pei",
     "Ding"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Xiang",
     "Yan"
    ],
    [
     "Rui",
     "Zhao"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": " Optimizing the Implementation of MMSE Enhancement for Robust Speech Recognition",
   "original": "B32",
   "page_count": 10,
   "order": 51,
   "p1": "318",
   "pn": "327",
   "abstract": [
    "In this paper several methods are proposed to optimize the implementation of minimum mean-square error (MMSE) estimation algorithm for robust automatic speech recognition (ASR). In the calculation of MMSE enhancement algorithm, the original confluent hyper-geometric function is approximated by a piece-wise linear function, which greatly reduces the computation load while keep the same performance. After enhancement, the spectrum is smoothed in both time and frequency domains with symmetric arithmetic series weights to compensate those spectrum components distorted by noise over-reduction. A tuning scheme is also proposed to control the enhancement by adjusting the a priori signal-to-noise ratio (SNR). Evaluation results confirm that the proposed methods efficiently optimize the MMSE enhancement algorithm and significantly improve the robustness of ASR system against background noises. Keywords: Background Noise, Robust Speech Recognition, MMSE Estimation Algorithm, Speech Enhancement, Spectrum Smoothing\n"
   ]
  },
  "guo06b_iscslp": {
   "authors": [
    [
     "Yanmeng",
     "Guo"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Speech Endpoint Detection Based on Sub-band Energy and Harmonic Structure of Voice",
   "original": "B33",
   "page_count": 9,
   "order": 52,
   "p1": "328",
   "pn": "336",
   "abstract": [
    "This paper presents an algorithm of speech endpoint detection in noisy environments, especially those with non-stationary noise. The input signal is ﬁrstly decomposed into several sub-bands. In each sub-band, an energy sequence is tracked and analyzed separately to decide whether a temporal segment is stationary or not. An algorithm of voiced speech detection based on the harmonic structure of voice is brought forward, and it is applied in the non-stationary segment to check whether it contain speech or not. The endpoints of speech are ﬁnally determined according to the combination of energy detection and voice detection. Experiments in real noise environments show that the proposed approach is more reliable compared with some standard methods.\n"
   ]
  },
  "ding06c_iscslp": {
   "authors": [
    [
     "Pei",
     "Ding"
    ]
   ],
   "title": " Improving the Robustness of LPCC Feature Against Impulsive Noise by Applying the FOP Method",
   "original": "B34",
   "page_count": 9,
   "order": 53,
   "p1": "337",
   "pn": "345",
   "abstract": [
    "Performance of an automatic speech recognition (ASR) system tends to be dramatically degraded in the presence of impulsive noise. In the previous work [1], we proposed flooring the observation probability (FOP) to compensate the adverse effect of impulsive noise on sensitive dimensions of Mel-frequency cepstral coefficient (MFCC) features. Linear prediction cepstral coefficient (LPCC) is another kind of widely used acoustic feature, and in this paper we study the performance of the FOP method when applied to LPCC features, including feature vector partition based upon noise sensitivity analysis of each feature dimension and flooring threshold calculation. Evaluation results confirm the efficiency of FOP method on LPCC feature. For example, the highest averaged error reduction rate (ERR) of 38.9% and 46.8% versus the baseline is obtained, respectively in simulated substitutive impulsive noise and machinegun noise environment. Keywords: Robust Speech Recognition, Impulsive Noise, Observation Probability, Flooring Threshold, LPCC acoustic feature\n"
   ]
  },
  "guo06c_iscslp": {
   "authors": [
    [
     "Lihui",
     "Guo"
    ],
    [
     "Xin",
     "He"
    ],
    [
     "Yue",
     "Lu"
    ],
    [
     "Yaxin",
     "Zhang"
    ]
   ],
   "title": " A Low-Cost Robust Front-end for Embedded ASR System",
   "original": "B35",
   "page_count": 9,
   "order": 54,
   "p1": "346",
   "pn": "354",
   "abstract": [
    "In this paper we propose a low-cost robust MFCC feature extraction algorithm which combines noise reduction and voice activity detection (VAD) for automatic speech recognition (ASR) system of embedded applications. To remedy the eﬀect of additive noise a magnitude spectrum subtraction method is used. A VAD is performed to distinguish speech signal from noise signal. It discriminates speech/nonspeech frames by employing an order statistics ﬁlter (OSF) on subband spectral entropy. A general RASTA ﬁltering on log Mel ﬁlter-bank energy trajectories are applied. Finally, a 26 dimensional feature vector is used in ASR system after feature selection. Experimental results show that the proposed front-end can obtain 30.08% and 62.55% relative improvements on Aurora2 and Aurora3 databases and 29.47% on a Mandarin database compared with the baseline obtained from ETSI standard MFCC front-end.\n"
   ]
  },
  "liu06d_iscslp": {
   "authors": [
    [
     "Cong",
     "Liu"
    ],
    [
     "Zhijie",
     "Yan"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Renhua",
     "Wang"
    ]
   ],
   "title": " A Comparative Study on Confidence Measure in Mandarin Command Word Recognition",
   "original": "B36",
   "page_count": 12,
   "order": 55,
   "p1": "355",
   "pn": "366",
   "abstract": [
    "Two categories of Conﬁdence Measure (CM) approaches for Mandarin command word recognition, i.e., Likelihood Ratio Testing (LRT) based CM and Word Posterior Probability (WPP) based CM, are investigated in this paper. Both Equal Error Rate (EER) and Conﬁdence Error Rate (CER) performances of these approaches are evaluated on two databases: A Mandarin telephone command word database for which a matched model can be trained, and a PDA command word database for which the model and testing environment are mismatched. Experimental results show that for the matched case, the WPP based CM outperforms LRT based CM, and it is also insensitive to the decision threshold. However, for the mismatched case, the performance of the WPP based method decreases dramatically because of the inaccurate estimate of posterior probabilities on the basis of the ill-formed word graph. The characteristics and robustness problems of these CM approaches are analyzed, and their performances are given when deployed to realistic tasks.\n"
   ]
  },
  "wang06b_iscslp": {
   "authors": [
    [
     "Jingying",
     "Wang"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": " Speaker Adaptation Using Projection to Latent Structure Algorithm",
   "original": "B37",
   "page_count": 7,
   "order": 56,
   "p1": "367",
   "pn": "373",
   "abstract": [
    "Correlation between observations of different states is an important apriori information reflecting speech characteristics, which is a key factor improving speech recognition system robustness. Since speech and noise are statistically independent, correlation information can be used to reduce noise effect on speech recognition performance in noisy environment. This paper proposed a new speaker adaptation method using Projection to Latent Structure (PLS) algorithm. PLS can extract a group of basis vectors reflecting the characteristics of codebook and adaptation data, so as residual, the noise can be subtracted from speech. Different from Eigenvoice method, PLS extracts basis vectors using adaptation data. Furthermore, it needs very small storage space. Experimental results on large vocabulary continuous speech database show that this method is superior to baseline, MAP, EV and MLLR, etc. Keywords: Projection to Latent Structure (PLS), Speaker Adaptation, Correlation.\n"
   ]
  },
  "zhang06f_iscslp": {
   "authors": [
    [
     "Hua",
     "Zhang"
    ],
    [
     "Yun",
     "Tang"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": " Unvoiced Landmark Detection for Segment-based Mandarin Continuous Speech Recognition",
   "original": "B38",
   "page_count": 10,
   "order": 57,
   "p1": "374",
   "pn": "383",
   "abstract": [
    "This paper presents an attempt to introduce unvoiced landmarks into statistical continuous speech recognition system. The unvoiced landmark detection algorithm proposed here locates the points in speech where the vocal folds stop or begin freely vibrating. In our experiments, 87.47% of stops and 98.94% of fricatives are segmented from speech after the unvoiced landmark detection, with a very low insertion error rate of 0.13%. Then these landmarks are incorporated into decoding process of segment model based recognizer as search beginning indicators. The eﬀectiveness of landmark detection algorithm is veriﬁed in our landmark-guided recognition system with 240 sentences in 863Test database. Key words: Speech recognition, segment model, landmark detection\n"
   ]
  },
  "liu06e_iscslp": {
   "authors": [
    [
     "Yiyan",
     "Liu"
    ],
    [
     "Yingchun",
     "Yang"
    ],
    [
     "Zhenyu",
     "Shan"
    ]
   ],
   "title": " Experimental Investigation into Alignment-based Acoustic Confidence Measures in Keyword Verification for Mandarin Speech",
   "original": "B39",
   "page_count": 12,
   "order": 58,
   "p1": "384",
   "pn": "395",
   "abstract": [
    "This paper introduces the methods to verify keyword hypothesis using alignment information, which can be easily implemented and integrated into the keyword spotting system in a straightforward way. Acquiring alignment information goes without any extra model training, and has no evident total processing time impact on original keyword spotting system. Different alignment-based confidence measures are evaluate using ROC curves. In addition, we introduce a novel evaluation metric, Confidence Error Cost Function (CECF), which improves upon Confidence Error Rate (CER). It is more general than CER since it takes different cost weights of FA and FR into consideration. We present the result of experiments aiming at assessing the quality and the limitations of different confidence measures and find out that the combination of minimum alignment cost and state duration normalized posterior probability gives the best performance. Keywords: Confidence Measure, Alignment, CECF, Keyword Verification\n"
   ]
  },
  "timofte06_iscslp": {
   "authors": [
    [
     "Radu",
     "Timofte"
    ],
    [
     "Ville",
     "Hautamaki"
    ],
    [
     "Pasi",
     "Franti"
    ]
   ],
   "title": " Speaker, Vocabulary and Context Independent Word Spotting System for Continuous Speech",
   "original": "B40",
   "page_count": 12,
   "order": 59,
   "p1": "396",
   "pn": "407",
   "abstract": [
    "Word spotting is a widely known subject in continuous speech recognition and the traditional approaches use either hidden Markov models (HMM) or Gaussian mixture models (GMM). In this paper, we propose a different approach based on language independent phoneme modeling. The proposed system is speaker and vocabulary independent, and it is easy to implement. An equal error rate (EER) of 3.34% and a figure of merit (FOM) of 45.58% are achieved on TIMIT corpus. Keywords: word spotting, continuous speech recognition, phonetic model, clustering, vocabulary independent, pattern matching\n"
   ]
  },
  "zhang06g_iscslp": {
   "authors": [
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Jian",
     "Shao"
    ],
    [
     "Jiang",
     "Han"
    ],
    [
     "Zhaojie",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": " Keyword Spotting Based on Phoneme Confusion Matrix",
   "original": "B41",
   "page_count": 12,
   "order": 60,
   "p1": "408",
   "pn": "419",
   "abstract": [
    "For many practical applications of keyword spotting, input signal is a spontaneous conversation while the acoustic model was trained with read speech because of data availability. Generally speaking, keyword spotting system will degrade significantly because of mismatch between acoustic model and spontaneous speech. To solve this problem, this paper presents a two-pass keyword spotting strategy. In order to improve the retrieval performance, an improved phoneme confusion matrix is adopted. It will give more freedom in the representation so as to alleviate the effect of mismatched training condition and of phoneme misrecognition. Furthermore, a hybrid confidence measure is applied to reject false alarms. Experiments show that the proposed algorithms significantly reduced equal error rate (EER) on the telephone conversational task. Keywords: keyword spotting, confusion matrix, confidence measure\n"
   ]
  },
  "kim06_iscslp": {
   "authors": [
    [
     "Young Kuk",
     "Kim"
    ],
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": " Performance Evaluation of Non-Keyword Modeling for Vocabulary-Independent Keyword Spotting",
   "original": "B42",
   "page_count": 11,
   "order": 61,
   "p1": "420",
   "pn": "430",
   "abstract": [
    "In this paper, we develop a keyword spotting system using vocabulary-independent speech recognition technique, and investigate several non-keyword modeling methods to improve its performance. In order to overcome the weakness of conventional syllable model, we propose the syllable filler based on syllable information of keywords and syllable-like filler model. The former prohibits syllable filler network from taking the common syllables that keyword network has for better descrimination between keywords and filler. According to our experiments, syllable filler model using syllable information of keyword yields error reduction rate of 52%-54%. The latter constructs syllable filler network by concatenating the clustered CI phonemes classes. It leads to a 75 times faster decoding than conventional syllable filler while not requiring a large size of text corpus. Keywords: vocabulary-independent keyword spotting, non-keyword modeling.\n"
   ]
  },
  "wang06c_iscslp": {
   "authors": [
    [
     "Xiangdong",
     "Wang"
    ],
    [
     "Feng",
     "Xie"
    ],
    [
     "Shouxun",
     "Lin"
    ],
    [
     "Yuelian",
     "Qian"
    ],
    [
     "Qun",
     "Liu"
    ]
   ],
   "title": " DOE and ANOVA based Performance Influencing Factor Analysis for Evaluation of Speech Recognition Systems",
   "original": "B43",
   "page_count": 11,
   "order": 62,
   "p1": "431",
   "pn": "441",
   "abstract": [
    "In this paper, a framework of performance influencing factor analysis (PIFA) is proposed for evaluation of speech recognition systems. For each system under evaluation, the influence of various data properties (e. g. accent, SNR, and speaking rate) on system performance is analyzed to show corresponding characteristics of the system. The main idea of this approach is to design test data by means of Design of Experiments (DOE) methods and perform analysis of variance (ANOVA) and other statistical analyses on the performance of each system. To use ANOVA which is far more powerful than rank tests generally used in performance evaluation, a method is proposed for generating performance measurement data satisfying the two basic assumptions of ANOVA. The PIFA approach is applied to the 2004 HTRDP ASR Evaluations, and analysis results are reported and interpreted, which are considered helpful for pointing out virtues and deficiencies of each system and accelerating improvement. Keywords: PIFA, ASR, Speech Recognition, Evaluation, DOE, ANOVA.\n"
   ]
  },
  "meng06_iscslp": {
   "authors": [
    [
     "Meng",
     "Meng"
    ],
    [
     "Shijin",
     "Wang"
    ],
    [
     "Jiaen",
     "Liang"
    ],
    [
     "Peng",
     "Ding"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": " Full Utilization of Closed-captions in Broadcast News Recognition",
   "original": "B44",
   "page_count": 9,
   "order": 63,
   "p1": "442",
   "pn": "450",
   "abstract": [
    "Lightly supervised acoustic model training has been recognized as an effective way to improve acoustic model training for broadcast news recognition. In this paper, a new approach is introduced to both fully utilize the un-transcribed data by using closed captions as transcripts and to select more informative data for acoustic model training. We will show that this approach is superior to regular method, which filters data only based on matching degree of closed-captions and ASR results without considering the effectiveness of data. By the way, an approximately correct transcription for manual amendment is obtained by this approach, which can reduce manual effort enormously for detailed annotation. Keywords: Lightly supervised acoustic model training, closed-caption, ASR.\n"
   ]
  },
  "shi06_iscslp": {
   "authors": [
    [
     "Yu",
     "Shi"
    ],
    [
     "Frank",
     "Soong"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ]
   ],
   "title": " Integrating Hypotheses of Multiple Recognizers for Improving Mandarin LVCSR Performance",
   "original": "B45",
   "page_count": 12,
   "order": 64,
   "p1": "451",
   "pn": "462",
   "abstract": [
    "In this paper, we investigate how to improve Mandarin LVCSR performance by integrating multiple hypotheses from recognizers running in parallel. Diﬀerent recognizers are trained by employing: (1) diﬀerent phone sets, (2) diﬀerent front-ends, and (3) diﬀerent training sets. Nbest hypotheses are merged into a character transition network (CTN) and ROVER is used to select the ﬁnal recognition decision. Both read and spontaneous speech are tested in the ROVER framework. In comparing with the best individual recognizer in the parallel group, the fully integrated ROVER system achieves a relative Chinese character error reduction of 10.1%.\n"
   ]
  },
  "liu06f_iscslp": {
   "authors": [
    [
     "Linquan",
     "Liu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": " English Alphabet Recognition Based on Chinese Acoustic Modeling",
   "original": "B46",
   "page_count": 10,
   "order": 65,
   "p1": "463",
   "pn": "472",
   "abstract": [
    "How to effectively recognize English letters spoken by Chinese people is our major concern in the paper. Some efforts are made to build Chinese extended Initial/Final (XIF) based HMMs for English alphabet recognition which can be integrated with large vocabulary continuous Chinese speech recognition (Chinese LVCSR) system based on a same XIF set. The alphabet-specific XIF HMMs are built using context-dependent modeling, decision tree based state clustering method, state-based phonetic mixture tying and pronunciation modeling techniques. Experiments have been done over a 32-speaker test set. Compared with English phoneme-based acoustic modeling, our proposed method can achieve a relative letter error rate reduction of 5.3% with a letter correctness of 97.2% for Chinese-accented English alphabet recognition. What’s more, the XIF-based HMMs for English alphabet can be integrated with Chinese LVCSR seamlessly to recognize Chinese as well as English letters simultaneously. Keywords: English alphabet recognition, Chinese speech recognition, acoustic modeling, pronunciation modeling, state-based phonetic mixture tying, state clustering.\n"
   ]
  },
  "zhu06_iscslp": {
   "authors": [
    [
     "Donglai",
     "Zhu"
    ],
    [
     "Rong",
     "Tong"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": " Minimum Classification Error Based Optimal Linear Combination for Spoken Language Identification",
   "original": "B47",
   "page_count": 12,
   "order": 66,
   "p1": "473",
   "pn": "484",
   "abstract": [
    "State-of-the-art language identiﬁcation systems are commonly constructed with multiple parallel classiﬁers to take advantage of diﬀerent levels of speech features. These classiﬁers are combined with a fusion module to make the ﬁnal decision. Following the maximum a posteriori (MAP) decision rule, the fusion of multiple classiﬁers can be transformed to a constrained optimal linear combination (OLC), where the linear weights represent the prior probabilities of the classiﬁers. We derive an optimization algorithm for the constrained linear weights based on the minimum classiﬁcation error (MCE) criterion. The proposed method is evaluated on the NIST 2003 LRE task. Compared with the best individual classiﬁer, the fusion approach reduces the equal error rate (EER) at relative rates of 10.2%, 24.4% and 25.7% for 30-second, 10-second and 3-second durations of speech segments, respectively.\n"
   ]
  },
  "wang06d_iscslp": {
   "authors": [
    [
     "Liang",
     "Wang"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Eric H.C.",
     "Choi"
    ]
   ],
   "title": " Automatic Tonal and Non-Tonal Language Classification and Language Identification Using Prosodic Information",
   "original": "B48",
   "page_count": 12,
   "order": 67,
   "p1": "485",
   "pn": "496",
   "abstract": [
    "In this paper, an innovative method for tonal and non-tonal language classification using prosodic information is reported. The normalized feature parameters that measure pitch changing speed and pitch changing level are used to train a 3-layer feedforward neural network for the classification. To demonstrate the effectiveness of the proposed method, the recognition rate and the processing time of the novel system are compared with a PPRLM system on a 2-language identification task. For the evaluation results of identifying English/Mandarin, the novel system can achieve a recognition rate of 83.3%, compared with 91.7% of the PPRLM system. However, the processing time of the novel system is only half of that of the PPRLM system. In another extended tonal and non-tonal language classification task with 6 languages, the novel system can achieve a classification rate of 80.6%. Possible applications of the new method to perform pre-classification in language identification are also discussed. Keywords: Tonal language, non-tonal language, pitch changing speed, pitch changing level, language identification.\n"
   ]
  },
  "chang06_iscslp": {
   "authors": [
    [
     "Wen-Chieh",
     "Chang"
    ],
    [
     "Ding-Yun",
     "Chen"
    ],
    [
     "Zi-He",
     "Chen"
    ],
    [
     "Zhi-Ren",
     "Zeng"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Yau-Tarng",
     "Juang"
    ]
   ],
   "title": " Incorporating Prosodic with Acoustic Information for ISCSLP'2006 Speaker Recognition Evaluation - Robust Cross-Channel Speaker Verification",
   "original": "B49",
   "page_count": 12,
   "order": 68,
   "p1": "497",
   "pn": "508",
   "abstract": [
    "In this paper, we present our speaker verification (SV) systems for the cross-channel text-independent and dependent speaker verification (TI-SV and TD-SV) tasks of ISCSLP’2006 speaker recognition evaluation (ISCSLP2006-SRE). To address the cross-channel issues and take advantage of the unique characteristics of Mandarin (i.e., tonal language), prosodic contours are modeled to assist the state-of-the-art spectral feature-based SV systems. Especially, two approaches are proposed including (1) latent prosody analysis (LPA) for modeling the prosodic behaviors of a speaker and (2) a Gaussian mixture model (GMM) for modeling the dynamics of the pitch and energy contours. Experimental results on the evaluation set of ISCSLP2006-SRE had demonstrated that the proposed methods of incorporating prosodic featurebased SV systems with spectral feature-based SV systems outperform the spectral feature only SV systems for both TIand TD-SV tasks, respectively. Keywords: Speaker verification, prosodic information, mean variance normalization and ARMA filtering (MVA), Gaussian mixture model (GMM), test normalization (T-norm), probabilistic latent semantic analysis (PLSA), latent prosody analysis (LPA).\n"
   ]
  },
  "lu06_iscslp": {
   "authors": [
    [
     "Xiang-Feng",
     "Lu"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": " Compensations for SVM in Text-Independent Speaker Verification",
   "original": "B50",
   "page_count": 12,
   "order": 69,
   "p1": "509",
   "pn": "520",
   "abstract": [
    "Support Vector Machines (SVMs) technique, as a kind of pattern classifier, is widely used in pattern classification including speaker verification. We study the asymmetrical character of speaker verification that uses SVM since the asymmetry between true and imposter speaker training sets degrade recognition rate. Asymmetrical costs kernel is implemented and based on it, we introduce a new method that compensate for the SVM scores according to SVM models. Experimental results are attached and analyzed, which show improvements in performance. Since compensations in score field do not entail heavy computations, the method introduced can easily be applied to the standard system.\n"
   ]
  },
  "zheng06_iscslp": {
   "authors": [
    [
     "Rong",
     "Zheng"
    ],
    [
     "Hongchen",
     "Jiang"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": " Exploiting GMM-based Quality Measure for SVM Speaker Verification",
   "original": "B51",
   "page_count": 10,
   "order": 70,
   "p1": "521",
   "pn": "530",
   "abstract": [
    "In this paper, we examine the problem of quality measurement for speaker verification using support vector machines (SVMs). An efficient Gaussian mixture models (GMMs) based quality estimation algorithm is proposed to potentially utilize speaker-specific broad acoustic-class characteristics. Some verification strategies are also considered in the test phase. We perform clustering-based vector pre-quantization to reduce the computational load and the redundancy in speech signal. Quality estimation is also integrated into test-length normalization. We then apply it to a text-independent speaker verification task using the NIST 2002 speaker recognition evaluation (SRE) database. Experimental results show that the proposed method can produce good classification accuracy. Keywords: Speaker verification, quality measure, vector pre-quantization, support vector machines, Gaussian mixture models.\n"
   ]
  },
  "guo06d_iscslp": {
   "authors": [
    [
     "Wu",
     "Guo"
    ],
    [
     "Renhua",
     "Wang"
    ],
    [
     "Lirong",
     "Dai"
    ]
   ],
   "title": " Feature Extraction and Test Algorithm for Speaker Verification",
   "original": "B52",
   "page_count": 8,
   "order": 71,
   "p1": "531",
   "pn": "538",
   "abstract": [
    "In this paper we introduce two methods to improve text-independent speaker verification. In feature extraction process the feature vectors of voiced speech and unvoiced speech independently. In test process, the test speech is adapted to a new model instead of calculating the log-likelihood, then the Mahalanobis Distances among the UBM model, the speaker model and the test speech model are calculated. These three models distances formed a triangle. The angle of the models can be obtained as the test scores. Further more the scores of the log-likelihood and the angle of the models can be fused to improve performance. When we employ the proposed algorithms, the EER of the speaker verification can be reduced by 20% compared with the baseline system.\n"
   ]
  },
  "luan06_iscslp": {
   "authors": [
    [
     "Jian",
     "Luan"
    ],
    [
     "Jie",
     "Hao"
    ],
    [
     "Tomonari",
     "Kakino"
    ],
    [
     "Tomonori",
     "Ikumi"
    ]
   ],
   "title": " Frame-level Nonlinearity for Robust DTW-based Speaker Verification",
   "original": "B53",
   "page_count": 8,
   "order": 72,
   "p1": "539",
   "pn": "546",
   "abstract": [
    "Dynamic time warping (DTW) is a successful algorithm in many matching and searching tasks. For the text-dependent speaker verification, it is still an appropriate choice when enrollment data are very limited. Yet DTW is very sensitive to the endpoint variations between the reference template and test examples. Most research reported on this issue is mainly in two directions: robust endpoint detector and endpoint constraint relaxation. In this paper, we intend to propose the third possible solution by employing a frame-level nonlinear transform. The parameter for the transform function may be universal, template-dependent or frame-dependent. This method is also able to realize the normalization of DTW matching distance at the same time. Results indicate that the performance of text-dependent speaker verification can be enhanced remarkably in both clean and noisy environments. Their relative reductions of EER are 20.6% and 35.0% respectively. We expect the proposed method may be effective in other DTW applications as well. Keywords: speaker verification, dynamic time warping, text-dependent, framelevel nonlinearity\n"
   ]
  },
  "kinnunen06_iscslp": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Chin-Wei Eugene",
     "Koh"
    ],
    [
     "Lei",
     "Wang"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Eng-Siong",
     "Chng"
    ]
   ],
   "title": " Temporal Discrete Cosine Transform: Towards Longer Term Temporal Features for Speaker Verification",
   "original": "B54",
   "page_count": 12,
   "order": 73,
   "p1": "547",
   "pn": "558",
   "abstract": [
    "In this paper, we propose the temporal discrete cosine transform (TDCT) feature for the speaker verification task. The TDCT feature captures temporal information from a longer time context beyond the conventional delta and double-delta coefficients. We evaluate the effectiveness of the TDCT feature on the NIST 2001, NIST 2004, and NIST 2005 speaker recognition benchmark corpora by using a standard GMM-UBM recognizer. We compare our results against the standard MFCC+Δ+ΔΔ front end, and with the shifted delta cepstrum (SDC) feature which is commonly used in the language identification task. The results indicate that the TDCT and SDC give similar accuracy, and that the TDCT feature outperforms MFCC+Δ+ΔΔ in most of the cases. Keywords: Text-independent speaker verification, temporal features, temporal discrete cosine transform, shifted delta cepstrum, Gaussian mixture model\n"
   ]
  },
  "kinnunen06b_iscslp": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Ville",
     "Hautamaki"
    ],
    [
     "Pasi",
     "Franti"
    ]
   ],
   "title": " On the Use of Long-Term Average Spectrum in Automatic Speaker Recognition",
   "original": "B55",
   "page_count": 9,
   "order": 74,
   "p1": "559",
   "pn": "567",
   "abstract": [
    "State-of-the-art automatic speaker recognition systems use mel-frequency cepstral coeﬃcients (MFCC) features to describe the spectral properties of speakers. In forensic phonetics, the long-term average spectrum (LTAS) has been used for the same purpose. LTAS provides an intuitive graphical representation which can be used to visualize and quantify speaker diﬀerences. However, few studies have reported the use of LTAS in automatic speaker recognition. Thus, the purpose of this paper is to systematically study how to use the LTAS in automatic speaker recognition. We will also ﬁnd out whether it provides additional discriminative information in respect to the MFCC-based system.\n"
   ]
  },
  "patil06b_iscslp": {
   "authors": [
    [
     "Hemant A.",
     "Patil"
    ],
    [
     "T. K.",
     "Basu"
    ]
   ],
   "title": " A New Data Fusion Technique and Performance Measure for Identification of Twins in Marathi",
   "original": "B56",
   "page_count": 12,
   "order": 75,
   "p1": "568",
   "pn": "579",
   "abstract": [
    "Speaker Recognition (SR) is an economic method of biometrics because of availability of low cost and high power computers. An important question which must be answered for the SR system is how well the system resists the effects of determined mimics such as those based on physiological characteristics especially identical twins or triplets. In this paper, a new data fusion technique (viz., majority rule for combining evidence from different feature sets) and a new performance measure is proposed for speaker identification of twins in an Indian language, viz., Marathi. The results have been compared with baseline SR system designed by using Linear Prediction Coefficients (LPC), Linear Prediction Cepstral Coefficients (LPCC) and Mel Frequency Cepstral Coefficients (MFCC) as input feature vectors and polynomial classifiers of 2nd and 3rd order approximation for speaker modeling.\n"
   ]
  }
 },
 "sessions": [
  {
   "title": " Rich Information Annotation And Spoken Language Processing",
   "papers": [
    "yuan06_iscslp",
    "zhang06_iscslp"
   ]
  },
  {
   "title": " Multilingual Corpus Development I",
   "papers": [
    "agrawal06_iscslp"
   ]
  },
  {
   "title": " Multilingual Corpus Development II",
   "papers": [
    "luong06_iscslp",
    "dawa06_iscslp",
    "patil06_iscslp",
    "zu06_iscslp"
   ]
  },
  {
   "title": " Robust Techniques For Organizing And Retrieving Spoken Documents",
   "papers": [
    "shao06_iscslp"
   ]
  },
  {
   "title": " Machine Translation And Language Modeling For Spoken Language",
   "papers": [
    "banchs06_iscslp",
    "hu06_iscslp",
    "liu06_iscslp"
   ]
  },
  {
   "title": " Speech Analysis, Enhancement, Coding And Synthesis",
   "papers": [
    "zhang06b_iscslp",
    "cui06_iscslp",
    "zou06_iscslp",
    "guan06_iscslp",
    "choi06_iscslp",
    "liu06b_iscslp",
    "qin06_iscslp",
    "zhang06c_iscslp",
    "wang06_iscslp",
    "qi06_iscslp",
    "tian06_iscslp",
    "li06_iscslp",
    "li06b_iscslp",
    "yi06_iscslp",
    "yu06_iscslp",
    "guo06_iscslp",
    "gu06_iscslp",
    "xu06_iscslp",
    "lee06_iscslp",
    "yuan06b_iscslp"
   ]
  },
  {
   "title": " Topics In Spoken Language Processing",
   "papers": [
    "hoshino06_iscslp",
    "li06c_iscslp",
    "sittiprapaporn06_iscslp",
    "ding06_iscslp",
    "dong06_iscslp",
    "pan06_iscslp",
    "liu06c_iscslp",
    "hu06b_iscslp",
    "zhang06d_iscslp",
    "li06d_iscslp",
    "huang06_iscslp",
    "zhang06e_iscslp",
    "wu06_iscslp",
    "yan06_iscslp"
   ]
  },
  {
   "title": " Speech Recognition",
   "papers": [
    "han06_iscslp",
    "chen06_iscslp",
    "yan06b_iscslp",
    "gharavian06_iscslp",
    "zeng06_iscslp",
    "ding06b_iscslp",
    "guo06b_iscslp",
    "ding06c_iscslp",
    "guo06c_iscslp",
    "liu06d_iscslp",
    "wang06b_iscslp",
    "zhang06f_iscslp",
    "liu06e_iscslp",
    "timofte06_iscslp",
    "zhang06g_iscslp",
    "kim06_iscslp",
    "wang06c_iscslp",
    "meng06_iscslp",
    "shi06_iscslp",
    "liu06f_iscslp"
   ]
  },
  {
   "title": " Recognition Of Speakers And Languages",
   "papers": [
    "zhu06_iscslp",
    "wang06d_iscslp",
    "chang06_iscslp",
    "lu06_iscslp",
    "zheng06_iscslp",
    "guo06d_iscslp",
    "luan06_iscslp",
    "kinnunen06_iscslp",
    "kinnunen06b_iscslp",
    "patil06b_iscslp"
   ]
  }
 ]
}