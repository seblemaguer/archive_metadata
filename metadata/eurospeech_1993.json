{
 "title": "3rd European Conference on Speech Communication and Technology (Eurospeech 1993)",
 "location": "Berlin, Germany",
 "startDate": "22/9/1993",
 "endDate": "25/9/1993",
 "conf": "Eurospeech",
 "year": "1993",
 "name": "eurospeech_1993",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "3rd European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1993)",
 "date": "22-25 September 1993",
 "papers": {
  "baker93_eurospeech": {
   "authors": [
    [
     "Janet M.",
     "Baker"
    ]
   ],
   "title": "Dictation, directories, and data bases; emerging PC applications forlarge vocabulary speech recognition",
   "original": "e93_0003",
   "page_count": 8,
   "order": 1,
   "p1": "3",
   "pn": "10",
   "abstract": [
    "Since its commercial introduction in 1990, general-purpose large vocabulary speech recognition has found broad-ranging applications. Although dictation and document creation applications predominate, other significant applications have fanned out to span database /information retrieval queries, mail routing by name/address designation, directory/information services, voice programming, and electronic mail generation. This keynote presentation focuses on application requirements, typical case studies of users in Europe and North America, and future opportunities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-1"
  },
  "barry93_eurospeech": {
   "authors": [
    [
     "William",
     "Barry"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Speech database annotation. the importance of a multi-lingual approach",
   "original": "e93_0013",
   "page_count": 8,
   "order": 2,
   "p1": "13",
   "pn": "20",
   "abstract": [
    "It is now recognised that progress in Speech Technology in particular, and the Speech Sciences in general, is dependent to a considerable extent on the quality of the speech databases that are available. Their coverage of speakers, speaking style, vocabulary, recording conditions etc. is crucial both to the quality of the system under development and the nature of speech-knowledge that can be gained from them. However, all these qualities can be undermined if the reliability of their labelling is suspect. The labelling of databases does not normally stand in the forefront of attention when database projects are undertaken, but in terms of effort it can swallow more than all the rest of the undertaking. Next to reliability; therefore, economy of effort is of paramount importance.\n",
    "Work at the authors' laboratories has pursued these two principles along the following lines: Reliability is maximised within the automatic procedure by the incorporation of an interactive component which allows the scrutiny and modification of boundary placement according to preselected criteria. The economy of effort which an automatic system provides is maximised, and the cost of intervention through the interactive process minimised by a segmentation and alignment process which runs in real-time. Above all, the system is geared towards economy of effort in terms of transferability of training.\n",
    "A multi-language approach lies at the heart of this feature. The Kohonen-type Self-Organising Neural Network plus Viterbi search with level-building is based on an acoustic-phonetic feature specification of the sounds of a language which allows for statements of equivalence between languages (Polyphonemes). This offers the option of (a) training the polyphoneme categories over several languages simultaneously to increase the occurrence of a given category, or (b) using the projected equivalences between languages to boot-strap a labelling system for a language in which recordings but no labelled material exists.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-2"
  },
  "lamel93_eurospeech": {
   "authors": [
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Identifying non-linguistic speech features",
   "original": "e93_0023",
   "page_count": 8,
   "order": 3,
   "p1": "23",
   "pn": "30",
   "abstract": [
    "Over the last decade technological advances have been made which enable us to envision real-world applications of speech technologies. It is possible to foresee applications, for example, information centers in public places such as train stations and airports, where the spoken query is to be recognized without even prior knowledge of the language being spoken. Other applications may require accurate identification of the speaker for security reasons, including control of access to confidential information or for telephone-based transactions. In this paper we present a unified approach to identifying non-linguistic speech features from the recorded signal using phone-based acoustic likelihoods. The basic idea is to process the unknown speech signal by feature-specific phone model sets in parallel, and to hypothesize the feature value associated with the model set having the highest likelihood. This technique is shown to be effective for text-independent sex, speaker, and language identification and can enable better and more friendly human-machine interaction. Text-independent speaker identification accuracies of 98.8% on TIMIT (168 speakers) and 99.2% on BREF (65 speakers), were obtained with one utterance per speaker, and 100% with 2 utterances for both corpora. Experiments estimating speaker-specific models without use of the phonetic transcription for the TIMIT speakers had the same identification accuracies obtained with the use of the transcriptions. French/English language identification is better than 99% with 2s of read, laboratory speech. On spontaneous telephone speech from the OGI corpus, the language can be identified as French or English with 82% accuracy with 10s of speech. 10 language identification using the OGI corpus is 59.7% with 10s of signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-3"
  },
  "peckham93_eurospeech": {
   "authors": [
    [
     "Jeremy",
     "Peckham"
    ]
   ],
   "title": "A new generation of spoken dialogue systems: results and lessons from the sundial project",
   "original": "e93_0033",
   "page_count": 8,
   "order": 4,
   "p1": "33",
   "pn": "40",
   "abstract": [
    "The current generation of interactive dialogue systems for over-the-telephone applications in real environments is limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. The five year ESPRIT SUNDIAL project, which concluded in August 1993, was aimed at developing a new generation of dialogue systems allowing spontaneous conversational interaction over the telephone. The project involved the development of four language prototypes for train timetable enquires in German and Italian and flight enquiries in English and French. A close integration of speech and language processing techniques has been achieved for understanding spoken input together with the use of AI techniques for intelligent dialogue management. Evaluation of prototypes in four languages has been completed with both naive and experienced users. A 96% transaction success rate has been achieved with the flight inquiry application in English over a local PBX connection using untrained speakers; lower performance was obtained for long distance PSTN connections with members of the public. A number of major lessons were learned from the project about the factors which are important in the design of spoken dialogue systems operating over the telephone. Some of these lessons are discussed in the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-4"
  },
  "moore93_eurospeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Whither a theory of speech pattern processing?",
   "original": "e93_0043",
   "page_count": 5,
   "order": 5,
   "p1": "43",
   "pn": "47",
   "abstract": [
    "This talk will raise numerous questions about the general nature of contemporary speech research. Are we making good progress, or are we in need of a glorious revolution? Do we have anything which could be called a 'science' of speech andt if not, when and where will one happen? It is proposed that future progress (and funding) of the field depends on the early establishment of a theory of speech pattern processing. (According to the dictionary, 'whither' -/wID@/- means (i) to what place? (ii) to what end or purpose? - not to be confused with 'wither' which means to droop, fade or shrivel up!)\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-5"
  },
  "noll93_eurospeech": {
   "authors": [
    [
     "Peter",
     "Noll"
    ]
   ],
   "title": "Speech coding for communications",
   "original": "e93_0479",
   "page_count": 10,
   "order": 6,
   "p1": "479",
   "pn": "488",
   "abstract": [
    "This paper describes recent approaches to speech coding at low bit rates with an emphasis on existing and upcoming standards and with a preference given to those coders where subjective quality scores were available. Speech coders for mobile radio are covered in some detail. A good to excellent speech coding quality can be achieved with bit rates of 1 b/sample (8 kb/s), expectations are that the rates can be reduced to less than 0.5 b/sample (4 kb/s) within the next decade. Not covered are important issues of complete speech communications links, such as sensitivity to and protection against bit errors or cell losses, adaptive noise suppression, and others.\n",
    "Keywords: Speech Coding, Source Coding, Digital Networks, Mobile Radio, Cordless Telephony.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-6"
  },
  "ney93_eurospeech": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Modeling and search in continuous speech recognition",
   "original": "e93_0491",
   "page_count": 8,
   "order": 7,
   "p1": "491",
   "pn": "498",
   "abstract": [
    "This paper gives an overview of the principles of a system for phoneme-based, large-vocabulary, continuous-speech recognition. In particular, the issues of modeling and search for acoustic recognition are addressed. Like many other systems, the recognition architecture is based on an integrated statistical approach. However, the characteristic features of the system are closer to classical pattern recognition: 1. Classical pattern recognition techniques like continuous mixture densities and linear discriminant analysis are extensively used, resulting in what can be viewed as a sort of 'statistical template matching'. 2. The framework of Hidden Markov modeling is used for time alignment only. 3. Time-synchronous beam search is used consistently throughout all tasks; by using a tree organization of the vocabulary and phoneme look-ahead, this one-pass strategy is able to handle a 10000-word task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-7"
  },
  "eskenazi93_eurospeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Trends in speaking styles research",
   "original": "e93_0501",
   "page_count": 9,
   "order": 8,
   "p1": "501",
   "pn": "509",
   "abstract": [
    "One of the principal interests of researchers in automatic speech processing at present is the adequate representation of the great variability in the speech signal. Some search for common behaviour among many speakers, \"invariants\" in the signal, yet others search for a chart of all that varies in order to better understand the limits of variability. In a first approximation, the study of speaking styles should be of little interest to the first group. It would seem to have little effect on the nature of the linguistic message. Yet, when applying speech recognition systems to increasingly natural speech and realistic applications, it has been found that a change in speaking style affects recognition results. Elsewhere, in working with the hard-of-hearing, it was found that processors on hearing aids could be greatly improved if the information used when one speaks clearly is modelled. And yet elsewhere, developers of text-to-speech synthesis systems recognise that their systems could benefit in intelligibility and naturalness from models of clear speech. The characterisation and subsequent modelling of this style phenomenon therefore have become of great interest. As for the second group, taking a view that we must understand all sources of variability in order to correctly process speech [LR84] [LR90], style is one of the many dimensions to be explored to the limits of speaker space. The term \"speaking styles\" has been used and defined in many ways [LI92], almost as numerous as the authors who have dealt with the subject. It can be said that many suprasegmental changes that could not be attributed to a more well-defined area, such as emotion [MU93] have been thrown on the heap which has until present been called style, by the speech community [EM92]. We shall return to the definition of speaking style below. First it is necessary to see what has been accomplished in communities which have a longer experience in studying style, sociolinguistics and psycholinguistics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-8"
  },
  "bridle93_eurospeech": {
   "authors": [
    [
     "John S.",
     "Bridle"
    ]
   ],
   "title": "Models of speech recognition; personal perspectives on particular approaches",
   "original": "e93_0513",
   "page_count": 3,
   "order": 9,
   "p1": "513",
   "pn": "515",
   "abstract": [
    "The aim of this keynote tutorial is to explore the nature of Automatic Speech Recognition and its relationships to human speech communication. We then examine various approaches evident in the papers presented at this meeting, based on consistency with linguistic theory, with statistical properties of speech data, or with theories of brain Junction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-9"
  },
  "lee93_eurospeech": {
   "authors": [
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "The conversational computer: an apple perspective",
   "original": "e93_1377",
   "page_count": 8,
   "order": 10,
   "p1": "1377",
   "pn": "1384",
   "abstract": [
    "This paper describes Apple's progress to date in spoken language technologies and systems. The first embodiment of our speech technologies is collectively called PlainTalk (TM), which includes Apple's speech recognition technology, speech synthesis technology, and a speech interface codenamed Casper. This paper also discusses Apple's vision of the \"Knowledge Navigator\", and what difficulties need to be overcome to deliver that vision.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-10"
  },
  "jekosch93_eurospeech": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "Speech quality assessment and evaluation",
   "original": "e93_1387",
   "page_count": 8,
   "order": 11,
   "p1": "1387",
   "pn": "1394",
   "abstract": [
    "The assessment and evaluation of speech quality and speech-output systems is a multi-dimensional problem. In this essay a broad overview of relevant methods and methodologies is given, and their range of validity is analysed. It shows that - although progress in the field can be noted - there is still a significant lack of knowledge and experience in many ways. Until now a number of important factors of speech quality have been identified, but reliable methods for monitoring the performance of speech quality and speech-output systems in praxi are hardly available. In other words, an integrated, application-oriented battery of assessment and evaluation procedures is still missing - as well as scientifically-based written guidelines to support non-experts when exposed to respective assessment and evaluation tasks. It is the aim of this contribution to review methods currently available in the field - including some recently novel approaches - in view of the task of predicting performance of speech-output devices in particular application scenarios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-11"
  },
  "santen93_eurospeech": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Timing in text-to-speech systems",
   "original": "e93_1397",
   "page_count": 8,
   "order": 12,
   "p1": "1397",
   "pn": "1404",
   "abstract": [
    "This paper discusses construction of the component of text to speech systems that is responsible for computing speech timing (the duration system). Currently, a variety of approaches is used, ranging from manually constructed sequential rule systems that incorporate a fair amount of linguistic knowledge, to systems that have been constructed by statistical means and that only minimally incorporate such knowledge. Recent developments in the availability of large labeled, segmented speech corpora seem to give the edge to statistical approaches. The paper discusses some general properties of timing in natural speech, and presents a concrete argument for why both linguistic knowledge and statistical analysis are essential. The challenge lies in designing systems that take optimal advantage of both. In addition, more knowledge is needed to resolve empirical issues raised by duration system construction.\n",
    "Keywords: speech timing, prosody\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-12"
  },
  "pieraccini93_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Esther",
     "Levin"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "Learning how to understand language",
   "original": "e93_1407",
   "page_count": 6,
   "order": 13,
   "p1": "1407",
   "pn": "1412",
   "abstract": [
    "In this paper we discuss learning paradigms for the problem of understanding spoken language. The basic idea consists in redefining the language understanding problem in terms of translation between a natural language and a formal language that represents the meaning of sentences. Within this framework, with the assumption that input and output sentences can be put into sequential correspondence, understanding can be seen as a problem of sequential transduction. In this case several techniques exist for learning the corresponding transducers, some of which can be properly stated in terms of Hidden Markov modeling (conceptual HMMs). If the sequential assumption does not hold, there are new algorithms that also seem able to solve the learning problem. This view of a language understanding system opens new perspectives in the field of automatic learning of language models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-13"
  },
  "ozawa93_eurospeech": {
   "authors": [
    [
     "Kazunori",
     "Ozawa"
    ],
    [
     "Masahiro",
     "Serizawa"
    ],
    [
     "Toshiki",
     "Miyano"
    ],
    [
     "Toshiyuki",
     "Nomura"
    ]
   ],
   "title": "M-LCELP speech coding at bit-rates below 4kbps",
   "original": "e93_0051",
   "page_count": 4,
   "order": 14,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "This paper presents the M-LCELP (Multi-mode Learned Code Excited LPC) speech coder, which has been developed for submission to the competition for setting the North American standards for half-rate digital cellular systems. M-LCELP develops several techniques to achieve high-quality synthetic speech. The MOS subjective test shows that 4.075kbps M-LCELP synthetic speech quality is high, equivalent to that of an 8kbps full-rate VSELP coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-14"
  },
  "lopezgonzalo93_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lopez-Gonzalo"
    ],
    [
     "Luis A.",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Fast vector quantization using neural maps for CELP at 2400bps",
   "original": "e93_0055",
   "page_count": 4,
   "order": 15,
   "p1": "55",
   "pn": "58",
   "abstract": [
    "The aim of the paper is to discuss fast vector quantization procedures for LPC parameters in CELP type coders at low bit rates. For this purpose, LPC representations will be based on the quantization properties of Self-Organizing Maps (SOM). These Maps provide an efficient representation of the LPC parameters that is exploited in two directions: a) to provide fast search procedures in VQ; and b) to obtain an efficient encoding of the LPC envelope. These two topics are discussed under a proposed 2400 bps CELP coder, that shows good quality in the reproduced speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-15"
  },
  "balss93_eurospeech": {
   "authors": [
    [
     "U.",
     "Balss"
    ],
    [
     "U.",
     "Kipper"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Dietrich",
     "Wolf"
    ]
   ],
   "title": "Improving the speech quality of CELP-coders by optimizing the long-term delay determination",
   "original": "e93_0059",
   "page_count": 4,
   "order": 16,
   "p1": "59",
   "pn": "62",
   "abstract": [
    "In this contribution we report on a method to control the delay determination in CELP coders, which improves the subjective speech quality without increasing the bit rate. Possible values of the delay of the long-term predictor were scored with respect to previous delay values. This scoring is done by weighting the closed-loop performance of the long-term predictor by means of a weighting factor, which depends on the difference between the previous delay value and the actual delay value. The final decision on the delay value is made on the basis of the weighted performances. Optimization and evaluation of this method were studied in simulation experiments using conventional CELP with stochastic excitation as well as ACELP with adaptive excitation at low data rates [2,3]. CELP coders using the new delay determination achieve a significant improvement of subjective speech quality. With an ACELP coder a good speech quality can be obtained even at very low data rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-16"
  },
  "garciamateo93_eurospeech": {
   "authors": [
    [
     "Carmen",
     "Garcia-Mateo"
    ],
    [
     "J. L.",
     "Alba-Castro"
    ],
    [
     "Luis A.",
     "Hernandez-Gomez"
    ]
   ],
   "title": "A stochastic speech coder with multi-band long-term prediction",
   "original": "e93_0063",
   "page_count": 4,
   "order": 17,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "In this contribution we present a CELP coder using a multi-band strategy to obtain its adaptive contribution. This procedure shows the advantage of a better representation of the quasi-periodic nature of the short-time spectrum. The frequency splitting is made using two-band QMF filters in order to avoid an excessive computational complexity. The proposed coder can be employed at a bit rate between 3 and 7 Kbps depending on the quantization techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-17"
  },
  "wery93_eurospeech": {
   "authors": [
    [
     "B. W. M.",
     "Wery"
    ],
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "Intelligibility evaluation of 4-5 kbps CELP and MBE vocoders: the hermes program experiment",
   "original": "e93_0067",
   "page_count": 4,
   "order": 18,
   "p1": "67",
   "pn": "70",
   "abstract": [
    "During the last years new promising techniques have emerged for voice coding at bit rates around 5 Kbps and below. These techniques include the Code Excited Linear Predictive vocoder (CELP) and the Multi Band Excited vocoder (MBE). The objective of this experiment was to select and specify a voice coding algorithm that could be used within the \"Voice Communication Assembly\" of the Hermes space plane. Because the basic principles of both vocoders are very different (analysis-by-synthesis and parametric), only a subjective intelligibility test offers a fair comparison. For this experiment, SAIT has implemented a CELP-based vocoder and a MBE-based vocoder using common design constraints (real-time full-duplex operation on a DSP TMS320C30 processor, 5 Kbps data stream including protection, and robust against up to 10\"^ burst BER). We performed extensive intelligibility tests on these systems. Our objective was to quantify the performance of the systems with clean and noise corrupted speech. These intelligibility tests were based on nonsense words (CVC or Consonant-Vowel- Consonant) and sentences (MOS and Speech Reception Threshold tests). The diversity of the tests makes it possible to validate them. It also simplifies the interpretation of the results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-18"
  },
  "dymarski93_eurospeech": {
   "authors": [
    [
     "P.",
     "Dymarski"
    ],
    [
     "N.",
     "Moreau"
    ]
   ],
   "title": "Algorithms for the CELP coder with ternary excitation",
   "original": "e93_0241",
   "page_count": 4,
   "order": 19,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "In this paper, several algorithms for the calculation of ternary excitation signals for CELP coders are analyzed. A new suboptimal algorithm for placing non-zero components in a sparse ternary excitation vector is proposed. This algorithm consists of successive minimizations of the angle between the perceptual speech vector and its model. It has been compared with the optimal algorithm, in which each permissible excitation vector is tested. The new algorithm offers a substantial complexity reduction with a very small decrease of speech quality. It can be also used in a CELP coder with orthogonalization of the ternary excitation vector relative to the long term prediction vector.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-19"
  },
  "mauc93_eurospeech": {
   "authors": [
    [
     "M.",
     "Mauc"
    ],
    [
     "G.",
     "Baudoin"
    ],
    [
     "M.",
     "Jelinek"
    ]
   ],
   "title": "Complexity reduction for federal standard 1016 CELP coder",
   "original": "e93_0245",
   "page_count": 4,
   "order": 20,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "In the past recent years, it appeared that sparse code books in CELP coders provide significant computation reduction without degrading the quality of the synthetic speech. We present a method that reduces the calculation complexity of the cross-correlation between the original speech p and the synthetic speech pin case of ternary valued samples (-1, 0, +1) code book sequences. After describing the method, we apply it to the Federal Standard 1016 CELP coder. We show that the computation reduction for calculating the cross-correlation can be up to 5.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-20"
  },
  "wuppermann93_eurospeech": {
   "authors": [
    [
     "F.",
     "Wuppermann"
    ],
    [
     "Christiane",
     "Antweiler"
    ],
    [
     "M.",
     "Kappelan"
    ]
   ],
   "title": "Objective analysis of the GSM half rate speech codec candidates",
   "original": "e93_0249",
   "page_count": 4,
   "order": 21,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "During the standardization process of the GSM half rate codec, extensive subjective listening tests were performed in November 1992 in order to select the final half rate codec candidate for the GSM system. In addition an unofficial objective analysis was performed by the authors using an objective measure for speech quality. This paper describes the fundamentals of this objective quality measure based on psychoacoustics, see also [1],[2]. A detailed comparison is given between the results of the subjective listening tests in terms of Mean Opinion Score (MOS) and the objective quality measure. A high correlation between both measures was observed for slightly distorted speech, a poorer correlation for larger distorted speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-21"
  },
  "gerson93_eurospeech": {
   "authors": [
    [
     "Ira A.",
     "Gerson"
    ],
    [
     "Mark A.",
     "Jasiuk"
    ]
   ],
   "title": "A 5600 BPS VSELP speech coder candidate for half-rate GSM",
   "original": "e93_0253",
   "page_count": 4,
   "order": 22,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "The bit rate for the speech and channel coder for the half-rate GSM channel is 11400 bps. A 5600 bps VSELP speech coder designed for this application is described. This speech coder is one of two candidates being considered by GSM for the half-rate channel. It employs a novel strategy for vector quantization of the reflection coefficients (ri), which combines high coding efficiency, low codebook search complexity, and low storage requirements. A computationally streamlined version of a zero-pole spectral noise weighting function is implemented. An adaptive pitch prefilter and an adaptive spectral postfilter are used to improve the speech coder's performance for both the tandemed and non-tandemed cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-22"
  },
  "kondoz93_eurospeech": {
   "authors": [
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ],
    [
     "M. R.",
     "Suddle"
    ]
   ],
   "title": "A speech coder for TV programme description",
   "original": "e93_0257",
   "page_count": 4,
   "order": 23,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In the last decade low bit rate speech coding research has received much attention resulting in newly developed good quality speech coders operating at as low as 4-8 kb/s. Although speech quality at around 8 kb/s is acceptable for a wide variety of telephony applications, more improvements in quality are necessary to make it acceptable to broadcast-type applications. In addition to the required low bit rate with acceptable speech quality, very low complexity decoders are necessary to provide a cost effective and compact solution. In this paper we describe a CELP speech coder to be used in audio description of television (AUDETEL). The quality of CELP coded speech has been improved significantly by a new codebook implementation which also simplifies the encoder/decoder complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-23"
  },
  "miki93_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Miki"
    ],
    [
     "Kazunori",
     "Mano"
    ],
    [
     "Hitoshi",
     "Ohmuro"
    ],
    [
     "Takehiro",
     "Moriya"
    ]
   ],
   "title": "Pitch synchronous innovation CELP (PSI-CELP)",
   "original": "e93_0261",
   "page_count": 4,
   "order": 24,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "This paper proposes a new speech coding method called PSI-CELP (Pitch Synchronous Innovation CELP). This method is based on CELP, but adds pitch synchronous innovation, which means that even random codevectors are adaptively converted to have pitch periodicity for voiced frames. In addition, this paper also proposes four other methods: (a) an FIR-type perceptual weighting filter using unquantized LPC parameters, (b) a fixed codebook that is prepared as part of the adaptive codebook for nonperiodic frames, (c) a random codebook which consists of two conjugate sub-codebooks, and (d) codebook searches using delayed decision. Due to these methods, SNR and subjective quality are significantly improved. These basic methods were adopted in the standard CODEC for Japanese half-rate digital mobile telephone service.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-24"
  },
  "moreno93_eurospeech": {
   "authors": [
    [
     "Asunción",
     "Moreno"
    ],
    [
     "José A. R.",
     "Fonollosa"
    ],
    [
     "Josep",
     "Vidal"
    ]
   ],
   "title": "Vocoder design based on HOS",
   "original": "e93_0519",
   "page_count": 4,
   "order": 25,
   "p1": "519",
   "pn": "522",
   "abstract": [
    "This paper deals with the application of Higher Order Statistics (HOS) in speech coding. A new model for speech production is implemented and the parameters of the vocal track are obtained from a linear combination of third order cumulants of the speech signal. Noisy signals have been used to test the system including colored Gaussian noise and sinusoidal interfering signals. Pitch and voicing decision are also obtained by HOS analysis of speech.\n",
    "Keywords: Cumulants, Speech coding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-25"
  },
  "sedgwick93_eurospeech": {
   "authors": [
    [
     "Nigel",
     "Sedgwick"
    ]
   ],
   "title": "Emulation of a formant vocoder at 600 and 800 bps",
   "original": "e93_0523",
   "page_count": 4,
   "order": 26,
   "p1": "523",
   "pn": "526",
   "abstract": [
    "A vocoder operating at a variety of low data rates has been implemented by software emulation. It uses Vector Quantisation (VQ), Variable Frame Rate Coding (VFR) with infil by replication and interpolation, and analysis by synthesis with a formant synthesis model. Experiments with a large talker-dependent codebook gives intelligible speech at 800 bps in context, and similar performance is expected with a talker-independent codebook. Intelligible speech at 600 bps is expected to be possible with further work. At 480 bps the speech is too slurred and further work using the same infil methods is not expected to give sufficient improvement.\n",
    "Keywords: speech coding, vocoder, formant vocoder, very low rate vocoder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-26"
  },
  "ma93_eurospeech": {
   "authors": [
    [
     "W.",
     "Ma"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "A pitch synchronized synthesizer for the IMBE vocoder",
   "original": "e93_0527",
   "page_count": 4,
   "order": 27,
   "p1": "527",
   "pn": "530",
   "abstract": [
    "Sinusoidal coding and Multi-Band Excitation (MBE) vocoders are very promising coding schemes, but require large computational complexity for generating the large number of sinusoids. This paper presents a synthesizer based on pitch period interpolation. As a result, it significantly reduces the computational requirement, in real time implementation. The computational load for a typical IMBE vocoder is reduced from 30% to 12% of the available DSP32C capacity. Since the pitch synchronized method mimics natural speech production, it produces better quality than the conventional sample by sample interpolation method.\n",
    "Keywords: Speech Coding, Speech Synthesizing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-27"
  },
  "dutoit93_eurospeech": {
   "authors": [
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Henri",
     "Leich"
    ]
   ],
   "title": "An analysis of the performances of the MBE model when used in the context of a text-to-speech system",
   "original": "e93_0531",
   "page_count": 4,
   "order": 28,
   "p1": "531",
   "pn": "534",
   "abstract": [
    "The use of a hybrid Hannonic/Stochastic model, such as the MBE one, is examined in the context of a High Quality TTS system (Fs=16 kHz). Analysis errors are studied in case of a direct analysis-synthesis scheme, and the exact responsibility of the analysis algorithm, rather than the model itself, is investigated. Through its application on well-known signals, it is found that: - Among the available analysis criteria, the Abrantes et al [2] approach slightly emerges, even though little audible improvements are obtained on real speech. - The MBE analysis of a single cosine with slowly tune-varying fundamental frequency introduces severe biases on its estimated amplitude and phase, especially for high central frequencies, while amplitude variations result in frequency-independent amplitude biases only.  These effects are due to the fact that a constant frequency and amplitude is assumed during the whole analysis frame. They are responsible for the existence of HF noise in synthesized speech. Keywords : Multi-Band analysis, Analysis Criteria, Time-Varying Parameters\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-28"
  },
  "chan93_eurospeech": {
   "authors": [
    [
     "C. F.",
     "Chan"
    ]
   ],
   "title": "High-quality synthesis of LPC speech using multiband excitation model",
   "original": "e93_0535",
   "page_count": 4,
   "order": 29,
   "p1": "535",
   "pn": "538",
   "abstract": [
    "This paper describes a method to achieve high-quality synthesis of speech from data coded in LPC-10 format. The method utilizes the Multi-Band Excitation (MBE) model for speech generation. In order to drive the MBE synthesizer, a set of voiced/unvoiced (V/UV) decisions for the pitch harmonics has to be regenerated from the coded LPC data. In this paper, we introduce a training and classification technique to extract the correlation information related to the spectrum envelope and the excitation. It was found that the speech spectrum envelope and the excitation are highly correlated, and a footprint of the V/UV mixture function can be extracted during the training stage and then stored alongside with the spectrum envelope information. This V/UV information can later be used to estimate a set of V/UV decisions for MBE synthesis. Unlike the conventional LPC speech which sounds buzzyness due to a single V/UV switch for the whole spectrum, it was demonstrated that the speech generated by the new method sounds very natural and of high quality.\n",
    "Keywords: Speech Synthesis, Multiband Excitation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-29"
  },
  "shoham93_eurospeech": {
   "authors": [
    [
     "Yair",
     "Shoham"
    ]
   ],
   "title": "High-quality speech coding at 2.4 kbps based on time-frequency interpolation",
   "original": "e93_0741",
   "page_count": 4,
   "order": 30,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "Speech coding by time-frequency interpolation (TFI) has recently been shown to offer high-quality coded speech at about 4 Kbps [1],[2]. This paper presents an improved TFI coder capable of delivering high-quality speech at 2.4 Kbps. The TFI method is briefly revisited and the structure of the coder is described. The performance of the 2.4 Kbps TFI coder is demonstrated by means of Mean Opinion Scores (MOS). It is shown that the coder performs almost the same as the full-rate standard coders GSM (13 Kbps) [9] and IS54 (8 Kbps)[6].\n",
    "Keywords: Speech Coding, Time-Frequency Interpolation, Vector Quantization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-30"
  },
  "marcato93_eurospeech": {
   "authors": [
    [
     "Luca",
     "Marcato"
    ],
    [
     "Enzo",
     "Mumolo"
    ]
   ],
   "title": "Coding of speech signal by fractal techniques",
   "original": "e93_0745",
   "page_count": 4,
   "order": 31,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "Some speech coding schemes are descripted in this paper. They are based on concepts related to the fractal theory. Although algorithms for compressing signals have already appeared in the literature, none of them is concentrated to the speech signal in particular. It is useful, instead, to take advantage of the knowledge concerning classical schemes of speech coding. The described algorithms are based on the fractal interpolation and on signal modeling by fractal related parameters. It will be shown that fractal techniques can successfully be used for coding speech.\n",
    "Keywords: fractal interpolation, fractal dimension, speech coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-31"
  },
  "asanuma93_eurospeech": {
   "authors": [
    [
     "Naomi",
     "Asanuma"
    ],
    [
     "Hiromi",
     "Nagabuchi"
    ]
   ],
   "title": "A new reference signal for evaluating the quality of speech coded at low bit rates",
   "original": "e93_0749",
   "page_count": 4,
   "order": 32,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "This paper proposes a reference signal for use in evaluating the speech quality of low bit-rate coded signals. The proposed signal is generated by adding a simulated low bit-rate coding distortion signal to the original signal. This low bit-rate coding distortion signal is modeled with both spectrum envelope distortion and additive white noise, so that the simulated coding distortion signal is synthesized using both Linear Predictive Coding (LPC) parameters derived from a signal who as spectrum envelope is distorted by spectrum-shaped noise, and by a LPC prediction error signal with added white noise. The simulated-signal parameters, the amount of spectrum-shaped noise and white noise, approximate the average characteristics of low bit-rate coding distortion signals. Experimental results show that the proposed reference signal suppresses variability in evaluation scores and improves their reproducibility in repeated assessment tests.\n",
    "Keywords: speech quality, low bit-rate coding, reference signal, modulated noise reference signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-32"
  },
  "ma93b_eurospeech": {
   "authors": [
    [
     "Changxue",
     "Ma"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A psychophysical study of fourier phase and amplitude coding of speech",
   "original": "e93_0753",
   "page_count": 4,
   "order": 33,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "A practical question in a Fourier transform coding of speech signals is to what accuracy their amplitude and phase spectra have to be represented without perceptible distortions. In this paper we are concerned with the audibility of these special types of quantization noise signals produced by coding Fourier amplitude and phase. From our psychophysical experiments, we found that the relative perceptual importance of phase and magnitude spectra in the Fourier transform coding is strongly dependent on the fundamental frequency of the vowel sounds and the window size used in the short time Fourier analysis.\n",
    "Keywords: Speech coding, Auditory masking.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-33"
  },
  "beautemps93_eurospeech": {
   "authors": [
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Rafael",
     "Laboissiere"
    ]
   ],
   "title": "Recovery of vocal tract midsagittal and area functions from speech signal for vowels and fricative consonants",
   "original": "e93_0073",
   "page_count": 4,
   "order": 34,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "This study deals with the ill-posed problem of inversion of the articulatory-to-acoustic relationship, i.e. the recovery of vocal tract geometry from formant frequencies. A small database of articulatory-acoustic data has been established for one subject. A midsagittal-to-area function conversion model, which works both for vowels and fricative consonants, has been developed from these data. This model has finally been used as a major constraint for an optimisation algorithm based on a gradient descent technique, in order to regularise the solution and solve the inversion problem. Other spatial and temporal smoothing constraints have been also used. Single configurations, as well as entire [VC] sequences could be recovered, using adequate initial conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-34"
  },
  "narayanan93_eurospeech": {
   "authors": [
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Abeer A.",
     "Alwan"
    ]
   ],
   "title": "Strange attractors and chaotic dynamics in the production of voiced and voiceless fricatives",
   "original": "e93_0077",
   "page_count": 4,
   "order": 35,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In this study, analysis techniques based on chaos theory were used to analyze far-field acoustic pressure waveforms of the fricatives /s/5 ///, /z/, and /$/. Results indicate the presence of low-dimensional chaos in the fricative time-series data. In addition, it was found that phase trajectories of the steady-state parts of the voiced fricatives were distinctly different from those of the voiceless ones; this observation suggests the potential use of phase trajectories as an effective voiced/voiceless classification technique for fricatives.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-35"
  },
  "nguyen93_eurospeech": {
   "authors": [
    [
     "Noel",
     "Nguyen"
    ],
    [
     "Philip",
     "Hoole"
    ]
   ],
   "title": "Frequency variations of the lowest main spectral peak in sibilant clusters",
   "original": "e93_0081",
   "page_count": 4,
   "order": 36,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "This work was aimed at analyzing the frequency variations of the lowest main spectral peak in the production of sibilants. According to the quantal theory of speech production, a continuous articulatory movement from /s/ to /f/ (or vice-versa) should provoke an abrupt and discontinuous change in the frequency of this peak. In the present work, a French native speaker was asked to produce consonant clusters of the type [sf] and [fs] combined with [a], [i] and [u]. Articulatory (electromagnetic) data and acoustic data were simultaneously recorded. First results show that variations of the acoustic output as a function of the location of the constriction are clearly non-linear when the surrounding vowels are unrounded. In the vicinity of [u] however, the acoustic transition between the two fricatives are more gradual, probably because [s] is produced in that context with a rounding of the lips which results in a lowering of the spectrum.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-36"
  },
  "loevenbruck93_eurospeech": {
   "authors": [
    [
     "Helene",
     "Loevenbruck"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Vocalic reduction : prediction of acoustic and articulatory variabilities with invariant motor commands",
   "original": "e93_0085",
   "page_count": 4,
   "order": 37,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "An inversion procedure from formant trajectories to central motor commands is presented here. It first involves the inference of vocal tract shapes by the inversion of an articulatory-to-acoustic model of the vocal tract. This many-to-one problem is solved by minimising the jerk of the articulatory parameters. Articulatory trajectories in different parts of the vocal tract are extracted from sagittal contours. Dynamic properties of speech articulators are then modelled by a second-order system, and articulatory movements are controlled by shifting the equilibrium position of the system. The inversion of this second model thus provides motor command sequences for a given articulatory trajectory. This procedure is applied to vowel reduction. The results suggest that the observed acoustic variability can be explained with an invariant motor command sequence associated with a specific dynamic parameterisation of the movement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-37"
  },
  "savariaux93_eurospeech": {
   "authors": [
    [
     "C.",
     "Savariaux"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "J. P.",
     "Orliaguet"
    ]
   ],
   "title": "Compensating for labial perturbation in a rounded vowel: an acoustic and articulatory study",
   "original": "e93_0089",
   "page_count": 4,
   "order": 38,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "We present an experiment in which lips are perturbed by a 20-mm diameter tube, for the production of the French vowel [u]. Nine naive speakers are analysed in 3 successive sessions: (1) without perturbation; (2) with the lip-tube; (3) with the lip-tube and with some information to help them to compensate. The analysis is based on X-ray views and formant patterns. Compensation abilities are evaluated by considering the first two formants. After the 2nd session, only one speaker could compensate the perturbation. All other speakers showed a strong increase of F2. In the 3rd session, behaviours of the speakers were observed depending on the given information: articulatory, acoustic or both. Results are analysed in the light of the classical control theories of speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-38"
  },
  "sock93_eurospeech": {
   "authors": [
    [
     "Rudolph",
     "Sock"
    ],
    [
     "Anders",
     "Löfqvist"
    ]
   ],
   "title": "Resistance of bilabials /p, b/ to anticipatory labial and mandibular coarticulation from vowel types /i, a, u/",
   "original": "e93_0541",
   "page_count": 4,
   "order": 39,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "The timing of lip and jaw movements and acoustic events was studied for one subject producing six Aku pseudo words, such as \"papa\" and \"baba\", at two speaking rates. Movement data from the upper and lower lips, and the jaw were obtained via an opto-electronic tracking device, together with a simultaneous audio recording. The acoustic duration of the medial bilabial closure, served as a temporal base for analyzing consonantal resistance to anticipatory vowel coarticulation. Within this temporal base, a phase was defined as the interval between peak raising or lowering velocity for an articulator and the acoustic onset of the following vowel. The proportion of time taken by each phase in the temporal base, provides a measure of anticipatory coarticulation of the vowel in the consonant. Results suggest that the rounded vowel /u/ has a higher coarticulatory influence on the labial consonant than the unrounded vowels /i, a/. Both voiced and the voiceless consonants have the same resistance to vowel coarticulation. Generally, all three articulators show structurally similar kinematic patterns.\n",
    "Keywords: resistance, anticipatory coarticulation, phases, kinematic, acoustic, patterns\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-39"
  },
  "jomaa93_eurospeech": {
   "authors": [
    [
     "Mounir",
     "Jomaa"
    ],
    [
     "Christian",
     "Abry"
    ]
   ],
   "title": "Jaw phasings and velocity profiles in arabic",
   "original": "e93_0545",
   "page_count": 4,
   "order": 40,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "The role of the jaw in rhythming quantity contrasts (VCC v.s. VVC) in Arabic was examined by monitoring vertical displacements, at two rates, for two Tunisians and two Kuweitis. Results show that at conversational rate, contrasts depend on differences in absolute timing along only one time domain: the cycle between two jaw lowering onsets. At fast rate, relative timing values no longer carry quantity contrasts for two speakers (one in each dialect) within this cycle, whereas the contrast is still present in the cycle between two jaw lowering peak velocities for all speakers. For velocity profiles the contrast is at best visible in the jaw closing gesture. VCCs are all asymmetrical at normal rate and the symmetric shape, characteristic of the VVC control is generally adopted at fast rate; except for the VCCs of the two speakers who resisted rate degradation in both cycles.\n",
    "Keywords: Jaw phasings, velocity profiles, Arabic, quantity contrasts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-40"
  },
  "olesen93_eurospeech": {
   "authors": [
    [
     "Morten",
     "Olesen"
    ]
   ],
   "title": "Derivation of the transfer function for a speech production model including the nasal cavity",
   "original": "e93_0549",
   "page_count": 4,
   "order": 41,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "The transmission part of an enhanced speech production model including the nasal tract is proposed. The discrete time system equivalent of the model is found based on the same acoustic assumptions as in the well known equivalency between a chain of tube sections and an AR-model. The transfer function is derived, but the expression is too complex to locate the singularities directly. It is possible though to determine the number of poles and zeros given the number of tube sections modelling the pharynx, the nasal tract and the upper oral tract respectively. Based on this knowledge a system identification tech- nique can be applied to the time domain implementation of the model to obtain an expression of the transfer function on normal form in which singularities are evident.\n",
    "Keywords: Speech production model, nasal cavity, articulatory speech analysis, ARMA\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-41"
  },
  "bavegard93_eurospeech": {
   "authors": [
    [
     "Mats",
     "Bavegard"
    ],
    [
     "Jesper",
     "Högberg"
    ]
   ],
   "title": "Using artificial neural nets to compare different vocal tract models",
   "original": "e93_0553",
   "page_count": 4,
   "order": 42,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "In this study artificial neural nets (ANN), relating articulation to acoustic features, are used as tools to investigate the ability of different vocal-tract models to describe the vocal tract. The vocal tract models are compared by means of a test material common to all evaluations. Different ANN configurations are used and investigated. Aspects of analysing natural speech signal with ANNs, trained on synthetic speech, are discussed. In addition, some effects of the speaker-dependency of the ANNs are investigated. The models are tested on synthetic speech (based on X-ray data of Swedish vowels) and natural speech from different speakers.\n",
    "Keywords: Artificial neural networks, speech signal inversion, vocal tract model, speaker-dependency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-42"
  },
  "foldvik93_eurospeech": {
   "authors": [
    [
     "Arne Kjell",
     "Foldvik"
    ],
    [
     "Ulf",
     "Kristiansen"
    ],
    [
     "Jorn",
     "Kvaerness"
    ]
   ],
   "title": "A time-evolving three-dimensional vocal tract model by means of magnetic resonance imaging (MRI)",
   "original": "e93_0557",
   "page_count": 2,
   "order": 43,
   "p1": "557",
   "pn": "558",
   "abstract": [
    "The purpose of this paper is not primarily to present research results on articulation but to show how acquisition by means of Magnetic Resonance Imaging (MRI) of repeated utterances of a word can be used to construct a three-dimensional time-evolving model of the articulatory tract, showing the changes that the shape of the vocal tract undergoes during an utterance.\n",
    "Keywords: Speech modelling, Magnetic Resonance Imaging (MRI).\n",
    ""
   ]
  },
  "schroeter93_eurospeech": {
   "authors": [
    [
     "Juergen",
     "Schroeter"
    ],
    [
     "Bert",
     "Cranen"
    ]
   ],
   "title": "Physiologically-motivated modeling of the voice source in articulatory analysis/synthesis",
   "original": "e93_0095",
   "page_count": 4,
   "order": 44,
   "p1": "95",
   "pn": "98",
   "abstract": [
    "This paper describes the implementation of a new parametric model of the glottal geometry aimed at improving female (and male) speech synthesis in the framework of articulatory analysis!synthesis of speech. The model is controlled by parameters that are tightly coupled to physiology, such as, for example, vocal-fold abduction. It is imbedded in an articulatory analysis/synthesis system (articulatory speech mimic). To introduce naturally-occurring details in our glottal flow waveforms, we included two different kinds of glottal leakage in our model: a \"linked leak\" and a \"parallel glottal chink\". While the first is basically an incomplete glottal closure that results in, among other things, a steeper roll-off of the glottal flow spectrum, the latter models a second glottal duct that is independent of the membranous (vibrating) part of the glottis. Our simulations show that, as far as deterministic excitation (not the glottal noise) is concerned, a parallel glottal chink can actually enhance spectral levels at high frequencies relative to the no-leakage case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-43"
  },
  "oliveira93_eurospeech": {
   "authors": [
    [
     "Luis C.",
     "Oliveira"
    ]
   ],
   "title": "Estimation of source parameters by frequency analysis",
   "original": "e93_0099",
   "page_count": 4,
   "order": 45,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "The new generation of text-to-speech systems needs the ability to control the voice quality of the synthesized speech by varying the excitation source. This feature is fundamental to improve naturalness and to synthesize female or child voices. The variation of the voice quality is also important when trying to synthesize expression. The problem involves two aspects: the ability to control the source parameters of the speech synthesizer and the possibility of extracting these parameters from natural speech. This paper describes a source model based on the poly-nomial model for the glottal flow suggested by Rosenberg [9] that has an exact representation in the frequency domain, and an automated procedure to estimate its parameters from natural speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-44"
  },
  "strik93_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Fitting a LF-model to inverse filter signals",
   "original": "e93_0103",
   "page_count": 4,
   "order": 46,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "A method is presented for the automatic extraction of voice source parameters from speech. An automatic inverse filtering algorithm is used to obtain an estimate of the glottal flow signal. Subsequently, an LF-model [1] is fitted to the glottal flow signal. In the current article we will focus on the improvement of the automatic fit procedure. To keep track of the performance of the fit procedure, a quantitative evaluation criterion is preferred. It is difficult to obtain such a criterion for natural speech. Therefore, we propose an evaluation method in which synthetic speech is used. We also conducted qualitative tests for disturbances that are often found in natural speech, i.e. source-filter interaction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-45"
  },
  "schoentgen93_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Modelling the glottal pulse with a self-excited threshold auto-regressive model",
   "original": "e93_0107",
   "page_count": 4,
   "order": 47,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "A novel glottal pulse model is presented. Conventional glottal pulse models are a concatenation of curves. For instance, the Liljencrants-Fant model(*) assumes that a typical pulse is made up of a raised cosine multiplied by a growing exponential (curve I) and a decaying exponential (curve II). The switch from curve I to II occurs at time instant T1 when the slope of the closing phase is at its most negative. The switch-back occurs at pulse onset. Curve I is a solution of a second-order and curve I a solution of a first-order linear differential equation. Therefore, the LF-model can be physically interpreted in terms of a linear parametric oscillator. Indeed, a parametric oscillator is driven by periodically changing one or more of its parameters. In this article, we address the following two problems. First, the LF-model is at odds with established theory. Indeed, the aerodynamic-myoelastic theory of vocal fold vibration maintains that the laryngeal oscillator is not driven but self-sustained. Second, the LF-model must be fitted pitch-synchronously. This means that before curves I and II can be adjusted to a given pulse, the pulse has to be preprocessed so as to determine the onsets of the opening and return phases. This is considered to be an annoying technical problem because no algorithm exists that can reliably extract this kind of information from the pulse under all circumstances. We show that these problems can be solved by turning the LF-model into a self-sustained oscillator. This is carried out by letting the switch from curve I to II and back depend on signal amplitude instead of time. In the statistics literature, such a model is known as a (S)elf-(E)xcited (T)hreshold (A)uto(R)egressive model. This model has been developed to represent time-series that are output by nonlinear systems that sustain self-excited vibrations. We show that: First, a SETAR-model can be pitch-asynchronously fitted to a glottal pulse. Second, a SETAR-model exhibits self-sustained oscillations. Third, a SETAR-model can easily be turned into an LF-model In other words, it is an intermediate step in an algorithm that fits a LF-model pitch-asynchronously. We fitted the SETAR-model to glottal pulses obtained by glottal inverse filtering. Results show that the quality of the fit is similar to the one achieved by the LF-model (*) Strictly speaking, the original LF-model represents the derivative of the glottal pulse. It is obtained by differentiating curves I & II and by taking into account that a is much smaller than co.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-46"
  },
  "denzler93_eurospeech": {
   "authors": [
    [
     "J.",
     "Denzler"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Going back to the source: inverse filtering of the speech signal with ANNs",
   "original": "e93_0111",
   "page_count": 4,
   "order": 48,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "In this paper we present a new method transforming speech signals to voice source signals (VSS) using artificial neural networks (ANN). We will point out that the ANN mapping of speech signals into source signals is quite accurate, and most of the irregularities in the speech signal will lead to an irregularity in the source signal, produced by the ANN (ANN-VSS). We will show that the mapping of the ANN is robust with respect to untrained speakers, different recording conditions and facilities, and different vocabularies. We will also present preliminary results which show that from the ANN source signal pitch periods can be determined accurately.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-47"
  },
  "leandro93_eurospeech": {
   "authors": [
    [
     "Manuel A.",
     "Leandro"
    ],
    [
     "Jose M.",
     "Pardo"
    ]
   ],
   "title": "Low cost speaker dependent isolated word speech preselection system using static phoneme pattern recognition",
   "original": "e93_0117",
   "page_count": 4,
   "order": 49,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "We present here a system that builds up a phonetic string from speech at its input. The front-end system has been designed in terms of low cost computation for easy real time implementation, easy and automatic speaker enrollment and low use of linguistic knowledge. It is compared with Olivetti's phonetic string build up system adapted for Spanish in ESPRIT Polyglot project, showing that it is competitive in terms of computation and recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-48"
  },
  "lamel93b_eurospeech": {
   "authors": [
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "High performance speaker-independent phone recognition using CDHMM",
   "original": "e93_0121",
   "page_count": 4,
   "order": 50,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "In this paper we report high phone accuracies on three corpora: WSJO, BREF and TIMIT. The main characteristics of the phone rec- ognizer are: high dimensional feature vector (48), context- and gender-dependent phone models with duration distribution, continuous density HMM with Gaussian mixtures, and n-gram probabilities for the phonotatic constraints. These models are trained on speech data that have either phonetic or orthographic transcriptions using maximum likelihood and maximum a posteriori estimation techniques. On the WSJO corpus with a 46 phone set we obtain phone accuracies of 72.4% and 74.4% using 500 and 1600 CD phone units, respectively. Accuracy on BREF with 35 phones is as high as 78.7% with only 428 CD phone units. On TIMIT using the 61 phone symbols and only 500 CD phone units, we obtain a phone accuracy of 67.2% which correspond to 73.4% when the recognizer output is mapped to the commonly used 39 phone set. Making reference to our work on large vocabulary CSR, we show that it is worthwhile to perform phone recognition experiments as opposed to only focusing attention on word recognition results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-49"
  },
  "gauvain93_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "M.",
     "Adda-Decker"
    ]
   ],
   "title": "Speaker-independent continuous speech dictation",
   "original": "e93_0125",
   "page_count": 4,
   "order": 51,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "In this paper we report progress made at LIMSI in speaker-independent large vocabulary speech dictation using newspaper speech corpora. The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling. Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models. Two corpora of read speech have been used to carry out the experiments: the DARPA Wall Street Journal-based CSR corpus and the BREF corpus containing recordings of texts from the French newspaper Le Monde. For both corpora experiments were carried out with up to 20K word lexicons. Experimental results are also given for the DARPA RM task which has been widely used to evaluate and compare systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-50"
  },
  "schukattalamazzini93_eurospeech": {
   "authors": [
    [
     "Ernst G.",
     "Schukat-Talamazzini"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Wieland",
     "Eckert"
    ],
    [
     "T.",
     "Kuhn"
    ],
    [
     "S.",
     "Rieck"
    ]
   ],
   "title": "Automatic speech recognition without phonemes",
   "original": "e93_0129",
   "page_count": 4,
   "order": 52,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Our paper addresses the problem of acoustic-phonetic modelling for large-vocabulary, speaker-independent continuous speech recognition. We propose an entirely new sub word unit for word modelling - the \"polygraph\". Polygraphs are essentially letters-in-context, and similarly to polyphone speech units [9], they allow left and right context strings of arbitrary length. As polygraphs are constructed from the orthographic word form, no reference to phonetic word transcriptions has to be made. Thus, a spoken language system based on polygraphs can be created without the cumbersome process of phonetically transcribing the items of large vocabularies, and the acquisition of \"new\" words becomes straightforward, provided the word spellings are known. In this paper, our techniques of phone-based contextual modelling along with the extension to the letter-based approach is described and detailed analyses of training and test data coverage with respect to subword speech units are given. A speaker-independent, 1081-word continuous utterance test with the ISADORA system yielding a word accuracy of 79.8% demonstrates the feasibility of automatic speech recognition without phonemes. Under the condition of vocabulary-independent training we observed 86.6% (97.4%) words to be correctly identified in speaker-independent 900-word isolated word mode if only the best (the top-10) word alternatives are considered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-51"
  },
  "seino93_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Seino"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Spoken language identification using ergodic HMM with emphasized state transition",
   "original": "e93_0133",
   "page_count": 4,
   "order": 53,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "This paper describes an automatic language identification method based on HMMs (Hidden Markov Models) for acoustic features. The hidden Markov modeling is used to represent the dynamics of the states of the vocal tract. We note here that each language has its proper phonotactics. For the experiment of identification, utterances in 4 languages (English, Japanese, Mandarin Chinese and Indonesian) were modeled by several HMMs. They were uttered by 15 male speakers (10 for training the HMM and 5 for testing) for each language. These trained HMMs showed considerable inter-language variations. The HMM topology used here is a fully structured (ergodic) model in which any state could transit to all other states includeing itself. Here, we used 2 kinds of HMMs: the discrete HMM with the codebook and the continuous density HMM. The HMM was trained using both the Baum-Welch (Forward-Backward) algorithm and the Viterbi algorithm. The latter was used for emphasizing and extracting the state transition. The extracted state sequence was modeled by the 2-nd order Markov model (tri-gram). For comparison, we also experimented on the identification using the VQ (Vector Quantization) distortion and the CMDF (Continuous Mixture Density output probability Functions). The results showed that the combined method of CHMM and tri-gram identified 4 languages very well (the best correct identification rate was 90.3%). Keywords; language identification, ergodic HMM, phonotactics, optimal state sequence, trigram.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-52"
  },
  "apolloni93_eurospeech": {
   "authors": [
    [
     "Bruno",
     "Apolloni"
    ],
    [
     "Dario",
     "Crivelli"
    ],
    [
     "Marco",
     "Amato"
    ]
   ],
   "title": "Neural time warping",
   "original": "e93_0139",
   "page_count": 4,
   "order": 54,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "We try to capture subsymbolically through a recurrent neural network the structure of a word utterance in terms both of the dynamic properties of the process generating it [positive knowledge] and of a companion hidden process which declares in its final state the inconsistency of that utterance with some deceiving candidate generating processes [negative knowledge]. Namely, we work on a bench of neural networks. Each one was trained on a set of template utterances of a same word for generating a trajectory of the Mel-cepstral parameters which get close to that of the training word and an adviser signal which is frustrated in its growing when the net runs on some misleading words. We use the generalization capability of that network for adapting the output trajectory to the utterances under recognition. This gives rise to a neural time warping which stretches or compresses the template signal in function of the actual utterance, unlike the usual time warpers which work on the current vs template utterance. A proper two-phase training strategy is developed. Classifying the word with the label of the better warping network not rejected by the adviser signal gives rise to a rate of success about 96% on a speaker independent vocabulary of the ten digits.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-53"
  },
  "cerf93_eurospeech": {
   "authors": [
    [
     "Philippe Le",
     "Cerf"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Speaker independent small vocabulary speech recognition using MLPs for phonetic labeling",
   "original": "e93_0143",
   "page_count": 4,
   "order": 55,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "This paper describes a hybrid speech recognition system using MLPs and HMMs, Instead of the common use of MLPs as probability generators, we propose to use MLPs as labelers for discrete parameter HMMs, Compared to the probabilistic approach, this has the advantages of having much more flexibility in our system design (e.g. no limitation to phonetic models) and being able to use much smaller MLPs, Compared to Euclidean labeling, our approach has the advantage of needing fewer EMM parameters per state while achieving greater recognition accuracy. Our recent results show that the best configuration is to use four independent MLPs as labelers. The results can be improved by training the MLPs for subphoneme classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-54"
  },
  "drygajlo93_eurospeech": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Multiresolution time-sequency speech processing based on orthogonal wavelet packet pulse forms",
   "original": "e93_0147",
   "page_count": 4,
   "order": 56,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "In this paper we present orthogonal transforms as a signal analysis and processing tool with the capability of variable multiresolution time-spectral decomposition of speech signals. Our prime interest is in a wavelet packet representation and processing of the discrete speech signal and we concentrate on fast orthogonal transform algorithms for their efficient implementation. The relationships between wavelet packet representation and multiresolution multirate filter banks are discussed. Two new concepts based on presented algorithms are described. A first one, is of a multiresolution multicarrier modulation. It is suggested for transmultiplexers whose structures depend on nonuniform subband decomposition and scrambling of sub-band signals. A second one, is of time-varying orthogonal transforms. Its basis functions are adjusted to best match the speech signal in the sense of the criterion employed, and it explores the speech signal behavior simultaneously in the time and spectral domains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-55"
  },
  "ambikairajah93_eurospeech": {
   "authors": [
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "M.",
     "Keane"
    ],
    [
     "L.",
     "Kilmartin"
    ],
    [
     "G.",
     "Tattersall"
    ]
   ],
   "title": "The application of the wavelet transform for speech processing",
   "original": "e93_0151",
   "page_count": 4,
   "order": 57,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "This paper describes the use of the wavelet transform as a front-end processor for a speech analysis system. The constant relative bandwidth analysis performed by the wavelet transform is compared to the performance of a cochlear model, which is simulated using cascaded filters. Multi-sampling rate implementations of both models are described. Several spectral plots are presented to illustrate the properties of constant bandwidth analysis and to compare the performance of the models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-56"
  },
  "iwahashi93_eurospeech": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Duration modelling with multiple split regression",
   "original": "e93_0329",
   "page_count": 4,
   "order": 58,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "In this paper, statistical segmental duration modelling is proposed for English speech synthesis using Multiple Split Regression ( MSR ) and a hierarchical error function. To realize duration control by statistical method according to characteristics of English duration: interactions between control factors and hierarchical structure of timing, a suitable statistical modelling method is desired. MSR is a statistical modelling method which has data driven dynamic structure with combinatorial optimization technique. It incorporates both linear and tree regressions as special cases, and extends them. It can express phenomena of interaction between control factors for duration properly. The hierarchical predictive error function is adopted to analyze hierarchical structure of duration control in syllable and segmental levels. Experimental results show that MSR obtains higher values of multiple correlation than either linear or tree regressions with the same number of free parameters. Moreover, the error analysis by hierarchical predictive error function shows that interactions exist between factors at segmental and syllable levels in duration control, and that predictive errors at segmental duration are compensated in a syllable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-57"
  },
  "altmann93_eurospeech": {
   "authors": [
    [
     "Gerry T. M.",
     "Altmann"
    ],
    [
     "Duncan",
     "Young"
    ]
   ],
   "title": "Factors affecting adaptation to time-compressed speech",
   "original": "e93_0333",
   "page_count": 4,
   "order": 59,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "Speech inputs vary widely in amount of background noise, speaking rate, and accent; yet listeners can quickly adapt to such variation and recognize the content of an utterance. The present study is aimed at understanding the mechanisms underlying this adaptation, and the processing unit which the human recognition system attempts to recover during adaptation. We examined adaptation to speech under various conditions of time compression. Specifically, we explored how the intelligibility of a target set of compressed English sentences varied as a function of prior exposure to three kinds of compressed stimuli: compressed English, compressed French, and in a separate study, compressed \"nonsense\" sentences. We also explored the degree to which the adapting effects of prior exposure to compressed speech persist over time. We conclude on the basis of the results that lexical level word recognition does not drive the adaptation mechanism. Instead, recognition of sub-lexical units, or suprasegmental regularities in rhythm, may form the basis for successful, and persistent, adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-58"
  },
  "roelands93_eurospeech": {
   "authors": [
    [
     "Marc",
     "Roelands"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Waveform similarity based overlap-add (WSOLA) for time-scale modification of speech: structures and evaluation",
   "original": "e93_0337",
   "page_count": 4,
   "order": 60,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "A synchronization criterion for overlap-add time-scale modification is derived through a least squares estimation of the modified short-time Fourier transform. Based on this finding, a structural time-domain framework for time-scale modification is described. One efficient variant, which was called the Waveform Similarity based Overlap-Add (WSOLA) method, produces high quality output when applied to speech, but can even be applied successfully to a broader class of signals, including multiple voices together and musical instruments. Fine-tuning the synchronization criterion, without affecting the high quality that is obtained, can make the computational cost very low, revealing the versatile possibilities for on-line operation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-59"
  },
  "wang93_eurospeech": {
   "authors": [
    [
     "Hsiao-Chuan",
     "Wang"
    ],
    [
     "Hsiao-Fen",
     "Pai"
    ]
   ],
   "title": "A study on the weighting factors of two-dimensional cepstral distance measure",
   "original": "e93_0341",
   "page_count": 4,
   "order": 61,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "This paper studies the weighting factor of the two-dimensional cepstrum (TDC) distance measure. By using the inverse standard deviation of feature as weighting factor, a vector quantization (VQ) based speech recognition method is used to recognize TDC speech pattern. The advantage of this speech recognition method is its simple complexity in implementation. The effect of the weighting factors on the TDC distance measure is also investigated. It shows that the performance of the inverse standard deviation is about 2% better than the one derived from quefrency weighting and time derivative weighting.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-60"
  },
  "kamp93_eurospeech": {
   "authors": [
    [
     "Yves",
     "Kamp"
    ],
    [
     "Changxue",
     "Ma"
    ]
   ],
   "title": "Connection between weighted LPC and higher-order statistics for AR model estimation",
   "original": "e93_0345",
   "page_count": 3,
   "order": 62,
   "p1": "345",
   "pn": "347",
   "abstract": [
    "This paper establishes the relationship between a weighted linear prediction method used for robust analysis of voiced speech and the autoregressive modelling based on higher-order statistics, known as cumulants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-61"
  },
  "chibelushi93_eurospeech": {
   "authors": [
    [
     "C. C.",
     "Chibelushi"
    ],
    [
     "J. S.",
     "Mason"
    ],
    [
     "R.",
     "Deravi"
    ]
   ],
   "title": "Integration of acoustic and visual speech for speaker recognition",
   "original": "e93_0157",
   "page_count": 4,
   "order": 63,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "This paper describes a speaker recognition system that uses both acoustic speech and visual speech (motion of visible articulators). Integration of acoustic and visual speech aims at improving recognition performance with regard to recognition accuracy, robustness against variability of input data, and protection against impersonation. As an initial step towards this goal, voice has been used together with still face images; this combination of vocal and facial information has resulted in better recognition accuracy than from either of the two constituents individually.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-62"
  },
  "montacie93_eurospeech": {
   "authors": [
    [
     "Claude",
     "Montacie"
    ],
    [
     "Jean-Luc Le",
     "Floch"
    ]
   ],
   "title": "Discriminant AR-vector models for free-text speaker verification",
   "original": "e93_0161",
   "page_count": 4,
   "order": 64,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "In this paper, a new free-text speaker verification method is proposed. This method uses a modeling of the spectral evolution of the speech signals, which is capable of processing some aspects of the inter-speaker variability : the AR-Vector models. To learn Discriminant AR-Vector models, three training techniques are proposed. These techniques are used to reduce the verification computation cost. The difficult choice of the impostor rejection criterion in the verification systems is discussed. The evaluation of this method is carried out on the TIMIT database recorded by cooperative speakers without impostor. A series of free-text speaker verification experiments are described. There is no specific corpus for the training sentences and the training corpus is different from the test corpus. The verification tests are made in the difficult conditions: for each experiment, a speaker is taken out of the database and is played impostor posing as the all database speakers. The experiments give first-rate results (i.e, verification rate of 99.8% for 630 speakers) without using more than two sentences for each test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-63"
  },
  "thompson93_eurospeech": {
   "authors": [
    [
     "J.",
     "Thompson"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "Within class optimization of cepstra for speaker recognition",
   "original": "e93_0165",
   "page_count": 4,
   "order": 65,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "By identifying highly speaker specific aspects of cepstral features, the task of speaker recognition can be potentially simplified. The identification process can be performed both phonetically and in the cepstral domain. The phonetic analysis aims to determine the temporal aspects of utterances which exhibit the highest degree of speaker specificity, while the cepstral analysis examines individual cepstra within these temporal divisions. This paper aims to complement work that has already been conducted on a phonetic basis, by performing analysis upon the individual cepstral coefficients within utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-64"
  },
  "bimbot93_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Luc",
     "Mathan"
    ]
   ],
   "title": "Text-free speaker recognition using an arithmetic-harmonic sphericity measure",
   "original": "e93_0169",
   "page_count": 4,
   "order": 66,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This paper presents theoretical aspects and experimental results relative to the use of an arithmetic-harmonic sphericity measure on covariance matrices, for text-free speaker recognition. The method is easy to implement, and computationally efficient. It provides remarkable results for the 420 speakers of the TIMIT database : performances of 98.5 % and 100 % correct identifications are achieved for test utterances of durations approximately equal to 3 and 15 seconds respectively. Cet article presente des aspects theoriques et des resultats experimentaux relatifs a l'utilisation d'une mesure de sphericite arithmetico-harmonique sur des matrices de covariance, en reconnaissance du locuteur independante du texte. La methode proposee est facile a metue en oeuvre, et peu couteuse en temps de calcul. Elle produit des resultats remarquables pour les 420 locuteurs de la base de donnees TIMIT : des performances de 98.5 % et 100 % d'identifica- tions correctes sont obtenues pour des tests de durees respectives d'environ 3 secondes et 15 secondes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-65"
  },
  "moreno93b_eurospeech": {
   "authors": [
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Dolors",
     "Poch"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Joaquim",
     "Llisterri"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Albayzin speech database: design of the phonetic corpus",
   "original": "e93_0175",
   "page_count": 4,
   "order": 67,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "This paper describes the phonetic content of Albayzin, a spoken database for Spanish designed for speech recognition purposes. A statistical study of a large sample of spontaneous speech is presented, and the phonetic and statistical criteria for the final constitution of the database are discussed. Finally, the contents of the phonetic database are analyzed\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-66"
  },
  "ribeiro93_eurospeech": {
   "authors": [
    [
     "Carlos",
     "Ribeiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Antonio",
     "Serralheiro"
    ]
   ],
   "title": "A software tool for speech collection, recognition and reproduction",
   "original": "e93_0179",
   "page_count": 4,
   "order": 68,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "The SAR system was designed for collecting, recognizing and reproducing speech. A user friendly environment, based on oral prompts, allows its use both locally or through the telephone network. This paper describes the architecture and functionality of this system, with emphasis on the software developped for the PC and the DSP board, and its application to the collection, training and test of an isolated digit recognizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-67"
  },
  "karjalainen93_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "An object-oriented database for speech processing",
   "original": "e93_0183",
   "page_count": 4,
   "order": 69,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "An object-oriented speech database system is described that allows automatic persistent storage and updating of almost arbitrary data types and relations describing speech. The system is programmed in Common Lisp and CLOS for flexibility and easy extendibility. It allows for powerful search, analysis, and modification of database contents. A phonetic representation framework for Finnish is available where the hierarchical structure of speech is easily manipulated. Deferred loading and caching of data to working memory allows for fast computation and effective memory usage even with very large databases. A graphical user interface supports manual interaction and transcription tools. A semiautomatic transcription system is under development.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-68"
  },
  "chan93b_eurospeech": {
   "authors": [
    [
     "Dominic S. F.",
     "Chan"
    ],
    [
     "Adrian J.",
     "Fourcin"
    ]
   ],
   "title": "Automatic annotation using multi-sensor data",
   "original": "e93_0187",
   "page_count": 4,
   "order": 70,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "The aim of this work is to provide a set of robust references which can be used for the phonetic annotation of both normal and pathological speech. The approach adopted is to make use of sensors of speech productive activity which give outputs which are of auditory, contrastive, value. An apparatus for multi-sensor recording and the physiological characteristics of the sensors in responding to speech production are described. Signal characteristics for different speech sound categories and speakers are presented. The performance of the sensors on continuous speech is assessed. Finally, a framework for the use of the sensor signals for (semi-)automatic annotation and labelling of dialogue is described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-69"
  },
  "draxler93_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Hans G.",
     "Tillmann"
    ],
    [
     "Barbara",
     "Eisen"
    ]
   ],
   "title": "Prolog tools for accessing the phondat database of spoken German",
   "original": "e93_0191",
   "page_count": 4,
   "order": 71,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "The PhonDat project within the German VERBMOBIL research initiative aims at creating and making accessible a very large database of symbolic and signal data of spoken high German. Currently, the PhonDat database consists of one corpus of sentences containing all phoneme combinations of high German, and of one corpus of sentences from a train enquiries scenario. All symbolic data is held in a Prolog system with a powerful database management system extension; signal data is stored in external files. The database is accessed through queries over the symbolic data. The result of a query evaluation is either again symbolic data, or a reference to signal files and signal fragments within these files. Two access modes are supported: a toolbox of pre-defined high-level query predicates for standard, albeit complex, queries; and the full Prolog programming language for custom applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-70"
  },
  "jekosch93b_eurospeech": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "Cluster-similarity: a useful database for speech processing",
   "original": "e93_0195",
   "page_count": 4,
   "order": 72,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "People working on spoken language technology are, amongst other things, confronted with the multiformity of the surface phenomenon speech. Automatic speech recognition is extremely difficult because there is a strong inter- and intra-speaker variability. People speak different in different situations. Developers of speech synthesis systems try to map that multiformity of natural speech onto automatically generalized speech. For both fields, speech recognition and speech synthesis, it is of central importance to understand the underlying principles of speech production and perception. Different scientific disciplines have contributed to collect information on this issue. In this paper a psycho-acoustic approach is described. Similarity profiles representing spaces of perceptual distinction are presented: Profile A is based on judgements gained in an introspective way, Profile B visualizes judgements on natural speech, and Profile C on synthetic speech. The study shows that there are quite severe differences compared to natural speech. The paper will concentrate on describing the perceptual dimensional representations of natural and synthetic speech. Data are compared and interpreted with regard to their role in synthesis assessment. A detailed analysis of test results will give some indications of why speech synthesizers often suffer from intelligibility and acceptability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-71"
  },
  "castagneri93_eurospeech": {
   "authors": [
    [
     "G.",
     "Castagneri"
    ],
    [
     "G. Di",
     "Fabbrizio"
    ],
    [
     "A.",
     "Massone"
    ],
    [
     "M.",
     "Oreglia"
    ]
   ],
   "title": "SIRVA - a large speech database collected on the Italian telephone network",
   "original": "e93_0199",
   "page_count": 3,
   "order": 73,
   "p1": "199",
   "pn": "201",
   "abstract": [
    "A speech database was collected over the Italian Public Switched Telephone Network from more than 2000 telephone customers. An automatic workstation has been developed in order to perform the collection; it was connected to two speech recognizers that have been tested on-line during the database collection. Results and solutions to different problems faced during the collection are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-72"
  },
  "steeneken93_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "J. A.",
     "Verhave"
    ],
    [
     "Tammo",
     "Houtgast"
    ]
   ],
   "title": "Objective assessment of speech communication systems; introduction of a software based procedure",
   "original": "e93_0203",
   "page_count": 4,
   "order": 74,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "The Speech Transmission Index (STI) [5], and the Room Acoustical STI (RASTI) [2], have been internationally accepted as objective standards for the prediction of the intelligibility of speech communication channels, [3]. Recently, the applicability, diagnostic power, and prediction accuracy of these indices have been further extended [7]. These extensions include the possibility to predict the speech intelligibility for both male and female speech, and for individual phoneme groups (plosives, fricatives, vowel-like consonants, and vowels). Based on the improved algorithm and on the experience obtained with the existing STI-measuring devices (25 systems were installed in other laboratories), a new development was initiated for a software based measuring system with standard equipment (personal computer with data-acquisition). This resulted in a software package which is system independent. The paper describes the improvement of the STI, the new measuring method, and will also give some examples of the applications (electro-acoustical transducers, and a telephone communication channel). The method can also be used to specify the input conditions for speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-73"
  },
  "danielsen93_eurospeech": {
   "authors": [
    [
     "Sven W.",
     "Danielsen"
    ]
   ],
   "title": "Enhanced direct assessment of speech input systems within the SAM-a esprit project",
   "original": "e93_0207",
   "page_count": 4,
   "order": 75,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "The paper presents the most recent enhancements of the standard methodologies for direct assessment of speech input systems which are developed and used within the European Community ESPRIT Project 6819 \"Speech Technology Assessment in Multilingual Applications\", SAM-A. The aim of the SAM-A project is to continue the development of methods, tools and databases to support performance assessment of speech input, output and speaker verification systems, based on the work performed in the now concluded well-known ESPRIT Project 2589, SAM. Focus in this paper is on the tools for direct assessment of speech input systems first developed in the SAM project, with an emphasis on the improvements already made and recognised improvements needed for execution of automated assessment of the more advanced recognisers, now being supported within SAM-A. Project participants have selected a number of speech recognition systems that will be employed as target recognisers in assessment experiments, which are briefly described in the paper as well.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-74"
  },
  "nicolas93_eurospeech": {
   "authors": [
    [
     "Pascale",
     "Nicolas"
    ],
    [
     "Pascal",
     "Romeas"
    ]
   ],
   "title": "Evaluation of prosody in the French version of multilingual text-to-speech synthesis: neutralising segmental information in preliminary tests",
   "original": "e93_0211",
   "page_count": 4,
   "order": 76,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "This study was carried out in the scope of a cooperation between INFOVOX AB, Sweden, and Laboratoire \"Parole et Langage\" URA CNRS 261, France. Our purpose is to quantify how far the prosody of a Text-to-Speech (TTS) system is perceived from French prosodic structures. It is assumed that neutralising all the segmental information is an important methodological precaution allowing to determine whether it is legitimate to study prosodic parameters of TTS systems regardless to any segmental aspect of speech. In order to test the discriminatory power of merely pitch and pauses in the task of distinguishing between synthetic and natural speech, the spectral information of the original signal is reduced to a steady amplitude synthetic [a] which has the same length and Fo values as the original utterances. The evaluation shows that significantly different scores are assigned to natural and synthetic spectrum-reduced items. The identification of acceptable and faulty synthetic patterns produces a two mode distribution of scores. Yet the discriminatory power of prosodic features varies according to specific TTS applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-75"
  },
  "saliu93_eurospeech": {
   "authors": [
    [
     "Sokol",
     "Saliu"
    ],
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Yasuo",
     "Endo"
    ],
    [
     "Yoshinobu",
     "Kikuchi"
    ]
   ],
   "title": "A clinical voice evaluation system",
   "original": "e93_0215",
   "page_count": 4,
   "order": 77,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "We introduce a clinical voice evaluation system. The system is primarily intended as a versatile voice assesment instrument at voice clinics. It also provides a useful research tool for speech scientists, otolaryngologists and speech therapists. The system includes a Windows application with associated dynamic link libraries, DSP programs, and a specific DSP system and interface hardware. The system needs a PC and Microsoft (R) Windows (TM) 3.1 Presentation Manager. The clinical voice evaluation system provides the voice record/playback interface, patient data editing, waveform segmentation and editing, voice analysis, analysis setting, file management, procedure playback/recording, and window managements. The analysis module evaluates amplitude and pitch period contours and calculates jitter and shimmer parameters,laryngeal noise, pitch synchronous and asynchronous slices of FFT and LPC spectra, histograms and LPC spectra of perturbation sequences, etc. Keywords; pathological voice, perturbation parameters, Windows application, DSP.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-76"
  },
  "wrench93_eurospeech": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "M. S.",
     "Jackson"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "D. S.",
     "Soutar"
    ],
    [
     "A. G.",
     "Robertson"
    ],
    [
     "J.",
     "MacKenzie"
    ],
    [
     "John",
     "Laver"
    ]
   ],
   "title": "A speech therapy workstation for the assessment of segmental quality: voiceless fricatives",
   "original": "e93_0219",
   "page_count": 4,
   "order": 78,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "A speech therapy workstation is under development for the purpose of supervised rehabilitation of oral cancer patients. The design of this workstation differs from that of most others of this type in three ways. Firstly, the software is designed to address the problems of this specific patient group. Secondly, patients and therapists are involved throughout the design process at the earliest opportunity. Thirdly, the project is based on advances in automated acoustic phonetic analysis which overcome many of the problems associated with formant analysis of voiceless fricatives and in so doing provide intuitive visual feedback for this category comparable to the F1/F2 plots for vowels. This paper discusses the constraints placed on the design of such a workstation, the solutions and the acceptability of the resulting system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-77"
  },
  "salavedra93_eurospeech": {
   "authors": [
    [
     "Josep M.",
     "Salavedra"
    ],
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Xavier",
     "Jove"
    ]
   ],
   "title": "A speech enhancement system using higher order ar estimation in real environments",
   "original": "e93_0223",
   "page_count": 4,
   "order": 79,
   "p1": "223",
   "pn": "226",
   "abstract": [
    "We study some speech enhancement algorithms based on the iterative Wiener filtering method due to Lim-Oppenheim [2], where the AR spectral estimation of the speech is carried out using a second-order analysis. But in our algorithms we consider an AR estimation by means of cumulant analysis. This work extends some preceding papers due to the authors, providing a behavior comparison between cumulant algorithms and classical auto-correlation one. Some results are presented considering AWGN that allows the best improvement and those noises (diesel engine and reactor noises) that leads to the worst one. An exhaustive empirical test shows that cumulant algorithms outperform the original autocorrelation algorithm, specially at low SNR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-78"
  },
  "bouquin93_eurospeech": {
   "authors": [
    [
     "R. Le",
     "Bouquin"
    ],
    [
     "G.",
     "Faucon"
    ],
    [
     "A.",
     "Akbariazirani"
    ]
   ],
   "title": "Proposal of a composite measure for the evaluation of noise cancelling methods in speech processing",
   "original": "e93_0227",
   "page_count": 4,
   "order": 80,
   "p1": "227",
   "pn": "230",
   "abstract": [
    "This work addresses the problem of evaluating noise reduction techniques for hand-free telecommunications. It summarizes objective quality measures that are used in such a context. Finding a high correlation between objective and subjective tests remains a key point. In this paper, we propose a composite measure that appears more correlated with listening tests than classical ones. Its parameters are first determined by a training period and then this measure is validated using different noise cancelling approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-79"
  },
  "crozier93_eurospeech": {
   "authors": [
    [
     "P. M.",
     "Crozier"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "C.",
     "Holt"
    ],
    [
     "E.",
     "Munday"
    ]
   ],
   "title": "The use of linear prediction and spectral scaling for improving speech enhancement",
   "original": "e93_0231",
   "page_count": 4,
   "order": 81,
   "p1": "231",
   "pn": "234",
   "abstract": [
    "A modified spectral scaling technique is proposed for enhancing speech corrupted by additive noise. Spectral scaling is a frequency domain noise reduction technique where FFT frequency samples are attenuated to a degree dependent on their signal to noise ratio, so that samples with a low signal to noise ratio are heavily attenuated. This technique reduces the level of noise, but the residual consists of a 'musical noise', which can be annoying to listen to. The noise reduction is improved by the technique described in this paper. For each frame, an estimate of the linear prediction (LP) spectrum is found, and is used to determine a spectral weighting function. The spectral weighting function is incorporated into a spectral scaling algorithm to further attenuate noise in spectral regions where the effect on speech quality is not perceived. The LP spectrum of the speech is estimated by applying LP analysis to noisy speech which has first been subjected to spectral scaling. Peaks that are caused by noise are removed from the resulting LP spectrum. Informal listening tests have shown that this method can significantly reduce the level of musical noise without affecting the speech quality, for input signal to noise ratios of 6 dB and higher.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-80"
  },
  "sorensen93_eurospeech": {
   "authors": [
    [
     "Helge B. D.",
     "Sorensen"
    ],
    [
     "Uwe",
     "Hartmann"
    ]
   ],
   "title": "Robust speaker-independent speech recognition using non-linear spectral subtraction based IMELDA",
   "original": "e93_0235",
   "page_count": 4,
   "order": 82,
   "p1": "235",
   "pn": "238",
   "abstract": [
    "A two-stage noise reduction approach for robust speaker-independent speech recognition is presented. The application is speech recognition in the presence of stationary and non-stationary car noise. The two-stage method calculates noise robust NSS-IMELDA feature vectors, which are used as input to Continuous Hidden Markov Models (CHMM). The new NSS-IMELDA features are based on an integration of two noise robust feature extraction methods - the Non-linear Spectral Subtraction (NSS) method [1] - developed by Lockwood et al. and the Integrated Mel-scale with Linear Discriminant Analysis (IMELDA) method [2] developed by Hunt et al. The NSS-IMELDA features result in better speech recognition in noise than pure an NSS based - or an IMELDA based recognition system. Results on the test database consisting of the test part of the TI-DIGITS database [3] and a non-stationary car noise database gave recognition rates of 98.8% and 98.9% at 0 dB and 15 dB, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-81"
  },
  "vieregge93_eurospeech": {
   "authors": [
    [
     "Willem H.",
     "Vieregge"
    ],
    [
     "A. P. A.",
     "Broeders"
    ]
   ],
   "title": "Intra- and interspeaker variation of /r/ in dutch",
   "original": "e93_0267",
   "page_count": 4,
   "order": 83,
   "p1": "267",
   "pn": "270",
   "abstract": [
    "In Dutch, the phonological variable M has a large number of different realizations. Among the most common of these are the alveolar trill and flap, the voiced uvular fricative and the uvular approximant. The distribution of these variants of /r/ is known to be conditioned by its phonological context and to vary for different accents as well as different speakers of Dutch. There are, however, no empirical studies which provide quantitative information about the actual nature of the variation in the realization of /r/ found in individual speakers of Dutch.\n",
    "The present paper reports on a study aimed to gain insight into the question whether or to what extent individual speakers differ consistently in their realization of /r/ and to examine how intraspeaker differences compare with interspeaker differences. In addition to their intrinsic descriptive interest, these questions have considerable relevance to auditory speaker identification both in the forensic context and beyond.\n",
    "Speech samples produced by male speakers of Dutch were analysed to define suitable /r/-contexts. Selected M-segments were subsequently transcribed by means of a consensus transcription. For all speakers, the distribution of [r]-variants was established and statistics were calculated, including the frequency of the modal and non-modal realizations. The results of this study give a first indication of the patterning of M-realizations in the speech of individual speakers of Dutch. It forms part of a larger project involving an analysis of a range of segmental variables in a variety of speech styles as realised by standard- as well as nonstandard-accented speakers of Dutch.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-82"
  },
  "tronnier93_eurospeech": {
   "authors": [
    [
     "Mechtild",
     "Tronnier"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "An acoustic approach to fricatives in Japanese and German",
   "original": "e93_0271",
   "page_count": 4,
   "order": 84,
   "p1": "271",
   "pn": "274",
   "abstract": [
    "This study presents a cross-language investigation of the acoustic quality of the /h/-part in the Japanese /hi/syllable, and/h/before /if in German and the German palatal fricative [q]. Two comparisons of the acoustic structure of specific effects within and between the two languages are shown, one of them dealing with the elicitation of absolute values these sounds may have in common, similar to loci found for stops, the other one focusing on tendencies rather than absolute values. In the latter comparison, the acoustic structure of the two German sounds are related to these realisations of the Japanese /hi/syllable, where the vowel is phonetically maintained and where it is not.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-83"
  },
  "viana93_eurospeech": {
   "authors": [
    [
     "M. Céu",
     "Viana"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Carlos",
     "Ribeiro"
    ],
    [
     "Amalia",
     "Andrade"
    ],
    [
     "Ernesto",
     "d'Andrade"
    ]
   ],
   "title": "The relationship between spelled and spoken portuguese: implications for speech synthesis and recognition",
   "original": "e93_0275",
   "page_count": 4,
   "order": 85,
   "p1": "275",
   "pn": "278",
   "abstract": [
    "This paper describes a set of statistical results concerning vowel and consonant distribution in European Portuguese formal reading, at very slow and normal rates, and discusses some of its implications to the development of several language processing tools.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-84"
  },
  "schmidt93_eurospeech": {
   "authors": [
    [
     "M.",
     "Schmidt"
    ],
    [
     "S.",
     "Fitt"
    ],
    [
     "C.",
     "Scott"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Phonetic transcription standards for european names (ONOMASTICA)",
   "original": "e93_0279",
   "page_count": 4,
   "order": 86,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "This paper details the standards identified for phonetic transcription of names as part of the ONOMASTICA project, a European-wide research initiative for the construction of a multi-language pronunciation lexicon of proper names. The main design criteria adopted by the consortium for the development of this multi-language pronunciation dictionary are discussed, including aspects such as phonetic transcription standards, definitions of quality, quality control mechanisms and language specific details concerning phonetic transcription and the annotation of the language of origin.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-85"
  },
  "andersen93_eurospeech": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "William",
     "Barry"
    ]
   ],
   "title": "Data-driven identification of poly- and mono-phonemes for four european languages",
   "original": "e93_0759",
   "page_count": 4,
   "order": 87,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "This research concerns the identification of a set of multi-lingual poly-phonemes, which are considered similar enough to be defined across a number of European languages. Results are given of a data-driven approach contrasting to our earlier approach, which was based on auditory phonetic decisions and phonetic statements in the literature. The results show that it is possible to substitute language-specific models with corresponding poly-phonemes trained on other languages and in doing so increase the performance.\n",
    "Keywords: Phoneme identification, language-independent poly-phonemes, language-dependent mono-phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-86"
  },
  "hunnicutt93_eurospeech": {
   "authors": [
    [
     "Sheri",
     "Hunnicutt"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Reversible letter-to-sound sound-to-letter generation based on parsing word morphology",
   "original": "e93_0763",
   "page_count": 4,
   "order": 88,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "This paper describes a reversible letter-tosound/sound-toletter system based on a strategy that combines data-driven techniques with a rule-based formalism. Our approach is to provide a hierarchical analysis of a word, including information such as stress pat- tern, morphology and syllabification, which incorporates probabilities that are trained from a parsed lexicon. Our training and testing corpora consisted of spellings and pronunciations for the high frequency portion of the Brown Corpus (10,000 words). We augmented the phonetic labels with markers indicating morphology and stress. We report here on two distinct grammars representing a historical perspective. Our early work with the first grammar inspired us to modify the grammar formalism, leading to greater constraint with fewer rules. We evaluated our performance on letter-to-sound generation in terms of whole word accuracy as well as phoneme accuracy. For the unseen test set, we achieved a word accuracy of 69.3% and a phone accuracy of 91.7% using a set of 49 distinct phonemes. Although we have no formal results on sound-to-letter generation, we believe that this formalism will be applicable for entering unknown words orally into a recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-87"
  },
  "moore93b_eurospeech": {
   "authors": [
    [
     "Jan",
     "Moore"
    ],
    [
     "Peter",
     "Roach"
    ]
   ],
   "title": "The role of context in the automatic recognition of stressed syllables",
   "original": "e93_0767",
   "page_count": 4,
   "order": 89,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "Three preliminary tests, using a neural network, were carried out in an attempt to establish the need for context for accurate automatic categorisation of stressed and unstressed syllables. The results indicate that a neural network can out-perform humans in the task of identifying stressed and unstressed syllables, in conditions where no immediate context is given and where a limited amount of immediate context is provided. The overall success rates of the neural network under each condition indicate that the inclusion of context significantly improves the number of correct responses.\n",
    "Keywords: Neural networks, context, stress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-88"
  },
  "young93_eurospeech": {
   "authors": [
    [
     "Duncan",
     "Young"
    ],
    [
     "Gerry T. M.",
     "Altmann"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Metrical structure and the perception of time-compressed speech",
   "original": "e93_0771",
   "page_count": 4,
   "order": 90,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "In the absence of explicitly marked cues to word boundaries, listeners tend to segment spoken English at the onset of strong syllables. This may suggest that under difficult listening conditions, speech should be easier to recognise where strong syllables are word-initial. We report two experiments in which listeners were presented with sentences which had been time-compressed to make listening difficult. The first study contrasted sentences in which all content words began with strong syllables with sentences in which all content words began with weak syllables. The intelligibility of the two groups of sentences did not differ significantly. Apparent rhythmic effects in the results prompted a second experiment; however, no significant effects of systematic rhythmic manipulation were observed. In both experiments, the strongest predictor of intelligibility was the rated plausibility of the sentences. We conclude that listeners' recognition responses to time-compressed speech may be strongly subject to experiential bias; effects of rhythmic structure are most likely to show up also as bias effects.\n",
    "Keywords: compressed speech, rhythm, plausibility.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-89"
  },
  "pasdeloup93_eurospeech": {
   "authors": [
    [
     "Valerie",
     "Pasdeloup"
    ],
    [
     "José",
     "Morais"
    ],
    [
     "Régine",
     "Kolinsky"
    ]
   ],
   "title": "Are stress and phonemic string processed separately? evidence from speech illusions",
   "original": "e93_0775",
   "page_count": 4,
   "order": 91,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "This paper describes a perceptual experiment which was run to determine if lexical stress pattern and phonemic string are represented separately at some level of processing. We used a target word recognition under dichotic listening presentation: two stimuli are presented simultaneously, one to one ear and one to the other ear. The stimuli are constructed in such a way that, if listeners combine the phonemic string from the stimulus presented to one ear with the stress pattern from the stimulus presented to the other ear, they experience the illusory perception of a word which corresponds to the prespecified target The results suggest that stress pattern and segmental string can be represented separately at least at a prelexical level Keywords : lexical stress, prosody, speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-90"
  },
  "son93_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Vowel identification as influenced by vowel duration and formant track shape",
   "original": "e93_0285",
   "page_count": 4,
   "order": 92,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "Synthetic vowels were used to investigate how listeners use vowel duration andformant track shape to determine vowel identity. The synthetic vowels had level or parabolically shaped formant tracks and variable durations. They were presented in isolation as well as in synthetic Consonant-Vowel-Consonant syllables. There was no evidence of perceptual compensatory overshoot for expected target-undershoot due to token duration or context. The only asserted effects of duration and context were in the number of long-and short-vowel responses. There was also no evidence that the listeners used the formant track shape or slopes independently to identify the synthetic vowel tokens. Tokens with curved formant tracks were mainly identified on their formant offset frequencies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-91"
  },
  "goldenthal93_eurospeech": {
   "authors": [
    [
     "William D.",
     "Goldenthal"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Modelling spectral dynamics for vowel classification",
   "original": "e93_0289",
   "page_count": 4,
   "order": 93,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "In this work, we are attempting to develop models which capture the dynamic characteristics and statistical dependencies of acoustic attributes in a segment-based framework. Our approach is based on the creation of a track, fa, for each phonetic unit a. The track serves as a model of the dynamic trajectories of the acoustic attributes over the segment. The tracks attempt to capture segment-level spectral dynamics without making any assumptions concerning the linearity or stationarity of the speech signal. The statistical framework for scoring incorporates the auto- and cross-correlation properties of the track error over time, within a segment. This paper presents the results of a series of vowel classification experiments using the TIMIT acoustic-phonetic corpus. Classification performance of 68.9% was achieved, which compares favorably to other vowel classification experiments using the same corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-92"
  },
  "stamenkovic93_eurospeech": {
   "authors": [
    [
     "Milan",
     "Stamenkovic"
    ],
    [
     "Juraj",
     "Bakran"
    ],
    [
     "Peter",
     "Tancig"
    ],
    [
     "Marijan",
     "Miletic"
    ]
   ],
   "title": "Perceptive and spectral volumes of synthesized and natural vowels",
   "original": "e93_0293",
   "page_count": 4,
   "order": 94,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "This paper presents an analysis of the perceptual stability depending on formant position (Fl,F2,F3) and compares recent findings on vowel normalization to the corpus of synthesized and natural Croatian vowels. The vowels were analysed through the light of their spectral and perceptual volume and density. The obtained results for synthesized vowels mainly confirmed with published results but there were some deviations with female and child vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-93"
  },
  "gubrynowicz93_eurospeech": {
   "authors": [
    [
     "Ryszard",
     "Gubrynowicz"
    ],
    [
     "Adam",
     "Wrzoskowicz"
    ]
   ],
   "title": "Labeller - a system for automatic labelling of speech continuous signal",
   "original": "e93_0297",
   "page_count": 4,
   "order": 95,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "The paper presents the labelling system based on EMM modelling applied with a limited set of phonemic classes. The units of description has been chosen in such way that each class of phones is relatively homogeneous in its mode of articulation, irrespective of their context The feature vector consists of 14 cepstral and 14 delta cepstral coefficients. The labelling errors were analyzed for two working moods: a) when the correct string of labels is known to the system, b) when not. In the former case the reference transcription resulting from text-to phoneme conversion is introduced under network sequence to Viterbi algorithm and the only possible errors are in segments boundaries. In the latter, all typical labelling errors are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-94"
  },
  "andersson93_eurospeech": {
   "authors": [
    [
     "Ake",
     "Andersson"
    ],
    [
     "Holger",
     "Broman"
    ]
   ],
   "title": "Towards automatic speech-to-text alignment",
   "original": "e93_0301",
   "page_count": 4,
   "order": 96,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "Time-alignment of several minutes of speech to the corresponding text can be divided into sub-tasks. First, perform a broad alignment to detect anchor-points. Second, use these anchor-points to achieve the desired detailed alignment. This paper describes a procedure for the broad alignment. Segments of voiced/unvoiced speech are used to produce the broad alignment. The speech signal is classified into segments of voiced/unvoiced events using a pitch- detection algorithm. The corresponding segments of voiced/unvoiced events are generated from the text. A warp algorithm matches the segments and the broad alignment is achieved. The proposed alignment procedure has been used on eleven data sets ( spoken by four speakers, three male and one female ) with a total error of 4.2% when an automatic pitch-detection algorithm was used to obtain the voiced/unvoiced events and an error of 2.7% when manually edited voiced/unvoiced events were used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-95"
  },
  "suaudeau93_eurospeech": {
   "authors": [
    [
     "Nelly",
     "Suaudeau"
    ],
    [
     "Regine",
     "Andre-Obrecht"
    ]
   ],
   "title": "Sound duration modelling and time-variable speaking rate in a speech recognition system",
   "original": "e93_0307",
   "page_count": 4,
   "order": 97,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "Among the features extracted from the speech signal, it's clear that someone are directly dependent on the elementary acoustic level whereas the others depend on the suprasegmental level such as the phonetic level. A major deficiency of a standard EMM is that it takes into account uniformly the informations. In this paper, we try to resolve this problem using the Two Level EMM which introduces the features with respect to their informative contents, either on the elementary acoustic level or on the phonetical level. Namely, the incorporation of global sound durations is explored. More, since variations in speaking rate affect sound durations, we propose to appropriatly adapt the sound duration pdf parameters. Experiments on french number database show that such an explicit introduction of prosodic parameters improves the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-96"
  },
  "jones93_eurospeech": {
   "authors": [
    [
     "M.",
     "Jones"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Using relative duration in large vocabulary speech recognition",
   "original": "e93_0311",
   "page_count": 4,
   "order": 98,
   "p1": "311",
   "pn": "314",
   "abstract": [
    "Current large vocabulary continuous speech recognisers (LVCSR) do not model the effects of speech rate on the speech unit durational characteristics. This paper presents work on the investigation of speech rate, presents three durational models which make use of this rate information and the integration of the models into a TIMIT based LVCSR is described. Although TIMIT contains controlled, read speech, with little speech rate variation, experimental work has shown that the relative duration models produce greater word error rate improvements than models which do not take account of the speech rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-97"
  },
  "gong93_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "William C.",
     "Treurniet"
    ]
   ],
   "title": "Duration of phones as function of utterance length and its use in automatic speech recognition",
   "original": "e93_0315",
   "page_count": 4,
   "order": 99,
   "p1": "315",
   "pn": "318",
   "abstract": [
    "Duration probability of phonemes is widely used as a constraint in phoneme-based continuous speech recognizers, and is known to improve recognition accuracy. Usually, models of phoneme duration are extracted from continuous utterances of sentences in a training database. However, tokens obtained from shorter utterances may not be well represented by models created from longer utterances. We designed an experiment to compute observed average phoneme duration as a function of the number of phonemes per utterance. We observed that the average duration consistently increases as the number of phonemes per utterance increases. The experiment showed that the average duration of phonemes in words spoken in isolation may be as much as 50% longer than the average duration of phonemes in continuously spoken sentences. The variation of phoneme duration as a function of utterance duration was modeled in both the phoneme probability estimation stage and the utterance search stage of a recognition system. As a result, a 47% reduction in word recognition errors was obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-98"
  },
  "forsyth93_eurospeech": {
   "authors": [
    [
     "M. E.",
     "Forsyth"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Duration modelling and multiple codebooks in semi-continuous HMMs for speaker verification",
   "original": "e93_0319",
   "page_count": 4,
   "order": 100,
   "p1": "319",
   "pn": "322",
   "abstract": [
    "Gaussian duration modelling in semi-continuous hidden semi-Markov models is used to improve the performance of an automatic speaker verification system tested on telephone quality speech. A performance improvement of 2.3% in equal error rate for a 12 digit string is reported. Cepstra, delta cepstra, energy and energy plus delta energy parameter sets are all shown to contain speaker discriminating information, and a combination of these parameters, using equal weightings improved the system performance from 2.3% to 1.3% for a 12 digit string.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-99"
  },
  "hochberg93_eurospeech": {
   "authors": [
    [
     "Michael M.",
     "Hochberg"
    ],
    [
     "Harvey F.",
     "Silverman"
    ]
   ],
   "title": "Constraining model duration variance in HMM-based connected-speech recognition",
   "original": "e93_0323",
   "page_count": 4,
   "order": 101,
   "p1": "323",
   "pn": "326",
   "abstract": [
    "Typical statistical speech recognition systems model the duration of acoustic segments with discrete-time, first-order Markov chains. Recent work in the area of hidden Markov models (HMMs) has extended the modeling approach to discrete-time, first-order semi-Markov processes. The Markov assumption that the states of a model are independent can result in word-duration statistics which are quite different from those observed during recognition or obtained through labeling. This paper presents an approach for extending the EMM frame-work such that a priori conditions on the model duration statistics are satisfied. The constrained variance EMM (CV-EMM) is presented as a means to capture both the robust state-duration modeling capability of the traditional ESMM while imposing constraints on the word-duration variance. The paper presents the CV-EMM framework and describes the parameter estimation processing. Results on a speaker-independent, connected-speech task are reported and compared with traditional EMM approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-100"
  },
  "tuerk93_eurospeech": {
   "authors": [
    [
     "Christine",
     "Tuerk"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "A new frequency shift function for reducing inter-speaker variance",
   "original": "e93_0351",
   "page_count": 4,
   "order": 102,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "Speaker normalisation remains a very significant problem in speech research. One of the most immediate applications for a solution would be in the area of multiple-speaker speech recognition systems. These systems are faced with the task of assigning phonetic labels to portions of input speech, a task which is extremely complicated due to the enormous amount of variability within a phonetic class. Finding a good normalisation transformation would reduce this variability. Theoretical aspects of speech related studies would also benefit from a normalisation solution as it should lead to a greater understanding of the essential acoustic correlates that define a sound. A solution would aid researchers in the areas of perception and psycholinguistics. Normalisation techniques could also contribute to speech synthesis applications, especially in the area of producing multiple voices. This paper describes a frequency domain shift function which reduces the amount of inter-speaker variance within a phonetic class. The shift function is dependent upon the speaker's geometric mean pitch. The shift function is easily parameterised in a piece-wise linear fashion. Application of the shift allows a 15.8 - 17.0% reduction of variance. This reduction falls within 0.2% of the optimal pitch-only shift function for the data studied. In addition to variance reduction and recognition applications, this shift is easily applied as a means for warping speaker quality. This technique is applicable to synthesis systems where multiple voice qualities are desired.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-101"
  },
  "ono93_eurospeech": {
   "authors": [
    [
     "Yoshio",
     "Ono"
    ],
    [
     "Hisashi",
     "Wakita"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Speaker normalization using constrained spectra shifts in auditory filter domain",
   "original": "e93_0355",
   "page_count": 4,
   "order": 103,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "In this paper we describe a speaker normalization method based on spectral shifts in the auditory filter domain. This method is characterized by using an estimated vocal tract length as a criterion to determine the spectral shift value. Certain constraints are found to be necessary for the shift in the auditory filter domain, and two techniques based on these constraints, the One-Bark shift and the refined Bark-scale shift, are introduced. When tested in vowel classification experiments, significant performance improvement was obtained for both techniques. We maintain that the method is useful for speaker normalization in speaker-independent speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-102"
  },
  "zhao93_eurospeech": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Self-learning speaker adaptation based on spectral variation source decomposition",
   "original": "e93_0359",
   "page_count": 4,
   "order": 104,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "In this paper, a self-learning speaker adaptation technique based on the separation of speech spectral variation sources is developed for improving speaker-independent continuous speech recognition. Statistical methods are formulated to remove spectral biases caused by speaker acoustic characteristics and channel mismatches and to adapt parameters of mixture Gaussian density phone models using unsupervised segmentation data from recognition feedback. Adaptation experiments demonstrate consistent performance improvements over a baseline speaker-independent continuous speech recognition system. On a TIMIT test sety where the task vocabulary size is 853 and the test set perplexity is 104, with each speaker speaking two to three sentences, the recognition word accuracy has been improved from 86.9% to 88.3% (10.7% error reduction). On a separate test set which contains a recording channel mismatch, where each speaker read 98 sentences and with a test set perplexity of 105, the recognition word accuracy has been improved from 69.3% to 85.2% (51.8% error reduction).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-103"
  },
  "kosaka93_eurospeech": {
   "authors": [
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Edward",
     "Willems"
    ],
    [
     "Jun-Ichi",
     "Takami"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "A dynamic approach to speaker adaptation of hidden Markov networks for speech recognition",
   "original": "e93_0363",
   "page_count": 4,
   "order": 105,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "This paper describes a new approach to dynamic speaker adaptation, which relies on switching between different methods of adaptation in order to gain maximum performance depending on the amount of speech data obtained through the speech recognition session. This adaptaion method has been successfully applied to a hidden Markov network (HMnet), which is an efficient representation of phoneme context-dependent HMMs. This speaker adaptation method has proven itself effective in improving the performance of a Japanese phrase recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-104"
  },
  "knohl93_eurospeech": {
   "authors": [
    [
     "Lars",
     "Knohl"
    ],
    [
     "Ansgar",
     "Rinscheid"
    ]
   ],
   "title": "Speaker normalization and adaptation based on feature-map projection",
   "original": "e93_0367",
   "page_count": 4,
   "order": 106,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "An efficient speaker-normalization method based on the mapping of two self-organizing feature maps is developed. The normalization system consists of a reference map that is trained on the reference speaker's feature space and of a test speaker's map, generated by a special topology-maintaining retraining of the reference map. The retraining procedure is called 'Forced Competitive Learning (FCL)\\ It ensures the topological identity of both maps and thereby implicitly establishes an 1:1 correspondency of the codebooks. This allows for an l:l-exchange of the feature vectors represented by the neurons of the reference map for those of the test map in the operation phase. Pilot tests on a 33 word database, including the 10 digits (3 male & 2 female speakers, 5 versions each) have been performed employing a simple HMM-isolated-word recognizer. The evaluation was based on speaker-dependent recognition and has shown an average adaptation efficiency of p = 0, 90 . Because of its error tolerance and its applicability to virtually every, even abstract feature space, the method proposed can broadly be applied as a front end to all kinds of VQ-based recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-105"
  },
  "leeuw93_eurospeech": {
   "authors": [
    [
     "Marcel de",
     "Leeuw"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Pitch synchronous calculation of acoustic cues using a cochlea model",
   "original": "e93_0373",
   "page_count": 4,
   "order": 107,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "In the framework of an acoustic-phonetic decoding task, a pitch synchronous cochlea model has been developed for the calculation of acoustic cues. Included stages are outer- and middle ear, basilar membrane, inner hair cell potential, inner hair cell/synapse and a pitch detection module. The basilar membrane model is shown to have sufficient temporal resolution for a good time domain pitch detection. Possibilities for the amelioration of the discriminative quality of the acoustic cues, using a pitch synchronous measure, are indicated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-106"
  },
  "mclaughlin93_eurospeech": {
   "authors": [
    [
     "Stephen",
     "McLaughlin"
    ],
    [
     "Andrew",
     "Lowry"
    ]
   ],
   "title": "Nonlinear dynamical systems concepts in speech analysis",
   "original": "e93_0377",
   "page_count": 4,
   "order": 108,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "The study detailed here considers vowel sounds from a non-linear dynamical systems perspective. In particular, it considers estimation of the dimension and largest Liapunov exponent of vowel sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-107"
  },
  "klaassen93_eurospeech": {
   "authors": [
    [
     "Arno J.",
     "Klaassen"
    ]
   ],
   "title": "Grouping of acoustical events using cable neurons and the theory of neuronal group selection",
   "original": "e93_0381",
   "page_count": 4,
   "order": 109,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "When representing speech signals as groups of spatio-temporal events, a problem is to find and group the events. Here, we propose to do so using self-organising cable neuron networks as found in the theory of Neuronal Group Selection. The basic principle of the learning rules that establish the process of group forming is reinforcement of coincidences: by differences in transmission delay times and post-synaptic integration times, a certain spatio-temporal input pattern will give rise to more or less simultaneously activation of several neurons. These neurons will tend to amplify the interconnections inbetween them and weaken their connections with other neurons. Once having identified groups of spatio-temporally correlated events at the lowest level, the grouping process can be repeated at the following level, i.e. grouping these groups, in order to recognise higher level entities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-108"
  },
  "gransden93_eurospeech": {
   "authors": [
    [
     "I. R.",
     "Gransden"
    ],
    [
     "S. W.",
     "Beet"
    ]
   ],
   "title": "Computationally efficient methods of calculating instantaneous frequency for auditory analysis",
   "original": "e93_0385",
   "page_count": 4,
   "order": 110,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "This paper proposes a novel, computationally efficient approach to the estimation of instantaneous frequency for auditory analysis. A comparison is made between this approach and a computationally efficient implementation of the analytic signal approach to IF estimation. A method that uses zero-crossings and higher order statistics is also described and compared to the above.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-109"
  },
  "cutugno93_eurospeech": {
   "authors": [
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Pietro",
     "Maturi"
    ]
   ],
   "title": "Analysing connected speech with wavelets: some Italian data",
   "original": "e93_0389",
   "page_count": 4,
   "order": 111,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "We discuss here the results of the application of a wavelet-transform analysis system to a small number of items of Italian connected speech from a large Italian database. In particular, we wish to emphasize the importance of a quasi-auditory analysis in the search of the acoustic correlates of perceptually distinctive features in connected speech. In this kind of speech, in fact, the acoustic data very often differ a big deal from those found in isolated, slowly uttered (nonsense) words, on which most of the acoustic phonetic research has been based so far. Some comparison between these two kinds of spoken materials analysed with wavelets is also attempted.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-110"
  },
  "marasek93_eurospeech": {
   "authors": [
    [
     "Krzysztof",
     "Marasek"
    ]
   ],
   "title": "Speech transients analysis using AR-smoothed wigner-ville distribution",
   "original": "e93_0393",
   "page_count": 4,
   "order": 112,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "The Joint Time-Frequency Representations (JTFRs) are the most promising techniques of spectral analysis of nonstationary signals. The potential advantages of those methods in speech research, especially most frequently used Wigner-Ville Distribution (WVD), were reduced by its important drawbacks: inter-components interferences, negative values, artifacts and spurious peaks. The results of recent investigations help to overcome this inconveniences, especially by smoothing, i.e. proper selection of window function in time, frequency and autocorrelation lag domains (Choi-Williams Distribution, Zhao-Atlas-Marks). The application of autoregressive modeling (AR) of PseudoWVD instead of FFT adds to this: smoothing of spectral envelope, more \"peaked\" spectrum (more visible resonant frequencies) and better frequency resolution. The time-frequency distribution is derived from AR coefficients of smoothed local autocorrelation sequences. The experiments with speech transients analysis, especially formant frequencies tracking were performed for various types of articulation for real and synthetic speech. The reliability of the method against added noise was also tested. The AR smoothed PWVD accurately follows the formants dynamics and variations. It works also fine in low SNR ratios. Reduced number of parameters, sufficiently describing spectral envelope suggest use of the method in speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-111"
  },
  "pitermann93_eurospeech": {
   "authors": [
    [
     "Michel",
     "Pitermann"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Comparison of the variability of formants and formant targets using dynamic modeling",
   "original": "e93_0397",
   "page_count": 4,
   "order": 113,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "The target theory of vowel production suggests that vowels are characterized by formant values called targets. It is assumed that these targets are fixed with respect to speaking rate and stress pattern. In the work described here we attempted to investigate this issue. We dynamically modelled formant time series to estimate unreached targets; then we compared their variability with the formant frequency variability for the [a] in the context [iai] for different speaking rate and stress pattern pairs. Below a certain speaking rate, our results were consistent with the hypothesis that the [a] is characterized in the (Fi,F2) plane by a fixed target with respect to speaking rate and stress pattern.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-112"
  },
  "schoentgen93b_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Zoubir",
     "Azami"
    ]
   ],
   "title": "Pitch-synchronous formant extraction by means of a compound auto-regressive model",
   "original": "e93_0401",
   "page_count": 4,
   "order": 114,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "We present a new method for pitch-synchronously extracting formants. Pitch-synchronous formants extraction is the estimation of formant frequencies and bandwidths during the closed phase (CP) of a glottal cycle. Conventionally, the formants are obtained from a linear auto-regressive model that is fitted to the signal portion emitted during the closed phase of the glottis. The positions and durations of the closed phases have to be determined beforehand. The problem with this approach is that methods that automatically detect the closed phase of the glottis cycle from the speech signal (or the laryngogram) are not totally reliable and the accurate measurements of CP- durations cannot be obtained under all circumstances and for all kinds of signals. Here, we present a method that does not require the detection of the CPs before modelling. The method uses a compound auto-regressive model (AR). When the model is fitted, error-minimisation automatically determines the positions of the closed and open phases of the glottis and the values of the model coefficients. This is achieved by fitting, inside a single analysis window, two linear constant coefficient auto-regressive models at the same time; the first to the CP-portions and the second to the rest of the signal. We present results obtained by applying this compound model to synthetic and natural vowels, and to natural sentences. Results show that the segmentation is robust and that the formant values obtained are typical of the closed phase values of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-113"
  },
  "teston93_eurospeech": {
   "authors": [
    [
     "Bernard",
     "Teston"
    ]
   ],
   "title": "A new air flowmeter design for the investigation of speech production",
   "original": "e93_0405",
   "page_count": 4,
   "order": 115,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "The study and development of an air flowmeter for the simultaneous study of oral and nasal airflow during speech are described. The specific functions of this device are used for the investigation of speech production and as an aid for diagnosing velum and larynx disorders and their therapeutic control.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-114"
  },
  "magnocaldognetto93_eurospeech": {
   "authors": [
    [
     "Emanuela",
     "Magno Caldognetto"
    ],
    [
     "Kyriaki",
     "Vagges"
    ],
    [
     "Giancarlo",
     "Ferrigno"
    ],
    [
     "Claudio",
     "Zmarich"
    ]
   ],
   "title": "Articulatory dynamics of lips in Italian /'vpv/ and /'vbv/ sequences",
   "original": "e93_0409",
   "page_count": 4,
   "order": 116,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "This research focuses on the study of upper and lower lip movements in the production of the Italian bilabial stops /p/ and /b/ in different vowel contexts. The lip movements were recorded and analysed with the ELITE system. The spatial and temporal characteristics (duration, displacement and velocity) of the symmetric /'VCV/ sequences, where C = /p, b/ and V=/a, i, u/, were analysed. The data show that the main effects of the voiced-voiceless contrast on the bilabial closure movement are a greater degree of lip compression and a greater closure velocity for /p/ than /b/. The effect of the flanking vowels is evident in the different closure duration of the upper and lower lips and in the different closure velocity of the lower lip. Moreover, the time interval between the peak velocity and the peak closure is not affected by neither the voiced-voiceless contrast nor by the vowel context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-115"
  },
  "elgendy93_eurospeech": {
   "authors": [
    [
     "Ahmed M.",
     "Elgendy"
    ]
   ],
   "title": "Restricted distribution of pharyngeal segments: acoustical or mechanical constraints?",
   "original": "e93_0413",
   "page_count": 4,
   "order": 117,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "The segmental organisation of the consonants in relation to vowel-context in Egyptian Arabic was investigated. The percentage of the frequency of occurrence for each consonant and the ratio of initial/medial and initial/final positions for CWC, CVCVC, and CVCC morphemes were counted from matrices contain all permissible combinations. The results revealed that consonants in a given sequence are selected according to their compatibility to preserve the temporal aspects of syllable structure. The relationship between consonant positions in a sequence depends, to a great extend, on their relative degree of jaw-height. The physiological constraints effecting segments involve the pharynx in their production have a severe impact on the over-all distributional patterns. The results could point out the predominant effect the mechanical factor has on the observed phonotactic patterns and further suggest that this factor can be used in improving methods of constructing artiulatory models of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-116"
  },
  "payan93_eurospeech": {
   "authors": [
    [
     "Yohan",
     "Payan"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Vowel normalization by articulatory normalization first attemps for vowel transitions",
   "original": "e93_0417",
   "page_count": 4,
   "order": 118,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "This paper presents a procedure for normalization based on the exploitation of the concept of formant to cavity affiliation as described in the vocalic theory of speech production. For closed vowels, this concept allows an estimation of the ratio of the respective back and front cavity lengths of two speakers, from the ratio of the associated formants. In order to propose a normalization of the vowel space of different speakers we used a \"reference speaker\", whose articulatory and acoustic properties are well defined: the articulatory model as proposed by Maeda. Vowel normalization consists then in the projection of the vocalic space of one given speaker into the vocalic space of our model. Geometrical relations between the studied speaker and the \"reference speaker\" are extracted from [i] and [u] for front and velo-palatal vowels, and from [a] and [u] for back and velo-pharyngeal vowels, [y] and [u] give some indications about lip shapes. This method is tested (1) by its ability to predict the standard vowels of a speaker starting from those of the \"reference speaker\", and (2) by its ability to propose realistic formant patterns for Maeda's model, starting from the corresponding formant transitions of a real speaker. Keywords : speaker normalization; speaker adaptation; vowel production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-117"
  },
  "miki93b_eurospeech": {
   "authors": [
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Naohisa",
     "Kamiyama"
    ],
    [
     "Nobuo",
     "Nagai"
    ]
   ],
   "title": "Synthesis and analysis of vocal source with vibration of larynx",
   "original": "e93_0421",
   "page_count": 4,
   "order": 119,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "In this paper we show a speech production model of the vocal tract analogue, which can simulate the propagating mechanical vibrational wave on the tract wall, synthesize vocal sounds with the vibrational wave, and analyze the spectral properties for the synthesized waves. We propose a new lattice filter model for the effect of wall vibration in the larynx, and show that the sound propagation is influenced with the vibration in the result of simulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-118"
  },
  "znagui93_eurospeech": {
   "authors": [
    [
     "Imad",
     "Znagui"
    ],
    [
     "Sami",
     "Boudelaa"
    ]
   ],
   "title": "Towards an acoustic-phonetic classification of modern standard arabic vowels",
   "original": "e93_0425",
   "page_count": 3,
   "order": 120,
   "p1": "425",
   "pn": "427",
   "abstract": [
    "As part of an investigation of the coarticulatory influence of \"lingual\" consonants on the quality of adjacent vowels in MSA, measurements were made of Fl & F2 of all the vowels in CV-structure real words pronounced by native Arabic speakers from different countries. It was found that Fl in vowels preceded by interdentals, dentals, palatals and post-palatals was lower than that in vowels preceded by emphatics, velars, uvulars and pharyngeals. F2, however was found to be higher in vowels following interdentals, dentals, palatals and postpalatals than in those following emphatics, uvulars and pharyngeals. Based on these findings, a binary classification of MSA vowels is proposed in terms of the correlation between tongue movements and acoustic-phonetic variance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-119"
  },
  "marchal93_eurospeech": {
   "authors": [
    [
     "Alain",
     "Marchal"
    ],
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Divers' speech: variable encoding strategies",
   "original": "e93_0429",
   "page_count": 4,
   "order": 121,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "In subaquatic and hypeibaric communication, several causes of distorsion have been identified: physical effects related to gas density and increased ambiant pressure. The present study investigates an other source of variation, i.e. the speaker himself who tends to adopt also specific strategies to compensate for the loss of intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-120"
  },
  "aguilar93_eurospeech": {
   "authors": [
    [
     "L.",
     "Aguilar"
    ],
    [
     "B.",
     "Blecua"
    ],
    [
     "M.",
     "Machuca"
    ],
    [
     "R.",
     "Mann"
    ]
   ],
   "title": "Phonetic reduction processes in spontaneous speech",
   "original": "e93_0433",
   "page_count": 4,
   "order": 122,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "Phonetic analysis of the speaking styles can involve the study of the differences in the manifestation of phonetic processes. The aim of this work is to observe and classify low-level reduction processes affecting consonants by means of an acoustic and auditive analysis of a sample of spontaneous speech in Spanish. A continuum of articulatory reduction arises out of the data. Moreover, the acoustic behaviour of the processes helps to establish differences between speaking styles.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-121"
  },
  "ganguli93_eurospeech": {
   "authors": [
    [
     "N. R.",
     "Ganguli"
    ]
   ],
   "title": "Spectral characteristics of fricative sound",
   "original": "e93_0437",
   "page_count": 4,
   "order": 123,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "The paper presents studies on acoustic features of fricative consonants in Bengali. The concentration of acoustic energies, antiformat positions, and durations of three fricatives namely dental, palatal and velar are described The effect of following vowel on these acoustic features are also studied. Spectrographic analysis of CV syllables were done by KAY Digital Sonagraph Model DSP 5500.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-122"
  },
  "bonastre93_eurospeech": {
   "authors": [
    [
     "Jean-Francois",
     "Bonastre"
    ],
    [
     "Henri",
     "Meloni"
    ]
   ],
   "title": "Automatic speaker recognition and analytic process",
   "original": "e93_0441",
   "page_count": 4,
   "order": 124,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "In some previous works on the speaker identification [3], we have underlined and quantified the influence of the context (enunciation situation, coarticulation) one someone's phonetic realisations. These results - recognition of 1 out of 20 speakers, with a safety margin over 99%, using from 5 to 7 phonemes - led us to check the reliability of such a process within the context of automatic speaker recogntion. Therefore, we have created a sound data base (each of the sixty speakers pronounced several hundreds sentences) during many mounth-lasting recordings. Selected speakers were not trained to do this kind of exercise and had very similar socio-cultural and age characteristics. There were usual recording condition (offices, far end corridor, etc.). After an speaker independant automatic localisation of phonemes, we drew up statistics on intra and inter-speaker variability using several spectral shape algorithms. Afterward we have measured tfie discrepencies due to the inaccuracies of the phoneme automatic localisation. The outcome shows a drop in the distances selective ability in comparison with the previous tests. A sharp fall mainly due to the phonemes automatic localisation's randomnessand DB recording type's uncertainty. Such a fact does not prevent the use of these techniques on a speaker recognition system given the passable performances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-123"
  },
  "duez93_eurospeech": {
   "authors": [
    [
     "Danielle",
     "Duez"
    ]
   ],
   "title": "Second formant locus-nucleus patterns in French and Swedish",
   "original": "e93_0445",
   "page_count": 3,
   "order": 125,
   "p1": "445",
   "pn": "447",
   "abstract": [
    "The objective of the present study is to investigate contextual assimilation and/or reduction in French and Swedish. F2 values at the consonant boundary and in the nucleus of the vowel -called the locus-nucleus patterns- were measured in identical nonsense words. It was found that the F2 locus-nucleus differences are smaller in French than in Swedish at the surface level, confirming previous results on spontaneous speech. In the two languages, prominent syllables were shown to exhibit a larger distance from the CV boundary to the minimum-maximim than non-prominent syllables. The differences observed were higher in Swedish, partly due to significantly longer vowels than in French. F2 locus-nucleus differences and vowel duration were found to be very close in non-prominent Swedish syllables and non- prominent French syllables. However the F2 locus-nucleus distances obtained for non-prominent syllables in French and in Swedish were found to be significantly different The results obtained in the present study show a clear influence of the prominence pattern on reduction processes in French and Swedish; however, they do not allow us to quantify the extent ofcoarticulation in either language.\n",
    "Keywords: Second formant locus-nucleus patterns, locus equation, vowel duration, reduction processes, comparison between French and Swedish.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-124"
  },
  "meunier93_eurospeech": {
   "authors": [
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Temporal organisation of segments and sub-segments in consonant clusters.",
   "original": "e93_0449",
   "page_count": 4,
   "order": 126,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "Consonant clusters are more often phonologically than acoustically described. We try to draw up an acoustic classification in order to analyse the general variability in clusters.Our aim, in the present study is to point out the temporal characteristics of each cluster class and to describe its internal segment organisation.\n",
    "Keywords: Consonant clusters, Segments, Durations, boundary, assimilation, variation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-125"
  },
  "betari93_eurospeech": {
   "authors": [
    [
     "Abdelkader",
     "Betari"
    ],
    [
     "Remy",
     "Bulot"
    ]
   ],
   "title": "Automatic recognition of arabic stop consonants",
   "original": "e93_0453",
   "page_count": 4,
   "order": 127,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "Standard Arabic is distinctive from other Indo-European languages by the articulation of sounds in the back part of the vocal track, by the feature of gemination and by the complexity of certain consonants from a velarisation. The stop consonants of Arabic do not escape these particularities and form the object of our study within the frame of speech recognition. With the help of a mixed system using Prolog rules and neural networks conjointly, we locate and identify the occlusives of Arabic as well as the nasal consonants in an ascendant phase of Acoustic-Phonetic Decoding. Keywords : Acoustic-Phonetic Decoding, standard Arabic, Prolog, neural network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-126"
  },
  "torres93_eurospeech": {
   "authors": [
    [
     "I.",
     "Torres"
    ],
    [
     "P.",
     "Iparraguirre"
    ]
   ],
   "title": "Acoustic-phonetic decoding of Spanish occlusive consonants",
   "original": "e93_0457",
   "page_count": 4,
   "order": 128,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "The design of Acoustic-Phonetic decoders involves the identification of the acoustic units as a final step. In this paper two automatic classifiers for the Spanish unvoiced occlusives are presented. Only the acoustic features of the burst segment, automatically segmented from the speech waveform, were considered in the parameters estimation. The analysis of these features was carried out in both the time and frequency domains. In the first case, the classifier was designed in a procedural form. Alternatively, in the second case a statistical classifier was obtained from a previous discriminant analysis of the parameters. Both classifiers were tested over a CV context corpus uttered by 40 speakers not included in the training corpus, resulting in a good rate of identification.\n",
    "Keywords: Speech recognition, Spanish occlusive features, acoustic-phonetic decoding, knowledge based.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-127"
  },
  "christov93_eurospeech": {
   "authors": [
    [
     "Philip",
     "Christov"
    ]
   ],
   "title": "Normalized vowel system representation for comparative phonetic studies",
   "original": "e93_0461",
   "page_count": 4,
   "order": 129,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "It this paper is proposed a normalized im relation to its centre of gravity two-formant graphic vowel systen representation for the purposes of the cross-language acoustic phonetic research. The formal reasonings are supported by normalized diagrams, in both Cartesian and polar coordinates, of the vowel systems of English, French, German and Spanish.\n",
    "Keywords: speech representation, acoustic phonetics, speech acoustics, computer graphics, vowels, formants\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-128"
  },
  "thilly93_eurospeech": {
   "authors": [
    [
     "Cécile",
     "Thilly"
    ]
   ],
   "title": "Influence of prevocalic consonant on vowel duration in French CV[p] utterances",
   "original": "e93_0465",
   "page_count": 4,
   "order": 130,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "In this article, the influence of the prevocalic consonant on vowel duration in isolated French CV[p] sequences is studied. Nine French-speaking male subjects pronounced a corpus constructed using vowels [a,i,u,y] and consonants [p,t,k,b,d,g,fs,f,v,z,3]. Results showed that vowels were longer 1) after voiced than after unvoiced consonants, 2) after plosives than after fricatives and 3) that vowel duration varied according to the place of articulation of the prevocalic consonant only when it was a fricative. Indeed, vowels were longer after labial than after dental fricatives and longer after dental than after palatoalveolar fricatives. These results are in contradiction with the widespread hypothesis that the prevocalic consonant has no influence on vowel duration [3,13,16,8]. They agree with Di Cristo [5] and Wajskop [17] on voicing and with O'Shaughnessy [12] on articulatory mode. It is also hypothesised that if segmentation criteria were comparable, disagreements between published results would disappear in some cases.\n",
    "Keywords: vowel duration in CV[p] utterances, prevocalic consonants, segmentation criteria.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-129"
  },
  "czigler93_eurospeech": {
   "authors": [
    [
     "Peter",
     "Czigler"
    ]
   ],
   "title": "Temporal variation in consonant clusters in Swedish",
   "original": "e93_0469",
   "page_count": 3,
   "order": 131,
   "p1": "469",
   "pn": "471",
   "abstract": [
    "In bisyllabic words with the grave accent, the durations of the konsonants /s, t, k/ as single long consonants, and int the clusters /st, kt, kst/ in medial word position have been measured with the intention of describing their temporal behaviour in and out of focus. The preliminary findings indicate that (1) durations are longer in words with focus accent, (2) durations decrease when the complexity of consonant clusters increases, and (3) the degree of compression varies between different consonants and consonant clusters.\n",
    "Keywords: Consonant clusters; durations; varying levels of complexity; different degrees of stress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-130"
  },
  "jassem93_eurospeech": {
   "authors": [
    [
     "Wiktor",
     "Jassem"
    ]
   ],
   "title": "Discriminant analysis of continuous consonantal spectra",
   "original": "e93_0473",
   "page_count": 4,
   "order": 132,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "The continuous spectra of Polish voiceless plosive bursts and fricatives, spoken by 3 male voices, were analysed by measuring the level at fa a 300 Hz between 0-..8 kHz in averaged FTTs.The weighted mean, the skeweness and the kurtosis of the spectral shape were calculated and formed the random variables in a trivariate Discriminant Analysis. Depending on the type of phoneme and he speaking voice, an accuracy of classifacation of the individual phonemes varied between 0 and 100 %.\n",
    "Keywords: speech recognition, plosives, fricatives\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-131"
  },
  "rooney93_eurospeech": {
   "authors": [
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Miriam",
     "Eckert"
    ],
    [
     "Steven",
     "Hiller"
    ],
    [
     "Rebecca",
     "Vaughan"
    ],
    [
     "John",
     "Laver"
    ]
   ],
   "title": "Training consonants in a computer-aided system for pronunciation teaching",
   "original": "e93_0561",
   "page_count": 4,
   "order": 133,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "This paper describes the treatment of consonants within SPELL, a computer-aided system for teaching pronunciation to foreign language learners. The system concentrates on those consonant features which have the greatest potential to disrupt the speakers' intelligibility. Errors by learners of English, French and Italian have been identified and ranked in terms of their effects on intelligibility. Consonant errors selected for teaching are analysed with a Hidden Markov Model segmenter. The design of suitable diagnostic and corrective feedback to students is currently being investigated.\n",
    "Keywords: Pronunciation Training, Phonetics, Consonant, Teaching Aids, Speech Technology\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-132"
  },
  "miksic93_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Miksic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Rhythm analysis of speech and music signals",
   "original": "e93_0565",
   "page_count": 4,
   "order": 134,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "The paper describes some of the rhythmical features of speech and music signals. Some parallels in duration analysis between music performance and spoken text are drawn. We focused on the rhythmical properties that could be easily applied to speech synthesis or to automatic segmentation of continuous, natural speech, as well as to segmentation of sampled music. In the analysis of rhythmical patterns of continuous speech, we discuss timing and stress at the syllable, the word and the sentence levels. We examine how stress, word endings and sentence endings effect the absolute syllable duration and the span of the deviations due to different speakers.\n",
    "Keywords: rhythm, rhythmical pattern, prosody, intonation, segmentation, speech database\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-133"
  },
  "laan93_eurospeech": {
   "authors": [
    [
     "Gitta P.M.",
     "Laan"
    ],
    [
     "Dick R. van",
     "Bergem"
    ]
   ],
   "title": "The contribution of pitch contour, phoneme durations and spectral features to the character of spontaneous and read aloud speech",
   "original": "e93_0569",
   "page_count": 4,
   "order": 135,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "The separate contribution of the intonation contour, phoneme durations, and spectral features of an utterance to the speech style character was studied by means of a listening experiment. Speech was used from 2 male speakers who each told 'spontaneously' something about themselves and afterwards read out their own transcribed text. Utterances were selected that were identical in wording and that were fluently spoken in both speech styles. The prosodic features pitch, duration, and energy were systematically exchanged between the two speech styles by means of TD-PSOLA. Subjects in the listening experiment were asked to classify the stimuli as either spontaneous or read. It appeared that intonation, phoneme durations, and spectral features all contain cues to a particular speech style, albeit that their separate influence does not dominate over the rest of the information sources of a speech style.\n",
    "Keywords: Spontaneous speech; read out speech; Speech style perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-134"
  },
  "garrido93_eurospeech": {
   "authors": [
    [
     "Juan M.",
     "Garrido"
    ],
    [
     "Joaquim",
     "Llisterri"
    ],
    [
     "Carme de la",
     "Mota"
    ],
    [
     "Antonio",
     "Rios"
    ]
   ],
   "title": "Prosodic differences in reading style: isolated vs. contextualized sentences",
   "original": "e93_0573",
   "page_count": 4,
   "order": 136,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "Global properties of the F0 contour and local phenomena related to lexical stress have been analyzed in a set of read isolated sentences. The results have been compared with those obtained for the same set of sentences embedded in paragraphs. The results show that the main differences are concentrated on global F0 contours.\n",
    "Keywords: Prosody, Reading Style, Declination, Stress.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-135"
  },
  "vroomen93_eurospeech": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Rene",
     "Collier"
    ],
    [
     "Sylvie",
     "Mozziconacci"
    ]
   ],
   "title": "Duration and intonation in emotional speech",
   "original": "e93_0577",
   "page_count": 4,
   "order": 137,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "Three experiments investigated the role of duration and intonation in the expression of emotions in natural and synthetic speech. Two sentences of an actor portraying seven emotions (neutral, joy, boredom, anger, sadness, fear, indignation) were acoustically analyzed. By copying pitch and duration of the original utterances to a monotonous one, it could be shown that both factors were sufficient to express the various emotions. In the second part, rules about intonation and duration were derived and tested. These rules were applied to resynthesized natural speech and synthetic speech generated from LPC-coded diphones. The results showed that emotions can be expressed accurately by manipulating pitch and duration in a rule-based way.\n",
    "Keywords: emotion, intonation, duration, synthetic speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-136"
  },
  "ayer93_eurospeech": {
   "authors": [
    [
     "C. M.",
     "Ayer"
    ],
    [
     "Melvyn J.",
     "Hunt"
    ],
    [
     "D. M.",
     "Brookes"
    ]
   ],
   "title": "A discriminatively derived linear transform for improved speech recognition",
   "original": "e93_0583",
   "page_count": 4,
   "order": 138,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "A discriminatively derived linear transform is described, which, when applied to a given set of acoustic parameters, is capable of improving the accuracy of a speech recognition system based on hidden Markov modelling. This work builds upon a linear transform known as IMELDA, which itself is known to provide an effective and computationally efficient spectral representation for speech recognition, especially when the speech has been degraded. IMELDA uses linear discriminant analysis to maximise a measure of separability of the states of a given set ofHMMs. The new transform is derived using a steepest descent method to minimise a measure of whole-word error rate. A weighting function is used that allows the transform to concentrate on those training tokens which yield borderline recognition decisions. Error rates for a speaker-independent E set recognition task are reduced by a factor of 2, resulting in an error rate of 4.0% on the BTL alphabet database, believed to be the best results published to date using this database. Like IMELDA, the new technique applies a single transformation to all input utterance frames, and is therefore computationally efficient.\n",
    "Keywords: speech recognition, discriminative training, hidden Markov modelling, HMM, linear discriminant analysis, IMELDA.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-137"
  },
  "saerens93_eurospeech": {
   "authors": [
    [
     "Marco",
     "Saerens"
    ]
   ],
   "title": "Hidden Markov models assuming a continuous-time dynamic emission of acoustic vectors",
   "original": "e93_0587",
   "page_count": 4,
   "order": 139,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "When using hidden Markov models for speech recognition, it is usually assumed that the probability that a particular acoustic vector is emitted at a given time only depends on the current state and the current acoustic vector observed. This model does not take account of the time correlation between successive acoustic vectors. We recently introduced two models that try to take account of the continuous-time dynamic nature of the speech signal. The first model assumes that, in a given state, the acoustic vectors are generated by a linear stochastic differential equation; the second one assumes that the acoustic vectors are generated by a particular continuous-time Markov process. This work is motivated by the fact that the time evolution of the acoustic vector is inherently dynamic and continuous, so that the modelling could be performed in the continuous-time domain instead of the discrete-time domain. By the way, the links between the discrete-time process obtained after sampling, and the original continuous-time system are not so trivial. In particular, the relationship between the coefficients of a continuous-time linear process and the coefficients of the discrete-time linear process obtained after sampling is nonlinear. We assign a probability density to the continuous-time trajectory of the acoustic vector inside the state, reflecting the probability that this particular path has been generated by the stochastic process associated with this state. This allows us to compute the likelihood of the uttered word. Reestimation formulae for the parameters of the process, based on the maximization of the likelihood, can be derived for the Viterbi algorithm. As usual, the segmentation can be obtained by sampling the continuous process, and by applying dynamic programming to find the best path over all the possible sequences of states.\n",
    "Keywords: Hidden Markov models, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-138"
  },
  "vaseghi93_eurospeech": {
   "authors": [
    [
     "Saeed V.",
     "Vaseghi"
    ],
    [
     "P. N.",
     "Conner"
    ],
    [
     "Ben P.",
     "Milner"
    ]
   ],
   "title": "Speech modelling using cepstral-time feature matrices",
   "original": "e93_0591",
   "page_count": 4,
   "order": 140,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "This paper explores the use of two dimensional cepstral-time features for the utilisation of correlation among successive speech spectral vectors, within a hidden Markov model (HMM) framework. A cepstral-time feature matrix is obtained from a two dimensional discrete cosine transform of a spectral-time matrix. Advantages of cepstral-time features are : a) a cepstral-time feature matrix is a simple and robust method for representation of short-time variation of speech spectral parameters, b) a cepstral-time matrix contains information on the transitional dynamics of feature vectors within the matrix, c) speech recognition based on cepstral time matrices is more robust in noisy environments, and d) using a matrix of M cepstral vectors implies a minimum HMM-state duration constraint of M vector units. A simple framework investigated in this paper for applications of cepstral-time features is a Finite State Matrix Quantiser (FSMQ). The FSMQ is a special case of the HMM and is used for initialisation of the training phase of HMMs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-139"
  },
  "abe93_eurospeech": {
   "authors": [
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "A bounded transition hidden Markov model for continuous speech recognition",
   "original": "e93_0595",
   "page_count": 4,
   "order": 141,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "An HMM for phonetic transcription is presented. The inter-state transitions are bounded around phone boundaries, which are estimated from the observation sequence by statistical phone boundary detectors. The detection is done using the ratio of two probabilities, a probability that the observation sequence in a window has a phone boundary and a probability that the observation sequence in a window does not have a phone boundary. Finding an optimal state sequence is done by a simple Viterbi algorithm with 2 variables(time and state). In phonetic transcription experiments the presented HMM achieved the best accuracy against HMMs with explicit modeling of state durations. The great improvement in total performance was due to reduction of insertion errors.\n",
    "Keywords: Phonetic Transcription, Hidden Markov Model, Bounded Transition, Statistical Phone Boundary Detection, Ergodic HMM\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-140"
  },
  "moyal93_eurospeech": {
   "authors": [
    [
     "Ami",
     "Moyal"
    ],
    [
     "Arnon",
     "Cohen"
    ]
   ],
   "title": "Speaker independent phoneme recognition using a heuristic search",
   "original": "e93_0599",
   "page_count": 4,
   "order": 142,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "This paper describes a speaker independent (Hebrew) phoneme recognition system from continuous speech. The system is based on a heuristic algorithm which performs sequential recognition of phonemes in several paths. Each path involves sequential identification of a string of phonemes by adding one phoneme at each step, to the accumulated phoneme string. The added phoneme is selected to maximize the probability of the new string, taking into account phoneme duration and neighborhood probabilities. The proposed algorithm has several advantages. It incorporates a priori knowledge on phoneme duration and neighborhood, it provides a set of the N best strings (with their probabilities) rather than the best string only and it may easily be implemented by a parallel processor. In an experimental evaluation of the proposed algorithm the following recognition results were achieved : 67.14% correct, 30.00% substitutions, 2.86% deletions and 33.25% insertions. The Viterbi algorithm [1] under the same conditions lead to the following results : 53.64% correct, 34.94% substitution, 11.43% deletions and 22.21% insertions. These results were estimated from the identified strings of phonemes by the well known weighted Levenshtein distance.\n",
    "Keywords: Continuous speech, Phoneme recognition, Heuristic algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-141"
  },
  "class93_eurospeech": {
   "authors": [
    [
     "F.",
     "Class"
    ],
    [
     "A.",
     "Kaltenmeier"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Optimization of an HMM - based continuous speech recognizer",
   "original": "e93_0803",
   "page_count": 4,
   "order": 143,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "This paper describes some optimizations to our speech recognition system, which is based on semi-continuous Hidden Markov Models (SCHMM) of subword units. The optimizations pertain to codebook generation, Linear Discriminant Analysis (LDA), initialized training, and definition of subword units. The recognition rate of the continuous version of the system increased from 79% to 95% combining all of the optimization steps.\n",
    "Keywords: SCHMM, LDA, continuous speech recognition, codebook generation, sub-word units\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-142"
  },
  "aerens93_eurospeech": {
   "authors": [
    [
     "Marco S.",
     "Aerens"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Linear and nonlinear prediction for speech recognition with hidden Markov models",
   "original": "e93_0807",
   "page_count": 4,
   "order": 144,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "When using hidden Markov models for speech recognition, it is usually assumed that the probability that a particular acoustic vector is emitted at a given time only depends on the current state and the current acoustic vector observed. This model does not take account of the dynamic nature of the speech signal. In order to introduce time correlation between successive acoustic vectors, some authors have proposed to consider the time series of observations on a state to be generated by a nonlinear deterministic process corrupted by a Gaussian additive noise. This results in the introduction of the prediction error in the likelihood function. In this paper, we review the basic ideas underlying these models. Thereafter, we briefly introduce an extension of the linear case, i.e. we permit the autoregressive coefficients to be corrupted by noise. Indeed, when working at the speech samples level, this is a simple way to take the intra and inter speaker variability into account; that is, to allow variability in the transfer function. In fact, this is what we are doing when extracting LPC coefficients and clustering them with Gaussian distributions. The advantage here is that we directly introduce the variability at the sample level. This leads to processes that are known as AutoRegressive Conditional Heteroscedastic (ARCH) processes, with nonconstant variances conditional on the past.\n",
    "Keywords: Hidden Markov models, autoregressive models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-143"
  },
  "lokbani93_eurospeech": {
   "authors": [
    [
     "M. N.",
     "Lokbani"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monne"
    ]
   ],
   "title": "Segmental post-processing of the n-best solutions in a speech recognition system",
   "original": "e93_0811",
   "page_count": 4,
   "order": 145,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "This paper presents a new statistical segmental post-processing of the N-best solutions obtained by a hidden Markov model based speech recognizer. This post-processing computes, for each solution, a new segmental score which is latter combined with the markovian score. The solution with the highest score identifies the answer. We introduce a new method in the statistical modelling of each segment involving two models: one for correct segmentation and one for incorrect segmentation. The first results presented here shows that this new approach leads to a 10 to 15 % reduction of the error rate.\n",
    "Keywords: N-best solutions, segmental post-processing, statistical modelling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-144"
  },
  "matsuoka93_eurospeech": {
   "authors": [
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study of on-line Bayesian adaptation for HMM-based speech recognition",
   "original": "e93_0815",
   "page_count": 4,
   "order": 146,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "In this paper, we study issues related to the use of online incremental adaptation for speech recognition. We use the segmental MAP algorithm to perform HMM adaptation. Two modes of incremental adaptation, namely supervised and unsupervised adaptation, are tested, in the supervised mode, the correct word transcription is used for MAP adaptation while in the unsupervised mode, the word transcription is provided by the recognizer. We report on results obtained for the DARPA Air Travel Information System (ATIS) task. Compared with the speaker independent recognition results, the supervised and unsupervised incremental adaptation algorithms reduce the word error rate by 25% and 5.6% respectively. More study is needed in improving adaptation efficiency and effectiveness. More study is also needed in bridging the gap between supervised and unsupervised adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-145"
  },
  "maxwell93_eurospeech": {
   "authors": [
    [
     "B. A.",
     "Maxwell"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Hidden Markov models using shared vector linear predictors",
   "original": "e93_0819",
   "page_count": 4,
   "order": 147,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "It has been previously shown that augmenting a standard HMM with a set of vector linear predictors can improve recognition rates compared with standard HMMs. The set of vector linear predictors associated with each state improve the HMMs ability to model the correlations in real speech data, and help to overcome the HMM state-conditional independence assumption. However, introducing extra parameters into the model requires more training data. This problem can be partly overcome by sharing the predictor parameters between multiple HMM states, and hence more robust, but less specific estimates of the predictor parameters are obtained. This paper develops the theory and im- plementation of arbitrarily shared vector linear prediction for hidden Markov models. For most predictor offsets, predictors shared across all states of an HMM provide more accurate recognition on both training and test data sets than equivalent HMMs without predictors when evaluated on a British English E-set recognition task.\n",
    "Keywords: Speech recognition, vector prediction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-146"
  },
  "omologo93_eurospeech": {
   "authors": [
    [
     "M.",
     "Omologo"
    ],
    [
     "P.",
     "Svaizer"
    ]
   ],
   "title": "Talker localization and speech enhancement in a noisy environment using a microphone array based acquisition system",
   "original": "e93_0605",
   "page_count": 4,
   "order": 148,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "This paper deals with the use of linear microphone arrays for detection, localization and enhancement of a generic acoustic message produced in a noisy environment. A CrosspowerSpectrum Phase based analysis and a Coherence Measure representation are presented, that allow an accurate time delay estimation employed for the acoustic source position hypothesis. Preliminary results in terms of source localization accuracy are given. Once source position is estimated, an enhanced version of the original acoustic message is derived, that can represent the input for a speech recognition system.\n",
    "Keywords: Microphone Arrays, Talker Localization, Speech Enhancement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-147"
  },
  "kobayashi93_eurospeech": {
   "authors": [
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Toshio",
     "Kanno"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Generalized cepstral modeling of speech degraded by additive noise",
   "original": "e93_0609",
   "page_count": 4,
   "order": 149,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "This paper proposes a technique for estimating speech parameters in noisy environment. The technique uses a spectral model represented by generalized cepstrum. Parameter estimation is based on maximum a posteriori (MAP) estimation. An iterative approach which has been formulated for all-pole modeling is applied to the generalized cepstral modeling. Since the generalized cepstral model includes the all-pole model as a special case, the technique can be viewed as a generalization of the all-pole modeling based on MAP estimation. The proposed technique is applied to the enhancement of speech and several experimental results are also shown.\n",
    "Keywords: Generalized Cepstrum, MAP Estimation, Speech Enhancement, Speech Analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-148"
  },
  "bakamidis93_eurospeech": {
   "authors": [
    [
     "Stylianos",
     "Bakamidis"
    ],
    [
     "George",
     "Carayannis"
    ]
   ],
   "title": "Noise quality improvement through SVD equalization",
   "original": "e93_0613",
   "page_count": 4,
   "order": 150,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "In the modern computer environments, special software is available for noise generation. Depending on the quality of the software, usually also related to the hardware, the spectral flatness of the obtained noise is generally acceptable for long sequences. For shorter sequences the open loop procedure usually utilized by the previously mentioned software fails to give the expected results. For number of samples less than one thousand the spectral flatness which is equivalent to the eigen value spread needs to be improved. A new method is proposed here based on the SVD analysis technique which leads to the improvement of the spectral flatness. For a given noise frame an algorithm is provided giving the possibility to set the eigenvalues to equal magnitudes which has as consequence the regeneration of a noise with increased spectral flatness. Simulation results gave significant improvement for both uniform and gaussian noise distributions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-149"
  },
  "xie93_eurospeech": {
   "authors": [
    [
     "Fei",
     "Xie"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Speech enhancement by nonlinear spectral estimation - a unifying approach",
   "original": "e93_0617",
   "page_count": 4,
   "order": 151,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "Noise suppression for speech enhancement is desirable in various speech applications. In this paper we present a solution to the nonlinear spectral estimation problem for speech enhancement. We start from rather simple statistical models for the short time spectral estimates of speech and noise (log-uniform or log-normal). By empirical data generation and a curve fitting approach we are able to get explicit, though simple, expressions for the MMSE estimator in function of input level and the model parameters for each frequency component. The great advantage of our approach is that it has a sound theoretical foundation, is general by the choice of its parameters, and almost as simple to use as classical spectral subtraction.\n",
    "Keywords: MMSE, Spectral estimation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-150"
  },
  "kroschel93_eurospeech": {
   "authors": [
    [
     "Kristian",
     "Kroschel"
    ],
    [
     "Keld",
     "Lange"
    ]
   ],
   "title": "Subband array processing for speech enhancement",
   "original": "e93_0621",
   "page_count": 4,
   "order": 152,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "Classical array processing systems for speech enhancement include three components. The first one is used for delay compensation of the speech signal in the different microphone channels, the second component is based on spectral subtraction or Wiener filtering to enhance the signal-to-noise-ratio, and the third component has to compensate residual noise components like musical tones. In this paper a new system based on this principal approach is presented. Instead of pure delay compensation an equalizer is used which compensates the delay of the speech signals and the differences of the transfer function of the different microphone channels. Depending on their position the microphones are related to subsections of the speech spectrum to avoid a dynamic delay compensation caused by the movement of the head of the speaker. A third improvement over the classical approach is given by the fact that instead of a classical FFT algorithm the Short-Time Fourier Transform (STFT) proposed by Portnoff is used which has been implemented using the FFT. Since the speech signal and the noise are instationary processes this transform is favourable. With this configuration the post processing is a simple addition of the partial results because the musical tones have been significantly removed by the other components of the system. It has been shown that the method presented in this paper can be realized in real time using an Intel i860 processor.\n",
    "Keywords: Array processing, noise reduction, speech enhancement, Short-Time Fourier Transform.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-151"
  },
  "pean93_eurospeech": {
   "authors": [
    [
     "Vincent",
     "Pean"
    ],
    [
     "Sheila",
     "Williams"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "The design and recording of icy, a corpus for the study of intraspeaker variability and the characterisation of speaking styles#",
   "original": "e93_0627",
   "page_count": 4,
   "order": 153,
   "p1": "627",
   "pn": "630",
   "abstract": [
    "This paper describes a corpus for the study of intraspeaker variability and of contrastive speaking styles. Design accounted for: different speaking styles being compared for the same linguistic content for a given speaker; the speaking styles being clearly produced and perceived to be different without direct prompting or reading; a series of specific predetermined contexts of phonological variation being elicited. In the \"seven errors\" task, the speaker describes objects which differ in two drawings and are places where possible phonological variation may occur. Styles include casual, clear and clear read. The signal from one speaker occupies about 60 MBytes (audio) in all, about 30 minutes of speech, representing an average of about 70 \"phonological contexts\" per speaker per style, covering: voicing, devoicing, schwa elimination, palatalisation, nasalisation, and geminates. Twenty one speakers of varying origins have already been recorded. The data is being labelled, using semi-automatic labelling techniques and will be perceptually verified to confirm style change. This methodology  has  also  been  used to collect a corresponding corpus of British English.\n",
    "Keywords: speaking styles, variability, speaker characterisation, database\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-152"
  },
  "ljolje93_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Speaker clustering for improved speech recognition",
   "original": "e93_0631",
   "page_count": 4,
   "order": 154,
   "p1": "631",
   "pn": "634",
   "abstract": [
    "Inter-speaker variability is one of the major problems in speaker independent speech recognition. Performance achieved in speaker dependent experiments far surpasses results achieved in speaker independent recognition experiments when using similar training data and recognizer structures. In this work we attempt to apply Maximum Entropy clustering to the speaker clustering problem for large vocabulary speech recognition. This technique avoids the problems associated with training using insufficient data. It is achieved by generating speaker cluster dependent models using a weighted sum of the cluster dependent data and cluster independent data. The speech models in the experiments use Hidden Markov Models to model phones, whose states are weighted Gaussian mixtures. The clustering algorithm only adjusts the mixture weights and does not modify any other part of the model. This results in only a modest performance improvement. Phone accuracy when tested on the DARPA Resource Management task improves from 83.75% to 84.15%, but the word accuracy on the Feb89 test set remains virtually unchanged.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-153"
  },
  "heuvel93_eurospeech": {
   "authors": [
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "A. C. M.",
     "Rietveld"
    ]
   ],
   "title": "Speaker-variability in spectral bands of dutch vowel segments",
   "original": "e93_0635",
   "page_count": 4,
   "order": 155,
   "p1": "635",
   "pn": "638",
   "abstract": [
    "Lists of 24 isolated /CVCa/-words were read out by 15 male native speakers of Dutch with ten replications. The middle frames of the steady-state parts of the vowels (/a,i,u/) were examined. These were coded as auditory band spectra containing 19 elements (filter bands) per vector. Discriminant analyses were carried out for every vowel so as to ascertain which filter bands were most effective in discriminating the speakers. It was then studied how these filter bands were related to formant positions. Three hypotheses were evaluated. The hypothesis that speaker-identity is primarily coded in formant band-widths showed the closest match to the results obtained.\n",
    "Keywords: Acoustic theory of speech; Speaker identification and verification\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-154"
  },
  "itahashi93_eurospeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Kimihito",
     "Tanaka"
    ]
   ],
   "title": "A method of classification among Japanese dialects",
   "original": "e93_0639",
   "page_count": 4,
   "order": 156,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "This paper describes a method of dialect classification of spoken Japanese, based on fundamental frequency (F0) contours of speech. Speech data of the famous Japanese folk tale \"Peach Boy\" were taken from a CD- ROM Japanese dialect speech corpus. The 14 dialect speech data were chosen from the major dialect districts. First, a jFo contour was approximated by a set of polygonal lines so that the mean square error between the lines and jFo values was minimized; the optimum boundaries of the lines were determined using a dynamic programming procedure. The starting frequency, slope and duration of each line were calculated. Secondly, parameters derived from Fq pattern were analyzed by using principal component analysis. Nineteen parameters including mean values and standard deviations of the above parameters were used for the analysis. Results show that the 14 dialects can be classified, based on these parameters.\n",
    "Keywords: Spoken Japanese dialect, Fundamental frequency, Slope of F0 contour, Principal Component analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-155"
  },
  "hernandezmendez93_eurospeech": {
   "authors": [
    [
     "J. A.",
     "Hernandez-Mendez"
    ],
    [
     "Anibal R.",
     "Figueiras-Vidal"
    ]
   ],
   "title": "Measuring similarities among speakers by means of neural networks",
   "original": "e93_0643",
   "page_count": 4,
   "order": 157,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "We compare several neural networks architectures to measure the degree of similarity among speakers. For each speaker of a reference set, Multilayer Perceptrons and Radial Basis Functions are trained to perform a non-linear principal component analysis of acoustic vectors, and Self-Organized Feature Maps are used to construct Vector Quantizers. As a first simple step, we use non-discriminant training to characterize speakers, and, then, the result is applied to combine speaker-dependent speech recognition models. In a second phase, discriminant training over speaker models is carried out, and speaker verification and identification performances of these networks are evaluated.\n",
    "Keywords: Speech recognition, speaker recognition, neural networks, similarity measures, models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-156"
  },
  "rangoussi93_eurospeech": {
   "authors": [
    [
     "Maria",
     "Rangoussi"
    ],
    [
     "Stylianos",
     "Bakamidis"
    ],
    [
     "George",
     "Carayannis"
    ]
   ],
   "title": "Robust endpoint detection of speech in the presence of noise",
   "original": "e93_0649",
   "page_count": 4,
   "order": 158,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "Endpoint detection of noisy speech signals is the problem addressed in this work. Existing endpoint detection methods perform poorly on speech signal recordings from low SNR environments, thus justifying the use of more sophisticated - yet more computationally expensive - methods. Two different methods are proposed here, based on the singular value decomposition (SVD) of the autocorrelation or the observation matrix of the speech signal. Modifications that reduce the computational complexity, as well as adaptive forms are also investigated. Results and relative merits of the proposed methods as compared to existing ones are illustrated on simulated noisy speech data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-157"
  },
  "angelini93_eurospeech": {
   "authors": [
    [
     "B.",
     "Angelini"
    ],
    [
     "F.",
     "Brugnara"
    ],
    [
     "D.",
     "Falavigna"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "R.",
     "Gretter"
    ],
    [
     "M.",
     "Omologo"
    ]
   ],
   "title": "Automatic segmentation and labeling of English and Italian speech databases",
   "original": "e93_0653",
   "page_count": 4,
   "order": 159,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "A system for automatic segmentation and labeling of speech has been developed that provides phone boundaries, given the linguistic content of a speech utterance. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer. Starting from some phonological rules, a network is derived that represents a wide variety of possible phonetic realizations of a given text, in order to cope with pronunciation variabilities. Given this network, both the most likely phone sequence and phone boundaries are determined by using the Viterbi algorithm. The system has been developed and tested both on the DARPA-TIMIT acoustic-phonetic continuous speech database of American English and on a similar Italian database. Given a tolerance of 20 ms, the system provides a correct boundary location of 86.2% for American English (90.9% for Italian), when trained with 256 (158 for Italian) phonetically rich sentences.\n",
    "Keywords: Speech Databases, Segmentation and Labeling, Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-158"
  },
  "farhat93_eurospeech": {
   "authors": [
    [
     "Azarshid",
     "Farhat"
    ],
    [
     "Guy",
     "Perennou"
    ],
    [
     "Regine",
     "Andre-Obrecht"
    ]
   ],
   "title": "A segmental approach versus a centisecond one for automatic phonetic time-alignment",
   "original": "e93_0657",
   "page_count": 4,
   "order": 160,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "This paper investigates a new time-alignment system of a speech waveform with its phonetic transcription. This system is based on continuous Hidden Markov Models (HMM) associated with Mel frequency cepstral coefficients (MFCC). Two different approaches are developed. In the first approach, namely the centisecond approach, the alignment is performed in one pass. The second one, namely the segmental approach, proceeds in two phases to achieve the phonetic alignment: the speech signal is firstly segmented with a temporal method. Each segment is represented by a vector of MFCC and constitutes an observation of our global HMM. The best results are obtained with the segmental approach. It produces 25% of disagreement with manual labelling at an accuracy of ±20 ms with only ten context-independent classes of phones.\n",
    "Keywords: Phonetic time-alignment, Hidden Markov Models, automatic segmentation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-159"
  },
  "heroaez93_eurospeech": {
   "authors": [
    [
     "I.",
     "Heroaez"
    ],
    [
     "J.",
     "Barandiaran"
    ],
    [
     "E.",
     "Monte"
    ],
    [
     "B.",
     "Etxebarria"
    ]
   ],
   "title": "A segmentation algorithm based on acoustical features using a self organizing neural network",
   "original": "e93_0661",
   "page_count": 3,
   "order": 161,
   "p1": "661",
   "pn": "663",
   "abstract": [
    "In this paper we investigate the use of a self-organizing map in an acoustic segmentation task. The aim is to obtain a limited number of acoustic classes and to segment whenever a change in the class between two adjacent frames occurs. Energy in different frequency ranges is used as input in the map training process. A structure based on a Kohonen map connected to a neural network trained with the back-propagation algorithm is proposed.\n",
    "Keywords: Kohonen map, neural networks, segmentation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-160"
  },
  "cosi93_eurospeech": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "SLAM: segmentation and labelling automatic module",
   "original": "e93_0665",
   "page_count": 4,
   "order": 162,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "An interactive Segmentation and Labelling Automatic Module (SLAM), especially developed for Windows-based Personal Computers, is described. The system is extremely user-friendly and it was designed with the aim of supporting speech scientists in assessing the very heavy and time-consuming task of segmenting a big amount of speech material such as that caused by the tremendous spread of new and always bigger speech data-bases. The system, which is based on the Multi-Level Segmentation theory, was built using Microsoft C++ and Windows 3.1 SDK software (*), and runs preferably on Intel 386/486-based personal computers running DOS 5.00 or higher and equipped with VGA and SuperVGA boards.\n",
    "Keywords: Multi-Level segmentation, Windows. (*) The following are trademarks of their respective companies: Intel 386/486 (Intel Corp.) Microsoft, DOS, Windows 3.1, MS SDK, MS C++ , MDI (Microsoft Corp.), PC-AT, VGA (IBM Corp.), Oros (Oros Inc.).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-161"
  },
  "heise93_eurospeech": {
   "authors": [
    [
     "Christian",
     "Heise"
    ],
    [
     "Hans-H.",
     "Bothe"
    ]
   ],
   "title": "Phone and syllable segmentation by concurrent window modules",
   "original": "e93_0669",
   "page_count": 4,
   "order": 163,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "This paper presents the state of the art of a phone and syllable segmentation program under NeXTStep with a graphically implemented user interface. It is based on a unique window technique that allows a concurrent visualization of the speech signal in different visual representations. Additionally, the user can output a selected part of the speech signal by means of the NeXT sound system. Representative key-pictures of sounds or certain sound clusters may be marked simultaneously with the help of an attached video recorder.\n",
    "Keywords: Concurrent Processing on DSP, Multi-user System, Segmentation, Sonagram, Syllables\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-162"
  },
  "eisen93_eurospeech": {
   "authors": [
    [
     "Barbara",
     "Eisen"
    ]
   ],
   "title": "Reliability of speech segmentation and labelling at different levels of transcription",
   "original": "e93_0673",
   "page_count": 4,
   "order": 164,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "The investigation presented in this paper is a first attempt to specify the reliability of manual segmentation and labelling. It is demonstrated that transcription performance does not only rely on external conditions, i. e. segmentation criteria, transcribers' labelling experience or characteristics of labelling devices but rather depends on internal acoustic-phonetic features of the analysed utterance and the level of abstraction of description. Three levels of transcription are introduced. The reliability of segmented data at each level is discussed on the basis of consistency measures within the judgements of different transcribers.\n",
    "Keywords: Speech segmentation, labelling, levels of transcription, interindividual consistency\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-163"
  },
  "bergem93_eurospeech": {
   "authors": [
    [
     "Dick R. van",
     "Bergem"
    ]
   ],
   "title": "On the perception of acoustic and lexical vowel reduction",
   "original": "e93_0677",
   "page_count": 4,
   "order": 165,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "The present study was designed to investigate how well listeners are able to unambiguously categorize an unstressed vowel in a word as either a full vowel or a schwa. It was found that listeners disagree in many cases on the assignment of a vowel to either of these categories. This suggests that listeners cannot properly distinguish between acoustic reduction (the loss of spectral quality of a full vowel) and lexical reduction (the substitution of a full vowel with a schwa). Other points of interest in the present study were the frequency of occurrence of words and speech styles; both were found to have a considerable influence on the process of vowel reduction.\n",
    "Keywords: Vowel reduction, vowel categories, frequency of occurrence of words, speech styles.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-164"
  },
  "ooyen93_eurospeech": {
   "authors": [
    [
     "Brit van",
     "Ooyen"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Pier Marco",
     "Bertinetto"
    ]
   ],
   "title": "Click detection in Italian and English",
   "original": "e93_0681",
   "page_count": 4,
   "order": 166,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "We report four experiments in which English and Italian monolinguals detected clicks in continous speech in their native language. Two of the experiments used an off-line location task, and two used an on-line reaction time task. Despite there being large differences between English and Italian with respect to rhythmic characteristics, very similar response patterns were found for the two language groups. It is concluded that the process of click detection operates independently from language-specific differences in perceptual processing at the sublexical level.\n",
    "Keywords: Speech perception, English, Italian, vowel duration, reaction time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-165"
  },
  "nix93_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Nix"
    ],
    [
     "Gareth",
     "Gaskell"
    ],
    [
     "William",
     "Marslen-Wilson"
    ]
   ],
   "title": "Phonological variation and mismatch in lexical access",
   "original": "e93_0685",
   "page_count": 4,
   "order": 167,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "Research into human speech recognition reveals an apparent conflict between the high resolution of the speech processing mechanisms and the variability of the speech input. Access to the lexicon is disrupted even by single feature mismatches, while fluent speech is marked by a wide variety of phonological processes which change the surface form of the words being uttered. The word sweet, for example, may be produced as [swit], [swik] or [swip], depending on its right context. This is a problem for a theory of lexical access, since sweek is a nonword, while sweep is an existing word in the language, and yet in each case the correct outcome is the recognition that the word sweet is intended. In a series of studies of this phenomenon, using gating and priming techniques, we find that phonologically regular distortions of the speech input do not create mismatch in lexical access, so long as these distortions occur in the appropriate phonological context. We discuss the implications of these findings for a theory of the speech recognition process, emphasising the abstractness of lexical form representations and the on-line involvement of processes of phonological inference in determining the correct interpretation of the incoming speech signal.\n",
    "Keywords: Phonology, Variation, Lexical Access, Mental Representations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-166"
  },
  "zon93_eurospeech": {
   "authors": [
    [
     "Monique van",
     "Zon"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Perception of word boundaries by dutch listeners",
   "original": "e93_0689",
   "page_count": 4,
   "order": 168,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "For understanding speech listeners spontaneously use a strategy that segments the speech stream into units. Recent work based on English suggests that speech segmentation is triggered by strong syllables. This was shown by a paradigm in which word boundary misperceptions were elicited in a task in which English native speakers listened to faintly audible fragments of sentences. In the present study we examined whether similar processing strategies can be observed for Dutch native speakers using the same paradigm. The results for Dutch are as predicted by the Metrical Segmentation Strategy.\n",
    "Keywords: speech perception, stress, segmentation strategy\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-167"
  },
  "bonneau93_eurospeech": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Linda",
     "Djezzar"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Perception of French stop bursts, implications for stop identification",
   "original": "e93_0693",
   "page_count": 4,
   "order": 169,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "An experiment is presented here concerning perception of release bursts in a corpus of natural tokens of French /p,t,k/ in CV context. The focus of the study was the perceptual role of spectral characteristics of the release burst; therefore tokens have the same duration of nearly 25 ms so that neither VOT nor formant transitions may influence listeners. Two training sessions appeared to be necessary to reach the utmost of listeners' ability to identify stops. Results show that the recognition rate are approximately the same for /p/ (89%), /t/ (87%) and /k/ (86%). More precisely, identification of /k/ in the context of a front vowel is not so high (75%) and is often confused with /t/; /k/ in the context of a back vowel is very well identified (98%). /t/in the context of /y/ or / / is confused (22%) with /k/. These very high identification rates militate in favour of acoustical invariants, even if, in some cases (for example in the context of a back vowel), the knowledge of the subsequent vowel may help the stop identification. Therefore, in order to provide further indication concerning the design of an analytical stop recognition system we plan to examine the role of prior knowledge of the subsequent vowel type.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-168"
  },
  "kacic93_eurospeech": {
   "authors": [
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Using isofrequency neural column for harmonic sound scene decomposition",
   "original": "e93_0697",
   "page_count": 4,
   "order": 170,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "In our paper we describe a novel technique for harmonic sound scene decomposition. It is based on the \"frequency-place\" mapping of a sound event. The defined set of harmonic neural networks follows the same organizational principles as can be seen in the auditory pathways and in the cerebral cortex of the human brain (tonotopic and vertical organizations of the neurons). The defined set of harmonic networks consists of 1066 first-class neurons, 23344 second-class neurons and 23344 interneurons. The experimental results have shown that the defined set of networks is capable to perform similar \"frequency-place\" mapping of the sound events, as is sought to be carried out in the auditory pathways. The results also indicate that the defined set might be useful in corrupted speech processing and speaker-independent speech processing and recognition\n",
    "Keywords: harmonic sound event, isofrequency neural column, tonotopic organization, frequency-place mapping, harmonic neural network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-169"
  },
  "datta93_eurospeech": {
   "authors": [
    [
     "A. K.",
     "Datta"
    ]
   ],
   "title": "Do ear perceive vowel through formants?",
   "original": "e93_0701",
   "page_count": 4,
   "order": 171,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "Vowels are believed to be perceived on the basis of the first two formants extracted by the auditory mechanism through an analysis of amplitude spectra. Nine pairs of regenerated vowel sounds are presented here each of which has same Fl and F2 but different perceptual categories. The regeneration procedure uses inter-alia the manipulation of phase spectra without disturbing the amplitude spectra.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-170"
  },
  "vyas93_eurospeech": {
   "authors": [
    [
     "Trupti",
     "Vyas"
    ],
    [
     "Michael J.",
     "Pont"
    ],
    [
     "Seyed J.",
     "Mashari"
    ]
   ],
   "title": "Speech recognition using auditory models and neural networks",
   "original": "e93_0705",
   "page_count": 4,
   "order": 172,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper explores the use of previously described computational model of sections of the mammalian auditory system (Pont and Damper, 1991) as the \"front end\" to a speech recognition system based on a conventional neural network. In the paper, experiments using two different auditory front ends were implemented, the first based on a model of the auditory nerve (AN), the second based on a model of the dorsal cochlear nucleus (DCN). In each case, the neural network was a multi-layered Perceptron. In the first experiment, the two hybrid recognisers were tested on isolated digits recorded in quiet conditions. Here it was found that both AN- and DCN-based models performed excellently. In the second experiment the same stimuli were used, but this time with added noise. In this case it was found that the performance of the AN-based recogniser remained high, but - as the SNR was decreased - the performance of the DCN-based device fell of dramatically. The results are discussed and some suggests for further studies are made.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-171"
  },
  "ma93c_eurospeech": {
   "authors": [
    [
     "Changxue",
     "Ma"
    ],
    [
     "Armin",
     "Kohlrausch"
    ]
   ],
   "title": "The influence of temporal processes on spectral masking patterns of harmonic complex tones and vowels",
   "original": "e93_0709",
   "page_count": 4,
   "order": 173,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "The detection of narrowband noise targets in maskers of equal-amplitude harmonics has been reported in an earlier contribution [4]. The results revealed that the masking patterns are strongly dependent on the fundamental frequency of the masker. In the present paper we report masking patterns of broadband harmonic complexes as a function of their spectral tilt and level, and the masking pattern of a vowel sound. We will focus on how the spectral masking patterns are influenced by temporal processes in the auditory system.\n",
    "Keywords: Auditory masking.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-172"
  },
  "kuwabara93_eurospeech": {
   "authors": [
    [
     "Hisao",
     "Kuwabara"
    ]
   ],
   "title": "Temporal effect on the perception of continuous speech and a possible mechanism in the human auditory system",
   "original": "e93_0713",
   "page_count": 4,
   "order": 174,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "Previous study suggests that a certain mechanism which makes acoustically uncertain words clear via contextual information exists in the process of human speech recognition. This mechanism is probably the most important one and plays a central role in the human speech recognition. This study has been conducted to investigate, through perceptual experiments, on how the mechanism works as a continuous speech proceeds and where it is located in the human auditory system. A dichotic fusion experiment has been conducted using two types of continuous three-vowel sequences. The fusion actually took place when the formants of the two stimuli were the same or very close to each other. A possible explanation was given under the working hypotheses postulated for this study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-173"
  },
  "jones93b_eurospeech": {
   "authors": [
    [
     "Edward",
     "Jones"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Comparison of various adaptation mechanisms in an auditory model for the purpose of speech processing",
   "original": "e93_0717",
   "page_count": 4,
   "order": 175,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "This paper presents preliminary results from the comparison of a number of existing models for the inner hair cell/synapse region of the auditory periphery, for the task of speech processing. Particular emphasis is placed on the models' ability to enhance dynamic characteristics of the speech signal, especially abrupt changes in amplitude, as reflected in the adaptation properties of each model. Such characteristics could be useful for the task of phonetic segmentation. Three adaptation models were chosen, and they were each combined with a computational model of basilar membrane mechanics for the purpose of generating responses. The responses of the basilar membrane/adaptation model combinations are compared with the responses which were obtained with a simple envelope-detecting type of inner hair cell model. In addition, an alternative model for adaptation is proposed. This model is computationally very simple, and is optimised for the task of enhancing dynamic aspects of the speech signal, rather than attempting to faithfully reproduce physiological data, as is the case with most other models. It is intended that a model of the type proposed in this paper could be useful for real-time implementation in a front-end processor for a speech recognition system.\n",
    "Keywords: speech processing, auditory modelling, neural adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-174"
  },
  "vartanian93_eurospeech": {
   "authors": [
    [
     "I. A.",
     "Vartanian"
    ],
    [
     "T. V.",
     "Chernigovskaya"
    ]
   ],
   "title": "Sensory-motor manifestations of speech-hearing interaction",
   "original": "e93_0721",
   "page_count": 3,
   "order": 176,
   "p1": "721",
   "pn": "723",
   "abstract": [
    "Analysis of our own experimental data and literature on morpho-functional organization of acoustic communication system in humans in comparison with animals suggests the unity of its separate substrata functioning, depending on the type of sensory-motor coordination of the final effectory reactions as well as on the complexity of the task presented. The procedure was developed to test this hypothesis, aiming at simultaneous perception of auditory stimuli monaurally presented and generation of vocal signals of the two types: a), precise imitation of the sound presented; b). recognition of complex sounds against spontaneous imitation of speech sounds -vowels, consonants or frequency modulated voice. The results show that precise imitation of familiar vowels of the native language is a nonlateralized function whereas precise imitation of unfamiliar vowels of a foreign language is the left-lateralized function demanding considerably more time for its realization than the first one. Recognition of artificial complex sounds depends on the quality of self-generated sounds.The data demonstrate an evidence of left hemisphere lateralization in rigid interaction of articulation program and acoustical recognition system, while vocalization-auditory interaction seems to be symmetrical.\n",
    "Keywords: speech-hearing interaction, lateralization\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-175"
  },
  "chernigovskaya93_eurospeech": {
   "authors": [
    [
     "T. V.",
     "Chernigovskaya"
    ],
    [
     "I. A.",
     "Vartanian"
    ],
    [
     "T. I.",
     "Tokareva"
    ]
   ],
   "title": "Syllable perception: lateralization of native and foreign languages",
   "original": "e93_0725",
   "page_count": 2,
   "order": 177,
   "p1": "725",
   "pn": "726",
   "abstract": [
    "Some aspects of speech-sound processing lateralization are discussed. Hemispheric differences in the execution of the tasks are investigated depending on the type of reaction (vocal or manual), different sides of stimulation, type of stimuli, presented monaurally to different ears of normal subjects. The foregoing acoustic and language experience of the subjects is considered in connection with their ability to fulfill the experimental task. The data demonstrate that processing of \"native \" and \"foreign\" syllables has the tendency to be controlled by different hemispheres:\"foreign\" need mostly left cerebral mechanisms both for the mimicking and for the identification of stimuli, while \"native\" can involve both hemispheres. Individual background plays significant role in cerebral balance of speech processing.\n",
    "Keywords: speech processing, lateralization, categorization\n",
    ""
   ]
  },
  "pont93_eurospeech": {
   "authors": [
    [
     "Michael J.",
     "Pont"
    ]
   ],
   "title": "Simulation of short-latency auditory evoked potentials: a pilot study",
   "original": "e93_0727",
   "page_count": 4,
   "order": 178,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "A technique is described for the simulation of brainstem auditory evoked potentials by means of a previously described computational model of the pre-cortical auditory nervous system [Pont, MJ. and Damper, R.I. (1991) J. Acoust. Soc. Am. 89: 1213-1228]. The simulated responses are seen to mimic responses obtained from subjects with normal and impaired hearing. Some proposed extensions both to the model and the technique are outlined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-176"
  },
  "kolinsky93_eurospeech": {
   "authors": [
    [
     "Regine",
     "Kolinsky"
    ],
    [
     "Jose",
     "Morais"
    ]
   ],
   "title": "Intermediate representations in spoken word recognition: a cross-linguistic study of word illusions",
   "original": "e93_0731",
   "page_count": 4,
   "order": 179,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "Listeners of either French or Portuguese experienced word illusions in a word detection task by combining constituents of two dichotically-presented sequences. Consonants migrated to a greater extent in Portuguese than in French. This difference could stem from the influence of vocalic reduction in Portuguese. Alternatively, the difference could be due to the different stress patterns of the two languages.These alternative hypotheses were tested by examining Brazilian Portuguese, which does not show a degree of vocalic reduction as important as in European Portuguese. The results show that Brazilian perform more like the Portuguese than the French. They thus support the idea that stress patterns might be crucial in determining the nature of the language-dependent intermediate representations of speech.\n",
    "Keywords: speech intermediate representations, influence of stress patterns and of vocalic reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-177"
  },
  "cao93_eurospeech": {
   "authors": [
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "Time - varing manner on formant trajectories of Chinese diphthongs",
   "original": "e93_0735",
   "page_count": 4,
   "order": 180,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "To examine the time-varing manner on articulatory movement of the diphthongs, an investigation related to Chinese diphthongs is conducted by observing the durational distribution within these sounds in Standard Chinese. The preliminary results obtained here indicate that articulatory movement from one target towards another in diphthongs is not casual, but well-planned in time domain, and as the acoustic correlate, the time-varing manner of formant trajectories is determined by certain phonological and phonetic constraints of a language. Consequently, it is language-specific.\n",
    "Keywords: diphthong, temporal distribution\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-178"
  },
  "gong93b_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Iterative transformation and alignment for speech labeling",
   "original": "e93_1759",
   "page_count": 4,
   "order": 181,
   "p1": "1759",
   "pn": "1762",
   "abstract": [
    "We address the problem of labeling phonemes in a large database of spoken sentences using a different, already labeled utterance of each sentence. In labeling an utterance, a time alignment between the utterance and a reference utterance is required. This alignment compares parameter vectors from different speakers and is therefore subject to an acoustic mismatch. The accuracy of conventional alignment methods which do not deal with the cross-speaker mismatch is thus limited. We improve speech database labeling by iterating two steps: alignment against the labeled speech and transformation of parameter vectors from test to reference. We propose to use a linear scalar transform for each component of a parameter vector. Under our experimental conditions, the results showed that the misalignment between the reference and test utterances was reduced, and the improved alignment improved the recognition rate. A recognition system trained using the phonetic labels provided by iteration of the alignment process gave a 50% reduction in sentence recognition error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-179"
  },
  "hubener93_eurospeech": {
   "authors": [
    [
     "Kai",
     "Hübener"
    ],
    [
     "Andreas",
     "Hauenstein"
    ]
   ],
   "title": "Controlling search in segmentation lattices of speech signals",
   "original": "e93_1763",
   "page_count": 4,
   "order": 182,
   "p1": "1763",
   "pn": "1766",
   "abstract": [
    "Multi-level segmentation of speech signals has become increasingly popular for quite a while. It yields a rich representation which captures both coarse and fine acoustic information in a uniform framework. However, determining which segments are useful, and how they should be combined to arrive at the correct segmentation of the acoustic signal has proved to be rather difficult. The approach described in this paper uses segment classification confidences as well as dynamically generated segment duration constraints for disambiguation. An experimental upper bound on performance using duration constraints is determined. Experiments show that the results compare well with a manual phonetic transcription.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-180"
  },
  "shimodaira93_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Mitsuru",
     "Nakai"
    ]
   ],
   "title": "Accent phrase segmentation using transition probabilities between pitch pattern templates",
   "original": "e93_1767",
   "page_count": 4,
   "order": 183,
   "p1": "1767",
   "pn": "1770",
   "abstract": [
    "This paper proposes a novel method for segmenting continuous speech into accent phrases by using a prosodic feature 'pitch pattern'. The pitch pattern extracted from input speech signals is divided into the accent segments automatically by using the One-Stage DP algorithm, in which reference templates representing various types of accent patterns and connectivity between them are used to find out the optimum sequence of accent segments. In case of making the reference templates from a large number of training data, the LBG clustering algorithm is used to represent typical accent patterns by a small number of templates. Evaluation tests were carried out using the ATR continuous speech database of a male speaker. Experimental results showed more than 91 % of phrase boundaries were correctly detected.\n",
    "Keywords: prosodic phrase, accent phrase segmentation, pitch pattern clustering\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-181"
  },
  "reichl93_eurospeech": {
   "authors": [
    [
     "W.",
     "Reichl"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Syllable segmentation of continuous speech with artificial neural networks",
   "original": "e93_1771",
   "page_count": 4,
   "order": 184,
   "p1": "1771",
   "pn": "1774",
   "abstract": [
    "This paper describes the utilization of multilayer perceptrons and radial basis function networks for the syllable segmentation of continuous speech by detecting the syllable nuclei. The artificial neural nets were trained to indicate the occurrence of vowels or diphthongs by means of the backpropagation algorithm and a non-iterative matrix-inversion method. In different experiments the syllable nuclei were correctly marked with more than 93 % and less than 2 % insertions.\n",
    "Keywords: Syllable segmentation, neural networks, multilayer perceptrons, radial basis functions\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-182"
  },
  "blomberg93_eurospeech": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Rolf",
     "Carlson"
    ]
   ],
   "title": "Labelling of speech given its text representation",
   "original": "e93_1775",
   "page_count": 4,
   "order": 185,
   "p1": "1775",
   "pn": "1778",
   "abstract": [
    "A system for phonetic and word level labelling of speech given the text representation of the utterance is being developed. A rule set designed for text-to-speech applications is used for the initial conversion of a given text to a base form phoneme transcription. Optional pronunciation is then predicted by rules and a lexicon. A speech corpus consisting of 2000 sentences spoken by one male speaker is currently being labelled at the department using this system. Results on a subset show that taking into account phone duration distribution and allowing for silent intervals between words is important. Optional pronunciation within words and at word boundaries did not improve the accuracy, possibly due to a limited phonetic discriminability of the system.\n",
    "Keywords: labelling, alignment\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-183"
  },
  "bosch93_eurospeech": {
   "authors": [
    [
     "Louis F. M. ten",
     "Bosch"
    ]
   ],
   "title": "On the automatic classification of pitch movements",
   "original": "e93_0781",
   "page_count": 4,
   "order": 186,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "In this paper, we discuss the construction of an algorithm that classifies pitch movements according to the IPO intonation system. We use a pitch stylization technique in order to obtain a continuous pitch contour over time, and a multi-linear classifier for the actual classification. In speaker-independent tests on a corpus of speech read by non-professionals, up to 81 % of the 279 pitch movements in the test corpus were correctly classified. These results are obtained by using information from the sampled speech data files only; a grammar will be used in the second stage of this study.\n",
    "Keywords: Automatic classification, multi-linear classification, IPO-intonation system, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-184"
  },
  "jensen93_eurospeech": {
   "authors": [
    [
     "U.",
     "Jensen"
    ],
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "Modelling of intonation contours at the sentence level using CHMMs and the 1961 o'connor and arnold scheme",
   "original": "e93_0785",
   "page_count": 4,
   "order": 187,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "This paper describes work on the recognition of British English intonation contours at the sentence level, using continuous density hidden Markov models (CHMMs) and the 1961 O'Connor and Arnold scheme for describing intonation contours. The scheme is presented briefly and the model optimisation procedure is described. Experimental results are presented for four tasks: transcription, type of nuclear tone, position of the voiced part of the accents and the nucleus (or principal accent). Taking into consideration the small training and test material, the results show a fairly high accuracy in all four tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-185"
  },
  "taylor93_eurospeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Automatic recognition of intonation from <i>F</i><sub>0</sub> contours using the rise/fall/connection model",
   "original": "e93_0789",
   "page_count": 4,
   "order": 188,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "This paper describes an automatic system for labelling intonational tune information based on the Rise/Fall/Connection model of intonation. The system is powerful in that it presupposes no prosodic knowledge of the utterance it is recognizing, and is capable of labelling all the intonational tune effects of English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-186"
  },
  "geoffrois93_eurospeech": {
   "authors": [
    [
     "Edouard",
     "Geoffrois"
    ]
   ],
   "title": "A pitch contour analysis guided by prosodic event detection",
   "original": "e93_0793",
   "page_count": 4,
   "order": 189,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "A left-to-right algorithm for analyzing pitch contours and estimating their underlying representation as a sequence of parametered commands is proposed. It is based on a generalization of recursive least squares optimization to a nonlinear model. This parameter fitting is embedded in a search guided by prosodic event detection, which determines when a candidate structure has to be augmented to keep fitting incoming data. The algorithm applied to Japanese read sentences successfully estimated 91% of lexical pitch accent positions.\n",
    "Keywords: prosody, intonation, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-187"
  },
  "demenko93_eurospeech": {
   "authors": [
    [
     "Grazyna",
     "Demenko"
    ],
    [
     "Ignacy",
     "Nowak"
    ],
    [
     "Janusz",
     "Imiolczyk"
    ]
   ],
   "title": "Analysis and synthesis of pitch movements in a read polish text",
   "original": "e93_0797",
   "page_count": 4,
   "order": 190,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "The paper presents a description of F0 control for purposes of the synthesis of Polish speech. It was assumed that information on sentence structure should be sufficient for adequate description of basic melodic patterns. From the corpus including about 1 minute-long newspaper passage, read three times by 6 persons, most frequent F0 contours were selected. In order to establish intonation pattern similarities in various fragments of the text, statistical methods of distance estimation between individual replications were applied, along with perceptual evaluation. The Fujisaki model was adopted to approximate fundamental frequency courses. Initially, standard values of parameters controlling the phrase component (defining declination) and accent component (determined for individual accent groups) were applied. Approximation results obtained from mathematical analysis indicated that modifications of functions controlling the phrase and accent components were necessary, both with respect to parameter values and the manner of their control. Information on sentence structure (the number of clauses, their structure, length and type) was applied in the module generating F0 contours. Control parameter values were optimized on the basis of results of perceptual experiments.\n",
    "Keywords: prosody, F0 modelling\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-188"
  },
  "ainsworth93_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "G. F.",
     "Meyer"
    ]
   ],
   "title": "Noise adaptation: speech recognition by auditory models and human listeners",
   "original": "e93_0825",
   "page_count": 4,
   "order": 191,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "Some of the effects of noise on the perception of plosive-vowel syllables have been investigated. It was found that noise added to the syllables for the duration of the speech had a more deleterious effect on perception than noise of the same intensity played continuously. Physiological experiments have shown that the response thresholds of cochlear nerve fibres to tones are raised by continuous background noise but not by short bursts of noise. It is suggested that this may be responsible for the speech perception results. In order to investigate this an auditory model was developed which incorporated response threshold shifts. This was interfaced to a hidden Markov model recogniser and tested with the same sounds that were employed in the human perception experiments. The recognition scores were greater with the threshold shifts than without them. Key words: Auditory model, cochlear nerve, noise adaptation, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-189"
  },
  "nolazcoflores93_eurospeech": {
   "authors": [
    [
     "J. A.",
     "Nolazco Flores"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Adapting a HMM-based recogniser for noisy speech enhanced by spectral subtraction",
   "original": "e93_0829",
   "page_count": 4,
   "order": 192,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "A common approach to improving the performance of a HMM based recogniser in noise is to apply Spectral Subtraction(SS) whereby an estimate of the noise spectrum is subtracted from the input signal spectrum. By using SS we obtain a signal with better features and lower variability. However, over-estimation and flooring make Spectral Subtraction a non-linear compensator and hence the noise level in the compensated system is reduced at the expense of introducing distortion into the speech signal. In this paper, we describe a noise compensation scheme in which SS is integrated into the Parallel Model Combination framework. This scheme allows a set of HMMs trained on clean speech to be compensated for both the effects of the noise and the non-linearity caused by the spectral subtraction. The paper presents an evaluation of this integrated SS-PMC approach using the Noisex 92 database. The results show that this mixed scheme outperforms the recognition performance of conventional Spectral Subtraction. For example, for the Lynx Helicopter noise at Odb SNR, standard fixed variance HMMs give 32% correct, adding SS gives 88% correct and the combined SS-PMC scheme gives 100% correct.\n",
    "Keywords: Speech recognition, noise, adaptation, spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-190"
  },
  "kobayashi93b_eurospeech": {
   "authors": [
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Ryuji",
     "Mine"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Speech recognition under the unstationary noise based on the noise Markov model and spectral-subtraction",
   "original": "e93_0833",
   "page_count": 4,
   "order": 193,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "In this paper, mechanisms to recognize speech in time varying noise are proposed. In these methods, the noise source and the speech source are modeled independently. The probabilities that these two models emit observed data sequence are calculated. We tested two methods. The first one uses HMMs to model both of noise and speech. The second one uses a normal Morkov model for noise representation. Adopting normal Markov model, the noise model itself become rather complex, however, the dynamical features such as delta cepstrum can be easily and precisely considered. Using these methods, 100 word recognition tests in car noise environment are performed. As the results, the performances are improved by 23 % and 26 % using the first and the second method, respectively, as compared with normal spectral subtraction.\n",
    "Keywords: Speech Recognition, Hidden Markov Model, Noise Reduction, Unstationary Noise\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-191"
  },
  "gales93_eurospeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "HMM recognition in noise using parallel model combination",
   "original": "e93_0837",
   "page_count": 4,
   "order": 194,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "This paper addresses the problem of automatic speech recognition in the presence of interfering noise. The approach adopted is to compensate the parameters of a clean speech model given the statistics of the interfering noise. In this work these statistics are assumed to be modelled with a Hidden Markov Model. The basic theory of static coefficient Parallel Model Combination (PMC) is reviewed and placed within the framework of approximating the Maximum Likelihood (ML) estimate of the corrupted speech model, given the clean speech and interfering noise models. A new form of PMC is described which improves the performance of fixed grand variance based recognition schemes, by compensating the variance to be more representative of the corrupted speech fixed grand variance. In addition, the paper examines the problem of compensating delta coefficients in a PMC framework. Expressions for ML estimates of delta coefficients are derived and computationally efficient approximations of these estimates are given. The effectiveness of compensating delta parameters is discussed.\n",
    "Keywords: speech recognition, noise compensation, HMM, PMC.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-192"
  },
  "buniet93_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Buniet"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Yolande",
     "Anglade"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "Selectively trained neural networks for connected word recognition in noisy environments",
   "original": "e93_0841",
   "page_count": 4,
   "order": 195,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "In this paper, we will describe a new method for small vocabulary connected word recognition. This method consists of three steps: vocalic nuclei detection, vowel identification and word identification. We report on its evaluation for the first two steps on two databases: a database of connected digits, NOISEX-92, and a subset of connected letters of the French BDSON database. It is shown that using neural networks is a robust method for automatically segmenting speech into vocalic/non-vocalic classes in connected speech. Moreover, for the vowel identification, preliminary results are given. Word identification is currently underway.\n",
    "Keywords: connected word recognition, noise robustness, artificial neural networks\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-193"
  },
  "angelini93b_eurospeech": {
   "authors": [
    [
     "B.",
     "Angelini"
    ],
    [
     "F.",
     "Brugnara"
    ],
    [
     "D.",
     "Falavigna"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "R.",
     "Gretter"
    ],
    [
     "M.",
     "Omologo"
    ]
   ],
   "title": "A baseline of a speaker independent continuous speech recognizer of Italian",
   "original": "e93_0847",
   "page_count": 4,
   "order": 196,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "The objective of this paper is to describe some preliminary results of the activity that is being carried out in IRST laboratories for the development of an HMM-based speaker independent continuous speech recognition system for the Italian language. The recognition system is being developed using the acoustic-phonetic continuous speech portion of APASCI database, also described in this paper. Performance is reported for different recognition tasks that were considered: without any phonetic constraint, the system provided a 72% Phone Accuracy; further, given a vocabulary size of 1019 words and no lexical constraints, a 73% Word Accuracy was achieved.\n",
    "Keywords: Speaker Independent Continuous Speech Recognition, Speech Databases, Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-194"
  },
  "bahl93_eurospeech": {
   "authors": [
    [
     "L. R.",
     "Bahl"
    ],
    [
     "P. V. de",
     "Souza"
    ],
    [
     "P. S.",
     "Gopalakrishnan"
    ],
    [
     "D.",
     "Nahamoo"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Word lookahead scheme for cross-word right context models in a stack decoder",
   "original": "e93_0851",
   "page_count": 4,
   "order": 197,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "Various experiments have conclusively shown that superior continuous speech recognition performance is obtained by using context dependent models for words. We have observed that using the phonetic context to the right across words when constructing word models improves recognition performance. In a stack decoder for large vocabulary continuous speech recognition, however, the right context information across words is not available when constructing a model for a hypothesized word, since we do not have any indication of what the following word is when computing the match for one word. In this paper we describe a look-ahead scheme that allows us to get some estimate of the right context to obtain an accurate match for a word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-195"
  },
  "grayden93_eurospeech": {
   "authors": [
    [
     "David B.",
     "Grayden"
    ],
    [
     "Michael S.",
     "Scordilis"
    ]
   ],
   "title": "Recognition of obstruent phonemes in speaker-independent fluent speech using a hierarchical approach",
   "original": "e93_0855",
   "page_count": 4,
   "order": 198,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "A hierarchical approach to obstruent phoneme recognition in continuous speech is presented. This approach is based on the division of the recognition process into smaller tasks which are easier to design, develop and optimise. The developed system first distinguishes between obstruents and other phonemes and then proceeds to classify the phonemes. Performance of around 80% is achieved. The system's tolerance to temporal misalignment makes it a good candidate for recognition in fluent speech.\n",
    "Keywords: Obstruent Phonemes, Hierarchical, TDNN\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-196"
  },
  "plannerer93_eurospeech": {
   "authors": [
    [
     "Bernd",
     "Plannerer"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "A continuous speech recognition system using phonotactic constraints",
   "original": "e93_0859",
   "page_count": 4,
   "order": 199,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "This paper describes a speaker-independent recognition system for continuous German speech based on semicontinuous Hidden-Markov-Models which produces a phonetic transcription of the spoken sentence. The recognition units are parts of syllables while the output is a phoneme level transcription. During recognition, the phonotactic constraints of German are taken into account by a micro syntax constrained Viterbi algorithm. A maximum likelihood training procedure based on Viterbi training together with a simple but efficient seed model generation algorithm is presented.\n",
    "Keywords: phonotactic constraints, semicontinuous HMMs, seed model generation, Viterbi training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-197"
  },
  "ouadou93_eurospeech": {
   "authors": [
    [
     "M.",
     "Ouadou"
    ],
    [
     "A.",
     "Rajouani"
    ],
    [
     "M.",
     "Zyoute"
    ],
    [
     "J.",
     "Rosenfeld"
    ],
    [
     "M.",
     "Najim"
    ]
   ],
   "title": "Joint arabic-hebrew speech synthesis system",
   "original": "e93_0865",
   "page_count": 4,
   "order": 200,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "This paper describes the first steps for adaptation of an Arabic text-to-speech synthesis system /I/ to Hebrew language. The text-to-speech synthesis system realized for Arabic language is described briefely. The speech synthesis system is based on the synthesis by rule using a cascade and parallel Klatt formant synthesizer /2/. This approach is justified by the resemblance of Arabic and Hebrew languages in terms of phonetic aspects. The work may be extended to the other Semitic languages. This adaptation work consists of analysis, synthesis and formulation of adequat rules. A deeper investigation of phonetic differences between the two languages is undertaken aiming at the realization of a complete bilingual speech synthesis system. The results dealing with the intelligibilty of the particular Hebrew consonants are acceptable.\n",
    "Keywords: Klatt formant speech synthesizer, bilingual synthesis, Arabic, Hebrew.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-198"
  },
  "lopezgonzalo93b_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lopez-Gonzalo"
    ],
    [
     "Gabor",
     "Olaszy"
    ],
    [
     "Geza",
     "Nemeth"
    ]
   ],
   "title": "Improvements of the Spanish version of the multivox text-to-speech system",
   "original": "e93_0869",
   "page_count": 4,
   "order": 201,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "In this contribution, we present the recent improvements of the Spanish version of the MULTIVOX text-to-speech system [1,2]. First the adaptation of the system to the Spanish version is considered in the framework of MULTIVOX. Then a new fast linguistic module for Spanish is probed to improve the naturalness of the synthetic speech. This result validates the methodology for prosodic modelling and encourage us for future improvements.\n",
    "Keywords: Text-to-Speech Synthesis, Prosodic Processing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-199"
  },
  "ljungqvist93_eurospeech": {
   "authors": [
    [
     "Mats",
     "Ljungqvist"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Generating intonation for Swedish text-to-speech conversion using a quantitative model for the <i>F</i><sub>0</sub> contour",
   "original": "e93_0873",
   "page_count": 4,
   "order": 202,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "In Swedish, the fundamental frequency contour (F0 contour) is known to be the main acoustic feature for word accent and sentence intonation. A model, based on an extension of the model for Japanese, is used for the generation process of F0 contours of Swedish. As the input to this model, two kinds of command are assumed: the phrase commands which are positive impulses except at the end of an utterance, and accent commands which are stepwise functions of both polarity. Analysis-by-Synthesis of F0 contours of both isolated words and sentences, uttered by two native speakers from the Stockholm region, indicated that the model can always generate very close approximations to observed Fo contours, and that the extracted parameters are systematically related to the underlying lexical word accent, syntactic structure, and focus. Furthermore, the model is introduced into a framework of text-to-speech conversion for Swedish and an outline is given for the derivation of Fo model parameters.\n",
    "Keywords: intonation, prosody, text-to-speech conversion, speech synthesis, fundamental frequency contour, analysis-by-synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-200"
  },
  "meyer93_eurospeech": {
   "authors": [
    [
     "P.",
     "Meyer"
    ],
    [
     "Hans-Wilhelm",
     "Rühl"
    ],
    [
     "R.",
     "Krüger"
    ],
    [
     "M.",
     "Kugler"
    ],
    [
     "L. L. M.",
     "Vogten"
    ],
    [
     "A.",
     "Dirksen"
    ],
    [
     "Karim",
     "Belhoula"
    ]
   ],
   "title": "PHRITTS - a text-to-speech synthesizer for the German language",
   "original": "e93_0877",
   "page_count": 4,
   "order": 203,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "PHRITTS is a text-to-speech synthesizer consisting of a grapheme-to-phoneme conversion, an accentuation, a duration control, an intonation contour generator, and a speech synthesizer based on diphones. After some text preprocessing the grapheme-to-phoneme conversion performs a moiph decomposition. For the rule-based conversion of the resulting morphs a special compiler is used. A word class assignment is made for the following focus-accent-based accentuation, which provides locations for pitch accents and phrase boundaries, followed by a simple context dependent calculation of the phoneme durations. The intonation contour is calculated from the accent pattern using rules derived from natural German speech. It consists of triangular or trapezium shaped pitch moves, placed on a simple pitch declination. They are calculated by linear interpolation in the logarithmic frequency domain. The diphone synthesis is based on 1573 German diphones, extracted from nonsense words spoken with a constant monotonous pitch, which makes it possible to directly concatenate the diphones by interpolation in the time domain, after calculation of the optimal phase at diphone boundaries. Pitch and duration are manipulated using a TD-PSOLA technique. Compared to an existing LPC-based synthesizer version, TD-PSOLA results in much more natural speech. The TTS-synthesizer is written in C-Code and runs in real time on a standard 486 PC with a DSP-board for AD-conversion.\n",
    "Keywords: German text-to-speech synthesis, accentuation, intonation, diphone synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-201"
  },
  "belhoula93_eurospeech": {
   "authors": [
    [
     "Karim",
     "Belhoula"
    ]
   ],
   "title": "Rule-based grapheme-to-phoneme conversion of names",
   "original": "e93_0881",
   "page_count": 4,
   "order": 204,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "Although the synthesis of names is a new topic, a significant amount of work has been done over the last few years on methods for their conversion. As the potential applications in most cases are targeted for public access, very good name pronunciation capabilities are required. We have been developing rule-based approaches for the grapheme-to-phoneme conversion of names (place names, surnames and first names). Our strategy in the development of morphological- and letter-to-sound rules was supported by a database derived from a telephone directory. To provide a good first name pronunciation, a cluster-based approach has been developed to predict the origins of names. This paper reports on our progress and results toward this development.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-202"
  },
  "murray93_eurospeech": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "Morag M.",
     "Black"
    ]
   ],
   "title": "A prototype text-to-speech system for scottish gaelic",
   "original": "e93_0885",
   "page_count": 3,
   "order": 205,
   "p1": "885",
   "pn": "887",
   "abstract": [
    "Gaelic is the language traditionally spoken on the western side of Scotland. A prototype text-to-speech system for the language has been produced, consisting of a parser module to produce grammatically correct test phrases, and a text-to-sound module to speak the test phrases. Text-to-phoneme rules were produced, and these are output to a commercial phoneme-based speech synthesiser.\n",
    "Keywords: Gaelic, text-to-speech, speech synthesis, language parser\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-203"
  },
  "imiolczyk93_eurospeech": {
   "authors": [
    [
     "Janusz",
     "Imiolczyk"
    ],
    [
     "Ignacy",
     "Nowak"
    ],
    [
     "Grazyna",
     "Demenko"
    ]
   ],
   "title": "A text-to-speech system for polish",
   "original": "e93_0889",
   "page_count": 4,
   "order": 206,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "The paper presents the system of automatic synthesis of Polish speech from text, developed at the Department of Acoustic Phonetics in Poznan. The element generating the acoustic signal is a special-purpose IC control led from a PC AT by means of original software comprizing the modules for text editing, phonemic transcription and synthesis of digital parameters. Results of perceptual tests prove the synthetic speech obtained to be highly intelligible.\n",
    "Keywords: text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-204"
  },
  "macchi93_eurospeech": {
   "authors": [
    [
     "Marian",
     "Macchi"
    ],
    [
     "Mary Jo",
     "Altom"
    ],
    [
     "Dan",
     "Kahn"
    ],
    [
     "Sharad",
     "Singhal"
    ],
    [
     "Murray F.",
     "Spiegel"
    ]
   ],
   "title": "Intelligibility as a function of speech coding method for template-based speech synthesis",
   "original": "e93_0893",
   "page_count": 4,
   "order": 207,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "We have been experimenting with various methods for coding the templates used in a concatenative speech synthesis system: standard pulse I noise-excited LPC; a newer waveform technique, PSOLA (pitch-synchronous overlap-and-add); and two types of residual-excited LPC (RELP): simple RELP, in which the residual was modified by truncation or padding with zeros, and PSOLA RELP, in which PSOLA was used to modify the residual. We used these techniques to code spoken words that were similar to the templates in an inventory, and resynthesized the words. We also modified the pitch of the words, as is required by text-to-speech synthesis systems, and resynthesized the pitch-modified words. We conducted listening tests to measure the consonant intelligibility in the words with and without the pitch change. Thus we were able to see how intelligibility was affected by the coding method itself and by changes to the pitch. The results showed that RELP provided higher intelligibility than PSOLA for voiced consonants, and considerably higher intelligibility than standard pulse/noise-excited LPC, even when pitch changes were imposed on the words. In addition, simple RELP performed about as well as PSOLA-RELP.\n",
    "Keywords: speech synthesis, speech coding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-205"
  },
  "gaved93_eurospeech": {
   "authors": [
    [
     "Maggie",
     "Gaved"
    ]
   ],
   "title": "Pronunciation and text normalisation in applied text-to-speech systems",
   "original": "e93_0897",
   "page_count": 4,
   "order": 208,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "There are several areas of text-to-speech woik which have often been neglected by researchers, but which become vitally important once an application for a system is envisaged. This paper highlights some of these areas, concentrating on text normalisation and pronunciation. Examples are given of areas being addressed in the text-to-speech system currently being developed at BT Laboratories. Our data-driven approach to developing pronunciation methods, particularly for proper names is explained. An overview of the BT Laureate text-to-speech system is given showing the general structure and, in particular, where text normalisation and pronunciation fit into the structure.\n",
    "Keywords: text normalisation, pronunciation, grapheme to phoneme alignment, pronunciation by analogy\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-206"
  },
  "house93_eurospeech": {
   "authors": [
    [
     "Jill",
     "House"
    ],
    [
     "Catriona",
     "MacDermid"
    ],
    [
     "Scott",
     "McGlashan"
    ],
    [
     "Andrew",
     "Simpson"
    ],
    [
     "Nick",
     "Youd"
    ]
   ],
   "title": "Evaluating synthesised prosody in simulations of an automated telephone enquiry service",
   "original": "e93_0901",
   "page_count": 4,
   "order": 209,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "Dialogue material has been collected using bionic Wizard-of-Oz simulations of an automated voice-driven dialogue system; system turns, using rule-based speech synthesis, were systematically varied for both text and prosody. In a separate pilot experiment, subjects evaluated the system turns on a number of parameters. While they did not always clearly differentiate between the different conditions, they seemed to associate the \"enhanced\" prosody, preferred in an earlier laboratory experiment, with a more likeable personality.\n",
    "Keywords: prosody, synthesis, dialogue, evaluation, simulation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-207"
  },
  "morton93_eurospeech": {
   "authors": [
    [
     "Katherine",
     "Morton"
    ],
    [
     "Marcel",
     "Tatham"
    ]
   ],
   "title": "Speech synthesis in dialogue systems",
   "original": "e93_0905",
   "page_count": 4,
   "order": 210,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "This paper deals with the need for speech synthesis in dialogue systems to incorporate tone of voice for cueing in the listener feelings concerning the attitude of the computer-speaker. Dialogue systems intended for different purposes require different global tones of voice to sound completely convincing. But in addition the synthetic speech needs local tones of voice to signify changing and adapting attitudes during the course of the dialogue with the human user of the system. We discuss the format of a tone of voice model and provide an example using intonation declination.\n",
    "Keywords: dialogue systems, speech synthesis, pragmatics, intonation, declination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-208"
  },
  "abadjieva93_eurospeech": {
   "authors": [
    [
     "Elissaveta",
     "Abadjieva"
    ],
    [
     "Iain R.",
     "Murray"
    ],
    [
     "John L.",
     "Arnott"
    ]
   ],
   "title": "Applying analysis of human emotional speech to enhance synthetic speech",
   "original": "e93_0909",
   "page_count": 4,
   "order": 211,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "The successful implementation of emotional effects in synthetic speech depends on two main factors: the available knowledge on how the emotional features of speech can be distinguished and described using the formalism of conventional speech processing techniques, and the incorporation of emotion dependent changes into the different stages of the speech production algorithm. This paper presents the results of a detailed analysis of human emotional speech, carried out to obtain formal descriptions of the five basic emotions. This was then used to create an emotion rule base, the analysis results for each emotion being coded as a set of rules introducing emotion-dependent changes to the synthesiser control parameters. Perception experiment results confirmed that the emotional rule base can produce recognisable emotion effects.\n",
    "Keywords: synthetic speech, emotional effects, affect, human speech analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-209"
  },
  "lewis93_eurospeech": {
   "authors": [
    [
     "Eric",
     "Lewis"
    ],
    [
     "Marcel",
     "Tatham"
    ]
   ],
   "title": "A generic front end for text-to-speech synthesis systems",
   "original": "e93_0913",
   "page_count": 4,
   "order": 212,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "This paper describes how the SPRUCE (Speech Response from Unconstrained English) system is capable of acting as a \"front end\" to any text-to-speech system whether it be based on diphones, phonemes, demi-syllables, syllables or even words. This is possible because the architecture of SPRUCE includes a large dictionary which contains phonetic information of individual words along with syllable, stress and other prosodic information. This representation is such that the basic linguistic information for driving any text-to-speech synthesiser is already contained within the SPRUCE synthesis system. The advantage of using SPRUCE as a front end for a synthesiser is that it enables the synthesis system to make use of SPRUCE'S large word dictionary and its natural sounding intonation algorithm.\n",
    "Keywords: SPRUCE, text-to-speech synthesis, syllables\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-210"
  },
  "luk93_eurospeech": {
   "authors": [
    [
     "Robert W. P.",
     "Luk"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Experiments with silent-e and affix correspondences in stochastic phonographic transduction",
   "original": "e93_0917",
   "page_count": 4,
   "order": 213,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "To improve stochastic phonographic transduction based on a set of letter-phoneme correspondences, a sample of incorrect machine pronunciations was given to a linguist who is also an English language teacher. He found that errors frequently arose with silent-e markings and affixation. This paper examines the effect on performance when silent-e and/or affix correspondences are explicitly added to the existing set. Results show that adding silent-e correspondences did not significantly improve performance, but adding affix correspondences did when testing with both the training set and unseen words. However, there was no significant improvement for proper names and novel words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-211"
  },
  "fries93_eurospeech": {
   "authors": [
    [
     "Georg",
     "Fries"
    ]
   ],
   "title": "Phoneme-dependent speech synthesis in the time and frequency domains",
   "original": "e93_0921",
   "page_count": 4,
   "order": 214,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "This paper describes a speech synthesizer which combines parametric speech synthesis with speech synthesis in the time domain. In the proposed system, the alternate or simultaneous use of both synthesis methods is controlled by a phoneme-dependent algorithm. Vowels and nasals are produced by the parametric system part - a formant-based synthesizer. Voiceless consonants are generated in the time domain. To synthesize voiced fricatives, voiced plosives and voiced/voiceless-transitions, both system components work simultaneously.\n",
    "Keywords: speech synthesis, formant synthesizer, hybrid system\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-212"
  },
  "karlsson93_eurospeech": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ],
    [
     "Lennart",
     "Neovius"
    ]
   ],
   "title": "Speech synthesis experiments with the glove synthesiser",
   "original": "e93_0925",
   "page_count": 4,
   "order": 215,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "In this paper we will present some experiments to improve the naturalness of synthetic female speech. A pole/zero pair has been introduced in the glove synthesiser to better match analysis data. In this paper we have focused on the consonants /h/, /l/, /m/ and /n/. An inverse filtering method, which includes automatic formant tracking and manual spectral and time domain matching, generates IF source parameter, formant and pole/zero data. Analysis data have been interpreted and resynthesised. Text-to-speech rules have been formulated according to the hand-edited resynthesis. These rules have improved the overall quality of the synthetic female speech according to an informal listening test.\n",
    "Keywords: hi-fi female speech synthesis, acoustic description of consonants\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-213"
  },
  "kraft93_eurospeech": {
   "authors": [
    [
     "Volker",
     "Kraft"
    ]
   ],
   "title": "Auditory detection of discontinuities in synthesis-by-concatenation",
   "original": "e93_0929",
   "page_count": 4,
   "order": 216,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "In order to find the best way of synthesis unit concatenation, this paper focuses on the auditory detectability of discontinuities. These may appear as jumps in energy, f0 and spectral characteristics due to concatenation. After introducing the main causes and effects of different discontinuities, a new test method is presented aiming at investigating the detectability of concatenation phenomena. As stimuli for this test synthetic sentences have been produced under controlled conditions. From the test results conclusions will be drawn on concatenation techniques and the test method itself.\n",
    "Keywords: text-to-speech; synthesis-by-concatenation; concatenation quality evaluation; speech perception\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-214"
  },
  "lee93b_eurospeech": {
   "authors": [
    [
     "Yun-Keun",
     "Lee"
    ],
    [
     "Seung-Kwon",
     "Ahn"
    ]
   ],
   "title": "Effects of the phase jitters on naturalness of synthesized speech",
   "original": "e93_0933",
   "page_count": 4,
   "order": 217,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "Speech synthesizers based on frequency domain analysis usually have problems when they don't use phase information. For instance, they generate monotonous and machine-like speech. It has been found that phase jitters (PJs) are important factors on naturalness of synthesized speech. We analyze the PJs of natural speech using pitch synchronous FFT and construct the PJ model from this analysis. We also demonstrated that the synthetic speech using power spectrum envelope(PSE) and the PJ components can be almost indistinguishable from the natural speech.\n",
    "Keywords: phase jitters, power spectrum envelope, pitch synchronous FFT\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-215"
  },
  "williams93_eurospeech": {
   "authors": [
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "Letter-to-sound rules for the welsh language",
   "original": "e93_0937",
   "page_count": 4,
   "order": 218,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "A set of letter-to-sound rules was written, in the form of context-sensitive rewrite rules. A publicly-available program was used that converted this style of rule to a C program. Three sets of rules were written, reflecting three separate passes through each input word. The first set added epenthetic vowels, while the second set located the vowels and lexical stress. The third set carried out grapheme-to-phoneme conversion. The rules were evaluated over text from a magazine article, and found to have a 96% success rate.\n",
    "Keywords: Text-to-speech synthesis, Welsh, letter-to-sound rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-216"
  },
  "muller93_eurospeech": {
   "authors": [
    [
     "Christel",
     "Müller"
    ],
    [
     "Fred",
     "Runge"
    ]
   ],
   "title": "Dialogue design principles - key for usability of voice processing",
   "original": "e93_0943",
   "page_count": 4,
   "order": 219,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "Communication systems with voice processing are becoming more and more important for all users of new services (audiotex, voice mail and fax, automated responses, ordering systems...). Usability of these new features and services depend essentially on the dialogue every user has to handle. It is common, that the success rate of voice controlled services in modern system applications with a determined dialogue handling is considered from the users point of view, i.e. from the person who takes part in these interactions. Speech interfaces represent a subset of user interfaces for multimodal dialogues and are of main interest for services in telecommunication applications. This paper analyses the different methods of dialogue design - from high level specification languages to graphical dialogue description tools. The evaluation of these dialogue design tools includes the formal and imperial criteria and the generic usability principles. The main research goal for dialogue design (to involve human factors into the design process) is considered.\n",
    "Keywords: dialogue design, speech interface, design tools and criteria, evaluation, human factors\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-217"
  },
  "dybkjaer93_eurospeech": {
   "authors": [
    [
     "Hans",
     "Dybkjaer"
    ],
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ]
   ],
   "title": "Wizard-of-oz and the trade-off between naturalness and recogniser constraints",
   "original": "e93_0947",
   "page_count": 4,
   "order": 220,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "The Wizard-of-Oz simulation technique has been used in the development of the dialogue model for a spoken language dialogue system. The paper focuses on the trade-off between system naturalness and the technological constraints imposed by the speech recogniser. The constraints enforce a strongly system-directed dialogue. Phrases and subjects influence the trade-off whereas voice distortion apparently does not.\n",
    "Keywords: Spoken language dialogue systems, Wizard-of-Oz, dialogue model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-218"
  },
  "jones93c_eurospeech": {
   "authors": [
    [
     "Cerian",
     "Jones"
    ],
    [
     "Roberto",
     "Garigliano"
    ]
   ],
   "title": "Dialogue analysis and generation: a theory for modelling natural English dialogue",
   "original": "e93_0951",
   "page_count": 4,
   "order": 221,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "This abstract outlines work in progress at the University of Durham concerning natural language processing. As a result of this work, a general theory has been developed which offers a means of modelling natural English dialogue by computer, and can be used for both the analysis and generation of dialogue. The implementation of this theory is currently in progress (commenced in early 1993) and forms a dialogue module which uses input from a large NLP system developed at Durham. The theory categorises natural dialogue into template-like schemas (called Dialogue Structure Models) which contain basic information about different types of dialogue, (e.g. lecture, interview etc.). This information provides the system with knowledge about the structure of the dialogue and how it comprises of its constituent parts, which we call Dialogue Elements (DE). The DEs are factors which influence and control the dialogue structure. For example, the Dominance DE informs the system as to which participant(s) has control within the dialogue. There are fourteen DEs within the theory, each carrying different information. Their combinations vary for each dialogue participant, depending upon the role being played. The DEs inform the system as to what is \"the norm\" within a given DSM. However, the dialogue system must be capable of utilising this information, and this requirement is fulfilled by attaching a set of \"constraints\" to each Dialogue Element. Each constraint is a possible action which the system may take, (e.g. if a participant has dominance, he/she may ask a question, change the topic etc.). The way in which the system selects the appropriate action to take depends upon a number of influencial factors, including the system's \"personality\", previous knowledge held about the user, and particular details of that individual dialogue etc. Once the system has selected the appropriate constraint, this information is passed to the generation module, translated into English and displayed upon the screen for the user to read and respond to. The process is then iterated, developing a dialogue.\n",
    "Keywords: Dialogue Structure, Natural Language Processing, Constraints, Planbox\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-219"
  },
  "macdermid93_eurospeech": {
   "authors": [
    [
     "Catriona",
     "MacDermid"
    ]
   ],
   "title": "Features of naive callers' dialogues with a simulated speech understanding and dialogue system",
   "original": "e93_0955",
   "page_count": 4,
   "order": 222,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "During the development of speech-based database enquiry systems for dialogue over the telephone with members of the general public, Wizard of Oz simulations (in which an accomplice plays the role of the system) were conducted. The simulations provided evidence of naive callers adapting their speech to the system's presumed capabilities. They have also shown that callers tolerate speech recognition errors where there is graceful error recovery. However, the data have raised questions about the need for constraints to be imposed on callers' initial utterances if dialogues are to be successful.\n",
    "Keywords: Simulation, Wizard of Oz, spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-220"
  },
  "duermael93_eurospeech": {
   "authors": [
    [
     "Fabrice",
     "Duermael"
    ],
    [
     "Bertrand",
     "Gaiffe"
    ]
   ],
   "title": "Refering to actions in man-machine command dialogues",
   "original": "e93_0959",
   "page_count": 4,
   "order": 223,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "In the framework of man-machine command dialogues, we argue that actions can be refered to by building up, from the user's utterance, a representation of the intended goal, which the system is to achieve. This goal will be considered as a final state, in order to enable a planning process starting from the current state of the application. Following our purpose, we point out some of the key elements which have to be taken into account, such as the temporal dynamics of predicates and their subordination to the objects of the task. We thus tackle some of the tricky questions encountered in current dialogue systems. Keywords : man-machine dialogue, reference to actions, prepositions, aktionsart.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-221"
  },
  "yamashita93_eurospeech": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Next utterance prediction based on two kinds of dialog models",
   "original": "e93_1161",
   "page_count": 4,
   "order": 224,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "This paper describes a method of predicting user's next utterances in spoken dialog based on two kinds of dialog models, SR-plan and TPN. The SR-plan is a low level model of interactions composed of a stimulus and a response and the TPN is a high level model representing transitions of topics in dialogs. The dialog manager predicts next utterances and provides the language processing unit with templates of case frames which are associated with SR-plans and instantiated according to the preceding utterances and the topic information. An experiment shows that the utterance prediction improves the performance of the utterance recognition and drastically reduces the search space in terms of candidates in the input word lattice.\n",
    "Keywords: Dialog model, Utterance prediction, Spoken dialog understanding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-222"
  },
  "andemach93_eurospeech": {
   "authors": [
    [
     "T.",
     "Andemach"
    ],
    [
     "G.",
     "Deville"
    ],
    [
     "L.",
     "Mortier"
    ]
   ],
   "title": "The design of a real world wizard of oz experiment for a speech driven telephone directory information system",
   "original": "e93_1165",
   "page_count": 4,
   "order": 225,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "This paper reports the design of a Wizard of Oz simulation of a spoken dialogue system in the context of a telephone directory information service. The experiment described here was carried out within the framework of a Belgian Telecom R&D project, the aim of which is to develop a prototype system for the automatic handling of directory information queries with voice input/output facilities. The authors' task in the project consists in designing the information dialogue models which will be used in the eventual prototype. The target application is the database of the Belgian Telecom Information Centre. The authors advocate some crucial methodological choices, which clearly distinguish this experiment from most current alternative approaches. The dialogue analysis is discussed and conclusions are drawn, the focus being on (i) the adequacy of the methodology to the design of dialogue models, (ii) the pertinence of such models in an automated environment, and (Hi) the evaluation of the dialogue sessions by the subjects.\n",
    "Keywords: Spoken dialogue, Wizard of Oz experiment, human factors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-223"
  },
  "young93b_eurospeech": {
   "authors": [
    [
     "Sheryl R.",
     "Young"
    ]
   ],
   "title": "Dialog structure and plan recognition in spontaneous spoken dialog",
   "original": "e93_1169",
   "page_count": 4,
   "order": 226,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "In real spoken language applications, speakers interact spontaneously and frequently diverge from the task at hand by initiating various types of domain, application or environmentally related subdialogs. We claim that unconstrained, task-oriented spontaneous spoken dialog is structured and predictable in spite of such phenomena as spurious topic changes and subdialogs. The discourse structure for any specific dialog is derived from the structure of the task, contextual constraints derived from prior interaction and the characteristics of a finite set of discourse plans responsible for subdialogs and topic changes. This paper describes a preliminary model of discourse structure and plan recognition for spontaneous spoken discourse that has been implemented and evaluated on a 5000 utterance test corpora drawn from two distinct spoken language applications. The model dynamically constrains a speech recognizer, simplifies the process of inferring meaning from a spontaneous spoken utterance and accounts for the subdialog phenomena ob- served. We describe these discourse plans, constraints on their occurrence and content, and their representation and processing. The model processes all subdialog phenomena using a domain plan tree, a current focus stack and a set of domain tree traversal algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-224"
  },
  "hirschberg93_eurospeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Christine",
     "Nakatani"
    ]
   ],
   "title": "A speech-first model for repair identification in spoken language systems",
   "original": "e93_1173",
   "page_count": 4,
   "order": 227,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "Self-corrections or repairs are often left unmodeled in current spoken language systems although they occur in about 10% of spontaneous utterances. We report on a study of acoustic-prosodic repair cues of potential use for repair identification, word fragment identification, and repair correction. The relative contributions of these and other text-based cues to repair identification were tested in a statistical model that achieved a precision rate of 91 % and recall of 86%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-225"
  },
  "young93c_eurospeech": {
   "authors": [
    [
     "Sheryl R.",
     "Young"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Recognition confidence measures for spontaneous spoken dialog",
   "original": "e93_1177",
   "page_count": 3,
   "order": 228,
   "p1": "1177",
   "pn": "1179",
   "abstract": [
    "This paper reports on a new technique for evaluating confidence in word strings produced by a speech recognition system used for processing limited domain spontaneous dialogs. The technique can also be used to produce confidence metrics for non-spontaneous speech. Spontaneous speech is especially difficult because unknown words, verbal noise and speech repairs and edits arc common phenomena that complicate the basic speaker-independent continuous speech recognition process. Our goal is to produce a confidence metric for spontaneously generated word strings that combines acoustic and higher-level knowledge sources through the use of Bayesian Updating. This confidence measure takes into account knowledge source reliability and ability to differentially discriminate misrecognitions. This wor£ is part of our larger project on automatically detecting and acquiring the meaning or out-of-vocabulary words. In estimating acoustic confidence, we first normalize the word score produced by the recognizer. This is done by subtracting the log-probability score for an all-phone recognition from the log-probability word score and normalizing for length. The all-phone score is generated by running the speecn recognizer on the utterance allowing any triphone to follow any other enphone with a trigram probability for tnphone sequences. A triphone is a context dependent phone model. Trigrams of the triphone sequences are computed from a large corpus of English language text. We use Bayesian Updating to turn the normalized word score into a confidence measure. For this, words are grouped into classes using alternate grouping methods. For each word class we estimate when a word in the class is seen with a particular score, what is the percentage of time that the word was correctly recognized. This estimate is made by running the recognition system on a training set of data. This gives us a airect measure of the confidence with which we can reject or accept a word based on acoustic measures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-226"
  },
  "zhao93b_eurospeech": {
   "authors": [
    [
     "R.",
     "Zhao"
    ],
    [
     "P.",
     "Kenny"
    ],
    [
     "P.",
     "Labute"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Issues in large scale statistical language modeling",
   "original": "e93_0965",
   "page_count": 4,
   "order": 229,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "We present our approach to three major issues in statistical language modeling with large vocabularies using large corpora: (1) word probability smoothing, (2) task adaptation and (3) access speed to word probabilities and memory management. The last issue is particularly important for real-time speech recognition systems. To handle smoothing and adaptation, we designed a nondeterministic trigram language model. We approach the last issue by caching.\n",
    "Keywords: deterministic language model, dynamic adaptation, hidden Markov model, nondeterministic language model, static adaptation, statistical language modeling, trigram language modeling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-227"
  },
  "garigliano93_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Garigliano"
    ],
    [
     "Kevin",
     "Johnson"
    ],
    [
     "Russell J.",
     "Collingham"
    ]
   ],
   "title": "A data-driven case for a spontaneous speech grammar",
   "original": "e93_0969",
   "page_count": 4,
   "order": 230,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "People rarely speak using the full rules of English grammar: that, however, does not stop us from understanding speech. This points to the fact that everyone uses some common constructs during spontaneous speech and that these constructs form the basis of everyone's understanding of spoken English. An in-depth analysis of a large corpus of spontaneous spoken English, in particular the spoken lecture, has been carried out which concentrated on identifying these common constructs and indicating problems posed by speakers of naturally spoken English. The results of the analysis show that these common constructs do exist. These islands of grammar, along with the features needed to deal with the unique problems posed by spontaneous speech, can be linked to form the major part of a speech grammar. This paper describes the analysis carried out and presents the results of this analysis. The overall aim of this work is to include these findings into a speech recognition system being developed at the University of Durham.\n",
    "Keywords: continuous speech recognition, spontaneous speech, speech grammar\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-228"
  },
  "kneser93_eurospeech": {
   "authors": [
    [
     "Reinhard",
     "Kneser"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improved clustering techniques for class-based statistical language modelling",
   "original": "e93_0973",
   "page_count": 4,
   "order": 231,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "The topic of this paper is to introduce a certain type of structure into a bigram language model by using the concept of word equivalence classes. We train these classes automatically, using an iterative clustering algorithm which finds a (local) optimum of some clustering criterion. We show that the conventional maximum-likelihood criterion performs well, but has the disadvantage that one has to specify the number of word classes in advance. We therefore modify this criterion using a special form of cross-validation, the leaving-one-out technique. The resulting algorithm is able to find both the unknown classification and the unknown number of classes at the same time. Clustering experiments were carried out on an English and a German text corpus comprising 1.1 million and 100,000 words, respectively. Compared to a word bigram model we could reduce the perplexity by more than 10% using a class model with automatically clustered classes. Combinations with the word model and with linguistically defined parts of speech lead to a further improvement of up to 37%.\n",
    "Keywords: Stochastic Language Modelling, Statistical Clustering, Leaving-One-Out Method\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-229"
  },
  "wright93_eurospeech": {
   "authors": [
    [
     "J. H.",
     "Wright"
    ],
    [
     "G. J. F.",
     "Jones"
    ],
    [
     "H.",
     "Lloyd-Thomas"
    ]
   ],
   "title": "A consolidated language model for speech recognition",
   "original": "e93_0977",
   "page_count": 4,
   "order": 232,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "Hybrid speech recognition systems using both bigram and grammar models yield improved performance compared with the use of either model alone, but performance is sub-optimal because the grammar is abandoned for sentences that fail to parse overall. By merging bigrams (in general n~grams) and grammars into a single framework we aim to combine the advantages of both, in particular structural capacity and trainability, in a robust recognition system. A substring parser allows whatever grammar structure is present to participate in scoring the candidate sentences. Extended bigrams using an information criterion capture remote dependencies and reduce perplexity. The first version of a consolidated model using these methods is described.\n",
    "Keywords: Language modelling, robust parsing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-230"
  },
  "mccandless93_eurospeech": {
   "authors": [
    [
     "Michael K.",
     "McCandless"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Empirical acquisition of word and phrase classes in the atis domain",
   "original": "e93_0981",
   "page_count": 4,
   "order": 233,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "We investigate an algorithm for automatically acquiring a context free grammar for sentences in the ATIS [1] domain. The learning process is a form of grammatical inference [2], and in the process of learning the grammar it generalizes beyond the training sentences. Performance is measured by coverage of the grammar on an independent test set, and by the corresponding test-set perplexity of a modified N-gram model which incorporates the learned grammar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-231"
  },
  "chiang93_eurospeech": {
   "authors": [
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Keh-Yih",
     "Su"
    ]
   ],
   "title": "The effects of parameter smoothing on robust learning in syntactic ambiguity resolution",
   "original": "e93_1183",
   "page_count": 4,
   "order": 234,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "To resolve the problems of syntactic ambiguities, a unified probabilistic score function approach was proposed in our previous works. The parameters used in the score function were estimated by the maximum likelihood algorithm, and 53.1% accuracy rate was observed. To further improve the performance in the testing set, a discrimination and robustness oriented adaptive learning algorithm was derived from the score function, and the accuracy rate was pushed to 64.3%. However, the parameters corresponding to those rare events are usually quite unreliable and cannot be well tuned by the adaptive learning algorithm. For improving the parameter estimation from sparse training data, the effect of parameter smoothing is investigated in this paper first, and about 56% accuracy rate is achieved with the smoothing methods. Then, a hybrid approach, which incorporates the smoothing techniques and the robust learning algorithm, is proposed to further improve the performance. A very promising result, 69.8% accuracy rate, is obtained using this hybrid method. The significant error reduction rate of 35.6% (from 53.1% to 69.8%) shows that the smoothing technique not only reduce the estimation error but also lead the learning process to a better local optimal point.\n",
    "Keywords: Adaptive Learning, Parameter Smoothing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-232"
  },
  "vidal93_eurospeech": {
   "authors": [
    [
     "Enrique",
     "Vidal"
    ],
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Esther",
     "Levin"
    ]
   ],
   "title": "Learning associations between grammars: a new approach to natural language understanding",
   "original": "e93_1187",
   "page_count": 4,
   "order": 235,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "Language Understanding can be seen as a process of translation from input natural language sentences into commands of a certain Semantic Language that drive the actions associated to the meaning of the sentences. Under this point of view, a new approach is introduced to automatically learn the required transducers from training sets of input-output examples. This approach overcomes certain input-output sequentiality problems of previous techniques. Experiments are presented with a subset of the DARPA ATIS corpus that show the capabilities of the new method to learn useful English-semantic mappings for this task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-233"
  },
  "jardino93_eurospeech": {
   "authors": [
    [
     "Michele",
     "Jardino"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "Language modelling for CSR of large corpus using automatic classification of words",
   "original": "e93_1191",
   "page_count": 4,
   "order": 236,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "Automatic word classification of a French corpus (2M words) has been performed, without any grammatical or semantic assumption. Training texts and test sets are both extracted from the newspaper \"Le Monde\". Model assessment is also given showing the role of test set size compared to the training set size.\n",
    "Keywords: Language Model, Simulated Annealing Process, Large Vocabulary Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-234"
  },
  "lucke93_eurospeech": {
   "authors": [
    [
     "Helmut",
     "Lucke"
    ]
   ],
   "title": "Inference of stochastic context-free grammar rules from example data using the theory of Bayesian belief propagation",
   "original": "e93_1195",
   "page_count": 4,
   "order": 237,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "The paper describes a new mechanism for inferring stochastic context free grammar rules from a corpus of training data. The approach is entirely statistical in nature and uses the theory of belief propagation in causal trees as described by Pearl [5]. In the proposed method the existing estimates of grammar rule probabilities are used to construct (partial) parse trees over segments of the utterance. A partial parse tree does not necessarily span an entire sentence, only some fragment of it. Belief propagation is then applied to these trees to obtain the posterior probability distribution of the non-terminal symbols at each grammar node. It is shown how the E-M algorithm can be used to re-estimate the production rule probabilities. The algorithm does not require any labels or segmentation of the input signal, and is thus completely un-supervised. A method for making the algorithm O(N2) rather than O(NZ) like the similar inside-outside algorithm is also proposed and evaluated. The method is compared against the bigram and trigram grammars and showed a significantly better performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-235"
  },
  "witschel93_eurospeech": {
   "authors": [
    [
     "Petra",
     "Witschel"
    ]
   ],
   "title": "Constructing linguistic oriented language models for large vocabulary speech recognition",
   "original": "e93_1199",
   "page_count": 4,
   "order": 238,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "Stochastic language models based on word n-grams require huge amount of training material and of storage especially for large vocabulary systems. Using n-grams based on classes much less training material is necessary and higher coverage can be achieved. Building classes on basis of linguistic characteristics has the advantage that new words can be mapped easily. To generate linguistic oriented language models training corpora are necessary where to each word its linguistic class has to be assigned. For this task we use commercially available linguistic knowledge bases of high coverage: a german lexicon and a grammar of a machine translation system. We first generate an initial language model using information derived from grammatical parse of training material. As next step linguistic structure represented statistically via the initial language model is extrapolated into any lexically tagged text. The initial language model in this way performs the basis of a bootstrapping process. Using the described technique we are presenting in this paper a tool which assigns to each word of lexically tagged text its linguistic oriented class respecting the sentence context. First evaluations show that 91.3% of classes are assigned correctly.\n",
    "Keywords: stochastic language models, large vocabulary speech recognition, tagging, n-grams, linguistic oriented classes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-236"
  },
  "banga93_eurospeech": {
   "authors": [
    [
     "Eduardo R.",
     "Banga"
    ],
    [
     "Carmen",
     "Garcia-Mateo"
    ]
   ],
   "title": "New frequency domain prosodic modification techniques",
   "original": "e93_0987",
   "page_count": 4,
   "order": 239,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "In this contribution we propose a new family of algorithms for prosodic modifications in the frequency domain, based on a harmonic model and a set of voiced/unvoiced decisions for different frequency bands. Due to the flexibility and simplicity of these algorithms they are ideal for applications like text-to-speech conversion.\n",
    "Keywords: Prosodic modifications, pitch harmonics, voiced/unvoiced decisions, text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-237"
  },
  "wang93b_eurospeech": {
   "authors": [
    [
     "H. D.",
     "Wang"
    ],
    [
     "D.",
     "Degryse"
    ],
    [
     "Fabrizio",
     "Carrara"
    ]
   ],
   "title": "A prosody modification approach for auditory user feedback in the spell pronunciation teaching system",
   "original": "e93_0991",
   "page_count": 4,
   "order": 240,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "In this paper, we present a LPC-based Analysis-Prosody Modification-Synthesis (APS) approach for auditory user feedback in the SPELL pronunciation teaching system. The basic idea of the approach is to demonstrate to students correct pronunciation by his own voice, given that the pronunciation difference would be clearer than simple play-back of the teacher's voice, due to the elimination of differences between the personalities of the two voices involved. The auditory user feedback by this approach is accomplished by three processing steps: analysis, prosody modification and synthesis. Listening tests revealed that this approach is very encouraging for teaching prosody patterns of a foreign language. It is independent of speaker and language, and very simple to implement. One important aspect of the approach is that it offers a potential tool for students to imitate his (her) own voice with a different and often unfamiliar prosody pattern. It might also be interesting to explore its efficiency in terms of pedagogical methodology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-238"
  },
  "takagi93_eurospeech": {
   "authors": [
    [
     "Tohru",
     "Takagi"
    ],
    [
     "Eiichi",
     "Miyasaka"
    ]
   ],
   "title": "A speech prosody conversion system with a high quality speech analysis-synthesis method",
   "original": "e93_0995",
   "page_count": 4,
   "order": 241,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "A speech prosody conversion system using a newly developed high quality speech analysis-synthesis method is presented. This system can convert prosodic features such as average pitch, pitch contour, speech rate and so on with small impairments in original voice personality. Users of this system can convert prosodic features of desired part of original speech waveforms to what they want by themselves with interactive operation. The speech analysis-synthesis method used here is characterized by correcting spectral envelopes distorted by pitch manipulation to keep original spectral envelopes.\n",
    "Keywords: Pitch contour, Speech rate, Prosody conversion\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-239"
  },
  "swerts93_eurospeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Rene",
     "Collier"
    ]
   ],
   "title": "On the perceived serial position of discourse units",
   "original": "e93_0999",
   "page_count": 4,
   "order": 242,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "One of the possible functions of intonation is its capacity to clarify textual structure. It may, for instance, indicate that a sentence is likely to be the last one in a sequence of statements forming a discourse unit. In order to investigate the effect of melodic cues on the perception of serial position, listening experiments were performed with short sentences, the intonation of which was manipulated with respect to three variables: CONTOUR TYPE (with or without end rise), REGISTER (high or low) and RANGE (narrow or wide). The actual testing concentrated on melodic cues to the 'last position' of an utterance in a spontaneous discourse. Judgments were expressed by absolute rating on a 'finality* scale. An ANOVA revealed that these finality judgments are influenced significantly by CONTOUR TYPE and REGISTER. The effect of RANGE was not significant, but there was a clear tendency for contours with a narrow range to be judged more final than contours with a wide range.\n",
    "Keywords: intonation, discourse structuretprosodic function\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-240"
  },
  "bagshaw93_eurospeech": {
   "authors": [
    [
     "Paul C.",
     "Bagshaw"
    ],
    [
     "Steven",
     "Hiller"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Enhanced pitch tracking and the processing of <i>F</i><sub>0</sub> contours for computer aided intonation teaching",
   "original": "e93_1003",
   "page_count": 4,
   "order": 243,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "A comparative evaluation of several pitch determination algorithms (PDAs) is presented. Fundamental frequency estimates, F0, are compared with laryngeal frequency estimates, Lx. An algorithm is presented which enables Lx contours to be generated from laryngograph data. We seek the most accurate method of FO extraction in order to minimise errors propagating into subsequent prosodic analysis. The super resolution pitch determinator [S] performs well relative to the other PDAs studied. Modifications made to this algorithm are described, which radically reduce the number of gross F0 errors and improve the classification of voiced and unvoiced sections of speech. The raw F0 contours produced by this enhanced algorithm are processed to form schematised contours used in computer aided intonation teaching. The series of processes used in the schematisation is described.\n",
    "Keywords: Pitch tracking, Intonation, Language teach- ing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-241"
  },
  "tadj93_eurospeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "Improved DVQ algorithm for speech recognition: a new adaptive learning rule with neurons annihilation",
   "original": "e93_1009",
   "page_count": 4,
   "order": 244,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "In this paper, three techniques are introduced to improve the DVQ algorithm. The first one consists in an automatic algorithm to determine the threshold <t, a function of the minimum class variance, by a cross validation procedure. The second improvement consists in a new adaptive learning rule based on a spatial geometry considerations. The algorithm is proposed to reduce an instability phenomenon which may appear during learning. Finally a criterion of neurons annihilation is proposed to remove unnecessary elements from the system to ensure network stability. Some experiments on real speech data are presented to show the effects of these three techniques on the network properties, including the learning time duration, the number of references and the recognition rate in each case. Keywords : Automatic Speech Recognition, Artificial Neural Network, Incremental Learning, Neurons Annihilation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-242"
  },
  "sasaki93_eurospeech": {
   "authors": [
    [
     "Taro",
     "Sasaki"
    ],
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Akira",
     "Iwata"
    ]
   ],
   "title": "Speaker-independent 212 word recognition using combNET-II",
   "original": "e93_1013",
   "page_count": 4,
   "order": 245,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "This paper describes a speaker-independent 212 word recognition method using dynamic features and averaged features of the speech spectrum based on a two-dimensional mel-cepstrum (TDMC) of a spoken word and a large scale neural network \"CombNET-II\". TDMC is defined as the two-dimensional Fourier transform of mel-frequency scaled logarithm spectra in the frequency and time domains. CombNET-II has a four-layered neural network with a comb structure. It consists of two parts of neural networks. The first part roughly classifies an input pattern into a category group and the second part precisely classifies the input pattern into a specified category. In this paper, the experiment of speaker-independent word recognition for 212 Japanese words uttered by 10 male speakers is carried out. In the experiment, dynamic features and averaged features based on TDMC are used as the input pattern of CombNET-IL A recognition accuracy of 95.5% can be obtained. This method reduced the amount of calculation to about 1/6 as compared with k-nn classifier. Keyword: word recognition, neural network, spectral features of speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-243"
  },
  "castano93_eurospeech": {
   "authors": [
    [
     "M. A.",
     "Castano"
    ],
    [
     "Enrique",
     "Vidal"
    ],
    [
     "Francicso",
     "Casacuberta"
    ]
   ],
   "title": "Learning direct acoustic-to-semantic mappings through simple recurrent networks",
   "original": "e93_1017",
   "page_count": 4,
   "order": 246,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "Recently, some approaches for Automatic Speech Understanding (ASU) have been proposed by Prieto et al, Segarra et al and Pieraccini et al. They are based on Regular Grammars or N-grams. Also, Neural Networks have been used by Stolcke in some experiments with synthetic data. However, the application of ASU to real-world problems generally supplies (acoustic-syntactic-semantic) models of considerable size. On the other hand, the acoustic and syntactic-semantic components of these models are often learned separately and are later integrated together. In this paper, we propose an approach to directly map the acoustic domain into a (large) semantic space through a Simple Recurrent Network of small size which, moreover, automatically learns all the features at the same time. As an application example, we consider a Continuous Speech Understanding (CSU) task recently considered by Prieto et al in which Grammatical Inference (GI) techniques were used.\n",
    "Keywords: Automatic Speech Understanding, Language Modeling, Neural Networks, Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-244"
  },
  "vaseghi93b_eurospeech": {
   "authors": [
    [
     "Saeed V.",
     "Vaseghi"
    ],
    [
     "Ben P.",
     "Milner"
    ]
   ],
   "title": "Noise-adaptive hidden Markov models based on wiener filters",
   "original": "e93_1023",
   "page_count": 4,
   "order": 247,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "In noisy speech recognition, Wiener filters may be applied either directly to the noisy speech or, alternatively, the filters can be used to adapt the HMM mean cepstral vectors. In this paper we present experimental results which demonstrate that Wiener filters used for the adaption of HMM cepstral means perform better than the direct application of Wiener filters to the noisy signal. Experiments indicate that the variance of the cepstral features decrease with increasing noise. A theoretical explanation of the decrease in variance is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-245"
  },
  "wong93_eurospeech": {
   "authors": [
    [
     "K. F.",
     "Wong"
    ],
    [
     "S. H.",
     "Leung"
    ],
    [
     "H. C.",
     "Ng"
    ]
   ],
   "title": "Noisy speech recognition using singular value decomposition and two-sided linear prediction",
   "original": "e93_1027",
   "page_count": 4,
   "order": 248,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "The objectives of this work is to present a new feature representation for speech recognition in noisy environment using a combination of the Two-Sided Linear Prediction and Singular Value Decomposition (TSLP-SVD). The two-sided prediction model has very robust feature against additive noise. Furthermore, the noise effect on estimating the feature vector can be reduced by using SVD to extract the dominant components of the speech signal. A new Nonlinear Noise Subtraction (NNS) is used for the further reduction of noise level during parameter estimation.\n",
    "Keywords: Noisy speech recognition, Two-Sided Linear Prediction, Singular Value Decomposition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-246"
  },
  "martin93_eurospeech": {
   "authors": [
    [
     "Franck",
     "Martin"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Recognition of noisy speech by composition of hidden Markov models",
   "original": "e93_1031",
   "page_count": 4,
   "order": 249,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "This paper proposes an algorithm for recognizing noisy-speech while avoiding the tedious training of noisy-speech HMMs. HMM composition combines a noise-source HMM and a phoneme HMM into one noise-added phoneme HMM. The speech recognizer is based on LPC cepstrum analysis. In the first set of speaker-dependent experiments consisting in recognizing 23 Japanese phonemes with a variety of stationary and nonstationary noises with signal-to-noise ratios ranging from 0 dB to 20 dB, the algorithm reduced the error of the phoneme-recognition rate by more than 75%. In the second set of speaker-dependent experiments consisting in recognizing continuous speech sentences, the composed HMMs could be obtained very rapidly and gave similar recognition rates to those of phoneme HMM models trained by using a large noise-added speech database. The efficiency, flexibility of the algorithm and its adaptability to new noises and to various SNRs make it a suitable basis for a real-time speech recognizer resistant to noise.\n",
    "Keywords: noisy speech, HMM composition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-247"
  },
  "gao93_eurospeech": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Noise reduction and speech recognition in noise conditions tested on LPNN-based continuous speech recognition system",
   "original": "e93_1035",
   "page_count": 4,
   "order": 250,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "In this paper, we present our new continuous speech recognition system, which is based on linked predictive neural networks, and a noise reduction neural network, which processes noise-collapsed speech signals in an auditory model based spectral domain. We also describe a series of noise reduction and speech recognition experiments in noise conditions in order to evaluate performance of both the LPNN-based recognizer and the noise reduction network. The experimental results show that our LPNN-based speech recognition system has better performance in noise conditions than a probabilistic model based system. In all experimental cases, the noise reduction network performed well and decreased the recognition error rate by at least 13%.\n",
    "Keywords: Speech Recognition, Neural Networks, Noise Reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-248"
  },
  "trompf93_eurospeech": {
   "authors": [
    [
     "Michael",
     "Trompf"
    ],
    [
     "Ralf",
     "Richter"
    ],
    [
     "Harald",
     "Eckhardt"
    ],
    [
     "Heidi",
     "Hackbarth"
    ]
   ],
   "title": "Combination of distortion-robust feature extraction and neural noise reduction for ASR",
   "original": "e93_1039",
   "page_count": 4,
   "order": 251,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "A hybrid approach to reliable word recognition under adverse environmental conditions is described in this paper. The overall strategy is to find at first a distortion-robust signal representation and to remove the noise components in this domain afterwards. Several preprocessing steps help to enhance the robustness of the basic feature set generated from standard lpc-cepstrum analysis: calculation of temporal derivatives, principal component analysis and reordering of the coefficient set with respect to a predefined figure of merit. Subsequent neural network-based noise reduction widely removes the remaining noise component. The contribution of each step is evaluated separately by an isolated word recognizer, and some aspects of cost reduction for realtime implementation are discussed.\n",
    "Keywords: dimensionality reduction, feature vectors, Lombard effect, neural networks, noise reduction, noise robustness, word recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-249"
  },
  "mokbel93_eurospeech": {
   "authors": [
    [
     "C.",
     "Mokbel"
    ],
    [
     "J.",
     "Monné"
    ],
    [
     "D.",
     "Jouvet"
    ]
   ],
   "title": "On-line adaptation of a speech recognizer to variations in telephone line conditions",
   "original": "e93_1247",
   "page_count": 4,
   "order": 252,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "This paper presents the effects of variations in telephone line conditions on speech signals and its influence on speaker-independent recognition performance. Measurements made on a large data base collected over the telephone network show, for a given call, an important constant component perturbing the signal. This component varies greatly according to the call, reducing the discrimination between different vocabulary words. This disturbance is mainly caused by the convolved line transfer function that seems more harmful than the additive ambient noise in the databases. Cepstral subtraction is investigated to reduce the convolved disturbance. The long-term cepstrum for a given call is computed and then subtracted from the cepstra of all the utterances to be recognized We propose to subtract the first coefficients of the long-term cepstrum which smooths the corresponding long-term logarithm spectrum. This gives satisfactory results and the system obtained is more robust than the basic recognizer (20% reduction of the error rate). Another approach to normalize speech data and reduce the line effects is to use a neural network. A multilayer perception is trained to bring acoustical vectors nearer to the corresponding basic HMM gaussian mean vectors, the longterm cepstrum being presented as an input Preliminary experiments give encouraging results.\n",
    "Keywords: Telephone line effects, HMM, Cepstral Subtraction, Neural Networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-250"
  },
  "wittmann93_eurospeech": {
   "authors": [
    [
     "Matthias",
     "Wittmann"
    ],
    [
     "Otto",
     "Schmidbauer"
    ],
    [
     "Abdulmesih",
     "Aktas"
    ]
   ],
   "title": "Online channel compensation for robust speech recognition",
   "original": "e93_1251",
   "page_count": 4,
   "order": 253,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "This paper focusses on the problem of different acoustic channels in real world speech recognition scenarios. We describe an approach which enables a recognition system to compensate the influence of transmission, microphon or recording hardware. The compensation works with single-channel recordings and is based on an online estimation of the logarithmic longterm spectrum of speech. This estimated spectrum is substracted from the short-term absolute spectra. In experiments with a speaker-independent continuous speech recognizer our method essentially improves recognition performance. Using test utterances with transmission channels, which do not occur in the training material, the word error rate is reduced from 40 % to 29 %. In contrast to common compensation methods, our approach does not degrade performance, when test and training data are taken from the same channel.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-251"
  },
  "alexandre93_eurospeech": {
   "authors": [
    [
     "Patrice",
     "Alexandre"
    ],
    [
     "Jerome",
     "Boudy"
    ],
    [
     "Philip",
     "Lockwood"
    ]
   ],
   "title": "Evaluation of car noise reduction/compensation techniques for digit recognition in a speaker-independent context",
   "original": "e93_1255",
   "page_count": 4,
   "order": 254,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "In the present paper we report some recent experiments we have made in an isolated, speaker-independent (SI), recognition task under car-noise conditions. This work is an extension of the previous results we obtained in a speaker-dependent (SD) context [2][3][12][13]. The main topic of interest concerns the use of root-homomorphic deconvolution schemes [11], since it has been shown that such method gives an optimal solution to solve the deconvolution problem of voiced speech with the constraint to minimise the effects of the background noise [3]. In an SI context, the first complication comes from the inter-speaker variabilities. To minimise this effect, a low-dimensional MEL-scaled filter bank [5] is generally used. It is of evidence that such a processing destroys the convolutional structure of speech. Consequently, the root-homomorphic deconvolution is no longer justified. The goal of this paper is to give some experimental results of root-homomorphic schemes combined with a MEL filter bank representation. In an attempt to reduce the effects of the noise on the parameters, it was recently suggested the use of high-pass filtering techniques [9] [10], assuming that the stationary component of noise can be separated from speech. We show here that this filtering technique can be interpreted as a spectral subtraction technique, for which the noise model is estimated continuously; no voice activity detection (VAD) is required (VAD-free spectral subtraction). With such a consideration, we show that this technique is less efficient than the spectral subtraction technique, for which the noise model is estimated during non-speech activity (VAD-dependent spectral subtraction). The experiments are carried out using an HMM-based isolated digit recogniser.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-252"
  },
  "brancaccio93_eurospeech": {
   "authors": [
    [
     "A.",
     "Brancaccio"
    ],
    [
     "C.",
     "Pelaez"
    ]
   ],
   "title": "Experiments on noise reduction techniques with robust voice detector in car environment",
   "original": "e93_1259",
   "page_count": 4,
   "order": 255,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "In this paper, the attention is focussed on the importance of improving robustness against noise of all the pre-processing steps in a single-channel speech recognition system for operation in car environment. Different kind of parameter representation, noise reduction techniques and an automatic end-point detector were evaluated. Experiments were conducted by using a real car database and a Dynamic Time Warping (DTW) recogniser. At first, in an ideal condition (manual segmentation), the behaviour of two cepstral parameter sets and different speech enhancement techniques were compared. Afterwards, the performances of the recogniser using the best parameter were also evaluated in case of an automatic speech segmentation. Finally, an evaluation of the complete recognition chain with two different integrated Noise Reduction techniques was made.\n",
    "Keywords: Signal Pre-Processing, Feature Extraction, Noise Reduction, End-Point Detection, Speech Recognition, Car Environment\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-253"
  },
  "nakamura93_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Toshio",
     "Akabane"
    ],
    [
     "Seiji",
     "Hamaguchi"
    ]
   ],
   "title": "Robust word spotting in adverse car environments",
   "original": "e93_1045",
   "page_count": 4,
   "order": 256,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "This paper presents a novel word recognition technique which allows hand-free dialing under adverse car environments. The algorithm is based on speaker dependent word spotting. The paper compared and investigated the methods of spectral subtraction, short time modified coherence, multi-microphone, dynamic and accelerated features, weighted distance measures and multi-templates by word recognition experiments. The experiments are carried out using real speech database uttered in adverse car environments. The experiments show that the method using sinusoidal weighted lifter and the method using multi-templates are robust against the adverse environments.\n",
    "Keywords: word spotting, distance measure, adverse environment\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-254"
  },
  "rose93_eurospeech": {
   "authors": [
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Definition of subword acoustic units for wordspotting",
   "original": "e93_1049",
   "page_count": 4,
   "order": 257,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "This paper describes a study that was performed to evaluate several acoustic modeling techniques for HMM wordspotting. The wordspotting task involves unconstrained conversational speech utterances spoken over the public switched telephone network. Derived from the Switchboard speech corpus [4], the task is to detect a small vocabulary of keywords from running speech given approximately two hours of conversational speech utterances for training. The study was performed using a hidden Markov model (HMM) wordspotter based on a continuous speech recognition model. Several interesting results were obtained that have application to small vocabulary speech recognition problems where the input speech is relatively unconstrained and out-of-vocabulary utterances are poorly represented in training. These results concern the use of decision tree based allophone clustering for defining acoustic subword units, different representations for non-vocabulary words occurring in the input speech utterance, and the definition of simple language models for constraining the possible word transitions in the vicinity of keywords.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-255"
  },
  "kiyama93_eurospeech": {
   "authors": [
    [
     "Jiro",
     "Kiyama"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Spontaneous speech recognition by sentence spotting",
   "original": "e93_1053",
   "page_count": 4,
   "order": 258,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "A speaker-independent system for recognizing sentence speech is proposed based on sentence spotting algorithm. The system consists of 1) the Partial Matching Method (PMM : this was already proposed by one of us) for spotting demiphonemes, and 2) the Vector Continuous Dynamic Programming (VCDP) for spotting sentences. The algorithm for sentence spotting is completely synchronized with frame of speech analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-256"
  },
  "jeanrenaud93_eurospeech": {
   "authors": [
    [
     "P.",
     "Jeanrenaud"
    ],
    [
     "K.",
     "Ng"
    ],
    [
     "M.",
     "Siu"
    ],
    [
     "J.R.",
     "Rohlicek"
    ],
    [
     "H.",
     "Gish"
    ]
   ],
   "title": "Phonetic-based word spotter: various configurations and application to event spotting",
   "original": "e93_1057",
   "page_count": 4,
   "order": 259,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "In previous work [1], we reported initial word spotting results using our phonetically based word spotter. In this paper, we first present a new version of our forward-backward based keyword score. We then concentrate on the choice of an HMM configuration, where a configuration defines how keyword and non-keyword components are combined. Possible components are a phoneme loop, a large or reduced vocabulary, and a language model. We examine various combinations of these components and discuss some issues in choosing an appropriate configuration for a given application. We then show how keyword spotting can be easily extended to \"event\" spotting by simply substituting a sub-grammar describing the event in place of a keyword. Experimental results are shown for various configurations and we explore how lexical coverage and the presence of a phoneme loop affect the overall performance. We show that the vocabulary can be significantly reduced with limited impact on performance. Finally, we present some initial results on an event spotting task.\n",
    "Keywords: word spotting, hidden Markov model\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-257"
  },
  "imamura93_eurospeech": {
   "authors": [
    [
     "Akihiro",
     "Imamura"
    ],
    [
     "Mikio",
     "Kitai"
    ]
   ],
   "title": "An application of word-spotting in a voice activated service entry system",
   "original": "e93_1061",
   "page_count": 4,
   "order": 260,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "This paper describes a speaker-independent word spotting algorithm and its application in a voice activated service name entry system over telephone networks. The system allows a user to specify a keyword with extraneous speech in response to system audio prompts. A task oriented multi-speaker isolated-word database is used to construct keyword HMMs. The score of keywords is computed by using the continuous Viterbi decoding algorithm with score normalization using a background HMM. The candidates for keywords are obtained after evaluating the partial matches. The dialogue design of the system improves user-friendliness and shortens the user's operation time by employing a newly proposed adaptive confirmation procedure. The procedure effectively repeats commands and masks inappropriate control functions. An evaluation over real telephone lines is presented that confirms improved user interface for menu selection tasks.\n",
    "Keywords: Word spotting, Dialogue system\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-258"
  },
  "lleida93_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Josep M.",
     "Salavedra"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "E.",
     "Monte"
    ],
    [
     "A.",
     "Martinez"
    ]
   ],
   "title": "Out-of-vocabulary word modelling and rejection for keyword spotting",
   "original": "e93_1265",
   "page_count": 4,
   "order": 261,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "In this paper, we deal with the problem of non-keyword modelling and rejection in a Hidden Markov Model (HMM) based Spanish keyword spotting. When talking about the performance of a keyword spotting system in terms of false alarm rejection, the non-keyword modelling and the rejection techniques are two relevant topics. With regard to the non-keyword modelling, our approach is to define a set of task independent filler models which can be used in any application. In this paper we investigate the performance of a set of filler definition in the problem of detecting digits embedded in utterances. Particularly, we are working with three filler definitions: phonetic fillers, syllabic fillers and word-based fillers. For false alarm rejection, we handle the problem as a post processor of the HMM word spotting recogniser. We design a specific classifier based on a Neural Network and linear discriminant functions to classify a keyword hypothesis in keywonl/non-keyword.\n",
    "Keywords: Keyword spotting, hidden Markov models, filler models, false alarm rejection, linear discriminant functions, Neural Network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-259"
  },
  "okane93_eurospeech": {
   "authors": [
    [
     "M. J.",
     "O'Kane"
    ],
    [
     "P. E.",
     "Kenne"
    ]
   ],
   "title": "Word and phrase spotting with limited training",
   "original": "e93_1269",
   "page_count": 4,
   "order": 262,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "Experiments using a computationally efficient wordspotter which works well on very limited training data are described. Most wordspotters reported in the literature are either HMM or neural network based. This wordspotter uses yet another architecture based on parallel broad encodings. The most interesting aspect of the use of this wordspotter is the results it achieves with limited training data even from a single speaker. Improvements in wordspotting as the amount of training data is increased are given as a function of the speakers in the training set and the amount of training data for each speaker. Speaker-dependent results are also given.\n",
    "Keywords: wordspotter, training\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-260"
  },
  "boite93_eurospeech": {
   "authors": [
    [
     "Jean-Marc",
     "Boite"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Bart",
     "D'hoore"
    ],
    [
     "Marc",
     "Haesen"
    ]
   ],
   "title": "A new approach towards keyword spotting",
   "original": "e93_1273",
   "page_count": 4,
   "order": 263,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "Hidden Markov Models (HMMs) are now widely used for isolated word and continuous speech recognition and, given their success, have been applied recently to the keyword spotting (KWS) problem. In this paper, a new approach is presented, which does not attempt to explicitly model extraneous speech. This approach is tested on a speaker independent, over the telephone line, 10 word lexicon database and compared with several other KWS algorithms using explicit garbage modelling. As the resulting methods should be used in a real time system, we focus on these approaches that do not require extensive computation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-261"
  },
  "alvarezcercadillo93_eurospeech": {
   "authors": [
    [
     "J.",
     "Alvarez-Cercadillo"
    ],
    [
     "Luis A.",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Grammar learning and word spotting using recurrent neural networks",
   "original": "e93_1277",
   "page_count": 4,
   "order": 264,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "An hybrid network system for continuous speech recognition is presented, based in subword units. At the first stage we detect the order, duration and likelihood of the phoneme sequence with a phoneme spotting system based in Hidden Markov Models. At the second level, a Recurrent Neural Network performs the phoneme decoding by selecting the chain of phonemes that forms a legal word. The network was able to learn the underlying phonetic grammar and could reduce the false alarm rates.\n",
    "Keywords: Continuous Speech Recognition; Hybrid Network; Phoneme Spotting System; Recurrent Neural Network; Grammar learning.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-262"
  },
  "okawa93_eurospeech": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Word spotting in conversational speech based on phonemic unit likelihood by mutual information criterion",
   "original": "e93_1281",
   "page_count": 4,
   "order": 265,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "This paper proposes a novel scheme for keyword-spotting in conversational speech using frame-level likelihood of phonemes and statistics of their duration. Since spontaneous utterances include many ill-formed sentences, it is most difficult to realize a highly advanced continuous speech recognition system based on a top-down syntax driven process. We, therefore, propose a bottom-up method to detect keywords in continuous speech based on a dynamical programming technique using both phonemic and durational likelihood. Our algorithm basically depends on island-driven both-side-free DP method. In the performance test of the speaker-dependent keyword spotting, it was found that, compared to the conventional continuous DP method, the erroneous candidates and the processing time decreases to 1/6 in new method. This result shows the feasibility of our method for continuous speech recognition, especially for conversational style utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-263"
  },
  "dohnal93_eurospeech": {
   "authors": [
    [
     "F.",
     "Dohnal"
    ]
   ],
   "title": "Generalized frequency domain adaptive filter for acoustic echo canceller",
   "original": "e93_1067",
   "page_count": 1,
   "order": 266,
   "p1": "paper 1067",
   "pn": "",
   "abstract": [
    "The most significant problems of the acoustic echo canceller realizations are high computational complexity and insufficient convergence rate of the applied adaptive algorithms. The use of the frequency domain block adaptive filter [1] essentially reduces computational complexity and increases convergence rate. Of course, in implementations of the frequency domain block adaptive filter, the size of the block must be set to twice the number of filter tap weights. The associated processing delay, equal to the block size, appeared to be prohibitive in acoustic echo cancellers. The basic method of solving this problem is to segment the impulse response into small blocks. This idea has been employed in the subband acoustic echo cancellers. The analysis of the realization of the frequency domain block adaptive filter and the subband acoustic echo canceller where DFT filter bank was employed to filter bank realization [2] has been performed. It has followed from this analysis that this subband acoustic echo cancellers do not take into account the effect of the multiplicative modifications in the DFT filter bank [3]. The effect of the multiplicative modification has been taken into account in the fast convolution methods used for the frequency domain block adaptive filter realization. This effect has brought the decrease of the convergence rate and increase of the steady-state mean square error by time domain aliasing in the subband acoustic echo cancellers. On the basis of these facts the new adaptive filter for the acoustic echo canceller [4] has been derived . The new adaptive filter has been named the generalized frequency domain adaptive filter and this adaptive filter is a generalization of the subband adaptive filter and the frequency domain block adaptive filter. The qualities of the design algorithm were evaluated by computer simulation with acoustic echo path impulse response model for white noise and real speech signal excitation. The result of simulation performance showed convergence behaviour improvement of new adaptive algorithm on the subband acoustic echo cancellers applied till the present time. From the computational complexity analysis follow the realization possibility of new algorithm for acoustic echo path length 256 ms, 16 kHz sampling rate and 16 ms processing delay on modern signal processor in real time.\n",
    ""
   ]
  },
  "crestel93_eurospeech": {
   "authors": [
    [
     "J.",
     "Crestel"
    ],
    [
     "M.",
     "Guitton"
    ]
   ],
   "title": "Estimation of speech signal classification features in a simulated hyperbaric environment",
   "original": "e93_1069",
   "page_count": 4,
   "order": 267,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "This paper deals with the following question: can the conventionnal classification algorithms be transposed to the \"hyperbaric\" speech signal case? An objective estimate of discriminating features computed on synthetic signals, that is on tests signals, appears to be a relevant approach. As it is, five basic features commonly involved in voiced-unvoiced decision rules are consistently computed, first on \"air\" vowels signals considered as references, then on similar \"heliox\" vowels signals. Globally, the discriminating features abilities are preserved and are even likely to be improved, provided that thresholds are adapted.\n",
    "Keywords: \"hyperbaric\" speech, classification features, speech modelization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-264"
  },
  "pollak93_eurospeech": {
   "authors": [
    [
     "Petr",
     "Pollak"
    ],
    [
     "Pavel",
     "Sovka"
    ],
    [
     "Jan",
     "Uhlir"
    ]
   ],
   "title": "Noise suppression system for a car",
   "original": "e93_1073",
   "page_count": 4,
   "order": 268,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "The whole system for noise suppression in speech recorded in a running car was designed. One channel spectral subtraction method with full-wave rectification was chosen because of its robustness, simplicity, and non-musical tone output. The improvement of noise suppression was gained by the repetition of this method. Directional microphones for the signal picking up were chosen to improve the input signal-to-noise ratio (SNR) of corrupted speech signal. Segment speech/pause detector based on energy tracking was used with some prefiltration of corrupted speech to improve detector function.\n",
    "Keywords: noise suppression, speech enhancement, spectral subtraction, musical tones, speech/pause detection\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-265"
  },
  "heitkamper93_eurospeech": {
   "authors": [
    [
     "Peter",
     "Heitkamper"
    ],
    [
     "Michael",
     "Walker"
    ]
   ],
   "title": "Adaptive gain control and echo cancellation for hands-free telephone systems",
   "original": "e93_1077",
   "page_count": 4,
   "order": 269,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "The combination of adaptive gain control and acoustic echo cancellation can reduce the problems arising with hands-free telephone systems. Using a short-length echo canceller, the acoustic coupling between the loudspeaker and the microphone can be decreased consuming only a small amount of processing power. The quality of the local speech signal is improved and residual echoes are attenuated by means of the adaptive gain control. Attached with the gain control is a simple speech detection method and a cross-correlation estimate, which give information about the speakers5 activities. This information can be used to control the adaptation of the echo canceller. The algorithm was implemented on a digital signal processor and was found to operate well in wide bandwidth of 12kHz in various environments.\n",
    "Keywords: Hands-free Telephones, Echo Cancellation, Adaptive Gain Control, Speech Processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-266"
  },
  "campbell93_eurospeech": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ]
   ],
   "title": "Predicting segmental durations for accommodation within a syllable-level timing framework",
   "original": "e93_1081",
   "page_count": 4,
   "order": 270,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "This paper describes a two-level model for the prediction of timing in English speech. It details an experiment using stochastic methods to learn segmental duration distribution characteristics for accommodation into a syllable-level timing framework, and shows that while 86% of syllable duration variability can be predicted, there is greater freedom in durational specification at the segmental level.\n",
    "Keywords: segmental duration syllable-level control, accommodation, elasticity, speaker-individuality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-267"
  },
  "fjallbrant93_eurospeech": {
   "authors": [
    [
     "Tore",
     "Fjallbrant"
    ],
    [
     "Fisseha",
     "Mekuria"
    ],
    [
     "Shahrokh",
     "Amirijoo"
    ]
   ],
   "title": "A filtersank based on physiologically measured characteristics in an auditory model for speech signal processing",
   "original": "e93_1085",
   "page_count": 4,
   "order": 271,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "It is shown that digital linear filter networks with characteristics similar to physiologically measured magnitude, phase and impulse response characteristics of channels with lower centre frequencies in the auditory pathway up to the auditory nerve can be found. An IIR filterbank with similar characteristics has been designed on a psychoacoustic frequency scale. An auditory image in a tonotopical versus periodicity space has ben constructed. This image is shown to posess a number of attractive features for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-268"
  },
  "jean93_eurospeech": {
   "authors": [
    [
     "Fu-Rong",
     "Jean"
    ],
    [
     "Chih-Chung",
     "Kuo"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Spectral sensitivity weighted transform coding for LSP parameters",
   "original": "e93_1089",
   "page_count": 4,
   "order": 272,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "The line spectrum pair (LSP) is one of the most effective representations of the speech short-time spectrum. About 34 bits/frame is needed for direct quantization of LSP parameters to maintain a reasonable accuracy. Based on the spectral-sensitivity-weighted Euclidean distance of LSP parameters, a hybrid TC/DPCM coding of LSP parameters which takes into account the spectral sensitivity weighting is proposed . In addition to the scalar quantization, two vector quantization methods which are multi-stage VQ and partitioned VQ, are considered. With the frame period of 10 ms, the best result shows that the spectral distortion limen of 1 dB2 can be achieved at 18 bits/frame with in-training test data and 20 bits/frame with out-of-training test data, respectively.\n",
    "Keywords: line spectrum pair, spectral distortion, vector quantization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-269"
  },
  "martin93b_eurospeech": {
   "authors": [
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "An efficient algorithm to estimate the instantaneous SNR of speech signals",
   "original": "e93_1093",
   "page_count": 4,
   "order": 273,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "This contribution presents an efficient algorithm to estimate the instantaneous signal-to-noise ratio of speech signals. The algorithm is capable to track non stationary noise signals and has a low computational complexity. It does not need a speech activity detector nor histograms to leam signal statistics. The algorithm is based on the observation that a noise power estimate can be obtained using minimum values of a smoothed power estimate. This paper will present this algorithm, its performance, its limits, and some applications.\n",
    "Keywords: SNR, time delay estimation, speech enhancement\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-270"
  },
  "mauuary93_eurospeech": {
   "authors": [
    [
     "L.",
     "Mauuary"
    ],
    [
     "J.",
     "Monne"
    ]
   ],
   "title": "Speech/non-speech detection for voice response systems",
   "original": "e93_1097",
   "page_count": 4,
   "order": 274,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "In Voice Response Systems (VRSs) allowing the speaker-independent recognition of small vocabularies over the telephone network, a significant number of errors is due to faintness in speech/non-speech detection. In this paper, we firstly present a speech/non-speech detection evaluation method. This method helps to evaluate the speech/non-speech detection algorithm currently used by CNET. We then propose an improved adaptive version of this speech/nofl-speech detection algorithm. The evaluation results show great improvements.\n",
    "Keywords: Speech/non-speech detection; Adaptive automaton Robustness; Voice Response Systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-271"
  },
  "osipov93_eurospeech": {
   "authors": [
    [
     "Alexander",
     "Osipov"
    ],
    [
     "Vladimir",
     "Zentsov"
    ]
   ],
   "title": "Time-spectral approach to compiling speech reconstruction",
   "original": "e93_1101",
   "page_count": 2,
   "order": 275,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "The method of compiling speech reconstruction with its modifications which differs in favour way in its good articulation, natural properties of speech synthesized as well hardware implementation simplicity is proposed. The method is a combination of direct and parametric speech encoding and based on spectral-time approach when time intervals between signal zero crossings are used as spectral expansion ones and spectral expansion coefficients - as spectral parameters.\n",
    "Keywords: compiling speech reconstruction, spectral expansion in functional series, speech articulation and quality estimates.\n",
    ""
   ]
  },
  "haigh93_eurospeech": {
   "authors": [
    [
     "J. A.",
     "Haigh"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "A voice activity detector based on cepstral analysis",
   "original": "e93_1105",
   "page_count": 2,
   "order": 276,
   "p1": "1105",
   "pn": "1106",
   "abstract": [
    "This paper proposes a new approach to speech end-point detection based on cepstral analysis. The algorithm is based on explicit (static) modelling of speech and non-speech, and decisions are made on each incoming (overlapped) cepstral frame, according to model similarity scores. The cepstral analysis provides excellent level-independence, meaning that parameter adjustment, decision thresholds etc, are unnecessary. A high degree of robustness to additive noise is demonstrated, even though the models are static. Accurate end-points are recovered with SNR levels of 0dB.\n",
    ""
   ]
  },
  "paulus93_eurospeech": {
   "authors": [
    [
     "Jürgen",
     "Paulus"
    ],
    [
     "Christiane",
     "Antweiler"
    ],
    [
     "Christian G.",
     "Gerlach"
    ]
   ],
   "title": "High quality coding of wideband speech at 24 kbit/s",
   "original": "e93_1107",
   "page_count": 4,
   "order": 277,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "This paper proposes a Wideband-CELP-Coding scheme (band-width 7kHz ) at 24 kbit/s. The codec introduces a delay of just 10 ms. This fulfills the requirements of a possible codec candidate for wideband speech coding within DECT or video applications [1]. The analysis-by-synthesis structure of the proposed Wideband-CELP-Codec includes an alternative LPC analysis concept, where the autocorrelation function is calculated recursively [2]. This special LPC scheme provides an improved speech quality and a reduction of computational complexity in comparison to conventional algorithms for the LPC analysis. In addition a stochastic sparse codebook with extremely low computational effort is presented resulting in a neglectable amount of storage. The CCJTTG.722 standard was applied as reference codec, in order to compare the new coding scheme in terms of subjective quality. With the proposed Wideband-CELP a speech quality is achieved, which is equivalent to the reference codec operating at 56kbit/s.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-272"
  },
  "dia93_eurospeech": {
   "authors": [
    [
     "H.",
     "Dia"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Y.",
     "Mahieux"
    ]
   ],
   "title": "A 32 kbit/s wideband speech coder based on transform coding",
   "original": "e93_1111",
   "page_count": 4,
   "order": 278,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "Wideband speech coding (0 to 7 kHz) finds a broad range of applications in modern telecommunication systems such as video-phone and teleconferencing. The quality obtained with the standard G722 of CCITT is sufficient, but the bit-rate (64 kbits/s) is too high. Recently, transform coding has made great performances in high-fidelity transmission of audio signals (Brandenburg & al., 1991). This technique has prompted us to use it for wideband speech coding. In order to obtain a good quality of coded speech, the voiced/unvoiced (V/UV) nature of speech has been taken into account at each stage of the algorithm development. In this paper, a fixed bitrate coder at 32 kbits/s and a variable bit-rate one with a mean bit-rate of 24 kbits/s are presented. Evaluation results about the subjective quality of the two coders are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-273"
  },
  "gottesman93_eurospeech": {
   "authors": [
    [
     "Oded",
     "Gottesman"
    ],
    [
     "Yair",
     "Shoham"
    ]
   ],
   "title": "Realtime implementation of high-quality 32 kbps wideband LD-CELP coder",
   "original": "e93_1115",
   "page_count": 4,
   "order": 279,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "The Wideband-Audio Low-Delay CELP (LD-CELP) coder produces speech with quality as high as the CCITT 64 kb/s standard (G.722) at half the bitrate. The computational load of the encoder is almost 900% processor time of the 12.5 MIPS DSP32c. This makes a real-time implementation impractical. We investigated the Gain-Shape Vector-Quantization (GSVQ) in order to reduce the computational load of the encoder. This paper describes a real-time implementation of the LD-CELP encoder based on the AT&T SURFboard using two DSP32c operating in parallel. A computational load of 180% processor time has been achieved. The respective decoder requires 42% processor time. The implementation of a full-duplexed coding system requires three 12.5 MIPS Digital-Signal-Processors (DSPs) and has one-way coding delay of less than lms. The coder also performs well for non-speech wideband audio signals such as music.\n",
    "Keywords: Wideband, LD-CELP.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-274"
  },
  "popescu93_eurospeech": {
   "authors": [
    [
     "A.",
     "Popescu"
    ],
    [
     "D.",
     "Vicard"
    ],
    [
     "F.",
     "Druilhe"
    ]
   ],
   "title": "A fixed-point implementation of the 16 kb/s LD-CELP speech coding algorithm",
   "original": "e93_1119",
   "page_count": 4,
   "order": 280,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "The AT&T LD-CELP speech coding algorithm was ratified in 1992 as CCITT standard G728. The LD-CELP algorithm meets low-delay requirements and offers very good speech quality and error robustness. Floating-point implementations of the algorithm are known on the AT&T DSP32C [3] and the Texas Instruments TMS320C30, but the algorithm has a reputation to be difficult to implement in fixed-point. We implemented the algorithm on the SGS-Thomson ST18933 general-purpose fixed-point processor. This paper describes our implementation, specifying data format at different stages of the algorithm and other algorithmic details. Several problems were encountered during this implementation and the solutions we propose here can be used in subsequent implementations and/or in a fixed-point specification of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-275"
  },
  "gerlach93_eurospeech": {
   "authors": [
    [
     "Christian G.",
     "Gerlach"
    ]
   ],
   "title": "Optimality of sequential quantization in analysis-by-synthesis speech codecs",
   "original": "e93_1123",
   "page_count": 4,
   "order": 281,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "In analysis-by-synthesis coders the problem of approximating the original signal by the synthesized signal is solved over a limited time interval only. In this contribution a systematic investigation of possible improvements by using extended or even unlimited intervals is presented.\n",
    "Keywords: Analysis-by-synthesis; vector quantization; LPC; distortion minimization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-276"
  },
  "kastantin93_eurospeech": {
   "authors": [
    [
     "Radwan",
     "Kastantin"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "A sub-band MPLPC coder for high quality speech coding at 16 kbit/s",
   "original": "e93_1127",
   "page_count": 4,
   "order": 282,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "Multi-pulse excitation model used for speech coding provides excellent quality at medium bit-rates. However it is not very suitable for producing high frequency components in speech signals. For instance, a SNR of 15 dB in the lower band (0-2 kHz) of speech can be reduced to 3 dB for the upper band (2-4 kHz) of the signal. This paper describes a new method which enhances the speech quality by improving reproduction of high frequency portions of the signal. The idea consists of splitting the total frequency range into two sub-bands by means of Quadratic Mirror Filters, and optimizing the coding in each band. Evaluations have shown that the SNR can be improved up to 9 dB in high frequency band, and an enhancement of the total speech quality is obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-277"
  },
  "mumolo93_eurospeech": {
   "authors": [
    [
     "Enzo",
     "Mumolo"
    ],
    [
     "Alessio",
     "Rebelli"
    ]
   ],
   "title": "Optimal multepulse excitation determination by simulated annealing",
   "original": "e93_1131",
   "page_count": 4,
   "order": 283,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "Multipulse is a well known, analysis by synthesis technique to model the LPC residual signal [1], With this model, speech can be coded at medium bit rates with good quality. However, the determination of the pulse locations is a combinatorial problem with no closed form solution. For these reason suboptimal procedures are used. In this paper, inhomogeneous SA structures have been used for solving the multipulse problem; the obtained performances are compared with suboptimal algorithms such as the Atal's ones. As compared to the one described in [2], this algorithm achieves an effective improvement in objective performances. As a matter of fact, an improvement of more than 1.5 dB is obtained. A set of performance curves will be also described in order to find the optimum trade-off configurations.\n",
    "Keywords: Stochastic Optimization, Simulated Annealing, Speech Coding, Multipulse Models\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-278"
  },
  "law93_eurospeech": {
   "authors": [
    [
     "K. W.",
     "Law"
    ],
    [
     "C. F.",
     "Chan"
    ]
   ],
   "title": "Split vector quantization of the LPC parameters using weighted lattice structure",
   "original": "e93_1135",
   "page_count": 4,
   "order": 284,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "In this paper, we propose a new split vector quantization (SVQ) for encoding the PARCOR coefficients. The new scheme partitions the lattice filter into several stages and the PARCOR coefficients in each stage are calculated based on the principle of LPC analysis. Each stage operates on the residual of the previous stage as similar to the multi-stage structure. The structure of the new split VQ scheme also lends itself to easily incorporate a weighting mechanism so that the masking property of the human sound reception is considered during quantization. Simulation results show that the new scheme achieves lower quantization distortion than other split VQ schemes.\n",
    "Keywords: Split vector quantization, Weighted lattice\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-279"
  },
  "bruhn93_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Bruhn"
    ]
   ],
   "title": "A new approach to noiseless interframe coding of LPC parameters in vector quantizer applications",
   "original": "e93_1139",
   "page_count": 4,
   "order": 285,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "Efficient encoding of LPC parameters plays an essential role in low-rate speech coding. Frequently the principles of vector quantization (VQ) are applied to line-spectral-frequency parameters (LSF), which represent the short-term power spectrum of the speech signal. For complexity reasons, ordinary full-search VQ generally does not allow an adequate representation of the parameters, therefore suboptimal product VQ methods are often used instead. One realization is the split VQ proposed by Paliwal and Atal accomplishing transparent encoding of LSF parameters at 24 bits/frame [1],[2]. This paper presents a new approach to entropy coding (EC) of LSF parameters introducing a relative index coding scheme (RIC). Combined with various VQ procedures it substantially reduces the bitrate by exploiting correlations between successive frames. In application to ordinary VQ of LSF Parameters the new scheme yields bit savings up to 3.8 bits/frame. Combined with split VQ transparent quantization is achieved at approx. 17.5 bits/frame. As RIC works as a postprocessing scheme for the VQ indices no additional distortion arises.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-280"
  },
  "svendsen93_eurospeech": {
   "authors": [
    [
     "Torbjörn",
     "Svendsen"
    ]
   ],
   "title": "Efficient quantization of speech spectral information",
   "original": "e93_1143",
   "page_count": 4,
   "order": 286,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "The transmission of the spectral information requires a major part of the total bit rate in today's medium-to-low bit rate speech coders. The speech spectrum is relatively smooth for a much longer period than the update rate of the spectral information for many speech sounds. A method for utilizing this is by variable frame rate segment quantization which as a first step identifies steady state portions of the speech signal and then represents each steady state segment by a simple approximation. In the present paper we show that segment quantization can be applied to reduce the bit rate necessary for transmitting the speech spectral information by a factor of two without compromising the total spectral distortion. As an example, using a simple scalar quantizer with 40 bits/segment, an average bit rate of 22.6 bits/frame resulted in a average spectral distortion of 1.06 dB. Using a more sophisticated quantizer allow for reducing the bit rate without increasing the spectral distortion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-281"
  },
  "feldes93_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Feldes"
    ]
   ],
   "title": "Enhancing robustness of coded LPC-spectra to channel errors by use of residual redundancy",
   "original": "e93_1147",
   "page_count": 4,
   "order": 287,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "In this paper a method is presented to enhance the performance of the error control scheme in speech transmission in a mobile radio system by the use of residual redundancy in the coded speech parameters. In particular the LPC spectral coefficients are considered here. The proposed scheme is located on the receiver side and consists of a combination of two estimators, a MLSE channel error correction unit with reliability output and a parameter estimator based on 'a-priori knowledge' about source statistic. The results in terms of objective measures and informal listening show that especially for very bad channel conditions significant improvements can be achieved and that computationally less complex variants yielded the best results.\n",
    "Keywords: Residual redundancy, LPC spectrum, Channel error correction, Softdecision Viterbi, Maximum Likelihood / MMSE - Estimation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-282"
  },
  "atungsiri93_eurospeech": {
   "authors": [
    [
     "S. A.",
     "Atungsiri"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Multi-rate source and channel coding for mobile communication systems",
   "original": "e93_1151",
   "page_count": 4,
   "order": 288,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "The mobile and personal communication systems (PCS) revolution is well underway [l]. Spectral efficiency, a major requirement of these systems, means the use of low bit rate (LBR) speech and channel coders with low frequency reuse factors. Rampant frequency reuse however, leads to high levels of co-channel interference which result in bursty errors in the base-band, thus degrading speech quality. This situation is worsened by the constraint on the redundancy for channel coding. In this paper, we propose the use of multi-rate speech and channel codecs for future PCS. In the base-band of such systems, a balance between the source and channel coding bit rate is struck based on a quantitative estimate of the prevailing channel conditions. This idea can either be used to increase the capacity of interference limited CDMA systems (with lower speech quality) or maintain an average speech quality over a wider range of channel degradations.\n",
    "Keywords: speech and channel coding, spread spectrum, mobile communications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-283"
  },
  "moriya93_eurospeech": {
   "authors": [
    [
     "Takehiro",
     "Moriya"
    ],
    [
     "Satoshi",
     "Miki"
    ],
    [
     "Kazunori",
     "Mano"
    ],
    [
     "Hitoshi",
     "Ohmuro"
    ]
   ],
   "title": "Training method of the excitation codebook for CELP",
   "original": "e93_1155",
   "page_count": 4,
   "order": 289,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "An iterative training method is proposed for the design of random codebooks and channel code mapping tables for a CELP coder. The method is based on a structured excitation source such as a conjugate structure, pitch isynchronization, rotation of samples, time-domain sloped gain, and code mapping to channel code. Closed-loop distortion measure, that is, perceptually weighted distortion between the input and the synthesized speech, is used in all training procedures. The training improves the SNR by around 1 dB and improves subjective quality. The channel code mapping and channel-matched distortion measure increases robustness against channel error.\n",
    "Keywords: codebook, CELP, training, code mapping\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-284"
  },
  "bruce93_eurospeech": {
   "authors": [
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Phrasing strategies in prosodic parsing and speech synthesis",
   "original": "e93_1205",
   "page_count": 4,
   "order": 290,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "This paper reports on some experiments and results from a research project called Prosodic Phrasing in Swedish in which the overall aim has been to investigate and model prosodic aspects of phrasing. A series of prosodic parsing experiments are reported where a prosody expert was given the task of identifying prosodic phrases solely on the basis of a visual representation of unknown spoken text passages showing the waveform, intensity and fundamental frequency. The results were then compared to two independent, auditively based transcriptions of the readings. The comparison demonstrated a close relationship between the two types of judgments, indicating that a rather reduced acoustic representation can serve as the basis for prosodic parsing. Discrepancies encountered were often the result of an interrelationship between phrase-boundary gestures and accentual gestures. Preliminary guidelines for the prosodic parsing of phrasing are proposed. The dependence between phrasing and accentuation is further explored in a speech synthesis framework, and the influence of focal accentuation on the phrasing impression is discussed\n",
    "Keywords: Prosody, Phrasing, Parsing, Synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-285"
  },
  "strangert93_eurospeech": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ],
    [
     "Bo",
     "Strangert"
    ]
   ],
   "title": "Prosody in the perception of syntactic boundaries",
   "original": "e93_1209",
   "page_count": 2,
   "order": 291,
   "p1": "1209",
   "pn": "1210",
   "abstract": [
    "Previous experiments on Swedish indicate (a) that it is possible to differentiate between syntactic boundaries on the basis of prosodic cues alone, (b) that the best results are obtained when pre- and post-boundary information is combined with information about the boundary itself (the silent interval), (c) that a fairly good categorization may be based exclusively on pre-boundary cues and (d) that, for the majority of the subjects tested, the silent interval appears to be the stronger cue when in conflict with other prosodic cues. The experiments to be reported are an extension of this work. A new test series has been designed applying the signal detection theory in order to explore in more detail how pre-boundary information combines with information about the boundary. Two tasks are compared: categorical judgments of type of boundary, and perceptual responses to the semantic information of the sequences.\n",
    "Keywords: Perception, prosody, syntactic boundaries, signal detection theory, Swedish.\n",
    ""
   ]
  },
  "pijper93_eurospeech": {
   "authors": [
    [
     "Jan Roelof de",
     "Pijper"
    ],
    [
     "Angelien",
     "Sanderman"
    ]
   ],
   "title": "Prosodic cues to the perception of constituent boundaries",
   "original": "e93_1210",
   "page_count": 5,
   "order": 292,
   "p1": "1210",
   "pn": "1214",
   "abstract": [
    "This article is concerned with the relationship between the strength of prosodic boundaries in spoken utterances as perceived by untrained listeners and the phonetic cues melodic discontinuity, pause and declination reset. It is shown that boundary strength can be approached and measured as a perceptual variable in its own right. Clear relationships are found between perceptual boundary strength and both phonetic events and predicted prosodic boundary levels.\n",
    "Keywords: prosody, intonation, pausing, boundary strength\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-286"
  },
  "grabe93_eurospeech": {
   "authors": [
    [
     "Esther",
     "Grabe"
    ],
    [
     "Tara",
     "Hoist"
    ],
    [
     "Francis",
     "Nolan"
    ],
    [
     "Paul",
     "Warren"
    ]
   ],
   "title": "Acoustic cues to syntactic structure - evidence from prosodic and segmental effects",
   "original": "e93_1215",
   "page_count": 4,
   "order": 293,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "This paper presents an experimental study of the acoustic and perceptual correlates of syntactic structure in connected speech, conducted as part of a project investigating the use of prosodic and segmental processes in the real-time parsing of speech*. Evidence comes from auditory and acoustic analyses of assimilation and stress shift in speech production, and from an on-line comprehension experiment investigating the role of stress shift as a cohesion cue. Our results suggest that assimilation and stress shift processes are probabilistic rather than deterministic cues to structure.\n",
    "Keywords: speech comprehension, cohesion cue, assimilation, stress shift, prosody\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-287"
  },
  "beaugendre93_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Beaugendre"
    ],
    [
     "Anne",
     "Lacheret-Dujour"
    ]
   ],
   "title": "Automatic generation of French intonation based on a perceptual study and morpho-syntactic information",
   "original": "e93_1219",
   "page_count": 4,
   "order": 294,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "Keywords: prosody, text-to speech synthesis, syntax. The goal of our communication is to present a set of intonosyntactic rules which has been elaborated according to phonetic and linguistic constraints and to be used in the text-to-speech synthesis system developed at LIMSI. The first stage of this work - an experimental phonetic approach of French intonation - is described in [Beaugendre 92]. We will focus this communication on the linguistic aspects of the French intonation and there acoustic correlates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-288"
  },
  "zahorian93_eurospeech": {
   "authors": [
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Zaki B.",
     "Nossair"
    ],
    [
     "Claude A.",
     "Norton III"
    ]
   ],
   "title": "A partitioned neural network approach for vowel classification using smoothed time/frequency features",
   "original": "e93_1225",
   "page_count": 4,
   "order": 295,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "A technique is described for extracting spectral/temporal features from speech segments such that more emphasis is given to the center of the segment and less to the end regions. A classification technique, called binary-pair partitioning (BPP), is also described. This method partitions an N-way classification task using N*(N-l)/2 elemental classifiers, each of which discriminates one pair of categories. These features and this classification technique resulted in 72.4% accuracy for classification of 16 vowels extracted from the DARPA/TIMIT data base in speaker-independent experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-289"
  },
  "kitamura93_eurospeech": {
   "authors": [
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Speaker-independent 100 word recognition using dynamic spectral features of speech and a neural network",
   "original": "e93_1229",
   "page_count": 4,
   "order": 296,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "This paper describes 100 word recognition using dynamic spectral features of speech and a neural network. Spectral features of speech are obtained by a Two-Dimensional Mel-Cepstrum (TDMC). TDMC is defined as the two-dimensional Fourier transform of mel-frequency scaled logarithm spectra in the frequency and time domains. A neural network has a three-layered feedforward network and it learns automatically using a back-propagation learning algorithm. In this studyt word recognition experiments of 100 Japanese city names uttered by 7 male speakers were carried out using smoothed dynamic and average spectral features of TDMC. The number of necessary elements of TDMC was studied. Experimental results have shown that this method gives the recognition accuracy of 98.1% for speaker-dependent 100 word recognition for multi-speaker and 94.8% for speaker-independent 100 word recognition, respectively. Keyword: Dynamic Spectral Features of Speech, Neural Network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-290"
  },
  "zhu93_eurospeech": {
   "authors": [
    [
     "Ming",
     "Zhu"
    ],
    [
     "Klaus",
     "Fellbaum"
    ]
   ],
   "title": "Speaker independent isolated word recognition using vector quantization and neural networks",
   "original": "e93_1233",
   "page_count": 4,
   "order": 297,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In this paper we present an experimental system for speaker independent isolated word recognition that is based on vector quantization and multilayer perceptron networks. Taking advantages of both supervised and unsupervised learning, we explored the system performance for generalizing with limited training data. Trained with code words, two groups of networks are built to classify static and dynamic feature vectors respectively, and within each group several networks with the same structure classify vector quantized sequences arranged in the order of minimal distortions. Each network makes its own weighted contribution to the final decision. Because these networks can be trained separately and each network is relatively small, the capacity of the whole network can be effectively extended and the performance of the system can be improved without an increased computational effort.\n",
    "Keywords: speaker independent speech recognition, isolated word recognition, neural networks, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-291"
  },
  "elenius93_eurospeech": {
   "authors": [
    [
     "Kjell O. E.",
     "Elenius"
    ],
    [
     "Hans G. C.",
     "Traven"
    ]
   ],
   "title": "Multi-layer perceptrons and probabilistic neural networks for phoneme recognition",
   "original": "e93_1237",
   "page_count": 4,
   "order": 298,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "Two artificial neural networks have been trained to recognise phonemes in continuous speech: multi-layer perceptron (MLP) nets and probabilistic neural networks (PNN). The speech material was recorded by one male Swedish speaker and the sentences were phonetically labelled. Fifty sentences were used for training and another fifty were used for testing. Both networks had a single hidden layer and 38 output nodes corresponding to Swedish phonemes. The MLP was trained by the supervised back-propagation algorithm. The PNN was trained by a self-organising clustering algorithm, a stochastic approximation to the expectation maximisation algorithm. The classification results for a feed-forward MLP and the PNN were rather similar, but an MLP with simple recurrency using context nodes gave the best performance. Several other differences of practical value was noted.\n",
    "Keywords: phoneme recognition, carticulation, back-propagation, multi-layer perceptron, simple recurrency, probabilistic neural network, expectation maximisation, supervised/unsupervised training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-292"
  },
  "blackburn93_eurospeech": {
   "authors": [
    [
     "C. S.",
     "Blackburn"
    ],
    [
     "Julie P.",
     "Vonwiller"
    ],
    [
     "R. W.",
     "King"
    ]
   ],
   "title": "Automatic accent classification using artificial neural networks",
   "original": "e93_1241",
   "page_count": 4,
   "order": 299,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "We describe the development and performance of an automatic English accent classification system to discriminate between the speech of subjects whose first language is Arabic, Chinese and Australian English The system operates on continuous speech samples of arbitrary duration. The classification is performed in stages. Abroad phonetic class segmenter divides incoming speech into one of voiced, unvoiced, stop and energy dip. For each of these segment types an artificial neural network is used to classify the accent. The sequence of accent labels from these four networks is examined to obtain a cumulative measure of the accent classification. Tested on a small set of data the system correctly classifies accents as rapidly as a trained phonetician.\n",
    "Keywords: accent classification, automatic speech recognition, artificial neural networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-293"
  },
  "huckvale93_eurospeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "The benefits of tiered segmentation for the recognition of phonetic properties",
   "original": "e93_1473",
   "page_count": 4,
   "order": 300,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "The modelling and recognition of the phonetic composition of speech using a number of independent levels or tiers is contrasted with conventional linear phone models both theoretically and experimentally.\n",
    "Keywords: Phonetic recognition, speech recognition, non-linear phonology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-294"
  },
  "lubensky93_eurospeech": {
   "authors": [
    [
     "David M.",
     "Lubensky"
    ]
   ],
   "title": "Generalized context-dependent phone modeling using artificial neural networks",
   "original": "e93_1477",
   "page_count": 4,
   "order": 301,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "Generalized context-dependent sub word modeling is part of an effort to develop a robust speech recognition system for a variety of applications over the telephone network. In this paper we investigate two major issues: (1) linguistically motivated context-clustering to capture the similarity of contextual effects and reduce the number of context-dependent categories; (2) phone-specific Multi Layer Perceptron (MLP) structures where each phone is modeled by one or more network, and the number of outputs in each network is based on the number of left and right contexts occurring in a training database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-295"
  },
  "hild93_eurospeech": {
   "authors": [
    [
     "Hermann",
     "Hild"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Speaker-independent connected letter recognition with a multi-state time delay neural network",
   "original": "e93_1481",
   "page_count": 4,
   "order": 302,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "We present a Multi-State Time Delay Neural Network (MS-TDNN) for speaker-independent, connected letter recognition. Our MS-TDNN achieves 98.5/92.0% word accuracy on speaker dependent/independent English letter tasks[7, 8]. In this paper we will summarize several techniques to improve (a) continuous recognition performance, such as sentence level training, and (b) phonetic modeling, such as network architectures with \"internal speaker models\", allowing for \"tuning-in\" to new speakers. We also present results on our large and still growing new German Letter data base, containing over 40.000 letters continuously spelled by 55 speakers.\n",
    "Keywords: Spelled Letter Recognition, Speaker-Independence, MS-TDNN\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-296"
  },
  "bodenhausen93_eurospeech": {
   "authors": [
    [
     "Ulrich",
     "Bodenhausen"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Tuning by doing: flexibility through automatic structure optimization",
   "original": "e93_1485",
   "page_count": 4,
   "order": 303,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "The successful application of speech recognition systems to new domains greatly depends on the tuning of the architecture to the new task, especially if the amount of training data is small. In this paper we present 1.) an improved version of our Automatic Structure Optimization (ASO) algorithm that does this tuning automatically and 2.) a new Automatic Validation Analyzing Control System (AVACS) that is designed to detect poorly generalizing models as early as possible and to selectively change their learning and automatic structuring process. ASO and AVACS were applied to a Multi State Time Delay Neural Network and could improve the generalization performance of an already handtuned architecture from 85% to 92.3% on an alphabet recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-297"
  },
  "windheuser93_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Windheuser"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Phonetic features for spelled letter recognition with a time delay neural network",
   "original": "e93_1489",
   "page_count": 4,
   "order": 304,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "In this paper we describe a TDNN based hybrid word recognition system with a novel phoneme representation based on phonetic features. This new representation is more compact than the traditional 1-out-of-N phonetic representation and leads to a smaller network. In different experiments on a spelling letter database we show that the set of phonetic features has to be chosen carefully to achieve good results. We compare the new representation against the standard phoneme representation in the same experiment and show that the phonetic feature representation leads to better recognition results and more stable learning. With the phonetic feature representation we reached a word recognition rate on an independent test set of the spelled letter task of 96.1%.\n",
    "Keywords: Word recognition, time delay neural networks, phonetic features, hybrid systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-298"
  },
  "bappert93_eurospeech": {
   "authors": [
    [
     "Veronika",
     "Bappert"
    ],
    [
     "Matthias",
     "Jobst"
    ]
   ],
   "title": "Training of a time-delay neural network for speech recognition by solving stiff differential equations",
   "original": "e93_1493",
   "page_count": 4,
   "order": 305,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "Time-Delay Neural Networks for robust speech recognition are usually trained using the back-propagation learning procedure. Since this learning procedure contains well known disadvantages like occasional instability or no guarantee of convergence, etc., we propose an algorithm which expresses the back-propagation in terms of solving a system of ordinary stiff differential equations. Tests have shown that the problem concerning the training of the TDNN is mathematically stiff, so that a special procedure suggested by Gear for solving stiff differential equations could be applied. Instead of optimizing the weights of the network by using fixed steps of small size, the stability of the differential equation solver allows computational steps of variable sizes that still follow the direction of the gradient's steepest descent. For the purpose of training speech recognizers, this procedure does not necessarily result in an acceleration of the computation time but it ensures the convergence without an adjustment of the learning rate and independent of the initial weight values.\n",
    "Keywords: Time-Delay Neural Network, back-propagation, stiff differential equations, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-299"
  },
  "sagayama93_eurospeech": {
   "authors": [
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Jun-ichi",
     "Takami"
    ],
    [
     "Akito",
     "Nagai"
    ],
    [
     "Harald",
     "Singer"
    ],
    [
     "Kouichi",
     "Yamaguchi"
    ],
    [
     "Kzumi",
     "Ohkura"
    ],
    [
     "Kenji",
     "Kita"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "ATREUS: a speech recognition front-end for a speech translation system",
   "original": "e93_1287",
   "page_count": 4,
   "order": 306,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "This paper describes the continuous speech recognition subsystem \"ATREUS\" which is used as the speech input stage in the experimental speech translation system \"ASURA.\" The speech recognition algorithm is SSS-LR/VFS which consists of context-dependent phone models (HMnet), a generalized LR parser, and vector field smoothing for speaker / environment adaptation.\n",
    "Keywords: continuous speech recognition, context-dependent phone models, LR parser, speaker adaptation, speech translation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-300"
  },
  "morimoto93_eurospeech": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Toshihisa",
     "Tashiro"
    ],
    [
     "Masaaki",
     "Nagata"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "ATR's speech translation system: ASURA",
   "original": "e93_1291",
   "page_count": 4,
   "order": 307,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "ASURA is an experimental speech-to-speech translation system. It recognizes Japanese input speech, translates to English and German, and output synthesized voices. The main features concerning speech recognition, interface between speech recognition and language processing, and language translation are described. The performance evaluation of the system is reported, too. The result shows that the system has high recognition and translation abilities with fairly good efficiency. We had an international joint experiment on an Automatic Interpreting Telephone System connecting this system with other systems developed by Carnegie Mellon University and Siemens Corporation/Karlsruhe University. The result of the experiment is also reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-301"
  },
  "woszczyna93_eurospeech": {
   "authors": [
    [
     "Monika",
     "Woszczyna"
    ],
    [
     "N.",
     "Coccaro"
    ],
    [
     "A.",
     "Eisele"
    ],
    [
     "A.",
     "Lavie"
    ],
    [
     "A.",
     "McNair"
    ],
    [
     "T.",
     "Polzin"
    ],
    [
     "Ivica",
     "Rogina"
    ],
    [
     "C. P.",
     "Rose"
    ],
    [
     "Tilo",
     "Sloboda"
    ],
    [
     "M.",
     "Tomita"
    ],
    [
     "J.",
     "Tsutsumi"
    ],
    [
     "N.",
     "Aoki-Waibel"
    ],
    [
     "Alex",
     "Waibel"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Recent advances in JANUS: a speech translation system",
   "original": "e93_1295",
   "page_count": 4,
   "order": 308,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "We present recent advances from our efforts in increasing coverage, robustness, generality and speed of JANUS, CM IPs speech-to-speech translation system. JANUS is a speaker-independent system which translates spoken utterances in English and also in German into one of German, English or Japanese. The system has been designed around the task of conference registration (OR). It has initially been built based on a speech database of 12 read dialogs, encompassing a vocabulary of around 500 words. We have since been expanding the system along several dimensions to improve speed, robustness and coverage and to move toward spontaneous input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-302"
  },
  "rayner93_eurospeech": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Ivan",
     "Bretan"
    ],
    [
     "David",
     "Carter"
    ],
    [
     "Michael",
     "Collins"
    ],
    [
     "Vassilios",
     "Digalakis"
    ],
    [
     "Bjorn",
     "Gamback"
    ],
    [
     "Jaan",
     "Kaja"
    ],
    [
     "Jussi",
     "Karlgren"
    ],
    [
     "Bertil",
     "Lyberg"
    ],
    [
     "Stephen",
     "Pulman"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Christer",
     "Samuelsson"
    ]
   ],
   "title": "Spoken language translation with MID-90's technology: a case study",
   "original": "e93_1299",
   "page_count": 4,
   "order": 309,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "We describe the architecture of the Spoken Language Translator (SLT), a prototype speech translation system which can translate queries from spoken English to spoken Swedish in the domain of air travel information systems. Though the performance given the level of effort so far has been extremely encouraging, more work is needed to provide a technology that will support widespread applications. With this goal, we have developed techniques for rapid development and for evaluation. These techniques allow us to estimate the level of effort required to achieve higher levels of performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-303"
  },
  "hazen93_eurospeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Automatic language identification using a segment-based approach",
   "original": "e93_1303",
   "page_count": 4,
   "order": 310,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "A segment-based Automatic Language Identification (ALI) system has been developed. The system was designed around a formal probabilistic framework. This framework forms the basis for investigating the ALI approach proposed by House and Neuburg which utilizes phonotactic constraints of languages. The system incorporates different components which model the phonotactic, prosodic, and acoustic properties of the different languages used in the system. The system was trained and tested using the OGI Multi-Language Telephone Speech Corpus. An overall system performance of 47.7% was achieved in identifying the language of test utterances.\n",
    "Keywords: Automatic language identification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-304"
  },
  "muthusamy93_eurospeech": {
   "authors": [
    [
     "Yeshwant",
     "Muthusamy"
    ],
    [
     "Kay",
     "Berkling"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Ronald",
     "Cole"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "A comparison of approaches to automatic language identification using telephone speech",
   "original": "e93_1307",
   "page_count": 4,
   "order": 311,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "A variety of approaches to language identification, based on (a) acoustic features, (b) broad-category segmentation, and (c) fine phonetic classification, are introduced. These approaches are evaluated in terms of their ability to distinguish between English and Japanese utterances spoken over a telephone channel. It is found that the best performance (86.3 % accurate classification of utterances with a mean length of 13.4 sec) is obtained when fine phonetic features are employed. In addition, the results show the importance of discriminatory training rather than likelihood estimation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-305"
  },
  "cheng93_eurospeech": {
   "authors": [
    [
     "Ying",
     "Cheng"
    ],
    [
     "Yves",
     "Normandin"
    ],
    [
     "Paul",
     "Fortier"
    ]
   ],
   "title": "Integration of neural networks and robust parsers in natural language understanding",
   "original": "e93_1311",
   "page_count": 4,
   "order": 312,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "In this paper, we describe a natural language understanding system which focuses on spoken language and integrates a neural network classifier and robust parsers. With the help of classification, the application domain is divided into several subsets. Parsers are constructed for each subset and as a result, the complexity of grammar construction is much smaller than if a grammar for the whole application domain had to be constructed. Furthermore, by using a neural network, our goal was to take the advantage of its learning ability and robustness to noise. Although the system was implemented in a short time, it performed reasonably well in its first participation in a DARPA ATIS (Air Travel Information System) evaluation in Nov. 92. It was quite robust to speech recognition errors in particular.\n",
    "Keywords: neural network, parser, natural language understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-306"
  },
  "dauchy93_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Dauchy"
    ],
    [
     "Christophe",
     "Mignot"
    ],
    [
     "Claude",
     "Valot"
    ]
   ],
   "title": "Joint speech and gesture analysis some experimental results on multimodal interface",
   "original": "e93_1315",
   "page_count": 4,
   "order": 313,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "We present some results from an experiment in multimodal human-computer interaction, done under the \"Wizard of Oz\" paradigm. This experience is a first glimpse at the use of future multimodal interfaces by average users. The means of communication available to the subjects were natural language and 2D gesture through a touch panel. Two operators simulated this intelligent multimodal interface. The subjects tended to make use of the flexibility of the machine in order to choose a comfortable type of expression. Further results should influence the design of vocal recognition systems based on spontaneous speech combined with gesture.\n",
    "Keywords: spoken dialogue, human factors, natural language processing, multimodal interfaces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-307"
  },
  "hirose93_eurospeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Yasuharu",
     "Asano"
    ]
   ],
   "title": "Generation of speech reply in the speech response system",
   "original": "e93_1319",
   "page_count": 4,
   "order": 314,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "A method was developed for generating speech responses, and was realized on a computer as a subsystem for the speech response system of questions and answers. This subsystem consists of two major parts; those for dialogue processing and dialogue speech synthesis. The former part manages the dialogue flow and generates the content of response in the form of semantic representation. In order to increase the adaptability to dialogue topics, topic-dependent rules of example-based description were introduced. The latter part first generates surface sentences for the response content. In order to increase the processing efficiency and the quality of output speech, the surface sentences are represented by phonological symbols which may have direct correspondence with the pronunciation for the sentences. The response speech is then generated using the speech synthesizer basically developed for the text-to-speech conversion system. Experiments were conducted on the response speech generation for the topic of \"ski areas\" and the results showed the validity of the developed method.\n",
    "Keywords: Dialogue Processing, Response Content Generation, Speech Synthesis, Prosodic Features, Focus\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-308"
  },
  "dermatas93_eurospeech": {
   "authors": [
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A fast multilingual probabilistic tagger",
   "original": "e93_1323",
   "page_count": 4,
   "order": 315,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "This paper presents and compares two versions of a novel automatic tagging system which is both language and tagset independent and has close to real-time response in personal computers. The system's prediction model is based on the HMMchain theory and tags each word of a text, which includes also unknown words, using the Viterbi algorithm. The first version carries out floating-point arithmetic operations while the second version these operations have been transformed to fixed-point ones. Thus a significant time response reduction is achieved with negligible influence ( <0.01%) on the prediction accuracy. The tagging system was tested on newspaper texts of 7 European languages using various sets of grammatical categories and texts with and without unknown words. The results proved to be satisfactory.\n",
    "Keywords: Probabilistic tagging, taggers, Viterbi algorithm, HMM, natural language processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-309"
  },
  "murakami93_eurospeech": {
   "authors": [
    [
     "Jin'ichi",
     "Murakami"
    ],
    [
     "Hiroki",
     "Yamatomo"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "The possibility for acquisition of statistical network grammar using ergodic HMM",
   "original": "e93_1327",
   "page_count": 4,
   "order": 316,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "This paper describes the use of a discrete Ergodic Hidden Markov Model (HMM) for automatic acquisition of a language model from a large amount of text data and discusses the possibility of using the HMM to acquire the concept of POS (part-of-speech). A discrete-output Ergodic HMMs has a similar structure to a stochastic network grammar (SNG), so automatic acquisition of an SNG from a large amount of text data is possible by using an Ergodic HMM and the Baum-Welch training procedure. In this model, the HMM state transition probabilities can be interpreted as the SNG transition probabilities. And the HMM state output probabilities will refrect the distributions of POSs (part-of-speeches). The results of experiments using text data show that a high similarity is found between high output probability words and the POS. This means that an Ergodic HMM has the ability to automatically acquire both an SNG and the concept of POS simultaneously from training text data. Experiments were also performed on sentence speech recognition. In these experiments, an Ergodic HMM outperformed the word bigram grammar for text-open data.\n",
    "Keywords: Language modeling, Stochastic network grammar, Ergodic HMM, Part-of-speech tagger\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-310"
  },
  "millien93_eurospeech": {
   "authors": [
    [
     "Evelyne",
     "Millien"
    ],
    [
     "Roland",
     "Kuhn"
    ]
   ],
   "title": "A robust analyzer for spoken language understanding",
   "original": "e93_1331",
   "page_count": 4,
   "order": 317,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper describes the CRIM Hybrid Analyzer for Natural Language, CHANEL, used as the NL component of CRIM ATIS (Air Travel Information System) system in the official DARPA November-92 evaluation. CHANEL is a robust analyzer with two components: 1) a parser based on Lexical Grammar formalism which analyzes important phrases in the sentence and finds their semantic values, and 2) a keyword classification tree (KCT) component which decides on the overall structure of the query. The system's input could be either text transcriptions of spoken sentences (NL test), or sentences output from the speech recognizer (SLS test). CHANEL converts the initial sentence into an intermediate semantic representation which is processed by another module to generate the final database query (in SQL) to obtain the information from the relational database.\n",
    "Keywords: speech understanding, robust parsing, lexical grammar, keyword classification trees, ATIS.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-311"
  },
  "dutton93_eurospeech": {
   "authors": [
    [
     "R. T.",
     "Dutton"
    ],
    [
     "John C.",
     "Foster"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "F. W.",
     "Stentiford"
    ]
   ],
   "title": "Identifying usability attributes of automated telephone services",
   "original": "e93_1335",
   "page_count": 4,
   "order": 318,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "This paper reports on research involving a series of large-scale field experiments using a new Wizard of Oz (WOZ) system for the investigation of users' attitudes towards automated telephone services. The paper focuses on the identification of a service's usability attributes, those features and characteristics of a system which influence the effectiveness, efficiency and satisfaction with which specified users can achieve specified goals in a particular environment. The attributes which are found to be most salient are used to form the content of a questionnaire designed to measure attitudes towards usability of automated telephone services. Experimental results derived by using such a questionnaire are discussed.\n",
    "Keywords: usability attributes, automated telephone services, speech interfaces, Wizard of Oz, questionnaires.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-312"
  },
  "hunt93_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Hunt"
    ]
   ],
   "title": "Utilising prosody to perform syntactic disambiguation",
   "original": "e93_1339",
   "page_count": 4,
   "order": 319,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "The syntactic andprosodic structures of spoken utterances are known to be related, although the relationship is difficult to define. This paper presents a novel method for modelling prosodic breaks from syntactic information obtained from an unconventional parser, the \"Link Parser\". The model is used to score candidate syntactic structures based on prosodic breaks. The technique obtains 88% accuracy in selecting an appropriate syntactic parse using hand labels, which compares well with 84% for human subjects.\n",
    "Keywords: syntax, prosody, disambiguation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-313"
  },
  "hiller93_eurospeech": {
   "authors": [
    [
     "Steven",
     "Hiller"
    ],
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Jean-Paul",
     "Leffevre"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Spell: an automated system for computer-aided pronunciation teaching",
   "original": "e93_1343",
   "page_count": 4,
   "order": 320,
   "p1": "1343",
   "pn": "1346",
   "abstract": [
    "This paper describes the application of speech technology in a workstation to improve the pronunciation of foreign language students. The SPELL workstation uses techniques of speech analysis to assess and improve learners1 pronunciation in modules for teaching consonant production, vowel quality, rhythm and intonation, in three European languages (English, French and Italian). Each teaching module is discussed in terms of its phonetic basis, the implementation of its analysis modules and a description of the associated graphic user interface.\n",
    "Keywords: Pronunciation, Speech Analysis, Speech Technology, Consonants, Vowels, Rhythm, Intonation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-314"
  },
  "rooney93b_eurospeech": {
   "authors": [
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Rebecca",
     "Vaughan"
    ],
    [
     "Steven",
     "Hiller"
    ],
    [
     "Fabrizio",
     "Carraro"
    ],
    [
     "John",
     "Laver"
    ]
   ],
   "title": "Training vowel pronunciation using a computer-aided teaching system",
   "original": "e93_1347",
   "page_count": 4,
   "order": 321,
   "p1": "1347",
   "pn": "1350",
   "abstract": [
    "This paper describes the treatment of vowels within SPELL, a computer-aided system for teaching pronunciation to foreign language learners. Vowel features in English, French and Italian are taught using an analysis based on formant extraction and between-speaker normalization. Graphical feedback shows the relationship between students' tokens and ideal vowel 'targets', constructed from native speaker data. Further work is planned to develop analysis techniques and feedback utilities for French nasal vowels, French front rounded vowels and the diphthongs of English and Italian.\n",
    "Keywords: Pronunciation Training, Phonetics, Vowel, Teaching Aids, Speech Technology\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-315"
  },
  "zajicek93_eurospeech": {
   "authors": [
    [
     "Mary",
     "Zajicek"
    ],
    [
     "Ken",
     "Brownsey"
    ]
   ],
   "title": "Methods for traversing a pre-recorded speech message network to optimise dialogue in telephone answering systems",
   "original": "e93_1351",
   "page_count": 4,
   "order": 322,
   "p1": "1351",
   "pn": "1354",
   "abstract": [
    "This paper addresses problems associated with conversations with computerised telephone answering systems. Using these systems, we often find ourselves listening to lengthy menus, forgetting the number of our desired option, choosing the wrong one and being unable to get back without hanging up completely, and wasting valuable time and money listening to inappropriate messages. In addition, user acceptance of this form of interaction is a significant problem[l]t and user's correct anticipation of what is required of them is difficult to manipulate [2], [3] This paper describes an approach to a dialogue enhancing techniques prototyped by the authors. Our aim was a) to make dialogues more flexible b) reduce the number of messages that needed to be listened to and c) reduce the number of unsatisfactory outcomes, i.e. when users do not achieve the objective of the phone call Most telephone answering systems comprise of a hierarchical menu system made up of pre-recorded speech messages which are accessed by the caller using a small speaker independent, vocabulary of words[4]. The dialogue structure can be viewed as a network of messages which must be traversed in order to reach action nodes which will enable the caller to either a) retrieve an item of pre-recorded information b) leave a message on an answering machine, or c) talk to the relevant human operator. In a standard hierarchical menu system - see FIG(1), network traversal is one directional, and between adjacent nodes. Our dialogue enhancing techniques enable the user to \"jump\" from one message node to another more appropriate one in a completely different part of the network. The techniques discussed are: - Simple backtracking - Heuristic relocation via keywords. - Direct access via keyword - Ternary choice\n",
    "Keywords: Cognitive Model, Dialogue, Heuristic, Network Traversal, Non-hierarchical, Speech, Usability, User Model\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-316"
  },
  "hanes93_eurospeech": {
   "authors": [
    [
     "Roger",
     "Hanes"
    ],
    [
     "Jo",
     "Salter"
    ],
    [
     "Paul",
     "Popay"
    ],
    [
     "Frances",
     "Hedley"
    ]
   ],
   "title": "Service creation tools for creating speech interactive services",
   "original": "e93_1355",
   "page_count": 4,
   "order": 323,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "This paper will describe service creation tools and discuss their benefits, using the BT VISAGE service creation tool as a specific example. This is described in some detail and is used currently within BT to generate advanced speech interactive network based services.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-317"
  },
  "hirschberg93b_eurospeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Deaccentuation and persistence of grammatical function and surface position",
   "original": "e93_1359",
   "page_count": 4,
   "order": 324,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "Deaccentuation is commonly explained as a consequence of the givenness of the information referred to. However, speakers often accent given information, so that a more complex explanation of deaccentuation is required. This work investigates the contribution of persistence of grammatical role and surface position to the occurrence of deaccentuation. We report a production experiment in which these features were varied systematically. The results indicated that persistence of grammatical role and surface position both contributed to deaccentuation.\n",
    "Keywords: repairs, disfluencies\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-318"
  },
  "euler93_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Euler"
    ],
    [
     "K.",
     "Riedel"
    ]
   ],
   "title": "Design and implementation of a speech server for unix based multimedia applications",
   "original": "e93_1363",
   "page_count": 4,
   "order": 325,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "In this paper we describe a general purpose speech recognition server (SRS) that provides a standard interface between applications and speech recognition modules. The recognition modules cover different techniques such as speaker dependent or independent, isolated or connected word recognition. The SRS is designed mainly for multimedia applications running on a network of UNIX workstations. Our concept uses multiple processes for the different tasks and UNIX interprocess communication techniques.\n",
    "Keywords: speech recognition, server, interprocess com- munication\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-319"
  },
  "goodine93_eurospeech": {
   "authors": [
    [
     "David",
     "Goodine"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Romaine: a lattice based approach to lexical access",
   "original": "e93_1367",
   "page_count": 4,
   "order": 326,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "This paper describes Romaine, a lexical access model designed to address the potential shortcomings of some lexical access approaches, namely, flat lexical representation and left-to-right search. The key ideas embodied in Romaine are a compact hierarchical representation of the lexicon, and a bottom-up, island-driven control strategy that clusters constituent hypotheses into aggregate sequences. We also discuss the results of a set of simulation experiments designed to help us understand the performance of the system as vocabulary size grows from 200 to 20,000 words. These results suggests that Romaine is computationally very efficient, and that the accuracy remains reasonable when segmentation and classifications errors are introduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-320"
  },
  "albina93_eurospeech": {
   "authors": [
    [
     "Toffee A.",
     "Albina"
    ],
    [
     "Erica G.",
     "Bernstein"
    ],
    [
     "David M.",
     "Goblirsch"
    ],
    [
     "Douglas E.",
     "Lake"
    ]
   ],
   "title": "A system for clustering spoken documents",
   "original": "e93_1371",
   "page_count": 4,
   "order": 327,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "The speech clustering system presented in this paper organizes a database of spoken documents (e.g., audio recordings of newspaper articles) according to topic, without the need for a priori knowledge of subject matter. Documents are represented by a histogram of acoustic features. Document histograms are compared with each other using a standardized similarity measure. Standard clustering techniques are employed to organize documents into clusters. This clustering approach is based on a system in use in the text processing community.\n",
    "Keywords: Automatic topic classification of spoken documents, n-gram analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-321"
  },
  "vergeynst93_eurospeech": {
   "authors": [
    [
     "Nathalie A.",
     "Vergeynst"
    ],
    [
     "Keith",
     "Edwards"
    ],
    [
     "John C.",
     "Foster"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Spoken dialogues for human-computer interaction over the telephone: complexity measures",
   "original": "e93_1415",
   "page_count": 4,
   "order": 328,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "This paper considers measures of the complexity of human-computer spoken dialogues, and describes how these measures can be applied in dialogue engineering for automated telephone services. The paper highlights fundamental concepts involved in measuring the complexity of spoken telephone dialogues and develops a design evaluation approach based on a set of measures of dialogue complexity at a hierarchy of linguistic levels, focusing on measures of information in spoken dialogues. The measures described are applied to dialogues which have been engineered for use in automated telephone services for catalogue ordering and credit card payment. Aspects of the dialogue component of this telephone speech interface are described and measures of information complexity are used to evaluate the dialogues for alternative designs of spoken user interfaces. The results of the analysis are presented in detail.\n",
    "Keywords: spoken dialogues, dialogue engineering, automated telephone service, complexity measures\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-322"
  },
  "hirschman93_eurospeech": {
   "authors": [
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "Christine",
     "Pao"
    ]
   ],
   "title": "The cost of errors in a spoken language system",
   "original": "e93_1419",
   "page_count": 4,
   "order": 329,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "Spoken language interfaces are rapidly improving in both speed and accuracy. However, miscommunication between system and user still occurs, with the potential for disrupting user/system interchange. This paper examines what happens when such miscommunication occurs: how quickly the user attempts to correct the system's non-understanding, what effect this has on subsequent communication, and how \"expensive\" miscommunication is in terms of solving a problem. We present a framework for analyzing detection and correction of system errors. Based on data from several experiments, we determined that in over 85% of the cases, the user detected a system misunderstanding in the next turn. For two systems with different overall performance (21% vs. 35% sentence understanding error) but identical response generation capabilities, errors took an average of 1.25 and 1.33 turns to detect, respectively; for a system with modified interaction and feedback strategies (and understanding error of 25%) the average dropped to 1.19 turns. This reduction in error detection time indicates that this measure is sensitive to aspects of the system/user interface beyond simple sentence understanding error.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-323"
  },
  "simpson93_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Simpson"
    ],
    [
     "Norman M.",
     "Eraser"
    ]
   ],
   "title": "Black box and glass box evaluation of the SUNDIAL system",
   "original": "e93_1423",
   "page_count": 4,
   "order": 330,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "The field of dialogue evaluation is still in a very early stage of development. This paper surveys relevant work and outlines the approach to evaluation developed in the SUNDIAL project. This evaluates a system in terms of a battery of metrics, divided between those which treat the system as a black box and those which look inside at parts of it (as though it were a glass box). Some of these metrics require the application of subjective judgement, so they can not be fully automated. We argue that this is a reasonable price to pay for a well-rounded evaluation of a spoken dialogue system.\n",
    "Keywords: Spoken dialogue systems, Evaluation, Black box metrics, Glass box metrics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-324"
  },
  "delogu93_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Ciro",
     "Sementina"
    ],
    [
     "Silvia",
     "Stecconi"
    ]
   ],
   "title": "A methodology for evaluating human-machine spoken language interaction",
   "original": "e93_1427",
   "page_count": 4,
   "order": 331,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "This paper reports an experiment for evaluating human-machine spoken language interaction. A phone-directory with information on Fondazione Bordoni employees were accessed by 54 users. The system was composed of a telephone interface; a simulated speech recognition system (Wizard of Oz); a database with the information on employees; a natural language processing module; a response generator; a text-to-speech synthesizer. Three different levels of evaluation have been identified: an overall evaluation of the user-system interaction, an user's performance evaluation, and a system's evaluation. Quantitative results and qualitative observations have been reported. Different modalities of generation (natural vs automatic) and test repetition (Testl vs Test2) were two dimensions by means of which the three kinds of evaluations have been performed.\n",
    "Keywords: Spoken Language Systems evaluation; dialogue understanding; Wizard of Oz simulation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-325"
  },
  "morin93_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Morin"
    ],
    [
     "Jean-claude",
     "Junqua"
    ]
   ],
   "title": "Error correction and ambiguity resolution in multimodal man-machine dialogue",
   "original": "e93_1431",
   "page_count": 4,
   "order": 332,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "Robustness has become a major concern in the realm of Man-Machine communication. The integration of text and speech input and output channels with applications responds to a growing need for user-friendly interfaces. How these interfaces will be able to master the various human factors involved and overcome recognition errors and ambiguities is a crucial issue in their application. Neglecting these new challenges will drastically limit the acceptance of such systems. In this paper we focus on the multimodal man-machine dialogue system PARTNER and present the typology of errors and ambiguities addressed in the system. Finally we discuss our ideas on error avoidance as a necessary complement to error management and present results obtained in the case of a graphic object manipulation tool application.\n",
    "Keywords: Error recovery, Robustness, Multimodal Dialogue Constraints in the recording of speech data bases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-326"
  },
  "castaing93_eurospeech": {
   "authors": [
    [
     "Marie-Franoise",
     "Castaing"
    ],
    [
     "Dominique",
     "True-Martini"
    ]
   ],
   "title": "Analysis of the speaker and operator behaviours",
   "original": "e93_1437",
   "page_count": 3,
   "order": 333,
   "p1": "1437",
   "pn": "1439",
   "abstract": [
    "BREF and ICY, large speech data bases, have been designed and recorded in the LIMSI (Laboratoire d'Informatique et de Mecanique pour les Sciences de Tlngenieur). The facilities for recording were built in the laboratory for a hundred speakers in artificial conditions : sound proofed room for the best recording quality, reading on a screen, instructions to be followed etc. In these situations, we have observed the influence of other parameters in addition to those taken in our initial hypothesis. The motivation is one of them and it is relevant in the context of the experimentations. In BREF, we observed the behaviour of the operator (guidance and control) and of the speakers: reading, pronouncing and tiring. The speakers read aloud the newspaper style texts on a screen. We took into account the chronological complexity of the texts to read and the familiarisation to the instructions together with the speaker being tired. In spite of indirect human-human communication, the results were excellent because the speakers were hired for this purpose. This consideration is examined.\n",
    "Keywords: vocal data bases, ergonomics, behavioural sciences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-327"
  },
  "ginestelmailland93_eurospeech": {
   "authors": [
    [
     "Alix de",
     "Ginestel-Mailland"
    ],
    [
     "Martine de",
     "Calmes"
    ],
    [
     "Guy",
     "Perennou"
    ]
   ],
   "title": "Multi-level transcription of speech corpora from orthographic forms",
   "original": "e93_1441",
   "page_count": 4,
   "order": 334,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "In order to translate phonological phenomena into speech-decoding systems, we have introduced a phonological model. It's based on contextual phonological groups (cpg's) and multi-pronunciation groups (mpg's). An interesting feature, with such a model, consists in allowing an automatic adaptation of the phonological component to suit a transcription of a corpus read aloud. Thus, the phonology involved can be tailed to a particular speaker or group of speakers. The purpose of this paper is both to describe the bases of the phonological model, and to present the system intended for automatic adaptation of the phonological component\n",
    "Keywords: Speech Recognition - Phonology -Automatic alignment - Learning\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-328"
  },
  "ljolje93b_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Michael D.",
     "Riley"
    ]
   ],
   "title": "Automatic segmentation of speech for TTS",
   "original": "e93_1445",
   "page_count": 4,
   "order": 335,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "Most computer text-to-speech systems are based on a large speech database of a single speaker which is fully segmented. Here we investigate an approach for automatic segmentation of speech using a probabilistic speech model which is normally used in speech recognition. The model is based on hidden Markov models (HMMs) and it can be used to provide phone boundary locations given speech and phone sequences. Results obtained on a phonetically rich set of 50 sentences, using a model trained on speech by the same speaker, are much better than the results obtained on a speaker independent task [1]. More than 80% of the automatically placed boundaries are within 11.5 ms of the boundary locations selected by a human segmenter.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-329"
  },
  "boeffard93_eurospeech": {
   "authors": [
    [
     "O.",
     "Boeffard"
    ],
    [
     "B.",
     "Cherbonnel"
    ],
    [
     "F.",
     "Emerard"
    ],
    [
     "S.",
     "White"
    ]
   ],
   "title": "Automatic segmentation and quality evaluation of speech unit inventories for concatenation-based, multilingual PSOLA text-to-speech systems",
   "original": "e93_1449",
   "page_count": 4,
   "order": 336,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "The quality of the synthetic speech produced by concatenation-based PSOLA text-to-speech systems depends on two main factors : the richness and adequacy of the prosodic contours which are generated and the choice of the speech units to be concatenated. This paper addresses the second issue : the building, segmentation and evaluation of high-quality speech unit inventories in several languages. The standard CNET speech unit inventories for French, Spanish and German contain diphones and a supplementary set of triphones and quadriphones which include highly coarticulated sounds. These three inventories were segmented in two ways : manually by phoneticians and automatically an HMM-based segmentation procedure. For the three languages, 85% of the segmentation marks found by the automatic method were correct when compared to those found by the manual method. A global quality test comparing the two versions shows that, except for German, subjective listener preference ratings are not significantly different.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-330"
  },
  "coile93_eurospeech": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "On the development of pronunciation rules for text-to-speech synthesis",
   "original": "e93_1455",
   "page_count": 4,
   "order": 337,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "This paper describes several aspects of the development of grapheme-to-phoneme systems for 6 different languages. We report on the lexical data bases we are using for these developments. Our method for the automatic alignment of orthographic and phonetic transcriptions is reviewed. We describe how the method can be used to facilitate the development of pronunciation rules. Performances are given for word data and name data. In the last part of the paper we report on some orthographic/phonetic properties of the different languages using the concepts of entropy and mutual information.\n",
    "Keywords: text-to-speech, lexical data bases, grapheme-to-phoneme conversion\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-331"
  },
  "daelemans93_eurospeech": {
   "authors": [
    [
     "Walter",
     "Daelemans"
    ],
    [
     "Antal van den",
     "Bosch"
    ]
   ],
   "title": "Tabtalk: reusability in data-oriented grapheme-to-phoneme conversion",
   "original": "e93_1459",
   "page_count": 4,
   "order": 338,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "In the traditional (knowledge-based) approach to the design of grapheme-to-phoneme modules in text-to-speech systems, it is claimed that various explicitly coded, language-specific, linguistic knowledge sources are necessary for a good performance. Due to knowledge acquisition bottlenecks, this implies long development cycles. As an alternative, we propose to use inductive methods from machine learning in a simple combined Trie Search and Similarity-Based Reasoning approach and show that, for Dutch, its performance is better than that of the knowledge-based approach and backpropagation learning. Furthermore, we show that our approach is reusable for any language for which a training corpus exists.\n",
    "Keywords: grapheme-to-phoneme conversion, text-to-speech, trie search, similarity-based reasoning, machine learning\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-332"
  },
  "lindstrom93_eurospeech": {
   "authors": [
    [
     "Anders",
     "Lindström"
    ],
    [
     "Mats",
     "Ljungqvist"
    ],
    [
     "Kjell",
     "Gustafson"
    ]
   ],
   "title": "A modular architecture supporting multiple hypotheses for conversion of text to phonetic and linguistic entities",
   "original": "e93_1463",
   "page_count": 4,
   "order": 339,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "In this communication we devise a distributed modular scheme for organizing the different types of knowledge needed in the first phase of text-to-speech conversion, namely the conversion of the input text to a symbolic notation, representing phonetic transcription together with syntactic, semantic and pragmatic information. We argue that the proposed scheme will provide for easier formulation and maintenance of knowledge, and that its modularity will also simplify the implementation of different modalities, e.g. various reading modes.\n",
    "Keywords: Text-to-Speech Conversion, TTS, speech synthesis, knowledge representation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-333"
  },
  "iles93_eurospeech": {
   "authors": [
    [
     "Jon",
     "Iles"
    ],
    [
     "William",
     "Edmondson"
    ]
   ],
   "title": "The use of a non-linear model for text-to-speech conversion",
   "original": "e93_1467",
   "page_count": 4,
   "order": 340,
   "p1": "1467",
   "pn": "1470",
   "abstract": [
    "We have attempted to apply modern linguistic theory, specifically non-linear phonology, to the task of text-to-speech conversion. We believe that approaches to text-to-speech conversion based on such linguistic theory offer greater possibility of improved naturalness in synthetic speech. This improvement is made possible by the ability of non-linear techniques to represent accurately the complex structures underlying natural speech, and also to supply an element of cognitive modelling to the task. This paper presents an outline description of the non-linear framework around which the proposed text-to-speech conversion system is to be constructed and the feature driven formant synthesis technique underlying the implementation.\n",
    "Keywords: Non-linear, non-segmental, speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-334"
  },
  "wieringen93_eurospeech": {
   "authors": [
    [
     "Astrid van",
     "Wieringen"
    ],
    [
     "John K.",
     "Cullen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The perceptual relevance of CV- and VC- transitions in identifying stop consonants: cross-language results",
   "original": "e93_1499",
   "page_count": 4,
   "order": 341,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "The relative perceptual weight of the initial (CV) and final (VC) vowel transitions is examined by means of a psycho-acoustical experiment with tone glides as well as an identification experiment with speech samples. The first experiment shows that sensitivity is significantly greater for transitions in final (VC) than in initial (CV) position. This perceptual asymmetry could be the result of temporal backward masking of the steady-state frequencies on the preceding transition. Identification scores of speech transitions in initial and final position do not, however, show the VC speech transition to be more consonant specific than the CV one. Data indicate that although a recency effect may affect discrimination of tone glides, various acoustic cues are used to identify plosives in speech. Therefore, perception of Dutch transitions are comparable for American English and Dutch subjects, despite acoustic-linguistic differences.\n",
    "Keywords: CV and VC speech(-like) transitions\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-335"
  },
  "heuven93_eurospeech": {
   "authors": [
    [
     "Vincent J. van",
     "Heuven"
    ],
    [
     "Willy",
     "Jongenburger"
    ]
   ],
   "title": "Perceptual effects of place and voicing assimilation in dutch consonants",
   "original": "e93_1503",
   "page_count": 4,
   "order": 342,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "In this study we develop and test the hypothesis that spoken word recognition in context is facilitated by anticipatory assimilation but inhibited by perseveratory assimilation. Three experiments are presented. Experiment I shows that homorganic final nasals provide sufficient perceptual cues to reconstruct the place of articulation of the following stop which was electronically eliminated from the stimulus. In experiment II the target stops were not eliminated from the stimulus but presented - through digital cross-splicing - after homorganic versus heterorganic nasals. Removing anticipatory assimilation proved detrimental to the recognition of the target stops, thereby showing a positive effect of anticipatory assimilation on word recognition. In experiment III cross-splicing was used to simulate blocking of perseveratory assimilation of voicing. Here the results show that targets were recognised better when assimilation was blocked, showing that, indeed, perseveratory assimilation inhibits spoken word recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-336"
  },
  "ooyen93b_eurospeech": {
   "authors": [
    [
     "Brit van",
     "Ooyen"
    ]
   ],
   "title": "Detection of vowels and consonants by human listeners: effects of minimising auditory memory load",
   "original": "e93_1507",
   "page_count": 4,
   "order": 343,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "This paper presents a speech perception experiment with vowels and consonants as phoneme detection targets in real English words. Phoneme targets were the two vowels /a/ and /i/, plus the two unvoiced stop consonants, /p/ and /t/. In contrast with a previous study that used the same materials, listeners heard the target phoneme specification immediately prior to stimulus presentation. The results showed that the reaction times for the vowels were not significantly longer than those to the consonants. It is concluded that vowels benefit more than consonants from having target specification and stimulus presentation occur closely together. It is proposed that the time lag between phoneme type and stimulus token reduces the effect of inherent perceptual ambiguity of vowels during a speeded response task.\n",
    "Keywords: Human speech recognition, vowels, consonants, phoneme detection, reaction time, auditory memory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-337"
  },
  "bailly93_eurospeech": {
   "authors": [
    [
     "Gerard",
     "Bailly"
    ]
   ],
   "title": "Resonances as possible representation of speech in the auditory-to-articulatory transform",
   "original": "e93_1511",
   "page_count": 4,
   "order": 344,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "This article presents a characterisation offormant trajectories based on an active tracking of each resonance of the vocal tract. This tracking is necessary due to the presence of focal points i.e. changes of affiliations between F2 and F3 in most of transitions between back and front sounds. We show that this tracking can be easely implemented without any analysis-by-synthesis process by assuming a minimum elasticity principle (smoothness) and a monotonous relation between resonances associated with the same cavity. Our proposal is supported by intensive analysis of natural speech, articulatory simulations and perceptual experiments.\n",
    "Keywords: Perception, Formants, Vocal Tract, Reso- nances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-338"
  },
  "goedemans93_eurospeech": {
   "authors": [
    [
     "Rob",
     "Goedemans"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "A perceptual explanation of the weightlessness of the syllable onset",
   "original": "e93_1515",
   "page_count": 4,
   "order": 345,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "Quantity-sensitive stress languages typically stress the heaviest syllable in a word. Segments in the syllabic nucleus (i.e. the vowels) and in the coda (i.e. the post-vocalic consonants) contribute to syllable weight; pre-vocalic consonants (syllable onset) add no weight. This study presents a phonetic explanation for the weightlessness of the syllable onset. Both a speech production and a perception experiment were carried out On the assumption that syllable weight can be operationalised in terms of duration, the production data provide no support for the weightlessness of the onset added onset consonants contributed as much, if not more, to syllable duration than consonants added in the coda. However, the perceptual data provide a clear correlate of the weight difference between onset versus nucleus and coda. The results of a duration adjustment task indicate that variations in the onset consonant are not heard; identical variations in the coda consonant are reproduced adequately, whilst variations in the vowel are even perceptually overestimated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-339"
  },
  "bocchieri93_eurospeech": {
   "authors": [
    [
     "Enrico",
     "Bocchieri"
    ]
   ],
   "title": "A study of the beam-search algorithm for large vocabulary continuous speech recognition and methods for improved efficiency",
   "original": "e93_1521",
   "page_count": 4,
   "order": 346,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "This study concerns the computation of a Beam-Search, large vocabulary, continuous speech recognizer based on subword Continuous Observation Density Hidden Markov Models. To find the computational bottle necks, we perform recognition experiments on the standard Darpa Resource Management (RM) speech data base, and we measure the computational requirements of the variuous algorithm components, as functions of the Beam-Search beamwidth. It appears that the computation of the Beam-Search recognizer is dominated by the state likelihoods, not only when the word lexicon is defined in terms of context-dependent phone models, but also, more surprisingly, in the case of context-independent phones. The reason of this finding is that the number of likelihood computations is rather insensitive to the beamwidth reduction. Then, we improve the system efficiency by a factor of 6 (without significantly affecting its accuracy) with a combination of techniques aimed at reducing both the likelihood computation and the search.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-340"
  },
  "fissore93_eurospeech": {
   "authors": [
    [
     "L.",
     "Fissore"
    ],
    [
     "E.",
     "Giachin"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "P.",
     "Massafra"
    ]
   ],
   "title": "Using grammars in forward and backward search",
   "original": "e93_1525",
   "page_count": 4,
   "order": 347,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "This paper focuses on the use of linguistic constraints in continuous speech decoding. In particular, it aims at verifying the advantages of the delayed use of linguistic constraints with respect to their use during the forward search. We evaluate the accuracy loss of the best \"backward\" derived sentence with respect to the one obtained by forward decoding using grammars. A \"backward\" de- rived sentence is obtained growing a complete path starting from the end of the sentence using a time asynchronous A* search which exploits the word lattice scores computed during a classical forward search without grammar. Results are evaluated on a 718 word, speaker independent recognition task. Applying linguistic constraints during forward decoding gives slightly better results than delaying them to the backward search, but the difference is small, and it has to be traded with the advantage of having N solutions instead of just one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-341"
  },
  "fink93_eurospeech": {
   "authors": [
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ],
    [
     "Bernd",
     "Seestaedt"
    ]
   ],
   "title": "Robust interpretation of speech",
   "original": "e93_1529",
   "page_count": 4,
   "order": 348,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "In all fields of pattern recognition there may arise situations where severe distortions of the sensor data or errors of processing prevent a successful analysis. This is especially true in speech recognition. Therefore, a speech understanding system can not rely on being able to interpret the whole or even a given fixed percentage of the input data. Rather a dynamic criterion has to be applied to decide when the analysis has produced the best results obtainable from the input data. We propose an appropriate criterion and show how the requirements for robust interpretation of speech can be met in a dialog system.\n",
    "Keywords: Speech Understanding, Robustness, Search\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-342"
  },
  "hetherington93_eurospeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ],
    [
     "Michael S.",
     "Phillips"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A* word network search for continuous speech recognition",
   "original": "e93_1533",
   "page_count": 4,
   "order": 349,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "In this paper we describe an algorithm for generating word networks in a continuous speech recognition system. Recently, iV-best search strategies have gained considerable popularity and have been used for multi-stage searches including interfacing speech recognition and natural language systems as well as applying more computationally expensive constraints in later stages. However, examination of TV-best lists reveals significant overlap between different hypotheses, with differences typically localized to regions where the acoustic signal is not robust. In order to improve both computational and representational efficiencies, we have developed a word network search. This search is very similar to our A* iV-best search, but contains an additional path-merging step. The resulting word networks contain the same N complete hypotheses that are within a specified score threshold of the best complete score, but in a much smaller form that is faster to compute. These word networks can then be used as search spaces by subsequent search stages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-343"
  },
  "lacouture93_eurospeech": {
   "authors": [
    [
     "Roxane",
     "Lacouture"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "Efficient lexical access strategies",
   "original": "e93_1537",
   "page_count": 4,
   "order": 350,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "In large vocabulary continuous speech recognition systems, one of the critical issues that have to be addressed is that of the search complexity. This paper studies two aspects of this problem. The first aspect is that of efficient pruning. We look at beam search and show that using a narrower beam for word endings can substantially improve the efficiency of the search. Secondly, we look at multipass search strategies and propose a first pass approach based on an approximate search using a tree representation of the lexicon. Experiments using this approach are presented.\n",
    "Keywords: Large vocabulary speech recognition, beam search, lexicon tree, forward-backward search.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-344"
  },
  "torres93b_eurospeech": {
   "authors": [
    [
     "I.",
     "Torres"
    ],
    [
     "Francisco",
     "Casacuberta"
    ]
   ],
   "title": "Multiple codebook Spanish phone recognition using semicontinuous hidden Markov models",
   "original": "e93_1543",
   "page_count": 4,
   "order": 351,
   "p1": "1543",
   "pn": "1546",
   "abstract": [
    "Up to now, Hidden Markov modelling (HMM) has been the most important approach to model sub-lexical units in acoustic-phonetic decoding. In this framework, the Semicontinuous HMM (SCHMM) allows us to obtain better recognition rates than the discrete one. Moreover, computational complexity is less than that of continuous mixture HMM. Within the framework of the SCHMM, this paper deals with the multiple codebook formulation of different approaches to the Viterbi-based re-estimation procedure. We aimed to take advantage of the jointly optimization of the different codebooks together with the parameters of the model. From this formulation, new re-estimation equations are obtained. The above approaches allow the different codebooks to be updated in the training phase while keeping the computational cost quite low. Several series of decoding experiments were carried out over a corpus of 1,700 Spanish sentences uttered by 10 speakers. The decoding results present a strong improvement when multiple observations are considered\n",
    "Keywords: Speech recognition, structural pattern recognition, hidden Markov models, Spanish acoustic-phonetic decoding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-345"
  },
  "bonafonte93_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Xavier",
     "Ros"
    ],
    [
     "Jose B.",
     "Marifio"
    ]
   ],
   "title": "An efficient algorithm to find the best state sequence in HSMM",
   "original": "e93_1547",
   "page_count": 4,
   "order": 352,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "Hidden Markov Modeling (HMM) techniques have been applied successfully to speech analysis. However, it has been claimed [1-7] that a major weakness of HMM is that the state duration probability density functions (SDPDF) are exponential, which is not appropriate for modelling speech events. In order to cope with this deficiency some authors have proposed to model explicitly the state duration. In these models the first order Markov hypothesis is broken in the loop transitions. Thus, the new models have been called Hidden Semi-Markov Models (HSMM). Different solutions have been proposed being the main common drawback the increase of the computational time by a factor D, being D the maximum time allowed in each state. In this paper a modified Viterbi algorithm which finds the best state sequence of HSMM is proposed. The proposed algorithm deals with log-convex parametric SDPDF. The log-convex property is fulfilled by the parametric functions usually applied. This method increases the computational burden with respect to conventional HMM by an empirical factor of just 3.2 without losing optimality and without increasing the storage with respect to other approaches. A more efficient algorithm is presented for the case that the duration of the states is modeled by bounded functions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-346"
  },
  "acero93_eurospeech": {
   "authors": [
    [
     "Alex",
     "Acero"
    ],
    [
     "C.",
     "Crespo"
    ],
    [
     "C. de la",
     "Torre"
    ],
    [
     "J. C.",
     "Torrecilla"
    ]
   ],
   "title": "Robust HMM-based endpoint detector",
   "original": "e93_1551",
   "page_count": 4,
   "order": 353,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "A new real time HMM-Based endpoint detector is proposed in this paper. Endpoint detection has been shown to be critical in automatic speech recognition systems. The system uses static (energy) and dynamic (delta energy) features of the signal on a frame by frame basis. The endpoint detector is trainable for the working conditions (i.e. telephone lines) and is able to track changes in background noise conditions. Our experiments indicate that high accuracy, low false rejection and low false alarm rates can be obtained with this new endpoint detector.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-347"
  },
  "galiano93_eurospeech": {
   "authors": [
    [
     "Isabel",
     "Galiano"
    ],
    [
     "Francisco",
     "Casacuberta"
    ]
   ],
   "title": "Experiments on Spanish phone recognition using automatically derived phonemic baseforms",
   "original": "e93_1555",
   "page_count": 4,
   "order": 354,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "We report a complete evaluation of a Spanish Inferred Stochastic Regular Grammar (ISRG)-based Continuous Acoustic-Phonetic Decoder. We designed three basic series of experiments on a standard Spanish database which involves 1700 sentences uttered by ten speakers. For each one of these series we report results by using different feature analyses and sub-word language models. The different improvements have lead to the best performances of the system: 78% for the Speaker Dependent experiment and 72% for the Speaker Independent experiments. Moreover, the introduced benchmarks are not only a baseline evaluation for Spanish phone recognition but also allow for a direct comparison of ISRG with the more \"standard\" Multiple-Codebook Semi-Continuous Hidden Markov Models (MCSCHMM)for the same task and unit baseline.\n",
    "Keywords: Syntactic Pattern Recognition, Grammatical Inference, Acoustic-Phonetic Decoding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-348"
  },
  "nakagawa93_eurospeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Hideyuki",
     "Suzuki"
    ],
    [
     "Li",
     "Zhao"
    ]
   ],
   "title": "Evaluation of VQ-distortion based HMM",
   "original": "e93_1559",
   "page_count": 4,
   "order": 355,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "We proposed a new speech recognition method which integrated a VQ-distortion measure and a discrete HMM. This VQ-distortion based HMM uses a VQ-distortion measure at each state instead of a discrete output probability used by a discrete HMM. Although this method is regarded as a refined version of the VQ-distribution based recognition method proposed by Burton et al, it is also considered as a special case of a mixtured distribution density HMM. In this paper, we describe the relationship between the VQ-distortion based HMM and conventional HMMs, and compare their speech recognition performance through the experiments on speaker-independent spoken digit recognition. This new method overperformed traditional HMMs.\n",
    "Keywords: HMM, VQ-distortion based HMM, spoken digit recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-349"
  },
  "song93_eurospeech": {
   "authors": [
    [
     "Jianming",
     "Song"
    ]
   ],
   "title": "Continuous HMM for word spotting and rejection of non vocabulary word in speech recognition over telephone networks",
   "original": "e93_1563",
   "page_count": 4,
   "order": 356,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "This paper deals with the problems of single word spotting and non vocabulary word rejection, with the purpose of enhancing recognition performance of conventional isolated word recognizers. The approach proposed in this paper is organized in three steps: 1) using a garbage model to represent extraneous speech, garbage model is generic HMM with a tied covariance matrix and trained on a wide range of speech with different characteristics. 2) applying a grammar-driven frame synchronous level building algorithm to generate three possible combinations of key word and extraneous speech, the patterns of the three candidates for un utterance are: only a key word or no key word at all, a key word with extraneous speech at the beginning or end; a key words with extraneous speech at both the beginning and end. 3) adding a post decoder to perform a sequence of validity tests on the multiple candidate strings. The candidate that passes all testing and still holds the best likelihood score is selected as spotted word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-350"
  },
  "huo93_eurospeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Chorkin",
     "Chaw"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Bayesian learning of the parameters of discrete and tied mixture HMMs for speech recognition",
   "original": "e93_1567",
   "page_count": 4,
   "order": 357,
   "p1": "1567",
   "pn": "1570",
   "abstract": [
    "In this paper a theoretical framework for Bayesian adaptive learning of discrete HMM parameters is presented. Formulations of MAP and segmental MAP estimation of DHMM parameters are developed. An empirical Bayes method to estimate the hyperparameters of prior density based on the moment estimate is proposed. We applied the proposed method to speaker adaptation problems using a 26-word English alphabet vocabulary. Speaker-adaptive training algorithm is shown to be effective in improving the performance of both speaker-dependent and speaker-independent speech recognition problems. The method proposed in this paper will also be applicable to other problems in HMM training for speech recognition such as sequential or batch training, context adaptation, parameter smoothing, and so on.\n",
    "Keywords: Bayesian learning; hidden Markov model; speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-351"
  },
  "fink93b_eurospeech": {
   "authors": [
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ],
    [
     "Ernst G.",
     "Schukat-Talamazzini"
    ]
   ],
   "title": "Speech recognition using semantic hidden Markov networks",
   "original": "e93_1571",
   "page_count": 4,
   "order": 358,
   "p1": "1571",
   "pn": "1574",
   "abstract": [
    "Semantic Hidden Markov Networks (SHMNs) were first introduced in [2] as a new technique of interfacing between linguistic analysis and word recognition in speech understanding systems. The main difference between SHMNs and the use of traditional language models is that SHMNs always refer to a linguistic concept and impose the linguistic structure as closely as possible on its acoustic counterpart - a hierarchically structured HMM. Normally the result of decoding a HMM is merely the sequence of best fitting elementary acoustic concepts, e.g. phonemes or words. Taking into account the structure of the recognition task a structured instance can be computed. This complex acoustic instance can easily be transformed into a linguistic instance by a recursive computation but without any searching. In this paper we present an algorithm for generating linguistic instances from word recognition results based on SHMNs. Additionally, we present recognition results obtained when evaluating a set of SHMNs for the task domain of train schedule information.\n",
    "Keywords: Speech Recognition and Understanding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-352"
  },
  "downey93_eurospeech": {
   "authors": [
    [
     "Simon",
     "Downey"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Peter",
     "Nowell"
    ],
    [
     "David",
     "Bijl"
    ],
    [
     "Kirsta",
     "Galloway"
    ],
    [
     "Keith",
     "Ponting"
    ]
   ],
   "title": "Experiments in vocabulary independent speech recognition using phoneme decision trees",
   "original": "e93_1575",
   "page_count": 4,
   "order": 359,
   "p1": "1575",
   "pn": "1578",
   "abstract": [
    "This paper reports the results of experiments in speaker-independent, vocabulary-independent speech recognition using Phoneme Decision Tree (PDT) methods. Phoneme-level hidden Markov models (HMMs) are trained on a corpus comprising a balanced set of general spoken English sentences and evaluated on a test corpus of continuously spoken airborne reconaissance reports. Vocabulary-dependent and -independent performance is compared. Experimental results are reported for \"PDT pruning\", which explore the trade-off between number of models and amount of training material per model, conventional uniform allocation of mixture components to HMM states, and non-uniform allocation of mixture components to states.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-353"
  },
  "gales93b_eurospeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Segmental hidden Markov models",
   "original": "e93_1579",
   "page_count": 4,
   "order": 360,
   "p1": "1579",
   "pn": "1582",
   "abstract": [
    "The most popular and successful acoustic model for speech recognition is the Hidden Markov Model (HMM). To use HMMs for speech recognition a series of assumptions are made about the waveform, some of which are known to be poor. In particular, the 'Independence Assumption' implies that all observations are only dependent on the state that generated them, not on neighbouring observations. In this paper, a new form of acoustic model is described called the Segmental Hidden Markov Model (SHMM) in which the effect of the 'Independence Assumption' on the observation likelihood is greatly reduced. In the SHMM all observations are assumed to be independent given the state that generated them but additionally they are conditional on the mean of the segment of speech to which they belong. Re-estimation formulae are presented for the training of both single and multiple Gaussian Inter Mixture models and a recognition algorithm is described. Additionally it is shown that the standard HMM, both in the single Gaussian mixture and multiple Gaussian mixtures cases, is just a subset of the SHMM. The new model is shown to provide better recognition performance on a wider set of synthetic data than the standard HMM.\n",
    "Keywords: speech recognition, HMM, segment models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-354"
  },
  "wang93c_eurospeech": {
   "authors": [
    [
     "Xue",
     "Wang"
    ],
    [
     "Louis F. M. ten",
     "Bosch"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Impact of dimensionality and correlation of observation vectors in HMM-based speech recognition",
   "original": "e93_1583",
   "page_count": 4,
   "order": 361,
   "p1": "1583",
   "pn": "1586",
   "abstract": [
    "The dimensionality and correlation between acoustic observation vectors and between the components within the vectors are investigated in terms of their impact on the performance of HMM (hidden Markov model)-based speech recognition. The dimensionality and correlation are manipulated with principal component analysis and linear discrimination analysis, on either a continuous density or a discrete density HMM system.\n",
    "Keywords: HMM, Speech Recognition, PCA, LDA, VQ\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-355"
  },
  "class93b_eurospeech": {
   "authors": [
    [
     "F.",
     "Class"
    ],
    [
     "A.",
     "Kaltenmeier"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Evaluation of an HMM speech recognizer with various continuous speech databases",
   "original": "e93_1587",
   "page_count": 4,
   "order": 362,
   "p1": "1587",
   "pn": "1590",
   "abstract": [
    "This paper describes a speaker-independent HMM continuous speech recognizer and its evaluation with three large continuous speech databases recorded at several German universities and industrial sites. These databases differ with regard to recording conditions, speaking rate, microphone location, amount of training and test data, and application dependency of the training material. The recognition system is based on mel-cepstral coefficients, a linear discriminant transform of cepstral data, a soft-decision vector quantizer with gaussian distributions, and semi-continuous Hidden Markov Models (SCHMM) of context-dependent subword units.\n",
    "Keywords: Continuous speech recognition, linear discriminant analysis, SCHMM, speech databases, word graphs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-356"
  },
  "wrzoskowicz93_eurospeech": {
   "authors": [
    [
     "Adam",
     "Wrzoskowicz"
    ]
   ],
   "title": "Hidden Markov models for noisy speech recognition",
   "original": "e93_1591",
   "page_count": 4,
   "order": 363,
   "p1": "1591",
   "pn": "1594",
   "abstract": [
    "This paper presents the development of the speech recognition systems based on Hidden Markov Models. HMM-based isolated words and continuous speech recognizers are presented. The network of connected HMM has been applied to improve accuracy of continuous speech recognition. The HMM network is build on a priori knowledge, for instance: phonetic description, phonological rules, syntax. The recognizers has been studied in an noisy environment. We have proposed two methods of creation Hidden Markov Models for noisy speech recognition. Both can be used in noise auto-adapting systems.\n",
    "Keywords: HMM, Noisy Speech Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-357"
  },
  "tsoukalas93_eurospeech": {
   "authors": [
    [
     "D. E.",
     "Tsoukalas"
    ],
    [
     "J.",
     "Mourjopoulos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Neural network speech enhancer utilizing masking properties",
   "original": "e93_1595",
   "page_count": 4,
   "order": 364,
   "p1": "1595",
   "pn": "1598",
   "abstract": [
    "A new method for speech enhancement is presented. The noise removal process is performed by a Neural Network which is fed with the noisy speech signal power spectrum and produces at its output the power spectrum of the enhanced signal. To train the Neural Network, the Auditory Masking Threshold of a rough estimate of the clean signal is used and the algorithm employed is an extended version of the backpropagation. The NN's used in a novel way, since training and operation modes are combined for every short time frame of the speech signal. Preliminary results have shown that the power spectrum of the enhanced signal is very close to the power spectrum of the clean signal and better than the spectrum obtained using the Power Spectral Subtraction method.\n",
    "Keywords: Speech Enhancement, Neural Network, Masking, Backpropagation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-358"
  },
  "castro93_eurospeech": {
   "authors": [
    [
     "Maria J.",
     "Castro"
    ],
    [
     "Juan C.",
     "Perez"
    ]
   ],
   "title": "Comparison of geometric, connections and structural techniques on a difficult isolated word recognition task",
   "original": "e93_1599",
   "page_count": 4,
   "order": 365,
   "p1": "1599",
   "pn": "1602",
   "abstract": [
    "The sequential structure and variable length of speech data suggest the use of structural techniques such as Hidden Markov Models or Grammatical Inference systems. In contrast, decision theoretic-based on \"geometric\" and classical (non-recurrent) connectionist methods deal with objects represented in a metric and/or vector space. This means that some technique has to be used to transform variable-length strings of parameters into d-dimensional vectors. Several such methods exist and some of them have been tested in this work in a difficult isolated word recognition task. The results of experiments with k-Nearest Neighbor, Multilayer Perceptron and Decision Surface Mapping are compared with others already reported using Hidden Markov Models, Error Correcting Grammatical Inference and Morphic Generator Grammatical Inference systems.\n",
    "Keywords: Pattern Recognition, Isolated Word Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-359"
  },
  "mellouk93_eurospeech": {
   "authors": [
    [
     "A.",
     "Mellouk"
    ],
    [
     "P.",
     "Gallinari"
    ],
    [
     "F.",
     "Rauscher"
    ]
   ],
   "title": "Prediction and discrimination in neural networks for continuous speech recognition",
   "original": "e93_1603",
   "page_count": 4,
   "order": 366,
   "p1": "1603",
   "pn": "1606",
   "abstract": [
    "We present a neural prediction system for continuous speaker independent speech recognition. We propose different neural predictors for modeling speech production and discriminative criteria for training. The best system allows to reach 74,9% accuracy on TIMIT which compares well with other state of the art systems. The behavior and performances of this system are then compared with a Hidden Control Neural Network implementation of the predictors.\n",
    "Keywords: Predictive Neural Networks, Discriminant training, Continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-360"
  },
  "ran93_eurospeech": {
   "authors": [
    [
     "Shuping",
     "Ran"
    ],
    [
     "J. Bruce",
     "Millar"
    ]
   ],
   "title": "Two schemes of phonetic feature extraction using artificial neural networks",
   "original": "e93_1607",
   "page_count": 4,
   "order": 367,
   "p1": "1607",
   "pn": "1610",
   "abstract": [
    "Distinctive features are extracted from the acoustic signal using artificial neural networks. It is shown that the different acoustic realisation of the features in vowels and stop consonants can be exploited to gain better feature extraction by treating them separately. Our result gives support to the existence of invariant cues for some features. Keyword: Distinctive features; Extraction; Invariant cues; Artificial Neural Networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-361"
  },
  "petek93_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ],
    [
     "Anuska",
     "Ferligoj"
    ]
   ],
   "title": "On use of discriminant analysis in predictive connectionist speech recognition",
   "original": "e93_1611",
   "page_count": 4,
   "order": 368,
   "p1": "1611",
   "pn": "1614",
   "abstract": [
    "A Linear Discriminant Analysis (LDA) is proposed to be used as a tool for verification of the usefulness of additional modeling of prediction error signal in a context-dependent Hidden Control Neural Network (HCNN-CDF) system. While showing that the squared prediction error of the HCNN-CDF system is not a white noise, the LDA also identified the most important components of the error vector signal which could be used to support the discrimination among predictive models of the system. This information is then used to enhance the context-dependent HCNN model to become more discriminant by modeling dynamics in acoustics and in error signal space. Our preliminary speaker-dependent tests with vocabulary size of 100 words confirmed an increase in word recognition rate and in discrimination power of the system. Recently, continuous speech recognition experiments at perplexity 100 have shown an increase in word accuracy from 76% to 87% on the test set database.\n",
    "Keywords: Automatic Speech Recognition, context-dependent Hidden Control Neural Network, large vocabulary countinuous speech recognition, Lin- ear Discriminant Analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-362"
  },
  "russell93_eurospeech": {
   "authors": [
    [
     "N. H.",
     "Russell"
    ],
    [
     "Frank",
     "Fallside"
    ],
    [
     "R. W.",
     "Prager"
    ]
   ],
   "title": "Non-linear time compression for lexical access",
   "original": "e93_1615",
   "page_count": 4,
   "order": 369,
   "p1": "1615",
   "pn": "1618",
   "abstract": [
    "The paper describes the development of a non-linear temporal compression algorithm applied to a sequence of phonetic classification probability vectors. This is employed as a means of data reduction between the two stages of a recogniser performing lexical access based on recurrent neural nets. The algorithm operates by identifying pseudo-stationary segments in the sequence of probability vectors, and forming the average vector of each segment thus identified. Boundaries between pseudo-stationary segments are located using a metric applied to adjacent vectors. Various metrics are compared, along with two heuristics and a novel NN technique which perform the segmentation task using the temporal profile generated by the metric. The compression ratios may be varied smoothly between 3.7 and 6.8, under the control of a threshold parameter with an associated segment deletion rate of between 3% and 27%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-363"
  },
  "brierton93_eurospeech": {
   "authors": [
    [
     "Richard",
     "Brierton"
    ],
    [
     "Nigel",
     "Sedgwick"
    ]
   ],
   "title": "Talker enrollment for speech recognition by synthesis",
   "original": "e93_1619",
   "page_count": 4,
   "order": 370,
   "p1": "1619",
   "pn": "1622",
   "abstract": [
    "In Recognition by Synthesis (RbS), subword models of the type used in Text-to-Speech (TtS) synthesis are used for speech recognition. These subword models are variable duration acoustic-phonetic segments. The segment parameters differ from talker to talker, a complete set being called a Talker Characterisation Table (TCT). We describe algorithms for automatically tuning a TCT to the speech of a particular talker, using connected speech enrolment utterances covering the whole phonetic range, manually transcribed at the phonemic level. Speech synthesised using the tuned TCT sounds more natural and more like that of the enrolled talker, when using both synthetic and copied natural prosody, for utterances inside and outside the enrolment set. Thus a generic rather than an utterance specific TCT has been produced. Algorithms are also described for automatically transcribing speech into a sequence of acoustic-phonetic segments, constrained only the phonotactics of the language and using a TCT tuned to the talker.\n",
    "Keywords: automatic speech recognition, recognition by synthesis, talker enrolment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-364"
  },
  "takeda93_eurospeech": {
   "authors": [
    [
     "Kauzya",
     "Takeda"
    ],
    [
     "Naomi",
     "Inoue"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Tomohiro",
     "Konuma"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Improving robustness of network grammar by using class HMM",
   "original": "e93_1623",
   "page_count": 4,
   "order": 371,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "In this paper, class HMM, an acoustic model of multiple words in the same domain-dependent class, is proposed. The proposed class HMM is trained by a set of words falling into a class so that the model outputs a certain score when it is matched with any word included in the class. This direct modeling of a class provides a simple method to improve a FSN grammar which takes semantic constraint into account, in order to accept more word sequences without reducing constraint. The key idea of the method is to match input speech with a sequence of classes, not words, when the input speech does not match well with the possible word sequences. This can be simply realized by bypassing a word in the grammar network using a class model. Due to the rough acoustic modeling of class HMM, furthermore, it is expected that the bypass will not match better than the original path and will not reduce the constraint of the original grammar. The effectiveness of the method is evaluated by recognition experiments using the extension telephone exchange domain including 260 words, and the static branching factor of the original grammar is 12.6. The result shows that 43 % of the test sentences that original grammar can not accept are recognized as a correct sequence of classes and words, and can be properly processed by the dialogue module. Furthermore, no degradation of recognition accuracy is found in recognizing 560 sentences which can be accepted by the original grammar. These results show that the recognition error due to the unexpected input can be reduced by half, and that the proposed scheme is effective for spontaneous speech input.\n",
    "Keywords: Spontaneous speech recognition, Network grammar\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-365"
  },
  "elliott93_eurospeech": {
   "authors": [
    [
     "J. A.",
     "Elliott"
    ],
    [
     "M. E.",
     "Forsyth"
    ],
    [
     "F. R.",
     "McInnes"
    ],
    [
     "N. W.",
     "Ramsey"
    ]
   ],
   "title": "Parallelising k-means clustering on distributed memory MIMD computers",
   "original": "e93_1627",
   "page_count": 4,
   "order": 372,
   "p1": "1627",
   "pn": "1630",
   "abstract": [
    "Two strategies are presented for the parallelisation of the k-means clustering algorithm on MIMD distributed-memory computers (multicomputers). The implementations are decribed in detail The advantages of each outlined.\n",
    "Keywords: Speech recognition, VQ Codebook, k-means clustering, parallel processing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-366"
  },
  "berenyi93_eurospeech": {
   "authors": [
    [
     "P.",
     "Berenyi"
    ],
    [
     "Klára",
     "Vicsi"
    ]
   ],
   "title": "On the proper sub-word unit inventory for CSR",
   "original": "e93_1631",
   "page_count": 4,
   "order": 373,
   "p1": "1631",
   "pn": "1634",
   "abstract": [
    "Considering the succes of ESPRIT projects, especially the multilingual speech recognizer systems, we examined the possibility to adapt these systems to Hungarian language. Aglutinative languages (Hungarian, Finnish, Turkish, etc.) has much more word-forms than indoeuropean languages have. The same problem, although to a lesser degree, holds for some indoeuropean languages, too, especially for Slavic languages, and perhaps also for German. So even a large vocabulary isolated word recognizer would require the incorporation of CSR ideas. It would be highly impractical simply enumerate the possible word-forms in some sort of vocabulary, as it is generally done in English, for words in aglutinative languages has their own sophisticated grammatical structure. In the case of real CSR the problem is even harder, indeed. So it is clear, that the units to be recognized at the lowest level have to be smaller than words. The exact size and definition of these units however, depends on the language, and has to be determined on empirical bases. To this end a statistical study of (Hungarian) language is conducted on the phonetic to syllabic level, and the acoustic structure of possible unit candidates is also studied. A Hungarian text database is analyzed after grapheme-to-phoneme conversion (what can be done quite well by rules alone for Hungarian). The statistical distribution of several entities (consonant-clusters, half syllables, syllables) is found to be different in continuous speech from that of isolated words. Based on the statistical examinations we found half syllable units to be the most compact description of the phonological structure of Hungarian language.\n",
    "Keywords: - Continuous speech recognition, - Automatic segmentation - Half syllable - Statistical study\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-367"
  },
  "deng93_eurospeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Don",
     "Sun"
    ]
   ],
   "title": "Speech recognition using the atomic speech units constructed from overlapping articulatory features",
   "original": "e93_1635",
   "page_count": 4,
   "order": 374,
   "p1": "1635",
   "pn": "1638",
   "abstract": [
    "We report our recent development of a feature-based general statistical framework for automatic speech recognition. The design of the feature-based atomic units of speech is aimed at a parsimonious scheme to share the inter-word and inter-phone speech data and at a unified way to account for the context-dependent behaviors in speech. We provide detailed descriptions of the design considerations for the recognizer and of key aspects of the design process. This process, which we call lexicon \"compilation\", consists of three elements: 1) establishing a feature-specification system; 2) constructing a probabilistic and fractional temporal overlapping pattern across the features; and 3) mapping from the feature-overlap pattern to a state-transition graph. A standard phonetic classification task from the TIMIT database is used as a testbed to evaluate the performance of the recognizer. The experimental results show error-rate reductions ranging from 15% to 27% compared with a conventional context-independent phonetic classifier.\n",
    "Keywords: speech recognition, features, non-linear phonology, hidden Markov model, articulatory gestures\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-368"
  },
  "siohan93_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "A Bayesian approach to phone duration adaptation for lombard speech recognition",
   "original": "e93_1639",
   "page_count": 4,
   "order": 375,
   "p1": "1639",
   "pn": "1642",
   "abstract": [
    "Speech recognition under noisy conditions is of great interest for practical purposes. When a speech recognition system is trained under clean conditions and tested under noisy conditions, recognition rates are very low, due to the mismatching between clean and noisy speech. A lot of methods have been developed in the past few years to improve the robustness of speech recognizers in adverse conditions. All of these techniques aim at reducing mismatches in the spectral domain between clean and noisy speech. In presence of noise, speech is modified according to the Lombard effect. This causes some changes in the speech signal, in particular a modification of the duration of each phone. In this paper, we try to reduce mismatches between the phone duration in Lombard and clean speech. We use the framework of Bayesian adaptation to re-estimate the parameter of duration model for each phone. When testing on a 49 alphanumeric vocabulary in isolated mode, we found the adaptation of phone duration models leads to an improvement in the recognition rate, especially for female speakers.\n",
    "Keywords: Lombard speech recognition, Bayesian adaptation, Phone duration model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-369"
  },
  "hernando93_eurospeech": {
   "authors": [
    [
     "J.",
     "Hernando"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Multiple multilabeling to improve HMM-based speech recognition in noise",
   "original": "e93_1643",
   "page_count": 4,
   "order": 376,
   "p1": "1643",
   "pn": "1646",
   "abstract": [
    "The performance of existing speech recognition systems degrades rapidly in the presence of background noise when training and testing cannot be done under the same ambient conditions. The aim of this paper is to propose the application of a simple multilabeling method, instead of the standard vector quantization -so called labeling-, as the front end for a speech recognizer based on the Vector Quantization (VQ) and Hidden Markov Models (HMM) approaches in order to increase its robustness to noise. Furthermore, not only cepstrum but also other features such as energy and dynamic parameters are evaluated and quantized independently in the multilabeling stage to represent more accurately characteristics of speech. The result of this process is a multiple multilabeling. Experimental results in the presence of additive white noise and car environment clearly demonstrate its good performance in isolated word recognition in noisy environments.\n",
    "Keywords: Noisy speech recognition, Vector Quantization, Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-370"
  },
  "richter93_eurospeech": {
   "authors": [
    [
     "Lutoslawa",
     "Richter"
    ],
    [
     "Piotr",
     "Domagaia"
    ]
   ],
   "title": "Discrimination of polish stop consonants based on mapped techniques",
   "original": "e93_1647",
   "page_count": 4,
   "order": 377,
   "p1": "1647",
   "pn": "1650",
   "abstract": [
    "The aim of the work reported is to test the possibility of speaker- and context-independent automatic discrimination of Polish stop consonants. A new approach to stop consonant discrimination has been proposed based on a linear combination of autocorrelation function values. By computing BETWEEN and WITHIN matrices for parameters representing different populations and by solving the general eigenproblem the direction of the eigenvector is determined, corresponding to the maximum eigenvalue. When projected in this direction, objects belonging to one population are most clustered and populations themselves maximally separated. It is in this direction that pair-wise projection of microsegments (\"each with each\") representing stop consonants was performed. Multidimensional parameter space was reduced to one dimension (axis). The material consisted of nonsense words, with most common Polish stop consonant contexts, produced by 20 speakers (10 male and 10 female). Experiments were conducted for male and female voices separately as well as for all the voices pooled. The burst segment has been found to provide better cues for phone identification then thefrication segment. The average identification rate (for voices pooled) was . 764.\n",
    "Keywords: stop consonants, classification, discriminant analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-371"
  },
  "eckert93_eurospeech": {
   "authors": [
    [
     "Wieland",
     "Eckert"
    ],
    [
     "Scott",
     "McGlashan"
    ]
   ],
   "title": "Managing spoken dialogues for information services",
   "original": "e93_1653",
   "page_count": 4,
   "order": 378,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "This paper presents an approach to managing spoken dialogues in information services systems. We describe how the approach, based upon a tri-partite model of interaction, addresses the problems of co-operativeness and portability across languages and task domains. This approach has been implemented in the generic dialogue manager of the Sundial dialogue systems. We outline the capabilities of this dialogue manager and describe our testing-methodology.\n",
    "Keywords: Oral Dialogue, Dialogue Management, Interpretation of Utterances, Cooperative System\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-372"
  },
  "heisterkamp93_eurospeech": {
   "authors": [
    [
     "Paul",
     "Heisterkamp"
    ]
   ],
   "title": "Ambiguity and uncertainty in spoken dialogue",
   "original": "e93_1657",
   "page_count": 4,
   "order": 379,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "In actual spoken dialogues, ambiguity is a special case of uncertainty. Uncertainty is the overall standard condition of a speech understanding system, not only because of possible deficiencies in the recognition, but also because people may make mistakes. The system has to have ways to deal with this condition. In SUNDIAL, the strategy chosen is to find the optimal interpretation with regard to context and situational setting, and to use the result of this interpretation in the following system utterance, so that mutual assurance of understanding what was meant is constantly given.\n",
    "Keywords: speech dialogue, ambiguity, uncertainty, dialogue strategy, semantics, SUNDIAL.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-373"
  },
  "gerbino93_eurospeech": {
   "authors": [
    [
     "Elisabetta",
     "Gerbino"
    ],
    [
     "Morena",
     "Danieli"
    ]
   ],
   "title": "Managing dialogue in a continuous speech understanding system",
   "original": "e93_1661",
   "page_count": 4,
   "order": 380,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "This paper describes the approach followed in implementing the Dialogue Manager of the continuous speech understanding system developed at Cselt during the last few years. In designing the dialogue strategies we have taken into account the specific goals a dialogue system for speech has to achieve. A Dialogue Manager for a speech application has to be robust enough to maintain an acceptable level of user/system interaction quality even when there are partial or complete failures at the lower levels. We will describe how our Dialogue Manager specific architecture meets these requirements.\n",
    "Keywords: Oral Dialogue Management, Flexible Dialogue Strategies, Predictions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-374"
  },
  "lefebvre93_eurospeech": {
   "authors": [
    [
     "P.",
     "Lefebvre"
    ],
    [
     "G.",
     "Duncan"
    ],
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "Speaking with computers: a multimodal approach",
   "original": "e93_1665",
   "page_count": 4,
   "order": 381,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "We propose to describe in this paper, the use of voice in a multimodal environment. We particulary pay attention to the interaction between a Human and the Unix Operating System. This works want to highlight the need to improve the impact of speech recognition systems on Human-Computer Interfaces by the use and the complementarity of other media. This approach shows that ergonomic benefits and efficient interaction due to the use of multiple media, can reduce the gap between a user and a computer task.\n",
    "Keywords: Multimodal Dialogue, Speech Recognition and Understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-375"
  },
  "morin93b_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Morin"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Habitable interaction in goal-oriented multimodal dialogue systems",
   "original": "e93_1669",
   "page_count": 4,
   "order": 382,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "An understanding of what makes a user interface habitable and natural is needed. Man-machine interfaces must be helpful and cooperative to be usable, especially when difficult situations occur in the interaction. User acceptance of these interfaces depends on their error robustness and their ability to provide assistance. This is especially important for error-prone input such as speech. In this paper we describe several generic mechanisms that have been developed to provide user guidance and improve naturalness in the PARTNER man-machine dialogue system. It is shown how these mechanisms can be used to ease the interaction with naive users and facilitate error recovery. Finally we present some results obtained in the case of an object manipulation tool developed with PARTNER.\n",
    "Keywords: Help, Error recovery, Multimodal dialogue\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-376"
  },
  "nielsen93_eurospeech": {
   "authors": [
    [
     "Jorn Stern",
     "Nielsen"
    ],
    [
     "Bo",
     "Baungaard"
    ]
   ],
   "title": "Test of voice quality on ATM based equipment",
   "original": "e93_1675",
   "page_count": 4,
   "order": 383,
   "p1": "1675",
   "pn": "1678",
   "abstract": [
    "This paper reports on the experience and results obtained from a voice quality assessment of a commercial available ATM-like packetized system. The system is designed to integrate voice, video, frame relay and other types of data over a wide range of transmission facilities. Typically the system will be organized in a network. Both voice and data compression techniques are provided. The aim of the voice quality assessment was to investigate how users judge the different compression rates compared with each other. The assessment was based on a subjective listening test method. This paper gives a description of the test equipment, an automatic voting equipment, and the listening test method. Finally the results of the assessment are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-377"
  },
  "klaus93_eurospeech": {
   "authors": [
    [
     "Harald",
     "Klaus"
    ],
    [
     "H.",
     "Klix"
    ],
    [
     "Jochem",
     "Sotscheck"
    ],
    [
     "Klaus",
     "Fellbaum"
    ]
   ],
   "title": "An evaluation system for ascertaining the quality of synthetic speech based on subjective category rating tests",
   "original": "e93_1679",
   "page_count": 4,
   "order": 384,
   "p1": "1679",
   "pn": "1682",
   "abstract": [
    "This paper describes the hardware and software architecture of an evaluation system for mean opinion score category rating tests. It is primarily designed for ascertaining the quality of synthetic speech. The first part describes the hardware components of the evaluation system and details of the evaluation methodology to determine the quality of synthetic speech samples. Experiments with 23 test subjects were performed to optimize the dialogue interface and the test procedures. Details of the optimization process and experimental results are outlined in the second part of the paper. The system was developed at the Institute of Telecommunications of the Technical University of Berlin in co-operation with the Berlin Branch of the Research and Technology Centre of the German Telekom.\n",
    "Keywords: Synthetic Speech, Evaluation, Subjective\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-378"
  },
  "mariniak93_eurospeech": {
   "authors": [
    [
     "Arnd",
     "Mariniak"
    ]
   ],
   "title": "A global framework for the assessment of synthetic speech without subjects",
   "original": "e93_1683",
   "page_count": 4,
   "order": 385,
   "p1": "1683",
   "pn": "1686",
   "abstract": [
    "In this paper a framework will be introduced which is specifically directed towards the quality assessment of synthetic speech by means of speech and speaker recognition techniques. The basic idea of this approach is that a quality judgement is closely related to the speech recognition process which is in principle a pattern recognition task. That means that auditory perception of speech can be described as a comparison of an unknown speech pattern with the listener's internal 'reference data base' of known speech patterns. The important question is which of the various features in the speech patterns are responsible for, e.g., speaker variability (and thus the distinction of synthetic and natural speech) and for speech quality in its global sense. The framework briefly can be described as follows: A perception-based analysis of speech samples of many speakers build a reference feature space. Samples of speech synthesizers are classified with regard to this reference set and distance measures are computed. The distance measures will be compared with subjective ratings which were obtained by listening tests. Data tell to what extent the distance measure is able to predict subjective ratings. It is obvious that the question is quite similar to issues in the research area of speech and speaker recognition, but it comprises a different way of looking at the things.\n",
    "Keywords: Speech Quality Assessment, Speech Synthesis, Speech Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-379"
  },
  "neovius93_eurospeech": {
   "authors": [
    [
     "Lennart",
     "Neovius"
    ],
    [
     "Parimala",
     "Raghavendra"
    ]
   ],
   "title": "Comprehension of KTH text-to-speech with \"listening speed\" paradigm",
   "original": "e93_1687",
   "page_count": 4,
   "order": 386,
   "p1": "1687",
   "pn": "1690",
   "abstract": [
    "The comprehension of natural and synthetic speech in Swedish and American English was investigated using a sentence-by-sentence listening paradigm. The synthesised speech was generated by the KTH text-to-speech systems. Results indicated that sentence listening times were significantly longer only for American English synthetic speech than natural speech. Text difficulty was found to be a significant variable in both Swedish and American English for sentence listening times and word recognition, and only in American English for proposition recognition. The results are discussed in terms of the quality of the synthesisers and factors involved in comprehension.\n",
    "Keywords: comprehension, synthetic speech, text-to-speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-380"
  },
  "tillmann93_eurospeech": {
   "authors": [
    [
     "Hans G.",
     "Tillmann"
    ],
    [
     "Bernd",
     "Pompino-Marschall"
    ]
   ],
   "title": "Theoretical principles concerning segmentation, labelling strategies and levels of categorical annotation for spoken language database systems",
   "original": "e93_1691",
   "page_count": 4,
   "order": 387,
   "p1": "1691",
   "pn": "1694",
   "abstract": [
    "This paper is based on experience gained in connection with the German Government's spoken language data collections PHONDAT (1) and Verbmobil-PHONDAT. It addresses some of the basic decisions that have been made to guarantee that the resulting speech signal database system can find broadest applications. PHONDAT was planned not only to serve speech technology with respect to reliable German training and assessment material for speech recognition devices and/or for text to speech systems, but also to enable the further development of phonetic knowledge of spoken German. Only after enough empirical data have become available in a proper format can the actually spoken form of a language be formally represented through a phonetic theory in a sufficiently complete manner. If we call such a phonetic theory of a given spoken language a complete phonetic theory, CPT, then the final aim of the PHONDAT projects consists in contributing to the development of a CPT of spoken German.\n",
    "Keywords: phonetic segmentation, phonetic labelling, speech data bases. (1) PHONDAT is the name of a joint research project of the universities of Bonn, Braunschweig, Kiel and Munich\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-381"
  },
  "wyard93_eurospeech": {
   "authors": [
    [
     "Peter",
     "Wyard"
    ]
   ],
   "title": "The comparative assessment of commercial speech recognisers",
   "original": "e93_1881",
   "page_count": 4,
   "order": 388,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "Methodological issues in the assessment of commercial telephone-based speaker-independent speech recognisers are discussed, focussing on the practical decisions which must be made to arrive at a suitable standard. Results are presented for eight such state of the art recognisers tested on a fairly large UK telephone speech database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-382"
  },
  "riccio93_eurospeech": {
   "authors": [
    [
     "A.",
     "Riccio"
    ],
    [
     "F.",
     "Ceglie"
    ],
    [
     "A.",
     "Brancaccio"
    ]
   ],
   "title": "Reliable assessment of speech recognisers for telephone environment",
   "original": "e93_1885",
   "page_count": 4,
   "order": 389,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "The need for a standardised assessment of Automatic Speech Recognisers has been already emphasised in the framework of several Research Projects and in particular in Europe in the context of ESPRIT (SAM, SUNSTAR). In general the procedures proposed and the audio material available are fully suitable for a laboratory assessment of the performances of an ASR, but as soon as the application field is concerned, the limitations of such methodologies becomes evident. In particular the adoption of speech databases recorded in quiet conditions (clean speech) and the total absence of any kind of impairment on the communication channel are the main reasons of unreliability. Therefore on the basis of the work already initiated inside the SUNSTAR Project and as an enhancement of the performances of the standard SESAM workstation, the paper firstly concentrates on the generation of telephone grade speech corpora and a methodology to collect them; secondly it describes the realisation of an assessment experiment in the telephone environment aiming at an evaluation of the robustness of the DIVA recogniser (and in particular of one of its component, the end-point detector). The assessment has been performed in according to the procedures developed in the ESPRIT 2589 (SAM) project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-383"
  },
  "garnierrizet93_eurospeech": {
   "authors": [
    [
     "Martine",
     "Garnier-Rizet"
    ]
   ],
   "title": "Evaluation of a rule-based text-to-speech system for French at the segmental level",
   "original": "e93_1889",
   "page_count": 4,
   "order": 390,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "This paper gives a first evaluation of the rule-based Text-to-Speech system for French which has been developed within the ESPRIT Polyglot project 1024 \"A Multilingual Speech-to-Text and Text-to-Speech system\". The three modules which process the phoneme-to-parameter conversion are reminded with an emphasis on the coarticulation rules. As the evaluation concerns the segmental level, three identification tests are carried out and proposed to ten listeners. They consist of isolated vowels, VCV and VCCV logatoms. The results show that the weakest points are the nasal vowel / /, the voiced fricatives with /v/, in particular, and the liquid /l/. Further improvement is also needed to model the coarticulation in the V-/u/ and /u/-V contexts.\n",
    "Keywords: Text-to-Speech system, rule synthesis, identification tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-384"
  },
  "delogu93b_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Andrea",
     "Paoloni"
    ],
    [
     "P.",
     "Ridolfi"
    ],
    [
     "Kyriaki",
     "Vagges"
    ]
   ],
   "title": "Intelligibility of speech produced by text-to-speech synthesizers over the orthophonic and telephonic channel",
   "original": "e93_1893",
   "page_count": 4,
   "order": 391,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "In this work we present an intelligibility evaluation experiment for Italian synthesizers that uses an open-response test in which listeners are instructed to simply write down what they heard on each trial. Six systems and a natural voice as a reference have been evalutated both over the orthophonic and telephonic channels. 57 VCV and 51 CV are used. This material allows the evaluation of consonants in both initial and central position. A dedicated software called SOAP (Speech Output Assessment Package) is used.\n",
    "Keywords: text-to-speech synthesizers intelligibility evaluation, adverse conditions\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-385"
  },
  "spiegel93_eurospeech": {
   "authors": [
    [
     "Murray F.",
     "Spiegel"
    ]
   ],
   "title": "Using the ORATOR® synthesizer for a public reverse-directory service: design, lessons, and recommendations",
   "original": "e93_1897",
   "page_count": 4,
   "order": 392,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "Many telecommunication applications for text-to-speech synthesis (TTS) Involve speaking names and/or addresses. Two services often considered are a caller-name Identification service (often called \"Audible Caller Name Delivery\" or \"Who's Calling\" in the U.S.). and a reverse directory service. This paper will describe our research, development, and recommendations In providing speech synthesis for a heavily used reverse directory service. Issues Involved include highly accurate name pronunciation, high synthesis Intelligibility, and required directory preprocessing.\n",
    "Keywords: reverse directory service, name pronunciation, speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-386"
  },
  "grau93_eurospeech": {
   "authors": [
    [
     "Sophie",
     "Grau"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Gael",
     "Richard"
    ]
   ],
   "title": "A speech formant synthesizer based on harmonic + random formant-waveforms representations",
   "original": "e93_1697",
   "page_count": 4,
   "order": 393,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "This paper describes a new type of speech synthesizer: a parametric-concatenation (PACO) speech synthesizer, which is suitable both for formant synthesis and concatenation synthesis. This synthesizer is based on a hybrid quasi-harmonic and random formant-waveforms model of the speech signal. The synthesizer can be controlled by acoustic parameters : formant parameters and voice source parameters expressed in frequency domain. These acoustic parameters are converted into sinusoidal and formant waveforms parameters. The keypoint of this method is that spectral amplitudes are set according to a parallel formant model (both on sinusoidal waveforms and random formant-waveforms), whereas the spectral phases are set according to a serial formant model for sinusoidal waveforms (and are randomly distributed for formant waveforms). This approach avoids the phase interference problems inherent to parallel synthesis, while keeping the advantage of formant amplitudes control. An automatic analysis-synthesis system is also proposed for segments coding. Our model has been successfully implemented both as a formant synthesis system and in a concatenation synthesis Text-To-Speech system. Keywords : Speech synthesis, formant and concatenation synthesis, harmonic representation, random formant waveforms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-387"
  },
  "hauptmann93_eurospeech": {
   "authors": [
    [
     "Alexander G.",
     "Hauptmann"
    ]
   ],
   "title": "SPEAKEZ: a first experiment in concatenation synthesis from a large corpus",
   "original": "e93_1701",
   "page_count": 4,
   "order": 394,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "This paper reports on a preliminary implementation of the SpeakEZ speech synthesis system. The system is built to explore the use of naturally occuring pronunciation, coarticulation and prosody for speech synthesis. SpeakEZ uses concatenative synthesis, choosing target phonemes from a database corpus of 115,000 prerecorded phonemes. The database of phonemes is segmented and labeled using the Sphinx speech recognition system. During synthesis, target phonemes are selected based on heuristics relating to phoneme context and syllable, word, and utterance position. The phonemes are concatenated in the time domain, using pitch synchronous overlap-add (PSOLA) smoothing between adjacent phonemes. Results from a preliminary evaluation of the system show that the system can at times provide excellent synthetic speech, but still has several shortcomings.\n",
    "Keywords: Speech synthesis, time domain concatenation, corpus-based, pitch synchronous overlap-add (PSOLA).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-388"
  },
  "kerkhoff93_eurospeech": {
   "authors": [
    [
     "J.",
     "Kerkhoff"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Designing control rules for a serial pole-zero vocal tract model",
   "original": "e93_1705",
   "page_count": 4,
   "order": 395,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "Our rule-based multi-lingual text-to-speech system uses a synthesizer based on the source-filter theory of speech production. The voice source and noise source are implemented in a conventional manner: the voice source is the mathematical function and the noise source is a random generator producing noise with a Gaussian amplitude distribution. The vocal tract filter is not conventional: we employ a pole-zero (ARMA) filter, implemented as a cascade of second order resonators and antiresonators. The development of effective rules to control the ARMA vocal tract proved to be more difficult than anticipated, mainly because the physical interpretation of the zeros may change abruptly. From a system control point of view it became clear that zeros and poles cannot be allowed to move independently. Moreover, it appeared that the behaviour of a time-varying ARMA system is sensitive to internal group delays that are immaterial in stationary systems. The paper explains the mathematics of the problem of simultaneous control of pole and zero parameters in an ARMA filter in detail. Next, the solution implemented in our rule-based text-to-speech system is described.\n",
    "Keywords: text-to-speech, pole-zero synthesizer\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-389"
  },
  "nakajima93_eurospeech": {
   "authors": [
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "English speech synthesis based on multi-layered context oriented clustering; towards multi-lingual speech synthesis",
   "original": "e93_1709",
   "page_count": 4,
   "order": 396,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "In this paper, we propose a new synthesis unit learning method aiming at multi-lingual speech synthesis and describe its application to English speech synthesis. The method termed Multi-Layered Context Oriented Clustering (ML-COC), is a generalized framework of the COC method which has been applied to Japanese speech synthesis. The conventional COC method produces a set of phonetic context dependent units through a cluster splitting process. In ML-COC, the notion of context is generalized and the factors other than phonetic context such as stressing and syntactical boundaries, are taken into account to capture the richer phoneme variations of English. A synthesis unit generation experiment shows that ML-COC produces about three times as many synthesis units as the conventional COC (Single-Layered COC: SL-COC) method, the average of inner-cluster variances of ML-COC units is 20% lower than that of SL-COC, and each ML-COC unit has about twice as many contexts as each SL-COC unit on average. These results suggest that the ML-COC synthesis units reflect the phonological structure of English much more conscientiously than do the SL-COC units.\n",
    "Keywords: speech synthesis, multi-lingual speech synthesis, context dependent unit, phonetic context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-390"
  },
  "tuerk93b_eurospeech": {
   "authors": [
    [
     "Christine",
     "Tuerk"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Speech synthesis using artificial neural networks trained on cepstral coefficients",
   "original": "e93_1713",
   "page_count": 4,
   "order": 397,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "Traditional synthesis systems often rely on a large set of rules and a hand-crafted set of synthesis parameters in order to produce output speech. Gathering the synthesis parameters and developing the rule set are very labour intensive tasks. This paper offers an alternative to these labour intensive tasks. A set of artificial neural networks (ANNs) are used to produce the filter parameters which drive a synthesiser. This set of ANNs is trained on data that is gathered fully automatically. The networks offer a storage-efficient means of synthesis without the need for explicit rule enumeration. The networks have the capability to produce temporal variation within a phonetic segment and differing outputs when input contexts are varied. Furthermore, the distributed architecture of the networks enables them to produce reasonable outputs when faced with novel inputs. In addition, a feedback mechanism incorporated into the architecture creates smooth transitions at segment boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-391"
  },
  "renals93_eurospeech": {
   "authors": [
    [
     "Steve",
     "Renals"
    ],
    [
     "David",
     "MacKay"
    ]
   ],
   "title": "Bayesian regularisation methods in a hybrid MLP-HMM system",
   "original": "e93_1719",
   "page_count": 4,
   "order": 398,
   "p1": "1719",
   "pn": "1722",
   "abstract": [
    "We have applied Bayesian regularisation methods to multi-layer percepuon (MLP) training in the context of a hybrid MLP-HMM (hidden Markov model) continuous speech recognition system. The Bayesian framework adopted here allows an objective setting of the regularisation parameters, according to the training data. Experiments have been carried out on the ARPA Resource Management database,\n",
    "Keywords: Speech Recognition, HMM, Neural Network, Regularisation, Bayesian\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-392"
  },
  "schmid93_eurospeech": {
   "authors": [
    [
     "P.",
     "Schmid"
    ],
    [
     "Ronald",
     "Cole"
    ],
    [
     "M.",
     "Fanty"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "M.",
     "Haessen"
    ]
   ],
   "title": "Real-time, neural network-based, French alphabet recognition with telephone speech",
   "original": "e93_1723",
   "page_count": 4,
   "order": 399,
   "p1": "1723",
   "pn": "1726",
   "abstract": [
    "We describe a real-time speaker-independent French alphabet recognizer that performs with sufficient accuracy for commercial use. The system (a) digitizes a sequence of letters separated by brief pauses and computes a RASTA-PLP spectral representation, zero-crossing rate and peak-to-peak amplitudes of the waveform; (b) uses a neural network to assign 23 phonetic category labels to successive time frames; (c) performs an initial segmentation of the speech by mapping the phonetic label scores for each frame to pronunciation models for each letter using a modified Viterbi search; (d) performs a second classification of each hypothesized letter using the segment boundaries provided by the first-pass segmentation, producing a set of 26 letter scores plus a score for the category \"Not-A-Letter\"; and (e) uses the letter scores (plus the score for the category \"Not-A-Letter\") to identify the spelled word from a data base. The system has been evaluated on calls that were not used for training either network. The system achieved 84.4% first choice letter recognition accuracy on the test set. The system has also been evaluated on 84 spelled names from different callers where it achieved 92.8% correct recognition of the 84 spelled names contained in a database of 50,000 names. The final system has been optimized to run in real-time on a PC-board based on a single DSP TMS320C30. The two passes described above are performed in real-time by the DSP while the name search (up to 50,000 names) is performed (as letters are recognized) by the PC.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-393"
  },
  "rigoll93_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Joint optimization of multiple neural codebooks in a hybrid connectionist-HMM speech recognition system",
   "original": "e93_1727",
   "page_count": 3,
   "order": 400,
   "p1": "1727",
   "pn": "1729",
   "abstract": [
    "This paper proposes a new approach for a hybrid connectionist-HMM speech recognition system. The system consists of a multi-feature HMM-based recognition module using three different neural networks as multiple neural codebooks. Each neural network receives a different feature (i.e. cepstrum, delta cepstrum, and delta power) as input and generates a vector quantizer label obtained from the firing neuron in the output layer. The neural networks are first trained separately using a special self-organizing information theory-based learning method. A 26% error reduction is obtained with this method, compared to the performance of the same system using multiple k-means vector quantizers with the same codebook size. In a second training phase, the neural codebooks are further refined by extending the information theory-based training criterion into a joint criterion reflecting the joint information content and the dependencies of the three different label streams. This further improves the error reduction rate to 30%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-394"
  },
  "kurimo93_eurospeech": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Using LVQ to enhance semi-continuous hidden Markov models for phonemes",
   "original": "e93_1731",
   "page_count": 4,
   "order": 401,
   "p1": "1731",
   "pn": "1734",
   "abstract": [
    "Experiments are made to enhance the discrimination ability of the SCHMMs by applying Learning Vector Quantization. The SCHMMs are used for the modeling of phonemes in a speaker-dependent speech recognition application to create the phonetic transcriptions of spoken utterances. The probability density functions for the cepstral feature vectors produced in each state of each model are modeled by mixtures of multivariate Gaussian density functions. The mean vectors of the Gaussian densities are chosen by clustering the feature vectors of the training samples by using the Self-Organizing Map (SOM). Then the Gaussians are modified to correspond better to the Bayesian decision surfaces between phonemes by tuning the mean vectors by the LVQ. The experiments indicate that by this careful placement of the mean vectors the recognition error rates for the SCHMMs decrease significantly. LVQ algorithms can also be successfully applied after Baum-Welch or Viterbi training to slightly modify the Gaussians using training samples which would other- wise be incorrectly recognized. This kind of error corrective tuning drops out some of the recognition errors also in the test data.\n",
    "Keywords: HMMs, LVQ, SOM, semi-continuous\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-395"
  },
  "aibar93_eurospeech": {
   "authors": [
    [
     "Pablo",
     "Aibar"
    ],
    [
     "Francisco",
     "Casacuberta"
    ]
   ],
   "title": "An improvement of the two-level DP matching algorithm using k-NN techniques for acoustic-phonetic decoding",
   "original": "e93_1735",
   "page_count": 4,
   "order": 402,
   "p1": "1735",
   "pn": "1738",
   "abstract": [
    "The distance-based approach has been used successfully in Isolated and Connected Word Recognition, but this approach has not been explored adequately for Acoustic-Phonetic Decoding. The K-Nearest Neighbor criterion constitutes one of the most powerful classification rules in Pattern Recognition based on the Decision Theory. This rule can be combined with an also powerful but less used algorithm such as the Two-Level algorithm. The main problem with this algorithm was its computational requirements, but this can be reduced adequately. In this paper, we present the use of the combination K-Nearest Neighbor and Two-Level algorithm for Acoustic-Phonetic Decoding. The results that are obtained by this combination are also presented, and they are higher than the ones obtained with more conventional methodologies.\n",
    "Keywords: Acoustic-Phonetic Decoding, Distance-based approach, Two-Level algorithm, K-Nearest Neighbor rule\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-396"
  },
  "bourlard93_eurospeech": {
   "authors": [
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Jean-Marc",
     "Boite"
    ],
    [
     "Bart",
     "D'Hoore"
    ],
    [
     "Marco",
     "Saerens"
    ]
   ],
   "title": "Performance comparison of hidden Markov models and neural networks for task dependent and independent isolated word recognition",
   "original": "e93_1925",
   "page_count": 4,
   "order": 403,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "In this paper, we compare the recognition performance which can be achieved for speaker independent isolated word recognition over the telephone line by standard phonemic Hidden Markov Models (HMMs) with a hybrid approach using HMMs together with a Multilayer Perceptron (MLP) to estimate the HMM emission probabilities. Recently, the latter approach has been shown particularly efficient for a large vocabulary, speaker independent, continuous speech recognition task (i.e. DARPA Resource Management database). Since this approach seems to be more robust for simple context-independent phoneme models, the aim of this paper is to compare the performance which can be achieved in the case of task independent training, i.e. when the phonemic models are trained on a database which does not contain the words used in the targeted application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-397"
  },
  "haffner93_eurospeech": {
   "authors": [
    [
     "Patrick",
     "Haffner"
    ]
   ],
   "title": "Connectionist speech recognition with a global MMI algorithm",
   "original": "e93_1929",
   "page_count": 4,
   "order": 404,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "This paper presents a new connectionist architecture which incorporates the Forward-Backward (ap) alignment procedure. This generalises the Viterbi alignment procedure which exists in most \"hybrid\" connectionist systems used for speech recognition. Our aB-TDNN architecture extends Multi-State Time Delay Neural Networks (MS-TDNNs) [4] to make them theoretically more consistent with the Back-Propagation training procedure and experimentally more robust. With the ap-TDNN, simple modelling assumptions about time alignment suggest choices in the architecture and the objective function which leads to improvements in performance. In particular, we show the possibility to train the aB-TDNN with a global one-pass algorithm based on Maximum Mutual Information (MMI): this system was trained on two speaker independent word recognition tasks, without any bootstrapping, within a reasonable learning time and with good performances.\n",
    "Keywords: Neural Networks, Forward-Backward Alignment, MMIE, TDNN\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-398"
  },
  "boiteau93_eurospeech": {
   "authors": [
    [
     "Denys",
     "Boiteau"
    ],
    [
     "Patrick",
     "Haffner"
    ]
   ],
   "title": "Connectionist segmental post-processing of the n-best solutions in isolated and connected word recognition task",
   "original": "e93_1933",
   "page_count": 4,
   "order": 405,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "Studies have shown that the segmental discriminative power of Neural Networks (NN) can be very useful to improve the performances of temporal sequence decoders like Hidden Markov Models (HMM) [1,2]. In this paper, we present a new connectionist segmental approach applied to the reordering of the N-best solutions provided by a HMM. The global system uses a segmental recognition framework, where phonetic segments are provided by the alignement of a speech utterance on a HMM. The scores of each solution are obtained by a two-level architecture. The first one is a «One Net One Class» connectionist architecture which provides phonetic scores for each phoneme belonging to a word, where phonetic scores can be interpreted as measures of validity of each segment labelling. The second level computes each word score as a product of the phonetic scores. In a final step, the NN scores and the HMM scores are combined for each of the N-best solutions in an optimal way in order to minimize word classification errors. The present system, with the knowledge of the 5-best solutions, leads to a 15 to 20% reduction of the error rate when compared to the one obtained with the HMM alone on several speaker-independent databases recorded over telephone networks.\n",
    "Keywords: N-best solutions, segmental connectionist process- ing, segment labelling validation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-399"
  },
  "martens93_eurospeech": {
   "authors": [
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Annemie",
     "Vorstermans"
    ],
    [
     "Nick",
     "Cremelie"
    ]
   ],
   "title": "A new dynamic programming/multi-layer perceptron hybrid for continuous speech recognition",
   "original": "e93_1937",
   "page_count": 4,
   "order": 406,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "A new segment-based Dynamic Programming (DP)/Multi-Layer Perceptron (MLP) hybrid for speaker-independent continuous speech recognition is described and evaluated. The system incorporates an auditory model front-end, an initial segmentation stage, a MLP-based phonetic segmentation and classification module, and a lexical network. The recognition is described as a search for the most likely lexical and phonetic decoding of the speech, given the acoustical observations. The search is performed by a Dynamic Programming (DP) algorithm.\n",
    "Keywords: Continuous speech recognition, neural networks\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-400"
  },
  "robinson93_eurospeech": {
   "authors": [
    [
     "Tony",
     "Robinson"
    ],
    [
     "L.",
     "Almeida"
    ],
    [
     "Jean-Marc",
     "Boite"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Frank",
     "Fallside"
    ],
    [
     "Michael M.",
     "Hochberg"
    ],
    [
     "D.",
     "Kershaw"
    ],
    [
     "P.",
     "Kohn"
    ],
    [
     "Y.",
     "Konig"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "J. P.",
     "Neto"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Marco",
     "Saerens"
    ],
    [
     "C.",
     "Wooters"
    ]
   ],
   "title": "A neural network based, speaker independent, large vocabulary, continuous speech recognition system: the WERNICKE project",
   "original": "e93_1941",
   "page_count": 4,
   "order": 407,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "This paper describes the research underway for the ESPRIT WERNICKE project. The project brings together a number of different groups from Europe and the US and focuses on extending the state-of-the-art for hybrid hidden Markov model/connectionist approaches to large vocabulary, continuous speech recognition. This paper describes the specific goals of the research and presents the work performed to date. Results are reported for the resource management talker-independent recognition task. The paper concludes with a discussion of the projected future work.\n",
    "Keywords: Recognition, Neural Nets, HMM\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-401"
  },
  "bothe93_eurospeech": {
   "authors": [
    [
     "Hans-H.",
     "Bothe"
    ],
    [
     "Frauke",
     "Rieger"
    ],
    [
     "Robert",
     "Tackmann"
    ]
   ],
   "title": "Visual coarticulation effects in syllable environment",
   "original": "e93_1741",
   "page_count": 4,
   "order": 408,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "This paper describes a method of analyzing visual articulation movements and determining coarticulation effects with respect to syllables. A modified DTW process (Dynamic Time Warping) has been used as a measure of local similarity and applied on the visual correlates of several frequent German syllables. These investigations lead to speaker dependent sets of coarticulation data. They are suggested to be used for a model-based computer-animation program showing an artificial speaker on the screen.\n",
    "Keywords: Coarticulation, DTW, Lip Movements, Local Similarity, Syllables, Visual Speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-402"
  },
  "shadle93_eurospeech": {
   "authors": [
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "J. N.",
     "Carter"
    ],
    [
     "T. P.",
     "Monks"
    ],
    [
     "J.",
     "Field"
    ]
   ],
   "title": "Depth measurement of face and palate by structured light",
   "original": "e93_1745",
   "page_count": 4,
   "order": 409,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "This paper describes a system that performs depth measurement of complex objects, and presents preliminary results using the system to measure an electropalatograph (EPG) palate and the lower face of a speaker. The depth measurements can be used to enhance electropalatography images, to reconstruct the face shape on a screen where it can be observed from any angle, to allow automatic distance measurements on EPG patterns, and to derive acoustically-significant parameters such as mouth shape during speech. The system uses structured light (parallel lines with a carefully coded colour sequence) projected onto the object, which is then recorded with a video camera; the images are then post-processed. Although the essential aspects of this process are the same for static and dynamic objects, the two cases present different experimental problems. Calibration methods and factors affecting resolution are discussed, as is comparison of this system to an existing double-video projection system developed in Grenoble.\n",
    "Keywords: Articulatory synthesis, face shape data, electropalatography, structured light\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-403"
  },
  "boe93_eurospeech": {
   "authors": [
    [
     "Louis-Jean",
     "Boe"
    ],
    [
     "Sonia",
     "Kandel"
    ],
    [
     "Annie",
     "Chappelet"
    ],
    [
     "Tahar",
     "Lallouache"
    ]
   ],
   "title": "Visiolab: a multimedia environment for the study of bimodal speech perception",
   "original": "e93_1749",
   "page_count": 4,
   "order": 410,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "A brief review of relevant phenomena occurring in bimodal speech perception is presented to show that communication systems must consider psycholinguistic factors in the conception of optimally efficient devices. This paper presents VisoLab, a highly convivial multimedia environment for the study of bimodal speech perception. VisioLab is conceived to provide researchers with relevant data on labial dynamics and geometry, as well as the corresponding acoustic information, of the utterances they intend to study. In addition, VisioLab enables the quantification of the perceptual effects resulting from the manipulation of the film quality (fps, dimension, definition and resolution of the image) and sound (sampling frequency, number of bits, signal to noise ratio), and/or their respective display timings. VisioLab is therefore a useful tool for the study of bimodal speech phenomena and for testing the effects on message comprehension due to technical problems arising in communication systems.\n",
    "Keywords: bimodal speech perception, visual and acoustic information, multimedia, sound and video-editing, QuickTime\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-404"
  },
  "robertribes93_eurospeech": {
   "authors": [
    [
     "Jordi",
     "Robert-Ribes"
    ],
    [
     "Tahar",
     "Lallouache"
    ],
    [
     "Pierre",
     "Escudier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Integrating auditory and visual representations for audiovisual vowel recognition",
   "original": "e93_1753",
   "page_count": 4,
   "order": 411,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "We present two models (issued from psychology) of audiovisual integration for speech perception. We propose an implementation of these models for audiovisual vowel recognition and we compare the results of these models in relation to their efficiency for speech recognition in noise.\n",
    "Keywords: audiovisual speech recognition and perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-405"
  },
  "baungaard93_eurospeech": {
   "authors": [
    [
     "Bo",
     "Baungaard"
    ],
    [
     "Jorn Stern",
     "Nielsen"
    ]
   ],
   "title": "Speech recognition over packetized voice systems",
   "original": "e93_1781",
   "page_count": 4,
   "order": 412,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "This paper reports the results from an assessment of speech recognition over a packetized voice system, which applies Adaptive Pulse Code Modulation (ADPCM) with different compression rates. In the future the penetration of packetized voice systems in the telephone network will increase. As speech recognition is foreseen to play an important role in services in the network, the speech recognition systems must be robust to the degradation introduced by the coding/decoding, which often is used in packetized voice systems. Three ADPCM coding schemes are assessed: 32 kbit/s, 24 kbit/s, and 16 kbit/s. A test with 64 kbit/s PCM is also conducted. All tests are carried out on a commercially available packetized voice system. The assessment is based on speaker independent recognition of isolated words using Continuous Hidden Markov Models (CHMM). Both whole word and triphone models are applied. The results clearly show that performance depends on the applied coding scheme. Performance decreases when training and test data have different coding schemes, and especially for 16 kbit/s a dramatic decrease is observed. Further is the capability to perform correct rejection highly influenced by the applied coding scheme.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-406"
  },
  "jenkins93_eurospeech": {
   "authors": [
    [
     "I. W. G.",
     "Jenkins"
    ]
   ],
   "title": "Voice applications on BT's derived services network",
   "original": "e93_1785",
   "page_count": 4,
   "order": 413,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "BT provides and supports advanced voice services via its Derived Services Network (DSN) which overlays the normal telephone network. This paper examines the latest network features to support independent service providers and its own managed voice services, showing how they can be combined in a powerful way to serve high call volume applications such as telemedia advertising.\n",
    "Keywords: Customer Access Manager, Mid-Call Diversion, Premium Rate Service, Remote Download, Remote Update, Service Management, Telemedia, Televote\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-407"
  },
  "magadur93_eurospeech": {
   "authors": [
    [
     "Jean-Yves",
     "Magadur"
    ],
    [
     "Frédéric",
     "Gavignet"
    ],
    [
     "Francois",
     "Andry"
    ],
    [
     "Francis",
     "Charpentier"
    ]
   ],
   "title": "A French oral dialogue system for flight reservations over the telephone",
   "original": "e93_1789",
   "page_count": 4,
   "order": 414,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "This paper presents the French version of the SUNDIAL speech understanding and dialogue system, which deals with flight enquiry and reservation over the telephone network.\n",
    "The system is divided into three main components. The speech recognizer uses hidden Markov models for the French phonemes, and a limited set of triphones and words of special importance for the application. The speech recognizer is connected to a linguistic analyzer, which parses a lattice of words and returns the best grammatical path. The core of the system is a dialogue manager which controls the progress of the dialogue, drives the understanding and answering interfaces, and accesses the flight database. It handles various dialogue phenomena, like confirmations, contestations, over-informative answers, repetition requests.\n",
    "At the end of the project, the system has been tested on a set of semi-naive users. Evaluation results, indicating a 70% understanding rate and 60% transaction success, are presented in this paper. A qualitative analysis of these results is given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-408"
  },
  "kuroiwa93_eurospeech": {
   "authors": [
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Naomi",
     "Inoue"
    ],
    [
     "Izuru",
     "Nogaito"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ],
    [
     "Makoto",
     "Shouzakai"
    ],
    [
     "Kunihiko",
     "Owa"
    ],
    [
     "Masahiko",
     "Takahashi"
    ],
    [
     "Ryuuji",
     "Matsumoto"
    ]
   ],
   "title": "A voice-activated extension telephone exchange system",
   "original": "e93_1793",
   "page_count": 4,
   "order": 415,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "A prototype of an extension telephone exchange system was developed for a company of about 200 employees, which connects a call to an extension telephone automatically by recognizing input sentences and, if necessary, asks questions to solve ambiguities. Insofar as concerns real-time speaker-independent continuous speech recognition, special hardware was designed using 9 DSPs (TMS320C30) for parallel processing of acoustic analysis, HMM probability calculation and Viterbi network search controlled by FSN grammar. An echo canceller is used to cancel the system announcement returned through the telephone-hybrid and to enable the system to detect user utterances at any time. As for speech output, a Klatt type text-to-speech hardware is used as well as the ADPCM decoder for the fixed form output. A preliminary test shows that 1) S3 % of the calls are correctly connected and 15 % of calls are rejected, and 2) the average time spent for the connection is about 50 seconds. In addition, the system has additional speech data storage facilities for constructing a large scale telephone speech database. Ongoing data collection is also described in the paper.\n",
    "Keywords: Telephone speech recognition, Real-time hardware\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-409"
  },
  "ortel93_eurospeech": {
   "authors": [
    [
     "William C. G.",
     "Ortel"
    ],
    [
     "Dina",
     "Yashchin"
    ]
   ],
   "title": "The VOIS project in retrospect",
   "original": "e93_1797",
   "page_count": 4,
   "order": 416,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "The Voice Operated Intercept Service (VOIS) automates the called-number Operator Number Identification (ONI) function in the New England Telephone Company call intercept system. The system makes use of automatic speech recognition (ASR) devices that were commercially available at the time of deployment in early 1990. The ASR devices are used whenever a caller does not respond by DTMF. Initially, VOIS handled 20,000 calls per day, coming from nearly 200 end-office switches: calls that would previously would have been routed to intercept operators. The older switching technology that required ONI for the intercept service is now essentially phased out. After its three years of deployment in the telephone network, VOIS has processed about 9 million calls. Its effectiveness has remained essentially the same throughout the three-year period: Speech recognition has been employed in 45% of those calls that are successfully automated. Traffic to ONI operators has been reduced by 85%. Calls for which ASR results do not meet predetermined criteria are forwarded to an ONI operator. Analysis of a speech database collected after deployment suggests ways in which the process by which ASR results are accepted could have been modified to increase the proportion of automated calls.\n",
    "Keywords: Automatic Speech Recognition, Speaker-Independent ASR, Telephone Operator Services, Operator Number Identification, ASR Error Detection\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-410"
  },
  "lleida93b_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Arturo",
     "Moreno"
    ]
   ],
   "title": "TELEMACO - a real time keyword spotting application for voice dialling",
   "original": "e93_1801",
   "page_count": 4,
   "order": 417,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "The problem of detecting a given set of words in fluent speech is one of the most interesting topics in speech recognition for practical real time applications. This paper present the TELEMACO system for automatic voice dialling which is based on the use of the keyword spotting technology to detect the dialling commands in fluent speech used by the IBERCOM Spanish telephone system. The user interface is based on a PC computer with a DSP board. The DSP board runs the speech recognition task and the interaction with the telephone line. The keyword vocabulary is composed by commands to dial, answer, hang-up, cancel, recall, store, etc. Each keyword is modeled by means of a discrete Hidden Markov Model. To model the non-keyword speech, syllabic fillers models and background models are used. The keyword spotting algorithm is a null grammar time-synchronous Viterbi search with two search spaces. The first search is over all the models (keywords and fillers) and the second search is only over the filler model. Thus, we can compare the behaviour of the filler model with the candidate keyword for each detection and decide if the keyword has been uttered or not. This process is done frame by frame. When a keyword is detected, the DSP board send the recognition word to the PC to take the corresponding action. The system has been implemented in a Windows environment.\n",
    "Keywords: Keyword spotting, hidden Markov models, filler models, voice dialling\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-411"
  },
  "wyard93b_eurospeech": {
   "authors": [
    [
     "Peter",
     "Wyard"
    ]
   ],
   "title": "The relative importance of the factors affecting recogniser performance with telephone speech",
   "original": "e93_1805",
   "page_count": 4,
   "order": 418,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "An experiment to determine the relative importance of the factors affecting recogniser performance with telephone speech is described. The design was controlled and balanced, and an ANOVA analysis was performed. Results are presented for single effects and interactions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-412"
  },
  "burger93_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Burger"
    ],
    [
     "Ulrich",
     "Schultheiß"
    ]
   ],
   "title": "A robust acoustic echo canceller for a hands-free voice-controlled telecommunication terminal",
   "original": "e93_1809",
   "page_count": 4,
   "order": 419,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "For a voice-controlled telecommunication terminal designed at the Forschungs- und Technologiezentrum of Telekom, a comfortable hands-free unit has been developed and implemented on two DSP-boards. Because of the well-known disadvantages of conventional speakerphone circuits this hands-free unit contains, as a central element, a canceller for acoustic echoes. This canceller is implemented as an adaptive FIR filter with approximately 1000 coefficients. A sampling rate of 8 kHz assumed, this allows more than 120 ms of an impulse response of a room to be cancelled. The coefficients of the canceller are adjusted by an NLMS algorithm supported by adaptive prewhitening filters. A special stepsize control renders the canceller robust against disturbing signals. As part of the hands-free unit, moreover, a canceller with 50 coefficients for the echoes of the hybrid and a weighting balance have been implemented. The balance is only intended to add the necessary attenuations which are not provided by the cancellers in order to ensure a stable and comfortable hands-free operation at any time.\n",
    "Keywords: Voice Controlled Telecommunication Terminal, Hands-free Telephones, Echo Cancellation, Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-413"
  },
  "hart93_eurospeech": {
   "authors": [
    [
     "J. E.",
     "Hart"
    ],
    [
     "P. A.",
     "Naylor"
    ],
    [
     "O.",
     "Tanrikulu"
    ]
   ],
   "title": "Polyphase allpass IIR structures for sub-band acoustic echo cancellation",
   "original": "e93_1813",
   "page_count": 4,
   "order": 420,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "The advantages and current limitations of sub-band approaches to echo cancellation are reviewed. Polyphase allpass IIR half-band decimators and interpolators are presented. Their performance is compared to QMF structures in NLMS sub-band echo cancellation for hands-free telephone signals recorded in a car. The polyphase allpass IIR case is shown to give around 2 dB more ERLE with one fifth of the number of multiplies compared to direct form QMF.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-414"
  },
  "monaghan93_eurospeech": {
   "authors": [
    [
     "James",
     "Monaghan"
    ],
    [
     "Christine",
     "Cheepen"
    ]
   ],
   "title": "Speech input systems and their effect on written language skills",
   "original": "e93_1817",
   "page_count": 4,
   "order": 421,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "The aim of the Computer Aided Learning Enhancement project (CALE) at the University of Hertfordshire is to provide a totally hands-free interface to a PC based computer system, for use by disabled school children. The project focusses on two major aspects - firstly the provision of a human centred, appropriately customised system designed to compensate for manual disability, and secondly the enhancement of the learning process and consequent improvement in skills resulting from the introduction of the interface into the learning environment. This paper deals with the latter aspect, drawing directly on examples from a particular case study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-415"
  },
  "olaszy93_eurospeech": {
   "authors": [
    [
     "Gabor",
     "Olaszy"
    ],
    [
     "Geza",
     "Nemeth"
    ]
   ],
   "title": "Voxaid: an interactive speaking communication aid software for the speech impaired",
   "original": "e93_1821",
   "page_count": 4,
   "order": 422,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "A speech communication aid for the vocally handicapped containing an unlimited text-to-speech synthesizer (MULTIVOX built into a laptop, notebook etc.) and a special text-creator program (VOXAID) for quick text generation for communication will be introduced. The input of this communication aid is text (from the keyboard) and the output is synthetic speech. The VOXAID software offers two forms for communication, a simple level with limited vocabulary (mainly for hospital environments) and an advanced level (for trained keyboard users) of creating unlimited texts. Telephone conversations can be performed by the system too. Synthetic speech can be transmitted to the microphone of a telephone receiver by means of an acoustic adaptor. Testing of this system is going on in a hospital in Hungary. Further examinations will be carried out in Hungary and abroad, supported by the COST-219 European project.\n",
    "Keywords: speech synthesis, vocally handicapped, text creator, vocal communication\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-416"
  },
  "hartmann93_eurospeech": {
   "authors": [
    [
     "U.",
     "Hartmann"
    ],
    [
     "K.",
     "Hermansen"
    ],
    [
     "F. K.",
     "Fink"
    ]
   ],
   "title": "Feature extraction for profoundly deaf people",
   "original": "e93_1825",
   "page_count": 4,
   "order": 423,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "Profoundly deaf people only have a minor frequency range available for reception of information in speech signals. These people do not benefit sufficiently from standard hearing aids. To overcome this problem we have developed a technique for transforming speech signals from one frequency range to another maintaining as much information and \"speech likeness\" as possible. Our concept includes 1) parametric modeling of the speech production system, 2) transformation of the speech production model to match the available frequency range, 3) resynthesis of the speech using the transformed model. In this way we can present the speech information of interest in a frequency range at choise. Supplementary, this concept is believed to reduce wideband background noise which is a problem for hearing disabled as well as for people with normal hearing ability. The technique is well suited for real time implementation (VLSI), primarily caused by numeric robustness of the algorithms. An example - a transformed danish token \"Polsevognen\" - is used for highlighting the benefits from using the technique.\n",
    "Keywords: Parametric modelling, Pseudodecom-position, Transposer, Speech enhancement, Noise reduction, Speech intelligibility\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-417"
  },
  "hauenstein93_eurospeech": {
   "authors": [
    [
     "Alfred",
     "Hauenstein"
    ]
   ],
   "title": "Architecture of a 10,000 word real time speech recognizer",
   "original": "e93_1829",
   "page_count": 4,
   "order": 424,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "Typical implementations of nowadays large vocabulary continuous speech recognizers fail to meet real time considerations. Therefore we present a twofold approach to reach real time operation of the search task for a 10 000 word continuous speech recognizer. At first, we develop search algorithms, which reduce the computations needed and are suitable for hardware implementation. Secondly, a single-chip VLSI coprocessor is introduced, which implements the algorithms developed. The coprocessor is designed in a 1.0 pm standard cell technology, covers an area of approximately 87 mm² and has a pincount of 284.\n",
    "Keywords: continuous speech recognition, real time operation, large vocabularies, search algorithms, hardware architecture.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-418"
  },
  "hermann93_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Hermann"
    ],
    [
     "Harald",
     "Eckhardt"
    ],
    [
     "Michael",
     "Trompf"
    ],
    [
     "Heidi",
     "Hackbarth"
    ]
   ],
   "title": "A noise-robust real-time word recognition hardware module",
   "original": "e93_1833",
   "page_count": 4,
   "order": 425,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "Adverse environmental conditions cause a decrease of performance in automatic speech recognition. To cope with this problem, a real-time hardware module for robust distortion-insensitive word recognition has been developed. In addition, different coding algorithms for transmission and dialog announcements have been implemented. Thus complemented by appropriate speech processing software, this general-purpose unit provides all flexibility needed for the integration into systems like mobile telephone, voice-activated control of subsets and information systems.\n",
    "Keywords: real-time implementation, robust feature extraction, background noise, neural net noise reduction, dynamic normalization, template adaptation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-419"
  },
  "koo93_eurospeech": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ]
   ],
   "title": "KARS: a speaker-independent, vocabulary-independent speech recognition system",
   "original": "e93_1837",
   "page_count": 4,
   "order": 426,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "In this paper, we present an information retrieval system based on recognition technology (KARS: Korea Telecom Automatic Recognition System). If a user commands isolated Korean words over the telephone according to the voice instruction, the system announces telephone numbers, locations and activities of sections in Korea Telecom Research Center. The system is a HMM (hidden Markov model)-based speaker-independent, vocabulary-independent isolated speech recognizer with 129 word recognition vocabularies. For the vocabulary-independence, we use the condext-dependent phones and also present the feature extraction method in which the features for training may be different from those for recognition. We have achieved speaker-independent, vocabulary-independent recognition results of 92.6%.\n",
    "Keywords: Vocabulary-Independence, Speech Recognition System, EMM\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-420"
  },
  "mcinnes93_eurospeech": {
   "authors": [
    [
     "F. R.",
     "McInnes"
    ],
    [
     "J. A.",
     "Elliott"
    ],
    [
     "N. W.",
     "Ramsey"
    ],
    [
     "M. E.",
     "Forsyth"
    ],
    [
     "A. M.",
     "Sutherland"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "A parallel processing keyword recogniser for police national computer enquiries",
   "original": "e93_1841",
   "page_count": 4,
   "order": 427,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "A parallel processing speech recognition system designed for police database enquiries is described. The system has both connected word recognition and keyword spotting capabilities. The current hardware platform for the parallel implementation is a network of transputers.\n",
    "Keywords: speech recognition, keyword spotting, parallel processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-421"
  },
  "paoloni93_eurospeech": {
   "authors": [
    [
     "Andrea",
     "Paoloni"
    ],
    [
     "Torbjörn",
     "Svendsen"
    ],
    [
     "B.",
     "Kaspar"
    ],
    [
     "Denis",
     "Johnston"
    ],
    [
     "Gunnar",
     "Hult"
    ]
   ],
   "title": "Cost232: speech recognition over the telephone line",
   "original": "e93_1845",
   "page_count": 4,
   "order": 428,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "This paper reports on the various studies which have been undertaken within the COST232 project \"Speech Recognition over the Telephone Line\" and summarises the various conclusions which have been drawn so far. It also introduces and describes a proposed reference system and a large telephone based database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-422"
  },
  "hazan93_eurospeech": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Bo",
     "Shi"
    ]
   ],
   "title": "Individual variability in the perception of synthetic speech",
   "original": "e93_1849",
   "page_count": 4,
   "order": 429,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "This work was aimed at investigating individual variability in the perception of synthetic speech. A homogeneous group of 50 listeners was tested on a wide range of synthetic speech material of increasing complexity. This included intelligibility tests for nonsense VCV utterances, semantically-anomalous and meaningful sentences and speech pattern identification tests for plosive place and voicing contrasts in three vocalic contexts. Data was examined for evidence of different listener strategies. The implications of these results for synthetic speech intelligibility and enhancement are examined.\n",
    "Keywords: Synthetic speech, speech perception, individual differences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-423"
  },
  "ludovic93_eurospeech": {
   "authors": [
    [
     "Ye. K.",
     "Ludovic"
    ],
    [
     "V. V.",
     "Pilipenko"
    ],
    [
     "G. E.",
     "Tseitlin"
    ],
    [
     "L. I.",
     "Nagornaya"
    ],
    [
     "T.",
     "Terzian"
    ]
   ],
   "title": "Speech recognition system and its application for blind PC users",
   "original": "e93_1853",
   "page_count": 3,
   "order": 430,
   "p1": "1853",
   "pn": "1855",
   "abstract": [
    "The speech recognition system described below is designed to control application programs run under MS-DOS on IBM PC AT by separately pronounced words. The system is realized as a TSR-program interfacing the application program via the keyboard buffer. The system uses the Sound Blaster (or compatible) to digitize speech signal. A brief description of the system helping blind PC users to work on PC is given, thereby PC is controlled by voice commands and the keyboard is voiced.\n",
    "Keywords: speech signal, recognition, personal computer, blind users, learning, system performance\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-424"
  },
  "music93_eurospeech": {
   "authors": [
    [
     "Bradley",
     "Music"
    ],
    [
     "Claus",
     "Povlsen"
    ]
   ],
   "title": "The NLP module of a spoken language dialogue system for Danish flight reservations",
   "original": "e93_1859",
   "page_count": 4,
   "order": 431,
   "p1": "1859",
   "pn": "1862",
   "abstract": [
    "This paper describes the first implementation of an NLP module of a prototype spoken language dialogue system for use in Danish flight reservations. In order to handle spontaneous spoken input, the module parses robustly and allows a combination of word-spotting and a semantic analysis based on syntactic structures. The semantic analysis is accomplished via rules mapping from syntactic structures to semantic interpretations that are used as slot-fillers in frame-like semantic objects. The module will soon be augmented to integrate phrase-spotting and pattern-matching, with the grammar-writer maintaining full control over the level of integration of the these approaches via the lexicon, syntactic grammars and semantic mapping rules.\n",
    "Keywords: syntactic parsing, semantic mapping, flexible approach\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-425"
  },
  "clementino93_eurospeech": {
   "authors": [
    [
     "D.",
     "Clementino"
    ],
    [
     "L.",
     "Fissore"
    ]
   ],
   "title": "A man-machine dialogue system for speech access to train timetable information",
   "original": "e93_1863",
   "page_count": 4,
   "order": 432,
   "p1": "1863",
   "pn": "1866",
   "abstract": [
    "This paper describes the integration of the main components of a man-machine dialogue system for the Italian language, which allows information services to be accessed through the telephone line. The components are: 1) a large vocabulary speaker-independent continuous speech recogniser (AFE), 2) a natural language understanding stage (LP), 3) an intelligent dialogue manager (DM) and 4) a message generator (MG), including a high quality text-to-speech synthesiser. A prototype of an intermediate development state for E-mail access was presented in [lj. In the current implementation, a user can access a train information service through a PBX telephone line. A general overview of the system architecture will be given together with an evaluation of the real-time demonstrator performance through experienced and naive users.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-426"
  },
  "blomberg93b_eurospeech": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Kjell O. E.",
     "Elenius"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ],
    [
     "Roger",
     "Lindell"
    ],
    [
     "Lennart",
     "Neovius"
    ]
   ],
   "title": "An experimental dialogue system: waxholm",
   "original": "e93_1867",
   "page_count": 4,
   "order": 433,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "Recently we have begun to build the basic tools for a generic speech dialogue system, WAXHOLM. The main modules, their function and internal communication have been specified. The different components are connected through a computer network. A preliminary version of the system has been tested, using simplified versions of the modules. We will give a general overview of the system and describes some of the components in more detail. Application specific data are collected with the help of Wizard-of-Oz techniques. The dialogue system is used during the data collection and the wizard only replaces the speech recognition module.\n",
    "Keywords: dialogue system, speech recognition, speech synthesis, parsing, speech database, linguistic analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-427"
  },
  "eckert93b_eurospeech": {
   "authors": [
    [
     "Wieland",
     "Eckert"
    ],
    [
     "T.",
     "Kuhn"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "S.",
     "Rieck"
    ],
    [
     "A.",
     "Scheuer"
    ],
    [
     "Ernst G.",
     "Schukat-Talamazzini"
    ]
   ],
   "title": "A spoken dialogue system for German intercity train timetable inquiries",
   "original": "e93_1871",
   "page_count": 4,
   "order": 434,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "This paper focuses on the evaluation of the German Sundial Demonstrator maintaining interactive conversations via microphone and telephone with users. The word recognizer was implemented by the University of Erlangen and currently obtains on our test set a word accuracy of over 92% in a speaker-independent task with perplexity 111. We also participated in the design and implementation of the multilingual Dialogue Manager which is responsible for cooperative system behaviour. Over 100 prototype dialogues led to the current version which is continually being extended. The overall system performance is tested with semi- naive users. Each subject faces four intercity train timetable scenarios, two of them are given, the others depend on the subjects' personal choice. Global system evaluation is done according to performance measures like contextual appropriateness, transaction success and dia- logue completion rate.\n",
    "Keywords: Spoken dialogue system, Architecture, User trials, Evaluation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-428"
  },
  "labropoulou93_eurospeech": {
   "authors": [
    [
     "Kyriaki",
     "Labropoulou"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "A telephone banking system based on HMM keyword recognition",
   "original": "e93_1875",
   "page_count": 4,
   "order": 435,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "In this paper it is presented a telephone-banking system for providing user account information through the public telephone network. The system is based on both Keyword spotting and connected digits recognition. It has been designed to fulfil a number of requirements such as security and adaptability to new users without training. The former is guaranteed by double checking access to the system and the latter by minimal system needs, limited to code number and user name in phonetic alphabet format.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-429"
  },
  "lewin93_eurospeech": {
   "authors": [
    [
     "Ian",
     "Lewin"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "David",
     "Carter"
    ],
    [
     "Sue",
     "Browning"
    ],
    [
     "Keith",
     "Ponting"
    ],
    [
     "Stephen",
     "Pulman"
    ]
   ],
   "title": "A speech-based route enquiry system built from general-purpose components",
   "original": "e93_2047",
   "page_count": 4,
   "order": 436,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "The adaptation of existing general-purpose speech recognition and language understanding systems can greatly reduce the cost of developing applications. However, the components must have appropriate characteristics for this to be possible. Work is in progress to adapt two task-independent components, the AURIX speech recognizer and the CLARE language processor to create a system allowing spoken queries of the PC-based Autoroute route planning pack- age.\n",
    "Keywords: adaptability, general purpose, speech recognition, language understanding, AURIX, CLARE\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-430"
  },
  "yang93_eurospeech": {
   "authors": [
    [
     "Changwen",
     "Yang"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "The inks ATIS system and its n-best interface",
   "original": "e93_2051",
   "page_count": 4,
   "order": 437,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "We are establishing a spoken language interface to an Official Airline Guide (OAG) Database, to which the general public could eventually book air tickets directly with computer databases (including over the telephone). It consists of two ATIS (Air Travel Information System) natural language systems (a full linguistic system and a keyword-based robust system), an SQL (Standard Query Language) generator, a speech recognizer, a speech synthesizer, a text generator, an N-best algorithm and an N-best interface. This paper describes three of our successfully developed modules: (1) the full linguistic ATIS natural language system which can process not only an isolated imperative, question, assertion, but also a complete booking scenario; (2) the SQL generator which has been connected to the full linguistic system to realize generic public transactions with a computer database through natural language sentences; (3) the N-best interface which guarantees that the full linguistic system will accept N sentence candidates sent from the speech recognizer. It also connects the full linguistic system with the robust system.\n",
    "Keywords: Natural Language Processing, Spoken, Language System, Syntax, Semantics, Pragmatics\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-431"
  },
  "nitta93_eurospeech": {
   "authors": [
    [
     "T.",
     "Nitta"
    ],
    [
     "Y.",
     "Masai"
    ],
    [
     "J.",
     "Iwasaki"
    ],
    [
     "S.",
     "Tanaka"
    ],
    [
     "Bi",
     "Karwo"
    ],
    [
     "H.",
     "Matsu'ura"
    ]
   ],
   "title": "A multimodal directory guidance system with an interactive mechanism",
   "original": "e93_2055",
   "page_count": 4,
   "order": 438,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "In this paper, a keyword spotting unit (KeySpot) is first described. KeySpot can recognize 200 keywords in continuous speech or 1000 isolatedly spoken words. The unit also contains an adaptive noise canceller. Next, a multimodal, keyword-based spoken dialogue system (MultiksDial) including KeySpot is described. The system provides multiple input channels of spontaneous speech and touch, as well as multiple output channels of graphics and voice response. The system also provides three sensors to detect the user's behavior and to plan interactive strategies. Better usability is shown on comparison with an ordinary touch-screen system through the experiment of a directory guidance task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-432"
  },
  "bonneaumaynard93_eurospeech": {
   "authors": [
    [
     "H.",
     "Bonneau-Maynard"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "David",
     "Goodine"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "A French version of the MIT-ATIS system: portability issues",
   "original": "e93_2059",
   "page_count": 4,
   "order": 439,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "This paper presents our recent research in developing L'ATIS, a French version of the MIT Air Travel Information Service (ATIS) system used to interrogate a database derived from the Official Airline Guide (OAG). We have adopted an approach of accessing the English based system at the level of the semantic frame, so as to produce a language-independent meaning representation. Thus the same core back-end component as in the English-based version can be used, with only the input and output modules replaced by French versions. In addition, the input module uses the same mechanism to convert an input sentence to a semantic frame, with the English grammar rules and constraints being replaced by corresponding French versions. A common approach for language generation is also used for both systems. Once a core system in French was operational, data were collected in the form of typed queries so as to expand the rules and vocabulary, as well as spoken queries using a wizard-of-Oz (WOZ) setup. Preliminary speech recognition error rates as well as an informal analysis of the performance of the NL component are provided.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-433"
  },
  "glass93_eurospeech": {
   "authors": [
    [
     "James R.",
     "Glass"
    ],
    [
     "David",
     "Goodine"
    ],
    [
     "Michael",
     "Phillips"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A bilingual Voyager system",
   "original": "e93_2063",
   "page_count": 4,
   "order": 440,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "This paper describes our initial efforts at porting the VOYAGER spoken language system to Japanese. In the process we have reorganized the structure of the system so that language dependent information is separated from the core engine as much as possible. For example, this information is encoded in tabular or rule-based form for the natural language understanding and generation components. The internal system manager, discourse and dialogue component, and database are all maintained in language transparent form. Once the generation component was ported, data were collected from 40 native speakers of Japanese using a wizard collection paradigm. A portion of these data was used to train the natural language and segment-based speech recognition components. The system obtained an overall understanding accuracy of 52% on the test data, which is similar to our earlier reported results for English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-434"
  },
  "kroger93_eurospeech": {
   "authors": [
    [
     "Bernd J.",
     "Kröger"
    ]
   ],
   "title": "A gestural approach for controlling an articulatory speech synthesizer",
   "original": "e93_1903",
   "page_count": 4,
   "order": 441,
   "p1": "1903",
   "pn": "1906",
   "abstract": [
    "Our concept for controlling a speech synthesizer for German is based on articulatory gestures 11,21. It is distinguished from segmental approaches 131 by providing a concrete quantitative model of articulatory dynamics. It describes intragestural movement patterns as well as intergestural coordination (gestural phasing).\n",
    "Keywords: Speech production, speech synthesis, phonetics\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-435"
  },
  "boersma93_eurospeech": {
   "authors": [
    [
     "Paul",
     "Boersma"
    ]
   ],
   "title": "An articulatory synthesizer for the simulation of consonants",
   "original": "e93_1907",
   "page_count": 4,
   "order": 442,
   "p1": "1907",
   "pn": "1910",
   "abstract": [
    "We present a model of the lungs, glottis, and vocal tract, that computes numerically the acoustic output that results from the positions, motions, and tensions of the muscles involved. The model is designed to be able to produce almost any possible speech utterance.\n",
    "Keywords: Articulatory synthesis, Vocal-tract model\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-436"
  },
  "carlson93_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Vowel dynamics in a text-to-speech system some considerations",
   "original": "e93_1911",
   "page_count": 4,
   "order": 443,
   "p1": "1911",
   "pn": "1914",
   "abstract": [
    "The purpose of the present study is to increase the naturalness of the rule synthesis developed at the Department, especially positional variants of phonemes. We have in this study chosen the Swedish short vowel /e/, as this vowel is highly variable and can show a great degree of reduction. A statistical analysis of the second formant value as a function of the phoneme context (CVC) gave as result that the variability expressed as standard deviation was reduced by 38% on the training material and 24% on the test material. Analysis of the spectra of /e/ vowels showed that there was a tendency of acoustically different allophones to appear, that is, different spectral envelopes, due to context and position.\n",
    "Keywords: analysis, synthesis, reduction, vowels\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-437"
  },
  "frehr93_eurospeech": {
   "authors": [
    [
     "Ida",
     "Frehr"
    ],
    [
     "Marianne",
     "Elmlund"
    ],
    [
     "Henrik",
     "Nielsen"
    ]
   ],
   "title": "Improving the spectral balance of digital speech synthesis applied to a female, synthetic voice",
   "original": "e93_1915",
   "page_count": 4,
   "order": 444,
   "p1": "1915",
   "pn": "1918",
   "abstract": [
    "Formant synthesis of speech based on Fant's acoustic model has proved to give a good approximation to natural speech spectra. Unfortunately analogue and digital filters do not behave similarly, especially at frequencies near the half sampling frequency. This paper deals with the deviation between Fant's model and a digital realization and how to minimize this problem. The emphasis is on female vowels.\n",
    "Keywords: Speech synthesis, Female voice, Digital filters, Spectral tilt.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-438"
  },
  "ishikawa93_eurospeech": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Tadashi",
     "Ebihara"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "A new model of excitation for text-to-speech synthesis",
   "original": "e93_1919",
   "page_count": 4,
   "order": 445,
   "p1": "1919",
   "pn": "1922",
   "abstract": [
    "This paper describes a new model of excitation for text-to-speech synthesis. A periodic pulse train is widely used as excitation of voiced speech in a vocoder. However, with this simplified model, it is difficult to synthesize high-quality speech. We propose a new model which represents residual signal as averaged features and these fluctuation. In this method, spectral feature and averaged pitch period is obtained from residual signal, and excitation signal is generated by these parameters with adding fluctuation component. Analysis of many sentence utterances shows an obvious quantitative relation between the spectral fluctuation and energy of speech. Thus in the model, fluctuation component is controlled by energy. An evaluation experiment is carried out. The results show that high quality synthetic speech is derived with the model. Furthermore, informal listening test results are presented which confirm that our method is effective in text-to-speech synthesis.\n",
    "Keywords: excitation, vocoder, text-to-speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-439"
  },
  "charpillet93_eurospeech": {
   "authors": [
    [
     "Francois",
     "Charpillet"
    ],
    [
     "Joseph Di",
     "Martino"
    ]
   ],
   "title": "A level-building top-down parsing algorithm for context-free grammars in continuous speech recognition",
   "original": "e93_1947",
   "page_count": 3,
   "order": 446,
   "p1": "1947",
   "pn": "1949",
   "abstract": [
    "We present in this paper a dynamic programming parsing algorithm in the framework of continuous speech recognition. The language of the applications we want to deal with, is described using a context-free grammar given in Greibach normal form. The parsing algorithm follows the level-building technique and finds the p sentences matching the speech signal with the best scores. Our approach is different from the existing ones, in the sense that it is suboptimal but with a lower order of complexity. By suboptimal we mean that a syntactically correct sentence with a score belonging to the p best ones can be missed by our algorithm. This leads us to discuss the trade-off between quality of results on the one hand and the complexity of the algorithm on the other hand. As a conclusion, we will see that in practice, our approach yields results of comparable quality in spite of their slight theoritical corruption.\n",
    "Keywords: Dynamic Programming, Context-Free, Grammars, Continuous Speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-440"
  },
  "collingham93_eurospeech": {
   "authors": [
    [
     "Russell J.",
     "Collingham"
    ],
    [
     "Roberto",
     "Garigliano"
    ]
   ],
   "title": "Using anti-grammar and semantic categories for the recognition of spontaneous speech",
   "original": "e93_1951",
   "page_count": 4,
   "order": 447,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "This paper provides an introduction to the syntactic and semantic sub-system of AURAID: a speech recognition aid for use by deaf students in lectures. This sub-system produces a useful word recognition level from a continuous sequence of phonemes as could be provided by a continuous speech phoneme recognition system. The dynamic programming stage matches the phoneme input with a dictionary to produce a word lattice. The parsing stage makes use of an \"anti-grammar\" and semantic categories in order to determine the best sequence of words through the lattice. AURAID has a vocabulary of 2200 words and works in real-time using a simulated continuous speech phoneme recognition system (modelled on the performance of the DRA (UK) Speech Research Unit's Armada system). The phoneme error rate provided by this simulation is approximately 26%. Word recognition rates of approximately 85% have been achieved on sections of the simulated data using unrestricted speech. The simulated data is taken from real University lectures on the subject of software engineering.\n",
    "Keywords: continuous speech recognition, spontaneous speech, grammar, semantic categories\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-441"
  },
  "isotani93_eurospeech": {
   "authors": [
    [
     "Ryosuke",
     "Isotani"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speech recognition using particle n-grams and content-word n-grams",
   "original": "e93_1955",
   "page_count": 4,
   "order": 448,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "This paper proposes a new stochastic language model for speech recognition based on particle N-grams and content-word N-grams. The conventional word N-gram model is considered as effective for speech recognition; however, it represents only local constraints between successive words and lacks the ability to describe global syntactic or semantic relationships between words. In the proposed method the language model gives the N-gram probability of the word sequences, with attention given only to particles or to content words to represent more global constraints. As an application of this model to speech recognition, a post-processor was constructed to select the optimum sentence candidate from a phrase lattice obtained by a phrase recognition system. The proposed method out-performed a CFG-based method in recognition accuracy, which demonstrates its effectiveness in improving speech recognition performance.\n",
    "Keywords: speech recognition, stochastic language model, particles, content words\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-442"
  },
  "dupont93_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Dupont"
    ]
   ],
   "title": "Dynamic use of syntactical knowledge in continuous speech recognition",
   "original": "e93_1959",
   "page_count": 4,
   "order": 449,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "The control of continuous speech recognition by a context-free based language model requires a parsing process which may overload the acoustic decoding algorithm. We present a new approach to integrate such a language model in the search process. This approach extends the beam search Viterbi algorithm. In our case, the pruning technique not only selects the most likely acoustic hypotheses but also governs the dynamic expansion of a network structure. This algorithm is general enough to cope with the self-embedded recursivity of context-free languages and it favourably compares with other parsing techniques applied to spoken inputs. We present results which show that the syntactical knowledge may be efficiently included at the frame level of an acoustic decoding algorithm.\n",
    "Keywords: Continuous Speech Recognition, Context-Free Language Models, Beam Search Viterbi Algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-443"
  },
  "plante93_eurospeech": {
   "authors": [
    [
     "Fabrice",
     "Plante"
    ],
    [
     "Jocelyne",
     "Borel"
    ],
    [
     "Christian",
     "Berger-Vachon"
    ],
    [
     "Isabelle",
     "Kauffmann"
    ]
   ],
   "title": "Acoustic detection of laryngeal diseases in children",
   "original": "e93_1965",
   "page_count": 4,
   "order": 450,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "In this work, the authors present their results on the acoustic detection of laryngeal diseases in children. Four types of parameters are studied : cepstral distances, error of Linear Prediction (LP), perturbation parameters (frequency and amplitude) and glottic noise estimators. French vowels /a/, /i/ and /v/ spoken separately and the sustained vowel /a/ are analyzed. The system is tested with 88 pathological subjects and 209 control subjects aged 5 to 12. A detection percentage of 96% was obtained with several parameters. The selected parameters changed with the sex. For the boys, the LP error is the best parameter. For the girl, the glottic noise estimator is the best parameter.\n",
    "Keywords: Spectral Analysis, Cepstral Coefficient, Linear Prediction, Perturbation parameter, Pitch Stability, Noise Estimators, Dysphonia, Child\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-444"
  },
  "deliyski93_eurospeech": {
   "authors": [
    [
     "Dimitar D.",
     "Deliyski"
    ]
   ],
   "title": "Acoustic model and evaluation of pathological voice production",
   "original": "e93_1969",
   "page_count": 4,
   "order": 451,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "An acoustic model of pathological voice production is presented. It describes the non-linear effects occurring in the acoustic waveform of disordered voices. The noise components such as fundamental frequency and amplitude irregularities and variations, sub-harmonic components, turbulent noise and voice breaks are formally expressed as a result of random time function influences on the excitation function and the glottal filter. A method for quantitative evaluation of these random functions is described. The method computes their statistical characteristics which can be useful in assessing voice in clinical practice. More than 33 acoustic parameters are computed: average fundamental frequency, phonatory frequency range, several frequency and amplitude short- and long-term perturbation and variation measures, noise-to-harmonic ratio, voice turbulence and soft phonation indexes, quantitative measures of voice breaks, sub-harmonic components and vocal tremors. This set of parameters, which corresponds to the model, allows a multi-dimensional voice quality assessment. A computer system based on above model and method was developed for the CSL model 4300 (Kay Elemetrics Corp.). A group of 68 people with normal and disordered voices was analyzed using the system in order to define normative values for the acoustic voice parameters.\n",
    "Keywords: acoustic voice analysis, signal processing, speech pathology, phoniatrics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-445"
  },
  "kasuya93_eurospeech": {
   "authors": [
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Yasuo",
     "Endo"
    ],
    [
     "Sokol",
     "Saliu"
    ]
   ],
   "title": "Novel acoustic measurements of jitter and shimmer characteristics from pathological voice",
   "original": "e93_1973",
   "page_count": 4,
   "order": 452,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "We first present a unified view of various jitter and shimmer measurements from pathological voice and propose a novel perturbation parameter to characterize jitter and shimmer well. The parameter is shown to be correlated better with perceptual judgments made by a trained laryngologist than some well-known jitter/shimmer parameters. Although most of the jitter/shimmer parameters have been devised to represent the overall degree of the hoarseness of voice, multidimensional measurements are often needed to gain a deeper insight into perturbation characteristics of the hoarseness. We propose an autoregressive-moving average (ARMA) analysis method to represent the perturbation characteristics and indicate that ARMA parameters provide with useful information on the perturbation properties.\n",
    "Keywords: Pathological voice, perturbation parameters, jitter, shimmer\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-446"
  },
  "krom93_eurospeech": {
   "authors": [
    [
     "Guus de",
     "Krom"
    ]
   ],
   "title": "An experiment involving the consistency and reliability of voice quality ratings for different types of speech fragments",
   "original": "e93_1977",
   "page_count": 4,
   "order": 453,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "An experiment was performed in which 6 listeners were asked to rate voice fragments obtained from a variety of speakers on a number of (pathological) voice quality aspects, including the overall degree of deviance (grade), breathiness, and roughness. Four different types of stimuli were presented to each listener; one was based on connected speech fragments, the other 3 were based on different segments of a sustained vowel (the onset, a mid-vowel segment and the whole vowel). Analyses were focussed on measures of listener consistency, and rating reliability. Considering the higher physiological complexity of the connected speech fragments as compared to the vowel-type fragments, we hypothesized that deviant aspects of the voice would be more prominent in the former, resulting in more consistent and more reliable voice quality ratings for connected speech stimuli. Within the three vowel-type stimuli, we expected lowest consistency and reliability for the ratings of the postonset stimuli, for similar reasons. Results indicated that stimulus type had little effect on either within-, or between- listener consistency of the voice quality ratings. Rating reliability was also hardly influenced by stimulus type. When determined as a function of the overall degree of deviance of a voice, the reliability of breathiness and roughness ratings was slightly higher for the whole vowel and vowel onset than for the connected speech and post-onset stimuli. We therefore conclude that connected speech stimuli are not necessarily to be preferred over vowel-type stimuli for a perceptual evaluation of pathological voice quality, and that the onset part of a vowel may contain voice quality cues that are less salient in the most stable part of a vowel.\n",
    "Keywords: voice quality perception, listener consistency, rating reliability\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-447"
  },
  "nord93_eurospeech": {
   "authors": [
    [
     "Lennart",
     "Nord"
    ],
    [
     "Britta",
     "Hammarberg"
    ],
    [
     "Elisabet",
     "Lundstrom"
    ]
   ],
   "title": "Laryngectomee speech in noise - voice effort and intelligibility",
   "original": "e93_1981",
   "page_count": 4,
   "order": 454,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "Different aspects of alaryngeal speech, both esophageal and tracheo-esophageal speech, are being analysed in a joint project between the Department of Speech Communication and Music Acoustics, KTH, and the Department of Logopedics and Phoniatrics, Karolinska Institutet. The purpose of the present part of the project was to evaluate the speech performance of four laryngectomee speakers and one normal speaker were evaluated while they were reading texts aloud with varying amounts of noise in their ears. The noise consisted of a number of voices in a cacophony. Acoustic speech parameters, such as sound pressure and spectral characteristics, were measured and compared among the subjects. Preliminary results show that the tracheo-esophageal speakers were able to raise their voice level as much as the normal laryngeal speakers. The esophageal speakers on the other hand were usually not able to produce as strong voice levels during the text readings. This type of test method with speech in background noise seems promising for assessment of voice effort. In a second part of the present investigation, intelligibility tests were performed. Normal-hearing listeners were asked to adjust the level of noise when exposed to the tape-recorded readings of the laryngectomized speakers, the task being to use a high level of noise while still being able to understand the read passage. The result of the intelligibility test revealed that the listeners tolerated a higher degree of noise, when listening to normal, laryngeal speakers, than to the alaryngeal speakers. Also, the listeners accepted a little more noise when listening to the tracheo-esophageal speakers than to the esophageal speakers.\n",
    "Keywords: laryngectomee speech, intelligibility tests, sound pressure level, speech in noise\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-448"
  },
  "horvei93_eurospeech": {
   "authors": [
    [
     "Berit",
     "Horvei"
    ],
    [
     "Georg",
     "Ottesen"
    ],
    [
     "Sveire",
     "Stensby"
    ]
   ],
   "title": "Analysing prosody by means of a double tree structure",
   "original": "e93_1987",
   "page_count": 4,
   "order": 455,
   "p1": "1987",
   "pn": "1990",
   "abstract": [
    "We have analysed prosody with a view to use the results in a model of prosody suited for a text-to-speech system. To study the prosody an earlier defined double tree data structure of relevant parameters, a data base program, and a query language have been used. The results from the analysis have been used in a parametric model of prosody. We give examples of results from the analysis, and at the end pitch contours generated by the model are compared to pitch contours of text read aloud. It is difficult by listening to distinguish between copy synthesis of the recorded speech and the synthesis of the model.\n",
    "Keywords: - Analysis of prosody - Database and query language - Model for prosody\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-449"
  },
  "caelenhaumont93_eurospeech": {
   "authors": [
    [
     "Geneviève",
     "Caelen-Haumont"
    ]
   ],
   "title": "Prosody and discourse interpretation",
   "original": "e93_1991",
   "page_count": 4,
   "order": 456,
   "p1": "1991",
   "pn": "1994",
   "abstract": [
    "The aim of this paper is to show that prosody relates directly to discourse interpretation. In an experimental framework of 36 readings of a text, and using a segmented and labelled data base, we intend to show that speakers adapt their prosody, and especially F0 organization, rather precisely to the linguistic content of the significates and to the communicative situation. Among the various possibilities of comprehension that the text allows, one is selected by the speaker. Then the marks of personal interpretation that the speaker invests in the text are supplied by F0 structuration and by specific F0 indices. This paper shows the main F0 strategies of interpretation that speakers provide, using cocepts from linguistics and psycholinguistics.\n",
    "Keywords: linguistic modelization, F0 indices, discourse interpretation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-450"
  },
  "epitropakis93_eurospeech": {
   "authors": [
    [
     "George",
     "Epitropakis"
    ],
    [
     "D.",
     "Tambakas"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Duration modelling for the greek language",
   "original": "e93_1995",
   "page_count": 4,
   "order": 457,
   "p1": "1995",
   "pn": "1998",
   "abstract": [
    "A novel two-level model of timing in speech is described. In this model, phoneme durations are first calculated at the lower-level of segmental characteristics and contextual effects, and at a second stage, the already predicted durations are modified to reflect the higher-level of rhythmic and structural organisation of the utterance. The model is based on results obtained from analysis of isolated words for the first level and results from continuous speech analysis for the second one. The complete model can be used in both isolated word and continuous speech recognition systems, and Text-To-Speech systems. The model has been implemented in the Greek language.\n",
    "Keywords: Duration models, Text-To-Speech systems, Isolated Word Recognition systems, Greek language\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-451"
  },
  "epitropakis93b_eurospeech": {
   "authors": [
    [
     "George",
     "Epitropakis"
    ],
    [
     "Nickolas",
     "Yiourgalis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Prosody control of TTS-systems based on linguistic analysis",
   "original": "e93_1999",
   "page_count": 4,
   "order": 458,
   "p1": "1999",
   "pn": "2002",
   "abstract": [
    "This paper presents a computationally efficient and linguistically well-motivated methodology for providing Text-To-Speech (TTS) systems with all the information needed for constructing naturally pronounced and meaningful output. The method, which is especially suited to inflectionally rich languages as Greek and is presented in the context of the Greek TTS system developed at Wire Communications Laboratory independently and in the framework of Esprit Polyglot-I project, has greatly improved the quality of the output speech.\n",
    "Keywords: Text-To-Speech synthesis, Prosody assignment, Linguistic analysis, Inflectionally rich languages\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-452"
  },
  "kompe93_eurospeech": {
   "authors": [
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "T.",
     "Kuhn"
    ],
    [
     "Marion",
     "Mast"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "K.",
     "Ott"
    ],
    [
     "Anton",
     "Batliner"
    ]
   ],
   "title": "Prosody takes over: a prosodically guided dialog system",
   "original": "e93_2003",
   "page_count": 4,
   "order": 459,
   "p1": "2003",
   "pn": "2006",
   "abstract": [
    "In this paper first experiments with naive persons using the speech understanding and dialog system EVAR are discussed. The domain of EVAR is train table inquiry. We observed that in real human-human dialogs when the officer transmits the information the customer very often interrupts. Many of these interruptions are just repetitions of the time of day given by the officer. The functional role of these interruptions is determined by prosodic cues only. An important result of the experiments with EVAR is that it is hard to follow the system giving the train connection via speech synthesis. In this case it is even more important than in human-human dialogs that the user has the opportunity to interact during the answer phase. Therefore we extended the dialog module to allow the user to repeat the time of day and we added a prosody module guiding the continuation of the dialog.\n",
    "Keywords: ASU, prosody, dialog\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-453"
  },
  "langlais93_eurospeech": {
   "authors": [
    [
     "P.",
     "Langlais"
    ],
    [
     "Henri",
     "Meloni"
    ]
   ],
   "title": "Integration of a prosodic component in an automatic speech recognition system",
   "original": "e93_2007",
   "page_count": 4,
   "order": 460,
   "p1": "2007",
   "pn": "2010",
   "abstract": [
    "We present here the integration of a prosodic component in an automatic speech recognition system. We have decided to restrict our sphere of investigation to corpus of steady grammatical structures (sentences) utterered in a straightforward manner. A bottom-up identification system of prosodic labels organized into a hierarchy allows to point out in a sentence the occurences of some prosodic phenomena (two-way emergence of a vowel fundamental frequency, lengthening of its duration ...). Then, a statistical analysis module quantizes - for a given corpus - the correlations between linguistic units and particular labels configurations. The rules we achieve are just as well used for bottom-up identification of component limits in a sentence as for top-down verification of lexical hypothesis. The recognition process splits up into several stages: a bottom-up acoustic and phonetic decoding allows to a lexical access module to output, for each detected vocalic area, a valued cohort of potential words. A first bottom-up extractor eliminates from these cohorts, candidates which cannot be superimposed on the measured prosodic factors and next suggest a filling in every cohort adding to the acoustic score of each word its micro-prosodic mark. Then, the grammatically correct phonemic strings, by means of proposition of valued hypotheses on the utterance divided in intonative groups. A mark representing the adequacy of the prosodic parameters measured with those found in the corpus, is allocated to each candidate sentence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-454"
  },
  "horne93_eurospeech": {
   "authors": [
    [
     "Merle",
     "Horne"
    ],
    [
     "Marcus",
     "Filipsson"
    ],
    [
     "Mats",
     "Ljungqvist"
    ],
    [
     "Anders",
     "Lindström"
    ]
   ],
   "title": "Referent tracking in restricted texts using a lemmatized lexicon: implications for generation of intonation",
   "original": "e93_2011",
   "page_count": 4,
   "order": 461,
   "p1": "2011",
   "pn": "2014",
   "abstract": [
    "An algorithm for referent tracking in a restricted domain is described which allows one to preprocess a text and automatically tag words as either contextually 'New' or 'Given'. The algorithm presupposes computational modelling of lexical semantic identity of sense relations as well as information on inflexional/derivational morphology and compounding. This information is available in a lemmatized lexicon of Swedish. Referent identity is defined on head-word representations derived from the text input on the basis of the inflexional expansion rules contained in the lexicon. Information on the New/Given status of words can subsequently be used in the F0-generating component of the text-to-speech system to trigger the assignment of focal vs non-focal word accents.\n",
    "Keywords: prosody, intonation, text-to-speech, referent-tracking, coreference, anaphora, new information, given information, focal accent, lexical relations, identity of sense\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-455"
  },
  "bannert93_eurospeech": {
   "authors": [
    [
     "Robert",
     "Bannert"
    ]
   ],
   "title": "Perceptual significance of focus accent in spoken Swedish",
   "original": "e93_2015",
   "page_count": 2,
   "order": 462,
   "p1": "2015",
   "pn": "2016",
   "abstract": [
    "An investigation was carried out aiming at establishing the certainty with which listeners can hear and identify focus accent in spoken Standard Swedish. A speech sample of two Stockholm speakers, one male and one female, was presented to a group of listeners who had to mark the most stressed word they heard in chunks of speech. In spite of a rather good agreement, the listeners showed a certain degree of variation. The acoustic correlates of the identified most stressed words were analysed.\n",
    "Keywords: Focus accent, accentuation, perception, macro prosody, spoken Swedish\n",
    ""
   ]
  },
  "montresor93_eurospeech": {
   "authors": [
    [
     "Silvio",
     "Montresor"
    ],
    [
     "Marc",
     "Baudry"
    ]
   ],
   "title": "Pitch estimation of speech signal with the wavelet transform",
   "original": "e93_2017",
   "page_count": 4,
   "order": 463,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "A speech signal pitch analyser based on the wavelet transform is proposed. The pitch period estimation is realized both with a time and a frequency domain method. We describe part of the analyser using the accurate resolution of the wavelet transform in the low frequencies. The use of progressive wavelets allows to extract from the complex wavelet coefficients information given with a phase and a modulus, like Fourier coefficients in the short-time fourier transform. The instantaneous frequencies calculated from the low frequency channels of the wavelet transform ( 50 to 500 Hz ), give an estimation of the fundamental frequency of the speech signal or one of its harmonics during the voiced segments. The paper closes with some results wich confirm the validity of the method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-456"
  },
  "rheem93_eurospeech": {
   "authors": [
    [
     "Jae Yeol",
     "Rheem"
    ],
    [
     "Myung Jin",
     "Bae"
    ],
    [
     "Sou Guil",
     "Ann"
    ]
   ],
   "title": "A spectral AMDF method for pitch extraction of noise-corrupted speech",
   "original": "e93_2021",
   "page_count": 4,
   "order": 464,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "In this paper, we propose a pitch extraction method based on the spectral average magnitude difference function (SAMDF) which is defined as the AMDF of the logmagnitude spectrum of speech. Since the SAMDF is defined on the spectrum of speech signal, the nulls of SAMDF are not affected seriously by the local peaks caused by the additive noise. Furthermore, the proposed method does not need to use any kind of spectral flattening method approach. Experimental result shows that the proposed method is effective for noise-corrupted speech signal and it is adequate for the pitch extraction of female's and child's speech signal.\n",
    "Keywords: Pitch Extraction, Spectral AMDF\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-457"
  },
  "yang93b_eurospeech": {
   "authors": [
    [
     "Gao",
     "Yang"
    ],
    [
     "Henri",
     "Leich"
    ]
   ],
   "title": "A reliable postprocessor for pitch determination algorithms",
   "original": "e93_2025",
   "page_count": 4,
   "order": 465,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "A difficult problem for a PDA (Pitch Determination Algorithm) of speech signal is to avoid gross errors such as halving pitch and multiple pitch. This paper proposes a new postprocessor which can reliablely and efficiently correct the gross errors. The basic principle of the postprocessor is based on utilizing the histogram of the past estimated values of pitch. It can be guaranteed that the correct value of pitch is situated in a small region centring around the peak position of the histogram. A reference value calculated in the small region and sometimes modified with the progressive values outside the small region is used to verify and correct the present estimated pitch. The results obtained by testing the data base in the laboratory showed that the PDA with the proposed postprocessor is so reliable that no gross error was found.\n",
    "Keywords: Speech Coding, Speech Analysis, Pitch estimate\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-458"
  },
  "meyer93b_eurospeech": {
   "authors": [
    [
     "G. F.",
     "Meyer"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Vowel pitch period extraction by models of neurones in the mammalian brain-stem",
   "original": "e93_2029",
   "page_count": 4,
   "order": 466,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "The cochlear nucleus is the first stage of information processing in the mammalian auditory system. It contains a variety of neurone types that extract specific features from sounds. Physiological experiments have shown that 'onset units' in the guinea-pig cochlear nucleus extract the pitch period both from vowel sounds and from harmonic complexes such as the missing fundamental complexes used in psychophysical experiments. A model simulating the activity in these cells has been developed. The model is tested with synthetic vowel sounds as well as missing fundamental stimuli and predicts the pitches heard accurately. The model is used to extract the pitch from natural speech sounds. The pitch estimate is compared with the fundamental frequency recorded with a laryngograph for the signal alone and with additive noise. The model gives a precise pitch estimate up to 0dB signal to noise ratio.\n",
    "Keywords: Auditory Model, cochlear nerve, cochlear nucleus, pitch perception, pitch extraction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-459"
  },
  "schoentgen93c_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Raoul de",
     "Guchteneere"
    ]
   ],
   "title": "Auto-regressive linear models of jitter",
   "original": "e93_2033",
   "page_count": 4,
   "order": 467,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "Jitter is defined as the fluctuations of the lengths of the glottis cycles during sustained phonation. Generally speaking, the amount of jitter is estimated by measuring successive glottal cycle durations and by computing a dispersion measure over an analysis interval of typically fifty cycles. A well-known descriptor of jitter is the so-called Period Perturbation Quotient (PPQ). The problem with this approach is that measures of dispersion do not adequately represent the chronological aspect of period sequences. In other words, the fact that fluctuations of consecutive period may be correlated is not taken into account. Therefore, we decided to study jitter by statistical time-series analysis methods, which are able to represent the interdependences of the fluctuations of neighbouring cycles. Accordingly, we developed an algorithm that measured cycle durations with great precision and used a stochastic linear model to represent cycle duration sequences. The model was an autoregressive linear model. The order of the model was determined on the basis of the partial correlation coefficients and it was confirmed by statistical tests of the whiteness of the residue. We fitted the model to cycle durations of the sustained vowels [a], [i] and [u] produced at a comfortable pitch and loudness level by 40 healthy speakers and 15 dysphonic speakers. Then, we calculated the PPQs of the whitened and unprocessed cycle fluctuations sequences. Results showed that the fluctuations of adjacent periods of a large majority of our vowel signals were correlated. When the fluctuations of adjacent cycles were strongly correlated, especially negatively, the PPQ of the residue was smaller than the PPQ of the raw sequence. By contrast, PPQ values of the whitened and unprocessed sequences were comparable when the correlation between adjacent cycles was weak. Also, AR model order and PPQ values appeared to be different for the three vowel qualities, for dysphonic and healthy speakers, and for male and female speakers.\n",
    "Keywords: Jitter, AR-models\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-460"
  },
  "wei93_eurospeech": {
   "authors": [
    [
     "Jianing",
     "Wei"
    ],
    [
     "David",
     "Howells"
    ],
    [
     "Andrew",
     "Faulkner"
    ],
    [
     "Adrian",
     "Fourcin"
    ]
   ],
   "title": "Larynx period detection methods in speech pattern hearing AIDS",
   "original": "e93_2037",
   "page_count": 4,
   "order": 468,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "In many applications 'pitch extraction' and larynx period measurement are synonymous but in work for the profoundly hearing impaired, and also when detailed information concerning prosodic contrasts is needed, the analysis in normal speech of regularly periodic and also aperiodic laryngeal excitation are essential. The second important need relates to the requirement for period to period accuracy even in conditions of reverberation and ambient noise, and third to the ability to process both normal and pathological speech. We have concentrated on the real-time use of MLP ('neural-net') and peak-picking methods implemented on wearable prostheses, and these are compared with electro-laryngographic techniques and with cepstral processing. Our emphasis is on algorithmic efficacy for both quiet and noisy speech. The developments described come from a programme of work aimed at the real-time analysis of the voice excitation components in speech, in association with the STRIDE project in the EC TIDE programme.\n",
    "Keywords: Fundamental Period Detection, Hearing Aids, Speech Signal Processing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-461"
  },
  "bezooijen93_eurospeech": {
   "authors": [
    [
     "Renee van",
     "Bezooijen"
    ]
   ],
   "title": "Fundamental frequency of dutch women: an evaluative study",
   "original": "e93_2041",
   "page_count": 4,
   "order": 469,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "It is generally assumed that the mean pitch (fundamental frequency) of a speaker's voice is determined both by physiological-anatomical and cultural factors. The present study is the first in a series of studies which aim at gaining insight into the cultural component of the pitch of women. This will be done (1) by examining the culture dependent and culture independent associations of different pitch levels in women with personality characteristics and (2) by examining the evaluations of these personality characteristics in women. The study reported on focused on the associations of Dutch listeners. Their personality ratings of pitch manipulated voices of Dutch women revealed a systematic association of low pitch with large, relaxed, arrogant, high prestige, male, insensitive, independent, strong, adult, and rational. Ratings for depressed, modern, sincere, and strange were not affected by the pitch manipulations. These results will be compared with the responses of the same Dutch listeners towards Belgian and Japanese women, and, in a later stage, to the ratings given by Belgian and Japanese listeners. Evaluations of the personality traits, in terms of desirable or undesirable, will also be examined. Ultimately, the findings will be related to differences in the production of F0 among Dutch, Belgian, and Japanese women.\n",
    "Keywords: pitch, cultural determinants\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-462"
  },
  "fujisaki93_eurospeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hideki",
     "Nasuno"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Proposal and implementation of a spoken word recognizer using utterance normalization and multiple templates on a single VLSI chip",
   "original": "e93_2069",
   "page_count": 4,
   "order": 470,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "A method of spoken word recognition has been proposed using utterance normalization, multiple templates, and path-limited DP matching to cope with speaker variability as well as to reduce the amount of computation. This paper shows the overall configuration of the system and the design of the template-matching part as a cyclic systolic array that can be implemented on an available gate array. Analysis of system operation by a circuit simulator showed that the system is capable of real time recognition of words from a 3,000-word vocabulary, assuming the use of eight templates per word.\n",
    "Keywords: Spoken Word Recognizer, Utterance Normalization, Multiple Templates, Path-Limited DP Matching, Cyclic Systolic Array\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-463"
  },
  "strong93_eurospeech": {
   "authors": [
    [
     "Robert",
     "Strong"
    ]
   ],
   "title": "CASPER: a speech interface for the macintosh",
   "original": "e93_2073",
   "page_count": 4,
   "order": 471,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "Great strides have been made recently in the areas of speech recognition and speech synthesis. At Apple, recent engineering has resulted in the ability to perform high-quality, real-time continuous speech recognition and synthesis on a personal computer. Casper is a system that integrates speech recognition and synthesis into a spoken interface for the Macintosh (TM) computer. This paper describes this system, and how key aspects of the underlying technology have influenced its design and user interface.\n",
    "Keywords: speech, speech recognition, speech synthesis, speech interface, human interlace, natural language\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-464"
  },
  "ellermann93_eurospeech": {
   "authors": [
    [
     "Claudia",
     "Ellermann"
    ],
    [
     "Stijn Van",
     "Even"
    ],
    [
     "Caroline",
     "Huang"
    ],
    [
     "Linda",
     "Manganaro"
    ]
   ],
   "title": "Dragon systems' experiences in small to large vocabulary multi-lingual speech recognition applications",
   "original": "e93_2077",
   "page_count": 4,
   "order": 472,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "In this paper multilingual application areas of large to small vocabulary speech recognition systems are discussed. Different system requirements, human factors issues, technical and design solutions will be presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-465"
  },
  "jouvet93_eurospeech": {
   "authors": [
    [
     "D.",
     "Jouvet"
    ],
    [
     "M. N.",
     "Lokbani"
    ],
    [
     "J.",
     "Monne"
    ]
   ],
   "title": "Application of the n-best solutions algorithm to speaker-independent spelling recognition over the telephone",
   "original": "e93_0208",
   "page_count": 4,
   "order": 473,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "This paper investigates speaker-independent spelling recognition over the telephone using a known dictionary of possible spelled names which provides very useful information. Several ways of using this knowledge are presented and compared: introduction of syntactical constraints at the decoding level, application of a syntactical post-processing of the N best solutions, and finally utilization of a retrieval procedure which looks for the most probable spelled name, knowing the recognized sequence of letters. A large part of the paper is devoted to the combination of the N-best solutions algorithm with the retrieval procedure. The results reported in this article are obtained using a database containing 3,000 utterances of spelled names of French cities recorded from 180 speakers over the telephone network. Application of the retrieval procedure to the first few solutions delivered by the N-best algorithm leads to a 40 % reduction in the spelling error rate for a dictionary of 120 city names, and a 25% reduction for a dictionary of 30,000 town and city names.\n",
    "Keywords: Markov modeling, N-best solutions, Spelling recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-466"
  },
  "braun93_eurospeech": {
   "authors": [
    [
     "Jerome",
     "Braun"
    ],
    [
     "Baruch",
     "Mazor"
    ]
   ],
   "title": "Language based approach to system control in speech recognition systems",
   "original": "e93_2085",
   "page_count": 4,
   "order": 474,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "We propose an approach to system control based on a formal language representation of fundamental control aspects. A formal grammar was defined to represent system control parameters and actions. Overall system behavior is mapped into a set of graphs and the associated language constructs. The real-time system control is accomplished by graph traversal and by the parsing and evaluation of the language constructs. By using the proposed method we obtained system control mechanism with a high degree of extensibility and flexibility.\n",
    "Keywords: Speech Recognition, Extensibility, Real Time, System Control\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-467"
  },
  "balestri93_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Balestri"
    ],
    [
     "Stefano",
     "Lazzaretto"
    ],
    [
     "Pier Luigi",
     "Salza"
    ],
    [
     "Stefano",
     "Sandri"
    ]
   ],
   "title": "The CSELT system for Italian text-to-speech synthesis",
   "original": "e93_2091",
   "page_count": 4,
   "order": 475,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "This paper describes the main components of \"Eloquens\", the CSELT diphone-based text-to-speech synthesis system for the Italian language. The general architecture includes some major modules: text analysis and lexicon access, linguistic/phonetic processing and prosodic control in a multi-level rule writing environment, diphone repertory, synthesizer model The latest progress especially in text processing, acoustic unit set and concatenation technique, and prosodic rule development, exhibits a marked improvement of the synthetic speech overall quality. Evaluation tests indicate an increase of both segmental and word intelligibility in a speech synthesis application task. The software structure is organized to allow easy portability on several platforms and real-time implementation, with multi-channel capabilities and interactive functions.\n",
    "Keywords: Text-to-Speech System, Diphone synthesis, Italian language\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-468"
  },
  "alissali93_eurospeech": {
   "authors": [
    [
     "Mamoun",
     "Alissali"
    ],
    [
     "Gerard",
     "Bailly"
    ]
   ],
   "title": "COMPOST: a client-server model for applications using text-to-speech systems",
   "original": "e93_2095",
   "page_count": 4,
   "order": 476,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "This article presents a Client-Server Model for multilingual text-to-speech synthesis. The server maintains a collection of TTS systems together with related reconfigurable descriptions, called scenarios. Applications of an authorized client can access to this collection via an Ethernet network on a simple request to the server. This server allows the client to customize the TTS processing (language, speaker, speech rate, intonation...) to its requirements by switching between different systems and/or reconfiguring the one it is currently using. The working environment, called COMPOST, has a three layered architecture: the development layer including a powerfull rule-compiler [3] and language-independent processing facilities (linguistic analyzers, PSOLA and Klatt synthesizers ...), the system construction layer including the Scenario Definition Language, and the server layer which has two main components: the process manager and the ressource manager.\n",
    "Keywords: Text-to-Speech Synthesis, Client-Server Model, Man-Machine Interface\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-469"
  },
  "traber93_eurospeech": {
   "authors": [
    [
     "Christof",
     "Traber"
    ]
   ],
   "title": "Syntactic processing and prosody control in the SVOX TTS system for German",
   "original": "e93_2099",
   "page_count": 4,
   "order": 477,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "An overview of the current state of the SVOX TTS system for German is presented in this article. A detailed presentation of the text analysis component is given and it is shown how word and sentence structure analysis and even number-to-phoneme conversion and grapheme-to-phoneme conversion can be expressed concisely and in an easily readable way in a definite-clause-grammar (DCG) formalism, which is interpreted by a chart-parser. We also give an overview of the prosody control in the system, which consists of rule-based accentuation and prosodic phrasing and of a purely statistical and neural-net-work-based acoustic interpretation of suprasegmental phono-logical information.\n",
    "Keywords: text-to-speech synthesis, prosody control, syntactic analysis, grapheme-to-phoneme conversion\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-470"
  },
  "prevost93_eurospeech": {
   "authors": [
    [
     "Scott",
     "Prevost"
    ],
    [
     "Mark",
     "Steedman"
    ]
   ],
   "title": "Using context to specify intonation in speech synthesis",
   "original": "e93_2103",
   "page_count": 4,
   "order": 478,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "A generator based on Combinatory Categorial Grammar using a simple and domain-independent discourse model can be used to direct synthesis of intonation contours for responses to data-base queries, conveying distinctions of contrast and emphasis determined by the discourse model and the state of the knowledge-base.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-471"
  },
  "abe93b_eurospeech": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "Statistical analysis of the acoustic and prosodic characteristics of different speaking styles",
   "original": "e93_2107",
   "page_count": 4,
   "order": 479,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "This paper reports the acoustic and prosodic characteristics of different speaking styles. Three speaking styles are examined by using three different types of texts: a paragraph of an artistic novel, advertisement phrases, and a paragraph of an encyclopaedia. A professional narrator uttered the three texts in appropriate speaking styles that were his own. For convenience, we refer to them as the novel, advertisement and normal speaking style. The analysis results are (1) the 1st formant frequency increases by about 20% in the order of novel, normal, and advertisement speaking style; (2) in terms of the 3rd formant frequency, the novel speaking style is 20% lower in frequency than the other speaking styles; (3) in terms of spectral tilt, the advertisement speaking style has a much flatter spectral tilt than the other speaking styles; (4) F0 range and phrase height assignments are quite different among the three speaking styles; (5) segmental duration in a phrase followed by pause is largely lengthened in the novel speaking style; (6) speech power is commonly modeled as a function of F0 for all speaking styles; and (7) in a syllable followed by pause, vowel devocalization occurs most frequently in the novel speaking style.\n",
    "Keywords: speaking style, synthesis-by-rule, prosody, formant frequency\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-472"
  },
  "hayamizu93_eurospeech": {
   "authors": [
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Detection of unknown words in large vocabulary speech recognition",
   "original": "e93_2113",
   "page_count": 4,
   "order": 480,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "As users often produce new words which are \"unknown\" to the system, processing unknown words is a key issue to speech recognition systems in the real world. This paper describes an approach to the problem of detecting unknown words, especially in a large vocabulary speech recognition. The approach consists of utilizing the recognition scores obtained both with and without a pre-defined dictionary. We studied the relationship between vocabulary sizes and detection rates of unknown words using this technique. It is shown that the detection task of unknown words is comparable to the recognition task for vocabulary sizes of over 1000 words.\n",
    "Keywords: Unknown Words, Speech Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-473"
  },
  "kenny93_eurospeech": {
   "authors": [
    [
     "P.",
     "Kenny"
    ],
    [
     "P.",
     "Labute"
    ],
    [
     "Z.",
     "Li"
    ],
    [
     "R.",
     "Hollan"
    ],
    [
     "M.",
     "Lennig"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A very fast method for scoring phonetic transcriptions",
   "original": "e93_2117",
   "page_count": 4,
   "order": 481,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "We present a new fast match for very large vocabulary continuous speech recognition in which phonetic transcriptions are scored at a cost of one floating-point operation per phone. The algorithm uses a table of estimates of phone scores and durations derived by searching a relatively small phonetic graph in a pre-processing step. The pre-processor runs in faster than real time on a mid-range work station.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-474"
  },
  "hetherington93b_eurospeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "New words: implications for continuous speech recognition",
   "original": "e93_2121",
   "page_count": 4,
   "order": 482,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "The goal of this paper is to understand issues related to the new-word problem in continuous speech recognition, so that we may be able to provide better acoustic and language models to facilitate their detection. We define new words as those outside of the system's vocabulary. Specifically, we present experimental results quantifying the likelihood of encountering new words in several different recognition tasks. We show that the rate of new word occurrence depends on the type of task, and can remain significant even for very large system vocabularies. We also investigate cross-task vocabulary coverage to assess the feasibility of building task-independent vocabularies to reduce the need for task-dependent training data. Finally, we exam- ine syntactic part-of-speech distribution as well as phonological properties of new words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-475"
  },
  "steinbiss93_eurospeech": {
   "authors": [
    [
     "Volker",
     "Steinbiss"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "B.-H.",
     "Iran"
    ],
    [
     "U.",
     "Essen"
    ],
    [
     "Reinhard",
     "Kneser"
    ],
    [
     "M.",
     "Oerder"
    ],
    [
     "H.-G.",
     "Meier"
    ],
    [
     "X.",
     "Aubert"
    ],
    [
     "Christian",
     "Dugast"
    ],
    [
     "D.",
     "Geller"
    ],
    [
     "W.",
     "Hollerbauer"
    ],
    [
     "H.",
     "Bartosik"
    ]
   ],
   "title": "The Philips research system for large-vocabulary continuous-speech recognition",
   "original": "e93_2125",
   "page_count": 4,
   "order": 483,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "This paper gives a status report of the Philips research system for phoneme-based, large-vocabulary, continuous-speech recognition. Like for many other systems, the recognition architecture is based on an integrated statistical approach. We describe the characteristic features of the system as opposed to other systems: 1. The Viterbi criterion is consistently applied both in training and testing. 2. Continuous mixture densities are used without tying or smoothing. 3. Time-synchronous beam search in connection with a phoneme look-ahead is applied to a tree-organized lexicon. The system has been successfully applied to the American English DARPA RM task. Here, we report experimental results for a German 13 000-word Philips internal dictation task. In addition to the scientific prototype, a PC version has been set up which is described here for the first time.\n",
    "Keywords: Continuous speech recognition, large vocabulary recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-476"
  },
  "minami93_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Tomokazu",
     "Yamada"
    ],
    [
     "Tatsuo",
     "Matsuoka"
    ]
   ],
   "title": "Very-large-vocabulary continuous speech recognition algorithm for telephone directory assistance",
   "original": "e93_2129",
   "page_count": 4,
   "order": 484,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "This paper describes an algorithm/or very large vocabulary (about 80,000 words) continuous speech recognition. To improve recognition accuracy and reduce the amount of computation, we implement an accurate and efficient algorithm based on a two-stage LR parser with phoneme HMMs. In this algorithm, we adopt the forward and backward trellis likelihoods for accurate scoring. We also adopt methods of adjusting windows and merging in phoneme sequences as well as grammatical states for efficient searching. This algorithm is applied to a telephone directory assistance system containing the names of more than 70,000 subscribers. The system recognizes the continuously uttered names and addresses. The results show that the system has good performance in spite of the large perplexity.\n",
    "Keywords: Continuous Speech Recognition, /IMM, Search Algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-477"
  },
  "matsunaga93_eurospeech": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Tomokazu",
     "Yamada"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Dictation system using inductively auto-generated syntax",
   "original": "e93_2135",
   "page_count": 4,
   "order": 485,
   "p1": "2135",
   "pn": "2138",
   "abstract": [
    "This paper describes a Japanese dictation system that can effectively deal with an unlimited vocabulary. An approach that automatically generates a character n-gram syntax, from both the original training text and newly generated text is proposed. Each sentence phrase of the newly generated text that is not included in the training text is created by using the training text and the character trigram model of that text. About one-third of the search space not covered by the training text is covered by the newly generated text, showing the effectiveness of the text auto-generation approach. Furthermore, compared with the common beam-search technique, the proposed search technique requires about three-fourths less processing time and allows more accurate recognition.\n",
    "Keywords: Speech Recognition, Language Modeling, Searching\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-478"
  },
  "antoene93_eurospeech": {
   "authors": [
    [
     "Jean-Yves",
     "Antoene"
    ],
    [
     "Bertrand",
     "Caillaud"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Syntax-semantics cooperation in micro: a multi-agent speech understanding system",
   "original": "e93_2139",
   "page_count": 4,
   "order": 486,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "MICRO is a multi-agents speech understanding system largely inspired by cognitive models. Our aim with this cognitive approach is to improve the adaptative abilities of the system. This paper mainly focuses on the modelization of the linguistic level. It describes the cooperation between, on the one hand, a syntactic parser using a Lexical Functional Grammar, and on the other hand, a semantic analyser that performs priming according the paradigm of differential compositionnal semantics.\n",
    "Keywords: speech understanding, cognitive modelization, multi-agents system, syntax-semantics cooperation, LFG grammar, compositionnal semantics\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-479"
  },
  "hwang93_eurospeech": {
   "authors": [
    [
     "M. Y.",
     "Hwang"
    ],
    [
     "F.",
     "Alleva"
    ],
    [
     "X.",
     "Huang"
    ]
   ],
   "title": "Senones, multi-pass search, and unified stochastic modeling in sphinx-II",
   "original": "e93_2143",
   "page_count": 4,
   "order": 487,
   "p1": "2143",
   "pn": "2146",
   "abstract": [
    "SPHINX-II is designed for large vocabulary, speaker-independent continuous speech recognition and is based on semi-continuous hidden Markov models. In the November 1992 ARPA speech evaluation, SPHINX-II achieved the lowest error rate (5%). This paper concentrates on the special techniques that made SPHINX-II successful and different from other systems. Specifically these include senonic decision trees for acoustic modeling, the multi-pass decoder to meet the challenge for very large vocabulary recognition, and the unified stochastic engine for jointly optimizing the acoustic and language model.\n",
    "Keywords: Shared-distribution models, senones, decision trees, multi-pass decoder, unified stochastic engine\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-480"
  },
  "issar93_eurospeech": {
   "authors": [
    [
     "Sunil",
     "Issar"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "CMLPs robust spoken language understanding system",
   "original": "e93_2147",
   "page_count": 4,
   "order": 488,
   "p1": "2147",
   "pn": "2150",
   "abstract": [
    "This paper outlines the general strategies followed in developing the CMU (Carnegie Mellon University) speech understanding system. Our system is oriented toward the extraction of information relevant to a task. It uses a flexible frame-based parser. Our system handles phenomena that are natural in spontaneous speech, for example, restarts, repeats and grammatically ill-formed utterances. It maintains a history of the key features of the dialogue. It can resolve elliptical, anaphoric and other indirect references. In this paper, we pay particular attention to how the context is modeled in our system. We will describe how the system handles corrections and queries that execeed its capabilities. We also address the issue of loose vs. tight coupling of speech recognition and natural language processing. The system has been used to model an Air Travel Information Service (ATIS) Task. In the November 92 DARPA Spoken Language Systems benchmark evalua- tion, the CMU ATIS system correctly answered 93.5% transcript inputs and 88.9% speech inputs. These were the best numbers reported for the evaluation.\n",
    "Keywords: Spontaneous Speech, Flexible Parser, Dia- logue, Loose vs. Tight Coupling, Corrections\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-481"
  },
  "sakai93_eurospeech": {
   "authors": [
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Michael",
     "Phillips"
    ]
   ],
   "title": "J-SUMMIT: Japanese spontaneous speech recognition",
   "original": "e93_2151",
   "page_count": 4,
   "order": 489,
   "p1": "2151",
   "pn": "2154",
   "abstract": [
    "This paper describes the application of the J-SUMMIT speech recognition system [1] to spontaneous Japanese speech. J-summit is a speech recognition system for Japanese being developed at MIT in the framework of SUMMIT which makes explicit use of acoustic-phonetic knowledge, embedded in a segmental framework that can be trained automatically [2, 3]. The goal of this work is both to assess the cross-language portability of the summit speech recognition system and to support the development of a bilingual version of the voyager speech understanding system. In order to apply j-summit to spontaneous speech, we first collected 1,400 spontaneous utterances from 40 speakers who spoke to a simulated Japanese version of the VOYAGER speech understanding system. In order to cover a reasonable portion of the training corpus, we extended the vocabulary to about 500 words and also changed the language model from a category-pair grammar to a category bigram, which was trained on the training portion of the spontaneous speech corpus. We trained the system on a 34-speaker subset of the spontaneous speech corpus and tested on the remaining six-speaker subset. The word error rate (the sum of insertion, deletion, and substitution errors) was 14.9% and the utterance error was 53.3% for the test set.\n",
    "Keywords: speech recognition, spontaneous speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-482"
  },
  "bellegarda93_eurospeech": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ]
   ],
   "title": "A new interface paradigm: automatic recognition of integrated speech and handwriting information",
   "original": "e93_2157",
   "page_count": 4,
   "order": 490,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "Because of the difficulties inherent to the analysis of such complex processes as speaking and (hand)writing, the machine recognition of the two most natural ways of communicating has so far met with only limited success. In this paper we consider a new paradigm: the integrated use of speech and handwriting information to improve the overall accuracy of an automatic recognizer. This approach is made possible by the complementarity of the two sources of information yielding the acoustic and pen stroke evidence. It leads to the development of ISWI, the Integrated Speech and Writing Interface. Preliminary results indicate the viability of ISWI, particularly in applications such as human factor studies. These results also bring useful insights into the feasibility of ISWI given the present level of speech and handwriting recognition performance. It seems reasonable to expect ISWI to become the logical choice for the ultimate user-friendly man-machine interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-483"
  },
  "rudnicky93_eurospeech": {
   "authors": [
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Factors affecting choice of speech over keyboard and mouse in a simple data-retrieval task",
   "original": "e93_2161",
   "page_count": 4,
   "order": 491,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "This paper describes some recent experiments that assess user mode selection behavior in a multi-modal environment in which actions can be performed with equivalent effect by speech, keyboard or scroller. Results indicate that users freely choose speech over other modalities, even when it is less efficient in objective terms, such as time-to-completion or input error. Additional evidence indicates that users appear to focus on simple input time in making their choice of mode, in effect minimizing the amount of personal effort expended.\n",
    "Keywords: Speech recognition, multi-modal systems, user preference\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-484"
  },
  "basson93_eurospeech": {
   "authors": [
    [
     "Sara",
     "Basson"
    ],
    [
     "Dina",
     "Yashchin"
    ],
    [
     "Ashok",
     "Kalyanswamy"
    ],
    [
     "Kim",
     "Silverman"
    ]
   ],
   "title": "Comparing synthesizers for name and address provision: field trial results",
   "original": "e93_2165",
   "page_count": 4,
   "order": 492,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "The NYNEX Automated Customer Name and Address (ACNA) service trial was designed to evaluate the feasibility of automating reverse directory assistance using text-to-speech synthesis. In a reverse directory service, callers provide a telephone number and in return are given the associated name and address (listing) information. The ACNA trial automated this process, using text-to-speech synthesis to deliver the listing information. Five high-end text-to-speech devices were evaluated over the course of the experiment, with approximately 5000 real customers processed through the ACNA position. Data such as requests for repetition, spelling, and survey responses were collected to compare the participating devices. ATT's TTS was rated by callers as the most understandable device. Berkeley Speech Technology's BeST device and ATT's TTS produced the highest proportion of listings that did not elicit clarification requests. Results along several dimensions suggest that automated CNA is acceptable to customers, although it does not perform as well as operator-handled CNA. Marked differences among the high-end synthesizers tested here highlight the importance of careful evaluation, even for services as constrained as ACNA.\n",
    "Keywords: evaluation methodology, text-to-speech synthesis, telephone application\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-485"
  },
  "silverman93_eurospeech": {
   "authors": [
    [
     "Kim",
     "Silverman"
    ],
    [
     "Ashok",
     "Kalyanswamy"
    ],
    [
     "Julie",
     "Silverman"
    ],
    [
     "Sara",
     "Basson"
    ],
    [
     "Dina",
     "Yashchin"
    ]
   ],
   "title": "Synthesiser intelligibility in the context of a name-and-address information service",
   "original": "e93_2169",
   "page_count": 4,
   "order": 493,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "This study uses an over-the-phone transcription task to address two issues in speech synthesis performance. One is the relationship between on the one hand results of a controlled experiment with compliant volunteers, and on the other hand performance of technology in field conditions with real users on a similar task. The second issue is the impact of domain-specific customisation of prosody for simple non-ambiguous texts on synthesis quality. The task is transcription of names and addresses, spoken by high-quality commercial speech synthesisers. Results show that (i) the largest differences between synthesisers will generalise from laboratory to field conditions, but relative rankings vary slightly on different scales, and (ii) prosodic customisation makes significant and consistent improvements on transcription accuracy, requests for repetitions, subjective ratings, and total time required to complete the task.\n",
    "Keywords: Synthesis, Prosody, Evaluation, Intelligibility\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-486"
  },
  "marzi93_eurospeech": {
   "authors": [
    [
     "Ruth",
     "Marzi"
    ]
   ],
   "title": "Enhancing user acceptance at the managerial workplace",
   "original": "e93_2173",
   "page_count": 3,
   "order": 494,
   "p1": "2173",
   "pn": "2175",
   "abstract": [
    "The work place on the managerial level has so far evaded full computer support. The bottleneck seems to be not the quality of the information presented or the decisions reached, but user acceptance. Here, a system is proposed, which puts its emphasis on methods to ease the use of complex systems and to enhance user acceptance. Host systems offer graphical user interfaces, but used wisely, adding speech input combined with a linguistic postprocessing component can be more efficient.\n",
    "Keywords: HL Processing, User model, User Interface, Knowledge Based System\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-487"
  },
  "suhm93_eurospeech": {
   "authors": [
    [
     "B.",
     "Suhm"
    ],
    [
     "Monika",
     "Woszczyna"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Detection and transcription of new words",
   "original": "e93_2179",
   "page_count": 4,
   "order": 495,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "This paper describes a model which enables a speech recognition system to automatically detect new words and to provide a rough phonetic transcription. In our approach to the new word problem the decision whether new words occurred in the speech input is not based exclusively on acoustic evidence but also on a language model designed to support the detection of new words. We describe preliminary experiments to create new word grammars on the Wall Street Journal task. Furthermore we present recognition results of our new word model using the recognition engine of the JANUS speech to speech translation system [1, 2], designed around the task of conference registration.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-488"
  },
  "jimenez93_eurospeech": {
   "authors": [
    [
     "Victor M.",
     "Jimenez"
    ],
    [
     "Andres",
     "Marzal"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "Efficient enumeration of sentence hypotheses in connected word recognition",
   "original": "e93_2183",
   "page_count": 4,
   "order": 496,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "A new algorithm is presented for the search of the N-best paths in weighted graphs applied to the computation of the N-Best Sentence Hypotheses in Continuous Speech Recognition based on the One-Stage procedure. It is also shown how Beam Search techniques can be integrated in the algorithm leading to a very efficient sentence hypotheses enumeration algorithm\n",
    "Keywords: N-Best Sentences, N Shortest Paths, Beam Search, Connected Word Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-489"
  },
  "oshaughnessy93_eurospeech": {
   "authors": [
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Locating disfluencies in spontaneous speech: an acoustical analysis",
   "original": "e93_2187",
   "page_count": 4,
   "order": 497,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "A primary difference between spontaneous speech and read speech concerns the presence of disfluencies, which are much more prevalent in spontaneous speech. Disfluencies often take the form of hesitation pauses (both filled and unfilled) and false starts. With false starts, the speaker interrupts the normal flow of speech to restart an utterance. With pauses, extraneous silences or \"uh\"-type sounds are inserted. The acoustic aspects of such restarts and pauses in a widely-used speech database were examined here, from the point of view of identifying them acoustically. Automatically locating such disfluencies could improve the performance of an automatic speech recognizer, by allowing the elimination from consideration of some hypotheses based on spectral analysis. Without such analysis, disfluent speech risks being misinterpreted by a recognizer.\n",
    "Keywords: Restarts, pauses, recognition, intonation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-490"
  },
  "nguyen93b_eurospeech": {
   "authors": [
    [
     "Roselyne",
     "Nguyen"
    ],
    [
     "Kamel",
     "Smaili"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Guy",
     "Perennou"
    ]
   ],
   "title": "Integration of phonological knowledge in a continuous speech recognition system",
   "original": "e93_2191",
   "page_count": 4,
   "order": 498,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "Building a dictation machine involves the management of a large amount of linguistic knowledge. We are dealing here with the explicit integration of a phonological module in the automatic dictation machine, MAUD, of which a first version has already been implemented. MAUD is made up of three main modules: an acoustic-phonetic decoder, a lexical module and a syntactic-semantic module. Some of the problems at the lexical level can be addressed with the help of a phonological module. The one we have developed comprises a set of phonological rules taken from generative phonology in order to transform phonetic forms into phonological ones. The most important problem is to formalize the rules in order to use them in speech recognition. The implementation of these rules leans on IRIT work based on two original notions: multi-pronunciation groups (mpg's) and contextual phonological groups (cpg's). Experimental results illustrate the impact of phonological knowledge in the overall recognition process of a dictation machine.\n",
    "Keywords: Dictation Machine, Continuous Speech Recognition, Multi-agents Architecture, Phonological Knowledge\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-491"
  },
  "dumouchel93_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Prosody and continuous speech recognition",
   "original": "e93_2195",
   "page_count": 4,
   "order": 499,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "We first analyze the distribution of three prosodic cues for continuous speech: segmental fundamental frequency, intensity and duration. Second, we propose a statistical prosodic model and show how it can be included in a Markov source-based recognizer. Finally, we present the performance results of different prosodic models in a very large vocabulary continuous speech recognizer.\n",
    "Keywords: prosody, suprasegmental features, Markov source-based continuous speech recognizer, very large vocabulary\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-492"
  },
  "bergmann93_eurospeech": {
   "authors": [
    [
     "H.",
     "Bergmann"
    ],
    [
     "H.-H.",
     "Hamer"
    ],
    [
     "A.",
     "Noll"
    ],
    [
     "A.",
     "Paeseler"
    ],
    [
     "H.",
     "Tomaschewski"
    ]
   ],
   "title": "Spoken-language processing for restricted domains: a sublanguage approach",
   "original": "e93_2199",
   "page_count": 4,
   "order": 500,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "After a short review of sublanguages used by people communicating their ideas in restricted domains, the implementation of a spoken language database query system is described. The implementation is based on a commercial speech recognition system using a finite-state network language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-493"
  },
  "young93d_eurospeech": {
   "authors": [
    [
     "Steve J.",
     "Young"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "The use of state tying in continuous speech recognition",
   "original": "e93_2203",
   "page_count": 4,
   "order": 501,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "This paper describes a method of robustly training context-dependent multiple Gaussian mixture HMM phone models without the need for a posteriori smoothing. The method involves clustering and then tying acoustically similar states within each allophone set in order to balance model complexity against the available data. The operational properties of the method are studied and results are presented for phone recognition on TIMIT. The method is shown to be robust, to give good recognition performance and to reduce computation in both recognition and training. All experiments were performed using the HTK portable HMM toolkit.\n",
    "Keywords: HMM state clustering phone recognition TIMIT HTK\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-494"
  },
  "woodland93_eurospeech": {
   "authors": [
    [
     "Phil C.",
     "Woodland"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "The HTK tied-state continuous speech recogniser",
   "original": "e93_2207",
   "page_count": 4,
   "order": 502,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "HTK is a portable software toolkit for developing systems using continuous density hidden Markov models developed by the Cambridge University Speech Group. This paper describes speech recognition experiments using HTK based systems for the DARPA Resource Management (RM) task. In particular good performance is obtained using a tied-state triphone based multiple mixture approach. This system was used in the final DARPA RM evaluation (September 1992) and was found to perform at a similar level to the main DARPA systems, and yet be efficient in terms of the total of parameters and computational load. The results for that system are given along with some recent experiments that investigated the use of male-female modelling in a tied-state HMM system.\n",
    "Keywords: Hidden Markov Models, Resource Management, State Clustering, HTK\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-495"
  },
  "devillers93_eurospeech": {
   "authors": [
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Christian",
     "Dugast"
    ]
   ],
   "title": "Combination of training criteria to improve continuous speech recognition",
   "original": "e93_2211",
   "page_count": 4,
   "order": 503,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "This paper is concerned with the combination of different learning criteria so as to improve continuous speech recognition performance. The learning criterion of Maximum Likelihood Estimation (MLE) used with the Viterbi algorithm in Hidden Markov Models (HMMs), and the discriminant one of Mean Squared Error (MSE) generally used with the gradient descent algorithm in a neural network such as the Time Delay Neural Network (TDNN) lead to different learning strategies. Each of those criteria generates particular internal representations and produces different classification errors. Combining both models in the recognition phase then allows to eliminate several misclassifications. Experiments have been conducted with such a combined system TDNN/HMM involving MLE and MSE learning criteria. The database tested is part of the Darpa Ressource Management Speaker Dependent database. The neural device is a hierarchical structure of TDNNs which makes training feasible for a large database on Unix workstations. The hybrid system consists of a linearly combination of the TDNN output scores and the HMMs probabilities during the recognition phase in order to minimize the classification errors. Such a system leads to a performance improvement of 15% to 20% compared to the state-of-the-art HMM systems.\n",
    "Keywords: hierarchical structure of TDNNs, combined system TDNN/HMM\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-496"
  },
  "zlokarnik93_eurospeech": {
   "authors": [
    [
     "Igor",
     "Zlokarnik"
    ]
   ],
   "title": "Experiments with an articulatory speech recognizer",
   "original": "e93_2215",
   "page_count": 4,
   "order": 504,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "An articulatory-based speech recognizer is presented which uses features derived from electromagnetic articulography. Results indicate that articulatory features can significantly improve the recognition performance if they are explicitly known. Experiments to estimate the articulatory features from the acoustic signal are described which manifest the problems involved in modeling the acoustic-articulatory relationship.\n",
    "Keywords: speech recognition, articulatory features, electromagnetic articulography, articulatory estimation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-497"
  },
  "antoniol93_eurospeech": {
   "authors": [
    [
     "Giuliano",
     "Antoniol"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Techniques for robust recognition in restricted domains",
   "original": "e93_2219",
   "page_count": 3,
   "order": 505,
   "p1": "2219",
   "pn": "2221",
   "abstract": [
    "This paper describes an Automatic Speech Understanding (ASU) system used in a human-robot interface for the remote control of a mobile robot. The intended application is that of an operator issuing telecontrol commands to one or more robots from a remote workstation. ASU is supposed to be performed with spontaneous continuous speech and quasi real time conditions. Training and testing of the system was based on speech data collected by means of Wizard of Oz simulations. Two kinds of robustness factors are introduced: the first is a recognition error-tolerant approach to semantic interpretation, the second is based on a technique for evaluating the reliability of the ASU system output with respect to the input utterance. Preliminary results are 90.9% of correct semantic interpretations7 and 89.1% of correct detection of out-of-domain sentences at the cost of rejecting 16.4% of correct in-domain sentences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-498"
  },
  "mouria93_eurospeech": {
   "authors": [
    [
     "Feriel",
     "Mouria"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Use of explicit context-dependent phonemic model in continuous speech recognition",
   "original": "e93_2223",
   "page_count": 4,
   "order": 506,
   "p1": "2223",
   "pn": "2226",
   "abstract": [
    "We discuss in this paper the possibility of modeling contextual variation and effects of speaking rate at a symbolic level. Contextual deformations of speech are described in a speech event-synchronized way rather than in the traditional time-synchronized way. Our objective is to compile dictionary phonetic transcriptions, together with context models, to produce a symbolic representation of speech in which context deformations and speech rate variations are taken into account by an explicit context-dependent model to enhance a continuous speech recognition system. We describe a model which takes explicitly into account the influence of contextual deformations as well as the rate of speaking in the continuous speech recognition. When tested on a 400 french words vocabulary, pronounced by three male and one female speakers, we found the use of our model leades to improvement in the recognition rate, especially for speaker independent mode.\n",
    "Keywords: Continuous speech recognition, Contextual deformations, speaking rate\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-499"
  },
  "gong93c_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Base transformation for environment adaptation in continuous speech recognition",
   "original": "e93_2227",
   "page_count": 4,
   "order": 507,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "A specific background noise, speaker or transmission line condition of a speech recognizer is referred as an environment. A mismatch between the training and operating environments can severely degrade recognition accuracy. We present a base transformation method for environment adaptation, which converts an environmental difference into a base difference and reduces the difference by a base transformation. Experiments were conducted on adapting to telephone quality speech, to a new speaker and to speech corrupted by additive Gaussian noise. Using two sentences (5 sec duration) as adaptation data, the method gives a telephone line adapted recognition accuracy of 93.5% and a speaker adapted accuracy of about 90%, for a city name recognition task. Using nine sentences (20 sec duration) with SNRs better than lOdB, a noise-adapted recognition accuracy of 90% was obtained on a 206 word recognition task.\n",
    "Keywords: environment adaptation, base transformation, noisy speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-500"
  },
  "mazor93_eurospeech": {
   "authors": [
    [
     "Baruch",
     "Mazor"
    ],
    [
     "Ming-Whei",
     "Feng"
    ]
   ],
   "title": "Improved a-posteriori processing for keyword spotting",
   "original": "e93_2231",
   "page_count": 4,
   "order": 508,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "In [I], we described a word-spotting technique consisting of a continuous-speech decoder and a post-decoder processor. The continuous-speech decoder performs a modified Viterbi search through a network of connected hidden Markov models (EMM) to produce hypothesized keyword segments. The post-processor then performs a discrete-word decoding to label each hypothesized keyword segment, generates corresponding a-posteriori measures, and decides the recognition outcome. In this paper, we present an approach for estimating post processor parameters that produce recognition errors near their empirical lower bounds. To develop and evaluate the performance of the proposed scheme we used the Road Rally speech corpus. The results show that at similar rejection levels the proposed approach reduces the substitution and false alarm rates reported in [I] by up to 35%.\n",
    "Keywords: Word Spotting, Post Processor, A-posteriori Measure, Decision Region, Operation Point\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-501"
  },
  "ortegagarcia93_eurospeech": {
   "authors": [
    [
     "J.",
     "Ortega-Garcia"
    ],
    [
     "J. M.",
     "Paez-Borrallo"
    ],
    [
     "Luis A.",
     "Hernandez-Gomez"
    ]
   ],
   "title": "Single and multi-channel speech enhancement for a word spotting system",
   "original": "e93_2235",
   "page_count": 4,
   "order": 509,
   "p1": "2235",
   "pn": "2238",
   "abstract": [
    "Speech recognition systems are often designed under controlled working conditions (usually known as \"laboratory\" conditions) that do not correspond in many cases to real recognition environments. In practice, recognition rates in laboratory conditions are severely damaged by unexpected factors, that can be classified from two points of view: a) speech extrinsic factors, like environmental noise, reverberations, channel distortions (due to acquisition, processing and/or transmission devices), bandwith reduction, etc., and b) speech intrinsic factors (non expected speech signal entering our recognition system), like spontaneous speech, unadaptation of the user to syntactical/grammatical requirements of the system, stress and others. From now on, we will call \"noise\", in a general manner, to all those extrinsic or intrinsic perturbations that will be added to our clean speech, producing noisy speech as a result. In this paper, both single-channel and multi-channel approaches to speech enhancement [1,2] for a word spotting system are presented. Single-channel approach is intended when no references of the noisy source are available -in this cases, speech enhancement could be accomplished with techniques as spectral subtraction or classical filtering. Multi-channel approach, needs at least one correlated reference of the noisy source and, in this other case, techniques as adaptive filtering are usually used [3]. We shall compare this two different approaches in a word spotting system, where several Spanish keywords are embedded in some spontaneous utterances.\n",
    "Keywords: Speech Recognition, Word Spotting, Spectral Subtraction, Adaptive Filtering, Noise Cancelling, Speech Prediction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-502"
  },
  "ney93b_eurospeech": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ],
    [
     "Ute",
     "Essen"
    ]
   ],
   "title": "Estimating 'small' probabilities by leaving-one-out",
   "original": "e93_2239",
   "page_count": 4,
   "order": 510,
   "p1": "2239",
   "pn": "2242",
   "abstract": [
    "In this paper, we apply the leaving-one-out concept to the estimation of 'small' probabilities, i.e. the case where the number of training samples is much smaller than the number of possible classes. After deriving the Turing-Good formula in this framework, we introduce several specific models in order to avoid the problems of the original Turing-Good formula. These models are the constrained model, the absolute discounting model and the linear discounting model. These models are then applied to the problem of bigram-based stochastic language modelling. Experimental results are presented for an English corpus of 1.1 million words.\n",
    "Keywords: Stochastic Language Modelling, Leaving-One- Out, Turing-Good Method, Insufficient Training Data\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-503"
  },
  "young93e_eurospeech": {
   "authors": [
    [
     "Sheryl R.",
     "Young"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Semantic and pragmatically based re-recognition of spontaneous speech",
   "original": "e93_2243",
   "page_count": 4,
   "order": 511,
   "p1": "2243",
   "pn": "2246",
   "abstract": [
    "This paper describes a novel architecture and algorithms for combining stochastic modeling and Natural Language Understanding techniques to help speech recognition and understanding. In this system, an utterance is initially processed by a speech recognizer using a standard class bigram language model to produce a single best scoring word string. This word string is then parsed by the Phoenix parser [1], which produces a semantic frame. The parser uses Recursive Transition Networks to represent semantic fragments, or word strings which are meaningful to the system. Semantic fragments of the utterance are assigned to slots in frames. Semantic, pragmatic and discourse knowledge is then applied to the parsed frame to identify misrecognized substrings and develop content predictions for the misrecognized regions. For this, we compute within utterance semantic constraints, constraints arising from speech repair acts (e.g. on-line edits and corrections) as well as dialog-based constraints arising from different types of sub-dialogs (or wnat have traditionally been called discourse and domain plans) and the content of prior inputs and system responses. The predictions correspond to a small subset of the semantic networks Known to the system. The region boundaries of the input along with the set of predicted semantic networks are passed to a Recursive Transition Network speech decoder which uses them in re-recognizing the specified region of the utterance. The networks used by the RTN decoder are the same ones used by the parser. Only the predicted subset of nets are used in the re-recognition. We describe our algorithms for detecting misrecognitions and generating predictions as well as the operation of our RTN-based recognizer. The system was prained on training data from the ARPA Air Travel Information Service (ATIS) task, and tested on an independent test set of 1000 utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-504"
  },
  "hildebrandt93_eurospeech": {
   "authors": [
    [
     "Bernd",
     "Hildebrandt"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Modeling of time constituents for speech understanding",
   "original": "e93_2247",
   "page_count": 4,
   "order": 512,
   "p1": "2247",
   "pn": "2250",
   "abstract": [
    "The analysis and interpretation of time constituents is important for most applications of speech understanding systems. Problems can be caused by the varying distribution of constituents. A basic set of time constituents were found in a corpus of domain specific (train schedule) utterances. A distributed representation of surface structure models and an incremental semantic analysis is used to manage the complexity. The knowledge base of the speecli understanding system that provides the framework for the analysis and interpretation of time constituents uses the semantic network language ERNEST.\n",
    "Keywords: Speech Understanding, Time Constituents\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-505"
  },
  "matousek93_eurospeech": {
   "authors": [
    [
     "Vaclav",
     "Matousek"
    ]
   ],
   "title": "Phonetic segmentation method for the continuous czech speech recognition",
   "original": "e93_2251",
   "page_count": 4,
   "order": 513,
   "p1": "2251",
   "pn": "2254",
   "abstract": [
    "The presented paper describes a new developed segmentation method for the extraction of phonetic segments from Czech speech signal The use of the proposed method is intended for the machine recognition of short continuous spoken Czech sentences. As the Czech language is a Slavonic language containing some original phonemes, a brief definition of the complete phoneme set and a short description of Czech speech phonetic analysis is presented in the second paragraph. The foundations of the developed segmentation method based on the speech signal cepstrum parameters evaluation and the experimental results obtained on the small database of continuous speech consisting of short Czech sentences spoken by male speakers are presented in the main part of the paper. Additionaly, the comparison of the developed simple method with the phoneme recognition procedure based on the application of classic dynamic programming method (DTW function) and on the using HMMs is discussed in the last paragraph. The primary acoustic analysis of the Czech speech signal and the cepstral parameters evaluation have been realized either by means of the \"Soundblaster Pro\" special PC interface card or by means of the \"SUN-OpenWindows Audio-Tool\".\n",
    "Keywords: Speech Signal Preprocessing, Feature Selection and Extraction, Acoustic-Phonetic Decoding, Phoneme Segmentation, Phoneme Classification, Word Hypothesis Generation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-506"
  },
  "hauptmann93b_eurospeech": {
   "authors": [
    [
     "Alexander G.",
     "Hauptmann"
    ],
    [
     "Lin L.",
     "Chase"
    ],
    [
     "Jack",
     "Mostow"
    ]
   ],
   "title": "Speech recognition applied to reading assistance for children: a baseline language model",
   "original": "e93_2255",
   "page_count": 4,
   "order": 514,
   "p1": "2255",
   "pn": "2258",
   "abstract": [
    "We describe an approach to using speech recognition in assisting children's reading. A state-of-the-art speaker independent continuous speech recognizer designed for large vocabulary dictation is adapted to the task of identifying substitutions and omissions in a known text. A baseline language model for this new task is detailed and evaluated against a corpus of children reading graded passages. We ;ire able to identify words missed by a reader with an average false positive rate of 39 % ;and a corresponding false negative rate of 37 %. These preliminary results ;ire encouraging for our long-term goal of providing automated coaching for children learning to read.\n",
    "Keywords: Speech recognition, language modeling, children's reading\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-507"
  },
  "weenink93_eurospeech": {
   "authors": [
    [
     "David J. M.",
     "Weenink"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Modelling speaker normalization by adapting the BIAS in a neural net",
   "original": "e93_2259",
   "page_count": 4,
   "order": 515,
   "p1": "2259",
   "pn": "2262",
   "abstract": [
    "We present a possible model for speaker normalization in terms of adaptation. Vowel identification is considered to be based on the integration of a number of decisions. Each decision is of the same simple form: decide whenever the accumulated evidence exceeds a certain threshold (bias). Changing the bias can influence a decision and consequently the identification. This process just as well describes human vowel perception as neural net simulation.\n",
    "Keywords: Speaker normalization, neural nets\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-508"
  },
  "artieres93_eurospeech": {
   "authors": [
    [
     "T.",
     "Artieres"
    ],
    [
     "P.",
     "Gallinari"
    ]
   ],
   "title": "Neural models for extracting speaker characteristics in speech modelization systems",
   "original": "e93_2263",
   "page_count": 4,
   "order": 516,
   "p1": "2263",
   "pn": "2266",
   "abstract": [
    "We analyze in this paper inherent limitations of prediction systems for speaker identification. We introduce different approaches for enhancing these systems and present results of tests on TIMIT with neural net predictive systems.\n",
    "Keywords: speaker recognition, predictive neural networks, sequence classification\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-509"
  },
  "zinke93_eurospeech": {
   "authors": [
    [
     "J.",
     "Zinke"
    ]
   ],
   "title": "Influence of pattern compression on speaker verification",
   "original": "e93_2267",
   "page_count": 4,
   "order": 517,
   "p1": "2267",
   "pn": "2270",
   "abstract": [
    "In this paper different techniques for pattern compression of a speaker verification system are presented. A bitrate reduction of the cepstral features to 8 bits by a better encoding shows only a small increase of the verification error. Trace segmentation and a simpler version without interpolation results in a given fixed length of the feature vector, but may be used only to reduce the length of longer phrases. Vector quantization especially in conjunction with DTW gives also the opportunity to use two pattern recognition techniques for different kind of speaker dependency within features.\n",
    "Keywords: Speaker Verification, Cepstrum, DTW, Trace-Segmentation, VQ\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-510"
  },
  "schiel93_eurospeech": {
   "authors": [
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "A comparative study of speaker adaptation under realistic conditions",
   "original": "e93_2271",
   "page_count": 4,
   "order": 518,
   "p1": "2271",
   "pn": "2274",
   "abstract": [
    "Five different algorithms for speaker adaptation are investigated under realistic conditions: The new speaker can use the system immediately for his purpose without knowing about the adaptation, there is no announcement of a new speaker, a change of speakers can take place any time and the amount of computation and memory for adaptation is less than 1 % of the recognition task itself The adaptation is carried out by re-estimating the codebooks of a phoneme based word recognizer using semicontinuous hidden Markov models (SCHMM). The movement of the mean vector in the codebook according to his euclidean distance to the observed vector achieved the best results during very short adaptation sessions of 20 spoken words and for the long term adaptation in sessions of 100 spoken words.\n",
    "Keywords: Speaker adaptation, codebook, semicontinuous EMM, realistic conditions, LVQ1, LVQ2\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-511"
  },
  "irvine93_eurospeech": {
   "authors": [
    [
     "D. A.",
     "Irvine"
    ],
    [
     "F. J.",
     "Owens"
    ]
   ],
   "title": "A comparison of speaker recognition techniques for telephone speech",
   "original": "e93_2275",
   "page_count": 4,
   "order": 519,
   "p1": "2275",
   "pn": "2278",
   "abstract": [
    "This paper reports on an investigation into the relative performance of several approaches to automatic Speaker Recognition (SR). The techniques which were examined were; Dynamic Time Warping (DTW), Vector Quantisation (VQ), and Hidden Markov Modelling (HMM). In order to test the various techniques, two sets of test speech data were created. The first set of test data was acquired under controlled conditions, and the speech data was digitised and manually end-pointed. This set of data was used as a reference test for the algorithms which were devised. The second body of data was acquired over a 'dialled-up' telephone link. In this case, the data was automatically prompted-for, acquired and end-pointed in real-time by computer. This provided test data which permitted the testing of the speaker recognition systems under realistic operating conditions. Using the LPC-derived cepstral coefficients to represent the test speech, three text-dependent speaker-recognition systems were produced. On testing, using several codebook sizes, the VQ-based system proved to have the best performance, with the HMM-based system and the DTW-based system producing similar, but less successful, results.\n",
    "Keywords: Speaker Recognition, Speaker Identification, Speaker Verification, Speech Databases\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-512"
  },
  "veth93_eurospeech": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Guido",
     "Gallopyn"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Speaker verification over telephone channels based on concatenated phonemic hidden Markov models",
   "original": "e93_2279",
   "page_count": 4,
   "order": 520,
   "p1": "2279",
   "pn": "2282",
   "abstract": [
    "In this paper, we describe a speaker verification system for telephone channels based on randomly prompted digit strings and using concatenated context-dependent phonemic hidden Markov models (HMMs). The main goal of this work was to achieve acceptable speaker verification performance while keeping the number of parameters (and, consequently, the amount of training material) as well as the CPU requirements relatively small. To optimize the performance of this system, several features (that had been separately suggested before) have been used, i.e.: (1) context-dependent phoneme models, (2) silence and garbage (click) models to remove extraneous parts out of the actual utterance, (3) better decision logic based on associated speakers (also referred to as \"cohort\" in [Rosenberg et al., 1992]), (4) better feature vectors using \"rasta\" processing as suggested in [Hermansky et al., 1991], (5) rejection of garbage utterances without significantly affecting the overall verification performance. In this paper we show how we further improved this system using increased trial length and automatic model adaptation. We show that these two techniques allow to achieve an average equal error rate (EER) on difficult (and realistic) tasks of 0.2%, which is 1.6 orders of magnitude smaller compared to the system we reported earlier [de Veth et al., 1993].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-513"
  },
  "cox93_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Speaker adaptation using a predictive model",
   "original": "e93_2283",
   "page_count": 4,
   "order": 521,
   "p1": "2283",
   "pn": "2286",
   "abstract": [
    "A new technique of speaker adaptation for use in speaker-independent speech recognition systems is presented. The training-data is used to build models (based on linear regression) of sounds. At recognition time, the models are used together with an incomplete set of sounds from a new speaker to estimate values for unheard sounds, which are then used to adapt the speaker-independent models. The technique reduced the error-rate from 17% to 5.3% when applied to a database of 104 speakers speaking the English alphabet.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-514"
  },
  "sun93_eurospeech": {
   "authors": [
    [
     "Z. P.",
     "Sun"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "Combining features via LDA in speaker recognition",
   "original": "e93_2287",
   "page_count": 4,
   "order": 522,
   "p1": "2287",
   "pn": "2290",
   "abstract": [
    "This paper discusses cepstial feature combinations via linear discriminant analysis (LDA) in the context of automatic speaker identification (ASI). Two static cepstral features are considered, namely standard MFCC and a subband filtered form derived via linear prediction known as RASTA-PLP. These two are compared along with their first order dynamic forms as both single and combined feature sets. LDA is shown to provide a useful means of combining (dissimilar) feature sets and permitting a direct trade-off between the number of coefficients and ASI performance, particularly when testing under noisy conditions. Also, the importance of pre-normalisation is demonstrated. It is shown that in the case of individual features, the two static forms give the best performance in clean conditions, with a cross-over to the two dynamic forms being better in the region of SNR=15dB. The LDA combination of the two static forms gives the best overall results under both clean and noisy conditions.\n",
    "Keywords: speaker recognition, cepstral feature combinations, linear discriminant analysis, robustness\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-515"
  },
  "elvira93_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Elvira"
    ],
    [
     "R. A.",
     "Carrasco"
    ]
   ],
   "title": "Neural networks for speech and speaker recognition through a digital telephone exchange",
   "original": "e93_2291",
   "page_count": 4,
   "order": 523,
   "p1": "2291",
   "pn": "2294",
   "abstract": [
    "This paper presents the results obtained in the comparison of several Artificial Neural Networks for phonetic speech recognition and speaker recognition tasks. Both processes are very important in the human-computer interface through telephone systems for automatic communication between a customer and an automatic receiver system such as an automatic dialing system. Speech samples stored from a standard digital telephone exchange system are being used to test several Artificial Neural Network models already trained using different features from the speech signal. Comparative results of all these experiments are presented, together with several conclusions obtained from these results.\n",
    "Keywords: Neural Networks, Speech and Speaker Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-516"
  },
  "homayounpour93_eurospeech": {
   "authors": [
    [
     "M. Mehdi",
     "Homayounpour"
    ],
    [
     "J. Philippe",
     "Goldman"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Jacqueline",
     "Vaissière"
    ]
   ],
   "title": "Performance comparison of machine and human speaker verification",
   "original": "e93_2295",
   "page_count": 4,
   "order": 524,
   "p1": "2295",
   "pn": "2298",
   "abstract": [
    "This paper concerns the problem of speech variability in Automatic Speaker Verification (ASV) systems. The performance of our ASV system was compared with the performance of human listeners on material spoken in four different emotional modes (neutral, happiness, fatigue and anger). Resulting variation in pitch contours, speaking rate, intensity, formant values, etc. had a relatively high influence on the performance of our ASV system and that of human listeners. In another experiment, a speaker verification task was done automatically and by human listeners on a telephone data base: 24 speakers tried to imitate two reference speakers. The last experiment was done by artificially varying the prosodic and spectral characteristics of speech as a means for simulating variation due to emotion and as a means to augment limited data bases for evaluating and comparing different ASV systems.\n",
    "Keywords: speaker verification, emotion, imitation, and TD-PSOLA\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-517"
  },
  "hannah93_eurospeech": {
   "authors": [
    [
     "M. I.",
     "Hannah"
    ],
    [
     "A. T.",
     "Sapeluk"
    ],
    [
     "Robert I.",
     "Damper"
    ],
    [
     "I. M.",
     "Roger"
    ]
   ],
   "title": "The effect of utterance length and content on speaker-verifier performance",
   "original": "e93_2299",
   "page_count": 4,
   "order": 525,
   "p1": "2299",
   "pn": "2302",
   "abstract": [
    "Utterance length is known to be an important factor in text-independent speaker verification. Here, we examine the effect of length on speaker separation for text-dependent verification. When both test data and templates are continuous-word strings, separation increases with length with apparent saturation at about 2 seconds. Simulating a possible practical scenario using continuous-word test data and templates formed by concatenating discrete words did not achieve useful separation for the longest tokens, apparently because of word-boundary effects. When test data and templates are discrete-word with matching scores accumulated over the maximum length, however, separation was good.\n",
    "Keywords: Speaker verification, text-dependent verification, utterance length\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-518"
  },
  "lipeika93_eurospeech": {
   "authors": [
    [
     "Antanas",
     "Lipeika"
    ],
    [
     "Joana",
     "Lipeikiene"
    ]
   ],
   "title": "The use of pseudostationary segments for speaker identification",
   "original": "e93_2303",
   "page_count": 4,
   "order": 526,
   "p1": "2303",
   "pn": "2306",
   "abstract": [
    "This paper is concerned with speaker identification problem. The identification is carried out comparing feature vectors extracted from pseudostationary parts of speech utterances. Both likelihood ratio and cepstral distances are used for comparing feature vectors. The identification approach is suitable for text-dependent and text-independent identification.\n",
    "Keywords: likelihood ratio distance, cepstral distance, pseudostationary segments\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-519"
  },
  "federico93_eurospeech": {
   "authors": [
    [
     "A.",
     "Federico"
    ],
    [
     "Andrea",
     "Paoloni"
    ]
   ],
   "title": "Bayesian decision in the speaker recognition by acoustic parametrization of voice samples over telephone lines",
   "original": "e93_2307",
   "page_count": 4,
   "order": 527,
   "p1": "2307",
   "pn": "2310",
   "abstract": [
    "This paper presents a solution to the speaker recognition problem in forensic applications, and gives the right statistical foundations to the decision task. All the related issues are restated, the modelization method is reconsidered for sparse experimental matrices and the algorithms for a suitable bayesian approach to the decision are derived following a more consistent theory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1993-520"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "baker93_eurospeech",
    "barry93_eurospeech",
    "lamel93_eurospeech",
    "peckham93_eurospeech",
    "moore93_eurospeech",
    "noll93_eurospeech",
    "ney93_eurospeech",
    "eskenazi93_eurospeech",
    "bridle93_eurospeech",
    "lee93_eurospeech",
    "jekosch93_eurospeech",
    "santen93_eurospeech",
    "pieraccini93_eurospeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "ozawa93_eurospeech",
    "lopezgonzalo93_eurospeech",
    "balss93_eurospeech",
    "garciamateo93_eurospeech",
    "wery93_eurospeech",
    "dymarski93_eurospeech",
    "mauc93_eurospeech",
    "wuppermann93_eurospeech",
    "gerson93_eurospeech",
    "kondoz93_eurospeech",
    "miki93_eurospeech",
    "moreno93_eurospeech",
    "sedgwick93_eurospeech",
    "ma93_eurospeech",
    "dutoit93_eurospeech",
    "chan93_eurospeech",
    "shoham93_eurospeech",
    "marcato93_eurospeech",
    "asanuma93_eurospeech",
    "ma93b_eurospeech"
   ]
  },
  {
   "title": "Articulatory Modelling",
   "papers": [
    "beautemps93_eurospeech",
    "narayanan93_eurospeech",
    "nguyen93_eurospeech",
    "loevenbruck93_eurospeech",
    "savariaux93_eurospeech",
    "sock93_eurospeech",
    "jomaa93_eurospeech",
    "olesen93_eurospeech",
    "bavegard93_eurospeech",
    "foldvik93_eurospeech"
   ]
  },
  {
   "title": "Voice Source Analysis and Modelling",
   "papers": [
    "schroeter93_eurospeech",
    "oliveira93_eurospeech",
    "strik93_eurospeech",
    "schoentgen93_eurospeech",
    "denzler93_eurospeech"
   ]
  },
  {
   "title": "HMM-Based Recognition System",
   "papers": [
    "leandro93_eurospeech",
    "lamel93b_eurospeech",
    "gauvain93_eurospeech",
    "schukattalamazzini93_eurospeech",
    "seino93_eurospeech"
   ]
  },
  {
   "title": "Speech Signal Processing",
   "papers": [
    "apolloni93_eurospeech",
    "cerf93_eurospeech",
    "drygajlo93_eurospeech",
    "ambikairajah93_eurospeech",
    "iwahashi93_eurospeech",
    "altmann93_eurospeech",
    "roelands93_eurospeech",
    "wang93_eurospeech",
    "kamp93_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "chibelushi93_eurospeech",
    "montacie93_eurospeech",
    "thompson93_eurospeech",
    "bimbot93_eurospeech"
   ]
  },
  {
   "title": "Data Bases, Speech Assessment, Noisy Speech",
   "papers": [
    "moreno93b_eurospeech",
    "ribeiro93_eurospeech",
    "karjalainen93_eurospeech",
    "chan93b_eurospeech",
    "draxler93_eurospeech",
    "jekosch93b_eurospeech",
    "castagneri93_eurospeech",
    "steeneken93_eurospeech",
    "danielsen93_eurospeech",
    "nicolas93_eurospeech",
    "saliu93_eurospeech",
    "wrench93_eurospeech",
    "salavedra93_eurospeech",
    "bouquin93_eurospeech",
    "crozier93_eurospeech",
    "sorensen93_eurospeech"
   ]
  },
  {
   "title": "Phonetics",
   "papers": [
    "vieregge93_eurospeech",
    "tronnier93_eurospeech",
    "viana93_eurospeech",
    "schmidt93_eurospeech",
    "andersen93_eurospeech",
    "hunnicutt93_eurospeech",
    "moore93b_eurospeech",
    "young93_eurospeech",
    "pasdeloup93_eurospeech"
   ]
  },
  {
   "title": "Phoneme Classification and Labelling",
   "papers": [
    "son93_eurospeech",
    "goldenthal93_eurospeech",
    "stamenkovic93_eurospeech",
    "gubrynowicz93_eurospeech",
    "andersson93_eurospeech"
   ]
  },
  {
   "title": "Duration Modelling in HMMs",
   "papers": [
    "suaudeau93_eurospeech",
    "jones93_eurospeech",
    "gong93_eurospeech",
    "forsyth93_eurospeech",
    "hochberg93_eurospeech"
   ]
  },
  {
   "title": "Speaker Adaptation and Normalization",
   "papers": [
    "tuerk93_eurospeech",
    "ono93_eurospeech",
    "zhao93_eurospeech",
    "kosaka93_eurospeech",
    "knohl93_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis, Articulatory Modelling",
   "papers": [
    "leeuw93_eurospeech",
    "mclaughlin93_eurospeech",
    "klaassen93_eurospeech",
    "gransden93_eurospeech",
    "cutugno93_eurospeech",
    "marasek93_eurospeech",
    "pitermann93_eurospeech",
    "schoentgen93b_eurospeech",
    "teston93_eurospeech",
    "magnocaldognetto93_eurospeech",
    "elgendy93_eurospeech",
    "payan93_eurospeech",
    "miki93b_eurospeech",
    "znagui93_eurospeech",
    "marchal93_eurospeech",
    "aguilar93_eurospeech",
    "ganguli93_eurospeech",
    "bonastre93_eurospeech",
    "duez93_eurospeech",
    "meunier93_eurospeech",
    "betari93_eurospeech",
    "torres93_eurospeech",
    "christov93_eurospeech",
    "thilly93_eurospeech",
    "czigler93_eurospeech",
    "jassem93_eurospeech"
   ]
  },
  {
   "title": "Prosody: Rhythm, Style, Emotion",
   "papers": [
    "rooney93_eurospeech",
    "miksic93_eurospeech",
    "laan93_eurospeech",
    "garrido93_eurospeech",
    "vroomen93_eurospeech"
   ]
  },
  {
   "title": "Improved Algorithms for HMMs",
   "papers": [
    "ayer93_eurospeech",
    "saerens93_eurospeech",
    "vaseghi93_eurospeech",
    "abe93_eurospeech",
    "moyal93_eurospeech",
    "class93_eurospeech",
    "aerens93_eurospeech",
    "lokbani93_eurospeech",
    "matsuoka93_eurospeech",
    "maxwell93_eurospeech"
   ]
  },
  {
   "title": "Noisy Speech and Enhancement",
   "papers": [
    "omologo93_eurospeech",
    "kobayashi93_eurospeech",
    "bakamidis93_eurospeech",
    "xie93_eurospeech",
    "kroschel93_eurospeech"
   ]
  },
  {
   "title": "Speaker Variability",
   "papers": [
    "pean93_eurospeech",
    "ljolje93_eurospeech",
    "heuvel93_eurospeech",
    "itahashi93_eurospeech",
    "hernandezmendez93_eurospeech"
   ]
  },
  {
   "title": "Segmentation and Labelling",
   "papers": [
    "rangoussi93_eurospeech",
    "angelini93_eurospeech",
    "farhat93_eurospeech",
    "heroaez93_eurospeech",
    "cosi93_eurospeech",
    "heise93_eurospeech",
    "eisen93_eurospeech",
    "bergem93_eurospeech",
    "ooyen93_eurospeech",
    "nix93_eurospeech",
    "zon93_eurospeech",
    "bonneau93_eurospeech",
    "kacic93_eurospeech",
    "datta93_eurospeech",
    "vyas93_eurospeech",
    "ma93c_eurospeech",
    "kuwabara93_eurospeech",
    "jones93b_eurospeech",
    "vartanian93_eurospeech",
    "chernigovskaya93_eurospeech",
    "pont93_eurospeech",
    "kolinsky93_eurospeech",
    "cao93_eurospeech",
    "gong93b_eurospeech",
    "hubener93_eurospeech",
    "shimodaira93_eurospeech",
    "reichl93_eurospeech",
    "blomberg93_eurospeech"
   ]
  },
  {
   "title": "Prosody: Analysis and Modelling of F0 Contours",
   "papers": [
    "bosch93_eurospeech",
    "jensen93_eurospeech",
    "taylor93_eurospeech",
    "geoffrois93_eurospeech",
    "demenko93_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition in Noise",
   "papers": [
    "ainsworth93_eurospeech",
    "nolazcoflores93_eurospeech",
    "kobayashi93b_eurospeech",
    "gales93_eurospeech",
    "buniet93_eurospeech"
   ]
  },
  {
   "title": "Speaker Independency",
   "papers": [
    "angelini93b_eurospeech",
    "bahl93_eurospeech",
    "grayden93_eurospeech",
    "plannerer93_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "ouadou93_eurospeech",
    "lopezgonzalo93b_eurospeech",
    "ljungqvist93_eurospeech",
    "meyer93_eurospeech",
    "belhoula93_eurospeech",
    "murray93_eurospeech",
    "imiolczyk93_eurospeech",
    "macchi93_eurospeech",
    "gaved93_eurospeech",
    "house93_eurospeech",
    "morton93_eurospeech",
    "abadjieva93_eurospeech",
    "lewis93_eurospeech",
    "luk93_eurospeech",
    "fries93_eurospeech",
    "karlsson93_eurospeech",
    "kraft93_eurospeech",
    "lee93b_eurospeech",
    "williams93_eurospeech"
   ]
  },
  {
   "title": "Dialogue Structure",
   "papers": [
    "muller93_eurospeech",
    "dybkjaer93_eurospeech",
    "jones93c_eurospeech",
    "macdermid93_eurospeech",
    "duermael93_eurospeech",
    "yamashita93_eurospeech",
    "andemach93_eurospeech",
    "young93b_eurospeech",
    "hirschberg93_eurospeech",
    "young93c_eurospeech"
   ]
  },
  {
   "title": "Language Modelling",
   "papers": [
    "zhao93b_eurospeech",
    "garigliano93_eurospeech",
    "kneser93_eurospeech",
    "wright93_eurospeech",
    "mccandless93_eurospeech",
    "chiang93_eurospeech",
    "vidal93_eurospeech",
    "jardino93_eurospeech",
    "lucke93_eurospeech",
    "witschel93_eurospeech"
   ]
  },
  {
   "title": "Prosody: Prosodic Parameter Manipulation",
   "papers": [
    "banga93_eurospeech",
    "wang93b_eurospeech",
    "takagi93_eurospeech",
    "swerts93_eurospeech",
    "bagshaw93_eurospeech"
   ]
  },
  {
   "title": "New Architectures for Neural Networks",
   "papers": [
    "tadj93_eurospeech",
    "sasaki93_eurospeech",
    "castano93_eurospeech"
   ]
  },
  {
   "title": "Noise Reduction and Channel Adaption",
   "papers": [
    "vaseghi93b_eurospeech",
    "wong93_eurospeech",
    "martin93_eurospeech",
    "gao93_eurospeech",
    "trompf93_eurospeech",
    "mokbel93_eurospeech",
    "wittmann93_eurospeech",
    "alexandre93_eurospeech",
    "brancaccio93_eurospeech"
   ]
  },
  {
   "title": "Word Spotting",
   "papers": [
    "nakamura93_eurospeech",
    "rose93_eurospeech",
    "kiyama93_eurospeech",
    "jeanrenaud93_eurospeech",
    "imamura93_eurospeech",
    "lleida93_eurospeech",
    "okane93_eurospeech",
    "boite93_eurospeech",
    "alvarezcercadillo93_eurospeech",
    "okawa93_eurospeech"
   ]
  },
  {
   "title": "Speech Processing and Coding",
   "papers": [
    "dohnal93_eurospeech",
    "crestel93_eurospeech",
    "pollak93_eurospeech",
    "heitkamper93_eurospeech",
    "campbell93_eurospeech",
    "fjallbrant93_eurospeech",
    "jean93_eurospeech",
    "martin93b_eurospeech",
    "mauuary93_eurospeech",
    "osipov93_eurospeech",
    "haigh93_eurospeech",
    "paulus93_eurospeech",
    "dia93_eurospeech",
    "gottesman93_eurospeech",
    "popescu93_eurospeech",
    "gerlach93_eurospeech",
    "kastantin93_eurospeech",
    "mumolo93_eurospeech",
    "law93_eurospeech",
    "bruhn93_eurospeech",
    "svendsen93_eurospeech",
    "feldes93_eurospeech",
    "atungsiri93_eurospeech",
    "moriya93_eurospeech"
   ]
  },
  {
   "title": "Prosody: Phrasing",
   "papers": [
    "bruce93_eurospeech",
    "strangert93_eurospeech",
    "pijper93_eurospeech",
    "grabe93_eurospeech",
    "beaugendre93_eurospeech"
   ]
  },
  {
   "title": "MLPs and TDNNs for Speech Recognition",
   "papers": [
    "zahorian93_eurospeech",
    "kitamura93_eurospeech",
    "zhu93_eurospeech",
    "elenius93_eurospeech",
    "blackburn93_eurospeech",
    "huckvale93_eurospeech",
    "lubensky93_eurospeech",
    "hild93_eurospeech",
    "bodenhausen93_eurospeech",
    "windheuser93_eurospeech",
    "bappert93_eurospeech"
   ]
  },
  {
   "title": "Speech Translation, Language Identification, Parsers",
   "papers": [
    "sagayama93_eurospeech",
    "morimoto93_eurospeech",
    "woszczyna93_eurospeech",
    "rayner93_eurospeech",
    "hazen93_eurospeech",
    "muthusamy93_eurospeech",
    "cheng93_eurospeech",
    "dauchy93_eurospeech",
    "hirose93_eurospeech",
    "dermatas93_eurospeech",
    "murakami93_eurospeech",
    "millien93_eurospeech",
    "dutton93_eurospeech",
    "hunt93_eurospeech",
    "hiller93_eurospeech",
    "rooney93b_eurospeech",
    "zajicek93_eurospeech",
    "hanes93_eurospeech",
    "hirschberg93b_eurospeech",
    "euler93_eurospeech",
    "goodine93_eurospeech",
    "albina93_eurospeech"
   ]
  },
  {
   "title": "Dialogue Evalution",
   "papers": [
    "vergeynst93_eurospeech",
    "hirschman93_eurospeech",
    "simpson93_eurospeech",
    "delogu93_eurospeech",
    "morin93_eurospeech"
   ]
  },
  {
   "title": "Data Bases",
   "papers": [
    "castaing93_eurospeech",
    "ginestelmailland93_eurospeech",
    "ljolje93b_eurospeech",
    "boeffard93_eurospeech"
   ]
  },
  {
   "title": "Letter to Sound and Architecture for TTS",
   "papers": [
    "coile93_eurospeech",
    "daelemans93_eurospeech",
    "lindstrom93_eurospeech",
    "iles93_eurospeech"
   ]
  },
  {
   "title": "Perception",
   "papers": [
    "wieringen93_eurospeech",
    "heuven93_eurospeech",
    "ooyen93b_eurospeech",
    "bailly93_eurospeech",
    "goedemans93_eurospeech"
   ]
  },
  {
   "title": "Search Algorithms",
   "papers": [
    "bocchieri93_eurospeech",
    "fissore93_eurospeech",
    "fink93_eurospeech",
    "hetherington93_eurospeech",
    "lacouture93_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition, HMMs, NNs",
   "papers": [
    "torres93b_eurospeech",
    "bonafonte93_eurospeech",
    "acero93_eurospeech",
    "galiano93_eurospeech",
    "nakagawa93_eurospeech",
    "song93_eurospeech",
    "huo93_eurospeech",
    "fink93b_eurospeech",
    "downey93_eurospeech",
    "gales93b_eurospeech",
    "wang93c_eurospeech",
    "class93b_eurospeech",
    "wrzoskowicz93_eurospeech",
    "tsoukalas93_eurospeech",
    "castro93_eurospeech",
    "mellouk93_eurospeech",
    "ran93_eurospeech",
    "petek93_eurospeech",
    "russell93_eurospeech",
    "brierton93_eurospeech",
    "takeda93_eurospeech",
    "elliott93_eurospeech",
    "berenyi93_eurospeech",
    "deng93_eurospeech",
    "siohan93_eurospeech",
    "hernando93_eurospeech",
    "richter93_eurospeech"
   ]
  },
  {
   "title": "Spoken Language Dialogue",
   "papers": [
    "eckert93_eurospeech",
    "heisterkamp93_eurospeech",
    "gerbino93_eurospeech",
    "lefebvre93_eurospeech",
    "morin93b_eurospeech"
   ]
  },
  {
   "title": "Speech Input/Output Assessment",
   "papers": [
    "nielsen93_eurospeech",
    "klaus93_eurospeech",
    "mariniak93_eurospeech",
    "neovius93_eurospeech",
    "tillmann93_eurospeech",
    "wyard93_eurospeech",
    "riccio93_eurospeech",
    "garnierrizet93_eurospeech",
    "delogu93b_eurospeech",
    "spiegel93_eurospeech"
   ]
  },
  {
   "title": "Synthesis: Sound Generation",
   "papers": [
    "grau93_eurospeech",
    "hauptmann93_eurospeech",
    "kerkhoff93_eurospeech",
    "nakajima93_eurospeech",
    "tuerk93b_eurospeech"
   ]
  },
  {
   "title": "Hybrid HMMs/ANNs for Speech Recognition",
   "papers": [
    "renals93_eurospeech",
    "schmid93_eurospeech",
    "rigoll93_eurospeech",
    "kurimo93_eurospeech",
    "aibar93_eurospeech",
    "bourlard93_eurospeech",
    "haffner93_eurospeech",
    "boiteau93_eurospeech",
    "martens93_eurospeech",
    "robinson93_eurospeech"
   ]
  },
  {
   "title": "Visual Cues",
   "papers": [
    "bothe93_eurospeech",
    "shadle93_eurospeech",
    "boe93_eurospeech",
    "robertribes93_eurospeech"
   ]
  },
  {
   "title": "Telecommunication, Application Aspects",
   "papers": [
    "baungaard93_eurospeech",
    "jenkins93_eurospeech",
    "magadur93_eurospeech",
    "kuroiwa93_eurospeech",
    "ortel93_eurospeech",
    "lleida93b_eurospeech",
    "wyard93b_eurospeech",
    "burger93_eurospeech",
    "hart93_eurospeech",
    "monaghan93_eurospeech",
    "olaszy93_eurospeech",
    "hartmann93_eurospeech",
    "hauenstein93_eurospeech",
    "hermann93_eurospeech",
    "koo93_eurospeech",
    "mcinnes93_eurospeech",
    "paoloni93_eurospeech",
    "hazan93_eurospeech",
    "ludovic93_eurospeech"
   ]
  },
  {
   "title": "Spoken Language Dialogue Application",
   "papers": [
    "music93_eurospeech",
    "clementino93_eurospeech",
    "blomberg93b_eurospeech",
    "eckert93b_eurospeech",
    "labropoulou93_eurospeech",
    "lewin93_eurospeech",
    "yang93_eurospeech",
    "nitta93_eurospeech",
    "bonneaumaynard93_eurospeech",
    "glass93_eurospeech"
   ]
  },
  {
   "title": "Synthesis: Articulatory and Source Modelling",
   "papers": [
    "kroger93_eurospeech",
    "boersma93_eurospeech",
    "carlson93_eurospeech",
    "frehr93_eurospeech",
    "ishikawa93_eurospeech"
   ]
  },
  {
   "title": "Syntactical Constraints",
   "papers": [
    "charpillet93_eurospeech",
    "collingham93_eurospeech",
    "isotani93_eurospeech",
    "dupont93_eurospeech"
   ]
  },
  {
   "title": "Pathological Voice Analysis",
   "papers": [
    "plante93_eurospeech",
    "deliyski93_eurospeech",
    "kasuya93_eurospeech",
    "krom93_eurospeech",
    "nord93_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis: Pitch and Prosody",
   "papers": [
    "horvei93_eurospeech",
    "caelenhaumont93_eurospeech",
    "epitropakis93_eurospeech",
    "epitropakis93b_eurospeech",
    "kompe93_eurospeech",
    "langlais93_eurospeech",
    "horne93_eurospeech",
    "bannert93_eurospeech",
    "montresor93_eurospeech",
    "rheem93_eurospeech",
    "yang93b_eurospeech",
    "meyer93b_eurospeech",
    "schoentgen93c_eurospeech",
    "wei93_eurospeech",
    "bezooijen93_eurospeech"
   ]
  },
  {
   "title": "Applications",
   "papers": [
    "fujisaki93_eurospeech",
    "strong93_eurospeech",
    "ellermann93_eurospeech",
    "jouvet93_eurospeech",
    "braun93_eurospeech"
   ]
  },
  {
   "title": "Synthesis: Systems, Syntax, Prosody",
   "papers": [
    "balestri93_eurospeech",
    "alissali93_eurospeech",
    "traber93_eurospeech",
    "prevost93_eurospeech",
    "abe93b_eurospeech"
   ]
  },
  {
   "title": "Large Vocabulary Systems",
   "papers": [
    "hayamizu93_eurospeech",
    "kenny93_eurospeech",
    "hetherington93b_eurospeech",
    "steinbiss93_eurospeech",
    "minami93_eurospeech"
   ]
  },
  {
   "title": "Continuous Speech Recognition Systems",
   "papers": [
    "matsunaga93_eurospeech",
    "antoene93_eurospeech",
    "hwang93_eurospeech",
    "issar93_eurospeech",
    "sakai93_eurospeech"
   ]
  },
  {
   "title": "Human Factors",
   "papers": [
    "bellegarda93_eurospeech",
    "rudnicky93_eurospeech",
    "basson93_eurospeech",
    "silverman93_eurospeech",
    "marzi93_eurospeech"
   ]
  },
  {
   "title": "Complex Forms of Speech & Speaker Recognition",
   "papers": [
    "suhm93_eurospeech",
    "jimenez93_eurospeech",
    "oshaughnessy93_eurospeech",
    "nguyen93b_eurospeech",
    "dumouchel93_eurospeech",
    "bergmann93_eurospeech",
    "young93d_eurospeech",
    "woodland93_eurospeech",
    "devillers93_eurospeech",
    "zlokarnik93_eurospeech",
    "antoniol93_eurospeech",
    "mouria93_eurospeech",
    "gong93c_eurospeech",
    "mazor93_eurospeech",
    "ortegagarcia93_eurospeech",
    "ney93b_eurospeech",
    "young93e_eurospeech",
    "hildebrandt93_eurospeech",
    "matousek93_eurospeech",
    "hauptmann93b_eurospeech",
    "weenink93_eurospeech",
    "artieres93_eurospeech",
    "zinke93_eurospeech",
    "schiel93_eurospeech",
    "irvine93_eurospeech",
    "veth93_eurospeech",
    "cox93_eurospeech",
    "sun93_eurospeech",
    "elvira93_eurospeech",
    "homayounpour93_eurospeech",
    "hannah93_eurospeech",
    "lipeika93_eurospeech",
    "federico93_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1993"
}