{
 "title": "2nd International Conference on Spoken Language Processing (ICSLP 1992)",
 "location": "Banff, Alberta, Canada",
 "startDate": "13/10/1992",
 "endDate": "16/10/1992",
 "conf": "ICSLP",
 "year": "1992",
 "name": "icslp_1992",
 "series": "ICSLP",
 "SIG": "",
 "title1": "2nd International Conference on Spoken Language Processing",
 "title2": "(ICSLP 1992)",
 "date": "13-16 October 1992",
 "papers": {
  "ladefoged92_icslp": {
   "authors": [
    [
     "Peter",
     "Ladefoged"
    ]
   ],
   "title": "Knowing enough to analyze spoken languages",
   "original": "i92_0001",
   "page_count": 4,
   "order": 1,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "Nobody can know everything about spoken language processing. Whatever aspect of the field we work in, we have to get help from experts in other fields. For example, for my work on the phonetic structures of dying languages, I rely on the talents of linguists, anthropologists, sound recording engineers, physiologists, psychologists and signal processing engineers to support my own phonetic knowledge. We are fortunate in that by coming to a congress such as this we can all profit and get a little help from our friends.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-1"
  },
  "mori92_icslp": {
   "authors": [
    [
     "Renato De",
     "Mori"
    ],
    [
     "R.",
     "Kuhn"
    ]
   ],
   "title": "Speech understanding strategies based on string classification trees",
   "original": "i92_0441",
   "page_count": 8,
   "order": 2,
   "p1": "441",
   "pn": "448",
   "abstract": [
    "This paper describes two strategies, operating at different levels of speech, which exploit special characteristics of the speech understanding task; both involve word islands within an utterance. First, a new upper bound based on the probability of the best possible parse is proposed for scoring partial interpretations of an acoustic signal. Subsequently, the paper describes a method of automating rule discovery for semantic parsers. The rules are incorporated in a structure called a String Classification Tree and involve patterns of key words; they are robust in the presence of production and recognition errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-2"
  },
  "kuhl92_icslp": {
   "authors": [
    [
     "Patricia K.",
     "Kuhl"
    ]
   ],
   "title": "Infants' perception and representation of speech: development of a new theory",
   "original": "i92_0449",
   "page_count": 8,
   "order": 3,
   "p1": "449",
   "pn": "456",
   "abstract": [
    "A new series of studies on adults' and infants' perception of phonetic \"prototypes,\" exceptionally good instances of phonetic categories, show that prototypes play a unique role in speech perception. Phonetic category prototypes function like \"perceptual magnets\" for other stimuli in the category. They attract nearby members of the category, rendering them more perceptually similar to the category prototype than would be expected on the basis of physical distance alone. Nonprototype stimuli from the category do not function in this way. Moreover, by 6 months of age, infants tested in the United States and Sweden show that the perceptual magnet effect is language-specific. Infants from the two countries exhibit the magnet effect only for the phonetic prototypes of their own native language. Thus, exposure to a specific language alters infants' perception of speech by 6 months of age. These results offer an explanation for the findings of a variety of studies on cross-language speech perception in infants and adults, have implications for second-language learning, and are consistent with data on the representation of cognitive categories outside the domain of speech. The results support a new model which describes how innate factors and experience with a specific language interact in the development of speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-3"
  },
  "hirose92_icslp": {
   "authors": [
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "The behavior of the larynx in spoken language production",
   "original": "i92_0457",
   "page_count": 2,
   "order": 4,
   "p1": "457",
   "pn": "458",
   "abstract": [
    "There is a concensus that the larynx is the organ of voice. Less well recognized is the role it serves in speech production. Recently, it has been shown that the larynx is a major contributor to articulatory adjustments. These laryngeal adjustments are produced by coordinated activities of the intrinsic and extrinsic muscles of the larynx. Since the introduction of fiberscopic and electromyographic techniques to the field of experimental phonetics, the nature of laryngeal adjustments in different languages has been explored in more detail than herefore was possible. Further, the use of different types of glottography and computer-controlled image analysis opened a new scope in laryngeal physiology. In this paper, the physiological correlates of basic features of laryngeal articulatory adjustments are discussed, based on the results obtained using a combination of recently developed experimental techniques.\n",
    ""
   ]
  },
  "lleida92_icslp": {
   "authors": [
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "José B.",
     "Marino"
    ],
    [
     "J.",
     "Salavedra"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Syllabic fillers for Spanish HMM keyword spotting",
   "original": "i92_0005",
   "page_count": 4,
   "order": 5,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "In this paper, we describe a preliminary investigation of the use of syllabic fillers to model the out-of-vocabulary utterances or non-keywords in fluent speech for Spanish speech recognition systems. The Spanish language has a syllabic structure which suggest to use syllabic models to deal with the out-of-vocabulary utterances. Furthermore, Spanish demisyllable units has been used successfully as recognition units in the context of continuous speech recognition [7,8]. Thus, our proposal is to use a compact representation of the vocabulary words and the out-of-vocabulary words in terms of demisyllable units and syllabic fillers. The paper evaluates the performances of the syllabic fillers to deal with out-of-vocabulary utterances. As application task, we use the detection of digits in fluent speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-4"
  },
  "komori92_icslp": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "David Rain",
     "Ton"
    ]
   ],
   "title": "Minimum error classification training for HMM-based keyword spotting",
   "original": "i92_0009",
   "page_count": 4,
   "order": 6,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "This paper compares and contrasts the keyword spotting performance of conventional maximum likelihood trained HMMs vs. that of minimum error trained HMMs. The unique aspect of this work is the use of a new minimum error classification algorithm [1] for training the continuous mixture density HMM components of an HMM-based keyword spotting system. The actual spotting algorithm used was the HMM garbage model approach proposed previously in [2] [3]. Speaker-independent keyword spotting experiments were performed using the ATR Japanese continuous speech database. The reported results show the clear superiority of the minimum error trained HMMs in the chosen keyword spotting application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-5"
  },
  "clary92_icslp": {
   "authors": [
    [
     "Gregory J.",
     "Clary"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A novel speech recognizer for keyword spotting",
   "original": "i92_0013",
   "page_count": 4,
   "order": 7,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "This paper presents a newly formulated speech, recognition algorithm for keyword spotting which uses a feature enhancing artificial neural network, a semi-continuous hidden Markov model, and a likelihood ratio test based on optimal detection theory to make decisions regarding possible keywords. The speech recognizer can be used to detect the occurrences of a single word within connected input speech streams in noisefree neutral or Lombard stressed environments. A keyword-dependent neural network [1] enhances speech, parameters and reduces the probability of false acceptances of non-keywords by adapting its weights and input layer width based on extracted speech characteristics [2]. Using the neural network reduces false acceptances by more than for mono-syllable keywords in a defined keyword spotting application [3]. Enhanced features are submitted to a semi-continuous hidden Markov model which produces a score indicating the presence of the represented keyword. A likelihood ratio test uses functions formed from keyword and non-keyword recognizer training data for detection. Receiver operating characteristics (ROC's) show that the new recognition algorithm can improve keyword spotting performance for neutral and Lombard effect speaking conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-6"
  },
  "gish92_icslp": {
   "authors": [
    [
     "Herbert",
     "Gish"
    ],
    [
     "Kenney",
     "Ng"
    ],
    [
     "J. Robin",
     "Rohlicek"
    ]
   ],
   "title": "Secondary processing using speech segments for an HMM word spotting system",
   "original": "i92_0017",
   "page_count": 4,
   "order": 8,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "In this paper we describe a secondary processing algorithm designed to improve word spotting performance by reducing the sensitivity of a primary HMM word spotting system to false alarms while maintaining high recognition accuracy. The concept behind the algorithm is to rescore the putative events hypothesized by the primary word spotter by generating a \"secondary\" score, which is designed to discriminate between true keyword occurrences and false alarms, and then combining it with the original HMM score. The secondary processor makes use of variable duration speech segments produced by a deterministic acoustic segmentation algorithm. The secondary processing algorithm is evaluated on a keyword spotting task using the Road Rally Database. Performance is shown to improve significantly over that of the baseline word spotting system with the greatest improvements taking place at low false alarm rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-7"
  },
  "feng92_icslp": {
   "authors": [
    [
     "Ming-Whei",
     "Feng"
    ],
    [
     "Baruch",
     "Mazor"
    ]
   ],
   "title": "Continuous word spotting for applications in telecommunications",
   "original": "i92_0021",
   "page_count": 4,
   "order": 9,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "Certain telecommunications services [1] currently handled by human representatives are very attractive for automation using automatic speech recognition (ASR). The interactions associated with those services tend to produce sentences with words and phrases (keywords) embedded in continuous speech input. In this paper, we report the development of speaker-independent continuous word recognition (spotting) capabilities to support the automation of such transactions over the public telephone network. The proposed recognition system uses statistical hidden Markov word models for both keywords and non-keywords. The recognition process includes a Viterbi search which segments the input sentence and generates keyword hypotheses followed by a post-decoder that processes the hypothesis segments, computes various a-posteriori likelihood measures, and determines the final outcomes (keyword name, accept / reject).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-8"
  },
  "copperi92_icslp": {
   "authors": [
    [
     "Maurizio",
     "Copperi"
    ]
   ],
   "title": "A low bit-rate CELP coder based on multi-path search methods",
   "original": "i92_0025",
   "page_count": 4,
   "order": 10,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "This paper deals with the design and performance evaluation of a CELP codec at 3.5 kbit/s based on multi-path search methods. The rationale of this approach stems from the fact that delayed-decision coding algorithms outperform conventional techniques, in which the parameters to be transmitted are optimized sequentially. Tree codes have been applied to both the spectral information, quantized via a multi-stage vector quantizer, and the excitation codevectors, along with relative optimal gains. Preliminary results of this study show that the proposed scheme is a viable approach for achieving good speech quality at bit-rates below 4 kbit/s.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-9"
  },
  "seza92_icslp": {
   "authors": [
    [
     "Katsushi",
     "Seza"
    ],
    [
     "Hirohisa",
     "Tasaki"
    ],
    [
     "Shinya",
     "Takahashi"
    ]
   ],
   "title": "Fully vector quantized arm a analysis combined with glottal model for low bit rate coding",
   "original": "i92_0029",
   "page_count": 4,
   "order": 11,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "In this paper, we propose a new low bit rate speech coding algorithm using a fully vector quantized ARM A analysis combined with glottal model(FVQ-GARMA). Some coding algorithm which estimate spectral parameters and glottal model parameters using Analysis-by-Synthesis (A-b-S) method was proposed to synthesize natural sounding speech. However, these conventional A-b-S methods have some problems - enormous computational load, unstable estimation, high coding bit rate because of pitch synchronous parameter quantization. To solve above problems, in the encoder of FVQ-GARMA, the analysis and the vector quantization are done simultaneously. A set of vector codes (AR,MA and glottal model) that minimizes the distortion between resultant synthetic speech and input speech is selected at every frame. In the decoder, synthetic speech is generated from parameters obtained by interpolating the set of vector codes at every pitch period. Pre-selections to reduce computational load are reported. New glottal model which is represented by 5 parameters in time-domain and easy to control glottal waveform is proposed. We show synthetic speech quality of 2.4 Kbps FVQ-GARMA is equal to that of 4.8 Kbps conventional CELP in subjective tests, and FVQ-GARMA can reduce computational load.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-10"
  },
  "paksoy92_icslp": {
   "authors": [
    [
     "Erdal",
     "Paksoy"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Allen",
     "Gersho"
    ]
   ],
   "title": "Vector quantization of speech LSF parameters with generalized product codes",
   "original": "i92_0033",
   "page_count": 4,
   "order": 12,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "Generalized product code (GPC) vector quantization (VQ) is applied to the coding of speech linear prediction filter parameters. We show that the performance of conventional product code VQ can be improved through the use of conditional feature codebooks and multiple-survivor encoding search. Two particular product code structures, split VQ (SVQ) and multistage VQ (MSVQ), are explored within the GPC framework for the quantization of prediction filter parameters in the line spectral frequency (LSF) domain. Our experiments show that with a suitable MSVQ scheme, 21 bits/frame suffice to encode the LSF parameters to furnish transparent-coding quality. Transparent coding with SVQ requires 22 bits/frame, but only half of the encoding complexity of the best MS VQ scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-11"
  },
  "shoham92_icslp": {
   "authors": [
    [
     "Yair",
     "Shoham"
    ]
   ],
   "title": "Low-rate speech coding based on time-frequency interpolation",
   "original": "i92_0037",
   "page_count": 4,
   "order": 13,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "This paper presents a new algorithm for high-quality speech coding and demonstrates the advantage of the proposed coder over the conventional CELP algorithm for low rate coding. The paper proposes an empirical but perceptually advantageous framework for voiced speech processing, called Time-Frequency Interpolation (TFI). The general formulation of the TFI technique is given first. Then, a 4.2 Kbps speech coder, based on TFI, is described. The performance of this coder is demonstrated in terms of formal MOS scores. It is shown that the 4.2 Kbps TFI coder is comparable in performance to the 8 Kbps North-American cellular standard IS54 coder and to the 13 Kbps European standard GSM coder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-12"
  },
  "taniguchi92_icslp": {
   "authors": [
    [
     "Tomohiko",
     "Taniguchi"
    ],
    [
     "Yoshinori",
     "Tanaka"
    ],
    [
     "Yasuji",
     "Ohta"
    ],
    [
     "Fumio",
     "Amano"
    ]
   ],
   "title": "Improved CELP speech coding at 4 kbit/s and below",
   "original": "i92_0041",
   "page_count": 4,
   "order": 14,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "This paper presents an improved CELP coder operating at 4 kb/s and below, which can provide speech quality equivalent to that reproduced by conventional CELP at around 8 kb/s. To maintain reproduced speech quality while reducing the bit rate, highly efficient coding schemes are employed both for spectral coding and excitation coding. For an efficient encoding of LSP parameters, 2-split, 2-stage vector quantizer (VQ) with partially adaptive codebook was applied. At 24 bits/frame, the proposed VQ exhibits the spectral distortion performance comparable to that of the conventional scalar quantizer at 30 bits/frame. The non-linear interpolation of LSP parameters was also adopted to cope with the degradation of prediction gain caused by the long analysis frame (40 ms). For a better representation of excitation signal, a coding strategy was switched frame by frame according to the phonetic nature of the speech signal. In the voiced frame, pitch synchronous pulse codebook was introduced to generate pitch periodicity. While in the unvoiced frames, computationally efficient stochastic codebook (Tree-delta codebook) was used. As a result of the computer simulation, the performance of our improved CELP coder at 3.4 kb/s was close to that of JDC (Japan Digital Cellular) standard VSELP operating at 6.7 kb/s. The estimated computational complexity of this coder is not as much as twice the complexity of VSELP.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-13"
  },
  "bonafonte92_icslp": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Montse",
     "Pardas"
    ]
   ],
   "title": "Efficient integration of coarticulation and lexical information in a finite state grammar",
   "original": "i92_0045",
   "page_count": 4,
   "order": 15,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "In this paper, an efficient integration of lexical information in a phonetic finite states automaton is presented. The approach is specially suitable for modelling coarticulation occurring inter and intra words. This approach has been used to extend a speech recognition system in order to give the N-Best sentences of words, not acoustic units, and to build complex lexical trees.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-14"
  },
  "leeper92_icslp": {
   "authors": [
    [
     "H. A.",
     "Leeper"
    ],
    [
     "A. P.",
     "Rochet"
    ],
    [
     "I. R. A.",
     "MacKay"
    ]
   ],
   "title": "Characteristics of nasalance in canadian speakers of English and French",
   "original": "i92_0049",
   "page_count": 4,
   "order": 16,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "This investigation was designed to evaluate, instrumentally (Nasometer), the relative amount of oral and nasal acoustic energy (in percent nasalance) displayed in the speech of 1751 Canadian speakers of English and French from several regions of the country. Results of the investigation indicate that there were significant differences among the data for two of the three passages assessed in English and French, indicating different levels of nasalance could be expected in production by normal speakers of various ages. In addition, there were differences in nasalance values for regions of the country and ages of the participants. For several of the passages, there were gender differences with female speakers demonstrating slightly greater nasalance values for certain age groups, languages, and regions of the country. These normative nasalance data in both languages are discussed in terms of the different nature of nasal phonemes (consonants and vowels) in the two languages, the articulatory component of each language, and the characteristics of the Nasometer itself.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-15"
  },
  "shadle92_icslp": {
   "authors": [
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "Andre",
     "Moulinier"
    ],
    [
     "Christian U.",
     "Dobelke"
    ],
    [
     "Celia",
     "Scully"
    ]
   ],
   "title": "Ensemble averaging applied to the analysis of fricative consonants",
   "original": "i92_0053",
   "page_count": 4,
   "order": 17,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "In an attempt to explain across-subject variability in fricative spectra, ensemble-averaged spectra were compared to time-averaged spectra for a corpus that consisted of nonsense syllables of the form /pViFV2/ repeated 10-14 times on one breath. Vi and V2 were chosen from /a,i,u/; F was one of /f,v,0,£, s,z,/,2/. Two subjects were recorded saying both this corpus and another corpus consisting of the same fricatives sustained for 3 seconds. A wide variety of articulatory data was also available for these subjects. An ensemble-averaged spectrum could be computed for the beginning (or middle or end) of all fricative tokens. Results show a pattern of spectral change through the fricative that is consistent with aerodynamic and articulatory measures. The non-stationarity thus revealed does not in itself explain the variability across subjects, however. Rather, the ensemble averaging allows a precision in timing of the analysis window which, when coupled with the articulatory data, shows more clearly the effect of vowel context on fricatives, and delineates the differences between /f/ and /$/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-16"
  },
  "slater92_icslp": {
   "authors": [
    [
     "Andrew",
     "Slater"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "Effects of stress and vowel context on velar stops in british English",
   "original": "i92_0057",
   "page_count": 4,
   "order": 18,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "Locus equations have been used in American and Swedish text-to-speech systems to predict formant frequencies at CV boundaries relative to vowel target frequencies. This work aims at establishing F2 locus equations for velar stops in Southern British English, and assessing their stability as phonetic context varies. Three speakers read all combinations of /i e acnooAu/ in /bVgVb/ words stressed on the first syllable in one list, and on the second in another. F2 locus equations were calculated for /Vg/ and /gV/. Allophonic variation in /gV/s was best accounted for by a 3-way grouping into [High Front], [-High -Round], and [Round] vowel contexts, rather than into [Front] and [Back], or [Front], [-Front -Round], and [Round]. No systematic patterns emerged for locus equations for /Vg/. For /gV/, slopes of regressions were shallower when V2 rather than VI was stressed, implying less CV coarticulation in stressed V2s, as expected. Variation in vowel quality across the consonantal closure changed slopes and increased error terms. Implications for text-to-speech rules and for theories of speech perception are briefly discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-17"
  },
  "caldognetto92_icslp": {
   "authors": [
    [
     "E. Magno",
     "Caldognetto"
    ],
    [
     "K.",
     "Vagges"
    ],
    [
     "G.",
     "Ferrigno"
    ],
    [
     "Maria Grazia",
     "Busa"
    ]
   ],
   "title": "Lip rounding coarticulation in Italian",
   "original": "i92_0061",
   "page_count": 4,
   "order": 19,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The spatio-temporal characteristics of the lip-rounding parameter in the bisyllabic sequences /tiCu/, /tiCCu/ and /tiCCCu/ are presented and discussed. The lip movement analysis was done with ELITE, a device for the tridimensional automatic processing of visible articulatory movements. The results of the analysis showed that the subjects differ from each other significantly. Moreover, the durations of the rounding movement differ, depending on the type and number of intervocalic consonants, while the rounding movement appears to be independent of the presence of a syllabic boundary The data seem to be in agreement with a 'look-ahead' rather than a 'time-locked' theory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-18"
  },
  "smeele92_icslp": {
   "authors": [
    [
     "P. M. T.",
     "Smeele"
    ],
    [
     "A. C.",
     "Sittig"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Intelligibility of audio-visually desynchronised speech: asymmetrical effect of phoneme position",
   "original": "i92_0065",
   "page_count": 4,
   "order": 20,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "In an earlier experiment we studied the effect of desynchronisation of visual and auditory information on the perception of nonsense CVC-words. Predictably, percent correctly recognized words was generally smaller in asynchronous conditions than in the synchronous condition. However, an asymmetrical effect was found: subjects' performance was poorer in the condition where audition preceded vision than when vision preceded audition. In order to gain insight into this asymmetrical effect and its causes, we have now examined subjects' responses to nonsense words on the phoneme level. Analysis of the individual phonemes which made up the original CVC-stimuli indicates that identification of the initial consonant was worse when vision preceded audition than vice versa. In contrast to this, identification of both the vowel and the final consonant was poorer when audition preceded vision. This effect of phoneme position may serve to explain why perception deteriorates more when audition precedes vision than when vision leads.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-19"
  },
  "laine92_icslp": {
   "authors": [
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Speech analysis using complex orthogonal auditory transform (coat)",
   "original": "i92_0069",
   "page_count": 4,
   "order": 21,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "Based on the orthogonal fam functions a complex, orthogonal filterbank is produced to realize a running complex orthogonal auditory transform (COAT). COAT produces time- Bark distributions of the signals under analysis and gives analytic signal at the output of every critical band (Bark-channel). These signals can be further processed to make a detailed harmonic analysis. The harmonic structure can be utilized in its turn for estimation of the fundamental frequency (F0).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-20"
  },
  "gao92_icslp": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Shaoyan",
     "Chen"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Auditory model based speech processing",
   "original": "i92_0073",
   "page_count": 4,
   "order": 22,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "This paper describes a, computational auditory model that be designed to reflect the processing steps and activities occurring in the human auditory systemmm. The model consists of the periphery auditory model and feedback control model from central nervous system to the cochlear. The feedback model is developed based on the efferent-induced effect observed in physiological experiments. The auditory model with and without feedback circuit are compared with the LPC analysis method using clean speech and noisy speech to stimulate the model in order to evaluate the speech processing capability of the model. The preliminary speech recognition results shows that the auditory model is effective for speech processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-21"
  },
  "tajchman92_icslp": {
   "authors": [
    [
     "Gary N.",
     "Tajchman"
    ],
    [
     "Nathan",
     "Intrator"
    ]
   ],
   "title": "Phonetic classification of timit segments preprocessed with lyon's cochlear model using a supervised/unsupervised hybrid neural network",
   "original": "i92_0077",
   "page_count": 4,
   "order": 23,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "We report results on vowel and stop consonant recognition with tokens extracted from the TIMIT database. Our current system differs from others doing similar tasks in that we do not use any specific time normalization techniques. We use a very detailed biologically motivated input representation of the speech tokens - Lyon's cochlear model as implemented by Slaney. This detailed, high dimensional representation, known as a cochleagram, is classified by either a backpropagation or by a hybrid supervised/unsupervised neural network classifier. The hybrid network is composed of a biologically motivated unsupervised network and a supervised back-propagation network. This approach produces results comparable to those obtained by others without the addition of time normalization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-22"
  },
  "holton92_icslp": {
   "authors": [
    [
     "Thomas",
     "Holton"
    ],
    [
     "Steven D.",
     "Love"
    ],
    [
     "Stephen P.",
     "Gill"
    ]
   ],
   "title": "Formant and pitch-pulse detection using models of auditory signal processing",
   "original": "i92_0081",
   "page_count": 4,
   "order": 24,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "Based on an analysis of a model of signal processing by the auditory system, we propose a local time-domain phase-correlation approach to the detection and categorization of sonorant speech features. In this approach, pitch pulses and formant frequencies are marked by characteristic patterns of phase-correlation in the output of groups of frequency-selective filters that correspond to the temporal sequence of firings of groups of nerve fibers in the cochlea. Algorithms for the detection of speech features based on the auditory model appear to improve upon conventional (i.e. spectrographic) techniques in several respects: they are resistant to additive noise, relatively independent of signal amplitude and spectral shaping of the input and speech-specific.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-23"
  },
  "hermansky92_icslp": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Towards handling the acoustic environment in spoken language processing",
   "original": "i92_0085",
   "page_count": 4,
   "order": 25,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Most speech parameter estimation techniques are sensitive to linear distortions introduced by the frequency response of the communication environment. Such linear distortions appear as an additive constant in the log spectrum. RelAtive SpectrAI (RASTA) processing, in which time trajectories of temporal representation of speech are band-pass filtered, makes speech analysis less sensitive to steady-state spectral factors in speech. We describe this approach, compare it to a conventional environment-equalizing blind deconvolution technique, and give experimental results from pilot experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-24"
  },
  "ciaramella92_icslp": {
   "authors": [
    [
     "Alberto",
     "Ciaramella"
    ],
    [
     "Davide",
     "Clementino"
    ],
    [
     "Roberto",
     "Pacifici"
    ]
   ],
   "title": "Real-time speaker-independent large-vocabulary CDHMM-based continuous telephonic speech recognizer",
   "original": "i92_0089",
   "page_count": 4,
   "order": 26,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "We describe a real-time continuous-speech recognition system for large vocabulary, speaker independent telephone accessed applications implemented on a multiple floating-point DSP environment. Both Continuous and Discrete Hidden Markov Models have been implemented; we summarize the performance of both cases and we detail the structure of the multi-DSP implementation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-25"
  },
  "lennig92_icslp": {
   "authors": [
    [
     "Matthew",
     "Lennig"
    ],
    [
     "Douglas",
     "Sharp"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Vishwa",
     "Gupta"
    ],
    [
     "Kristin",
     "Precoda"
    ]
   ],
   "title": "Flexible vocabulary recognition of speech",
   "original": "i92_0093",
   "page_count": 4,
   "order": 27,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Speaker-independent speech recognition over the telephone network has begun to be practical for certain simple transactions such as automation of collect and third-number-billed telephone calls (Lennig, 1990). However, the current state of the art requires that such recognizers be trained on the specific vocabulary to be recognized and requires collecting training tokens of each vocabulary item from hundreds or often thousands of different speakers. The goal of the current study is to obviate the need for vocabulary-specific training, thus allowing a new vocabulary to be introduced merely by providing phonemic transcriptions for the words. In this paper we present speaker-independent results over the telephone network in which not only the speakers but also the vocabularies used in the training and test sets are disjunct. The vocabulary consists of the names of the 1,561 companies having common stock listed on the New York Stock Exchange in mid- 1986. Without the aid of a language model, we have achieved speaker-independent, vocabulary-independent recognition results of 96% correct.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-26"
  },
  "chigier92_icslp": {
   "authors": [
    [
     "Benjamin",
     "Chigier"
    ],
    [
     "Hong C.",
     "Leung"
    ]
   ],
   "title": "The effects of signal representations, phonetic classification techniques, and the telephone network",
   "original": "i92_0097",
   "page_count": 4,
   "order": 28,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "In this paper, we investigate and compare the effectiveness of 11 signal representations for phonetic classification. We also study their interactions with different classification paradigms and feature extraction techniques. In addition, to quantify the effect of the telephone network on high quality wide-band speech, we compare our results on TTMIT and NTIMIT. All our classification experiments have approximately the same number of input dimensions. The signal representations we study fall into a few major categories that are based on: Fourier analysis, linear prediction, cepstral analysis, and auditory processing. Our results indicate that the classification error rate depends on whether the classification technique is well-matched with the signal representation. When a single Gaussian or the multi-layer perceptron is used, the DFT-based representations and PLP tend to have lower error rates than other representations. Compared to our earlier studies, our best error rates have also been reduced to 22% and 30% on TIMIT and NTIMIT, respectively. Furthermore, we have also found that the telephone network increases phonetic classification error rate consistently by a factor of 1.3.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-27"
  },
  "gulikers92_icslp": {
   "authors": [
    [
     "Leon",
     "Gulikers"
    ],
    [
     "Rijk",
     "Willemse"
    ]
   ],
   "title": "A lexicon for a text-to-speech system",
   "original": "i92_0101",
   "page_count": 4,
   "order": 29,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "This paper describes the development of a full-form lexicon, combined with an algorithm for quasi-morphological decomposition aiming at improved grapheme-to-phoneme conversion, word stress assignment, syllabification and word class assignment in a Text-to-Speech system. We will explain the way in which the optimal size of the lexicon was determined. Also, we describe a deterministic algorithm for decomposing words not found in the lexicon in terms of a sequence of lexicon entries and prefixes, suffixes and infixes. The performance of the lexicon+decomposition system is evaluated with a newspaper corpus comprising approximately 100,000 words. It appears that the system handles more than 95% of the regular words in the test corpus correctly. The system will have to be extended with a module that handles proper names.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-28"
  },
  "willemse92_icslp": {
   "authors": [
    [
     "Rijk",
     "Willemse"
    ],
    [
     "Leon",
     "Gulikers"
    ]
   ],
   "title": "Word class assignment in a text-to-speech system",
   "original": "i92_0105",
   "page_count": 4,
   "order": 30,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "Word Class Assignment (WCLA) in a Text-To-Speech (TTS) system is necessary for correct grapheme-to-phoneme conversion, and as a precursor for syntactic analysis (necessary for the calculation of intonation contours). Since the words of a language do not form a closed set (especially in languages like German and Dutch that can freely form new compounds), a lexicon will always be incomplete and thus must be supplemented by modules for morphological decomposition, and rule-based WCLA. Moreover, most words can have several word classes; thus, WCLA requires statistical and/or syntactical evaluation of a word class lattice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-29"
  },
  "bruce92_icslp": {
   "authors": [
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Aspects of prosodic phrasing in Swedish",
   "original": "i92_0109",
   "page_count": 4,
   "order": 31,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "This paper reports on an ongoing research project called 'Prosodic Phrasing in Swedish', the object of which is to investigate prosodic phrasing and grouping in Swedish. Different methods exploited within the project are the analysis of speech production data, the use of text-to-speech synthesis and the use of speech recognition (prosodic parser). Production data from specially designed test material for Swedish has shown that tonal and temporal cues are combined to signal differences in phrasing. To test this analysis a perceptual experiment was conducted using the KTH rule synthesis where duration and F0 could be changed interactively. 12 listeners participated in the test with the task of identifying optimal positions for two distinct interpretations of an ambiguous test sentence as well as a line of ambiguity. The results of the experiment confirm our initial analysis and provide interesting individual variation indicating different perceptual strategies which may also be related to speaking habits. Longer texts are used in prosodic parsing experiments where the task is to identify prosodic phrases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-30"
  },
  "sullivan92_icslp": {
   "authors": [
    [
     "K. P. H.",
     "Sullivan"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Synthesis-by-analogy: a bilingual investigation using German and English",
   "original": "i92_0113",
   "page_count": 4,
   "order": 32,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "Text-to-speech systems traditionally use a pronouncing dictionary from which the phonemic representation of an input word is retrieved. In the case of a 'novel' word absent from the dictionary, letter-to-phoneme rules are invoked to produce a pronunciation. A proposal in the psychological literature, however, is that human readers pronounce novel words (or 'pseudowords' conforming to the spelling patterns of the language) not using explicit rules, but by analogy with letter/phoneme patterns for words they already know. We are developing a synthesis-by-analogy system which is, accordingly, also a model of novel-word pronunciation by humans. We investigate the computational methods of assessing the orthographic analogy module and the 'flexible' (context-independent) GPC rule module, which produces the set of plausible, candidate pronunciations phonemic analogy requires. We compare the resultant assessments across language, method of assessment, size and content of the lexical database. The investigations into these modules produced useful results for both British English and German. However, the best results for each of the two languages were obtained from rather different detailed implementations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-31"
  },
  "manzara92_icslp": {
   "authors": [
    [
     "Leonard C.",
     "Manzara"
    ],
    [
     "David R.",
     "Hill"
    ]
   ],
   "title": "Degas: a system for rule-based diphone speech synthesis",
   "original": "i92_0117",
   "page_count": 4,
   "order": 33,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "This paper describes DEGAS, a Diphone Editor and Generator for Animation and Speech. DEGAS is a simple to use, yet powerful computer program designed to give researchers the means to develop speech synthesis rulebases in a diphone framework. It is an open system, allowing an unlimited number of arbitrary symbols for phones, synthesizer parameters, and categories. An arbitrary number of rules can be formed by combining these symbols with boolean operators. Target values for every parameter are prescribed by the user to define postures for each phone. A transition profile, zero or more special event profiles, and duration rules are specified by the user to define the interpolation between the two sets of targets in a diphone pair. DEGAS provides immediate audio and visual feedback, as well as a means to create a complete diphone inventory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-32"
  },
  "agrawal92_icslp": {
   "authors": [
    [
     "Shyam S.",
     "Agrawal"
    ],
    [
     "Kenneth N.",
     "Stevens"
    ]
   ],
   "title": "Towards synthesis of Hindi consonants using KLSYN88",
   "original": "i92_0177",
   "page_count": 4,
   "order": 34,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "This paper presents results of synthesis of Hindi consonants using KLSYN88 speech synthesizer. All frequently occuring 29 consonants of Hindi were synthesised in the initial position of CVC syllables. The central vowel /a/ and the final consonant III was always used to make the syllables into meaningful Hindi words. The words spoken by a standard Hindi male speaker were digitised at 10K samples per second using a VAX 750 computer system. Techniques employed for analysis include short term DFT magnitude spectrum, variations in formant frequencies, fundamental frequency and amplitude, and display of digital spectrograms. Quantitative acoustic parameters required for synthesis of phonetic features of phonemes were determined. The consonants were synthesised in combination with vowel Id to generate CV syllables and concatenated with the syllable /al/ to form CVC type synthetic words. A number of synthsiser control parameters were interactively varied for each sound till a satisfactory quality of synthetic speech and distinction among all the consonants was achieved. Special attention was paid to the synthesis of stops and affricates with various voicing and aspiration features. These sounds required careful selection and timing of source parameters. The spectral characteristics of synthesised and original sound segments were also compared to further improve the quality of synthetic speech. The results indicate that all the consonants of Hindi speech can be synthesised with natural quality. The perception tests indicate that the intelligibility scores for both types of speech are nearly the same. It is envisaged that further improvements are possible by careful control of synthesizer parameters and improvements in the aspiration source.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-33"
  },
  "pols92_icslp": {
   "authors": [
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "SAM Partners",
     "SAM Partners"
    ]
   ],
   "title": "Multi-lingual synthesis evaluation methods",
   "original": "i92_0181",
   "page_count": 4,
   "order": 35,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "In the past few years a suite of tests has been developed, tested and implemented within the Esprit-SAM project, in order to evaluate the performance of rule synthesizers in many different languages. Since, up to now, no single rule synthesizer has a fully acceptable segmental intelligibility, nor a reasonable prosody, systematic diagnostic evaluation and comparative tests remain necessary. The SAM segmental test, consisting of CV, VC, and VCV nonsense words, following the phonotactic constraints per language, is a proper means for that. Other word types, for instance including consonant clusters, can easily be added. Next there is the SUS test (semantically unpredictable sentences) in which five common grammatical structures are defined, as well as a list of words per word type, allowing one to generate ever-different test material. We also defined an overall quality test by using either a 20-points categorical estimation procedure, or a magnitude estimation procedure, where the subjects adjust the length of a line segment according to their quality judgment. We are developing prosodic tests in which the form and the function of prosodic characteristics in various word and sentence types are evaluated according to their appropriateness. The frequently neglected variability over listeners and between tests is also studied, as well as ways to measure speech quality in an objective way (by using physical means instead of listeners' judgments). Various test procedures have been implemented on a PC in a software package called SOAP.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-34"
  },
  "granstrom92_icslp": {
   "authors": [
    [
     "Björn",
     "Granström"
    ],
    [
     "Petur",
     "Helgason"
    ],
    [
     "Hoskuldur",
     "Thrainsson"
    ]
   ],
   "title": "The interaction of phonetics, phonology and morphology in an icelandic text-to-speech system",
   "original": "i92_0185",
   "page_count": 4,
   "order": 36,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "This paper reports on a joint project between the Royal Institute of Technology in Stockholm and the University of Iceland in Reykjavik. The project involves the adaptation to Icelandic of a (multi-lingual) text-to-speech synthesis system. The main purpose of the project has been the development of a communication aid for the handicapped population. Here, however, we concentrate on certain linguistic aspects of this project, mainly the modelling of the interaction between phonetics, phonology and morphology. Among the issues discussed are the phonetic and phonological aspects of preaspiration, devoicing of sonorants, palatalization, diphthongization and the influence of morphological boundaries on the applicability of certain phonological rules. The results presented illustrate potential of text-to-speech systems as a test-bench for linguistic studies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-35"
  },
  "strik92_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Joop",
     "Jansen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Comparing methods for automatic extraction of voice source parameters from continuous speech",
   "original": "i92_0121",
   "page_count": 4,
   "order": 37,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "Two methods are presented for automatic calculation of the voice source parameters from continuous speech. Both methods are used to calculate the voice source parameters for natural speech. However, for natural speech no objective test procedure seems available. Therefore, both methods were also tested on synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-36"
  },
  "koreman92_icslp": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "Bert",
     "Cranen"
    ]
   ],
   "title": "The influence of linguistic variations on the voice source characteristics",
   "original": "i92_0125",
   "page_count": 4,
   "order": 38,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "We will discuss a method to compaie the voice souice characteristics of consonants belonging to four different manner classes. Results from an experiment will be discussed, and the different voice source characteristics will be related to sonority.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-37"
  },
  "palmer92_icslp": {
   "authors": [
    [
     "Sarah K.",
     "Palmer"
    ],
    [
     "Jill",
     "House"
    ]
   ],
   "title": "Dynamic voice source changes in natural and synthetic speech",
   "original": "i92_0129",
   "page_count": 4,
   "order": 39,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "This paper describes a series of perceptual tests which demonstrate that listeners are capable of perceiving the systematic cycle-by-cycle changes which occur in the voice source waveform of natural speech as a result of laryngeal coarticulation. Having established that these dynamic changes are perceptible in natural speech we attempt to replicate the effect in synthesis using the KLSYN88 software speech synthesiser. This confirms that our synthesis strategy is appropriate and highlights the most important features of the glottal flow waveform needed to model the changes found in natural excitation. We then turn to detailed analysis of the anticipatory and perseverative coarticulation effects on vowels in the context of British English alveolar obstruents for male and female speakers by means of inverse filtering. The findings demonstrate predictable trends in the voice source parameters associated with different phonetic and allophonic variations and are allowing us to develop rules for the voiced excitation in these phonetic environments for high quality text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-38"
  },
  "imaizumi92_icslp": {
   "authors": [
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Jan",
     "Gauffin"
    ]
   ],
   "title": "Acoustic and perceptual modelling of the voice quality caused by fundamental frequency perturbation",
   "original": "i92_0133",
   "page_count": 4,
   "order": 40,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "This paper reports some results to clarify the acoustic arid perceptual characteristics of the voice qualities described by the scales \"Rough\", \"Creak\", \"Fry\" and \"Diplophonia\". Based on acoustic analyses of 102 voice samples, a synthesis model of pitch/amplitude perturbations was constructed. Through perceptual experiments on synthetic voice samples generated using the model, the following results were obtained. 1) The synthesis model constructed based on the acoustic analysis was found capable of generating the differences between the voice qualities denoted \"Rough\", \"Diplophonia\", \"Fry\" and \"Creak.\" 2) The \"Rough\" quality seemed to be perceived when listeners holistically perceived the effect of perturbations as one coherent quality. 3) Other qualities seemed to be perceived when listeners analytically perceived the effect of perturbations as two or more separate sets of frequency components (\"Diplophonia\"), an additional sensation of repeating impulses corresponding to the perturbation frequency (\"Fry\"), and the special case of \"Fry\" observed at final parts of the voice (\"Creak\").\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-39"
  },
  "kiritani92_icslp": {
   "authors": [
    [
     "Shigeru",
     "Kiritani"
    ],
    [
     "H.",
     "Imagawa"
    ],
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "Vocal cord vibration during consonants - high-speed digital imaging using a fiberscope",
   "original": "i92_1661",
   "page_count": 4,
   "order": 41,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "In order to observe vocal cord vibration during running speech, a new system of high-speed digital imaging was developed. The system consists of a fiberscope, a CCD image sensor and a digital image memory. The number of pixels is 128x32 and the frame rate is 2000 frames/sec. A special large size image memory (64Mbyte) was constructed which enables data recording for 6 seconds utterance. By using the system, analysis of vocal cord vibrations during the release of /h/ and /s/ were conducted. For /h/, vibration starts with wider separation of vocal cords. Adductive movement of the glottis continues over several pitch periods during the following vowel and there is a corresponding change in the voice source spectrum which are estimated through inverse filtering of the speech wave.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-40"
  },
  "traum92_icslp": {
   "authors": [
    [
     "David R.",
     "Traum"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "A \"speech acts\" approach to grounding in conversation",
   "original": "i92_0137",
   "page_count": 4,
   "order": 42,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "We propose that Grounding, the process of achieving mutual understanding between participants in a conversation, be analyzed in terms of the actions performed by the conversants which contribute to achieving this mutual understanding. We propose a set of Grounding Acts which facilitate this analysis. This paper describes Grounding Acts, and a \"grammar\" stipulating which series of performance of grounding acts result in grounded content.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-41"
  },
  "meltzer92_icslp": {
   "authors": [
    [
     "Sheila",
     "Meltzer"
    ]
   ],
   "title": "Antecedent activation by empty pronominals in Spanish",
   "original": "i92_0141",
   "page_count": 4,
   "order": 43,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "A cross-modal priming technique was used to look for activation of discourse antecedents by three kinds of subject pronominals available in Spanish. Despite strong pragmatic clues there was no indication of antecedent assignment to PRO, subject of a nonfinite verb. Little pro, by contrast, while also a phonologically null element, induced a significant plausibility effect, indicating activation of the pragmatically plausible antecedent, and exhibiting a pattern similar to that of overt pronouns. These data support a modular model of language processing in which the influence of pragmatic inference does not precede that of syntactically-driven information, where available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-42"
  },
  "smyth92_icslp": {
   "authors": [
    [
     "Ron",
     "Smyth"
    ]
   ],
   "title": "Multiple feature matching in pronoun resolution: a new look at parallel function",
   "original": "i92_0145",
   "page_count": 4,
   "order": 44,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "The goal of this paper is to examine various interpretations of the notion of 'parallel function' in pronoun assignment. The term usually denotes a strategy of assigning an ambiguous pronoun to an antecedent which has the same grammatical role (e.g., subject, object, etc.). However, I will show that the pronoun's grammatical and thematic roles both influence assignment; that differences in clausal attachment and global constituent structure can reduce the proportion of syntactically parallel assignment, consistent with a priming model of the comprehension of multiclause sentences; and that subject pronouns are very strongly biased toward subject assignment because the preceding clause always has a parallel subject NP, and because assignment begins before information (either supporting or contradictory) about the pronoun's thematic role becomes available to the processor. Implications for Al and for language acquisition are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-43"
  },
  "su92_icslp": {
   "authors": [
    [
     "Keh-Yih",
     "Su"
    ],
    [
     "Jing-Shin",
     "Chang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "A discriminative approach for ambiguity resolution based on a semantic score function",
   "original": "i92_0149",
   "page_count": 4,
   "order": 45,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "The performance of a spoken language processing system depends heavily on the ambiguity resolution capability of the underlying language model. Conventional stochastic context-free grammars do not provide contextual information and semantic constraints required for ambiguity resolution. In our formulation, a syntactic score function could be used to enhance the disambiguation capability by taking context-sensitivity into account; a semantic score could be used to further help resolve ambiguities in syntactic and semantic levels. Furthermore, a baseline model may not perform satisfactorily due to unreliable estimation of the parameters or lack of discrimination power and robustness of the baseline model. Therefore, it is desirable to increase the discrimination power and robustness of the baseline system. In this paper, an adaptive learning algorithm, with discrimination and robustness emphasized, is used to improve the baseline systems for ambiguity resolution. Performance improvement over the baseline system will be shown for some preliminary experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-44"
  },
  "minematsu92_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "The influence of semantic and syntactic information on spoken sentence recognition",
   "original": "i92_0153",
   "page_count": 4,
   "order": 46,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "The human process of spoken language recognition utilizes both lower-level acoustic-phonetic information and higher-level linguistic information. The present paper describes two experiments in which the gating paradigm was used to investigate the interaction and the trade-off between these sources of information in word identification and sentence recognition. Experiment I classifies the sentences into five broad categories according to their linguistic contents, and demonstrates the effects of these categorial differences upon word identification in sentences. By introducing the concept of commonness of the event described by a sentence to express the amount of semantic-pragmatic information, Experiment II shows the quantitative trade-off relationship between the acoustic-phonetic information and the semantic-pragmatic information in sentence recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-45"
  },
  "nygaard92_icslp": {
   "authors": [
    [
     "Lynne C.",
     "Nygaard"
    ],
    [
     "Mitchell S.",
     "Sommers"
    ],
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Effects of speaking rate and talker variability on the representation of spoken words in memory",
   "original": "i92_0209",
   "page_count": 4,
   "order": 47,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "The present paper reports a series of experiments designed to investigate the nature of perceptual compensation and the memory representations for spoken words produced at different speaking rates. The aim was to determine if variability in speaking rate has consequences for the encoding and processing of spoken words and if these consequences are comparable to those found for talker variability. A serial recall task was used to study the effects of changes in speaking rate and talker variability on the initial encoding, rehearsal, and recall of lists of spoken words. Presentation rate was manipulated to determine the time course and nature of processing. The results indicate that at fast presentation rates, variations in both speaking rate and talker characteristics incur a processing cost which influences the initial encoding and subsequent rehearsal of spoken words. At slower presentation rates, however, variation in talker results in improved recall in initial list positions while variation in speaking rate has no effect on recall performance. These results suggest that the processing of variability due to changes in speaking rate and talker differences may be the result of distinct operations. Talker information appears to be integrated into long-term representations of spoken words while rate information may be discarded or lost after initial stages of processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-46"
  },
  "quene92_icslp": {
   "authors": [
    [
     "Hugo",
     "Quene"
    ],
    [
     "Yvette",
     "Smits"
    ]
   ],
   "title": "On the absence of word segmentation at \"weak\" syllables",
   "original": "i92_0213",
   "page_count": 3,
   "order": 48,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "This research investigates the metrical segmentation strategy, which states that listeners attempt a lexical access at each metrically strong syllable, with this syllable as potential word onset. This paper reports on a \"word spotting\" experiment, where the target word corresponded with the second syllable of a two-syllable phrase. The onset of the target word was ambiguous. The metrical strength of the target word was manipulated by varying its phonological vowel length and accentuation independently. Neither of these two manipulations had a significant effect on subjects' hit rate or reaction time in spotting the target word. These results can be explained by assuming a different word segmentation strategy for Dutch, as compared to the metrical strategy reported for English. In addition, the results suggest that accentuation has an independent effect on word segmentation, most likely due to the enhanced perceptual salience of accented syllables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-47"
  },
  "sommers92_icslp": {
   "authors": [
    [
     "Mitchell S.",
     "Sommers"
    ],
    [
     "Lynne C.",
     "Nygaard"
    ],
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Stimulus variability and the perception of spoken words: effects of variations in speaking rate and overall amplitude",
   "original": "i92_0217",
   "page_count": 4,
   "order": 49,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "These studies investigated the effects of several sources of naturally occurring variability in speech, both in isolation and in combination, on the perceptual identification of spoken words. Identification accuracy was poorer for word lists containing tokens produced by multiple talkers or at multiple rates compared to the corresponding single-rate/single-talker conditions. Furthermore, simultaneous variations along both rate and talker produced greater reductions in perceptual identification than either source alone. In contrast, variability due to overall amplitude did not significantly alter subjects' ability to correctly identify stimulus items. These findings suggest that the acoustic waveform is subjected to one or more transformations that act upon item-specific information in the signal prior to arriving at phonetic decisions. Implications of the results for models of speech perception are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-48"
  },
  "mcqueen92_icslp": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Words within words: lexical statistics and lexical access",
   "original": "i92_0221",
   "page_count": 4,
   "order": 50,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "This paper presents lexical statistics on the pattern of occurrence of words embedded in other words. We report the results of an analysis of 25000 words, varying in length from two to six syllables, extracted from a phonetically-coded English dictionary (The Longman Dictionary of Contemporary English). Each syllable, and each string of syllables within each word was checked against the dictionary. Two analyses are presented: the first used a complete list of polysyllables, with look-up on the entire dictionary; the second used a sublist of content words, counting only embedded words which were themselves content words. The results have important implications for models of human speech recognition. The efficiency of these models depends, in different ways, on the number and location of words within words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-49"
  },
  "euler92_icslp": {
   "authors": [
    [
     "Stephan",
     "Euler"
    ],
    [
     "Joachim",
     "Zinke"
    ]
   ],
   "title": "Experiments on the use of the generalized probabilistic descent method in speech recognition",
   "original": "i92_0157",
   "page_count": 4,
   "order": 51,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "In this paper we present results of two experiments on discriminative training of speech recognition systems based on the generalized probabilistic descent method. Firstly, we apply the approach as an extension of the traditional Viterbi training in order to estimate parameters of tied density hidden Markov models, namely the mean vectors of the densities and the state specific weights for the densities. Secondly, we discuss a scheme for the combination of scores obtained from models for different feature sets. In this case, the generalized probabilistic descent method is used in order to estimate the weights of the individual model scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-50"
  },
  "cordoba92_icslp": {
   "authors": [
    [
     "Ricardo de",
     "Cordoba"
    ],
    [
     "José M.",
     "Pardo"
    ],
    [
     "Jose",
     "Colás"
    ]
   ],
   "title": "Improving and optimizing speaker independent, 1000 words speech recognition in Spanish",
   "original": "i92_0161",
   "page_count": 4,
   "order": 52,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "We have been working for a few years now in the research and development of 1000 words, isolated word, speaker independent speech recognition in Spanish using discrete HMM. Since we published the last available methods and results, we have been optimizing and improving the system in several directions: Using a new way for phoneme duration incorporation Using a new front end Augmenting the database Using context dependent models and different types of smoothing With all improvements, we have reached 84.18% word recognition for 1006 words vocabulary without grammar.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-51"
  },
  "pitrelli92_icslp": {
   "authors": [
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "Benjamin",
     "Chigier"
    ],
    [
     "Hong C.",
     "Leung"
    ]
   ],
   "title": "Multiple-level evaluation of speech recognition systems",
   "original": "i92_0165",
   "page_count": 4,
   "order": 53,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "Evaluations of speech recognizers typically focus on somewhat idealized versions of the types of utterances the recognizer would confront in a real application. One issue which our group has discussed previously is the use of laboratory speech rather than real-user speech, and the resulting over-optimistic projections of performance. This paper focuses on another problem: typical evaluations often exclude some or all classes of utterances which would occur in a real application but do not precisely match the type of input for which the recognizer was designed. Some example categories are utterances which include excess words not in the recognizer's vocabulary (non-target speech), utterances which lack target speech, and \"utterances\" lacking any speech at all. Such inputs to a recognizer may result from non-compliant users, or from pre-processing errors such as imperfect endpointing. We propose a more comprehensive evaluation strategy, using as an example an evaluation of a recognition system prototype for a city-name-recognition application. Our strategy is designed to meet two goals - to evaluate automation potential realistically, and to provide diagnostic information to pinpoint directions for future work on the system. To these ends, our evaluation treats both the overall system and the individual component modules within it. We learn that a surprisingly wide variety of \"recognition rates\" can meaningfully describe the accuracy of a system or portions of it. Consequently, accuracy statistics must be inteipreted and/or compared with extreme care.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-52"
  },
  "kimura92_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kimura"
    ],
    [
     "Mitsuru",
     "Endo"
    ],
    [
     "Shoji",
     "Hiraoka"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ]
   ],
   "title": "Speaker independent word recognition using continuous matching of parameters in time-spectral form based on statistical measure",
   "original": "i92_0169",
   "page_count": 4,
   "order": 54,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This paper describes a new speaker-independent speech recognition method, which effectively uses dynamic features of speech. The method uses a segment-based parameter which consists of a series of LPC cepstrum coefficients obtained during several frames, which involves the dynamic features. First, a segment-based matching based on a statistical distance measure is performed. To reduce the amount of calculation, the method utilizes a linear discriminant function for segment-level matching. Second, word-level matching is performed by accumulating segment-based likelihoods using either the DTff or the HMM. Experiments to recognize 100 Japanese city names uttered by 50 people show the validity of the present method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-53"
  },
  "roddeman92_icslp": {
   "authors": [
    [
     "R.",
     "Roddeman"
    ],
    [
     "H.",
     "Drexler"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Automatic derivation of lexical models for a very large vocabulary speech recognition system",
   "original": "i92_0173",
   "page_count": 4,
   "order": 55,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "It is well known [2] that the lexicon of any large vocabulary dictation system will have to be tuned to the end user's application. This would be no great problem if lexical entries in standard orthographic or phonemic representation were adequate. However, it appears that recognition performance is much improved if the lexical models can be specified so as to be in accordance with the peculiarities of the Acoustic Front-End (AFE) of the recognizer. This makes updating the lexicon a tedious, error prone process, that requires expertise far beyond what can be expected from the average end user. In a similar vein, during the initial development of a large vocabulary dictation system the recognition performance can be much improved if the lexical models are well adapted to the AFE. Here too, the creation of the lexicon can become almost prohibitive if it must be done completely by hand- For these reasons we have developed a completely automatic procedure for adapting the lexicon to the AFE. We have used this tool in the conversion of an existing Isolated Word Speech Recognition from Italian [1] to Dutch. Use of the tool proved to speed up the development of the systems enormously. The work reported here was done in the framework of the ESPRIT Project POLYGLOT. keywords: Isolated-Word-Recognition preselection lexicon\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-54"
  },
  "cutler92_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Response time as a metric for comparison of speech recognition by humans and machines",
   "original": "i92_0189",
   "page_count": 4,
   "order": 56,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "The performance of automatic speech recognition systems is usually assessed in terms of error rate. Human speech recognition produces few errors, but relative difficulty of processing can be assessed via response time techniques. We report the construction of a measure analogous to response time in a machine recognition system. This measure may be compared directly with human response times. We conducted a trial comparison of this type at the phoneme level, including both tense and lax vowels and a variety of consonant classes. The results suggested similarities between human and machine processing in the case of consonants, but differences in the case of vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-55"
  },
  "ulagaraj92_icslp": {
   "authors": [
    [
     "S. M. (Raj)",
     "Ulagaraj"
    ]
   ],
   "title": "Characterization of directory assistance operator-customer dialogues in AGT limited",
   "original": "i92_0193",
   "page_count": 4,
   "order": 57,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "This work focuses on AGT Limited* Directory Assistance (DA) Operator Services in Edmonton and is based on recordings of operator-customer dialogues. Twenty five hundred conversations were sampled and transcribed into text. A Directory Assistance State Model (DA-SM) was developed from analyzing the customer-operator dialogues. The model represents the logical stages that exist, and the tasks that are performed by the DA operator while handling a call. Using the model, the major call patterns and subpatterns were identified and their characteristics measured. DA-SM suggested that the operator-customer dialogues follows a logical flow in an organized pattern. The primary operator tasks were then identified for the purpose of finding potential speech recognition applications. From the primary tasks it was identified that the best task for streamlining call flow is early prompting of the location information from the customer. A field study conducted during eight weeks suggested that the operator could prompt the customer to provide the location information during initial dialogue. Keywords: Automatic Speech Recognition; Operator Services; Directory Assistance State Model; Human Factors; Speech dialogues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-56"
  },
  "hunnicutt92_icslp": {
   "authors": [
    [
     "Sheri",
     "Hunnicutt"
    ],
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Analysis of the effectiveness of system error messages in a human-machine travel planning task",
   "original": "i92_0197",
   "page_count": 4,
   "order": 58,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "We present an experiment in human-machine spoken language interaction to determine whether subjects made use of diagnostic error messages. These experiments were performed within an air travel domain, using data from 40 subjects who were asked to solve travel planning scenarios by querying a database of airline schedules and fares. A human transcriber typed input to a natural language back-end, which did the remainder of the processing. When the back-end could not process the input, it issued one of several types of diagnostic message, nagging an unknown word, a sequence it couldn't parse, or a failure to retrieve information from the database. First, we classified each error message as to whether it was used to rephrase the following query; second, we determined whether the set of error messages in a discourse segment led to eventual recovery (defined as getting an answer to the original query). Our analysis showed that speakers almost always (86% of the time) made use of the error message in forming their next query. The analysis also showed that subjects could recover (get an answer to their query) in most cases (79%), even though they received an initial error message. We then discuss the effects of a more aggressive understanding strategy, and the problems of error detection and correction in a fully automated system which uses automatic speech recognition rather than a human transcriber.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-57"
  },
  "goodine92_icslp": {
   "authors": [
    [
     "David",
     "Goodine"
    ],
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Evaluating interactive spoken language systems",
   "original": "i92_0201",
   "page_count": 4,
   "order": 59,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "As the DARPA spoken language community moves towards developing useful systems for interactive problem solving, we must develop new evaluation metrics to assess whether these systems aid people in solving problems. In this paper, we report on experiments with two new metrics: task completion and logfile evaluation (where human evaluators judge query correctness). In one experiment, we used two variants of our data collection system (with a human transcriber) to compare an aggressive system using robust parsing to a more cautious \"full-parse\" system. In a second experiment, we compared a system using the human transcriber to a fully automated system using the speech recognizer. There were clear differences in task completion, time to task completion, and number of correct and incorrect answers. These experiments lead us to conclude that task completion and logfile evaluation are useful metrics for evaluating interactive systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-58"
  },
  "jekosch92_icslp": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "The cluster-identification test",
   "original": "i92_0205",
   "page_count": 4,
   "order": 60,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "In this paper a novel general test architecture for measuring speech intelligibility will be introduced. The entire approach is called CLuster-IDentification (CLID). It is a test framework, i.e., a battery of interdependent elementary variables that - in their definite organisational pattern - go to make up any (intelligibility) test. Only by putting these variables into a specific form a particular test is designed. Parameters that have to be itemized when, e. g., the intelligibility of a specific speech output system is aimed at to be assessed are, amongst others, structure of vocabulary, frequency of occurrence of test items, response form, etc. CLID-frame, consequently, is an environment in which factors that have an impact on the assessment procedure (and thus on the test results) are parametrized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-59"
  },
  "kenny92_icslp": {
   "authors": [
    [
     "Patrick",
     "Kenny"
    ],
    [
     "R.",
     "Hollan"
    ],
    [
     "G.",
     "Boulianne"
    ],
    [
     "H.",
     "Garudadri"
    ],
    [
     "Yan-Ming",
     "Cheng"
    ],
    [
     "Matthew",
     "Lennig"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Experiments in continuous speech recognition with a 60,000 word vocabulary",
   "original": "i92_0225",
   "page_count": 4,
   "order": 61,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "We present a new search algorithm for very large vocabulary continuous speech recognition. Continuous speech recognition with this algorithm is only about 10 times more computationally expensive than isolated word recognition. We report preliminary recognition results obtained by testing our recognizer on \"books on tape\" using a 60,000 word dictionary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-60"
  },
  "boulianne92_icslp": {
   "authors": [
    [
     "G.",
     "Boulianne"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Matthew",
     "Lennig"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Paul",
     "Mermelstein"
    ]
   ],
   "title": "HMM training on unconstrained speech for large vocabulary, continuous speech recognition",
   "original": "i92_0229",
   "page_count": 4,
   "order": 62,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "Training hidden Markov models for large vocabulary, continuous speech recognition requires large amounts of data. Books on tape are an easily available, very large source of orthographically transcribed speech data. Use of this source of data is problematic for current training algorithms, however, because they require the speech to be first segmented into isolated sentences. In this paper we present a training algorithm to find the maximum likelihood sequence of states in a phonetic HMM model of an unsegmented, unlimited length speech utterance.\n",
    "The algorithm that we propose has computation time proportional to utterance length, but requires only a fixed amount of memory, independent of utterance length, so that speech input does not have to be segmented into sentences. It considers successive windows on the speech observations. A full Viterbi search is carried on to the end of each window ; then only N paths are retained as the starting paths for the next window. The N survivor paths are not chosen according to their current likelihood, but by looking ahead at their short-time future likelihoods. In practice, we show that N can be reduced to one, while keeping the search optimal, with a limited amount of look-ahead.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-61"
  },
  "rainion92_icslp": {
   "authors": [
    [
     "David",
     "Rainion"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Appropriate error criterion selection for continuous speech HMM minimum error training",
   "original": "i92_0233",
   "page_count": 4,
   "order": 63,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "This paper describes a new algorithm for sentence error minimisation training of phoneme based HMM speech recognition systems. The important aspect of this work is that the minimisation criterion is chosen to directly minimise those errors observable to the final user(2,e. sentence errors), not errors resulting simply as a consequence of the choice of classifier structure, e. phoneme errors). The recognition performance of the resulting minimum error(ME) HMMs is compared against that of standard maximum likelihood(ML) trained models. In every performance measure made the ME HMMs proved superior.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-62"
  },
  "nagai92_icslp": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Kenji",
     "Kita"
    ],
    [
     "Toshiyuki",
     "Hanazawa"
    ],
    [
     "Tadashi",
     "Suzuki"
    ],
    [
     "Tomohiro",
     "Iwasaki"
    ],
    [
     "Tsuyoshi",
     "Kawabata"
    ],
    [
     "Kunio",
     "Nakajima"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "Hardware implementation of realtime 1000-word HMM-LR continuous speech recognition",
   "original": "i92_0237",
   "page_count": 4,
   "order": 64,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "This paper describes the design and development of the architecture of dedicated hardware which recognizes continuous speech sentences using a 1000-word vocabulary. It is based on the HMM-LR mechanism, which is an integration system of speech recognition and language analysis. Many real-time techniques using 33 DSPs for multi-processing were developed for the heavy computation of the trellis algorithms and state duration control. The paper gives performance data with results within a pipeline delay of approximately 2 or 3 seconds, independently of the length of the sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-63"
  },
  "bates92_icslp": {
   "authors": [
    [
     "Madeleine",
     "Bates"
    ],
    [
     "Robert",
     "Bobrow"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Robert",
     "Ingria"
    ],
    [
     "Francis",
     "Kubala"
    ],
    [
     "John",
     "Makhoul"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "David",
     "Stallard"
    ]
   ],
   "title": "Design and performance of HARC, the BBN spoken language understanding system",
   "original": "i92_0241",
   "page_count": 4,
   "order": 65,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "In this paper we describe the design and performance of a complete spoken language understanding system currently under development at BBN. The system, dubbed HARC (Hear And Respond to Continuous speech), successfully integrates state-of-the-art speech recognition and natural language understanding subsystems. The system has been tested extensively on a restricted airline travel information (ATIS) domain with a vocabulary of over 1000 words. In this application, the system functions as an electronic airline guide, searching a database to answer questions posed by the user. HARC is implemented in portable, high-level software that runs in real time on today's workstations to support interactive online human-machine dialogs at a very comfortable pace. No special purpose hardware is required other than an A/D converter to digitize the speech. The system works well for any native speaker of American English and does not require any enrollment data from the users. HARC has shown consistently high performance in formal evaluations on the ATIS domain. At the February 1992 DARPA-sponsored tests administered by the National Institute of Standards and Technology (NIST), the HARC system gave the highest speech recognition and overall understanding performance of all participating systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-64"
  },
  "shirotsuka92_icslp": {
   "authors": [
    [
     "O.",
     "Shirotsuka"
    ],
    [
     "G.",
     "Kawai"
    ],
    [
     "Michael",
     "Cohen"
    ],
    [
     "J.",
     "Bernstein"
    ]
   ],
   "title": "Performance of speaker-independent Japanese recognizer as a function of training set size and diversity",
   "original": "i92_0297",
   "page_count": 4,
   "order": 66,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "Experiments investigated the effects of training set size and diversity of speech data in training an HMM-based, speaker-independent, continuous Japanese speech recognition system. Two different types of diversity were investigated: speaker diversity and phonetic diversity. The results indicate that greater amounts of training data improve recognition performance and that, given a fixed amount of training data, greater diversity of training materials both in terms of speakers and phonetic contexts improve recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-65"
  },
  "yamaguchi92_icslp": {
   "authors": [
    [
     "Kouichi",
     "Yamaguchi"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Kenji",
     "Kita"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Continuous mixture HMM-LR using the a* algorithm for continuous speech recognition",
   "original": "i92_0301",
   "page_count": 4,
   "order": 67,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "This paper describes a new continuous mixture HMM version of the HMM-LR continuous speech recognition system. Each HMM output probability density function is characterized by a 34-component Gaussian mixture. The system performs comparably to the previous system (discrete HMM-LR) even though it does not use duration control requiring considerable computation effort. We also describe two new search methods; an A* algorithm based and a hybrid best-first search. These two new algorithms are experimentally compared with conventional beam search techniques. The A* algorithm based search has been found effective in reducing the number of phoneme verifications by half with little degradation of recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-66"
  },
  "kita92_icslp": {
   "authors": [
    [
     "Kenji",
     "Kita"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Kazumi",
     "Ohkura"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Continuously spoken sentence recognition by HMM-LR",
   "original": "i92_0305",
   "page_count": 4,
   "order": 68,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This paper describes recent efforts to improve the HMM-LR speech recognition system for continuously spoken sentences. The HMM-LR system has been applied to Japanese phrase recognition and has attained high recognition performance. However, up to now, the system has not been applied to continuously spoken sentence recognition. In this work, several improvements have been made on the system. The first improvement is HMM training with continuous utterances as well as word utterances. In previous implementation, HMMs have been trained with only word utterances. Continuous utterances are included in HMM training data because coarticulation effects are much stronger in continuous utterances. The second improvement is the development of a sentential grammar for Japanese. The sentential grammar was created by combining inter- and intra-phrase grammars, which were developed separately. The third improvement is the incorporation of stochastic linguistic knowledge, which includes stochastic CFG and a bigram model of production rules. The system was evaluated using continuously spoken sentences from a conference registration task that include approximately 750 words. A sentence accuracy of 83.9% was attained in the speaker-dependent condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-67"
  },
  "ito92_icslp": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Word pre-selection using a redundant hash addressing method for continuous speech recognition",
   "original": "i92_0309",
   "page_count": 4,
   "order": 69,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "This paper describes a fast word pre-selection method for continuous speech recognition. It is essential to implement a bottom-up word pre-selection method to a continuous speech recognition system because the time for recognition grows as dictionary size increases. In this paper, a new word pre-selection method, \"the extended redundant hash addressing method\" is proposed. This method is an extension of the redundant hash addressing method to word spotting. Moreover, word spotting accuracy is improved using information of phoneme recognition error. Word spotting experiments showed that the following two results: the proposed method was about five times faster than the word spotting using the continuous DP matching and the word spotting accuracy of the proposed method was comparable to that of the continuous DP matching.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-68"
  },
  "ljolje92_icslp": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Michael D.",
     "Riley"
    ]
   ],
   "title": "Optimal speech recognition using phone recognition and lexical access",
   "original": "i92_0313",
   "page_count": 4,
   "order": 70,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "We present an optimal speech recognition system which is based on phone recognition and lexical access. The complete recognition process consists of five consecutive steps: 1) prediction of phone boundary locations; 2) phone likelihood calculation - context independent phone lattice generation; 3) word lattice generation; 4) A* search for n best sentences; 5) rescoring using context dependent models and across word boundary co-articulation. Optimality, in this context, implies that the correct answer is never discarded by any of the stages of the recognition process.\n",
    "Optimal search is achieved by providing phone lattices which are rich enough to preserve the correct phone sequence and yet small enough to allow efficient lexical access routines. Thus separation of the search for the most likely word sequence into consecutive partial searches with a simple interface does not degrade the performance since we insure that the correct answer is always passed on to the next level of processing. This also facilitates explicit modeling of alternative pronunciations within the scope of the DARPA Resource Management Task, which we use for testing the recognition system. The alternative pronunciations are explicitly used in aligning the training utterances when generating the acoustic models, and in the search for the best path through the phone lattice in lexical access. The whole system, although intimately connected to phonetic transcriptions of speech, has been implemented using automatic techniques and never requires human transcription of either training or testing utterances.\n",
    "The best achieved phone recognition performance on the Feb 89 test set is 84.5% accurate and 87.7% correct. The best achieved word recognition accuracy using the word pair grammar, context independent lattices, but without across word co-articulation is 95.1%. After re-scoring on the top 10 sentence candidates using context dependent acoustic models and alternative phonetic realizations obtained with the knowledge of the whole sentence the accuracy improves to 96.6%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-69"
  },
  "waegner92_icslp": {
   "authors": [
    [
     "Nick",
     "Waegner"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "A trellis-based language model for speech recognition",
   "original": "i92_0245",
   "page_count": 4,
   "order": 71,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "This paper discusses tree and trellis-based models as alternatives to conventional n-gram models as the basis for language modelling in automatic speech recognition. The advantage of these models lies in their compactness and the manner in which extended context is used to enhance performance. The latter is confirmed by experiment using models trained and tested on subsets of the Lancaster-Oslo/Bergen corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-70"
  },
  "zoltowski92_icslp": {
   "authors": [
    [
     "Carla B.",
     "Zoltowski"
    ],
    [
     "Mary P.",
     "Harper"
    ],
    [
     "Leah H.",
     "Jamieson"
    ],
    [
     "Randall A.",
     "Helzerman"
    ]
   ],
   "title": "PARSEC: a constraint-based framework for spoken language understanding",
   "original": "i92_0249",
   "page_count": 4,
   "order": 72,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "We have extended Maruyama's [5, 6, 7] constraint dependency grammar (CDG) to process a lattice or graph of sentence hypotheses instead of separate text strings. A post-processor to a speech recognizer producing N-best hypotheses generates the word graph representation, which is then augmented with information required for parsing. We will summarize the CDG parsing algorithm and then describe how the algorithm is extended to process a word graph on a single processor machine.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-71"
  },
  "jones92_icslp": {
   "authors": [
    [
     "G. J. F.",
     "Jones"
    ],
    [
     "J. H.",
     "Wright"
    ],
    [
     "E. N.",
     "Wrigley"
    ]
   ],
   "title": "The HMM interface with hybrid grammar-bigram language models for speech recognition",
   "original": "i92_0253",
   "page_count": 4,
   "order": 73,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "This paper investigates the interface between the HMM pattern matcher and language models for speech recognition. A number of parameters are found to be important to the optimal performance of speech recognition systems. We consider the selection of match factors between these stages in the recognition process and their relationship with the width of the beam search. We develop improvements to a hybrid language model consisting a probabilistic context-free grammar and a bigram able to process both grammatical and non-grammatical speech input. Finally, consideration is given to the development of an algorithm for dynamic variation of the beam search pruning factor based on the ambiguity of the data input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-72"
  },
  "kai92_icslp": {
   "authors": [
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A frame-synchronous continuous speech recognition algorithm using a top-down parsing of context-free grammar",
   "original": "i92_0257",
   "page_count": 4,
   "order": 74,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In this paper, a frame-synchronous continuous speech recognition algorithm using a context-free grammar and its evaluation are described. A frame-synchronous parsing algorithm for context-free grammar based on topdown strategy is used for the prediction of successive words. The parsing process is incorporated into the One-Pass search algorithm [1] with the use of finite-state-automaton. Of course, since the number of the states becomes large, we should use the beam search method on the prediction and prune unreliable candidate trees in the search space. In the evaluation experiments, we used syllable-based continuous density HMMs. By using the proposed algorithm, we obtained the sentence recognition rate of 90% in a speaker adaptation mode, while our conventional system based on word spotting and lattice parsing algorithm obtained 80.7% for the task of perplexity 10.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-73"
  },
  "pereira92_icslp": {
   "authors": [
    [
     "Fernando",
     "Pereira"
    ],
    [
     "David",
     "Roe"
    ]
   ],
   "title": "Empirical properties of finite state approximations for phrase structure grammars",
   "original": "i92_0261",
   "page_count": 4,
   "order": 75,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "The construction of a speech understanding application requires a method for extracting language models of appropriate size and perplexity from the application grammar. We describe a method for approximating context-free grammars by finite-state models with a range of sizes and perplexities and present experimental results of its application to a variety of grammars, including a fairly large grammar for a spoken-language translation application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-74"
  },
  "seneff92_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Language modelling for recognition and understanding using layered bigrams",
   "original": "i92_0317",
   "page_count": 4,
   "order": 76,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "This paper describes a new implementation of the TINA parser, which allows it both to run more efficiently and to achieve greater constraint for the recognizer than the original implementation. It also supports an integrated search that provides both full-parse and robust-parse theories using a best-first search order. In TINA probability assignments are applied to sibling-sibling transitions conditioned on the parent category. However, the left sibling is simply the context-insensitive [start] category whenever the right sibling is the first child. The new implementation restructures the nodes so that they occur in layers, thus allowing these first children to condition their probabilities on a left sibling, even when that left sibling has a different parent. The result is about a 10% improvement in perplexity. The parser was also redesigned so as to support a great deal more structure sharing. The main change is to defer unifications until a complete theory is obtained, so that nodes don't have to be cloned for branching theories. We trained the parser on some 10,000 sentences from the ATIS domain, and achieved a test-set perplexity comparable to that obtained from a word trigram language model. The parse tree also provides a meaning representation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-75"
  },
  "goddeau92_icslp": {
   "authors": [
    [
     "David",
     "Goddeau"
    ]
   ],
   "title": "Using probabilistic shift-reduce parsing in speech recognition systems",
   "original": "i92_0321",
   "page_count": 4,
   "order": 77,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "This paper describes the application of probabilistic shift-reduce parsing to the problem of continuous speech recognition. In previous work, a probabilistic LR parser was applied to the task of speech understanding in the MIT voyager system. The performance metric for that task was based on the fraction of utterances for which correct semantics were produced. For the speech recognition task, word and sentence recognition accuracy are the important performance criteria. In this work, the probabilistic LR language model is extended with robust parsing techniques to achieve 100% coverage, even though the underlying grammar has incomplete coverage. The resulting model provides additional constraints over a word bigram, but retains the trainability and efficiency of the simpler model. Recognition experiments were performed in the DARPA atis task domain using a version of the MIT summit speech recognizer with context-independent phoneme models. Integrating the new language model into the summit JV-best search algorithm decreased the word error rate from 24.1% to 21.5% and the sentence error rate from 72.9% to 65.2%, when compared to a word bigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-76"
  },
  "howells92_icslp": {
   "authors": [
    [
     "Tim",
     "Howells"
    ],
    [
     "David",
     "Friedman"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "Broca, an integrated parser for spoken language",
   "original": "i92_0325",
   "page_count": 4,
   "order": 78,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "Broca is a parser for spoken language in which natural language processing is tightly integrated with lexical and phonological processing. This is in contrast to the N-best approach usually used in speech recognition, in which the natural language component acts as an autonomous post-process to a Viterbi style search. In our system integration is achieved by expressing phonological, lexical, and natural language structures all in the form of an augmented context free grammar. Processing in Broca proceeds through four stages. The speech signal is mapped to a perceptually based reduced representation [1], A neural network classifier produces phoneme estimates at a frame rate of nine milliseconds. This phoneme stream is segmented using a hierarchical clustering algorithm [2]. Then the integrated grammar is dynamically matched against the segmentation by the application of a probabilistic parsing algorithm.\n",
    "The advantage of a parsing approach is that it exploits higher order information present in the speech signal. This potentially includes phonological phenomena such as stress pattern and prosodic grouping, as well as the syntax and semantics of natural language. This information is lost in a Viterbi style search.\n",
    "Broca is a speaker-independent, continuous speech system. We have evaluated it on an in-house database of spoken utterances for an X window manager task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-77"
  },
  "rao92_icslp": {
   "authors": [
    [
     "P. V. S.",
     "Rao"
    ],
    [
     "Nandini",
     "Bondale"
    ]
   ],
   "title": "Blank slate language processor for speech recognition",
   "original": "i92_0329",
   "page_count": 4,
   "order": 79,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "We describe here a novel 'language processor' (LP) which uses syntactic and semantic properties of language for optimising the performance of a Hindi (an Indian language) Speech recognition system for railway reservation enquiry task. The acoustic level recognizer provides several alternatives for each word with associated 'confidence levels'. Using these, the LP has to generate a 'most likely' sentence consistent with the constraints imposed by syntax and semantics. The LP requires no apriori knowledge of the syntax or semantics of the language and acquires this during a training phase. During the recognition phase, it uses this knowledge at the sentence level to correct for word recognition errors made by the acoustic level recognizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-78"
  },
  "jackson92_icslp": {
   "authors": [
    [
     "Eric",
     "Jackson"
    ]
   ],
   "title": "Integrating two complementary approaches to spoken language understanding",
   "original": "i92_0333",
   "page_count": 4,
   "order": 80,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "A current goal in spoken language understanding research is to combine the robustness of domain-specific template fillers (e.g., script and case frame-based systems) with the syntactic coverage of parser-based systems. This paper describes an integration of a pair of systems representing each of these types into a new system that takes advantage of their complementary strengths.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-79"
  },
  "pelillo92_icslp": {
   "authors": [
    [
     "Marcello",
     "Pelillo"
    ],
    [
     "Mario",
     "Refice"
    ]
   ],
   "title": "Learning compatibility coefficients for word-class disambiguation relaxation processes",
   "original": "i92_0389",
   "page_count": 4,
   "order": 81,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "Relaxation labeling processes are iterative procedures that aim at reducing local ambiguities through a parallel exploitation of contextual information. Recently they have been successfully applied to the problem of word-class disambiguation. In relaxation processes contextual information is embedded in a set of \"compatibility coefficients.\" Several statistical-based interpretations have been given for compatibility coefficients, and recently a new optimization standpoint has been introduced. In this paper we discuss these two approaches for learning compatibilities on the basis of a training sample, and suggest a way to combine them. Experimental results conducted over a sample text are given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-80"
  },
  "hatazaki92_icslp": {
   "authors": [
    [
     "Kaichiro",
     "Hatazaki"
    ],
    [
     "Jun",
     "Noguohi"
    ],
    [
     "Akitoshi",
     "Okumura"
    ],
    [
     "Kazunaga",
     "Yoshida"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "INTERTALKER: an experimental automatic interpretation system using conceptual representation",
   "original": "i92_0393",
   "page_count": 4,
   "order": 82,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "This paper presents an experimental automatic interpretation system named INTERTALKER, which recognizes speaker-independent naturally spoken Japanese and English, translates between the two languages, and converts the result into spoken output. In addition, the system also translates the input into French and Spanish at the same time. The system is composed of speech recognition, multi-lingual translation using the pivot method, and rule-based speech synthesis using the pitch controlled residual wave excitation method. Speaker-independent continuous speech recognition is achieved by using demi-syllable speech units. Speech recognition and language translation are tightly integrated using a conceptual representation, a language independent expression of a sentence. The system is robust in its ability to compensate for possible errors in speech recognition, as well as to be tolerant of the grammatical incorrectness and ambiguities in spoken language. Also, because necessary keywords in the network are directly mapped into a conceptual representation, variations in input sentences can easily be accommodated by adding them to the grammar network. The sentence recognition accuracy for the ticket reservation task of 500 word vocabulary was 83%, and its translation accuracy was 93%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-81"
  },
  "morimoto92_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Kazumi",
     "Ohkura"
    ],
    [
     "Masaaki",
     "Nagata"
    ],
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "Enhancement of ATR's spoken language translation system: SL-TRANS2",
   "original": "i92_0397",
   "page_count": 4,
   "order": 83,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "This paper reports an overview of the recent enhancement of ATR's spoken language translation system that can translate Japanese speech to English. First, a speaker adaptation technique is introduced in the speech recognition module so that the system can be available for many users. Second, a phrase category predictor that uses inter-phrase context-free-grammar rules is adopted in the speech recognition module instead of a dependency analysis so that the system can deal with a large vocabulary size of nearly 1,000 words while keeping high recognition accuracy. Third, a new interface between the speech recognizer and the spoken language translator is proposed. Finally, the results of experiments are reported and current performance and future direction of our study are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-82"
  },
  "morimoto92b_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "Continuous speech recognition using a combination of syntactic constraints and dependency relationship",
   "original": "i92_0401",
   "page_count": 4,
   "order": 84,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "This paper proposes a Japanese continuous speech recognition mechanism, in which full sentence level context-free grammar (CFG) and a kind of semantic constraint called the \"dependency relationship between phrases\" are used during speech recognition in integrated way. A dependency relationship in Japanese is a modification relationship between two phrases; it includes adverbial modification relationships such as a case-frame phrase to a predicate phrase, or adnominal modification relationships such as a noun phrase to a noun phrase. To improve efficiency using this kind of relationship in speech recognition, rigorous semantic analysis is not performed, rather, a simple \"matching with examples\" approach is adopted. Experiment has been carried out and results are compared with a case in which only CFG constraints are used. It shows that speech recognition accuracy is improved and overhead is small enough.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-83"
  },
  "pieraccini92_icslp": {
   "authors": [
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Zakhar",
     "Gorelov"
    ],
    [
     "Esther",
     "Levin"
    ],
    [
     "Evelyne",
     "Tzoukermann"
    ]
   ],
   "title": "Automatic learning in spoken language understanding",
   "original": "i92_0405",
   "page_count": 4,
   "order": 85,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "In this paper we propose a mechanism for learning the parameters of a model that constitutes the basis of the natural language component of a speech understanding system. The model defines a representation of the meaning of a sentence as a sequence of elemental semantic units. The sentence production mechanism, in this paradigm, is equivalent to a noisy channel whose input is the sequence of meaning units and whose output is a sequence of acoustic observations. The decoding (i.e., the understanding) is then formalized as the problem of finding the meaning given the acoustic representation. The automatic estimation of the model parameters is possible if a statistically significant set of sentence examples is available, and if each sentence is provided with the correct meaning. Unfortunately a database of sentences annotated with their meaning is not available at the moment. Instead we have a database, within the DARPA ATIS project [4], in which each sentence is given a correct answer. In this paper we discuss the problem of automating the training procedure and we give some experimental results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-84"
  },
  "robb92_icslp": {
   "authors": [
    [
     "Michael P.",
     "Robb"
    ],
    [
     "Harold R.",
     "Bauer"
    ]
   ],
   "title": "Prespeech and early speech coarticulation: american English and Japanese characteristics",
   "original": "i92_0265",
   "page_count": 4,
   "order": 86,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Development of CV coarticulation was examined in one English-reared and one Japanese-reared infant. Vocalizations were collected from each child across their first two years of life. A specific CV form, common in each child's prespeech and early speech vocalizations, was selected for analysis. Coarticulation was evaluated by determining the slope of Fl and F2 transitions following consonant release. Discussion focuses on the speech motor skills characterizing coarticulation development, and the influence of English and Japanese language-learning environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-85"
  },
  "kojima92_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Formation of phonological concept structures from spoken word samples",
   "original": "i92_0269",
   "page_count": 4,
   "order": 87,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper describes a framework for phonological concept, formation, which is the task of acquiring an efficient representation of phonemes from spoken word samples with-out using any transcriptions except for the identification of the words. The phoneme models are represented as networks of segments, each of which forms a compact distribution of spectral features. We call this representation a phonological concept. The learning process is performed by searching in a hypotheses space for which each hypothesis is produced by modifying a set of phoneme models. This system potentially enables us to improve speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-86"
  },
  "rochet92_icslp": {
   "authors": [
    [
     "Bernard L.",
     "Rochet"
    ],
    [
     "Fangxin",
     "Chen"
    ]
   ],
   "title": "Acquisition of the French VOT contrasts by adult speakers of Mandarin Chinese",
   "original": "i92_0273",
   "page_count": 4,
   "order": 88,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "The present study was designed to investigate whether perceptual training in the form of structured identification tasks with synthetic stimuli could have an effect on the perception and the production of voicing contrasts in the stop consonants of Standard French by native speakers of Mandarin Chinese. Twelve adult subjects took part in the experiment, which consisted of a pretest, 3 hours of training in six half-hour sessions, and a posttest. The pretest and the posttest consisted of identifying and imitating natural stimuli, and of identifying synthetic CV stimuli from several continua in which the voice onset time (VOT) duration of the initial stops varied from -60 ms to +130 ms in 10 ms steps. Training materials were 7 sets of synthetic stimuli consisting of a labial stop followed by the vowel /u/, sequenced according to a technique based on perceptual fading. After three hours of training, a) the subjects' identification functions were closer to those of native speakers of French for the trained continuum; b) there had been transfer of the training effect to the other places of consonantal articulation, and to the vowels [a] and [i] for the labial stops; and c) improved performance was also observed at the production level (imitation task). These findings suggest that adults may learn to perceive and produce non-native speech contrasts with limited but structured perceptual training, and that training effect may transfer to phonetic contexts other than those included in the training set. This last result is interpreted as reflecting the fact that although voicing distinctions are actualized at different locations on the VOT continuum in different languages, variations in VOT duration in terms of phonetic context exhibit some well-defined universal tendencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-87"
  },
  "gasser92_icslp": {
   "authors": [
    [
     "Michael",
     "Gasser"
    ]
   ],
   "title": "Phonology as a byproduct of learning to recognize and produce words: a connectionist model",
   "original": "i92_0277",
   "page_count": 4,
   "order": 89,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "This paper investigates the possibility that phonological knowledge emerges out of learning how to process words. A connectionist model of word recognition and production is presented, and a series of experiments is described in which a network is trained to recognize or produce a small set of words from an artificial or a real natural language. In the process of learning these tasks, the network develops internal, distributed representations of its state at different points during processing. In one set of experiments, the internal representations which emerge during a recognition task are treated as inputs to other networks, where their adequacy as syllable representations is tested. It is shown that the representations (1) support word production as well as recognition, (2) support mutation, insertion, and deletion processes, and (3) are robust to noise in the input. In another experiment, representations which develop during a production task are analyzed using a dimensionality reduction technique. It is shown that two of the dimensions exhibit some of the properties of the tiers of autosegmental analyses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-88"
  },
  "hurlburt92_icslp": {
   "authors": [
    [
     "Michael S.",
     "Hurlburt"
    ],
    [
     "Judith C.",
     "Goodman"
    ]
   ],
   "title": "The development of lexical effects on children's phoneme identifications",
   "original": "i92_0337",
   "page_count": 4,
   "order": 90,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "We conducted two experiments to investigate the extent to which lexical knowledge affects children's interpretation of phonetic input In Experiment one, five-year-old children and adults identified the initial phoneme in stimuli from 18 acoustic continua varying in voice onset time (VOT). For each continuum, one of the two possible phonemic categorizations formed a word (e.g., desk) and the other did not (e.g., tesk). Both children and adults tended to identify the initial phoneme as consistent with a word when the tokens were from the middle of a VOT continuum. In addition, children showed a lexical effect at the endpoints of some continua, where acoustic information was clear. Experiment two addressed one possible explanation of the finding that children, but not adults, are influenced by lexical context even when the acoustic input is clear. Children may adopt a strategy to report words, because of the large number of ambiguous targets (i.e., onsets from within the continua). If so, the lexical effect should disappear when all tokens are clear voiced and voiceless stop consonants from the endpoints of the continua. Five-year-olds identified endpoint stop consonants in words and nonwords. The lexical effect was greatly attenuated for these stimuli relative to the endpoint stimuli in Experiment 1, suggesting a strategic component to children's performance. Nonetheless, a small lexical effect remained for these items. The results suggest that five-year-olds engage in very similar processes to adults enabling then to use lexical context to identify phonemes. Under certain conditions, children weigh lexical context more heavily in making phoneme decisions than do adults.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-89"
  },
  "halle92_icslp": {
   "authors": [
    [
     "P. A.",
     "Halle"
    ],
    [
     "B. de",
     "Boysson-Bardies"
    ]
   ],
   "title": "Word recognition before production of first words?",
   "original": "i92_0341",
   "page_count": 4,
   "order": 91,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "The current study examined whether 11-month-old and 12-month-old French infants were able to recognize familiar words in a situation yielding no extra-linguistic cues, before they made identified attempts at producing such words. A head-turn preference paradigm was used to compare infants' interest for familiar words against rare words. Lists of familiar words were auditorily presented to each child from one side, lists of rare words from the other side. A preference for familiar words was found to be very consistent in 12-month-olds, just emerging in 11-montb-olds. These results reveal the existence of a developing receptive lexicon by 11 months, which seems to be closely related to the first production lexicon.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-90"
  },
  "deguchi92_icslp": {
   "authors": [
    [
     "Toshisada",
     "Deguchi"
    ],
    [
     "Shigeru",
     "Kiritani"
    ],
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Fumi",
     "Katoh"
    ]
   ],
   "title": "The effect of fundamental frequency for vowel perception in infants",
   "original": "i92_0345",
   "page_count": 4,
   "order": 92,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Human speech perception shows remarkable abilities to normalize wide variations in speech sounds such as due to speaker difference and context difference. Among such abilities is perceptual normalization of vocal tract size effects. We have been conducting series of experiments on the developmental study of this ability.\n",
    "In our previous paper (K. Sakata et al., 1990), we reported the results of experiments comparing the ability of adults and young children aged 3-5. In this paper, preliminary results on the infants aged 5-7 month will be presented. Experiments on infants have been conducted using the technique of conditioned head-turn responses. The experiments consist of two parts, experiment 1 and 2. Experiment 1 was conducted essentially following the procedures developed by Kuhl et al. Based on the experience in the experiment 1, slightly modified procedures were employed in the experiment 2. At the same time, control experiment on the adult subjects were conducted under experimental conditions similar to that of infants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-91"
  },
  "treurniet92_icslp": {
   "authors": [
    [
     "William C.",
     "Treurniet"
    ]
   ],
   "title": "Objective measurement of phoneme similarity",
   "original": "i92_0281",
   "page_count": 4,
   "order": 93,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "Similarities among phonemes may be measured using auditory discriminations made by human observers or by comparing numerically the features obtained from the acoustic data. A variation of the latter approach is proposed which does not require one to decide beforehand which feature dimensions are relevant. A procedure based on a neural network learning model was developed which yielded a unique, multi-dimensional descriptor for each phoneme. The similarity of a phoneme to each other phoneme could then be easily observed by comparing their respective descriptor vectors. Two methods were employed to examine informally how the distance between any pair of descriptors varied with the perceptual similarity of the phonemes they represented. A dendrogram resulting from a hierarchical clustering analysis of the 49 vectors showed a reasonable grouping of phonemes. Also, a plot of the 49 dimensional arrays mapped to two dimensions via Sammon's (1969) algorithm, showed that spatially adjacent phonemes were quite likely to be perceptually similar phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-92"
  },
  "riley92_icslp": {
   "authors": [
    [
     "Michael D.",
     "Riley"
    ],
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Recognizing phonemes vs. recognizing phones: a comparison",
   "original": "i92_0285",
   "page_count": 4,
   "order": 94,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "This paper evaluates different ways of \"spelling\" a word in a speech recognizer's lexicon. In particular, we compare using, as the source of sub-words units for which we build acoustic models, (1) a coarse phonemic representation, (2) a single, fine phonetic realization, and (3) multiple phonetic realizations with associated likelihoods. We describe how we obtain these different pronunciations and we evaluate them on the DARPA Resource Management Task using the word-pair grammar (perplexity 60). We obtain 93.4% word accuracy using phonemic pronunciations, 94.1% using a single phonetic pronunciation per word, and 96.3% using multiple phonetic pronunciations per word with associated likelihoods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-93"
  },
  "derwing92_icslp": {
   "authors": [
    [
     "B. L.",
     "Derwing"
    ],
    [
     "Terrance M.",
     "Nearey"
    ],
    [
     "R. A.",
     "Beinert"
    ],
    [
     "T. A.",
     "Bendrien"
    ]
   ],
   "title": "On the role of the segment in speech processing by human listeners: evidence from speech perception and from global sound similarity judgments",
   "original": "i92_0289",
   "page_count": 4,
   "order": 95,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "Though most psychologists and linguists have treated the phonemic segment as the basic unit for speech recognition in all of its aspects (perception, storage, retrieval, etc.), the supposed universal character of the phoneme has been questioned in the phonetic and psychological literature. This paper outlines (a) recent work in speech perception which suggests that the segment is the most appropriate perceptual unit, at least for English, (b) a new study that confirms prior work in modeling global sound similarity judgments for English and supports the all-purpose character of the segment, and (c) a recent extension to Arabic that points in the same direction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-94"
  },
  "wiebe92_icslp": {
   "authors": [
    [
     "Grace E.",
     "Wiebe"
    ],
    [
     "Bruce L.",
     "Derwing"
    ]
   ],
   "title": "The syllabic status of postvocalic resonants in an unwritten low German dialect",
   "original": "i92_0293",
   "page_count": 4,
   "order": 96,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "The status of postvocalic liquids and nasals in the nucleus and coda is investigated using speakers who are illiterate in their mother tongue, Low German, but are literate in a second language, English. Using two experimental techniques, it was found that the postvocalic resonants form part of the nucleus, that there was a hierarchy of cohesiveness depending on the resonant and that the complexity of the coda affects the cohesiveness of the vowel-resonant nucleus. Given the varying cohesiveness of the nucleus, a scalar bonding model of the syllable was proposed as more suitable than a strictly hierarchical one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-95"
  },
  "sluijter92_icslp": {
   "authors": [
    [
     "Agaath",
     "Sluijter"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ],
    [
     "A. H.",
     "Neijt"
    ]
   ],
   "title": "The influence of focus distribution and lexical stress on the temporal organisation of the syllable",
   "original": "i92_0349",
   "page_count": 4,
   "order": 97,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "According to Neijt two independent representations for prosodic prominence are needed in languages such as Dutch and English. A non-culminative autosegmental structure with high and low tones accounts for pitch accents in focused constituents, and a culminative metrical structure accounts for the lexical stress position in a word, which is phonetically coded in relative duration. The most far reaching consequence following from this proposal is that relative temporal structure of a word does not change if a pitch accent is shifted to an unstressed syllable. Sluijter showed on the basis of syllable duration measurements that this prediction was untenable. However, since the rhyme of the syllable is the most important part for stress assignment, we reanalysed our duration data for the onset and rhyme portions separately. Our results now support the hypothesis of a culminative metrical structure, which remains observable even when pitch accents do not cooccur.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-96"
  },
  "sluijter92b_icslp": {
   "authors": [
    [
     "Agaath",
     "Sluijter"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "The development and perceptive evaluation of a model for paragraph intonation in dutch",
   "original": "i92_0353",
   "page_count": 4,
   "order": 98,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "The present research aims at a partial model for synthesized text intonation, restricted to utterances consisting of a single intonation phrase. Intonational characteristics of short utterances, embedded in systematically varied positions of a paragraph were investigated. The effects of paragraph position on sentence prosody were mainly located in the beginning of the utterance. A model was formulated capturing the main findings and it was perceptually evaluated within the framework of a system for text-to-speech synthesis. The results indicated that the model contributed to the naturalness of the synthesized intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-97"
  },
  "kaiki92_icslp": {
   "authors": [
    [
     "Nobuyoshi",
     "Kaiki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Pause characteristics and local phrase-dependency structure in Japanese",
   "original": "i92_0357",
   "page_count": 4,
   "order": 99,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "In this paper, the relation between pause characteristics and local phrase dependency structure is examined in about 28,000 accent phrase boundaries in sentence speech data from ten professional broadcasters. The distribution of pause length showed the existence of two main types (long and short). The distribution of pause lengths normalized by tempo showed the two peaks more clearly at one and three mora-lengths. The linguistic analysis of these pause boundaries showed: 1) long pauses, inserted by many speakers, occurred predominantly at right-branching phrase boundaries, phrase boundaries with punctuation marks, and 2) short pauses, showing a less uniform distribution, usually occurred at left-branching phrase boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-98"
  },
  "mobius92_icslp": {
   "authors": [
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Matthias",
     "Pätzold"
    ]
   ],
   "title": "F0 synthesis based on a quantitative model of German intonation",
   "original": "i92_0361",
   "page_count": 4,
   "order": 100,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "This paper presents an adaptation of Fujisaki's quantitative intonation model to German and its application to F0 synthesis by rules. The parameter values of the model were determined by an automatic approximation of naturally produced F0 contours. By means of statistical methods, the sources of variation of the parameter values were examined. On the basis of the significant factors, standard values were derived that capture the effects of both linguistic and speaker-dependent features. A set of rules was formulated in order to generate intonational prototypes that relate to linguistic categories such as sentence modality, sentence accent, and word accent. By means of the TD-PSOLA algorithm, the original F0 data of utterances were replaced by the rule-generated contours. Acceptability of the artificial intonation patterns as well as the adequate realization of linguistic categories were evaluated perceptually by berth phonetically trained subjects and 'prosodically naive' listeners. In the first experiment, both groups of listeners were able to tell the original contours and their close approximations apart, although presumably mainly on the basis of some unavoidable audible effects of the F0 manipulation and not by perceived differences in the intonation contours. As it turned out in the second experiment, the rule-generated contours for declaratives were generally not much less acceptable than the close approximations of the original contours, while the ratings for interrogatives were significantly lower. The experts' ratings were considerably higher than those of the naive subjects. The third experiment aimed at obtaining detailed judgments regarding the realization of word accents and sentence modality that will help to improve several specific rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-99"
  },
  "ross92_icslp": {
   "authors": [
    [
     "K.",
     "Ross"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Factors affecting pitch accent placement",
   "original": "i92_0365",
   "page_count": 4,
   "order": 101,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "Correct pitch accent placement can substantially improve the naturalness of text-to-speech synthesis, and has been acknowledged as an important problem in synthesis research. This paper describes an empirical study of the factors that affect pitch accent placement based on the Boston University radio news corpus. Issues examined included accent placement on function words, relative importance of different influencing factors, early accent placement within the word, and variability across speakers. Incorporating the results of these four analyses into questions for decision-tree-based pitch accent prediction results in an algorithm with an accuracy of 95% on syllables marked in all of four read versions of a story and only 7% false assignment of pitch accent to syllables not marked by any of the speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-100"
  },
  "swerts92_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Ronald",
     "Geluykens"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Prosodic correlates of discourse units in spontaneous speech",
   "original": "i92_0421",
   "page_count": 4,
   "order": 102,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "This paper describes an acoustic analysis and perceptual evaluation of the prosodic structure of a spontaneously produced monologue. It was found that a speaker can demarcate larger-scale topical units in spoken discourse by means of intonation (use of melodic boundary markers, scaling of maxima in pitch movements, general decline in average F0) and temporal structure, i.e. by the use of pauses with variable durations. In a perception test, it was examined to what extent this prosodic structure may be important to listeners. Subjects were confronted with three unintelligible (band-pass filtered) versions of a fragment of the elicited monologue: (1) with the original prosody unchanged; (2) with constant pause duration and the original speech melody; (3) with monotonous pitch and the original pause structure. They were instructed to indicate the boundaries of the larger-scale topical units in the three versions. Subjects were able to detect correctly the major discourse boundaries in all three filtered versions in a significant amount of cases. They performed best when confronted with version 1. Version 2 in its turn did better then version 3, which suggests that intonation is a perceptually more important factor than pause structure for the clarification of the thematic make-up of a text, though the latter dimension is certainly not negligible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-101"
  },
  "nakajima92_icslp": {
   "authors": [
    [
     "Shin'ya",
     "Nakajima"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "Prosody as a cue for discourse structure",
   "original": "i92_0425",
   "page_count": 4,
   "order": 103,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "This paper describes how well prosodic information correlates with the topic structure of discourse. To investigate this correlation systematically, first we introduce the notion of utterance unit which can be viewed as the basic unit in conversations. We then define four topic boundary classes: Topic Shifty Topic Continuation, Elaboration, and Speech-Act Continuation. The prosodic parameters- onset, first-peak, and final pitch frequencies- are measured at these topic boundaries to show how these prosodic parameters vary with the topic structure. Finally, we propose a schematic algorithm which identifies the topic boundaries via the prosodic parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-102"
  },
  "grosz92_icslp": {
   "authors": [
    [
     "Barbara",
     "Grosz"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Some intonational characteristics of discourse structure",
   "original": "i92_0429",
   "page_count": 4,
   "order": 104,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "This paper reports on a study of the relationship between acoustic-prosodic variation and discourse structure, as determined from an independent model of discourse. We present results of two pilot studies. Our corpus consisted of three AP news stories recorded by a professional speaker. Discourse structure was labeled by subjects either from text alone or from text (with all orthographic markings except sentence-final punctuation removed) and speech, following Grosz & Sidner 1986; average inter-labeler agreement for structural elements varied from 74.3%-95.1%, depending upon feature. These elements of global structure, together with elements of local structure such as parentheticals and attributive tags, were correlated with variation in intonational and acoustic features such as pitch range, contour, timing, and amplitude. We found statistically significant associations between aspects of pitch range, amplitude, and timing with features of global and local structure both for labelings from text alone and for labelings from speech. We further found that global and local structures can be reliably identified from acoustic and prosodic features with (cross-validated) success rates of 86-97%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-103"
  },
  "fujisaki92_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Haitao",
     "Lei"
    ]
   ],
   "title": "Prosody and syntax in spoken sentences of standard Chinese",
   "original": "i92_0433",
   "page_count": 4,
   "order": 105,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "While the tonal features of Chinese isolated syllables are quite clear and well studied, tonal features of sentences are less well defined and thus scarcely analyzed. The present study utilizes an analytic and quantitative formulation of the process of fundamental frequency contour generation to decompose an F0 contour into its constituents: the phrase components and the tone components. Analysis of a number of sentences, uttered by two male speakers of Standard Chinese, revealed some important phenomena such as tone sandhi and phrasing. Prosodic structures, partially manifested by the phrase components, are often found to be different from the syntactic structures given by the generative grammar, but are believed to reflect the actual process of utterance generation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-104"
  },
  "bishop92_icslp": {
   "authors": [
    [
     "Kathleen",
     "Bishop"
    ]
   ],
   "title": "Modeling sentential stress in the context of a large vocabulary continuous speech recognizer",
   "original": "i92_0437",
   "page_count": 4,
   "order": 106,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "Recently, researchers have been studying the representation of \"stress\" and its relation to continuous phone recognition and continuous speech recognition. [4] has claimed an improvement in error rate due to an explicit marking of lexical stress when performing continuous speech recognition. On the other hand, [1] reported that using two levels of stress (as opposed to one level) did not reduce the error rate when performing phone recognition of continous speech. The English speaker-dependent continuous speech recognition system developed by Dragon Systems currently uses three levels of lexical stress for each of seventeen vowels. The English phoneme alphabet used by the system also includes twenty-six consonants (including three syllabic consonants, /M/, /N/, and /L/), totaling seventy-seven phonemes. The different stress levels lead to a large number of parameters to estimate when training models and performing recognition tasks. Motivated by the desire to downsize the parameter set, this paper is a preliminary study of how to maintain recognition performance when the number of stress levels is reduced. Performance results are reported in terms of the Wall Street Journal 5000-word vocabulary recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-105"
  },
  "ohkura92_icslp": {
   "authors": [
    [
     "Kazumi",
     "Ohkura"
    ],
    [
     "Masahide",
     "Sugiyama"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speaker adaptation based on transfer vector field smoothing with continuous mixture density HMMs",
   "original": "i92_0369",
   "page_count": 4,
   "order": 107,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "This paper describes a method of speaker adaptation for continuous mixture density HMMs (CDHMMs). Speaker adaptation in CDHMMs is regarded as a kind of retraining problem where a small amount of training data is available. The \"Vector Field Smoothing method (VFS)\" is used to deal with the problem of retraining with insufficient training data. \"VFS\" is applied simultaneously to inter-speaker and speaking-style adaptation. In this paper, the standard speaker is a male and the unknown speakers for adaptation are both one male and one female. When 11 sentences are uttered for adaptation phrase-by-phrase instead of word-by-word, the 23 phoneme recognition rate is 87.4% (none adaptation: 47.3%). The phrase recognition rate for HMM-LR is 85.1% (none adaptation: 21.5%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-106"
  },
  "matsubka92_icslp": {
   "authors": [
    [
     "Tatsuo",
     "Matsubka"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaker adaptation by modifying mixture coefficients of speaker-independent mixture Gaussian HMMs",
   "original": "i92_0373",
   "page_count": 4,
   "order": 108,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "This paper proposes a new speaker adaptation method that uses speaker-independent HMMs as initial models, and it emphasizes the feature distribution of the target speaker's speech during adaptation training. A mixture Gaussian HMM with a large number of distributions attains good recognition accuracy for speaker-independent speech recognition when sufficient training speech is available. The proposed method uses such speaker-independent mixture Gaussian HMMs as initial models, and modifies mixture coefficients to maximize the likelihood for the target speaker. This method does not require phoneme segmentation and labeling of training speech, although it uses supervised training.\n",
    "The adapted models using this method were evaluated by comparing with speaker-independent and speaker-dependent models. When the number of training words was less than 200, the speaker-adapted model achieved better recognition than either the speaker-independent or the speaker-dependent models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-107"
  },
  "gong92_icslp": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Minimization of speech alignment error by iterative transformation for speaker adaptation",
   "original": "i92_0377",
   "page_count": 4,
   "order": 109,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "Adapting a speech recognition system to a new-speaker may require parameter space transformation. By this transformation the space of the new speaker is mapped into a reference space, thus improving the quality of the similarity measure.\n",
    "The transformation is determined using vector pairs given by Dynamic Time Warping (DTW) for template-based systems or, equivalently by Viterbi alignment for hidden Markov model based systems, of utterances of the reference speaker against that of a new speaker.\n",
    "Traditionally, the vector pairs are given by a single DTW. It is assumed that the vector-wise acoustic similarity measure in DTW actually measures the phonetic similarity. However, since utterances by different speakers correspond to different spectral spaces which can be totally different, the acoustic similarity measure used by DTW may not correspond to phonetic similarity. From a phonetic point of view, the acoustic similarity measure may have no meaning.\n",
    "We propose to optimize the alignment by iterative DTWs, in order to reduce incorrect vector mappings. The principle consists in progressively moving, by applying transformation, the parameter space occupied by the new speaker's utterance, towards the space of the reference speaker's. At each move, a new alignment is determined, which is presumed to be phonetically better than the previous one. Using this alignment, a new transformation is estimated and applied. The procedure stops when no alignment improvement can be observed. Experiments show significant decrease of alignment error.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-108"
  },
  "hattori92_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Hattori"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Vector field smoothing principle for speaker adaptation",
   "original": "i92_0381",
   "page_count": 4,
   "order": 110,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "This paper describes a new supervised speaker adaptation method based on vector field smoothing, for small size adaptation data. This method assumes that the correspondence of feature vectors between speakers can be viewed as a kind of smooth vector field, and interpolation and smoothing of the correspondence are introduced into the adaptation process for higher adaptation performance.\n",
    "The proposed adaptation method was applied to discrete HMM based speech recognition. The evaluation experiments showed that the proposed method with 10 word adaptation data, produced almost the same results as the conventional codebook mapping method with 25 words. These experiments clearly confirmed the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-109"
  },
  "kobayashi92_icslp": {
   "authors": [
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Spectral mapping onto probabilistic domain using neural networks and its application to speaker adaptive phoneme recognition",
   "original": "i92_0385",
   "page_count": 4,
   "order": 111,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "A feature parameter space called PRPG (Probability Ratios between Phoneme Group pairs) is utilized for speaker adaptive phoneme recognition. The coordinate conversion is performed by neural networks. Each outputnode of the network represents a posteriori probability of phoneme group. Therefore, distance in the PRPG coordinate system corresponds directly to the difference of likelihood. The area with the same information for speech recognition is compressed into one point. Moreover, by the definition of the coordinate system, the meaning of axes are equivalent among different speakers, so the speaker adaptation can be easily performed without trajectory mapping. The experimental results show that the scores of the speaker-adaptive recognition in the PRPG domain are always superior to those of the speaker-dependent recognition in the spectral domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-110"
  },
  "lefevre92_icslp": {
   "authors": [
    [
     "Jean-Paul",
     "Lefevre"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "Claudio",
     "Maggio"
    ],
    [
     "Mario",
     "Refice"
    ],
    [
     "Fabio",
     "Gabrieli"
    ],
    [
     "Michelina",
     "Saving"
    ],
    [
     "Luigi",
     "Santangelo"
    ]
   ],
   "title": "An interactive system for automated pronunciation improvement",
   "original": "i92_0409",
   "page_count": 4,
   "order": 112,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "In this presentation, we will introduce a project whose final objective is the development of a workstation designed to improve the pronunciation of a given language by non-native speakers. The SPELL (Interactive System for Spoken European Language Training) workstation is intended to be a self-instructional device aimed at intermediate ability foreign language learners. Three languages are involved including English, French and Italian. This study is based primarily on the analysis of the speech characteristics of non-native speakers and on the development of tools to be used in the automated assessment and improvement of non-native language pronunciation. The output of the first phase of the project will be an initial demonstrator system able to process the speech of non-native speakers in order to identify and correct pronunciation errors. This will involve the analysis of the phonetic characteristics of the speaker's mother tongue, the identification and measurement of his/her pronunciation errors, as well as the use of aids helping him/her to understand his/her errors and showing how he/she can correctly pronounce words and short phrases using both audio and visual feedbacks. In a second phase, refinements will be introduced in order to achieve a marketable product. The technical objectives of the project are to develop methods for analyzing the characteristics of speech produced by non-native speakers (both at macro and micro levels), to develop metrics for identifying differences between a non-native speaker's pronunciation and a model offered by the system, and to provide user friendly feedback which will help to improve pronunciation. The main technical innovation behind SPELL is the application of well founded phonetic and phonological principles for teaching selected aspects of the pronunciation of the language under consideration. This paper begins with a general description of the framework of the SPELL system. This general discussion will set out some basic assumptions about the system and the phonetic issues which it addresses. Then, some technical issues about the user interface software, which takes advantage of an object oriented approach in a powerful windowing environment, are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-111"
  },
  "rooney92_icslp": {
   "authors": [
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Steven M.",
     "Hiller"
    ],
    [
     "John",
     "Laver"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Prosodic features for automated pronunciation improvement in the spell system",
   "original": "i92_0413",
   "page_count": 4,
   "order": 113,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "This presentation describes the analysis of the prosodic features of intonation and rhythm within the SPELL system, a workstation for the automated assessment and improvement of English, French and Italian pronunciation by non-native speakers. For each language, a limited range of phonologically distinctive intonation contours has been chosen. These contours are characterized using a system of pitch anchor points and pitch trajectories. A similarity metric evaluates the acceptability of a student's intonation using a smoothed fundamental frequency contour and an automatic segmentation of the student's utterance derived by a Hidden Markov Model (HMM) technique. The analysis of rhythm concentrates on the control of salience relationships within an utterance (the contrast between weak and strong syllables) using the parameters of vowel quality and duration only. Judgements arc expressed in terms of the weak-strong syllable contrast, obtained indirectly using the HMM segmenter with phrase models which allow for errors on these two parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-112"
  },
  "benedetto92_icslp": {
   "authors": [
    [
     "Maria-Gabriella Di",
     "Benedetto"
    ],
    [
     "Fabrizio",
     "Carraro"
    ],
    [
     "Steven M.",
     "Hiller"
    ],
    [
     "Edmund",
     "Rooney"
    ]
   ],
   "title": "Vowels pronunciation assessment in the spell system",
   "original": "i92_0417",
   "page_count": 4,
   "order": 114,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "The Spell research on the phonetic segmental aspects of speech is focused on the class of vowels. The aim of this aspect of the Spell project is to succeed in indicating to a given speaker speaking a non-native language (Italian, French, or English (American English and British English RP)) how to improve the pronunciation of vowels belonging to the non-native vowel system. In order to automatically provide the information needed to improve the pronunciation, a vowel representation capable of guiding the speaker must be selected. Two different vowel representation strategies, based on two different normalization methods, were implemented. An analysis of the efficiency of these procedures within the Spell system in the case of the Italian vowel system is reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-113"
  },
  "poirier92_icslp": {
   "authors": [
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "Self-organizing map with supervision for speech recognition",
   "original": "i92_0459",
   "page_count": 4,
   "order": 115,
   "p1": "459",
   "pn": "462",
   "abstract": [
    "This paper proposes a new connectionist model of supervised learning called WYLINWYT map, based on Kohonen's Self Organizing Map (SOM). SOM has been early used in speech processing. It produces, by an unsupervised learning, a topological representation of the speech input space. The unsupervised learning mode is not fitted to obtain acceptable results in speech recognition. WYLINWYT shows a new learning scheme that improve speech recognition capabilities of SOM. WYLINWYT combines the advantages of a topological representation in the map space and the discriminating power of supervised learning. Phoneme recognition experiments are performed in a corpus of 200 phonetically balanced sentences. Compared to basic SOM, the proposed recognition method shows an improvement in the recognition accuracy of more than 5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-114"
  },
  "haan92_icslp": {
   "authors": [
    [
     "Gregory R. De",
     "Haan"
    ],
    [
     "Ömer",
     "Egecioglu"
    ]
   ],
   "title": "Topology preservation for speech recognition",
   "original": "i92_0463",
   "page_count": 4,
   "order": 116,
   "p1": "463",
   "pn": "466",
   "abstract": [
    "When comparing Self-Organizing Feature Map (SOFM) learning with the generalized Lloyd algorithm for vector quantizer design, it becomes apparent that an implicit goal of SOFM learning is to reduce the summed distortion between the input vector and all vectors in the neighborhood of the closest SOFM weight vector. We present appropriate neighborhood-based distortion measures for SOFM and derive a variant of the generalized Lloyd algorithm which, unlike traditional SOFM learning, monotonically reduces neighborhood distortion. We show that topology preservation naturally results from minimization of neighborhood distortion, and such distortion can be used to quantify the degree to which topology is preserved in SOFMs. We also report on the use of the topology preserving properties of SOFMs for speaker independent isolated digit recognition. SOFMs are found to be effective for input pattern normalization, pattern recognition, and feature integration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-115"
  },
  "bradshaw92_icslp": {
   "authors": [
    [
     "Gary",
     "Bradshaw"
    ],
    [
     "Alan",
     "Bell"
    ]
   ],
   "title": "Towards the performance limits of connectionist feature detectors",
   "original": "i92_0467",
   "page_count": 4,
   "order": 117,
   "p1": "467",
   "pn": "470",
   "abstract": [
    "Attempts to improve the performance of a connectionist network trained to detect selected phonetic features in multispeaker connected speech indicate some of the limitations on the information available at a peripheral level of speech analysis. The three-layer feedforward network has 12 detector outputs, and is trained over large subsets of sentences from the MIT Ice Cream database. Its input consists of smoothed spectral vectors sampled at 15 msec intervals. Little contextual information is available to the detectors, since each vector has an effective window of about 30 msec. Overall, the detector network generalizes very well to new speakers and new sentences: average a' drops only to .93 on test data from .94 on training data. Frequently occurring features like sonorance are better discriminated than infrequent ones like rhotic, mainly because the learning algorithm gives greater weight to the many negative training instances than to the few positive ones; discrimination is improved by weighting the learning rate for positive and negative vectors in inverse proportion to their frequency of occurrence. Performance was also improved modestly by adding preceding and following vectors to the input. Several other modifications yielded little or no performance improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-116"
  },
  "sorensen92_icslp": {
   "authors": [
    [
     "Helge B. D.",
     "Sorensen"
    ]
   ],
   "title": "Context-dependent and -independent self-structuring hidden control models for speech recognition",
   "original": "i92_0471",
   "page_count": 4,
   "order": 118,
   "p1": "471",
   "pn": "474",
   "abstract": [
    "Most of the Neural-, Markov- or Hybrid-(Neural/Markov) models for continuous speech recognition or isolated word recognition have fixed architecture during training. A possible consequence is non-optimal and often too small or too large models. At ICASSP92 we proposed a Self-structuring Hidden Control (SHC) neural model [1] for isolated word recognition. These self-structuring models can generate near-optimal model architectures during training. In this paper we extend this work on isolated word recognition and describe how SHC models can be integrated in a continuous speech recognition system. Context-Independent SHC models can model isolated words or phones efficiently and Context-Dependent SHC models can model triphones efficiently. The results presented in this paper show that CI-SHC models and CD-SHC models are potential alternatives to traditional pattern models. Furthermore an analysis of the self-structuring capabilities of SHC models is presented by showing the relation between pattern complexity and model complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-117"
  },
  "caraty92_icslp": {
   "authors": [
    [
     "Marie-José",
     "Caraty"
    ],
    [
     "Claude",
     "Montacié"
    ],
    [
     "Claude",
     "Barras"
    ]
   ],
   "title": "Integration of frequential and temporal structurations in a symbolic learning system",
   "original": "i92_0475",
   "page_count": 4,
   "order": 119,
   "p1": "475",
   "pn": "478",
   "abstract": [
    "This paper presents an approach to integrate frequential and temporal structurations of the speech signal in a symbolic learning and rule-based recognition process. This approach is evaluated on experiments of vowels recognition in continuous speech and compared with a statistical approach. To take into account the frequential and temporal structurations, we choose the Clustering of Spectral Peaks (CSP) measure based on peak parameters space and the Generalized Temporal Decomposition (GTD) modeling the spectral evolution. Charade is the chosen symbolic learning system. From our experiments, a priori strategic temporal information, such as localization or segmentation obtained from GTD technique using CSP measure, is not shown to be useful information for vowels identification. Our comparative study symbolic versus statistical approach is very encouraging for further research on symbolic process. Indeed, the results obtained from symbolic approach are quite comparable to the best results obtained from statistical approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-118"
  },
  "monte92_icslp": {
   "authors": [
    [
     "E.",
     "Monte"
    ],
    [
     "José B.",
     "Marino"
    ],
    [
     "Eduardo",
     "LLeida"
    ]
   ],
   "title": "Smoothing hidden Markov models ay means of a self organizing feature map",
   "original": "i92_0535",
   "page_count": 3,
   "order": 120,
   "p1": "535",
   "pn": "538",
   "abstract": [
    "This paper proposes a method for smoothing the Hidden Markov Models (HMM) with the VQ done by means of the Self Organising Feature Maps (SOFM). The use SOFM gives rise to a special property of the probability of emission matrix of the HMM. This property is that when ordering the probability of emission matrix following the order of the SOFM; neighbouring symbols will have similar probabilities. In order to smooth the HMM we propose to filter the probability of emission matrix by a filter that makes use of this property. We also compare this method with another method for smoothing the HMM; the coocurrence method. The recognition rate improvement achieved by the method that we propose is better than the recognition rate obtained by means of the coocurrence method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-119"
  },
  "mantysalo92_icslp": {
   "authors": [
    [
     "Jyri",
     "Mantysalo"
    ],
    [
     "Kari",
     "Torkkola"
    ],
    [
     "Teuvo",
     "Kohonen"
    ]
   ],
   "title": "LVQ-based speech recognition with high-dimensional context vectors",
   "original": "i92_0539",
   "page_count": 4,
   "order": 121,
   "p1": "539",
   "pn": "542",
   "abstract": [
    "In this paper we have applied the Learning Vector Quantization methods, including the latest developments [1, 3, 2, 4], to the task of Finnish speaker-dependent speech recognition. The main objective was to study the effect of radically increasing the dimensionality of the context vectors. The high-dimensional feature vectors in our work represent the whole phoneme and they are formed by both averaging and concatenating shorttime feature vectors within a time domain window. Excellent results are achieved in separate phoneme classification of Finnish speech. Moreover, we also show how this method can be applied in combined labeling and segmentation of continuos speech. In this task we use an additional segmentation LVQ-codebook, and combine the information using HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-120"
  },
  "kurimo92_icslp": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Kari",
     "Torkkola"
    ]
   ],
   "title": "Application of self-organizing maps and LVQ in training continuous density hidden Markov models for phonemes",
   "original": "i92_0543",
   "page_count": 4,
   "order": 122,
   "p1": "543",
   "pn": "546",
   "abstract": [
    "We present experiments in using neural network based methods to initialize continuous observation density hidden Markov models (CDHMMs). Proper initialization provides an easy way to avoid excessive amount of iterations, when maximum likelihood algorithms are used to estimate the parameters of CDHMMs. This is important in, for example, phoneme based automatic speech recognition, where the output density functions of the states of HMMs are complex and a lot of training data must be used. In our work CDHMMs are used as phoneme models in the task of transcribing speech into phoneme sequences. The probability density function of the output distribution for a state is approximated by mixture of a large number of multivariate Gaussian density functions (typically 25). We present experiments of initializing the means of mixture Gaussians by Sell-Organizing Maps (SOMs) and Learning Vector Quantization (LVQ). The results of the experiments indicate that initialization by SOMs speeds up the convergence in ML-parameter estimation, when error rate is used as a measure. The same applies to LVQ especially combined with segmental K-means algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-121"
  },
  "dalsgaard92_icslp": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ]
   ],
   "title": "Identification of mono- and poly-phonemes using acoustic-phonetic features derived by a self-organising neural network",
   "original": "i92_0547",
   "page_count": 4,
   "order": 123,
   "p1": "547",
   "pn": "550",
   "abstract": [
    "The research reported here is concerned with two analyses in which the identification of individual phonemes, defined across three European languages, and the discrimination between groups of phonemes are being pursued.\n",
    "First, a set of acoustic-phonetic features are used to identify individual poly-phonemes, whose realisational properties across several languages are similar enough to be equated, and individual mono-phonemes, which are phonemes treated separately in these languages. Second, a confusion matrix is established from which it is possible to identify the confusions between and across sets of poly-and monophonemes. As a basis for this analysis, the phonemes are each modelled by a multi-dimensional Gaussian density probability function where the parameters are a set of principal components derived from the acoustic-phonetic features.\n",
    "The acoustic-phonetic features are those describing place- and manner-of-articulation. They are derived by means of a Self-Organising Neural Network, which has been stimulated and calibrated to perform the non-linear transformation of a vector of speech signal cepstrum coefficients into the vector of acoustic-phonetic features. Results show a fairly good accuracy in identifying phonemes within feature-groups when sufficient data are available for training/simulation and testing. However, within these group-confusions occur mostly between neigboring phonemes which differ only in a few distinctive features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-122"
  },
  "utela92_icslp": {
   "authors": [
    [
     "Pekka",
     "Utela"
    ],
    [
     "Samuel",
     "Kaski"
    ],
    [
     "Kari",
     "Torkkola"
    ]
   ],
   "title": "Using phoneme group specific LVQ-codebooks with HMMs",
   "original": "i92_0551",
   "page_count": 4,
   "order": 124,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "We have implemented a novel combination of LVQ-codebooks and hidden Markov models (HMMs) for transcribing spoken Finnish words into phonemes. A separate LVQ-codebook was trained for the unvoiced plosives /k/, /p/ and ft/. HMMs were then modified to accommodate two parallel phoneme codebooks and a third codebook representing the power of the speech signal. The error rate of phonemic transcription was as low as 9.1 %. A preliminary test with an additional /b/, /d/ and /g/ codebook was also carried out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-123"
  },
  "iwahashi92_icslp": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Speech segment network approach for an optimal synthesis unit set",
   "original": "i92_0479",
   "page_count": 4,
   "order": 125,
   "p1": "479",
   "pn": "482",
   "abstract": [
    "In this paper, a Speech Segment Network ( SSN ) approach is proposed for construction of a small speech unit set with which high quality speech can be synthesized. s The SSN approach selects a speech unit set in which segmental and/or inter-segmental distortions are minimized by using combinatorial optimization methods such as iterative improvement or simulated annealing. Experimental results using diphone segments showed that the optimal diphone unit sets with total or maximum of inter-segmental distortion reduced by about 35%, 70% respectively can be constructed by this method. This reduction rate is enhanced as the segment population increased. Effectiveness of this unit set design was also perceptually confirmed by listening test using speech synthesized with the selected diphone unit set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-124"
  },
  "sagisaka92_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Nobuyoshi",
     "Kaiki"
    ],
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Katsuhiko",
     "Mimura"
    ]
   ],
   "title": "ATR μ-talk speech synthesis system",
   "original": "i92_0483",
   "page_count": 4,
   "order": 126,
   "p1": "483",
   "pn": "486",
   "abstract": [
    "This paper describes ATR μ-Talk speech synthesis system which is designed to serve a research paradigm where system building problems can be systematically investigated by algorithmic treatments with objective measures. Non-Uniform Unit (y) selection scheme is employed to consider unit set construction and its usage simultaneously. Automatic generation, efficient construction and reduction of the unit set are discussed. The importance of Automatic Tuning of Rules for prosody control (ATR) is stressed, and some experimental results obtained through the development of the current system are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-125"
  },
  "coile92_icslp": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ],
    [
     "Steven",
     "Leys"
    ],
    [
     "Luc",
     "Mortier"
    ]
   ],
   "title": "On the development of a name pronunciation system",
   "original": "i92_0487",
   "page_count": 4,
   "order": 127,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "The synthesis of names and addresses is a difficult topic for general text-to-speech systems. Language tagging strategies and special name pronunciation rules need to be developed. In this development, databases play a crucial part. We describe the characteristics of a name database consisting of more than 500,000 Dutch, French and German entries. Some aspects of our strategy for developing name pronunciation systems are highlighted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-126"
  },
  "karlsson92_icslp": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ]
   ],
   "title": "Consonants for female speech synthesis",
   "original": "i92_0491",
   "page_count": 4,
   "order": 128,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "This study reports on some results from an ongoing project to systematically describe consonants in female speech. All Swedish consonants and consonant allophones have been recorded in nonsense words read in a carrier phrase. The acoustic properties for the consonants are measured by inverse filtering and by matching natural speech with synthetic speech. The results are tested, using the KTH speech synthesis system. The aim of the project is to produce natural-sounding female text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-127"
  },
  "santen92_icslp": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Diagnostic perceptual experiments for text-to-speech system evaluation",
   "original": "i92_0555",
   "page_count": 4,
   "order": 129,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "Perceptual methods are described for detecting problems in components of text-to-speech systems. One feature of our approach is the central role played by text-based computational procedures. Their primary use here is in the form of automatic methods for generating text materials with maximal coverage of the domain of a text-to-speech system. Other uses include a system for automatic phonemic scoring of orthographic responses in transcription tasks. A second important feature of our approach is usage of multiple perceptual paradigms. This is needed both to capture the inherent perceptual multidimensionality of synthetic speech and to handle the multiplicity of problems caused by different system components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-128"
  },
  "balestri92_icslp": {
   "authors": [
    [
     "Marcello",
     "Balestri"
    ],
    [
     "Enzo",
     "Foti"
    ],
    [
     "Luciano",
     "Nebbia"
    ],
    [
     "Mario",
     "Oreglia"
    ],
    [
     "Pier Luigi",
     "Salza"
    ],
    [
     "Stefano",
     "Sandri"
    ]
   ],
   "title": "Comparison of natural and synthetic speech intelligibility for a reverse telephone directory service",
   "original": "i92_0559",
   "page_count": 4,
   "order": 130,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "A challenging application of text-to-speech synthesis is the reading of names and addresses, for example in the reverse directory service automation. This task undergoes rather severe conditions, as the proper pronunciation of names is particularly difficult, and the telephone network may cause acoustic degradations of the speech signal. Nevertheless very high segmental intelligibility is needed to avoid user rejection.\n",
    "A project has been started at CSELT for the development of an experimental reverse directory service on the Italian telephone network, by exploiting the capabilities of a diphone-based text-to-speech synthesis system, augmented with specialized name pronunciation rules.\n",
    "This paper describes the intelligibility evaluation of a consistent data base of surnames and addresses of the Italian telephone directory, by comparing natural speech and text-to-speech synthesis in two conditions: 16 kHz as a high quality reference, and 8 kHz PCM as telephone standard. The speech synthesis segmental intelligibility of several phonetic contexts is discussed, and referred to the natural speech performance. Results are interpreted also in the framework of the application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-129"
  },
  "sproat92_icslp": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "David",
     "Yarowsky"
    ]
   ],
   "title": "A corpus-based synthesizer",
   "original": "i92_0563",
   "page_count": 4,
   "order": 131,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "This paper describes NewExpress, the new text-to-phonetic-representation component of the AT&T Bell Laboratories Text-to-Speech system (TTS). To the best of our knowledge, NewExpress represents the first extensive use of corpus-based linguistic techniques in a text-to-speech program. We discuss the use of such techniques in the system in four main areas: general pitch accent assignment, prosodic phrasing, pitch accent assignment in noun compounds, and homograph disambiguation. We demonstrate that these techniques afford an improvement in the performance of TTS.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-130"
  },
  "hirokawa92_icslp": {
   "authors": [
    [
     "Tomohisa",
     "Hirokawa"
    ],
    [
     "Kenzo",
     "Itoh"
    ],
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "High quality speech synthesis based on wavelet compilation of phoneme segments",
   "original": "i92_0567",
   "page_count": 4,
   "order": 132,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "A speech synthesis system is developed which directly compiles phoneme wavelet segments selected from a wavelet dictionary containing over 45,000 entries to yield high quality synthesized voice. In ICSLP'90, we proposed the wavelet selection and wavelet concatenate methods used in our system. To realize the system, we establish prosody pattern setting by rules and a wavelet modification procedure to achieve the design goals. Phoneme duration is set according to phoneme environment, and phoneme power is controlled by both pitch frequency and phoneme environment. Tests show the average errors in vowel duration and consonant duration are 28.8msec and 16.8msec respectively, and the vowel power average error is 2.93dB. Wavelet pitch frequency is controlled by an approach based on the pitch synchronous overlap-add method. To avoid abrupt changes in voice spectrum and wavelet shape, an interpolation operation is carried out between voiced wavelets. The synthesized speech has high intelligibility and naturalness, while the original speaker quality is retained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-131"
  },
  "williams92_icslp": {
   "authors": [
    [
     "David R.",
     "Williams"
    ],
    [
     "Corine A.",
     "Bickley"
    ],
    [
     "Kenneth N.",
     "Stevens"
    ]
   ],
   "title": "Inventory of phonetic contrasts generated by high-level control of a formant synthesizer",
   "original": "i92_0571",
   "page_count": 4,
   "order": 133,
   "p1": "571",
   "pn": "574",
   "abstract": [
    "A method for simplifying the control of a Klatt-type synthesizer is described in which values of a small set of high-level (HL) parameters are transformed into values for the larger set of low-level (LL) acoustic parameters that typify formant synthesizers. The HL parameters include natural frequencies of the vocal tract, area of oral constriction, specification of a nasal side branch, active control of intraoral pressure for obstruents, and state of the glottal source. Determination of the LL source parameters involves the calculation of a set of intermediate parameter values which reflect the pressures and flows within the vocal tract. To illustrate the method, we describe the synthesis of speechlike sounds in which the HL orifice parameters are varied through their ranges of values, as well as the synthesis of a number of consonants in medial (VCV) position.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-132"
  },
  "goldstein92_icslp": {
   "authors": [
    [
     "Mikael",
     "Goldstein"
    ],
    [
     "Ove",
     "Till"
    ]
   ],
   "title": "Is % overall error rate a valid measure of speech synthesiser and natural speech performance at the segmental level?",
   "original": "i92_1131",
   "page_count": 4,
   "order": 134,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "Intelligibility of 18 Swedish consonants embedded in 3 symmetrical vowel contexts (a_a, i_i and o_o) was assessed using the VCV test procedure SOAP v3.0 (ESPRIT PROJECT 2589 (SAM)) implemented on a PC computer. 24 subjects with normal hearing assessed the 54 different VCV combinations. Two synthesizer systems, the KTH and the INFOVOX system were assessed. Natural speech was used as a baseline condition. Although the % overall error rate for Natural speech was very low (5.56%) as compared to the KTH (8.72%) and INFOVOX (12.27%) synthesis systems, it was located to three VCV words that obtained very high error rates: otjo (54%), atja (50%) and oho (34%). For the VCV word oho, Natural speech yielded a significantly higher % error rate than that obtained for each of the two synthesis systems.\n",
    "Differences between two correlated proportions (% correct) obtained for each VCV combination and system assessed by the same group of subjects were tested using the new program module DPROP.EXE, which uses the result files generated by the SOAP software as input files. The program tests for differences between all VCV combinations generated by two different systems that have been assessed by the same group of subjects. The testing (at the 2%-level, two-tailed t-test) of VCV words between KTH-Natural speech yielded 4 significant (*) differences (+ongo*, +ingi*, +ini*, and -oho*), between INFOVOX-Natural speech 9 significant differences (+ovo*, +ingi*, +ongo*, +omo*, +ibi*, +opo*, +olo*, +ama* and -oho*) and between INFOVOX-KTH synthesis 3 significant differences (+ovo*, +opo* and +ibi*). In order to be significant, the % error difference had to be of the order of 30%. The use of the % overall error rate as a valid diagnostic synthesiser performance measure is discussed. A measure that treats all insignificant VCV differences the same way as significant differences, by adding them together into an % overall error rate. This measure is compared to significance testing of individual VCV words, as well as the use of Natural speech as a 'true' baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-133"
  },
  "jongenburger92_icslp": {
   "authors": [
    [
     "Willy",
     "Jongenburger"
    ],
    [
     "Renee van",
     "Bezooijen"
    ]
   ],
   "title": "Text-to-speech conversion for dutch: comprehensibility and acceptability",
   "original": "i92_1135",
   "page_count": 4,
   "order": 135,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "In the majority of studies to date, evaluation of synthetic speech has been focussed on segmental intelligibility. However, as the development of text-to-speech systems progresses and real-life applications are getting more common, the need for evaluation at higher levels of linguistic organization becomes increasingly relevant. In the present paper two studies are reported evaluating text-to-speech conversion for Dutch at the level of the paragraph. The first study determined comprehensibility for visually impaired and sighted subjects. The second study examined acceptability of voice-and-speech aspects of synthetic output as a function of experience.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-134"
  },
  "katoh92_icslp": {
   "authors": [
    [
     "Masayo",
     "Katoh"
    ],
    [
     "Shin'ichiro",
     "Hashimoto"
    ]
   ],
   "title": "The rhythm rules in Japanese based on the centers of energy gravity of vowels",
   "original": "i92_1139",
   "page_count": 4,
   "order": 136,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "This paper proposes some new rules of rhythm in Japanese, based on the Center of Energy Gravity of Vowels: CEGV. These rules have risen from isochrony of mora, which linguists consider a characteristic feature of Japanese. Assuming the CEGV as the timing point of rhythm, and the duration between CEGV's(DceflV) as the parameter to determine the rhythm, we can form the Japanese rhythm simply by using only 29 rules (the first approximate rule-set). The isochrony of more than two moras is said to be another Japanese feature. Based on this, an additional rhythm rule (the second approximate i%ule) is proposed to prevent rhythm disturbances due to some exceptional combinations of the rules in the first approximate rule-set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-135"
  },
  "itoh92_icslp": {
   "authors": [
    [
     "Kenzo",
     "Itoh"
    ],
    [
     "Tomohisa",
     "Hirokawa"
    ],
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "Segmental power control for Japanese speech synthesis",
   "original": "i92_1143",
   "page_count": 4,
   "order": 137,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "This paper proposes a segmental power control method for speech synthesis by rule. The innovation of this method lies in its use of the phoneme environment characteristics and the relationship between speech power and pitch frequency. First, the Permissible Threshold (PT) for power modification is measured by subjective experiments using phoneme power manipulated speech material. As a result, it is concluded that the PT of phoneme power modification is 4.1 dB. This experimental result is significant when discussing power control and gives a criterion for power control accuracy. Next, the relationship between speech power and pitch frequency is analyzed using a very large speech data base. The results show that the relationship between phoneme segmental power and pitch frequency is affected by the kind of phoneme, the adjoining phonemes, rising or falling pitch conditions, and initial or final position of sentence. Finally, we propose that the segmental speech power should be controlled by the pitch and phoneme environment. This new method yields an averaged root mean square error between real and estimated speech power of 2.17 dB. This value indicates that 94% of the estimated power values are within the Permissible Threshold of human perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-136"
  },
  "schoentgen92_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Glottal waveform synthesis with volterra shapers",
   "original": "i92_1147",
   "page_count": 4,
   "order": 138,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "Recently, we proposed a nonlinear zero-memory input-output model of the glottal pulse. This so-called Volterra model represents pulse shape with great accuracy. The model's driving function is a cosine. The number of model coefficients is typically equal to 2 x 40. Indeed, the model consists of two polynomials. The first models the even and the second the odd component of the glottis signal. The Volterra coefficients are directly calculated from the Fourier coefficients. The model outputs the original pulse when the amplitude of the driving cosine is equal to one, its phase is equal to zero and its frequency equal to the original frequency. In this article we suggest using the Volterra model for synthesizing glottal pulses. Models with a small number of control parameters are preferred for synthesis. Therefore, we use the Volterra model differently. We calculate once and for all the coefficients cj and dj for a small number of glottal pulses obtained from a speaker and we keep them fixed while we adjust amplitude, phase and frequency of the driving cosine. We thus generate glottal pulses of different shapes with the same model. Indeed, nonlinear systems are not characterized by a transfer function. This means that the shape of the output of a Volterra model depends on the shaping functions and also on control parameters of the driving cosine. We report an experiment that we carried out to test this synthesis method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-137"
  },
  "ceder92_icslp": {
   "authors": [
    [
     "Ken",
     "Ceder"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "Yet another rule compiler for text-to-speech conversion?",
   "original": "i92_1151",
   "page_count": 4,
   "order": 139,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "An interactive environment for developing applications and conducting research in the domain of text-to-speech conversion has been designed. The system is implemented in Prolog and is a new generation of the subclass of speech synthesis-by-rule systems, called rule compilers. It integrates different levels of linguistic and acoustic analyses and is constructed specifically for linguists and phoneticians. Special notational formalisms have been devised, which as closely as possible correspond to phonetic and linguistic terminology. Two types of synthesis strategies are included in the system, the formant synthesis and the polyphone synthesis, i.e., concatenation of speech units of arbitrary size. In the present paper it is shown how this system can be used for implementing a text-to-speech synthesis for Swedish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-138"
  },
  "iwata92_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Yukio",
     "Mitome"
    ]
   ],
   "title": "Prosody generation models constructed by considering speech tempo influence on prosody",
   "original": "i92_1155",
   "page_count": 4,
   "order": 140,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "A duration model and a pitch pattern generation model are proposed, in which speech tempo influences on the duration and pitch are considered. Analysis results for the speech tempo influences are described. In the analysis, it was discovered that of all the factors affecting speech rates for individual phrases, at fast tempo, the phrase position within the sentence is the most influential factor, while, at normal and slow tempos, the most important factor is whether or not a pause exists after the phrase. The analysis also revealed that, while pitch frequency values may differ at different tempos, their normalized pitch patterns for a given sentence are quite similar. On the basis of these results, a duration model has been constructed, which determines a suitable tempo for a given sentence or paragraph, and estimates durations for the individual phrases that constitute the sentence or paragraph. The durations for the phonemes within the phrases are estimated according to the phoneme environment. A pitch pattern generation model has been also constructed, which determines the normalized pitch pattern that is little affected by changes in speech tempo. The model then calculates the pitch frequencies which would actually be produced at various tempos. These models have speech tempo parameters, and can generate adequate durations and pitch contours according to the tempo.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-139"
  },
  "monaghan92_icslp": {
   "authors": [
    [
     "Alex I. C.",
     "Monaghan"
    ]
   ],
   "title": "Extracting microprosodic information from diphones - a simple way to model segmental effects on prosody for synthetic speech",
   "original": "i92_1159",
   "page_count": 4,
   "order": 141,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "One of the major problems in the phonetic realisation of synthetic intonation is the modelling of local segmental effects on the course of F0, despite the fact that in a system using concatenation synthesis these effects are already present in the units recorded from natural speech. The usual procedure in concatenation synthesis is to remove all F0 information from the units and then reimpose a synthetic F0 contour: however, it is possible to take advantage of the original F0 information to model local segmental effects and thereby produce a more natural F0 contour. This paper proposes a method for extracting microprosodic information from the F0 contours of diphones recorded from natural speech, and presents some preliminary results of its application in the TTS system developed at Edinburgh University's Centre for Speech Technology Research (CSTR) [3]. Problems with the work reported here are also discussed, as are directions for future research both on microprosody and on evaluation experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-140"
  },
  "hessen92_icslp": {
   "authors": [
    [
     "Arjan van",
     "Hessen"
    ]
   ],
   "title": "Generation of natural sounding speech stimuli by means of linear cepstral interpolation",
   "original": "i92_1163",
   "page_count": 4,
   "order": 142,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "This paper describes a method for the generation of sets of natural sounding speech stimuli, slowly changing from one speech signal to another. Stimulus continua created with this method were used in a large number of psycho-physical identification and discrimination experiments [1]. Two recorded speech stimuli between which a continuum is made are first analyzed according to the Sine Wave Generation method [2,3,4,5,6,7,8]. This results in a set of parameters per frame, containing the frequency, and the amplitude and phase of vocal tract and vocal source, at the 50 major peaks of the Short-Time FFT spectrum. Because the vocal tract amplitudes in each frame comprise the information about the spectral envelope, modifying these amplitudes results in a modified spectral envelope, and thus in a different \"timbre\".\n",
    "Linear interpolation between the spectral envelope amplitudes (SE-amplitudes) of the two recorded speech sounds results in a set of spectral envelopes that slowly change from one sound to the other. Replacing the original SE-amplitudes of one of the two original stimuli (the mother stimulus) with those of the interpolated set, results (after resynthesizing) .in set of stimuli that differ only in timbre; they slowly change from one sound to another.\n",
    "High quality speech is obtained because the stimuli are resynthesized with all their original parameters; only the SE-amplitudes are modified. The thus created speech sounds contain all the speaker specific characteristics of the \"mother stimulus\" and sound very natural because no important information is lost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-141"
  },
  "campbell92_icslp": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ],
    [
     "Colin",
     "Wightman"
    ]
   ],
   "title": "Prosodic encoding of syntactic structure for speech synthesis",
   "original": "i92_1167",
   "page_count": 4,
   "order": 143,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "We present results of a perceptual test of the output of a speech synthesiser that uses a concise encoding of prosody. The synthesis paradigm assumes that two parameters of prominence and boundary strength are adequate to determine the acoustic correlates of systematic prosodic variation. The synthesis system utilises a large database of continuous speech as source units for concatenation, selecting on the basis of both segmental and suprasegmental characteristics. We show that the synthesiser is capable of conveying the information that listeners need in order to differentiate phonetically similar, syntactically ambiguous sentences at levels significantly better than chance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-142"
  },
  "hertz92_icslp": {
   "authors": [
    [
     "Susan R.",
     "Hertz"
    ],
    [
     "Marie K.",
     "Huffman"
    ]
   ],
   "title": "A nucleus-based timing model applied to multi-dialect speech synthesis by rule",
   "original": "i92_1171",
   "page_count": 4,
   "order": 144,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "This paper presents a new timing model for rule-based speech synthesis which underlies the rules we are developing for five American English dialects, and which we are beginning to extend to other languages. The model leads to extremely efficient development of high-quality rules for different dialects and languages, and, more generally, provides new insights into the nature of speech. The paper presents the basic tenets of the model, showing how it leads to generalizations about speech patterns within and across dialects (and languages) that cannot be captured in more conventional models. The introduction discusses the nature of utterance representations structured in accordance with our model, using an example from General American English. The second section illustrates our application of the model to rule-based synthesis of American English dialects, focussing in particular on the straightforward and accurate rules for formant timing made possible by the model. The third section discusses the directions our work is currently taking, including development of a novel, multi-dialect relational database for extracting cross-dialect and intra-dialect generalizations, and experiments aimed at determining when observed variability in timing patterns within and across dialects is perceptually important.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-143"
  },
  "house92_icslp": {
   "authors": [
    [
     "Jill",
     "House"
    ],
    [
     "Nick",
     "Youd"
    ]
   ],
   "title": "Evaluating the prosody of synthesized utterances within a dialogue system",
   "original": "i92_1175",
   "page_count": 4,
   "order": 145,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "An experiment was carried out to evaluate the acceptability of different prosodic algorithms for synthetic speech in contexts appropriate to a dialogue information service. The results suggest that there may be a significant advantage in using an enriched representation to guide the prosody of the synthesized output, to supplement text-to-speech rules. Annotations on the text string encode contextual information, and are designed to be passed on to the synthesizer by an automatic language generator.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-144"
  },
  "tatham92_icslp": {
   "authors": [
    [
     "Marcel",
     "Tatham"
    ],
    [
     "Eric",
     "Lewis"
    ]
   ],
   "title": "Prosodics in a syllable-based text-to-speech synthesis system",
   "original": "i92_1179",
   "page_count": 4,
   "order": 146,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "This paper describes the assignment of correct rhythm and intonation contours to speech generated by the SPRUCE (SPeech Response from Unconstrained English) speech synthesis system. The system employs around 10,000 normalised parametrically analysed syllables excised from recorded natural human speech as the basic units from which the synthetic output is built. Incoming plain text prompts a search and retrieval strategy in a 200,000 entry word dictionary. For each entry in the dictionary there is information concerning syntactic category, semantic relationships and syllable based phonological content. The paper discusses the philosophy and strategies involved in the SPRUCE method of prosodic assignment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-145"
  },
  "belrhali92_icslp": {
   "authors": [
    [
     "R.",
     "Belrhali"
    ],
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Louis-Jean",
     "Boe"
    ]
   ],
   "title": "From lexicon to rules: toward a descriptive method of French text-to-phonetics transcription",
   "original": "i92_1183",
   "page_count": 4,
   "order": 147,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "This study presents a linguistic analysis of phenomena in French text-to-phonetics transcription and its goal is text-to-phonetics transcription dictionaries of words and compound words. The descriptive language TOPH {Transcription Orthographique PHonetique) [2] is the formal and computational tool organizing the description proposed here, and it permits the integration of rules and lexica into a text-to-phonetics grammar. A minimal grammar, constituting the core of the phoneticization process, has been enlarged by systematically exploring a representative lexicon of French, the ICP's dictionary Le 60 000, with phonetic transcriptions being taken from the dictionary Le Petit Robert [13]. Several linguistically defined classes of morphemes or words emerged from this analysis, and each class is represented by a lexicon and activation rules. Through this extension, the rate of successful formalization has been improved to nearly 100% for the dictionary as a whole.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-146"
  },
  "elmlund92_icslp": {
   "authors": [
    [
     "Marianne",
     "Elmlund"
    ],
    [
     "Ida",
     "Frehr"
    ],
    [
     "Niels Reinholt",
     "Petersen"
    ]
   ],
   "title": "Formant transformation from male to female synthetic voices",
   "original": "i92_1187",
   "page_count": 4,
   "order": 148,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "In order to find a relation between Danish male and female, synthetic vowels, a set of female vowels was developed within the framework of a Danish text-to-speech system [1]. A panel of 37 naive listeners judged the vowels to be convincingly female. The female vowels were compared to a set of synthetic male vowels previously developed for the text-to-speech system. Through this work a simple formant transformation between male and female vowels, was found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-147"
  },
  "rentzepopoulos92_icslp": {
   "authors": [
    [
     "P. A.",
     "Rentzepopoulos"
    ],
    [
     "George K.",
     "Kokkinakis"
    ]
   ],
   "title": "Multilingual phoneme to grapheme conversion system based on HMM",
   "original": "i92_1191",
   "page_count": 4,
   "order": 149,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "A phoneme to grapheme conversion system which is based on Hidden Markov Models and on the Viterbi algorithm is presented. Being an entirely statistical method, the conversion does not need a dictionary, resulting in a system which can handle any word of a given language. The language model is created from a verticalized text which contains both the graphemic and the phonemic form of each word. The size of the training text sufficient for a good performance of the algorithm is not big (the transition matrix is saturated after about 30-40 thousand words). It must also be noted that there is no need for an expert for the training since this is done automatically. Considering that the training procedure is done off line, the proposed algorithm has many advantages: it is fast, it does not need large amount of memory and responds linearly to the length of the input word. Furthermore it does not need a language model to disambiguate homophones. Finally it is mainly language independent and so it can be applied to any language with only minor modifications. This system was initially tested on the Greek language. Afterwards, the system was also tested on Italian, French, English and German with various success rates depending on the language specific parameters. In this paper, the results of the tests on the above languages are presented and evaluated taking into account the various parameters of the system. This system is also compared to a phoneme to grapheme conversion system which is based on rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-148"
  },
  "hara92_icslp": {
   "authors": [
    [
     "Noriyo",
     "Hara"
    ],
    [
     "Hisayoshi",
     "Tsubaki"
    ],
    [
     "Hisashi",
     "Wakita"
    ]
   ],
   "title": "Fundamental frequency control using linguistic information",
   "original": "i92_1195",
   "page_count": 4,
   "order": 150,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "The appropriate control of fundamental frequency (FO) contours is extremely important for a high quality synthesis-by-rule system. However, most of the conventional F0 controlling methods only uses accent position and syntactic structure. Giving attention to the variety of parts of speech, we investigated the variation of F0 contours in speech spoken by a professional announcer. Wider and higher F0 values were observed in the case of adverbs. On the other hand, conjunctions showed lower F0. Demonstrative pronouns in the middle of the sentence were uttered with higher F0. From these observations, a new algorithm which takes into account the part of speech was proposed and installed in two synthetic speech systems: a synthesis-by-rule system and a PARCOR analysis-synthesis system. The evaluation result for the synthesis-by-rule system did not show significant improvement. In the PARCOR analysis-synthesis system, however, the new algorithm showed significant improvement in the naturalness for adverbs and conjunctions but not for demonstrative pronouns. These results suggest that the new F0 contour algorithm when combined with accurate duration control and advanced voice source modeling can generate voice quality as high as that from analysis-synthesis systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-149"
  },
  "breen92_icslp": {
   "authors": [
    [
     "Andrew P.",
     "Breen"
    ]
   ],
   "title": "A comparison of statistical and rule based methods of determining segmental durations",
   "original": "i92_1199",
   "page_count": 4,
   "order": 151,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "It has been shown [1] that the duration of phonetic segments is an important prosodic factor in the production of natural sounding synthetic speech. It has also been shown [2] that the durations of phonetic segments are affected by a number of factors such as stress, phrase boundaries, phonetic context and speaking rate. In attempting to predict the behaviour of segmental durations, many researchers have concentrated on one or other of two differing approaches; Rule based methods and statistical methods [3]: rule based methods attempt to explicitly model factors known to affect segmental durations, while statistical methods rely solely on the brute force tabulation of large amounts of annotated data. This paper compares the performance of a statistically based method of determining phonetic segment duration, currently being developed at BT laboratories, with an implementation of the duration rules developed by D. Klatt [4][5]. The performance of each method will be compared against hand annotated durations obtained from a large body of test phrases spoken by a number of different speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-150"
  },
  "andrews92_icslp": {
   "authors": [
    [
     "J. R.",
     "Andrews"
    ],
    [
     "K. M.",
     "Curtis"
    ],
    [
     "Volker",
     "Kraft"
    ]
   ],
   "title": "Generation and extraction of high quality synthesis units",
   "original": "i92_1203",
   "page_count": 4,
   "order": 152,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "This paper proposes a novel method for the generation and extraction of high quality speech synthesis units, namely demisyllables. The design incorporates a hybrid system combining both formant speech analysis and synthesis, enabling the extraction and subsequent formant coding of the demisyllables. The German language was used as a source for the demisyllable extraction. The identification of the speech synthesis units was based upon the analysis of data obtained from three different databases for the chosen language. The demisyllables were embedded in carrier words in specifically constructed sentences, coded in SAMPA and recorded in a sterile environment. SAMPA having been chosen as it is the new standard European phonetic alphabet allowing multi-lingual transcription. This approach ensured high speech quality and uniformity in the synthesis elements. The analysis and synthesis system was designed and implemented on a dual processing structures thus resulting in maximum system performance. We present the results of the embedment, extraction, recording and formant coding techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-151"
  },
  "boogaart92_icslp": {
   "authors": [
    [
     "T.",
     "Boogaart"
    ],
    [
     "Kim",
     "Silverman"
    ]
   ],
   "title": "Evaluating the overall comprehensibility of speech synthesizers",
   "original": "i92_1207",
   "page_count": 4,
   "order": 153,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "In real world applications involving synthetic speech, listeners should be able to comprehend the speech as easily and quickly as possible. Not only the intelligibility of phonemes contributes to the comprehensibility of speech, but suprasegmental factors (e.g. intonation, duration, accents, pauses) as well. Whereas methods for assessing segmental intelligibility are well known, it is less clear how overall quality can be measured. One method that might be used is the comprehension test. In a comprehension test, subjects hear a few sentences or paragraphs and answer question(s) about the content. However, the percentage of questions answered correctly may over-estimate the real-life comprehensibility of a synthesizer, because listeners may compensate for the less-than-perfect intelligibility by spending extra cognitive effort, that may go at the cost of other tasks that must be performed at the same time, like driving or monitoring.\n",
    "NYNEX developed a comprehension test containing items that approach real world applications. The test items were carefully constructed to ensure that they could not be answered without hearing the speech. In this paper this test is evaluated. To that end 322 subjects, recruted via an advertisement in a local newspaper, participated in the comprehension test, listening to synt hetic or natural speech. Half of the subjects did a concurrent mouse-controlled tracking task as well.\n",
    "Despite the clearly superior quality of the natural speech, the comprehension test failed to show a statistically significant difference in the comprehensibility of natural and synthetic speech. Moreover, the tracking task did not seem to fulfill its intended function either. Detailed analysis of the data and the results led to a proposal for a redesign of the procedure for measuring the comprehensibility of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-152"
  },
  "boeffard92_icslp": {
   "authors": [
    [
     "Olivier",
     "Boeffard"
    ],
    [
     "Laurent",
     "Miclet"
    ],
    [
     "S.",
     "White"
    ]
   ],
   "title": "Automatic generation of optimized unit dictionaries for text to speech synthesis",
   "original": "i92_1211",
   "page_count": 4,
   "order": 154,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "This article covers text-based synthesis research carried out at the CNET, based on the concatenation of acoustic units and the utilization of the TD-PSOLA method [MOU 90]. First, a technique used in the automatic segmentation of a logatome data base shall be presented. This technique enables acoustic units to be extracted from the logatome data base. The logatomes are segmented in phonemes using Hidden Markov Models, and the synthesis units are extracted using this segmentation. The interest of this technique resides in the fact that manual segmentation of learning data is no longer necessary. Then, we shall present an algorithm used in determining the best instant of concatenation of two acoustic units. Results have demonstrated that the difference between the manual and automatic segmentation of a French dictionary of units is inferior to 30 ms for 90% of the units. Two other dictionaries (one in German) were also segmented using this method. Informal listening tests were also carried out with success.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-153"
  },
  "kasuya92_icslp": {
   "authors": [
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Seiki",
     "Kasuya"
    ]
   ],
   "title": "Relationships between syllable, word and sentence intelligibilities of synthetic speech",
   "original": "i92_1215",
   "page_count": 4,
   "order": 155,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "The paper investigates appropriate methods to evaluate the intelligibility of Japanese synthetic speech generated by rule at the syllable and word levels from the viewpoint of its relationships with the sentence intelligibility. The intelligibility scores of the second syllable of the disyllable(CVCV) and of the word presented in semantically anomalous sentences have been found to reflect well the word intelligibility in the normal sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-154"
  },
  "hill92_icslp": {
   "authors": [
    [
     "David R.",
     "Hill"
    ],
    [
     "Craig-Richard",
     "Schock"
    ],
    [
     "Leonard C.",
     "Manzara"
    ]
   ],
   "title": "Unrestricted text-to-speech revisited: rhythm and intonation",
   "original": "i92_1219",
   "page_count": 4,
   "order": 156,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "A new speech-synthesis-by-rules system has been developed, at the University of Calgary, in an object-oriented programming environment, on the NeXT computer. This paper outlines the models used to create rhythm and intonation for the synthesised speech. A companion paper (DEGAS: a system for rule-based diphone speech synthesis) outlines the framework for segment specification.\n",
    "The rhythm model is based on data obtained from real speech and represents a continuation of earlier work. The difficulties in reconciling the structure used for synthesis with the structure assumed for traditional segmental analysis are outlined. The intonation model is based on the descriptive framework developed by M.A.K. Halliday, as used for teaching non-native speakers to produce reasonable intonation for spoken English. The encouraging results from preliminary subjective testing of the intonation model, with utterances synthesised using the system, are described. On the basis of these tests, together with informal evaluation, we conclude that the speech produced represents a significant improvement in naturalness and intelligibility. The research has provided the basis for a new commercial text-to-speech system now being marketed by Trillium Sound Research Inc.-a successful technology transfer exercise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-155"
  },
  "rozsypal92_icslp": {
   "authors": [
    [
     "Anton J.",
     "Rozsypal"
    ]
   ],
   "title": "Wavelet speech synthesizer in the classroom and speech laboratory",
   "original": "i92_1223",
   "page_count": 4,
   "order": 157,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "Vowel perception studies indicate that in vowel recognition, hearing uses short-term frequency analysis with the temporal window duration of the order of 10 is. This suggests that vowels should be synthesized not only in terms of their long-term amplitude spectra, but also that the proper phase relationships of their components should be preserved. For these reasons the \"Wavelet Speech Synthesizer\" has been developed for generation of voiced speech sounds. The signal is synthesized in the time domain as a convolution of the glottal wave with wavelets, each representing an impulse response of a formant resonantor of the vocal tract. The paper describes the principles of the synthesis, the process of encoding the speech parameter tracks by the user, and several possible classroom and research applications of this synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-156"
  },
  "portele92_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Birgit",
     "Steffan"
    ],
    [
     "Rainer",
     "Preuß"
    ],
    [
     "Walter F.",
     "Sendlmeier"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "HADIFIX - a speech synthesis system for German",
   "original": "i92_1227",
   "page_count": 4,
   "order": 158,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "HADIFIX is a speech synthesis system for German. It transforms a phonemic input string with accent markers into a speech signal. To obtain high-quality speech, time-domain elements with a structure similar to demisyllables are concatenated. A method to generate vowel-to-vowel transitions for time-domain units is used when necessary. Prosodic manipulations are carried out using the TD-PSOLA algorithm [1]. An F0 contour is generated using Fujisaki's model [2,3]. The quality of the synthesized speech was compared with that of natural speech and of an LPC-based version using a rhyme test, a word test, a dictation, and a subjective ranking test. While the intelligibility is high, the difference in naturalness between synthesized and natural speech is still large. Another outcome of the test is the superiority of the time-domain units, compared with the LP-coded ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-157"
  },
  "delogu92_icslp": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "S.",
     "Conte"
    ],
    [
     "A.",
     "Paoloni"
    ],
    [
     "C.",
     "Sementina"
    ]
   ],
   "title": "Two different methodologies for evaluating the comprehension of synthetic passages",
   "original": "i92_1231",
   "page_count": 4,
   "order": 159,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "In the present work we focus our attention on the type of understanding which is caused by listening to passages produced by vocal synthesizers, using two different methodologies: multiple choice test (Test 1) and click monitoring task (Test 2). Two texts taken from a published Italian reading comprehension test and pronounced by a human speaker and by the Olivetti PC Vox synthesizer were used. In Test 1, 150 subjects after listen to a passage had to answer ten multiple choice questions. In Test 2, other 50 subjects during the listening of a passage had to recognize some computer generated clicks distributed in the passage. The results of Test 1 show no statistically significant differences in performance between subjects who listened to natural passages and those who listened synthetic passages. The results of Test 2 show that click monitoring responses were faster for clicks in natural speech compared to those in synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-158"
  },
  "gussenhoven92_icslp": {
   "authors": [
    [
     "Carlos",
     "Gussenhoven"
    ],
    [
     "Toni",
     "Rietveld"
    ]
   ],
   "title": "A target-interpolation model for the intonation of dutch",
   "original": "i92_1235",
   "page_count": 4,
   "order": 160,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "A complete computer simulation of a linguistic component like intonation would involve three rather different tasks. First, it would need to produce semantically appropriate selections from the available inventory of morphemes, on the basis of communicative intentions. Second, it would provide the phonological shapes of these morphemes, and combine them to form a complete phonological representation of the intonation of the sentence. And third, it would translates this phonological representation into correctly timed F0 events.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-159"
  },
  "davis92_icslp": {
   "authors": [
    [
     "Katharine",
     "Davis"
    ],
    [
     "Patricia K.",
     "Kuhl"
    ]
   ],
   "title": "Best exemplars of English velar stops: a first report",
   "original": "i92_0495",
   "page_count": 4,
   "order": 161,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "Studies on vowels by Kuhl and her colleagues have shown that certain stimuli in a vowel category are rated as \"better exemplars\" (prototypes) than others. These stimuli also exhibit a \"perceptual magnet effect\" in that acoustically similar stimuli are perceptually drawn towards the prototype [1,2]. Vowel prototypes are language specific, both in adults and in 6-month-old infants [3]. A new series of studies has extended this work to consonants. The initial goals were twofold: 1) identify the preferred exemplars of naturally produced English voiced and voiceless velar stops in initial position, and 2) examine the contribution of various acoustic cues contained in these stops (pre-release lead voicing, silence between lead voicing and the release burst, aspiration, burst duration, and burst locus. Ten adult speakers, five male and five female, recorded twenty different words with initial velar stops before the vowel /ae/. The 200 tokens (twenty words, ten /g/-initial and 10 /k/-initial, by ten speakers) were digitized and edited to include only two pitch periods in addition to the initial consonant.  The resulting stimuli were presented to eight subjects in two judgment tasks: 1) a two-choice (/k/ or /g/) identification task which also measured reaction time, and 2) a category goodness rating task wherein subjects were asked to rate each stimulus as to how \"good\" an exemplar of its category (/k/or/g/) it was. Acoustic parameters of the stimuli were correlated with subjects' identifications and goodness ratings to determine which variables had the most effect on subjects' goodness judgments. Results showed that the subjects' judgments were strongly influenced by the acoustic variables. The variables contributing the most were, 1) the individual speaker who produced the stimulus, 2) the durations of both lead and lag time, and 3) the presence vs. the absence of lead time in the /g/'s.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-160"
  },
  "stevens92_icslp": {
   "authors": [
    [
     "Kenneth N.",
     "Stevens"
    ],
    [
     "Sharon Y.",
     "Manuel"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ],
    [
     "Sharlene",
     "Liu"
    ]
   ],
   "title": "Implementation of a model for lexical access based on features",
   "original": "i92_0499",
   "page_count": 4,
   "order": 162,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "A feature-based model for accessing words in a lexicon from measurements on the speech signal is proposed, and the implementation of some components of the model is described. Words are represented in a lexicon in terms of hierarchies of distinctive features arranged in matrix form. Acoustic measurements are made on an utterance in order to estimate the pattern of features implemented by the speaker, and access to lexical items is based on these hypothesized features. The acoustic measurements are of two kinds: (1) they identify and classify critical locations in the signal, and (2) they indicate the activity of articulatory structures in the vicinity of these critical locations. Proposed procedures for representing the lexicon and for accessing words from the lexicon have the potential of being relatively insensitive to speaking style and to context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-161"
  },
  "huber92_icslp": {
   "authors": [
    [
     "Dieter",
     "Huber"
    ]
   ],
   "title": "Perception of aperiodic speech signals",
   "original": "i92_0503",
   "page_count": 4,
   "order": 163,
   "p1": "503",
   "pn": "506",
   "abstract": [
    "This paper addresses the perceptual relevance of laryngealization as a potential boundary cue in continuous speech utterances. It investigates, in other words, the problem whether the short-time occurrences of various patterns of aperiodic voice vibration frequently found at boundary locations in human speech can actually be perceived and discriminated by human listeners in normal communicative situations, and thus may be taken to contribute (1) to the signal information needed for the correct recognition and inter- pretation of the structural properties of the message, and (2) to the impression of naturalness in human versus computer speech. Four patterns of laryngealization have been examined systematically: glottalization, creaky voice, creak and diplophonic phonation. The results of this study indicate that human listeners evidently exploit aperiodicity in the acoustical speech signal for segmentation but not for classification purposes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-162"
  },
  "kato92_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Acceptability and discrimination threshold for distortion of segmental duration in Japanese words",
   "original": "i92_0507",
   "page_count": 4,
   "order": 164,
   "p1": "507",
   "pn": "510",
   "abstract": [
    "Acceptability of temporal naturalness and temporal discrimination threshold were measured for various vowel segments in isolated words by modifying original segmental durations. A large size perceptual experiment using 1462 stimuli of 70 segments revealed that word acceptability is affected by the segment attributes and context such as vowel color, position in a word and, accent. An additional experiment showed that the above factors also affect discrimination threshold consistently.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-163"
  },
  "bonneau92_icslp": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Sylvie",
     "Coste"
    ],
    [
     "Linda",
     "Djezzar"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Two level acoustic cues for consistent stop identification",
   "original": "i92_0511",
   "page_count": 4,
   "order": 165,
   "p1": "511",
   "pn": "514",
   "abstract": [
    "We present an analytical approach for automatic speech recognition based on the detection of acoustic cues. We have defined two levels of cue: \"strong\" cues both highly visible and efficiently discriminating, and \"weak\" cues, used in case of absence of \"strong\" cues. Our strategy based on the two levels of cue has the following advantages: it prevents the recognition system from proposing an inconsistent solution it permits an early decision when possible thanks to the detection of \"strong\" cues. We obtain nearly 50% correct early stop identification with \"strong\" burst cues; that validates our approach. An operational system is already running for voiceless stops followed by back vowels in continuous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-164"
  },
  "carlson92_icslp": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Vowel classification based on analysis-by-synthesis",
   "original": "i92_0575",
   "page_count": 4,
   "order": 166,
   "p1": "575",
   "pn": "578",
   "abstract": [
    "In this paper we report on a sequence of experiments designed to explore the use of analysis-by-synthesis methods for speech recognition and speech analysis in general. An intermediate representation of the speech signal is formulated in terms of speech-synthesis-like parameters.\n",
    "Using an multi-layer perceptron as a common classifier, we have performed several vowel classification experiments based on these parameters. The results of the experiments indicate that we are able to obtain the same classification performance as a more traditional spectral representation using nearly an order of magnitude fewer dimensions.\n",
    "We have also developed a speaker normalization procedure that improves classification rate compared to the one we obtain with a simple male/female normalization.\n",
    "In our last set of experiments we have studied the influence of the context on the classification result. The best classification results in our experiments were achieved by a combination of default formants and labels specifying the context together with speaker normalization of the automatically measured synthesis parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-165"
  },
  "benedetto92b_icslp": {
   "authors": [
    [
     "Maria-Gabriella Di",
     "Benedetto"
    ],
    [
     "Jean-Sylvain",
     "Lienard"
    ]
   ],
   "title": "Extrinsic normalization of vowel formant values based on cardinal vowels mapping",
   "original": "i92_0579",
   "page_count": 4,
   "order": 167,
   "p1": "579",
   "pn": "582",
   "abstract": [
    "An extrinsic vowel normalization method is proposed. In the method proposed, a-priori information referring to the first and second formant values (Fl and F2) of three vowels of each speaker is used. In order to obtain the normalized formant values of a given speaker's vowels in a given language it is necessary to: gather reference information on three vowels, for example [i,#,u]. This information corresponds to the Fl and F2 values for [i,<z,u] averaged over a population of native speakers. map the formant values of [i,a,u] of the new speaker, onto the reference formant values. This operation can be achieved by linear transformation (isomorphic transformation). the normalized formant values of the other vowels can be obtained by applying the isomorphic transformation. Results are presented for vowels of American English, and briefly reported for vowels of Italian, and French. Comparison between a classical F1-F2 and the mapped vowel representations in terms of statistical parameters are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-166"
  },
  "nearey92_icslp": {
   "authors": [
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "Applications of generalized linear modeling to vowel data",
   "original": "i92_0583",
   "page_count": 4,
   "order": 168,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "Generalized linear models (GLM) provide a flexible framework for investigating many long-standing issues concerning the relation of FO and formant frequencies to vowel categories. These issues include the choice of frequency scale (e.g., log Hz, Bark, ERB), the effect of F0, and the importance of longer-term, speaker-dependent extrinsic information such as formant ranges or average fundamental frequency to the specification of vowel quality. As noted in [2, 31 the differences in the empirical consequences of alternate models can be quite subtle. The present paper illustrates how GLM and related techniques may inform the choice among competing approaches from three perspectives: the modeling of patterns in production data, data-analytic pattern recognition, and direct perceptual modeling. Analysis of Fl patterns from the individual Peterson and Barney [4] data indicates a small but consistent advantage to the log scale over the other two and a preference for extrinsic, formant-average information as a normalization parameter. Simple pattern recognition schemes show relatively little difference among the alternate scales. Initial perceptual modeling via logistic regression on the data from [5] also fails to provide evidence for a substantial difference among the scales. Perceptual experiments analogous to those of [6] specifically designed to be sensitive to the relatively subtle differences among the scales will likely be necessary to resolve the issue of \"the best scale\" for the representation of vowel quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-167"
  },
  "pisoni92_icslp": {
   "authors": [
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Some comments on invariance, variability and perceptual normalization in speech perception",
   "original": "i92_0587",
   "page_count": 4,
   "order": 169,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "One of the central problems in speech perception concerns variability, specifically, the mapping relations between acoustic attributes of the signal and discrete linguistic categories resulting from perceptual analysis. Previous accounts of speech perception have treated variability as an undesirable source of noise that must be reduced or eliminated through perceptual normalization. In this paper, I present an alternative view of the categorization process in speech perception that is based on findings obtained from non-analytic approaches to cognition. According to this view, variability in speech is considered to be \"lawful\" and \"informative\" for perceptual analysis. Perceptual normalization, as discussed in past theoretical accounts, may not involve a true \"loss\" of information but rather may entail the encoding of specific instances and the details of perceptual analysis. This approach to categorization in speech perception provides a new way of dealing with a number of long-standing problems in the field.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-168"
  },
  "goldinger92_icslp": {
   "authors": [
    [
     "Stephen D.",
     "Goldinger"
    ],
    [
     "Thomas J.",
     "Palmeri"
    ],
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Words and voices: perceptual details are preserved in lexical representations",
   "original": "i92_0591",
   "page_count": 4,
   "order": 170,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "Memory for spoken words and speaker's voices was assessed in two experiments. In the first experiment, continuous recognition memory was superior for words repeated in the same voice as their original presentation than for words presented in one voice and later repeated in another. Tin's same-voice recognition advantage was consistent up to delays of 3-4 minutes between first and second presentations. In the second experiment, discrete recognition memory and implicit memory were examined. The explicit recognition memory test replicated the same-voice advantage observed in continuous recognition memory, but the advantage disappeared over time. The implicit memory test, however, showed a same-voice advantage in perceptual identification that was robust over time, lasting up to a week. Taken together, the results suggest that perceptually detailed, episodic memories of spoken words are retained in memory. These episodes affect conscious recognition decisions in the short-run, and they affect online spoken word recognition for indefinite periods thereafter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-169"
  },
  "cheng92_icslp": {
   "authors": [
    [
     "Yan Ming",
     "Cheng"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Speech enhancement using a statistically derived filter mapping",
   "original": "i92_0515",
   "page_count": 4,
   "order": 171,
   "p1": "515",
   "pn": "518",
   "abstract": [
    "We view the speech enhancement task in two aspects: reduction of the perceptual noise level in degraded speech and reconstruction of the degraded information, which may result in improvement of speech intelligibility. We are also very interested in noise-independent speech enhancement where test noise environments could differ in intensity from those of algorithm development. To this end, we have developed in this paper an algorithm called Noise-Independent Statistical Spectral Mapping (NISSM) to estimate a speech enhancement Wiener filter. NISSM consists of a noise-resistant transformation, which converts noisy speech to a set of noise-resistant features, and a spectral mapping function, which maps the features to autoregressive spectra of clean speech. We will show that the proposed algorithm effectively reduces noise intensity. When the noise intensity of training differs from that of testing, NISSM outperforms significantly a conventional spectral mapping. The algorithm operates frame-by-frame and is designed for real-time application. The noise interference could be stationary or non-stationary white noise with variable intensity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-170"
  },
  "beattie92_icslp": {
   "authors": [
    [
     "V. L.",
     "Beattie"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Hidden Markov model state-based cepstral noise compensation",
   "original": "i92_0519",
   "page_count": 4,
   "order": 172,
   "p1": "519",
   "pn": "522",
   "abstract": [
    "In order to make speech recognition technology viable in realistic environments, high performance in noise has become an important goal for speech recognition research. We present a Hidden Markov Model (HMM) cepstral noise compensation method which effectively uses the available information about the speech signal and the background noise, thereby improving recognition performance significantly with only a minimal computational load. The algorithm is based on Wiener filtering the corrupted signal, which corresponds to a multiplication in the frequency domain of the noisy speech spectrum and the Wiener filter frequency response. In the quefrency domain this corresponds to an addition. Therefore if the frequency response of the Wiener filter can be estimated, the cepstral representation of this filter can be applied as an additive correction factor during recognition. Comparative testing shows that cepstral compensation performs as well or better than other current noise-robust methods, usually at a much lower computational cost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-171"
  },
  "brown92_icslp": {
   "authors": [
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Martin P.",
     "Cooke"
    ]
   ],
   "title": "A computational model of auditory scene analysis",
   "original": "i92_0523",
   "page_count": 4,
   "order": 173,
   "p1": "523",
   "pn": "526",
   "abstract": [
    "A computational model of auditory scene analysis is presented which is able to segregate speech from an arbitrary noise intrusion. A representational approach to hearing is adopted, in which models of higher auditory organization - auditory maps - are employed to make particular aspects of the auditory scene explicit. Specifically, maps extract information about periodicity, onsets, offsets and frequency transitions in different spectral regions. The performance of the system has been evaluated using the task of segregating speech from a variety of interfering noises, such as \"cocktail party\" noise, tones, music and other speech. The waveform of the segregated speech can be recovered by a resynthesis technique, and informal listening tests suggest that the speech is highly intelligible and quite natural. Additionally, signal-to-noise ratios have been compared before and after segregation by the system. An improvement in signal-to-noise ratio is obtained for each noise intrusion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-172"
  },
  "nandkumar92_icslp": {
   "authors": [
    [
     "S.",
     "Nandkumar"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Robert J.",
     "Stets"
    ]
   ],
   "title": "A new dual-channel speech enhancement technique with application to CELP coding in noise",
   "original": "i92_0527",
   "page_count": 4,
   "order": 174,
   "p1": "527",
   "pn": "530",
   "abstract": [
    "In this study, low bit-rate speech coding in noisy acoustic environments is addressed. Speech coding based on various forms of linear predictive coding is known to be sensitive to additive background noise. This study investigates in detail a new auditory constrained enhancement (ACE) technique as a front-end enhancer for a 4800 bps CELP coder operating in noisy environments. A novel, dual-channel, auditory based constrained iterative enhancement scheme is developed and shown to improve quality over all classes of speech. Performance of the proposed enhancement-coding tandem is evaluated using objective speech quality measures for nine background noise conditions over a subset of the NIST/TIMIT database. Average objective quality measures indicate that CELP maintains the level of distortion for speech coded in white Gaussian noise, but further degrades speech coded in non-stationary colored noise. A significant quality improvement for the tandem is shown, which is consistent over all speech classes and most phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-173"
  },
  "moreno92_icslp": {
   "authors": [
    [
     "Asunción",
     "Moreno"
    ],
    [
     "José A. R.",
     "Fonollosa"
    ]
   ],
   "title": "CUMULANT - based voicing decision in noise corrupted speech",
   "original": "i92_0531",
   "page_count": 4,
   "order": 175,
   "p1": "531",
   "pn": "534",
   "abstract": [
    "In this paper it is shown that the use of third order statistics is useful for voicing decision of a speech signal. We focus our attention in noisy speech contaminated with gaussian and non gaussian noises. Voicing decision based in second order statistics has been widely used with very good results in clean speech and speech contaminated with gaussian noise. We will show that Higher Order Statistics (HOS) are more insensitive to a wide kind of noises, even those produced by sources of periodic noise as car engines. The algorithm has been applied in noise corrupted speech, at different levels of signal to noise ratio, and with different kinds of noise. The results show that this new algorithm gives in all the cases a much better voicing decision than a second order based algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-174"
  },
  "anglade92_icslp": {
   "authors": [
    [
     "Yolande",
     "Anglade"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Selectively trained neural networks for the discrimination of normal and lombard speech",
   "original": "i92_0595",
   "page_count": 4,
   "order": 176,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "The purpose of this work is to improve the automatic recognition of confusable words, considering such typical examples as French and American-English Alphabets. Our study proposes a comparison between global methods like DTW or HMM and a new method using neural networks. This method is based on the search for 2 discriminative frames inside the confusable words bearing the distinction between them. Then a parametrization is done and resulting vectors are given to neural networks. The tests conducted on normal speech, Lombard speech without additive noise and Lombard speech with additive noise show a general improvement of the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-175"
  },
  "rosenberg92_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Joel",
     "DeLong"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "The use of cohort normalized scores for speaker verification",
   "original": "i92_0599",
   "page_count": 4,
   "order": 177,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "A likelihood ratio scoring technique for speaker verification is described. The likelihood score for the speaker whose identity is claimed is compared with the scores of a \"cohort\" of other speakers assigned to that speaker. The likelihood ratio is used as a \"normalized\" verification score. This normalization technique can be viewed as providing a dynamic threshold which compensates for some kinds of trial-to-trial variations. In particular, it is shown that the use of cohort normalized scores compensates for the degradation obtained by comparing verification utterances recorded using an electret microphone with models constructed from training utterances recorded with a carbon button microphone. Cross-microphone verification equal-error rate drops from 22% using unnormalized scores to 4.8% using cohort normalized scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-176"
  },
  "matsui92_icslp": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Speaker recognition using concatenated phoneme models",
   "original": "i92_0603",
   "page_count": 4,
   "order": 178,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "This paper investigates a new text-dependent speaker recognition method in which the key texts can be changed every time the recognizer is used and the voice is accepted only when the true speaker utters the prompted text. This method solves the problem in conventional methods which use fixed texts and thus can be defeated by recording the true speaker's voice. Our method is based on the following techniques. First, a text-independent speaker spectral model is created for each speaker as a 1-state 64-mixture-Gaussian hidden Markov model (HMM) with diagonal covariance matrices. Next, the mixture weighting factors are estimated for each phoneme class using training utterances of each speaker. Then the likelihood of the phoneme-class-concatenation model corresponding to the prompted key text is used for the recognition decision. When the proposed method is used, the rejection rate for speech uttered by the true speaker that differs from the key text is as high as 80.7% and the verification rate is 99.9%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-177"
  },
  "bennani92_icslp": {
   "authors": [
    [
     "Younes",
     "Bennani"
    ]
   ],
   "title": "Speaker identification through a modular connectionist architecture: evaluation on the timit database",
   "original": "i92_0607",
   "page_count": 4,
   "order": 179,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "We present a connectionist system for text independent speaker identification. We have developed an architecture based on the cooperation of several connectionist modules to achieve this identification. The system is composed of a typology detector and a set of expert modules. Each expert module of the system is concerned with the discrimination between speakers of the same typology. The score used in the final decision is obtained weighting the scores of the typology detection module with those of the expert modules. The system has been tested on a population of 102 speakers extracted from the DARPA-TIMIT database. Perfect identification has been observed, specifically, an interval of confidence 95% for [99.9%, 100.0%] recognition with a precision of 0.1%. The performances of our system are compared with those of a system based on multivariate auto-regressive models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-178"
  },
  "montacie92_icslp": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Jean-Luc Le",
     "Floch"
    ]
   ],
   "title": "AR-vector models for free-text speaker recognition",
   "original": "i92_0611",
   "page_count": 4,
   "order": 180,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "In this paper, a new text-independent speaker recognition method is proposed. This method uses a modeling of the spectral evolution of the speech signals, which is capable of processing some aspects of the inter-speaker variability : the AR-Vector models. Some inter-speaker measures are presented and their advantages/inconvenients are discussed. A training technique to learn discriminant AR-Vector models is proposed. The evaluation of this method is carried out on the TIMIT database recorded by cooperative speakers without any impostor. A series of text-independent speaker identification experiments are described. There is no specific corpus for the training sentences and the training corpus is different from the test corpus. Two speech qualities are tested (i.e., good quality and phone quality). The experiments with good speech quality give first-rate results (i.e, identification rate of 100% for 420 speakers) without using more than two sentences for each test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-179"
  },
  "schiel92_icslp": {
   "authors": [
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Rapid non-supervised speaker adaptation of semicontinuous hidden Markov models",
   "original": "i92_1463",
   "page_count": 4,
   "order": 181,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "Classification by semicontinuous hidden Markov models (SCHMM) achieves very good performance in speaker dependent mode, but the performance decreases considerably in the speaker independent mode. Hence there is a need for a rapid and non-supervised adaptation of the speaker independent system to an unknown user without any restrictions. The new speaker can use the system immediately for his purpose without knowing about the adaptation (no fixed texts). A supervision by the user is optional, but not obligatory. Two methods of adaptation are described: The first algorithm called SPONGE rapidly adapts the parameters of the used codebooks to the data of the new speaker. The second algorithm called ADDMIX re-trains the parameters of the SCHMM themselves (mixture coefficients only). Both methods can handle changes of speakers without any announcement by the users. The described algorithms are applicable to all speaker independent speech recognition systems using SCHMMs. The relative improvement of the recognition rate is about 10-20 %, depending on the unknown speaker. The algorithm SPONGE is compared to the well-known LVQl and LVQ2 by T. Kohonen and two other algorithms developed at our institute. The algorithm ADDMIX is compared with three other methods intending to sharpen the mixture coefficients within the states of the SCHMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-180"
  },
  "ederveen92_icslp": {
   "authors": [
    [
     "D.",
     "Ederveen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Rule-based recognition of phoneme classes",
   "original": "i92_1467",
   "page_count": 4,
   "order": 182,
   "p1": "1467",
   "pn": "1470",
   "abstract": [
    "The aim of our research is to acquire and implement the acoustic-phonetic knowledge of the Dutch language which is necessary for the automatical generation of a phoneme lattice from the acoustical speech signal. This phoneme lattice, generated by the acoustical preprocessor, can be used as input for the lexical module of a speech recognizer. The preprocessor is implemented in Prolog, so as to facilitate the representation of the knowledge in a rule-based system.\n",
    "In this paper we will present rules used for recognition of phoneme classes, together with the formalism used for implementing the rules. In addition, some recognition results will be presented. As recognition of classes is performed independently, i.e. one recognition step for each class, recognition results (segmentlists) are obtained for every class. Calculation of overall performance involves a decision mechanism, which we will also discuss, to combine the segment-lists of individual phoneme classes to a lattice of phoneme classes, using the measures of certainty associated with the elements in the segment-list, and different types of knowledge, e.g. phonological and statistical knowledge.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-181"
  },
  "yi92_icslp": {
   "authors": [
    [
     "Jie",
     "Yi"
    ],
    [
     "Kei",
     "Miki"
    ]
   ],
   "title": "A new method of speaker-independent speech recognition using multiphone HMM",
   "original": "i92_1471",
   "page_count": 4,
   "order": 183,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "In this paper, we describe a new HMM based multiphone method developed to reduce training data and training time as well as to restrain the effect of contextual holes. We define a multiphone as all possible phoneme combinations which consist of less than 4 phonemes. For the purpose of reducing the number of units, we train the most high-frequency-in-use multiphones instead of training all the multiphones. Recognition results are obtained by applying our method to the Japanese Common Speech Data Corpus. The results from the training-vocabulary show that our method achieves the same recognition accuracy as that of triphone HMM's. For the non-training vocabulary, we demonstrate that our method, compared to triphone method, reduces the error rate by as much as 70%. We also propose a two-stage search algorithm based on a pre-selection step and a detailed A* search. We show that compared to the Viterbi beam-search, the two-stage search algorithm just takes 70% of the computing time without reducing the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-182"
  },
  "koo92_icslp": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Chong-Kwan",
     "Un"
    ]
   ],
   "title": "A speaker adaptation based on corrective training and learning vector quantization",
   "original": "i92_1475",
   "page_count": 4,
   "order": 184,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "In this paper, we present a speaker adaptation technique based on corrective training(CT) and learning vector quantization (LVQ). Our algorithm consists of two stages: codebook adaptation and hidden Markov model(HMM) parameter adaptation. In the stage of codebook adaptation, we propose a codebook adaptation scheme using a neurally-inspired LVQ with highly discriminant ability. In the stage of HMM parameter adaptation, we propose a modified corrective training algorithm for speaker adaptation in which the HMM parameter adaptation obtained by probability transformation matrix arc re-estimated to maximize the recognition rate on the adaptation speech. With this method, the recognition rate for new speakers can be improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-183"
  },
  "shirai92_icslp": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Phoneme recognition in continuous speech based on mutual information considering phonemic duration and connectivity",
   "original": "i92_1479",
   "page_count": 4,
   "order": 185,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "This paper discusses an optimal method to decide phonemic sequence using frame-level likelihood of phonemes and the statistics of their duration and connectivity. In the results of phoneme recognition in continuous speech, there are often many deletion and insertion errors. Therefore, it is important to reduce such errors to realize highly advanced continuous speech recognition system. Our algorithm is based on DP method. The duration of each phoneme is expressed by stochastic model, and the connectivity of phonemes is determined on the basis of phone tical and phonological knowledge. Furthermore, we also propose its application for word detection using a method to decide phrase boundary by prosodic information. As the result, the performance of the speaker dependent recognition is improved to 94.6% (3.7% insertions, 3.5% deletions) for word utterance and 67.1% (17.7%, 16.4%) for sentence utterance, respectively. And the performance of word detection is 69.0% for independent words. These scores are much better than those obtained in our previous system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-184"
  },
  "koga92_icslp": {
   "authors": [
    [
     "Shinji",
     "Koga"
    ],
    [
     "Ryosuke",
     "Isotani"
    ],
    [
     "Satoshi",
     "Tsukada"
    ],
    [
     "Kazunaga",
     "Yoshida"
    ],
    [
     "Kaichiro",
     "Hatazaki"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "A real-time speaker-independent continuous speech recognition system based on demi-syllable units",
   "original": "i92_1483",
   "page_count": 4,
   "order": 186,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "This paper describes a real-time speaker-independent continuous speech recognition system. In order to achieve speaker-independent continuous speech recognition with real-time response, demi-syllable speech units, a bundle search algorithm, and multi-processing techniques were used. The use of demisyllables allows all transitions between phonemes to be represented, thus improving the recognition accuracy. Bundle search is a frame synchronous technique used with Viterbi search to reduce the computational load needed to search the finite state grammar network used. The search process is divided into three pipelined stages : frame-level likelihood calculation, word-level search and sentence level network search. Each of these three pipelined stages is further split into several sub-processes. These processes are run in parallel on a multi-processor machine. Real-time performance was achieved in the evaluation experiments using a 500 word vocabulary. 83.0% sentence accuracy and 95.5% word recognition accuracy were achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-185"
  },
  "vaseghi92_icslp": {
   "authors": [
    [
     "Saeed V.",
     "Vaseghi"
    ],
    [
     "Ben P.",
     "Milner"
    ]
   ],
   "title": "Speech recognition in noisy environments",
   "original": "i92_1487",
   "page_count": 4,
   "order": 187,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "Speech and noise process are considered as cluster of points in a multidimensional spectral space. The effects of noise on a signal cluster is to increase the variance and to move the centroid in the general direction of noise. The degradations in observation likelihood is expressed in terms of the mean and variance of noise spectrum.\n",
    "In many cases of erroneous speech recognition the correct model is among the few with the highest scores. This observation forms the basis for re-evaluation of the few high scoring candidates before making a final decision. The noisy input signal is filtered by state dependent Wiener filters derived from the most likely state sequence of each HMM model. A revised score for each candidate model and the filtered signal is calculated. The model with the highest score is selected. This method achieves on average 30% reduction in recognition error compared to uncompensated scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-186"
  },
  "mcinnes92_icslp": {
   "authors": [
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "An enhanced interpolation technique for context-specific probability estimation in speech and language modelling",
   "original": "i92_1491",
   "page_count": 4,
   "order": 188,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "A recurring problem in speech and language modelling is the estimation of context-specific probability distributions from sparse data. Robust estimates can be obtained by interpolating the probability estimates obtained from context-specific statistics with more general ones. An interpolation technique is described here which, is based on a least-squares weighting formula but with deleted estimation incorporated to optimise its parameters. Perplexity results are given for various statistical language models incorporating this interpolation technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-187"
  },
  "fissore92_icslp": {
   "authors": [
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "G.",
     "Micca"
    ],
    [
     "G.",
     "Sperto"
    ]
   ],
   "title": "Channel adaptation for a continuous speech recognizer",
   "original": "i92_1495",
   "page_count": 4,
   "order": 189,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "This paper deals with adaptation to new test environments of an HMM recognizer trained in a given condition. In particular, a speaker-independent continuous speech recognizer has been trained using a multispeaker database collected through a telephone with an electret transducer and tested with several types of hand-sets having different spectral response characteristics. The results of these experiments show that system performance drops dramatically on test utterances collected through carbon transducers. A procedure for hierarchical spectral mapping, originally proposed for speaker adaptation, is used f0 channel adaptation. A codebook adapted to the new channel is obtained from a database including a limited number of adaptation sentences. Test sentences are then vector quantized using the adapted codebook. Tested with sentences acquired through a carbon transducer, more than 60% of performance improvement is achieved, while nearly 100% of the errors can be recovered when electret type transducers are used for testing.\n",
    "Adapted HMMs are then estimated by aligning the sentences collected in the new environment against the HMMs trained in the original environment, and nearly 92% of the errors are recovered in the carbon transducer test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-188"
  },
  "cifuentes92_icslp": {
   "authors": [
    [
     "S.",
     "Cifuentes"
    ],
    [
     "J.",
     "Colas"
    ],
    [
     "M.",
     "Savoji"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "A new algorithm for connected digit recognition",
   "original": "i92_1499",
   "page_count": 4,
   "order": 190,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "In this paper we present an Unconstrained Level Building (ULB) algorithm based on the known Level-Building approach [1][11], which allows to deal, in a new particular form, the problem of the interior pauses or silences produced when strings of digits are pronounced in a natural way (strings of digits may contain silences or pauses between digits, due, for example, to the speaker's need of breathing, or to his hesitation in recalling or reading the next digit to pronounce). To evaluate the performance of the new algorithm a speaker dependent connected digits Spanish data base has been used, consisting of 129 digit strings, with lengths ranging from one to five digits, pronounced by a male speaker. The recognition set contains 46 strings, and the training set 64. The 19 remaining strings have been used, as an independent testing set, to control the convergence of the training procedure. Several experiments were carried out to determine the best set of conditions for recognition of digit strings. The improvement obtained on String Accuracy using the ULB algorithm instead of a Level Building with an HMM for the silence is 4.3%, and without any silence modelling is nearly 11%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-189"
  },
  "ruske92_icslp": {
   "authors": [
    [
     "Günther",
     "Ruske"
    ],
    [
     "Bernd",
     "Plannerer"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Stochastic modeling of syllable-based units for continuous speech recognition",
   "original": "i92_1503",
   "page_count": 4,
   "order": 191,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "This paper presents a new concept for a demisyllable-based speech recognition system for German where final consonant clusters are subdivided into rudiments and suffixes. These units are represented by Semicontinuous HMMs. All equations for modeling the time duration with Gaussian or Gamma distributions are given. An economic solution applies simple \"post-processing\" by a temporal weighting of each HMM state. The experimental results show an increase in recognition rate with duration modeling from 53.8% to 56.3% for initial consonant clusters, from 68.3% to 69.4% for rudiments and from 78.3% to 80.8% for suffixes, all from continuous speech in a speaker-independent mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-190"
  },
  "goblirsch92_icslp": {
   "authors": [
    [
     "David M.",
     "Goblirsch"
    ],
    [
     "Toffee A.",
     "Albina"
    ]
   ],
   "title": "HARK: an experimental speech recognition system",
   "original": "i92_1507",
   "page_count": 4,
   "order": 192,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "As part of its internal research program, MITRE has developed an experimental speech recognition system for small-vocabulary simple-grammar applications. This system, called Hark, has three parts: a signal processor for extracting feature vectors from the acoustic signal, a neural network for classifying the feature vectors, and a dynamic programming module for decoding the utterance. Hark is speaker independent and handles continuous speech with pauses after sentences. This paper provides an overview of the Hark system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-191"
  },
  "nagai92b_icslp": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Jun-Ichi",
     "Takami"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "The SSS-LR continuous speech recognition system: integrating SSS-derived allophone models and a phoneme-context-dependent LR parser",
   "original": "i92_1511",
   "page_count": 4,
   "order": 193,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "This paper describes a system for accurate continuous speech recognition called \"ATREUS/SSS-LR\". A phoneme-context-dependent LR parser drives allophonic HMMs represented by a shared-state network automatically generated by the Successive State Splitting (SSS) algorithm. In this system, the SSS principle has also been applied to duration clustering: optimal clusters of phoneme-context-dependent durations are automatically generated independently of the HMnet-based allophonic classes.\n",
    "ATREUS/SSS-LR achieved a phrase recognition rate of 93.2%, the best recognition result achieved in the 1000-word recognition experiments conducted at ATR. This recognition rate was obtained with a smaller beam width than used with discrete HMM(fuzzy vector quantization) and continuous mixture density HMM. This shows that the SSS-LR can realize both fast parsing and high accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-192"
  },
  "sakai92_icslp": {
   "authors": [
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Michael",
     "Phillips"
    ]
   ],
   "title": "J-SUMMIT: a Japanese segment-based speech recognition system",
   "original": "i92_1515",
   "page_count": 4,
   "order": 194,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "This paper describes the application of the SUMMIT speech recognition system to continuously spoken Japanese speech within a limited domain. Summit is a speech recognition system being developed at MIT which makes explicit use of acoustic-phonetic knowledge, embedded in a segmental framework that can be trained automatically [1], and has been ported to many applications in English [2, 3]. In order to cope with Japanese speech, we prepared a word lexicon with baseform pronunciations and morphological categories, wrote a set of phonological rules, and developed a morphological category-pair grammar for Japanese. Preliminary recognition experiments within the VOYAGER domain [2] were performed using data read by a single male speaker. Word and sentence error rates of 7.1% and 41.6%, respectively were obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-193"
  },
  "mizuta92_icslp": {
   "authors": [
    [
     "Shinobu",
     "Mizuta"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Optimal discriminative training for HMMs to recognize noisy speech",
   "original": "i92_1519",
   "page_count": 4,
   "order": 195,
   "p1": "1519",
   "pn": "1522",
   "abstract": [
    "In this paper, we describe a training method for continuous mixture density HMMs, named optimal discriminative training (ODT), and its implementation for speech recognition in noise. Conventional maximum likelihood estimation method (MLE) for HMM training has a problem that the recognition performance is not considered in the training procedure. ODT is one of corrective learning methods, applied to continuous mixture density HMMs, and these HMMs are especially useful for speaker-independent speech recognition. Under noisy environments, the recognition categories are tend to be difficult to discriminate in feature space, so by using ODT the improvement of recognition accuracy is more expected. Here, after description the training algorithm of ODT, we discuss the effects of ODT to improve the robustness for adverse environments by the word recognition experiments in noise, where the noise signal takes various levels and spectral characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-194"
  },
  "kuroiwa92_icslp": {
   "authors": [
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ],
    [
     "Kunihiko",
     "Owa"
    ],
    [
     "Makoto",
     "Shozakai"
    ],
    [
     "Ryuji",
     "Matsumoto"
    ]
   ],
   "title": "Architecture and algorithms of a real-time word recognizer for telephone input",
   "original": "i92_1523",
   "page_count": 4,
   "order": 196,
   "p1": "1523",
   "pn": "1526",
   "abstract": [
    "We describe a new real-time isolated word recognizer. The recognizer is designed for an extension number guidance system which looks up and announces an extension number by voice input over the telephone network. To deal with telephone quality speech input which includes noise and distortion during transmission over the telephone network, we have developed feature extraction and a word detection algorithm. These techniques use wide band-pass filter outputs which are generally used to decide whether speech is voiced sound or unvoiced sound. The recognizer is implemented with six DSP chips and executes the above functions in real-time. Finally, the recognizer is evaluated using a large telephone voice database consisting of more than 500 speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-195"
  },
  "kuwano92_icslp": {
   "authors": [
    [
     "Hiroyasu",
     "Kuwano"
    ],
    [
     "Kazuya",
     "Nomura"
    ],
    [
     "Atsushi",
     "Ookumo"
    ],
    [
     "Shoji",
     "Hiraoka"
    ],
    [
     "Taisuke",
     "Watanabe"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ]
   ],
   "title": "Speaker independent speech recognition method using word spotting technique and its application to VCR programming",
   "original": "i92_1527",
   "page_count": 4,
   "order": 197,
   "p1": "1527",
   "pn": "1530",
   "abstract": [
    "An important problem in the practical use of speech recognition technology is the robustness to environment noise and unnecessary utterances. We have already reported in IEEE ICASSP '87[i] that the recognition method using the word spotting technique is effective on such conditions. However, the word spotting error, which part of a word incorrectly matches with other words occurred in some vocabularies. CLM-alpha(Improved Continuous Linear Expansion / Compression Matching) Method was adopted as a recognition method. In this method, two matching procedure steps are carried out; a primary matching procedure is characterized by word spotting, and a secondary matching procedure is performed on the sequence of similarity scores obtained through the primary matching procedure. As a result, the word spotting error was minimized and the overall recognition performance was improved.\n",
    "In addition, the application of newly developed voice-recognition LSI which uses a CLM-alpha method in the remote control for VCR has made it possible to make program reservations using voice dialog. In an evaluation test using the hardware implementation of the control, an average recognition rate of 95.9% was obtained with a 58-word vocabulary which was divided into 6 word groups, spoken by 141 men and women in an office environment. At present, the Japanese version is on the market.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-196"
  },
  "lennon92_icslp": {
   "authors": [
    [
     "S.",
     "Lennon"
    ],
    [
     "E.",
     "Ambikairajah"
    ]
   ],
   "title": "Transputer implementation of front-end processors for speech recognition systems",
   "original": "i92_1531",
   "page_count": 4,
   "order": 198,
   "p1": "1531",
   "pn": "1534",
   "abstract": [
    "This paper describes the transputer implementation of two different front-end processors suitable for use in a speech recognition system. The two preprocessors are a physiologically-based cochlear model and a parallel filter bank based on the mel frequency scale. The structure of the cochlear model is a cascade of 128 digital filters. The parallel filter bank consists of 19 independent channels. The inherent structure of the preprocessors is reflected in the topology of the transputer configuration used to implement them. The two preprocessors developed, were tested, for their suitability for use as a front-end, to a two-layer Kohonen neural network based speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-197"
  },
  "minami92_icslp": {
   "authors": [
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Phoneme HMM evaluation algorithm without phoneme labeling",
   "original": "i92_1535",
   "page_count": 4,
   "order": 199,
   "p1": "1535",
   "pn": "1538",
   "abstract": [
    "This paper proposes a phoneme HMM evaluation algorithm that does not require phoneme labeling and that gives a result which is independent of the recognition task. This algorithm is applied to an evaluation of speaker independent HMMs trained by using a speech database uttered by 64 speakers. The algorithm is compared with the conventional evaluation algorithm based on phoneme labels. The results show that the proposed algorithm is highly useful for HMM phoneme model evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-198"
  },
  "noll92_icslp": {
   "authors": [
    [
     "A.",
     "Noll"
    ],
    [
     "H.",
     "Bergmann"
    ],
    [
     "H. H.",
     "Hamer"
    ],
    [
     "Annedore",
     "Paeseler"
    ],
    [
     "H.",
     "Tomaschewski"
    ]
   ],
   "title": "Architecture of a configurable application interface for speech recognition systems",
   "original": "i92_1539",
   "page_count": 4,
   "order": 200,
   "p1": "1539",
   "pn": "1542",
   "abstract": [
    "For the integration of a speech recognition system into an application the application designer needs experience in two different areas: detailed knowledge of the application process and undesirably also detailed knowledge of the underlying speech recognition system.\n",
    "This paper describes the architecture of the SpeechMaster development system which is based on a configurable application interface for a connected-word recognition system. This system aims on making the development and integration process more transparent and opens speech recognition technology to a large number of application areas.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-199"
  },
  "fanty92_icslp": {
   "authors": [
    [
     "Mark",
     "Fanty"
    ],
    [
     "John",
     "Pochmara"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "An interactive environment for speech recognition research",
   "original": "i92_1543",
   "page_count": 4,
   "order": 201,
   "p1": "1543",
   "pn": "1546",
   "abstract": [
    "A UNIX software environment for speech recognition research is described. This environment is undergoing development under an NSF Software Capitalization grant and will be made public when complete. The speech tools will allow users to compute and display a variety of signal representations of speech, to label speech at a number of levels, to train and evaluate neural network classifiers and to display the processing stages of speech algorithms. The tools include file formats and support routines for audio files, two-dimensional data files (e.g. FFT output), neural network classifiers and time-aligned label files. The LYRE and AUTOLYRE programs display speech data in a variety of ways. NOPT is a conjugate gradient descent neural network training program which is limited in flexibility but easy to use and fast. A distributed version for networked workstations exists and will continue to be enhanced. A small but useful set of signal processing routines is provided including Perceptual Linear Predictive Analysis (PLP). Speech algorithms include a pitch tracker for high-quality speech and dynamic programming optimization code for use with phoneme probability matrices (e.g. as computed by a neural network classifier). Although the software currently reflects the biases of the Center for Spoken Language Understanding towards neural-network-based recognition using DFT- and PLP-based features, we hope that the user community continues to enhance the tool set. We have tried to design it with flexibility and growth in mind.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-200"
  },
  "abe92_icslp": {
   "authors": [
    [
     "Y.",
     "Abe"
    ],
    [
     "K.",
     "Nakajima"
    ]
   ],
   "title": "An approach to unlimited vocabulary continuous speech recognition based on context-dependent phoneme modeling",
   "original": "i92_1547",
   "page_count": 4,
   "order": 202,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "This paper describes a continuous speech recognition system searching an arbitrary phoneme string applying a contextual model. Two contextual models, called Mixture Multiple Linear Phonetic-Context Model(MM-LPCM) and Mixture Single LPCM(MS-LPCM), are presented. Both models are designed to represent more complicated variations not represented by the original LPCM. In isolated phoneme recognition experiments, the MM-LPCM achieved the minimum error rate of 4.6% which is lower than the original LPCM by 1.4 points. Context-dependent search is based on a hypothesis-and-test scheme, in which a phoneme string hypothesis is expanded by appending one phoneme at a time considering the left and right contexts. A mechanism to bound the phonemic boundaries is introduced to reduce insertion errors in the search. The context-dependent search algorithm with the improved contextual model achieved the total phoneme error rate of 12.9%, which was half of context-independent search.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-201"
  },
  "wooters92_icslp": {
   "authors": [
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Acoustic subword models in the berkeley restaurant project",
   "original": "i92_1551",
   "page_count": 4,
   "order": 203,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "Creating a lexicon for a speech recognizer that uses word models based on phonemes can be a difficult task when the lexicon of the recognizer must contain proper nouns or uncommon words. Recently, researchers have developed techniques for automatic pronunciation learning in which the word models are based on data-driven subphonetic units. These units are able to model acoustic events at a much finer level of detail. In this paper we explore the use of these new techniques within our hybrid MLP/HMM speech recognizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-202"
  },
  "jacobsen92_icslp": {
   "authors": [
    [
     "Claus Nedergaard",
     "Jacobsen"
    ]
   ],
   "title": "SIRtrain, an open standard environment for CHMM recognizer development",
   "original": "i92_1555",
   "page_count": 4,
   "order": 204,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "This paper describes a new approach in the development of training- and testing software for Continuous Hidden Markov Models (CHMM) that has been successfully implemented and utilized by the ESPRIT E project 2094, SUNSTAR. The basic idea behind the new software, called SIRtrain, has been to adopt existing de-facto standards from another ESPRIT project, SAM, thus enabling SIRtrain to work in conjunction with already available speech databases and software tools. Also, the use of a modular and object oriented approach contributes to making SIRtrain a general platform for testing out new principles within speech recognition. Being written in C++ and running on ordinary SUN SPARCstations without additional hardware, the software allows researchers to implement and test new ideas without having to invest in expensive hardware equipment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-203"
  },
  "kobayashi92b_icslp": {
   "authors": [
    [
     "Yutaka",
     "Kobayashi"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "Segmented trellis algorithms for the continuous speech recognition",
   "original": "i92_1559",
   "page_count": 4,
   "order": 205,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "In this paper, we propose two versions of the HMM-trellis calculation algorithms, STA-I and STA-II, suitable for continuous speech recognition systems. The Segmented Trellis Algorithms make use of the HMM's property that each state models a particular interval of the acoustic signal. The STA-I was implemented in our system, SUSKIT-IL It achieved three times faster processing speed without decreasing the average sentence recognition rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-204"
  },
  "xu92_icslp": {
   "authors": [
    [
     "Bo",
     "Xu"
    ],
    [
     "Z. W.",
     "Lin"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "D. X.",
     "Xu"
    ],
    [
     "Y. Q.",
     "Gao"
    ]
   ],
   "title": "A. 46,500 word Chinese speech recognition system",
   "original": "i92_1563",
   "page_count": 3,
   "order": 206,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "This paper introduces a large vocabulary isolated words speech recognition system for Chinese speech based on HMM. modelling of phonemes.Through the concatenation of phonemes a word HMM model can be formed in training stage,in this case the coarticuiation of connected utterance is preserved in the mode Lin recognition phase, Syllable String Network(SSN) without any lexical constraint outputs one or two phonetic strings with different length,then every word in library is matched and scored comparing with these owe or two strings, faking 50-100 candidates,rebuildrag lexical tree,a less pruning Viterbi Beam Search(VBS) is applied to get final resulLThe system achieves 86% recognition rate for top-1 and 94% for top 5.A concept of using pre-computcd confusion matrices is proposed for phoneme string mathcing in this paper.Also the way of estimation of these matrices is provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-205"
  },
  "chen92_icslp": {
   "authors": [
    [
     "Dao Wen",
     "Chen"
    ]
   ],
   "title": "Study of the time extension flat net for speech recognition",
   "original": "i92_1567",
   "page_count": 1,
   "order": 207,
   "p1": "1567",
   "pn": "1568",
   "abstract": [
    "This paper studied the supervisor flat net for speech recognition, The extension function model,a input pattern time sequence relation model,is discussed. The experiments indicated that the converage performance of our model is superior than the multi-layer perception (MLP).\n",
    ""
   ]
  },
  "fallside92_icslp": {
   "authors": [
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "A hidden Markov model structure for the acquisition of speech by machine, ASM",
   "original": "i92_0615",
   "page_count": 4,
   "order": 208,
   "p1": "615",
   "pn": "618",
   "abstract": [
    "A structure for the acquisition of speech by machines, asm, employing neural networks, for the 'simultaneous' training of recognition and synthesis has been presented [1]. In the present paper it is shown that there is an equivalent structure employing HMMs. By analogy with recent results relating HMM and neural network architectures it is seen that the two methods for asm are equivalent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-206"
  },
  "masai92_icslp": {
   "authors": [
    [
     "Yasuyuki",
     "Masai"
    ],
    [
     "Shin'ichi",
     "Tanaka"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Speaker-independent keyword recognition based on SMQ/HMM",
   "original": "i92_0619",
   "page_count": 4,
   "order": 209,
   "p1": "619",
   "pn": "622",
   "abstract": [
    "This paper describes a speaker-independent keyword recognition system based on hidden Markov models (HMM's). We propose a new matrix quantization (MQ) algorithm called Statistical MQ (SMQ) that uses an orthogonalized phonetic segment codebook and a word beginning frame prediction (BFP) algorithm to achieve accurate and efficient word-spotting. The SMQ effectively incorporates pattern variations of each phonetic segment into the orthogonalized phonetic segment codebook containing about 700 phonetic segments, and transforms the input speech into a sequence of phonetic symbols. The BFP algorithm predicts a word beginning frame in which the next Viterbi alignment should be generated, using the transition frame at which the initial transition to the second state occurred in the most recent Viterbi alignment. The proposed keyword recognition system has been tested on a data set of 32 keywords, 5 auxiliary words, and 17 unknown words. The test data is compound words uttered by 6 unknown speakers and the keyword recognition accuracy was 91.1%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-207"
  },
  "cardin92_icslp": {
   "authors": [
    [
     "Regis",
     "Cardin"
    ],
    [
     "Diane",
     "Goupil"
    ],
    [
     "Roxane",
     "Lacouture"
    ],
    [
     "Evelyne",
     "Millien"
    ],
    [
     "Charles",
     "Snow"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "CRIM's spontaneous speech recognition system for the ATIS task",
   "original": "i92_0623",
   "page_count": 4,
   "order": 210,
   "p1": "623",
   "pn": "626",
   "abstract": [
    "This paper gives an overall description of CRIM's ATIS (Air Travel Information System) natural language speech understanding system, with special emphasis on the speech recognition component. This spontaneous speech recognizer is used to generate the N-best sentence hypotheses which are needed by our natural language component in order to interpret the spoken sentence. This interpretation is then converted into an SQL query which is used to generate the desired answer from the relational database. The words in the system's vocabulary are built from a set of context-dependent phonetic HMMs. We use discrete HMMs with 4 codebooks, one for each of the following parameter sets: mel cepstral coefficients and energy, as well as their derivatives. We describe our training procedure for both the HMMs and the language model. Despite having only recently started working on large-vocabulary tasks, we feel that we should soon be able to bring our system's recognition performance (now around 82% word correct) to a level comparable to that of the other sites working on the ATIS task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-208"
  },
  "brugnara92_icslp": {
   "authors": [
    [
     "F.",
     "Brugnara"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Improved connected digit recognition using spectral variation functions",
   "original": "i92_0627",
   "page_count": 4,
   "order": 211,
   "p1": "627",
   "pn": "630",
   "abstract": [
    "The use of Spectral Variation Functions (SVFs) for Automatic Speech Recognition (ASR) is discussed. Two types of SVFs are compared and results for different SVF orders are presented. A 20% string error-rate reduction has been obtained on the 10 kHz TI/NIST connected digit corpus, by adding a fourth order SVF to a set of acoustic features made of S Mel scaled Cepstral Coefficients(MCC), their time derivatives, the normalized signal energy and its time derivative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-209"
  },
  "tridgell92_icslp": {
   "authors": [
    [
     "Andrew",
     "Tridgell"
    ],
    [
     "Bruce",
     "Millar"
    ],
    [
     "Kim-Anh",
     "Do"
    ]
   ],
   "title": "Alternative preprocessing techniques for discrete hidden Markov model phoneme recognition",
   "original": "i92_0631",
   "page_count": 4,
   "order": 212,
   "p1": "631",
   "pn": "634",
   "abstract": [
    "In this paper a number of alternative pre-processing configurations are applied to an HMM-based phoneme recognition system and evaluated on the TIMIT speech corpus. It is demonstrated that there is considerable advantage in the addition of processing steps after the initial signal processing. F-ratio analysis gives a clear ranking of the discriminatory power of commonly used features such as log-power, zero-crossing rate, cepstral, delta cepstral and band-power coefficients. Results have been obtained that demonstrate a 20% reduction in the mis-classification rate using a linear discriminant analysis transformation from a 43-variable feature set to a 10-variable linearly transformed feature set. Finally the paper demonstrates that vector quantisation using totally non-parametric classification trees can lead to phoneme classification results competitive with those achieved using traditional techniques, while at the same time offering much faster evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-210"
  },
  "niedermair92_icslp": {
   "authors": [
    [
     "Gerhard Th.",
     "Niedermair"
    ]
   ],
   "title": "Linguistic modelling in the context of oral dialogue",
   "original": "i92_0635",
   "page_count": 4,
   "order": 213,
   "p1": "635",
   "pn": "638",
   "abstract": [
    "This paper describes the integration of linguistic knowledge into the German prototype SUNGerm of the European SUN-DIAL project. The goal of the project is to construct a telphone based real time system for oral inquiries into a database of intercity train schedules [1]. We will describe the system set up and the interaction among the speech recognition, linguistic analyser, and dialogue modules. We suggest how these modules can effectively interact to improve recognition and understanding through language models dependent on the context of the dialogue. Additionally, we outline the linguistic analysis concerning phenomena frequently encountered in oral dialogue. We describe how these phenomena are captured in a semantic interface language (SIL) which is uniform for the different language prototypes within the SUNDIAL project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-211"
  },
  "andry92_icslp": {
   "authors": [
    [
     "Frangois",
     "Andry"
    ]
   ],
   "title": "Static and dynamic predictions : a method to improve speech understanding in cooperative dialogues",
   "original": "i92_0639",
   "page_count": 4,
   "order": 214,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "We describe a method based on two independent and complementary predictions mechanisms, which is used to improve the recognition and the understanding performances of the SUNDIAL1 speech and dialogue system. We exploit information produced by the dialogue manager component of the SUNDIAL system which consists of intentional contents (list of dialogue acts) and proposition al contents (task types associated to a list of domain related semantic types). The static predictions mechanism corresponds to the determination of characteristic states of the dialogue related to the resolution of the task. The dialogue state reference is then transmitted to the recognition module, which activates a specific sub-lexicon and word-pair grammar. The dynamic predictions mechanism is based on two trials (semantic and dialogic), which are applied to the candidate strings produced by the parser. Tests show a significant definite improvement of the sentence understanding rate of the dialogue system with both kinds of mechanism.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-212"
  },
  "heisterkamp92_icslp": {
   "authors": [
    [
     "Paul",
     "Heisterkamp"
    ],
    [
     "Scott",
     "McGlashan"
    ],
    [
     "Nick",
     "Youd"
    ]
   ],
   "title": "Dialogue semantics for an oral dialogue system",
   "original": "i92_0643",
   "page_count": 4,
   "order": 215,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "This paper describes the component in the SUNDIAL spoken dialogue system responsible for semantic interpretation of user input. This component, the 'belief module', constructs interpretations using several sources of knowledge, including static knowledge definitions and a dynamic model of dialogue context, and addresses the \"problem of the hypothetical nature of the recognition and parsing results as well as the general problems of under-specification and ambiguity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-213"
  },
  "nagata92_icslp": {
   "authors": [
    [
     "Masaaki",
     "Nagata"
    ]
   ],
   "title": "Using pragmatics to rule out recognition errors in cooperative task-oriented dialogues",
   "original": "i92_0647",
   "page_count": 4,
   "order": 216,
   "p1": "647",
   "pn": "650",
   "abstract": [
    "This paper describes two pragmatic criteria for ruling out speech recognition candidates that are correct syntactically and semantically, but incorrect contextually. The first criterion is whether the referents of the referring expressions in the candidates, especially zero pronouns in Japanese, can be properly identified or not. We have implemented an ellipsis resolution method that identifies zero pronouns designating the conversational participants based on the uses of honorific and deictic expressions, and employed the \"fewest unidentified zero pronouns heuristics,\" in order to penalize the recognition candidates that have spurious zero pronouns whose referents cannot be identified. The second criterion is whether the candidate forms a natural local discourse in terms of the speech act sequence. We have classified the input utterances into nine Elocutionary Force Types (IFTs), such as REQUEST, INFORM, etc., and used its trigram to rule out erroneous candidates by predicting the next utterance type. The pilot experiment shows that the IFT trigram predicts the IFT of the next utterance with better than 70% accuracy when there is IFT ambiguity in the recognition results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-214"
  },
  "takebayashi92_icslp": {
   "authors": [
    [
     "Yoichi",
     "Takebayashi"
    ],
    [
     "Hiroyuki",
     "Tsubo"
    ],
    [
     "Yoichi",
     "Sadamoto"
    ],
    [
     "Hideki",
     "Hashimoto"
    ],
    [
     "Hideaki",
     "Shinchi"
    ]
   ],
   "title": "A real-time speech dialogue system using spontaneous speech understanding",
   "original": "i92_0651",
   "page_count": 4,
   "order": 217,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "We have developed a task-oriented speech dialogue system based on spontaneous speech understanding and response generation (TOSBURG) for unspecified users. The system consists of a noise-robust keyword-spotter, a semantic keyword lattice parser, a user-initiative dialogue manager and a multimodal response generator. After noise immunity keyword-spotting has been performed, the spotted keyword candidates are analyzed by a new keyword lattice parser to extract the semantic content of input speech. Using the dialogue history and situation, the dialogue manager understands input speech based on the semantic contents, and generates a confirmation message to the user about ambiguous points to help overcome difficulties due to the imperfection of speech understanding. The real-time dialogue system has been constructed for a fast food ordering task using two general purpose workstations and four DSP accelerators.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-215"
  },
  "yang92_icslp": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "A semantic and pragmatic analysis of tone and intonation in Mandarin Chinese",
   "original": "i92_0655",
   "page_count": 4,
   "order": 218,
   "p1": "655",
   "pn": "658",
   "abstract": [
    "Recent trends in research on intonation have suggested that it is a universal feature with basis in physiology, neurology, and evolution. This paper addresses intonation in Mandarin Chinese as a specific case using acoustic data collected from natural discourse situations. The parameters of duration, fundamental frequency, amplitude, and pitch range are analyzed, and the relationship between stress and rhythm is explored. This study finds that intonation affects all parts of an utterance, and that lexical tones are modified substantially under different situations and states. Stress operates under the Principle of Disequilibrium by disrupting the normal rhythmic hierarchy. The expression of finality or definiteness is often associated with falling tone contours while hesitation and continuation are often characterized by raised and lengthened contours. It is proposed that intonation is an essential component of meaning, and that attitudes and emotions are crucial determinants of prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-216"
  },
  "tsukuma92_icslp": {
   "authors": [
    [
     "Yoshimasa",
     "Tsukuma"
    ]
   ],
   "title": "On prosodic features in speech - comparative studies between Japanese and standard Chinese",
   "original": "i92_0659",
   "page_count": 4,
   "order": 219,
   "p1": "659",
   "pn": "662",
   "abstract": [
    "This paper illustrates the importance of a voice fundamental frequency contour resetting (an F0-resetting) for marking a major syntactic boundary in Japanese. For the learners of the spoken Japanese language, some tentative ideas expressing the F0-resetting are suggested in an old Japanese folk tale known as Momotaro, ' The Peach Boy'. This symbol, ' t', denotes that a phrase F0-resetting is placed at the beginning of every major syntactic boundary. Since the F0 functions distinctively at the word level in Chinese, it is assumed that the free manipulation of the F0 would be difficult for Chinese and therefore, the prosodic symbol such as the above could be an appropriate aid for them. In order to see if the prosodic features account for various syntactic structures, the F0-resettings, extracted acoustically, are compared with the corresponding syntactic boundaries. Then, the acoustic results by a native speaker of Japanese are compared with those by two Chinese students studying Japanese as a foreign language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-217"
  },
  "campbell92b_icslp": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ]
   ],
   "title": "Prosodic encoding of English speech",
   "original": "i92_0663",
   "page_count": 4,
   "order": 220,
   "p1": "663",
   "pn": "666",
   "abstract": [
    "This paper describes an algorithm for determining stress groups in a spoken utterance from acoustic parameters of the speech waveform. It uses normalised and smoothed measures of duration and energy to produce an index of stress that is highest on focussed parts of the utterance and lowest at the boundaries between stress groups. 81% of stressed words were correctly identified, with a false detection rate of less than 5%. The location of contrastively focussed words in the utterance was correctly recognised in 76% of cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-218"
  },
  "fant92_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Prediction of syllable duration, speech rate and tempo",
   "original": "i92_0667",
   "page_count": 4,
   "order": 221,
   "p1": "667",
   "pn": "670",
   "abstract": [
    "A model of syllable duration is outlined. It takes into account segmental structures, lexical categories, stress and word accent assignment and positional effects within a syntactic frame. The model is used for predicting temporal structures in text reading to be compared with specific utterances. Prediction errors are small enough to sort out speaker specific variations in local emphasis-deemphasis and speech rate. The concept of speech tempo in relation to speech rate is discussed. Other applications of the model are in studies of speaking style and language contrasting patterns, and as a tool for quantifying syllabic stress.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-219"
  },
  "carlson92b_icslp": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Experiments with emotive speech - acted utterances and synthesized replicas",
   "original": "i92_0671",
   "page_count": 4,
   "order": 222,
   "p1": "671",
   "pn": "674",
   "abstract": [
    "We will report on our current work to model extralinguistic features in speech. As a starting point we analysed sentences representing different emotions produced by actors. In another project, this material had been evaluated by a listener panel. Sentences with successfully acted emotions were selected for further analysis. The analysis corroborated earlier findings concerning speech tempo and fundamental frequency contours. We also found differences in segmental phonetic realizations, partially correlated with speaker efforts, such as energetic angry arid restrained, sad speaking styles. The parametric representation of the analysis was simplified to conform with our rule based text-to-speech system. Several manipulated synthetic versions were compared and evaluated for perceived emotions. The experiment showed that emotions are signalled through a complex interaction of segmental and prosodic cues. In addition to the normally used parameters like fundamental frequency dynamics and level, speaking rate and segmental realization we also observed supporting non-phonetic sounds that enhanced the perceived emotional quality. We will in our presentation give example of the synthetic stimuli and discuss potential problems in the modelling of emotional speech in the present framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-220"
  },
  "caspars92_icslp": {
   "authors": [
    [
     "J.",
     "Caspars"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Phonetic properties of dutch accent lending pitch movements under time pressure",
   "original": "i92_0731",
   "page_count": 4,
   "order": 223,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "Time pressure is used as a new method to isolate the important phonetic aspects of Dutch accent lending pitch movements. The accent lending rise (T) and fall ('A') were realised under three types of time pressure: (i) normal vs. fast speech, (ii) on a long vs. short vowel, (iii) and with single vs. multiple pitch movements within the same time span. The question is how the pitch movements are adjusted to the shrunken space, assuming that the most important aspects are kept unimpaired. Effects on excursion size, duration and steepness of pitch movements are examined for the three types of time pressure. Also, the position within the pitch register in normal and fast speech is compared. A search for 'anchor points' for the pitch rise and pitch fall is carried out, i.e., synchronization points with some aspect of the segmental structure, which remain the same under all conditions. We found that the pitch rise is time compressed under all pressure types. Time compression is strongest for multiple pitch move- ments. In fast speech the pitch movements are raised in register. For the start of the accent lending rise an anchor point is found in the syllable onset. For the fall no anchor points are found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-221"
  },
  "terken92_icslp": {
   "authors": [
    [
     "Jacques",
     "Terken"
    ],
    [
     "Karin van den",
     "Hombergh"
    ]
   ],
   "title": "Judgments of relative prominence for adjacent and non-adjacent accents",
   "original": "i92_0735",
   "page_count": 4,
   "order": 224,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "Accents mark focal, i.e. important information. Published production and perception data show that accents may differ in \"accent strength\". In order to find out whether this information might be used by listeners to determine which information is more focal, an experiment was set up to find out whether listeners can make judgments of relative accent strength equally well for adjacent accents and non-adjacent accents: if they can do so only for adjacent accents, the usefulness of variation in accent strength as a cue to variation in communicative weight would clearly be limited. Judgments about relative accent strength for adjacent and non-adjacent accents were compared both for a real sentence and for reiterant speech. It was found that the task was much easier for the real sentence than for reiterant speech, and that for the real sentence judgments of relative accent strength can be made equally well for adjacent and non-adjacent accents. It is concluded that variation in accent strength might be used to signal differences in communicative weight. In addition, it was concluded that listeners need linguistic structure to support the interpretation of prosodic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-222"
  },
  "beaugendre92_icslp": {
   "authors": [
    [
     "F.",
     "Beaugendre"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Anne",
     "Lacheret-Dujour"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "A perceptual study of French intonation",
   "original": "i92_0739",
   "page_count": 4,
   "order": 225,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "This paper presents an experimental phonetic study of French intonation. It aims at designing a melodic model for a French Text-To-Speech synthesis system. The methodology of intonation research adopted here is in the tradition of a series of studies, for several European languages, developed at the Institute for Perception Research (IPO, Eindhoven, NL). This work is based on a perceptual description of French intonation, where pitch contours are described in terms of simple pitch movements. The problems addressed in the paper are the definition, standardization and syntactic properties of such movements, starting from a corpus of read text. The melodic model for French was defined in 3 stages: 1. F0-contours stylization; 2. pitch movements standardization; 3. study of a grammar of French intonation. At each stage, perceptual tests were performed in order to check the perceptual quality (equality or acceptability) of the synthetic F0 contours. This work was within the framework of the CEC-ESPRIT-POLYGLOT-2104 project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-223"
  },
  "liberman92_icslp": {
   "authors": [
    [
     "Mark",
     "Liberman"
    ],
    [
     "J. Michael",
     "Schultz"
    ],
    [
     "Soonhyun",
     "Hong"
    ],
    [
     "Vincent",
     "Okeke"
    ]
   ],
   "title": "The phonetics of IGBO tone",
   "original": "i92_0743",
   "page_count": 4,
   "order": 226,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "Igbo, a language of the Kwa branch of the Niger-Congo family, is spoken by about 15 million people in southeastern Nigeria. Its phonology, morphology and syntax have been widely studied, especially with reference to the intricate patterning of lexical tone. This paper is a preliminary study of the phonetic interpretation of Igbo tone. We use an experimental method first applied to English ([3, 4]), in which a speaker varies pitch range orthogonally with variation in tonal material, and we compare the success of different models in characterizing the interaction of tone identity, phrasal position, tone sequence, and pitch range in determining patterns of measured F0 values.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-224"
  },
  "shattuckhufnagel92_icslp": {
   "authors": [
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Stress shift as pitch accent placement: within-word early accent placement in american English",
   "original": "i92_0747",
   "page_count": 4,
   "order": 227,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "A theory of pitch accents in English must answer at least three questions: What is the inventory of pitch accent types, what are their acoustic correlates, and where can speakers place them. These questions have not proved easy to answer, partly because speakers differ in where they choose to put pitch accents in a given sentence, listeners differ in where they perceive them, and theories make predictions which seem to differ from both. This paper summarizes a series of recent empirical findings regarding one aspect of the third question: pitch accent placement within words. We reprise the data from an analysis of words that are candidates for early pitch accent placement in a corpus of FM radio news stories (see Ross et al., this volume [17]) and present additional data from critical pairs of sentences read in the laboratory, suggesting that a) early accent placement in a word (i.e. before the main-stress syllable) is influenced by structural factors, such as a tendency to place an accent early in a new prosodic constituent, as well as by rhythmic factors, such as a tendency toward regular placement of pitch accents, b) double accenting of words is not uncommon, particularly when no other words in the phrase are accented, again showing a tendency to accent early in the phrase, and c) the final or nuclear accent in a phrase is almost invariably located on the main-stress syllable of its word. These findings support a model in which speakers can optionally accent nearly any full-vowel syllable before the main-stress syllable of words in prenuclear position, and selections among those options are influenced in part by factors related to both structure and rhythm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-225"
  },
  "morton92_icslp": {
   "authors": [
    [
     "Katherine",
     "Morton"
    ]
   ],
   "title": "Adding emotion to synthetic speech dialogue systems",
   "original": "i92_0675",
   "page_count": 4,
   "order": 228,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "Current synthetic speech systems produce voice output which is unacceptable for general public use. Listeners report it sounds mechanical, machine-like and is not pleasant to listen to. A model is being developed which adds variability to the wave-form; this variability conveys emotion or attitude and produces more natural sounding synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-226"
  },
  "spring92_icslp": {
   "authors": [
    [
     "Cari",
     "Spring"
    ],
    [
     "Donna",
     "Erickson"
    ],
    [
     "Thomas",
     "Call"
    ]
   ],
   "title": "Emotional modalities and intonation in spoken language",
   "original": "i92_0679",
   "page_count": 4,
   "order": 229,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "This study reports on listener perception of contrastive emphasis in natural language utterances varied for emotional modality. Length and F0 correlates of well-perceived corrected digits indicate that length but not a L+H* rising contour cues listeners that the digit is emphasized. Length of vowels poorly perceived for contrastive emphasis but agreed by listeners to be emotional are compared with well-perceived corrected digits. Utterances which listeners agree are emotional are shown to have a high misperception rate for the corrected digit, and those with low listener agreement for emotion have a low misperception rate for the corrected digit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-227"
  },
  "slamacazacu92_icslp": {
   "authors": [
    [
     "Tatiana",
     "Slama-Cazacu"
    ]
   ],
   "title": "Are any \"press-conferences\", \"interviews\" or \"dialogues\" true dialogues?",
   "original": "i92_0683",
   "page_count": 4,
   "order": 230,
   "p1": "683",
   "pn": "686",
   "abstract": [
    "The paper analyses three influential means of vehiculating information from political or economical factors to large social strata, which became much in fashion in some European countries in the last 2-3 years: \"Press-Conferences\" (PC:D1 - addressed by political or economical leaders to populations via mass-media - \"Interviews\"- Is - sometimes called \"Dialogues\" - between a reporter and a leader (D2), and \"Dialogues\" - organized by a reporter to occur between several leaders (D3). The author submitted the corpus to analyses of the basis of the \"Dynamic-Contextual\" (DC) model of the act of communication and of 4 criteria of true dialogue (D). The criteria were not satisfied by these forms of communication (FsC) when delivered in a political key, as compared with the one in literature, art, everyday life key.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-228"
  },
  "bronstein92_icslp": {
   "authors": [
    [
     "Arthur J.",
     "Bronstein"
    ]
   ],
   "title": "The categorization of the dialects and speech styles of north american English",
   "original": "i92_0687",
   "page_count": 4,
   "order": 231,
   "p1": "687",
   "pn": "690",
   "abstract": [
    "North American English pronunciation usage has not developed a particular model for either of the two countries, a situation quite differently demonstrated by the four current pronouncing dictionaries of British English and the sole pronouncing dictionary of American English. American English standard pronunciations do not stem from a tradition associated with a language academy nor with a norm associated with the usages of a social elite or of those from a particular region of the country. Current American and Canadian English 'standard' forms belong to numerous regions as well as to many stylistic and social categories. This paper reports on an ongoing project that is gathering and categorizing such forms for a large database, ultimately to be used for an updated dictionary of American and Canadian English pronunciation. Pronunciations, to be retrievable by category, will, where appropriate, carry particular regional designations (as when certain regionally identifiable vocalic forms in words like barn, furrow, third, four, dance, bath, cot, home, house, roof, etc. appear in different parts of the continent). Additionally, we plan to include and label pronunciations in special phonetic contexts (e.g. \"going to\", \"extra allowance\", \"bitter\"), stylistic entries that carry chronological labels (e.g. \"Maria\", \"humble\"). SDelline, casual, formal, fast-speech pronunciations, and those closely, if not exclusively, associated with certain social groups.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-229"
  },
  "blaauw92_icslp": {
   "authors": [
    [
     "Eleonora",
     "Blaauw"
    ]
   ],
   "title": "Phonetic differences between read and spontaneous speech",
   "original": "i92_0751",
   "page_count": 4,
   "order": 232,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "Our research investigates phonetic differences between read and spontaneous speech. In particular, we are interested in differences that enable listeners to distinguish between the two speech types. In the experiment described below, we test the hypothesis that the stressed syllable in words with a high information load (the accent-bearing syllable) , contains important information for this perceptual distinction. A listening test, employing a gating paradigm, was carried out, with matching spontaneous and read speech material. The results show that accented syllables indeed contain more information than unaccented syllables. Some acoustic correlates of the accents in read and spontaneous speech are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-230"
  },
  "eskenazi92_icslp": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Changing speech styles: strategies in read speech and casual and careful spontaneous speech",
   "original": "i92_0755",
   "page_count": 4,
   "order": 233,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "This paper examines segmental and suprasegmental elements which contribute to an impression of one speaking style as opposed to another. A corpus containing three styles of speech, casual, careful and read, for the same linguistic content was gathered. Examination of global and individual results reveals that: 1) spontaneous styles of speech cannot be considered to be linear modifications of read speech (careful speech is not necessarily faster than read speech, but slower than casual speech, for example), but probably closer to separate types of modifications of casual speech; and 2) the same perceived style is achieved by different speakers in different ways.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-231"
  },
  "umeda92_icslp": {
   "authors": [
    [
     "Noriko",
     "Umeda"
    ],
    [
     "Karen",
     "Wallace"
    ],
    [
     "Josephine",
     "Horna"
    ]
   ],
   "title": "Usage of words and sentence structures in spontaneous versus text material",
   "original": "i92_0759",
   "page_count": 4,
   "order": 234,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "This paper reports striking differences in the usage of words and sentence structures between spontaneously spoken material and written texts in English. The text material consists of twenty texts, containing an average of 700 words each, which are easy to read aloud and use minimal technical vocabulary. The spontaneous material consists of the transcribed conversations of eleven native speakers of English (six males, five females), each of whom participated in two hours of conversation. Using the AT&T UNIX system (Writer's Workbench) parser, all sentences were analyzed in terms of sentence type, verb types, content/function ratio, grammatical classes of words, average word length, part of speech of the sentence beginning, and overall complexity. Material from different texts and speakers was analyzed separately. All speakers show similar patterns with regard to these analysis categories. Different texts, on the other hand, show considerable variation. As a group, texts show higher complexity than the spontaneous speech. The results imply that although speakers' personalities and topics of conversation vary, they use easier words and simpler constructions than carefully written texts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-232"
  },
  "daly92_icslp": {
   "authors": [
    [
     "Nancy A.",
     "Daly"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Statistical and linguistic analyses of F0 in read and spontaneous speech",
   "original": "i92_0763",
   "page_count": 4,
   "order": 235,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "This paper describes our study of prosodic differences between read and spontaneous speech taken from human/machine problem-solving dialogues, as captured by F0. Our research had two goals: first, to show that significant intonational differences exist between speech from these two styles and that these differences can be expressed quantitatively, and second, to demonstrate that the encoding of prosodic information is more salient in spontaneous speech than in read speech. Our analysis of over 4000 read/spontaneous utterance pairs from many speakers indicates that the mean of F0 is statistically significantly higher for spontaneous speech than for read, but that F0 is about equally variable in the two styles. In addition, we found that the encoding of final boundary tone information is more easily obtained from spontaneous speech than from read speech. Over 80% of final boundary tones from spontaneous speech were correctly classified, as opposed to less than 70% of those from read speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-233"
  },
  "shockey92_icslp": {
   "authors": [
    [
     "Linda",
     "Shockey"
    ],
    [
     "Edda",
     "Farnetani"
    ]
   ],
   "title": "Spontaneous speech in English and Italian",
   "original": "i92_0767",
   "page_count": 3,
   "order": 236,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "Electropalatographic and acoustic data were collected from two Italian and two English subjects engaged in spontaneous conversation. A subset of the dental/alveolar consonants were examined and found to show consistently less closure than their equivalents in citation-form speech in both languages. Environments for relative lack of closure differed from language to language. A wide range of degrees of closure were found, indicating that conversational realisations are best described using a continuum rather than discrete categories. We suggest that this can be accomplished using a gestural approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-234"
  },
  "lefebvre92_icslp": {
   "authors": [
    [
     "Claude",
     "Lefebvre"
    ],
    [
     "Dariusz A.",
     "Zwierzyriski"
    ],
    [
     "David R.",
     "Starks"
    ],
    [
     "Gary",
     "Birch"
    ]
   ],
   "title": "Further optimisation of a robust IMELDA speech recogniser for applications with severely degraded speech",
   "original": "i92_0691",
   "page_count": 4,
   "order": 237,
   "p1": "691",
   "pn": "694",
   "abstract": [
    "Research described in a previous paper [1] demonstrated that high accuracy of recognition of degraded speech is possible to achieve with an IMELDA acoustic representation. The present paper extends these findings and reports on new, incremental improvements to the recognition system. An IMELDA transform is derived for each individual user and it preserves the most salient acoustic features, simultaneously minimising the effects of signal degradation. Increasing recognition accuracy to 99% on speech recorded in a helicopter for the tested population of speakers has been possible through the introduction of a new method of deriving a noise threshold and a modified computation of an IMELDA transform. Problems pertinent to the integration of a prototype recogniser into a helicopter, and preliminary results of in-flight recognition tests are described. Finally, a short section deals with issues involved in computing a transform on a personal computer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-235"
  },
  "stern92_icslp": {
   "authors": [
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Fu-Hua",
     "Liu"
    ],
    [
     "Yoshiaki",
     "Ohshima"
    ],
    [
     "Thomas M.",
     "Sullivan"
    ],
    [
     "Alejandro",
     "Acero"
    ]
   ],
   "title": "Multiple approaches to robust speech recognition",
   "original": "i92_0695",
   "page_count": 4,
   "order": 238,
   "p1": "695",
   "pn": "698",
   "abstract": [
    "This paper compares several different approaches to robust speech recognition. We review CMU's ongoing research in the use of acoustical pre-processing to achieve robust speech recognition, including the first evaluation of pre-processing in the context of the DARPA standard AXIS domain for spoken language systems. We also describe and compare the effectiveness of three complementary methods of signal processing for robust speech recognition: acoustical pre-processing, microphone array processing, and the use of physiologically-motivated models of peripheral signal processing. Recognition error rates are presented using these three approaches in isolation and in combination with each other for the speaker-independent continuous alphanumeric census speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-236"
  },
  "kitamura92_icslp": {
   "authors": [
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Satoshi",
     "Ando"
    ],
    [
     "Etsuro",
     "Hayahara"
    ]
   ],
   "title": "Speaker-independent spoken digit recognition in noisy environments using dynamic spectral features and neural networks",
   "original": "i92_0699",
   "page_count": 4,
   "order": 239,
   "p1": "699",
   "pn": "702",
   "abstract": [
    "This paper describes a speaker-independent word recognition method in noisy environments using dynamic and averaged spectral features of speech and neural networks. Spectral features of speech are obtained from a two-dimensional mel-cepstrum (TDMC). TDMC is defined as the two-dimensional Fourier transform of mel-frequency scaled log spectra in the frequency and time domains. In this paper, several regions of dynamic and averaged spectral features of TDMC word are used as training data of neural networks. Neural networks are feed-forward networks with three layers and learn automatically by a back propagation training algorithm. In order to improve the recognition performance in noisy environments, the learning order and SNR of the training data are considered in this study. Experimental results of speaker-independent word recognition for Japanese ten digits show that the proposed method gives better results especially in low SNR environments than a usual method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-237"
  },
  "cairns92_icslp": {
   "authors": [
    [
     "Douglas A.",
     "Cairns"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "ICARUS: an mwave-based real-time speech recognition system in noise and lombard effect",
   "original": "i92_0703",
   "page_count": 4,
   "order": 240,
   "p1": "703",
   "pn": "706",
   "abstract": [
    "Automatic speech recognition by computer must address two issues to perform reliably in actual recognition environments. The first issue is real-time system performance. The second issue is the effect of background noise and/or speaker stress on recognition performance. A considerable effort has been made to develop speech recognition in tranquil environments. However, speech recognition algorithms formulated in tranquil environments generally perform poorly in adverse environments (background noise and/or speaker stress). In this paper, we propose a real-time recognition system called ICARUS which addresses the effect of background noise/speaker stress. The motivation for our stress compensation scheme is discussed with respect to the variation of speech characteristics spoken in noisy environments. Results of the proposed system are given for several speakers. ICARUS represents a first attempt at real-time speech recognition in adverse conditions. Early performance results, though not consistent over all speakers, show improvement in recognition of as much as +7.1 % for noise free Lombard speech, +15.7 % and +7.1 % for Lombard speech corrupted by additive white Gaussian and non-stationary cooling fan noise respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-238"
  },
  "mokbel92_icslp": {
   "authors": [
    [
     "C.",
     "Mokbel"
    ],
    [
     "L.",
     "Barbier"
    ],
    [
     "Y.",
     "Kerlou"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Word recognition in the car: adapting recognizers to new environments",
   "original": "i92_0707",
   "page_count": 4,
   "order": 241,
   "p1": "707",
   "pn": "710",
   "abstract": [
    "Several techniques for the adaptation of DTW and HMM speech recognizers to car environment are presented and tested. These techniques are : nonlinear spectral subtraction, spectral subtraction with neural networks, two strategies of feature parameters transformation using linear regression or neural networks, and finaly two approaches to adapt directly the HMM states distributions. Results given prove the superiority of parameters transformations on spectral subtraction, neural networks transformation on linear regression transformations, and adding noise to references on speech enhancement strategy. They prove also the importance of the choice of the spectral representation and the corresponding distance measure. A number of perspectives exists : defining adaptative transformations, transformations by class of noise, adapting the HMM parameters using NN.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-239"
  },
  "meyer92_icslp": {
   "authors": [
    [
     "P.",
     "Meyer"
    ],
    [
     "Hans-Wilhelm",
     "Rühl"
    ],
    [
     "L. L. M.",
     "Vogten"
    ]
   ],
   "title": "German announcements using synthetic speech the Gauss system",
   "original": "i92_0711",
   "page_count": 4,
   "order": 242,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "In this paper we describe a system that uses keywords produced by a speech synthesizer embedded in carrier sentences of natural speech. This allows to generate unrestricted application-specific information, based on a set of fixed announcements. Since most of the naturalness of the utterances lies in the prosody of the carrier sentence, such a system is highly preferable to pure synthetic announcements.\n",
    "Three methods to insert synthetic keywords into carrier sentences were tested and compared to pure synthetic announcements. In one method the keyword was presented in isolation with pauses to the carrier sentences. For the other methods, we modelled different kinds of coarticulation with the carrier sentence. It was found, that for most cases it is sufficient or even better to present the keywords in isolation. This procedure facilitates the technique of keyword insertion because just a correct pitch contour for the keyword must be created.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-240"
  },
  "jack92_icslp": {
   "authors": [
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "J. C.",
     "Foster"
    ],
    [
     "F. W.",
     "Stentiford"
    ]
   ],
   "title": "Intelligent dialogues in automated telephone services",
   "original": "i92_0715",
   "page_count": 4,
   "order": 243,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "This paper describes research work into the design, implementation and usability evaluation of intelligent dialogues for automated telephone services. The project involves a series of large-scale field experiments using a new real-time Wizard of Oz (WOZ) scheme designed to permit investigation of users' attitudes towards simulated automated telephone services, and the evaluation of users' perceived usability of such services. The project is distinguished from most previous studies by the choice of highly constrained application domains; by the degree of control the WOZ software provides over the experimental variables; by the care being taken to quantitatively measure users' attitudes and perceived usability; and by the large subject population (1000 UK subscribers) available for use in the experiments. The experimental programme itself consists of a series of WOZ experiments in which the characteristics of the speech interface (including an automatic speech recogniser) are manipulated and the dependent variables of user attitude and perceived usability are measured by both a telephone questionnaire and a postal questionnaire. Experimental results with a subject population of 256 subscribers are discussed in the paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-241"
  },
  "nielsen92_icslp": {
   "authors": [
    [
     "Palle Bach",
     "Nielsen"
    ],
    [
     "Anders",
     "Baekgaard"
    ]
   ],
   "title": "Experience with a dialogue description formalism for realistic applications",
   "original": "i92_0719",
   "page_count": 4,
   "order": 244,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "This paper reports work undertaken in the ESPRIT II project 2094 SUNSTAR with the main emphasis on the specification and implementation of the dialogue description formalism. Preliminary results of this work were reported at the ICSLP '90 conference. This paper discusses the further development of the dialogue description formalism and reports the experience of applying it in practical and realistic applications. The experience repotted is mainly based on the implementation of a demonstrator by Jydsk Telefon. The purpose of the demonstrator is to offer abbreviated dialling and alarm call services to subscribers in a limited field trial.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-242"
  },
  "lerner92_icslp": {
   "authors": [
    [
     "Solomon",
     "Lerner"
    ],
    [
     "Baruch",
     "Mazor"
    ]
   ],
   "title": "Compensating for additive-noise in automatic speech recognition",
   "original": "i92_0723",
   "page_count": 4,
   "order": 245,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "Mismatch in the quality and characteristics of speech to be recognized and training speech significantly affect the performance of automatic speech recognition (ASR) systems. Since our applications are over the public switched telephone network (PSTN), we have been investigating the impact of the network on the speech features and recognition and have explored several compensation strategies. To this end, we developed [1] a normalization method to compensate for the effects of spectral shaping (linear filtering) in DTW recognizers. This compensation method, essentially a cepstral subtraction operation, improves recognition performance significantly. In this paper, we extend the work in [1] to address the effects of additive-noise and to modify the cepstral subtraction method for HMM recognizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-243"
  },
  "matsunaga92_icslp": {
   "authors": [
    [
     "Sho-ichi",
     "Matsunaga"
    ],
    [
     "Toshiaki",
     "Tsuboi"
    ],
    [
     "Tomokazu",
     "Yamada"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Continuous speech recognition for medical diagnoses using a character trigram model",
   "original": "i92_0727",
   "page_count": 4,
   "order": 246,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "This paper describes a continuous speech recognition system for medical diagnoses that uses a trigram model based on sequences of Japanese characters. Dictation of medical diagnoses is one of the most promising applications of speech recognition. We devised a prototype based on consonant-vowel spotting using the DP matching technique, as demonstrated at ICSLP'90, and have since improved this system by using a Japanese character trigram model , a phrase syntax and phoneme-based hidden Markov models. Speaker-dependent recognition tests have been done on 543 phrases about X-ray CT scanning. The word lexicon has about 3600 entries. The trigram model reduced the character perplexity from 11.8 to 3.6. Using the new system, 95.8% of the input phrases were correctly transcribed, compared with the 61.5% reported at ICSLP'90. These results show the effectiveness of the character trigram model for continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-244"
  },
  "lu92_icslp": {
   "authors": [
    [
     "Chengxiang",
     "Lu"
    ],
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Hisayoshi",
     "Suzuki"
    ]
   ],
   "title": "A three-dimensional FEM simulation of the effects of the vocal tract shape on the transfer function",
   "original": "i92_0771",
   "page_count": 4,
   "order": 247,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "This paper describes an implementation of three-dimensional finite element method to examine the effects of actual vocal-tract shape on the characteristics of acoustic transmission and radiation.\n",
    "Using a set of finite element models of the vocal tract, we calculated the distributions of sound pressure and transfer responses for vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-245"
  },
  "oshimat92_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Oshimat"
    ],
    [
     "Vincent L.",
     "Gracco"
    ]
   ],
   "title": "Mandibular contributions to speech production",
   "original": "i92_0775",
   "page_count": 4,
   "order": 248,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "In this paper we examine jaw opening and closing action during the production of a variety of vowels and consonants. Of interest was the identification of potential neural control strategies and biomechanical contributions underlying speech production by evaluating, separately and in combination, kinematic and electromyographic (EMG) activity of the jaw at normal and fast speaking rates. We found that EMG and movement characteristics for jaw opening were systematically related and varied according to vowel identity. However, at a fast speaking rate, jaw motion differences were reduced or eliminated for the different vowels, and EMG and movement characteristics for jaw closing were less consistently related and showed fewer consonant-related variations. Inspection of jaw opening and closing movement relations for the different CVCs revealed strong covariation across the movement cycle. In contrast, EMG relations were less related suggesting a substantial biomechanical contribution to jaw closing movement characteristics. Jaw actions were found to be organized around a movement cycle (syllabic unit) suggesting a codependent programming across movement phases (opening/closing). One of the consequences of increased speaking rate was the use of an overall lower than normal peak jaw position for the vowels and consonants and an apparent compensation of the reduced jaw motion by the tongue. Together these results suggests that speech motor control is organized according to vocal tract level goals that span opening and closing actions rather than targets associated with individual articulators.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-246"
  },
  "matsumura92_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ]
   ],
   "title": "Measurement of three-dimensional shapes of vocal tract and nasal cavity using magnetic resonance imaging technique",
   "original": "i92_0779",
   "page_count": 4,
   "order": 249,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "The purpose of this study is to construct a new three-dimensional vocal tract model that consider acoustical interactions between the vocal tract and the nasal cavity for advanced speech processing. To construct the model, three-dimensional shapes of the vocal tract and the nasal cavity of 10 adult males are measured in the steady state production of Japanese vowels using the magnetic resonance imaging technique. Measurement time is 15 second for a mid-sagittal section of the vocal tract, is 184 second for 20 axial sections from the larynx to the nasal cavity at a 6 mm interval, and is 184 second for 20 coronal sections from the tip of the nose to the atlas at a 5 mm interval. A computer algorithm for boundary tracing of the vocal tract and the nasal cavity from the MR images is proposed. The algorithm is based on the threshold operation. Thresholds of the gray level are computed from gray levels at border points of the vocal tract or the nasal cavity. Three-dimensional shapes of the vocal tract and the nasal cavity are obtained from the MR images using the present algorithm. The validity of the present algorithm is confirmed by the comparison the profile obtained by the algorithm with the original MR images.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-247"
  },
  "kiritani92b_icslp": {
   "authors": [
    [
     "Shigeru",
     "Kiritani"
    ],
    [
     "Hajime",
     "Hirose"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Tsutomu",
     "Sato"
    ]
   ],
   "title": "Electromyographie studies on the production of pitch contour in accentless dialects in Japanese",
   "original": "i92_0783",
   "page_count": 4,
   "order": 250,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "Most previous studies on the quantitative analysis of pitch contour in Japanese are concerned either with the Tokyo dialect or with the Kinki dialect. In these dialects, pitch contour is generally characterized as the superposition of a component related to word accent and a component related to the prosodic phrase. However, in Japanese, there are many local dialects which do not have distinctive word accent (accentless dialects). These dialects can be expected to serve as an interesting and valuable target for the study of intonation contours because, first of all, in these dialects, pitch contours corresponding to prosodic phrases are directly manifested as actual pitch contours without any interference from word accent. Furthermore, a recent study by one of the present authors (K. Maekawa 5)) has revealed that many accentless dialects show characteristic pitch contours corresponding to prosodic phrases which are quite different from those observed in the Tokyo or Kinki dialects. In the Tokyo dialect, for example, the basic pattern of the pitch contour for the unit of the prosodic phrase can be characterized as a declination pattern (i.e., the initial pitch rise and the gradual pitch fall toward the end of the utterance). The boundaries of the prosodic phrases in the utterance are generally signaled by a pitch rise at the boundaries. However, accentless dialects often have their own characteristic pitch pattern for prosodic phrases and also for marking prosodic boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-248"
  },
  "sonoda92_icslp": {
   "authors": [
    [
     "Yorinobu",
     "Sonoda"
    ],
    [
     "Kohichi",
     "Ogata"
    ]
   ],
   "title": "Improvements of magnetometer sensing system for monitoring tongue point movements during speech",
   "original": "i92_0843",
   "page_count": 4,
   "order": 251,
   "p1": "843",
   "pn": "846",
   "abstract": [
    "The purpose of this paper is to present design improvements of magnetometer sensing system for monitoring tongue point movements during speech. The measuring principle is similar to that used in earlier prototype system. The system basically consists of two magnetometer sensor units and a small permanent magnetic rod as a pellet which is attached to a desired point of tongue surface. Serious problems with tilting movements of the tongue are encountered when magnetic sensing systems make use of static magnetic fields as well as alternating ones. If each axis of the sensor units and the rod are not mutually parallel, the strength of the signal detected by the sensor decreases, and therefore estimated distances result greater than actual ones. These problems were resolved by a special hardware and software architecture which automatically detects and corrects tilting angles. The measuring principle and its technical realization are described, and preliminary experimental results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-249"
  },
  "alku92_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Inverse filtering of the glottal waveform using the Itakura-saito distortion measure",
   "original": "i92_0847",
   "page_count": 4,
   "order": 252,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "Estimation of the glottal pulseform with inverse filtering is studied in this paper. Automatic computation of the glottal source is usually performed by applying LPC-analysis in order to estimate the vocal tract transfer function. However, the performance of linear prediction decreases as the fundamental frequency of speech increases. Therefore, a new algorithm for computing the vocal tract model is presented. The new method applies the discrete version of the Itakura-Saito distortion measure when the poles of the vocal tract filter are determined. The preliminary results when synthetic acid natural vowels were analysed are discussed shortly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-250"
  },
  "motoki92_icslp": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Nobuhiro",
     "Miki"
    ]
   ],
   "title": "Measurement of intraoral sound pressure distributions of Japanese vowels",
   "original": "i92_0851",
   "page_count": 4,
   "order": 253,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "in this paper the distributions of sound pressure in the oral cavity are studied based on acoustic measurement which was performed by inserting a probe microphone into the oral cavity during steady-state utterance of the Japanese vowels /a/ and /o/. The actual sound pressure levels and the relative distributions of sound pressure on the midsagittal plane for these vowels are presented. By using the results of our previous measurement using a plaster replica of the oral cavity, 2-dimensional sound pressure distribution for /a/, which approximates the measured midsagittal sound pressure curve, is also computed. The computed pressure distribution agrees with the measured data to within 2 dB differences when an average asymptotic decay of 6 dB/oct is given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-251"
  },
  "marchal92_icslp": {
   "authors": [
    [
     "Alain",
     "Marchal"
    ],
    [
     "William J.",
     "Hardcastle"
    ],
    [
     "K.",
     "Nicolaidis"
    ],
    [
     "N.",
     "Nguyen"
    ],
    [
     "F.",
     "Gibbon"
    ]
   ],
   "title": "Non-linear annotation of multi-channel speech data",
   "original": "i92_0787",
   "page_count": 4,
   "order": 254,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "ESPRIT II Basic Research Action ACCOR aims at investigating the articulatory-acoustic correlations in coarticulatory processes in seven European languages (English, French, German, Italian, Irish Gaelic, Catalan and Swedish). In order to allow for cross-language comparisons, it is particularly important that partners in the project adopt a common methodology, i.e. standardised investigation tools and normalised measurement procedures at specific locations in the speech signals. The physiologic and acoustic database first needs to be segmented and labelled according to principles that do not preclude any theoretical interpretation. The conventional phonological approach to identifying segmental boundaries that implicitly considers coarticulation as a phonological feature-spreading process is rejected. On the contrary, a non-linear annotation of the articulatory and acoustic events has been adopted, based on the evidence provided by the different channels of information: acoustic waveform, airflow traces, linguo-palatal contact patterns, jaw and lip movements. We show with reference to the analysis of /kl/ clusters how this truly multi-level approach potentially leads to a better understanding of articulatory-acoustic mapping.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-252"
  },
  "fujiwara92_icslp": {
   "authors": [
    [
     "Shingo",
     "Fujiwara"
    ],
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "A phoneme labelling workbench using HMM and spectrogram reading knowledge",
   "original": "i92_0791",
   "page_count": 4,
   "order": 255,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "This paper proposes a workbench for the phoneme labelling of speech data, that acts as a powerful tool in reducing the effort required to create phoneme labels. The proposed workbench consists of two modules: a user interface module and a phoneme segmentation engine that performs automatic phoneme segmentation. An operator can label speech data interactively by using the window and referring easily to automatic phoneme boundaries. The phoneme segmentation engine is based on the hidden Markov model (HMM) and spectrogram reading knowledge (SRK). The performance of the phoneme segmentation engine was estimated with a 5,240 Japanese word speech database. The segmentation rates of the engine were 96.1%(50ms) and 89.1%(30ms). The quality of phoneme labels produced by operators using the workbench was then estimated. The average error of the created labels was about 6ms, with the standard deviation at about 10ms. The workbench architecture, the user interface and the performance of the phoneme segmentation engine are presented. An implemented workbench is also described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-253"
  },
  "phillips92_icslp": {
   "authors": [
    [
     "Michael",
     "Phillips"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Automatic discovery of acoustic measurements for phonetic classification",
   "original": "i92_0795",
   "page_count": 4,
   "order": 256,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "This paper describes a novel procedure for determining a set of acoustic measurements for use in a segment-based speech recognition system. Rather than fully specifying a set of acoustic measurements, a set of generalized measurements such as the average spectral amplitude and the movement of the spectral prominences, is devised. The free parameters associated with these generalized measurements are determined through an optimization procedure, with the help of a large body of training data. Several controlled phonetic classification experiments are presented to illustrate the reduction in error rate as more complex measurements are incorporated. This technique has been successfully utilized in the development of our summit speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-254"
  },
  "katunobu92_icslp": {
   "authors": [
    [
     "Itou",
     "Katunobu"
    ],
    [
     "Hayamizu",
     "Satoru"
    ],
    [
     "Tanaka",
     "Hozumi"
    ]
   ],
   "title": "Detection of unknown words and automatic estimation of their transcriptions in continuous speech recognition",
   "original": "i92_0799",
   "page_count": 4,
   "order": 257,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "Current continuous speech recognition systems are designed to recognize words within a vocabulary. In order to make speech recognition systems more flexible, convenient and robust, they should be able to process unknown words. This paper introduces a new technique for processing unknown words in a continuous speech recognition system. Two types of processing, one with stochastic language models without any other linguistic knowledge and the other with a dictionary and a grammar, are dynamically controlled to detect and transcribe the unknown words automatically. We tested this method by speaker independent continuous speech recognition experiments using a task with 113 word vocabulary, with bunsetu perplexity 8.2. Preliminary results showed a detection rate for 75% of the unknown words, with a false alarm rate of 11% and a phone recognition rate of 51% for the unknown words detected.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-255"
  },
  "brugnara92b_icslp": {
   "authors": [
    [
     "F.",
     "Brugnara"
    ],
    [
     "D.",
     "Falavigna"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "A HMM-based system for automatic segmentation and labeling of speech",
   "original": "i92_0803",
   "page_count": 4,
   "order": 258,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "This paper describes a system that has been developed to provide either the segmentation of labeled speech or the labeling and segmentation of speech, when only the orthographic transcription is available. The system is conceived to provide a broad correspondence between acoustic and phonetic levels of representation, without requiring a great human effort in the phonetic transcription and segmentation of a speech database. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model(HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DA11PA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, when the system is trained with some manually presegmented material. The size of this material represents an important information: to this purpose, system performance lias been evaluated with respect to this parameter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-256"
  },
  "luk92_icslp": {
   "authors": [
    [
     "Robert W. P.",
     "Luk"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "A modification of the viterbi algorithm for stochastic phonographic transduction",
   "original": "i92_0855",
   "page_count": 4,
   "order": 259,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "This paper describes a modification and a fast implementation of the Viterbi algorithm as used in stochastic phonographic transduction. The Viterbi algorithm has a linear time-complexity with respect to the length of the letter string and cubic complexity if we consider the number of letter-phoneme correspondences to be a variable of the problem size. Since the number of correspondences can be large, processing time is long. If the correspondences are pre-compiled to a finite state automaton to simplify the matching process, then the time complexity is reduced by a large multiplicative factor which is determined empirically. The space complexity is increased linearly with respect to the number of states in the automaton.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-257"
  },
  "bagshaw92_icslp": {
   "authors": [
    [
     "Paul C.",
     "Bagshaw"
    ],
    [
     "Briony J.",
     "Williams"
    ]
   ],
   "title": "Criteria for labelling prosodic aspects of English speech",
   "original": "i92_0859",
   "page_count": 4,
   "order": 260,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "We report a set of labelling criteria which have been developed to label prosodic events in clear, continuous speech, and propose a scheme whereby this information can be transcribed in a machine readable format. We have chosen to annotate prosody in a syllabic domain which is synchronised with a phonemic segmentation. A procedural definition of syllables based on the grouping of phones is presented. The criteria for hand labelling the prominence of each syllable, tone-unit boundaries and the pitch movement associated with each accented syllable, are described. Work to automate this process is presented and experimental results evaluating its performance are included.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-258"
  },
  "gong92b_icslp": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "DTW-based phonetic labeling using explicit phoneme duration constraints",
   "original": "i92_0863",
   "page_count": 4,
   "order": 261,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "We address the problem of automatic phonemic machine labeling of speech database. We are particularly interested in using one hand-labeled utterance of a speaker to automatically label utterances of the same text for a large variety of speakers.\n",
    "Conventional DTW algorithms used in speech recognition may produce one-to-many or many-to-one alignment between reference and test. Since the difference in speaking rate is globally much lower, the two types of alignment results are not realistic.\n",
    "We describe a new DTW algorithm which explicitly incorporates in the best path search procedure constraints of phoneme duration. During the alignment of two utterances, the pathes goes across successive phoneme boundaries, both in reference axis and in the test axis. The basic idea is, for each phoneme-to-phoneme transition, weighting the cumulated distance by the duration difference between reference and test phonemes such that its durational deviation from the reference will penalize the path cost. Conventional DTW algorithm is just a special case of the new algorithm.\n",
    "The results show that: Without phoneme duration constraint, about 11% of machine labeled segments are useless because of abnormal time duration. With phoneme duration constraint this percent drops to 0.05%. At the meantime, due to the improved alignment accuracy we observed phrase recognition error reduction of 5%, on a 33.6 thousand vocabulary task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-259"
  },
  "silverman92_icslp": {
   "authors": [
    [
     "Kim",
     "Silverman"
    ],
    [
     "Mary",
     "Beckman"
    ],
    [
     "John",
     "Pitrelli"
    ],
    [
     "Mori",
     "Ostendorf"
    ],
    [
     "Colin",
     "Wightman"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Janet",
     "Pierrehumbert"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "TOBI: a standard for labeling English prosody",
   "original": "i92_0867",
   "page_count": 4,
   "order": 262,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "An understanding of prosody is critical in basic research in speech and natural language processing, and in the technology for building high quality speech synthesis and spoken language understanding systems. Sufficient understanding and development of computational models require large amounts of prosodically transcribed speech. Unfortunately there is no single standard for prosodic transcription that is analogous to IPA for phonetic segments. To meet this need, a group of researchers with expertise in a variety of approaches to prosodic analysis and speech technology have developed TOBI: an agreed transcription system which builds on much recent progress in prosodic modelling. In a study with twenty transcribers with varied experience using this system and a total of 20,000 decisions, high inter-transcriber reliability was achieved. We report this and other evaluations of TOBI which document the consistency with which it can be used. We propose this system as a standard for prosodic transcription of large speech corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-260"
  },
  "eisen92_icslp": {
   "authors": [
    [
     "Barbara",
     "Eisen"
    ],
    [
     "Hans-Günther",
     "Tillmann"
    ],
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "Consistency of judgements in manual labelling of phonetic segments: the distinction between clear and unclear cases",
   "original": "i92_0871",
   "page_count": 4,
   "order": 263,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "For the the development of industrial speech recognition systems a set of phonemically balanced sentences read by six German speakers has been manually segmented and labelled by a group of well trained transcribers. In addition three subjects re-labelled the set of utterances from one speaker 10-12 months after the first session. This paper presents an analysis of labelling results in terms of intra- and interindividual consistency using Prolog tools for detailed examination of our data. We show that inconsistency in judgements is not equally distributed over the whole range of phonetic categories but rather is concentrated on a smaller subset of critical cases. Moreover, intra- and interindividual agreement between labelled series of utterances turned out to be speaker specific and highly dependent on phonetic context. The consequences of our results for an optimization of manual labelling procedures are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-261"
  },
  "fant92b_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Vocal tract area functions of Swedish vowels and a new three-parameter model",
   "original": "i92_0807",
   "page_count": 4,
   "order": 264,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "Vocal tract area functions of 13 Swedish vowels have been derived from midsagittal tracings of X-ray pictures, supported by a limited tomographic material. With a few exceptions, e.g. F2 of [u:], [o] and [o], calculated formant frequencies show a substantial agreement with data measured during the X-ray session. The sensitivity of formant frequencies to variations in vocal tract area functions have been studied.\n",
    "Observed covariation of overall vocal tract dimensions revealed a number of dependent relationships that enable a prediction of overall length, inter-incisor distance, and assymmetries of cavity shapes from the basic specification of location and area of tongue constriction and the degree of liprounding. The new model thus preserves physiological constraints that make it better suited for a future adaption to consonantal modifications than earlier three-parameter models. Nomograms of formant frequencies for systematically varied model parameters are shown. Problems related to inverse mapping, from formant frequencies to model parameters, are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-262"
  },
  "junqua92_icslp": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Acoustic and production pilot studies of speech vowels produced in noise",
   "original": "i92_0811",
   "page_count": 4,
   "order": 265,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "We conducted two pilot studies: 1) to evaluate the influence of speech loudness on acoustic parameters, and 2) to analyze inter-articulatory relationships in vowel production in noisy and non-noisy conditions. The acoustic study revealed that the tense and lax vowel quadrangles (for the male and female speakers) tend to shift upward with increased vocal loudness (especially for the lax vowels). The production study brought to light that: 1) the type of noise influences speech production, and 2) the modification of speech production due to background noise is speaker-dependent and context-dependent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-263"
  },
  "laprie92_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Marie-Odile",
     "Berger"
    ]
   ],
   "title": "Active models for regularizing formant trajectories",
   "original": "i92_0815",
   "page_count": 4,
   "order": 266,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "Formant tracking is one of the most important issues in speech recognition. Nevertheless, most algorithms are based on local methods and take into account global formant properties with difficulty. We therefore propose a formant tracking algorithm which provides a global point of view on formant tracking. The salient idea is to combine local tracking to generate elementary formant tracking hypotheses and an active method to regularize global formant trajectories in the following way: the formant trajectory is the closest curve to that of the hypothesis maximizing energy incorporated by the formant and which is sufficiently smooth. The main qualities of this algorithm are robustness and ability to detect accurate and regular formant trajectories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-264"
  },
  "carre92_icslp": {
   "authors": [
    [
     "Rene",
     "Carré"
    ],
    [
     "Samir",
     "Chennoukh"
    ],
    [
     "Mohamad",
     "Mrayati"
    ]
   ],
   "title": "Vowel-consonant-vowel transitions: analysis, modeling, and synthesis",
   "original": "i92_0819",
   "page_count": 4,
   "order": 267,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "Male vowel-consonant-vowel formant transitions are analyzed using cepstral techniques. The first three formants are detected and represented as trajectories in the F1-F2 plane. Distinctive formant trajectories are clearly revealed, exhibiting specific trajectory angles at the onset during the initial vowel, during the consonant production, and at the offset during the target vowel. Formant stabilities are also studied. These trajectories are interpreted using a \"distinctive region\" model (Mrayati, Carre & Guerin, 1988). The acoustic tube is divided longitudinally into regions, the boundaries of which are deduced from the zero-crossings of the sensitivity functions of the uniform tube. The lengths of the regions are equal to a fixed percentage of the \"effective\" length of the tube. Thus, the command strategy for controlling the model is simple: transversal at specific places. Formant trajectories are generally monotonic for area variations between 1 and 16 cm2, and pseudo-orthogonal behaviour is obtained, i.e., for each of the regions, an increase or a decrease in its cross-sectional area leads to well-defined formant transitions for the formants that are considered. V1CV2 trajectories predicted by the model correspond to those observed on natural speech. The timing control of the model is discussed. A simple synthesis-by-rule technique is deduced using the distinctive region model as a co-articulation model. Good synthesis of V1CV2 sequences is obtained from such a technique. Theoretical consequences on the speech production mechanism are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-265"
  },
  "stone92_icslp": {
   "authors": [
    [
     "Maureen",
     "Stone"
    ],
    [
     "Subhash",
     "Lele"
    ]
   ],
   "title": "Representing the tongue surface with curve fits",
   "original": "i92_0875",
   "page_count": 4,
   "order": 268,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "The present study sought to characterize 3D tongue shape for the phones [s], [/], [I] and [a], and to detail C to V movement. The study examined multiple cross-sectional tongue profiles extracted from ultrasound images. The profiles were fit with second order functions and sufficiently characterized in 2D by the a0 and a2 terms, ie, midsagittal tongue height and concavity/convexity. Results provided \"shape signatures\" for the individual phones that were biologically transparent and reduced tongue shape complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-266"
  },
  "harris92_icslp": {
   "authors": [
    [
     "Katherine S.",
     "Harris"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Peter J.",
     "Alfonso"
    ]
   ],
   "title": "Muscle forces in vowel vocal tract formation",
   "original": "i92_0879",
   "page_count": 3,
   "order": 269,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "This experiment is an extension of earlier work, [1,2] aimed at understanding the relationship of the principal muscle forces underlying vowel tongue shape. The experiment was done in two parts. A single male talker, TB, was recorded for both speech samples. Stimuli for both parts of the experiment were multiple tokens of disyllables of the form / pVp/, with the vowels /EE, IH, AY, EH, AE, A, AW, OOH, UW/. For Part One of the experiment, electromyographic recordings were made from the principal extrinsic muscles of the tongue using hooked wire electrodes. For Part Two, x-ray microbeam recordings of tongue body, lip, and jaw were made on the same subject for the same inventory. In general, tongue X and Y position correlated for front pellet and front vowels, but the situation is more complicated for rear pellets. While the correlations of extrinsic tongue muscle activity with tongue pellet positions were as one might expect, no vowel in the set can be considered as a lax equivalent of any other, as Wood's [3] model suggests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-267"
  },
  "hirayama92_icslp": {
   "authors": [
    [
     "Makoto",
     "Hirayama"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Mitsuo",
     "Kawato"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Neural network modeling of speech motor control",
   "original": "i92_0883",
   "page_count": 4,
   "order": 270,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "An artificial neural network approach to modeling speech production is presented. The model uses simultaneously recorded data for neuromuscular activity (EMG), articulator motion, and the speech acoustics and based on dynamic optimization principle based on forward dynamics of articulators. The experimental configuration and data processing are explained, then the simulation result of forward dynamics modeling, which is the essential part of our modeling approach, is presented. Preliminary result of the forward acoustic modeling, which learns the correlation between articulator trajectories for the lips & jaw and PARCOR parameters of acoustic waveform, is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-268"
  },
  "vatikiotisbateson92_icslp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Makoto",
     "Hirayama"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Mitsuo",
     "Kawato"
    ]
   ],
   "title": "The articulatory dynamics of running speech: gestures from phonemes?",
   "original": "i92_0887",
   "page_count": 4,
   "order": 271,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "As demonstrated by Hirayama, V.-Bateson, Kawato, & Honda [1], we are attempting to model speech production by means of neural networks. At one stage, a 3-layer perceptron learns the dynamics relating muscle activity and articulator motion and, at a later stage, another perceptron learns the PARCOR parameters relating the effect of articulator motion on vocal tract shape and the speech acoustics. After learning, motor commands to the musculo-skeletal system are generated through time by performing the trajectory formation and inverse dynamics using a cascade neural network, parametrized by the via point and smoothness constraints imposed by the phoneme input string and global performance factors, such as speaking rate and speaking style. Articulator trajectories are then generated which serve as input to the PARCOR synthesizer that produces the speech acoustics. Although this effort is still in its infancy, it is proving to be a fairly successful piece of engineering and we think a discussion of the attendant speech science and motor control issues is warranted. Therefore, in this paper, we discuss our modeling effort in terms of well-known problems common to both computational modeling and speech motor control, such as excess degrees-of-freedom in the mapping between different levels, coordinate transformation between articulator and task space variables, and extrinsic versus intrinsic timing. Problems of less common concern, such as biological and cognitive plausibility, are also considered. For the most part, we focus on issues leading up to the generation of articulator motion and leave discussion of the articulatory-to-acoustic transform to a later date.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-269"
  },
  "keating92_icslp": {
   "authors": [
    [
     "Patricia",
     "Keating"
    ],
    [
     "B.",
     "Blankenship"
    ],
    [
     "D.",
     "Byrd"
    ],
    [
     "E.",
     "Flemming"
    ],
    [
     "Y.",
     "Todaka"
    ]
   ],
   "title": "Phonetic analyses of the TIMIT corpus of american English",
   "original": "i92_0823",
   "page_count": 4,
   "order": 272,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "This paper reports a set of studies of some phonetic characteristics of the American English represented in the TIMIT speech database. First we describe generally how we use the non-speech files on the TIMIT CD with a commercial database program. Some studies of pronunciation variation using only the segmental transcriptions and durations of TIMIT are then described. Results of such studies should be useful not only for linguistic phonetics but also for speech recognition lexicons and text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-270"
  },
  "byrd92_icslp": {
   "authors": [
    [
     "Dani",
     "Byrd"
    ]
   ],
   "title": "Sex, dialects, and reduction",
   "original": "i92_0827",
   "page_count": 4,
   "order": 273,
   "p1": "827",
   "pn": "830",
   "abstract": [
    "A set of phonetic studies based on analysis of the TIMIT speech database is presented which addresses topics relevant to the linguistic and speech recognition communities. Using a database methodological approach, these studies detail new results on the effect of speakers' sex and dialect region on pronunciation.1 This report concerns speaker-dependent effects on certain phonetic characteristics of speech often involved in reduction such as speech rate, stop releases, flapping, central vowels, non-canonical phonation type, syllabic consonants, and palatalization processes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-271"
  },
  "ohala92_icslp": {
   "authors": [
    [
     "Manjari",
     "Ohala"
    ],
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Phonetic universals and hindi segment duration",
   "original": "i92_0831",
   "page_count": 4,
   "order": 274,
   "p1": "831",
   "pn": "834",
   "abstract": [
    "We report initial results of an acoustic phonetic study of Hindi designed to further text-to-speech synthesis of the language. We explore the usefulness of prior work on phonetic universals in predicting sub-phonemic phonetic variation. We conclude that phonetic universals cannot always predict such variation but it does serve to direct the search for it.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-272"
  },
  "erickson92_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Osamu",
     "Fujimura"
    ]
   ],
   "title": "Acoustic and articulatory correlates of contrastive emphasis in repeated corrections",
   "original": "i92_0835",
   "page_count": 3,
   "order": 275,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "Using simulated telephone dialogues in which the subject makes repeated corrections of a street address digit sequence, articulatory and intonational patterns are observed for four native speakers of American English. The articulatory patterns are recorded by the computer-controlled x-ray microbeam system at the University of Wisconsin. Effects of the contrastive emphasis, which are assumed to be associated with the digit to be corrected, are examined. Effects of emotional involvement of the subject are observed in a situation where the experimenter, as the dialogue partner, persistently cites an incorrect digit sequence, asking for confirmation/correction by the subject.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-273"
  },
  "tajchman92b_icslp": {
   "authors": [
    [
     "Gary N.",
     "Tajchman"
    ],
    [
     "Marcia A.",
     "Bush"
    ]
   ],
   "title": "Effects of context and redundancy in the perception of naturally produced English vowels",
   "original": "i92_0839",
   "page_count": 4,
   "order": 276,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "Here we report the result of series of perceptual categorization experiments with the TIMIT Acoustic Phonetic Speech Corpus. The goal of these experiments was to identify primary acoustic and linguistic factors effecting the perception of naturally produced vowels. Over a set of three experiments, vowel tokens were presented to listeners in the following five different types of context: (1) no context at all; (2) the vowel in CVC context; (3) the vowel in an 'stripped'-syllable context; (4) the vowel in one-syllable-word context; and (5) the vowel in two-syllable-word context. A primary result is that identification performance improved with greater amounts of context (ie. 66.3% no-context, 79.4% CVC, and 81.5% word for the one-syllable word tokens.) These results provide evidence in support of non-segmental theories of vowel representation. Also, an interaction in identification accuracy over the three experiments suggests differences in linguistic redundancy across the token sets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-274"
  },
  "cole92_icslp": {
   "authors": [
    [
     "Ronald",
     "Cole"
    ],
    [
     "Krist",
     "Roginski"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "A telephone speech database of spelled and spoken names",
   "original": "i92_0891",
   "page_count": 3,
   "order": 277,
   "p1": "891",
   "pn": "894",
   "abstract": [
    "This report describes a telephone speech corpus collected at the Oregon Graduate Institute's Center for Spoken Language Understanding. Over four thousand people called in response to public requests. They were prompted by a recorded voice to say and spell their first and last names-with and without pauses, to say what city they grew up in and what city they were calling from, and to answer two yes/no questions. In order to collect sufficient instances of each letter, about 1000 callers also recited the alphabet. Each call is checked and transcribed by two people. In addition, a subset of the calls is being phonetically labeled.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-275"
  },
  "muthusamy92_icslp": {
   "authors": [
    [
     "Yeshwant K.",
     "Muthusamy"
    ],
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Beatrice T.",
     "Oshika"
    ]
   ],
   "title": "The OGI multi-language telephone speech corpus",
   "original": "i92_0895",
   "page_count": 4,
   "order": 278,
   "p1": "895",
   "pn": "898",
   "abstract": [
    "The OGI Multi-language Telephone Speech Corpus is designed to support research on automatic language identification and multi-language speech recognition. The corpus consists of up to nine separate responses from each caller, ranging from single words to short topic-specific descriptions to 60 seconds of unconstrained spontaneous speech. The utterances were spoken over commercial telephone lines by speakers of English, Farsi (Persian), French, German, Japanese, Korean, Mandarin Chi- nese, Spanish, Tamil, and Vietnamese. We have completed the initial phase of our data acquisition effort: the recording and initial verification of utterances produced by 100 different speakers in each of the 10 languages. We describe the recording protocol, data collection procedure, ongoing corpus development, prelim- inary results of the statistical evaluation of the 10 languages, and plans to provide orthographic transcriptions of the speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-276"
  },
  "paul92_icslp": {
   "authors": [
    [
     "Douglas B.",
     "Paul"
    ],
    [
     "Janet M.",
     "Baker"
    ]
   ],
   "title": "The design for the wall street journal-based CSR corpus",
   "original": "i92_0899",
   "page_count": 4,
   "order": 279,
   "p1": "899",
   "pn": "902",
   "abstract": [
    "The DARPA Spoken Language System (SLS) community has designed, implemented, and globally distributed significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby supplying a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data de-, sign, text processing steps, lexicons, and testing paradigms to be incorporated into the multi-faceted WSJ CSR Corpus. As of this writing, only the WSJ-pilot or Phase-one corpus (~80 hrs.) has been implemented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-277"
  },
  "hirschman92_icslp": {
   "authors": [
    [
     "Lynette",
     "Hirschman"
    ]
   ],
   "title": "Multi-site data collection for a spoken language corpus - MAD COW",
   "original": "i92_0903",
   "page_count": 4,
   "order": 280,
   "p1": "903",
   "pn": "906",
   "abstract": [
    "This paper describes the multi-site spoken language data collection procedure for the ATIS (Air Travel Information System) domain, which has been co-ordinated by MADCOW (Multi-site ATIS Data Collection Working group). We summarize the motivation for this effort, the implementation of the multi-site data collection paradigm, and the accomplishments of MADCOW in monitoring the collection and distribution of 14,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech, natural language and spoken language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-278"
  },
  "phillips92b_icslp": {
   "authors": [
    [
     "Michael",
     "Phillips"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Collection and analyses of WSJ-CSR corpus at MIT",
   "original": "i92_0907",
   "page_count": 4,
   "order": 281,
   "p1": "907",
   "pn": "910",
   "abstract": [
    "Recently, the DARPA community in the United States started a new data collection initiative in the Wall Street Journal (WSJ) domain to support research and development of very large vocabulary continuous speech recognition (CSR) systems. Since August 1991, our group has actively participated in the development of the WSJ-CSR corpus. The purpose of this paper is to document our involvement in this process, from recording and transcription to analyses and distribution. We will also present the results of an experiment investigating the preprocessing of the prompt text.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-279"
  },
  "abrash92_icslp": {
   "authors": [
    [
     "Victor",
     "Abrash"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Michael",
     "Cohen"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Yochai",
     "Konig"
    ]
   ],
   "title": "Connectionist gender adaptation in a hybrid neural network / hidden Markov model speech recognition system",
   "original": "i92_0911",
   "page_count": 4,
   "order": 282,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "An approach to modeling long-term consistencies in a speech signal within the framework of a hybrid Hidden Markov Model (HMM) / Multilayer Perception (MLP) speaker-independent continuous-speech recognition system is presented. Several ways to model male and female speech more accurately with separate models are discussed, one of which is investigated in depth. A method which combines gender-independent and -dependent MLP training is demonstrated, improving recognition accuracy while retaining robustness. A series of network architectures (using our training method) for the connectionist estimation of gender-dependent HMM observation probabilities are evaluated in terms of recognition performance and number of additional parameters needed. Experimental evalutation shows a significant improvement in word recognition accuracy over the gender-independent system with a moderate increase in the number of parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-280"
  },
  "cohen92_icslp": {
   "authors": [
    [
     "Michael",
     "Cohen"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "David",
     "Rumelhart"
    ],
    [
     "Victor",
     "Abrash"
    ]
   ],
   "title": "Hybrid neural network/hidden Markov model continuous-speech recognition",
   "original": "i92_0915",
   "page_count": 4,
   "order": 283,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "In this paper we present a hybrid multilayer perceptron (MLP)/hidden Markov model (HMM) speaker-independent continuous-speech recognition system, in which the advantages of both approaches are combined by using MLPs to estimate the state-dependent observation probabilities of an HMM. New MLP architectures and training procedures are presented which allow the modeling of multiple distributions for phonetic classes and context-dependent phonetic classes. Comparisons with a pure HMM system illustrate advantages of the hybrid approach both in terms of recognition accuracy and number of parameters required.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-281"
  },
  "fink92_icslp": {
   "authors": [
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ],
    [
     "Ernst-Günter",
     "Schukat-Talamazzini"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Semantic hidden Markov networks",
   "original": "i92_0919",
   "page_count": 4,
   "order": 284,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "Although much effort has been put into speech understanding systems there still exists a rather wide gap between acoustic recognition and linguistic interpretation. We propose a formalism for an extremely close interaction of acoustic recognition and higher level analysis. Instead of a strict horizontal interface at the level of hypothesized word sequences or lattices, a vertical interface to the acoustic component is used that can be accessed from linguistic concepts of any degree of abstraction. As the linguistic knowledge is represented in the formalism of Semantic Networks and acoustic recognition is based on Hidden Markov Models the close interaction between the two components was termed Semantic Hidden Markov Networks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-282"
  },
  "lehiste92_icslp": {
   "authors": [
    [
     "Use",
     "Lehiste"
    ],
    [
     "Donna",
     "Erickson"
    ]
   ],
   "title": "Hesitation sounds: is there coarticulation across pause?",
   "original": "i92_0923",
   "page_count": 4,
   "order": 285,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "The paper analyzes hesitation sounds (\"shwas\") occurring in free conversation. Shwa has been described as a featureless vowel with no articulatory target of its own. However, most of the studies involving shwa have considered shwas occurring in unstressed syllables within a word or similar close-knit unit. The present paper analyzes occurrences of shwas as hesitation sounds in unrehearsed conversation. The results of the acoustic analysis indicate that shwas occurring as hesitation sounds resemble the sounds [/\\] or [a] found in stressed monosyllables produced by the same speakers, and thus cannot properly be called targetless. There appears to be no coarticulation between hesitation shwas and preceding or following sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-283"
  },
  "bickley92_icslp": {
   "authors": [
    [
     "Corine A.",
     "Bickley"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ]
   ],
   "title": "Acoustic analysis of laughter",
   "original": "i92_0927",
   "page_count": 4,
   "order": 286,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "Spontaneous laughter produced by two subjects was analyzed and compared to speech, measurements of both temporal and spectral characteristics being made. Speech and laughter were found to be quite similar in \"syllable\" duration and in the number of syllables per second. However, timing within syllables differed markedly. Fundamental frequency and rms amplitude of the laughter were also rather speech-like, although some extremes were observed. Formant structure of the laughter was also similar to speech. Bandpass filtering in the region of the third formant, however, showed the vocalic portions to include significant amounts of noise and breathiness, implying a more abducted vocal-fold configuration in the vocalic portions for laughter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-284"
  },
  "oshaughnessy92_icslp": {
   "authors": [
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Analysis of false starts in spontaneous speech",
   "original": "i92_0931",
   "page_count": 4,
   "order": 287,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "A primary difference between spontaneous speech and read speech concerns the use of false starts, where a speaker interrupts the flow of speech to restart his utterance. The acoustic aspects of such restarts in a widely-used speech database were examined here. Identifying the type of restart in such cases could improve the performance of an automatic speech recognizer, by eliminating from consideration some hypotheses based on spectral analysis. Results are given here which could allow simple identification of most restarts and their type.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-285"
  },
  "lickley92_icslp": {
   "authors": [
    [
     "Robin J.",
     "Lickley"
    ],
    [
     "Ellen G.",
     "Bard"
    ]
   ],
   "title": "Processing disfluent speech: recognising disfluency before lexical access",
   "original": "i92_0935",
   "page_count": 4,
   "order": 288,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "As work on speech understanding moves towards the study of spontaneous rather than carefully prepared read speech, the problems posed by disfluency need to be addressed. The first problem for the processor is to detect that a disfluency has occurred. Previous experiments [11] have shown that listeners are usually able to detect disfluency within one word of the interruption. This paper presents results of a further experiment winch looks more closely at recognition points of disfluency and of the following word. It is found that listeners are able to detect that disfluency has occurred soon after the onset of the following word and prior to recognition of the word itself. Taken together with the results of an experiment with low-pass filtered speech [12], the results suggest that prosodic information may play a key role in the processing of disfluent speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-286"
  },
  "morin92_icslp": {
   "authors": [
    [
     "Philippe",
     "Morin"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "A flexible multimodal dialogue architecture independent of the application",
   "original": "i92_0939",
   "page_count": 4,
   "order": 289,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "In this paper, we present the general architecture of the multimodal dialogue system PARTNER which is designed for goal-oriented applications using speech input and output. Organized around three major components: an I/O manager; a Dialogue manager, and an Application manager, its architecture permits easy implementation of new applications, and new modalities as well as different languages. A common meaning representation language is used as a vehicle for internal communications. Our objective is to design and implement a Man-Machine dialogue interface that is generic and efficient while emphasizing its human-likeness. Currently, the proposed architecture is being validated for several real-world applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-287"
  },
  "cucchiarini92_icslp": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Renee van",
     "Bezooijen"
    ]
   ],
   "title": "Familiarity with the language transcribed and context as determinants of intratranscriber agreement",
   "original": "i92_0987",
   "page_count": 4,
   "order": 290,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "This paper discusses some of the findings obtained in a study on transcription variation. The aim of this study was to determine to what extent segmental transcriptions are influenced by the following factors: familiarity with the language transcribed, context, and speech style. Speech material in which these factors were systematically varied was transcribed by a number of subjects on different occasions. The work presented here concerns the amount of variation observed within transcribers. Mean distance values between transcriptions, which represent the inverse of intratranscriber agreement, were calculated separately for consonants and vowels by means of a transcription alignment algorithm. The data obtained were subsequently submitted to analyses of variance. In discussing the results attention is focussed on the effects of the factors 'familiarity with the language transcribed' and 'context' and the interaction between them.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-288"
  },
  "shriberg92_icslp": {
   "authors": [
    [
     "Elizabeth E.",
     "Shriberg"
    ],
    [
     "Robin J.",
     "Lickley"
    ]
   ],
   "title": "Intonation of clause-internal filled pauses",
   "original": "i92_0991",
   "page_count": 4,
   "order": 291,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "Clause-internal filled pauses and preceding peak F0 values for American and British English speakers were analyzed to determine whether the intonation of filled pauses is relative to, or independent of, prior prosodic context. Higher peaks were found to be systematically associated with higher filled-pause values within speakers, supporting the \"relative\" hypothesis. In modeling this relationship it was found that a linear model, in which filled-pause F0 was expressed as an invariant (over speakers) proportion of the distance between peak and baseline, produced results nearly identical to those of a two-parameter model in which the coefficients of peak and baseline were allowed to vary freely. Analyses of additional variables showed the model to be less appropriate for filled pauses after sentence-initial peaks, but unaffected by temporal variables\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-289"
  },
  "wade92_icslp": {
   "authors": [
    [
     "Elizabeth",
     "Wade"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Patti",
     "Price"
    ]
   ],
   "title": "User behaviors affecting speech recognition",
   "original": "i92_0995",
   "page_count": 4,
   "order": 292,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "We attempt to explain a decrease in recognition word error rate observed when users interacted over time with a spoken language system. We found no change in the language used (as measured by sentence perplexity), and only a small decrease in the number of out-of-vocabulary words. However, a behavior adversely affecting recognition, hyperarticulation, decreased over time. In addition, the acoustic match of hyper-articulated utterances to the system models also improved over time. We conclude that improvement in recognition was due to changes in speech rather than in language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-290"
  },
  "manuel92_icslp": {
   "authors": [
    [
     "Sharon Y.",
     "Manuel"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ],
    [
     "Marie K.",
     "Huffman"
    ],
    [
     "Kenneth N.",
     "Stevens"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ]
   ],
   "title": "Studies of vowel and consonant reduction",
   "original": "i92_0943",
   "page_count": 4,
   "order": 293,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "In normal (casual) speaking modes, speakers often modify, or seemingly delete, segments that are produced in citation forms of the same words. This paper discusses three examples of how attention to the acoustic detail of spoken language can reveal aspects of the articulation which are perhaps not readily apparent in more cursory examinations of the speech signal. A lexical access model which is sensitive to such acoustic detail will find a better match between both normal and citation spoken forms and their shared abstract representation, than one which is not.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-291"
  },
  "umeda92b_icslp": {
   "authors": [
    [
     "Noriko",
     "Umeda"
    ]
   ],
   "title": "Formant frequencies of vowels in English function words",
   "original": "i92_0947",
   "page_count": 4,
   "order": 294,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "This paper investigates the context-dependent variability demonstrated by function words. Thirty different monosyllabic function words were selected from a twenty-minute text reading by one speaker. Traces of Fl, F2 and F3 were obtained for the vowels in multiple occurrences of these words, covering as many phonemic, grammatical, and positional conditions as were available. Formant behavior was examined in terms of various factors, and results were contrasted with previous findings for stressed vowels from content words. Fl's variability is fairly predictable from vowel identity; F3 varies within a small range and is relatively high; the pattern for F2 is similar to that of corresponding stressed vowels, though its range is extremely wide (particularly for /i/ vowels). In phrase-initial position, especially when the function word begins with a vowel, Fl and F2 frequencies approach canonical values at vowel onset. A centralizing effect is seen with F2 in phrase-medial position, and with both Fl and F2 in phrase-final position. The effect of a consonant environment on F2 depends on a complex interplay of factors: the consonant's identity, its position (preceding or following) relative to the vowel, whether a phrase boundary intervenes, and the syllabic make-up of the function word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-292"
  },
  "benoit92_icslp": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ],
    [
     "Tayeb",
     "Mohamadi"
    ]
   ],
   "title": "The lip benefit: auditory and visual intelligibility of French speech in noise",
   "original": "i92_0951",
   "page_count": 4,
   "order": 295,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "Bimodal perception allows a better understanding of speech than audition alone. In this paper, we quantify the intelligibility gain from presenting the speaker's face along with the auditory stimuli, as a function of distortion by added white noise. Eighteen French subjects with good audition and vision were given a closed choice identification test of three vowels [i, a, y] and six consonants [b, v, z, 3, r, 1] under auditory alone and audiovisual presentation conditions. Mean identification scores first give us a measurement of the global improvement provided by bimodal perception: the audio alone identification score decreases from 72% to 8% when S/N decreases from -6 dB to -18 dB, while the audiovisual identification score only decreases from 93% to 77%. Next, the comparison of confusion matrices allows us to discuss the respective effects of each vowel: in audio alone condition of perception, [a] is more intelligible than [i], which is in turn more intelligible than [y], especially under highly degraded acoustic conditions; in bimodal perception, and under highly degraded acoustic conditions (S/N < -18 dB), [yj is more easily identified than [a], which is in turn more easily identified than [i]. Finally, we quantify the importance of the contextual effect of the three vowels on the auditory and the audio-visual intelligibility of the six consonants: [y] distorts both the audio alone and the audiovisual intelligibility of surrounding consonants, [a] improves both, [i] is the vocalic context which best facilitates the visual identification of its surrounding consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-293"
  },
  "oster92_icslp": {
   "authors": [
    [
     "Anne-Marie",
     "Öster"
    ]
   ],
   "title": "Phonological assessment of deaf children's productive knowledge as a basis for speech-training",
   "original": "i92_0955",
   "page_count": 4,
   "order": 296,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "Prelingually deaf children unavoidably make deviations in production although they possess an abstract phonological system. The systems differ from those of normal speakers due to the fact that the phonological systems of deaf children are built up through vision, tactilation and maybe some residual hearing. Traditionally, assessments of deaf speech describe what a deaf child is not capable of articulating through conventional error-analyses. Hence, only speech sounds that the child never articulates correctly are treated in the speech clinic. The sounds that the child articulates correctly are disregarded and also taken for granted to be used correctly. However, it has been shown that even if a child knows how to articulate a sound correctly this does not imply that the usage is correct in his spoken language. Furthermore, many of the articulatory deviations made by deaf children are in fact attempts to realize phonological contrasts. Hence, a conventional error-analysis provides no information about a deaf child's usage of his or her productive knowledge and pays no attention to whether a deviant articulation might signal a \"correct\" contrast. It is more appropriate to base a speech-training programme on a phonological assessment that determines what the speech does express.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-294"
  },
  "seki92_icslp": {
   "authors": [
    [
     "Hideaki",
     "Seki"
    ],
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Takehiko",
     "Harada"
    ],
    [
     "Hiroshi",
     "Hosoi"
    ]
   ],
   "title": "Factors affecting voicing distinction of stops for the hearing impaired",
   "original": "i92_0959",
   "page_count": 4,
   "order": 297,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "In order to develop speech enhancement methods for the hearing impaired, acoustical and auditory factors affecting on the voicing distinction for stop consonants in V1CV2 contexts were examined. The effects of the level of V1 and the silent interval between Vt and C on the voicing distinction for stops in V1CV2 contexts were examined in Experiment I. And, the effect of the fundamental frequency on the voicing distinction for stops in V1CV2 contexts was examined in Experiment II. The following results were obtained. 1) The identification rate for the voiced stop Ibl decreased when the level of V1 decreased when the VOT was fixed at 30ms. 2) The identification rate for the voiced stop /b/ decreased when the silent interval between V1 and C increased when the VOT was fixed at 30ms. 3) Sensori-neural hearing- impaired subjects needed a longer silent interval compared to normal subjects to identify /p/. 4) Normal subjects needed a longer silent interval to identify /p/ when the presentation level was lower. 5) When the lower the F0 was, the longer the dt needed to detect unvoiced stops.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-295"
  },
  "boothroyd92_icslp": {
   "authors": [
    [
     "Arthur",
     "Boothroyd"
    ],
    [
     "Robin S.",
     "Waldstein"
    ],
    [
     "Eddy",
     "Yeung"
    ]
   ],
   "title": "Investigations into the auditory F0 speechreading enhancement effect using a sinusoidal replica of the F0 contour",
   "original": "i92_0963",
   "page_count": 4,
   "order": 298,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "Constant-amplitude sinusoidal replicas of the voice fundamental frequency (Fo) contours of video-recorded sentences were prepared off-line by a combination of automatic and manual Fo estimation. The replicas were then re-synchronized with the video portions of the original recording. In 12 normally hearing adults, the replicas were found to be as effective a supplement to speechreading as the low-pass filtered output of an electroglottograph. Both signals increased the number of words recognized in sentences of known topic by almost 40 percentage points. Using a constant-frequency version of the replicas, it was found that about 1/3 of the Fo speechreading enhancement effect could be accounted for by voicing detection alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-296"
  },
  "cutugno92_icslp": {
   "authors": [
    [
     "Francesco",
     "Cutugno"
    ]
   ],
   "title": "Some considerations on pitch and timing control in deaf children",
   "original": "i92_0967",
   "page_count": 4,
   "order": 299,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "Spontaneus speech (approx.10 hours) produced by ten deaf children (5 males and 5 females, mean age 5 years old, with severe sensory-neural hearing impairment) was recorded during rehabilitation sessions and some selected parts were analyzed by means of a KAY 5500 DSP-Sonagraph. Many phonetic measurements were performed on this speech material (mean vowel formant values, forma/it transition patterns, power spectrum of stationary unvoiced consonants, prosodic parameters), but particular attention was paid to pitch and timing control. All measurements were compared with an analogous set of data obtained from a control group of ten normal children of the same mean age.\n",
    "Fundamental frequency and peak intensity values were measured for each period and on this base temporal patterns were drawn and jitter and shimmer histograms were calculated for both groups; furthermore, global word duration, stressed vowel duration, and silent pauses were measured in order to evaluate the performances of deaf children in controlling timing parameters of speech. The results show that while the vowel production seems to underlie a sort of cathegorization, pitch and timing control definitely need auditory feedback.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-297"
  },
  "baum92_icslp": {
   "authors": [
    [
     "Shari R.",
     "Baum"
    ]
   ],
   "title": "Rate of speech effects in aphasia: an acoustic analysis of voice onset time",
   "original": "i92_1019",
   "page_count": 4,
   "order": 300,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "The production of voice onset time (VOT) contrasts at two rates of speech was investigated in three groups of subjects: nonfluent and fluent aphasic patients and non-neurological controls. Subjects produced the consonants [bdgptk] in the environment preceding the vowels [i e a o u] in a carrier phrase at a slow/normal and a fast rate of speech. For the normal speakers, acoustic analyses revealed significantly shorter VOTs at the fast as compared to the slow/normal rate, with a larger change evident in the voiceless relative to the voiced stop consonants. Both groups of aphasic patients produced rate changes that were smaller in magnitude than those of the normal subjects. Further, both brain-damaged patient groups produced VOTs that were shorter in the fast condition compared to the slow condition. However, a number of aberrant patterns emerged. For the nonfluent aphasic subjects, as in previous studies, voiced and voiceless consonants were produced with somewhat overlapping VOT distributions, suggesting a deficit in speech timing in these subjects. For the fluent aphasic patients, no differences in magnitude of VOT change emerged for voiceless relative to voiced consonants. Implications of these findings for theories of the nature of speech production impairments in aphasic patients are considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-298"
  },
  "bhatt92_icslp": {
   "authors": [
    [
     "Parth M.",
     "Bhatt"
    ]
   ],
   "title": "Fundamental frequency attributes following unilateral left or right temporal lobe lesion",
   "original": "i92_1023",
   "page_count": 4,
   "order": 301,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "This paper presents the results of an instrumental analysis of four fundamental frequency (F0) attributes (range, phrase or sentence final rises, phrase or sentence final falls and non-final variability) in the spontaneous speech output of two matched groups of adult, right-handed, Francophone subjects. Subjects with left temporal lesion produce lower values for all four attributes when compared to subjects with right temporal lesion. Direct comparison of these results does not, however, produce statistically significant results because of the high degree of inter-speaker variability in both groups. This suggests that individual speaker characteristics may still dominate F0 production even when subjects suffer focal unilateral lesion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-299"
  },
  "hosoi92_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Hosoi"
    ],
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Takehiko",
     "Harada"
    ],
    [
     "Hideaki",
     "Seki"
    ]
   ],
   "title": "Cue extraction and integration in speech perception for the hearing impaired",
   "original": "i92_1027",
   "page_count": 4,
   "order": 302,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "Temporal and spectral factors affecting speech perception were analyzed for the hearing impaired through three experiments. Results suggested the following. 1) The hearing impaired needed a longer vowel and a longer inter-stimuli interval to succeed in cue extraction and integration than the normal hearing subjects. 2) For normal hearing subjects, the vowel intelligibility was affected significantly by spectral distortion. 3) For the hearing impaired, the higher the speech rate was, the lower the word intelligibility was. For the inner-ear hearing impaired, word intelligibility was higher than monosyllable intelligibility at normal speech rate, but not at fast rate. This indicates that the inner-ear hearing impaired have high ability to extract and integrate temporally distributed cues which are richer in words than monosyllables, but this ability does not function at faster speech rate. For the retrocochlear hearing-impaired, cue extraction and integration seemed difficult even at normal speech rate. These results suggest that hearing impairments constrain temporal processing capabilities in several ways.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-300"
  },
  "nabelek92_icslp": {
   "authors": [
    [
     "Anna K.",
     "Nabelek"
    ]
   ],
   "title": "The relationship between spectral details in naturally produced vowels and identification errors in noise and reverberation",
   "original": "i92_1031",
   "page_count": 4,
   "order": 303,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "Vowel identification was tested in quiet, noise and reverberation with 20 normal-hearing subjects and 20 hearing-impaired subjects. Stimuli were 15 English vowels spoken in a /b-t/ context by six male talkers. Each talker produced five tokens of each vowel. For hearing-impaired subjects, some vowels were confusable even in undegraded listening conditions and for normal-hearing subjects some vowels became confusable in degraded listening conditions. Apparently, both degradation of listening conditions and perceptual limitations introduced by hearing impairment caused reduction of available information in the vowels. Examination of error clusters allowed an indication of which segments of the vowels were masked by noise or by reverberation and which segments were not perceived by hearing-impaired subjects. Spectral analysis of the vowel stimuli revealed differences in detailed structure among vowels produced by various talkers and among the five tokens produced by each talker. As a result of these differences, error clusters were dependent upon talker and upon particular vowel token.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-301"
  },
  "jamieson92_icslp": {
   "authors": [
    [
     "Donald G.",
     "Jamieson"
    ],
    [
     "Leonard",
     "Cornelisse"
    ]
   ],
   "title": "Speech processing effects on intelligibility for hearing-impaired listeners",
   "original": "i92_1035",
   "page_count": 4,
   "order": 304,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "Alternative hearing aid amplification and signal processing schemes have been developed (a) to suppress noise, improving listeners' subjective assessments of the resulting sound quality and/or (b) to improve speech intelligibility for hearing-impaired listeners. This paper presents selected studies of the effects of various types of processing on both perceived quality and measured intelligibility. One study examined the effects of low- and high-cut filtering, superimposed on a suitable hearing aid gain function, using simulations running on a digital-signal processing (DSP) board. Another study compared two types of amplification systems, using purpose-built hearing aids in which the gain functions had been precisely matched to the needs of the individual hearing-impaired listener.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-302"
  },
  "fuyuan92_icslp": {
   "authors": [
    [
     "Mo",
     "Fuyuan"
    ],
    [
     "Li",
     "Changli"
    ],
    [
     "Chen",
     "Tao"
    ]
   ],
   "title": "Chinese recognition and synthesis system based on Chinese syllables",
   "original": "i92_0971",
   "page_count": 4,
   "order": 305,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "This paper describes a Chinese recognition and synthesis system based on syllables that is a part of research efforts recently toward man-machine communication in unlimited vocabulary or very large vocabulary. We believe that the first step of this effort is a Chinese recognition and synthesis system of all Chinese syllables, because of special structure of Chinese which is tone language and monosyllable based language. The high recognition accuracy of syllable ( 97% for 5 candidates ) and rather natural and intelligible synthesizing speech is achieved in real time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-303"
  },
  "yogo92_icslp": {
   "authors": [
    [
     "Hirofumi",
     "Yogo"
    ],
    [
     "Naoki",
     "Inagaki"
    ]
   ],
   "title": "Accelerated stochastic approximation method based parameter estimation of monosyllables and their recognition using a neural network",
   "original": "i92_0975",
   "page_count": 4,
   "order": 306,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "This paper describes monosyllabic recognition, specifically stop consonants, using dynamic critical band spectra and a three-layered neural network. Parameters of stop consonants were estimated by utilizing an adaptive digital filter with fast convergence property, and were converted into critical band spectra similar to human hearing. The spectra were programmed into a three-layered neural network, and the stop consonants were identified. The learning property of the neural network was improved by augmenting common bias to the input-output function of the network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-304"
  },
  "altosaar92_icslp": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "Diphone-based speech recognition using time-event neural networks",
   "original": "i92_0979",
   "page_count": 4,
   "order": 307,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "In this paper we present a new speech recognition strategy that is based on diphones as the primary recognition unit in a time-event neural network (TENN) framework. TENN is based on a two- phase approach to identifying a speech unit: event detection followed by classification. We investigate two different implementation configurations, an integrated vs. a cascaded system, and report on their performance. Preliminary results show that for some of the most frequent diphone classes in Finnish recognition rates of 93-97% on the diphone level are possible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-305"
  },
  "flammia92_icslp": {
   "authors": [
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "Segment based variable frame rate speech analysis and recognition using a spectral variation function",
   "original": "i92_0983",
   "page_count": 4,
   "order": 308,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "This paper reports on applications of a Spectral Variation Function to speech analysis and recognition. A Spectral Variation Function is a correlation measure between successive windows of acoustic observation vectors, and the function is well suited for speech analysis in which acoustic events are associated with segments of variable duration, rather than with frames sampled at a fixed rate. The Spectral Variation Function is used in word recognition experiments using continuous densities hidden Markov models. To prepare the experiments first each utterance is segmented by a peak-detection algorithm applied to the Spectral Variation Function, and the number of states in each HMM word model is set to the average number of segments as found by the function in the training set for that word. Second, a few (3 or 4) vectors per segment are selected as observations for the HMMs.\n",
    "The frame selection reduces the number of observations by approximately 50% with respect to a fixed incoming frame rate of 10 ms. As a consequence, the computational load of both training and recognition is greatly reduced, while the computational load for the frame selection algorithm is limited. Using the variable frame rate technique gives recognition accuracy results which are comparable to those found with standard HMMs with fixed 10 ms frame rate. The multi-speaker, isolated word task experiments (17 speakers, 20 words, 924 test utterances) give 94.5% accuracy whereas fixed frame rate HMMs give between 93.4% and 94.0% accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-306"
  },
  "benoit92b_icslp": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ]
   ],
   "title": "Intelligibility of the French spoken in France compared across listeners from France and from the Ivory Coast",
   "original": "i92_0999",
   "page_count": 4,
   "order": 309,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "An identical intelligibility test was run in Grenoble with twenty French listeners and in Abidjan with twenty Ivorian listeners for whom French is a second language. The listeners had similar educational backgrounds, but differed in their linguistic competence with regards to the comprehension of French as spoken in France. Each subject had to manually transcribe 500 syntactically simple and semantically unpredictable sentences consisting of the most frequent French monosyllables. There were five stimulus series, one of spectrally distorted natural speech, and four of synthetic speech, produced by cross-varying two coding techniques and two prosodic models. The natural speech was obtained from the same French speaker whose voice served as reference in the text-to-speech synthesizers. The two automatic prosodic models were designed on the basis of French as spoken in France. Results were compared across the two groups of subjects. Overall intelligibility scores, measured as the number of correctly transcribed sentences, fully discriminate the performances of the Ivorian and the French listeners. Nevertheless, the perceived linguistic complexity is independent of the listener's origin. The linguistic complexity is here quantified in terms of the number of independent decision units each subject dealt with when transcribing responses. This number is calculated as the ratio r = log (Ps) / log (Pw) where Ps is the proportion of correct sentences and Pw the proportion of correct words. This index is strongly dependent on the linguistic redundancy contained in well-formed sequences. It is here used in order to estimate the ability with which listeners take advantage of syntactic information when decoding semantically anomalous but syntactically correct sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-307"
  },
  "brousseau92_icslp": {
   "authors": [
    [
     "Julie",
     "Brousseau"
    ],
    [
     "Sally Anne",
     "Fox"
    ]
   ],
   "title": "Dialect-dependent speech recognizers for canadian and european French",
   "original": "i92_1003",
   "page_count": 4,
   "order": 310,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "In the last year Dragon Systems, Inc., a research company in the field of speech recognition, has become interested in the impact that dialect can have upon recognition rates. Research is now being done for two major dialects of the French (Canadian and European) and the English (UK and American) languages. Results from a small isolated-word speaker-adaptive speech recognizer shows that an average increase of 3% in recognition rates is obtained by a speaker dictating to a system trained with acoustic data from their native dialect. A larger system is currently under development for Canadian French and preliminary testing is going forward. These results are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-308"
  },
  "muthusamy92b_icslp": {
   "authors": [
    [
     "Yeshwant K.",
     "Muthusamy"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Automatic segmentation and identification of ten languages using telephone speech",
   "original": "i92_1007",
   "page_count": 4,
   "order": 311,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "This paper extends our previous work on automatic language identification using 4 languages and high-quality speech, to automatic identification of 10 languages using telephone speech. The systems described here consist of two parts: (a) segmentation of telephone speech into seven broad phonetic categories and (b) classification of languages using feature measurements derived from the broad phonetic categories. Both the segmentation and classification stages use fully connected, feed-forward neural networks. When tested on new speakers from the 10 languages, the multi-language segmentation algorithm agrees with the handlabels 79.8% of the time. Classifiers were trained to identify (i) all 10 languages, (ii) each language vs. all others, (iii) the pairs English-Z, where L is one of the remaining 9 languages, and (iv) the triples English-L-O£/ier, where Other consists of the remaining 8 languages. Performance varied from 47.7% for the single 10-language network to 88.6% for the English-Tamil network. Classification performance of human listeners on short excerpts of speech is also reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-309"
  },
  "nakagawa92_icslp": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Yoshio",
     "Ueda"
    ],
    [
     "Takashi",
     "Seino"
    ]
   ],
   "title": "Speaker-independent, text-independent language identification by HMM",
   "original": "i92_1011",
   "page_count": 4,
   "order": 312,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "This paper describes an automatic language identification method based on HMMs (Hidden Markov Models) for acoustic features. The hidden Markov modeling is used to represent the dynamics of the states of the vocal tract. Each language has its proper phonotactics. For the experiment of the identification, utterances of 4 languages (English, Japanese, Mandarin Chinese and Indonesian) were modeled by several HMMs. They were uttered by 15 male speakers (10 for training the HMM and 5 for testing) for each language. These trained HMMs showed considerable inter-language variations. A HMM topology was a full structured (ergodic) model that any state could transit to every states. And we used 2 kinds of HMMs; the DHMM (discrete HMM) with the codebook and the CHMM (Continuous density HMM). The HMM was trained using both the Baum-Welch (Forward-Backward) algorithm and the Viterbi algorithm. The latter was used for emphasizing the state transition probability. For comparison, we also experimented on the identification using the VQ (Vector Quantization) distortion, and the CMDF (Continuous Mixture Density output probability Functions). The results showed that the CHMM identified 4 languages very well (best correct identification rate was 86.3%), and the CHMM had a performance very nice than DHMM (47.6%). Addition, the CHMM had a compareable performance with the CMDF. In particular, the identification between English and Japanese was perfectly performed with the accuracy of more than 95%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-310"
  },
  "itahashi92_icslp": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Tsutomu",
     "Yamashita"
    ]
   ],
   "title": "A discrimination method between Japanese dialects",
   "original": "i92_1015",
   "page_count": 4,
   "order": 313,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "This paper describes a method of dialect discrimination based on differences in slopes of fundamental frequency contours of speech. The fundamental frequency contours were approximated by polygonal lines which were determined using a dynamic programming procedure. The starting frequency and slope of each line were calculated. Mean values, standard deviations and histograms of these parameters were calculated for about 40 seconds of speech spoken by male speakers of standard Japanese and four typical Japanese dialects. Results show that the four dialects and standard Japanese can be discriminated based on the slopes and relative starting frequencies of the approximated lines.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-311"
  },
  "boyanov92_icslp": {
   "authors": [
    [
     "B.",
     "Boyanov"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Pathological voice analysis using cepstra, bispectra and group delay functions",
   "original": "i92_1039",
   "page_count": 4,
   "order": 314,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "A new approach for the analysis and evaluation of the pathological voices is proposed. The analysis tools include the following algorithms: robust pitch period (To) detection, high resolution pitch-synchronous spectral analysis, deconvolution using the cepstrum (c(t)), separation of harmonics from other spectral components, calculation of the smoothed spectrum corresponding to the vocal tract by means of the negative of the first derivative of the phase of the signal termed the group delay function (GDF) and calculation of the bispectra (B(fl,f2)) of the harmonics. Pitch synchronous spectral analysis is realized to evaluate c(t), GDF and B(fl,f2) with minimal error. Low quefrency windowed c(t) (corresponding to the vocal tract impulse response) is used for the GDF calculation. The harmonics are evaluated using the value calculated for To and spectral thresholds are adjusted for the low and high spectral regions found by means of GDF. B(fl,f2) is calculated only over the harmonics in order to reduce computation and facilitate the interpretation of the results. The effects are being quantified and the new measures should help the evaluation and classification of pathological voices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-312"
  },
  "fu92_icslp": {
   "authors": [
    [
     "Qianje",
     "Fu"
    ],
    [
     "Peyu",
     "Xia"
    ],
    [
     "Ren Hua",
     "Wang"
    ]
   ],
   "title": "Lateralization of speech sounds by binaural distributing processing",
   "original": "i92_1043",
   "page_count": 4,
   "order": 315,
   "p1": "1043",
   "pn": "1046",
   "abstract": [
    "This paper proposed a lateralization method of speech sounds that had no restrictions with regard to sound field conditions, including the stationary and moved multisignals under the real circumstance. It is specially designed to model the space perception function of the human auditory system , which is the basis for the further binaural processing method such as COCKTAIL PARTY PROCESSOR. This important aim was achieved by applying the knowledge of binaural space perception of the human auditory system. Combining the crosscorrelation processing between binaural channels with the distributing characteristics of the location information of the different sound sources, a integrating processing unit of the location information in the different auditory channels was introduced in this pa per,then a composite judgement unit was introduced to simulate the location judgement function of the central nerve system. The individual steps of the processing scheme were described and preliminary results were presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-313"
  },
  "rump92_icslp": {
   "authors": [
    [
     "H. H.",
     "Rump"
    ]
   ],
   "title": "Timing of pitch movements and perceived vowel duration",
   "original": "i92_1047",
   "page_count": 4,
   "order": 316,
   "p1": "1047",
   "pn": "1050",
   "abstract": [
    "The hypothesis was tested that the timing of accent-lending pitch movements influences the perceived duration of a vowel. Dutch subjects were asked to adjust the physical duration of a vowel so as to fit into the temporal structure of a sentence. The vowel occurred in a monosyllabic word embedded in a carrier sentence. Three pitch movements on the vowel were used, a rise, a rise-fall, and a fall.\n",
    "Two opposite trends were found: the earlier the fall, the longer the duration of the target vowel was adjusted, the earlier the rise or rise-fall, the shorter its duration was adjusted. Control experiments indicated that the results should be interpreted in terms of a trade-off between the effects on prominence of timing of pitch movements and physical segment duration. It is concluded that late timing of pitch movements enhances the perceived vowel duration, but that this effect depends on the kind of pitch movement: the effect is cancelled in the case of late rises and rise-falls, whereas it is enhanced in the case of late falls by virtue of the enhancing effect on prominence of the accented syllable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-314"
  },
  "liu92_icslp": {
   "authors": [
    [
     "J. P.",
     "Liu"
    ],
    [
     "G.",
     "Baudoin"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Studies of glottal excitation and vocal tract parameters using inverse filtering and a parameterized input model",
   "original": "i92_1051",
   "page_count": 4,
   "order": 317,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "Speech analysis for high quality speech synthesis or high accuracy speech recognition requires realistic models not only for the vocal tract but also for the voice source. This paper presents a comparison between two analysis methods for the calculation of the voice source and vocal tract parameters for voiced sounds. The first method computes once the vocal tract parameters and then ajusts a glottal parametric model on the residual by a quadratic optimization procedure. The second method iteratively computes the parameters of the vocal tract and glottal models. It was shown that first method, though a least complexity leads to signal-noise ratio slightly lower than the second method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-315"
  },
  "norris92_icslp": {
   "authors": [
    [
     "Dennis",
     "Norris"
    ],
    [
     "Brit van",
     "Ooyen"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Speeded detection of vowels and steady-state consonants",
   "original": "i92_1055",
   "page_count": 4,
   "order": 318,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "We report two experiments in which vowels and steady-state consonants served as targets in a speeded detection task. In the first experiment, two vowels were compared with one voiced and once unvoiced fricative. Response times (RTs) to the vowels were longer than to the fricatives. The error rate was higher for the consonants. Consonants in word-final position produced the shortest RTs. For the vowels, RT correlated negatively with target duration. In the second experiment, the same two vowel targets were compared with two nasals. This time there was no significant difference in RTs, but the error rate was still significantly higher for the consonants. Error rate and length correlated negatively for the vowels only. We conclude that RT differences between phonemes are independent of vocalic or consonantal status. Instead, we argue that the process of phoneme detection reflects more finely grained differences in acoustic/articulatory structure within the phonemic repertoire.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-316"
  },
  "slawinski92_icslp": {
   "authors": [
    [
     "Elzbieta B.",
     "Slawinski"
    ]
   ],
   "title": "Temporal factors in the perception of consonants for different age and hearing impairment groups",
   "original": "i92_1059",
   "page_count": 4,
   "order": 319,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "Change in the amplitude envelope and transition duration were suggested as acoustical cues used in the phonemic distinction between /b/ and /w/. The role of these acoustical cues in phonetically based trading relations was examined by assessing the ability of normal and hearing-impaired listeners to identify stimuli on two synthetic /b/-/w/ continua. These continua differed in their amplitude envelope, but changed in the same way along the dimension of transition duration. Results indicated, that both groups of listeners demonstrated the categorical perception in both amplitude type conditions. Furthermore, it was found that the phonemic boundary was dependent on both the type of continuum used and the listeners hearing status.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-317"
  },
  "alwan92_icslp": {
   "authors": [
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "The role of F3 and F4 in identifying place of articulation for stop consonants",
   "original": "i92_1063",
   "page_count": 4,
   "order": 320,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "This study develops an analytical procedure for predicting perceptual confusions of speech sounds in noise. The focus of the study was the perceptual role of formant frequencies higher than F2 in signalling the place of articulation for the stop consonants /b,d/ in /Co/ syllables. An identification experiment in which the stimuli were synthetic syllables mixed with a band-pass noise masker was conducted. The masker was centered around the F2 region. A perceptual metric was developed to analyze the results of the experiment. The metric was based on a combination of theoretical and empirical results and considered both within-hand and above-band masking of the formant frequencies. Results show that listeners were able to distinguish /be/ and /Aof syllables even when the entire F2 trajectory was predicted to be masked. Amplitude differences in F3 and F4 appeared to be used as place cues, even though the trajectories of these formants were the same for the two consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-318"
  },
  "sawallis92_icslp": {
   "authors": [
    [
     "Thomas R.",
     "Sawallis"
    ]
   ],
   "title": "A new measure for perceptual weight of acoustic cues: an experiment on voicing in French intervocalic [t,d]",
   "original": "i92_1067",
   "page_count": 4,
   "order": 321,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "An experiment is presented here concerning perception of edited consonant length, voicing amplitude, and burst+aspiration amplitude in a corpus of natural tokens of French intervocalic [t, d], with the editing designed using the statistical distributions of those cues as previously measured in the selfsame corpus. This design largely preserves the natural variation of the non-target cues. Then, mathematical tools borrowed from signal detection theory [5] are used on the results of these tests to derive perceptual sensitivity measurements for the cues. These measurements can be compared autonomously, without trading-relationship-style equivalences, across cues, environments, and languages. The properties of this type of measurement make it amenable to use not only in speech perception, but also in formal phonology and general linguistics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-319"
  },
  "wrench92_icslp": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "John",
     "Laver"
    ],
    [
     "M. S.",
     "Jackson"
    ],
    [
     "D. S.",
     "Soutar"
    ],
    [
     "A. G.",
     "Robertson"
    ],
    [
     "J.",
     "MacKenzie"
    ]
   ],
   "title": "Objective speech quality assessment in patients with intra-oral cancers: voiceless fricatives",
   "original": "i92_1071",
   "page_count": 4,
   "order": 322,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "This paper presents an acoustic-phonetic analysis of voiceless fricatives produced by patients undergoing treatment for intra-oral cancer. A method is proposed with the purpose of regularly assessing the quality of such speech before and during rehabilitation. Speech quality is to be measured in terms of accuracy and separation of speech sounds within this class, in comparison with the patient's own prc-opcrativc speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-320"
  },
  "connell92_icslp": {
   "authors": [
    [
     "Bruce",
     "Connell"
    ]
   ],
   "title": "Tongue contact, active articulators, and coarticulation",
   "original": "i92_1075",
   "page_count": 4,
   "order": 323,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "This paper is based electropalatographic data from Ibibio. At issue is the hypothesis proposed by Recasens [7], [8] that coarticulation varies monotonically and inversely with the degree of tongue dorsum contact. Some support for the monotonic/inverse hypothesis is found, however it is also found some modification is needed in order to take adequate account of the relationship between coarticulation and active articulator.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-321"
  },
  "kashino92_icslp": {
   "authors": [
    [
     "Makio",
     "Kashino"
    ],
    [
     "Astrid van",
     "Wieringen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Cross-languages differences in the identification of intervocalic stop consonants by Japanese and dutch listeners",
   "original": "i92_1079",
   "page_count": 4,
   "order": 324,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "A joint Japanese and Dutch experiment studied the effect of language on the perceptual contribution of temporally distributed cues (i.e. VC transitions and CV transitions) in identifying intervocalic voiceless stop consonants. Japanese and Dutch listeners identified Japanese stop consonants from isolated VCV, CV, VC, or VC1-C2V syllables (all extracted from natural Japanese VCV utterances) under varying noise conditions, in which up to 70 ms of the release burst and the vocalic transition are replaced by noise. The two language groups perform very similar for the VCV and CV stimuli; the average consonant identification scores of both groups are higher for the VCV stimuli than for the CV stimuli when the CV cues are largely eliminated by noise replacement. The difference between the results of the two types of stimuli indicates that Japanese as well as Dutch listeners can use the VC cues (present in VCV stimuli) in identifying the stop consonants. However, the identification scores for the VC and VC1-C2V stimuli are lower for Japanese than for Dutch listeners, suggesting that the phonotactic system influences identification. Due to the absence of syllable-final stop consonants in the Japanese language, Japanese listeners are less inclined to use the VC transitions as cues to a syllable-final stop consonant than Dutch listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-322"
  },
  "tsuzaki92_icslp": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ]
   ],
   "title": "Effects of typicality and interstimulus interval on the discrimination of speech stimuli: within-subject comparison",
   "original": "i92_1083",
   "page_count": 4,
   "order": 325,
   "p1": "1083",
   "pn": "1086",
   "abstract": [
    "To investigate how learned knowledge would affect the perceptual judgment for speech sounds as well as how such effects would chagne during the processing time and retention interval, Japanese listeners were tested with the AX discrimination task in various interstimulus intervals (1ST) as a within-subject factor. Speech stimuli were synthesized to comprise both typical and atypical stimulus sets. In the typical set, the standard stimulus had characteristics of the typical Japanese /aba/ sound. In the atypical set, the standard was not a good exemplar of the Japanese /aba/ sound although it was usually recognized as /aba/. The performance of the subjects was analyzed in terms of both the center and width of discriminal processes. Even at relatively short ISIs, there was a tendency for the typical standard to have a smaller width score than the atypical standard, suggesting that the perceptual identity of the typical stimulus was higher than that of the atypical stimulus. At long ISIs, the effect of the typicality became less salient. The estimation of the center of discriminal processes indicated that perceptual distance along the stimulus continuum is asymmetrical and that the direction of the asymmetry was different between the typical and atypical condition. This partially supports the idea of the perceptual magnet. Because the effect of ISI was completely different from what has been observed in previous studies on the categorical perception, this paradigm could seize a new aspect of the influence of knowledge on perception. The results indicate that the differentiation by the prototypes for phoneme classes occurs at an early stage of perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-323"
  },
  "cole92b_icslp": {
   "authors": [
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Yeshwant K.",
     "Muthusamy"
    ]
   ],
   "title": "Perceptual studies on vowels excised from continuous speech",
   "original": "i92_1087",
   "page_count": 4,
   "order": 326,
   "p1": "1087",
   "pn": "1090",
   "abstract": [
    "We examined the ability of human listeners to classify vowel sounds into sixteen different categories when the sounds are excised from fluent speech and presented in isolation. The duration of each sound and its phonetic category were provided as part of the TIMIT corpus of phonetically labeled utterances. The specific objectives of the research were: (a) to compare the classification responses of listeners with the phonetic labels provided by experts, (b) to understand the influence of phonetic context on classification performance, (c) to determine the influence of prior exposure to the speaker's voice on vowel classification, and (d) to establish more effective performance benchmarks for the evaluation of phonetic classification algorithms. Vowels presented in isolation were identified with about 54.8% accuracy, compared to the phonetic labels provided by experts. Providing listeners with additional context, by extending the speech excerpt to include the segments preceding and following the vowel, improved classification performance to about 65.9%. In a second experiment, information about the talker who produced the vowel excerpt was presented, in the form of a short phrase, just before the vowel excerpt was presented for classification. There was a small but significant increase in listener-labeler agreement with prior exposure to the speaker's voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-324"
  },
  "weitzman92_icslp": {
   "authors": [
    [
     "Raymond S.",
     "Weitzman"
    ]
   ],
   "title": "The relative perceptual salience of spectral and durational differences",
   "original": "i92_1095",
   "page_count": 4,
   "order": 327,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "In order to assess the interaction of spectral and durational differences that may affect their relative importance as cues for vowel contrasts, two concept formation experiments were conducted in which subjects were trained to categorize synthesized vowel stimuli that differed in the value of Fl and in duration. In Experiment 1, Fl of the two category tokens differed by 44 Hz (0.4 bark) and in Experiment 2, by 131 Hz (1.2 bark). In both experiments the duration difference of the two stimuli were the same (10 ms). After subjects had learned to distinguish the stimuli, they were asked to categorize new stimuli. How the subjects responded to these new stimuli was used to evaluate the relative salience of durational and spectral differences. Although the duration differences were the same in both studies, the results in Experiment 1 indicated that duration was the more salient property for the category distinction, while in Experiment 2, the Fl difference was not only more salient but effectively masked the duration difference. These results strongly suggest that whether or not an acoustic parameter is a cue for a category distinction will depend on its strength relative to other co-occurring differences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-325"
  },
  "koopmansvanbeinum92_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "Can 'level words' from one speaking style become teaks' when spliced into another speaking style?",
   "original": "i92_1099",
   "page_count": 4,
   "order": 328,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "Is the listener using fixed reference targets, when perceiving either read or spontaneous speech, or is ne/she attuning his/her perception mechanism to the specific speaking style, applying some kind of style normalization. To answer this question we spliced function words from three different speaking styles into two types of lexically identical short sentences, either from free conversation, or from the same text read aloud. To avoid F0-influences the sentences were made unvoiced by LPC-resynthesis. It turned out that in a number of cases function words segmented from more carefully pronounced speaking styles were heard as accented when spliced into more reduced speaking styles, merely because of their longer duration and/or more spectral contrast than the original words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-326"
  },
  "gable92_icslp": {
   "authors": [
    [
     "Beverley",
     "Gable"
    ],
    [
     "Helen",
     "Nemeth"
    ],
    [
     "Martin",
     "Haran"
    ]
   ],
   "title": "Speech errors and task demand",
   "original": "i92_1103",
   "page_count": 3,
   "order": 329,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "This study examined the effects of task demand on the frequency of speech errors. Subjects spoke continuously in three levels of task demand: low, moderate, high. Task demand was manipulated through the use of an additional task. The frequency of various speech errors increased with increased task demands. Of particular interest were sound errors that are lexically legitimate (word) and anomalous (non-word). With low task demands, more word errors were produced than non-word errors. Under moderate and high task demands, more non-word errors were produced than word errors. More non-word errors were produced at moderate and high task demands than at low task demands while the number of word errors remained unchanged. These results are consistent with earlier studies on sound error elicitation and lend support for an editing process that detects and corrects errors prior to articulation. Accordingly, under low task demands non-word errors are rarely articulated because the editor, through access to the lexicon, easily detects and corrects non-word errors but not word errors. The results also suggest that the editor requires cognitive resources. As task demands increase, resources become unavailable to the editor allowing the non-word errors to slip past the editor as do word errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-327"
  },
  "esling92_icslp": {
   "authors": [
    [
     "John H.",
     "Esling"
    ],
    [
     "B. Craig",
     "Dickson"
    ],
    [
     "Roy C.",
     "Snell"
    ]
   ],
   "title": "Analysis of phonation type using laryngographic techniques",
   "original": "i92_1107",
   "page_count": 4,
   "order": 330,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "In order to develop a laryngographic (Lx) waveform measurement algorithm for phonation types which is independent of frequency variation, Lx and acoustic signals of a sustained vowel at seven pitch increments for a set of standard models of laryngeal voice qualities are captured and analyzed in the Computerized Speech Lab (CSL) environment. Auditory assessments are used to categorize the data; a procedure is applied to eliminate DC float in the Lx signal and flatten the baseline; a pitch-extraction algorithm is introduced to compute pitch and jitter; speed-quotient and open-quotient techniques are applied to derive Lx-period ratio comparisons; and a cepstral procedure is used to establish periodicity indices for the pitch-differentiated waveform samples. The research objective is to reliably identify and distinguish degrees of breathy voice, whispery voice, harsh voice or ventricular voice, where the creaky voice/ modal voice/ falsetto frequency dimension is viewed as a confounding factor in the recognition of phonation types in the breathiness/ harshness dimension. The practical objective is to develop speech and language technology with potential clinical applications in the analysis of pathological phonatory qualities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-328"
  },
  "shigeno92_icslp": {
   "authors": [
    [
     "Sumi",
     "Shigeno"
    ]
   ],
   "title": "Effect of prototypes of vowels on speech perception in Japanese and English",
   "original": "i92_1111",
   "page_count": 4,
   "order": 331,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "Stimulus order effects in vowel discrimination were examined. Two experiments were conducted to investigate (l)how the Japanese listeners identify American English vowels and (2) how the Japanese discriminate the English vowel pairs. Five Japanese subjects participated in the experiments. The stimuli were synthesized American English vowels. It was found that stimulus order effects were largest when the stimulus pairs were in the neighborhood of the English vowels that corresponded to Japanese prototype vowels, although the effects were smaller when the stimulus pairs were far from Japanese prototype vowels. The results suggest that stimulus order effects should be determined by the prototype vowels of listeners' native language and by continuum endpoints employed for the experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-329"
  },
  "morohashi92_icslp": {
   "authors": [
    [
     "Tomo-o",
     "Morohashi"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ],
    [
     "Hiroyuki",
     "Yashima"
    ],
    [
     "Jouji",
     "Suzuki"
    ]
   ],
   "title": "Characteristics of voice picked up from outer skin of larynx",
   "original": "i92_1115",
   "page_count": 4,
   "order": 332,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "The objective of this paper is to clearly understand the characteristics of the voice picked up from the outer skin of a larynx (VOSL) with a throat microphone. This microphone is frequently employed in noisy environments to obtain the speech quality of high signal-to-noise ratio. It is known that VOSL lacks high frequency component comparing to the normal speech radiated from the lips. The transfer characteristics of the path from the glottis to the outer skin of a larynx is speculated from the long term spectrum. It is revealed that the relative amplitude between vowels of VOSL is different from that of the normal speech. Even if high frequency component of VOSL is emphasized, its characteristics do not completely get close to that of the normal speech. Furthermore, measurement of formant frequency on VOSL is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-330"
  },
  "nabelek92b_icslp": {
   "authors": [
    [
     "Igor V.",
     "Nabelek"
    ]
   ],
   "title": "Coding of voicing in whispered plosives",
   "original": "i92_1119",
   "page_count": 3,
   "order": 333,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "In distinction to phonated speech, in whispered speech the vocal cords do not vibrate and the quasi-periodic vibration with harmonics present in phonation is replaced by noise with continuous spectrum. However, it is not just the presence or absence of vocal cord vibration which distinguishes voicing from non-voicing. There are secondary voicing cues which make perceptual discrimination between whispered voiced and voiceless speech sounds possible (e.g., Chen and Nabelek, 1986). However, the number of discrimination errors in the whispered speech is higher than in the phonated speech. In whisper, the largest number of errors occurred in cognates. In those, the place of articulation was, in general, identified correctly but the voicing feature was often misidentified. Still, the distinction between voicing and non-voicing was significant. Spectrographic analysis showed differences in formant trajectories, onset rates of bursts following the release, energy distributions, and others, that might serve as cues in voicing identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-331"
  },
  "cheesman92_icslp": {
   "authors": [
    [
     "Margaret F.",
     "Cheesman"
    ],
    [
     "Shelly",
     "Lawrence"
    ],
    [
     "Allison",
     "Appleyard"
    ]
   ],
   "title": "Performance on a nonsense syllable test using the articulation index",
   "original": "i92_1123",
   "page_count": 4,
   "order": 334,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "A modification of the Distinctive Features Difference (DFD) test [1] was developed as part of a speech perception test battery, primarily for use in testing new hearing aid circuitry and noise reduction algorithms. The DFD(m) test requires listeners to identify each of 21 consonant sounds, spoken by two male and two female talkers, presented in an /ACd/ context against a noise background. Identification errors can be scored in terms of either the overall percent-correct item identification or the number of feature differences between each target sound and response. Performance-intensity functions were obtained with a 70 dB(A) background noise masker that was spectrally shaped to match the long-term average spectrum of the 84 test items. Progressive low-pass and high-pass filtering of the speech was used to obtain the crossover frequency of the Articulation Index importance weights for the combined 4-talker speech materials. The crossover frequency was 2170 Hz, higher than that previously found for other sets of nonsense syllables. Articulation Indices were computed using three methods and their relatively suitability for application to the modified DFD test was investigated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-332"
  },
  "jamieson92b_icslp": {
   "authors": [
    [
     "Donald G.",
     "Jamieson"
    ],
    [
     "Ketan",
     "Ramji"
    ],
    [
     "Issam",
     "Kheirallah"
    ],
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "CSRE: a speech research environment",
   "original": "i92_1127",
   "page_count": 4,
   "order": 335,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "CSRE (The Canadian Speech Research Environment) is a comprehensive, microcomputer-based system designed to support speech research using IBM/AT-compatible micro-computers [4]. CSRE provides a powerful, low-cost facility in support of speech research, using mass-produced and widely-available hardware. The project is non-profit, and relies on the cooperation of researchers at a number of institutions. Work on the project is supported primarily by the fees generated when the software is distributed. Version 3.0 of CSRE has been used since 1989 by researchers in more than 100 laboratories in 12 countries. Version 4.0 offers a wider range of functions, runs faster, uses higher resolution displays, and supports additional hardware systems, including digital signal processing boards. Functions include speech capture, editing, and replay; several alternative spectral analysis procedures, with color and surface/3D displays; parameter extraction/tracking and tools to automate measurement and support data logging; alternative pitch-extraction systems; parametric speech (KLATT80) [7] and non-speech acoustic synthesis, with a variety of supporting productivity tools; and a comprehensive experiment generator, to support behavioral testing using a variety of common testing protocols.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-333"
  },
  "hata92_icslp": {
   "authors": [
    [
     "Kazue",
     "Hata"
    ],
    [
     "Yoko",
     "Hasegawa"
    ]
   ],
   "title": "A study of F0 reset in naturally-read utterances in Japanese",
   "original": "i92_1239",
   "page_count": 4,
   "order": 336,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "F0 reset has been studied by researchers advocating differing theories regarding F0 downward trend in the course of utterances, and the definition of F0 reset depends heavily on their theoretical assumptions (Ladd [1]). In the declination theory, F0 reset is considered to be an upward shift of the global declination line, whether it is defined as a peak-to-peak \"topline\" or a valley-to-valley \"baseline\". In the downstep theory, by contrast, F0 is considered to be reset when downsteps of accented syllables are blocked. This paper investigates the F0 reset phenomenon using naturally-read texts in Japanese; auditorily determining F0 reset locations and acoustically characterizing this perception. The results indicate that the F0 rise between the preboundary low value and the postboundary peak value is the most consistent with the perception of F0 reset location.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-334"
  },
  "wang92_icslp": {
   "authors": [
    [
     "H. Samuel",
     "Wang"
    ],
    [
     "Fu-Dong",
     "Chiu"
    ]
   ],
   "title": "On the nature of tone sandhi rules in taiwanese",
   "original": "i92_1243",
   "page_count": 3,
   "order": 337,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "Taiwanese tone sandhi was argued by-generative phonologists as productive and reducible to a rule-type operation. This paper attempts to explore into the nature of the productivity of the phenomenon by conducting an experiment. Twenty nonsense words were made up with four words for each of the five tones. Eighteen native speakers were asked to perform a task in which the nonsense words assumed the meaning of color adjectives. The subjects were instructed to use the words in sentences in a tone sandhi environment. The items were run through five times. The results showed that different tones demonstrated different degrees of productivity, and that productivity increased over five trials. This seems to indicate that familiarity of lexical items does contribute to the productivity of the phenomenon. A non-generative, organizational account is proposed to accomodate the results. It is argued that phonological forms coexist in the mental structure, and linguistic generalizations serve as organizational principles rather than as derivational/deterministic procedures for relevant forms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-335"
  },
  "nathan92_icslp": {
   "authors": [
    [
     "Geoffrey S.",
     "Nathan"
    ]
   ],
   "title": "How shallow is phonology: declarative phonologies meet fast speech",
   "original": "i92_1247",
   "page_count": 3,
   "order": 338,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "In recent work a number of researchers have suggested that it is possible to build a phonological theory in which the process metaphor is no longer necessary. The motivation for this sudden rush to non-process phonology has been the interest in neural network models, which allow only declarative-type statements. This paper presents evidence for multiple levels in phonology, using examples from English fast speech phenomena that suggest the limitation to an exact number of levels in non-process phonology is in error. The models require that particular rules appear on particular levels. If, in pre-declarative terms, a rule precedes another, the first must state relationships between the morphophonemic and phonemic level, while the second must state relationships between the phonemic and phonetic level. I show that, given these assumptions, the single process of Flap Deletion, applying in allegro American English, must be on both levels, leading to the Declarative Phonology equivalent of an ordering paradox.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-336"
  },
  "hosaka92_icslp": {
   "authors": [
    [
     "Junko",
     "Hosaka"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Noriyoshi",
     "Uratani"
    ]
   ],
   "title": "Analyzing postposition drops in spoken Japanese",
   "original": "i92_1251",
   "page_count": 4,
   "order": 339,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "In this paper we analyze postposition drops in nominal phrases of spoken Japanese. This analysis is aimed at being applied to syntactic constraints within a speech recognition module of a speech translation system. Speech recognition often fails to recognize short words. Among syntactic categories, postpositions belong to such a group. In machine translation of Japanese, postpositions are often used as a processing trigger. However, if a postposition is unspoken, this approach fails. To overcome this problem as well as reflect actual usage, we have studied postposition drops in our dialogue database. We first retrieved all nominal phrases without any postposition. We then studied the nominal phrases in which the dropped postpositions can be naturally recovered and those for which postposition recovery is very difficult. Further, we investigated ways of predicting the recoverable postpositions that require little calculation. Finally, we discuss the possibility of applying the analysis to syntactic constraints within speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-337"
  },
  "zhang92_icslp": {
   "authors": [
    [
     "Jialu",
     "Zhang"
    ],
    [
     "Xinghui",
     "Hu"
    ]
   ],
   "title": "Fundamental frequency patterns of Chinese in different speech modes",
   "original": "i92_1255",
   "page_count": 4,
   "order": 340,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "The fundamental frequency F0 patterns of standard Chinese were examined at both macro (passage) level and micro (period) level in different speech modes using a Laryngograph. The results show that: 1. The more speech effort speaker makes the higher mean F0, mid F0, and modal F0 in connected speech, at the same time the pitch range is expanded; 2. The value of mean F0, mid F0, and modal F0 is different in different speech styles (isolated syllables, isolated sentences, and connected speech); 3. The speech rate has apparent influence on glottal waves but no significant influence on the F0 distribution function; 4. The closed phase is roughly inverse of F0 in isolated syllables with tones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-338"
  },
  "kvale92_icslp": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ],
    [
     "Ante Kjell",
     "Foldvik"
    ]
   ],
   "title": "The multifarious r-sound",
   "original": "i92_1259",
   "page_count": 4,
   "order": 341,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "The most common pronunciation of hi in Norwegian is as an apical alveolar tap, i.e. the tongue tip touches the alveolar ridge and makes a short closure phase. In a manual segmentation task [1] we thus assumed that the tongue movement for the alveolar tap is a symmetric one. In our segmentation of hi we therefore included 10ms on each side in addition to the segment of less intensity in the spectrogram to include the tongue tip movement. In this paper we will demonstrate that different phonemic contexts systematically affect the realisation of /r/ and how this leads to some exceptions from the lOms-addition-rule mentioned above.\n",
    "Depending on the speaker's dialect background, the /r/-phoneme in Norwegian is produced as an apical tap or trill, a uvular tap or trill, or a post-palatal, velar or uvular fricative [2]. Recent studies [2] show a widespread and rapid change from an apical to a dorsal pronunciation of/r/ in South/West Norwegian dialects. Reasons for this ongoing change will be discussed.\n",
    "Based on the annotated European multilingual EUROM.O speech database we will discuss hi pronunciations in different languages and how hi has been segmented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-339"
  },
  "mcrobbieutasi92_icslp": {
   "authors": [
    [
     "Zita",
     "McRobbie-Utasi"
    ]
   ],
   "title": "The role of preaspiration duration in the voicing contrast in skolt sami",
   "original": "i92_1263",
   "page_count": 3,
   "order": 342,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "This paper summarizes the results of an acoustic analysis of preaspiration duration in Skolt Sdini, an Eastern dialect of Sami (a Finno-Ugric language). It will show that the characteristic durational patterns associated with disyllabics belonging to different structural types remain the same, regardless of whether preaspiration is present or not. In Skolt Sami, durational ratios rather than absolute durational values are significant in signalling differences between the main structural types in the language [3,4]. These durational ratios remain constant, regardless of the durational values associated with preaspiration. It will be argued that the voicing contrast of stop and affricate consonants in word-medial position is not to be considered as one between voicing and voicelessness, but rather as a contrast due to the manifestation of preaspiration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-340"
  },
  "yamada92_icslp": {
   "authors": [
    [
     "Eiji",
     "Yamada"
    ]
   ],
   "title": "Parameter setting for abstract stress in tokyo Japanese",
   "original": "i92_1267",
   "page_count": 4,
   "order": 343,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "This paper shows that the principles-and-parameters approach in generative phonology can also be applied to pitch-accent languages such as Tokyo Japanese. In Halle and Vergnaud (1987), various stress patterns of words and phrases in a number of stress-accent languages have been accounted for by rules and parameters. Likewise, if we define a certain prosodical point in Tokyo Japanese as an abstract stress, its location can be predicted by the rules and parameters. Moreover, parallels and differences between the parameters of Tokyo Japanese and those of four languages (Latin, Polish, Turkish, ancient Greek) are shown with the proposed Edge Demarcation Convention within the new theoretical framework of metrical phonology formulated by Halle and Idsardi (1992).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-341"
  },
  "ottesen92_icslp": {
   "authors": [
    [
     "Georg E.",
     "Ottesen"
    ]
   ],
   "title": "A method for studying prosody in texts read aloud",
   "original": "i92_1271",
   "page_count": 4,
   "order": 344,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "This paper descibes a method for studying prosody by establishing a double tree structure of relevant parameters and searching for relationships between parameters at different levels. The tree structure contains both phonetic and linguistic parameters. Pitch values, phoneme durations and word accents are found from a recording of texts read aloud. A query language has been designed to search in this type of data structure. The method is implemented as an interactive program. Analysis results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-342"
  },
  "heuven92_icslp": {
   "authors": [
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Linguistic versus phonetic explanation of consonant lengthening after short vowels: a contrastive study of dutch and English",
   "original": "i92_1275",
   "page_count": 3,
   "order": 345,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "In Dutch, intervocalic consonants are longer after short vowels than after long vowels. Two explanations for this phenomenon are experimentally tested in this paper. The first is a universal explanation, suggesting that short vowels are abruptly stopped (\"checked\"), so that the tail-portion of the vowel is traded for longer closure duration. The second is a language specific explanation: in the phonology of Dutch, consonants after short vowels have to be analyzed as geminates; longer consonant duration in this context reflects the underlying geminate status. If the language specific explanation is correct, the lengthening effect should not be found for English, which does not require intervocalic consonants after short vowels to be analyzed as geminates. If the universal phonetic account is true, the consonant lengthening effect should be found in any language, including Dutch and English. We report an experiment set up to choose between these two competing accounts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-343"
  },
  "elenius92_icslp": {
   "authors": [
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Comparing phoneme and feature based speech recognition using artificial neural networks",
   "original": "i92_1279",
   "page_count": 4,
   "order": 346,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "An artificial neural network has been trained by the error back-propagation technique to recognise phonemes and words. The speech material was recorded by a male Swedish talker and was labelled by a phonetician. There were 38 output nodes corresponding to Swedish phonemes. The training algorithm was somewhat modified to increase the training speed. Introducing coarticulation information by adding simple recurrency to the net is shown to more effective than expanding the size of the input spectral window. The phoneme recognition network was used with dynamic programming for time alignment to recognise connected digits. It was compared to a similar recogniser based on nine quasi-phonetic features instead of 38 phonemes. The phoneme based system performed better than the feature based one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-344"
  },
  "strangert92_icslp": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ]
   ],
   "title": "Prosodic cues to the perception of syntactic boundaries",
   "original": "i92_1283",
   "page_count": 3,
   "order": 347,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "The reported experiments were designed to study the contribution of prosodic cues to syntactic boundary categorization. The results indicate (1) that it is possible to differentiate between syntactic boundaries on the basis of prosodic cues alone (2) that the best results are obtained when pre- and post-boundary information is combined with information about the boundary itself (the silent interval) (3) that a fairly good categorization may be based exclusively on pre-boundary cues and (4) that the silent interval appears to be the stronger cue when in conflict with other prosodic cues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-345"
  },
  "taylor92_icslp": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "A new model of intonation for use with speech synthesis and recognition",
   "original": "i92_1287",
   "page_count": 4,
   "order": 348,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "This paper describes a synthesis from analysis scheme for producing natural sounding intonation for speech synthesis. The paper presents a new method of describing Fo contours in terms of three basic phonetic intonation elements. Details are given of an automatic system for labelling Fo contours, which could be used for speech recognition purposes. Current work on extracting a phonological description from this phonetic description is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-346"
  },
  "weiss92_icslp": {
   "authors": [
    [
     "Rudolf",
     "Weiss"
    ]
   ],
   "title": "Computerized error detection/correction in teaching German sounds: some problems and solutions",
   "original": "i92_1291",
   "page_count": 4,
   "order": 349,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "Some problems and solutions related to automated error detection/correction as they pertain to the improvement of foreign language pronunciation are discussed as well as their underlying premises. A model is proposed as a functional solution which is based on pedagogical framing and hierarchical error processing. The application of contrastive/applied phonetic principles both in the predicting of errors and the pedagogical framing of sounds allows the development of efficient analysis routines which do not rely upon full-spectrum matching and maximizes the chances for success.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-347"
  },
  "elgendy92_icslp": {
   "authors": [
    [
     "Ahmed M.",
     "Elgendy"
    ]
   ],
   "title": "Velum and epiglottis behavior during the production of Arabic pharyngeals and laryngeals: a fiberscopic study",
   "original": "i92_1295",
   "page_count": 4,
   "order": 350,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "An experiment was designed to account for the mechanism underlying the production of a set of speech sounds articulated in the back cavity of the vocal tract and to examine the assumption of possible interaction between nasal, pharyngeal and laryngeal articulation. The results obtained from 9 Egyptian speakers of Arabic by using a wide-angle fiberscope revealed that the velopharyngeal port was open during the production of the four pharyngeal consonants i.e. A, ft, Y/%/ and the laryngeals /?, h/ for all subjects in the non-nasal context. The degree of opening varies as a function of the constriction location in the pharynx and the height of the following vowel. Moreover, simultaneous active move- ment of the epiglottis together with a substantial mesial displacement of the lateral phayrngeal walls at the level of the laryngeaopharynx was also observed. The results suggest that the underlying mechanism used to produce pharyngeal segments (contrary to the common knowledge) involves a complex process and not solely retracting the back of the tongue toward the posterior wall as in the case of the pharyngealized (emphatic) consonants and that the possible interaction between nasal, pharyngeal and laryngeal articulation is conditioned by both aerodynamic and mechanical constraints.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-348"
  },
  "silverman92b_icslp": {
   "authors": [
    [
     "Kim",
     "Silverman"
    ],
    [
     "Eleonora",
     "Blaauw"
    ],
    [
     "Judith",
     "Spitz"
    ],
    [
     "John F.",
     "Pitrelli"
    ]
   ],
   "title": "A prosodic comparison of spontaneous speech and read speech",
   "original": "i92_1299",
   "page_count": 4,
   "order": 351,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "A persistent problem for keyword-driven speech recognition systems is that users often embed the to-be-recognized words or phrases in longer utterances. The recognizer needs to locate the relevant sections of the speech signal and ignore extraneous words. Prosody might provide an extra source of information to help locate target words embedded in other speech. In this paper we examine some prosodic characteristics of 160 such utterances and compare matched read and spontaneous versions. Half of the utterances are from a corpus of spontaneous answers to requests for the name of a city, recorded from calls to Directory Assistance operators. The other half are the same word strings read by volunteers attempting to model the real dialogue. Results show a consistent pattern across both sets of data: embedded city names almost always bear nuclear pitch accents and are in their own intonational phrases. However the distributions of tonal make-up of these prosodic features differ markedly in read versus spontaneous speech, implying that if algorithms that exploit these prosodic regularities are trained on read speech, then the probabilities are likely to be incorrect models of real-user speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-349"
  },
  "ohala92b_icslp": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ],
    [
     "Maria Grazia",
     "Busa"
    ],
    [
     "Karen",
     "Harrison"
    ]
   ],
   "title": "Phonological and psychological evidence that listeners normalize the speech signal",
   "original": "i92_1303",
   "page_count": 4,
   "order": 352,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "The great phonetic variability in functionally identical phonological units is one of the major problems in automatic speech recognition. Why isn't it also a problem when humans hear speech? The answer could be that human listeners somehow normalize the variable speech signal. We believe that many sound changes in the history of languages give evidence of listeners attempting to normalize heard speech, the process making itself apparent when it goes awry. We believe that dissimilation - removal of a feature from one of two sites in a word where it was distinctive - is due to the listener mistakenly attributing the feature's presence in one site as non-distinctive spill-over from the other site. We report attempts to duplicate two dissimilatory sound changes via speech perception experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-350"
  },
  "hinkelman92_icslp": {
   "authors": [
    [
     "Elizabeth A.",
     "Hinkelman"
    ]
   ],
   "title": "Intonation and the request/question distinction",
   "original": "i92_1307",
   "page_count": 4,
   "order": 353,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "Both linguistic intuitions and psychological evidence argue that intonation plays a role in the interpretation of spoken English utterances. The work presented here is a pilot study for exploration of this role. We suggest that intonation is one of several extrapositional linguistic features of an utterance which, along with extralinguistic information, signal that the content of an utterance is a question, assertion, request, greeting, or some other speech act. We present our model for recognition of speaker intentions. We show, using a forced-choice paradigm, that the request/question distinction can be intonationally disambiguated. We discuss the intonational features that may be responsible, and indicate the steps necessary to making use of these results in automated speech act recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-351"
  },
  "port92_icslp": {
   "authors": [
    [
     "Robert F.",
     "Port"
    ],
    [
     "Fred",
     "Cummins"
    ]
   ],
   "title": "The English voicing contrast as velocity perturbation",
   "original": "i92_1311",
   "page_count": 4,
   "order": 354,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "How do the discrete phonological units of the lexicon map onto continuous-time articulatory gestures and continuous-time auditory signals? The distinctive feature of [vm'ce] in syllable-coda position in English raises these questions with a vengeance. For minimal pairs like buzz/bus, clamber/clamper, tens/tense, etc, most measurable time intervals associated with the first syllable of these words are affected by the value of [voice]. Several of the rules in the traditional standard phonologies of English and many so-called 'phonetic implementation rules' serve to account for the various large and small temporal effects associated with the feature. We show that a very simple model for the English voicing contrast can be proposed that may account for these effects only if this phonological feature is phonetically defined as a velocity perturbation of a periodic dynamical system for English syllables. We summarize some evidence for the generalization that localized speaking-rate changes characterize a change in voicing. Then we suggest a general mathematical form for this dynamic effect that requires only a few parameters. This model implements the voicing feature as a perturbing forcing function for an underlying syllable oscillator.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-352"
  },
  "ziolkowski92_icslp": {
   "authors": [
    [
     "Michael S.",
     "Ziolkowski"
    ],
    [
     "Mayumi",
     "Usami"
    ],
    [
     "Karen L.",
     "Landahl"
    ],
    [
     "Brenda K.",
     "Tunnock"
    ]
   ],
   "title": "How many phonologies are there in one speaker? some experimental evidence",
   "original": "i92_1315",
   "page_count": 4,
   "order": 355,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "This paper addresses questions concerning the mental representation of language through the results of a study investigating the effects of various training methods for altering language performance. Specifically, we question whether a single phonological form accompanies an item in the lexicon (the standard assumption of linguistic theory) or whether each item bears two distinct phonological representations in the grammar - one underlying perception, the other driving production. We test these alternatives by examining whether subjects' performance on perception and production tasks is differentially affected by the training method used to teach phonological contrasts that are absent from learners' native system but distinctive in a foreign language. Speakers whose native language is American English were instructed in producing the phonemic quantity contrasts of Japanese by working with either a Level HI tape recorder, a native speaker of Japanese, or a computer that provided real-time visual comparisons between learners' productions and native Japanese models. Analysis of subjects' performance on a battery of pre- and post-training tests reveals that these various training methods may differentially tune production or perception acuity in a manner more consistent with the theory that lexical items carry two distinct phonological forms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-353"
  },
  "sato92_icslp": {
   "authors": [
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "Decomposition into syllable complexes and the accenting of Japanese loanwords",
   "original": "i92_1319",
   "page_count": 4,
   "order": 356,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "This paper describes accenting characteristics of Japanese loanwords, and proposes a strategy to determine accent types of these words. About four hundred fifty unitary loanwords are prepared having more than 5 morae. Six native speakers, with a Tokyo dialect, are instructed to assign the most separable intra-word juncture and accent location of the words. The experimental result shows that Japanese loanword accentuation is closely related to the word formation quasi-structure and the metrical structure formed by compounded syllables in the word. The syllable compounding indicates that Japanese loanwords are represented by sequences of syllable complexes, and a phonological structure can be given to a loanword without morphological structure. Thus, it is clarified that accentuation rules parallel Japanese compound word accent rules. Moreover, an additional new accenting rule is also proposed based on the Japanese metrical structure of unitary words. The newly obtained relation between syllable compounding and accentuation will successfully explain not only the previous rule that the accent is placed at the antepenultimate mora, but also general accenting characteristics including exceptions to the rule.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-354"
  },
  "cao92_icslp": {
   "authors": [
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "Temporal structure in bisyllabic word frame: an evidence for relational invariance and variability from standard Chinese",
   "original": "i92_1323",
   "page_count": 4,
   "order": 357,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "The effort of this paper is concentrated on searching for relational invariance in temporal distribution of the bisyllabic structure in Standard Chinese by observing the durational variabi I ity of bisyllabic words indifferent conditions with regard to context, position, stress type and so on in real speech. Experimental results indicate that both of durational variability of bisyllabic words and temporal distribution within the word in Chinese are determined by multiple influences from either higher or lower layers, however, the influence from morphophonemic contrast of word-stress is the most powerful one, and the other influences must be governed by such morphophonemic constraint. In this sense, temporal structure in Chinese is word-stress dominated, and as relational invariance, such word-stress based temporal patterns are relatively stable and kept as a fixed group in the continuous speech through intrinsic adjustment and compensation.Consider ing of the special status of bisyllabic words, this kind of patterns may be served as a miniature on timing of real speech in Standard Chinese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-355"
  },
  "wang92b_icslp": {
   "authors": [
    [
     "Shih-ping",
     "Wang"
    ]
   ],
   "title": "The integration of phonetics and phonology: a case study of taiwanese \"gemination\" and syllable structure",
   "original": "i92_1327",
   "page_count": 4,
   "order": 358,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "The aim of this paper is to integrate the phonetic experiments with the phonological theories. I will argue that there is no true gemination in Taiwanese and then explore Taiwanese syllable structure. It is commonly assumed that Taiwanese syllable-final /ptk/ sounds undergo gemination before the diminutive/nominalized suffix /a/, e.g., \"/ap-a/ -> [abba]\" ([3]). However, the results of our duration experiments reveal that the previous impressionistic view is untenable. Gemination does not in fact occur. Instead, /p,k/ undergo free variation, i.e., vocing or spirantization, while A/ is flapped. As for syllable break, resyllabification and foot-internal structure are separately proposed. To reinforce the above observations, the phenomena of quality (iptkJ), quantity (CIV length) and tone sandhi are provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-356"
  },
  "bradford92_icslp": {
   "authors": [
    [
     "James H.",
     "Bradford"
    ]
   ],
   "title": "Towards a robust speech interface for teleoperation systems",
   "original": "i92_1331",
   "page_count": 4,
   "order": 359,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "A number of recent papers on the design of speech interfaces have argued in favour of short command sequences. The argument usually proceeds as follows: suppose that a given speech recognizer has a word recognition rate of 95%. The probability of recognizing a 10 word utterance without error is about 60% (i.e. 0.9510). In a more realistic setting with background noise and speaker variability recognition rates often drop below 80%. With an 80% recognition rate, the probability of recognizing a 10 word utterance without error is only 11%! This, as well as the difficulties encountered with command confirmation and error correction dialogues tends to support a preference for short command sequences.\n",
    "However, this paper presents a design methodology which favours processing commands in larger aggregates called \"command paragraphs.\" The application of command paragraphs dramatically increases the amount of locally available contextual information. Typically a paragraph consists of a number of individual commands that when taken together, completely specify how a task is to be performed. Thus the grammar of the constituent commands and knowledge about how the commands work together to achieve a task can be used to correct recognition errors made on a word-by-word basis.\n",
    "It has been shown that in the context of traditional keyboard interfaces, command paragraphs posses powerful error recovery properties [1, 2].\n",
    "This paper describes a prototype which tests the paragraph approach with an interface using a speaker-dependent discrete-word recognizer. The prototype itself controls a simulated robotic arm which manipulates various objects in Earth orbit. The user can launch satellites, weld beams, etc., using only verbal commands. Feedback is provided by a computer driven animation. Initial experience with the prototype is very encouraging. In one demanding trial, a command paragraph specifying a weld on a free floating beam contained 10 separate command sentences for a total of 43 words. The test was conducted in a noisy environment and the operator was suffering a cold. As a result there were 17 recognition errors. A total of 16 errors were corrected by grammatical analysis and the remaining error by analysis of the task semantics. Thus the interface was able to deduce the correctly worded paragraph without recourse to correction dialogues. This is in contrast with the traditional approach which would have required 17 corrections on a word-by-word basis.\n",
    "The use of longer rather than shorter command sequences as input to speech interfaces represents a significant departure from the usual design paradigm. This paper describes a first attempt to study the effectiveness of the paragraph approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-357"
  },
  "cosi92_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "P.",
     "Frasconi"
    ],
    [
     "M.",
     "Gori"
    ],
    [
     "N.",
     "Griggio"
    ]
   ],
   "title": "Phonetic recognition experiments with recurrent neural networks",
   "original": "i92_1335",
   "page_count": 4,
   "order": 360,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "In order to prove the potential power of \"learning by examples\" paradigm for problems of Automatic Speech Recognition, an experiment was set up, regarding an extremely difficult Italian phonetic recognition problem: the automatic discrimination of the so called Italian alphabet i-set: /bi/, /tSi/, /di/, /dZi/, /!/, /pi/, /ti/, NM plus other two i-like stimuli /LI/, /si/. The achieved speaker independent mean recognition rate was around 65%, but either better results are expected at the end of the experiments going on.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-358"
  },
  "goldstein92b_icslp": {
   "authors": [
    [
     "Mikael",
     "Goldstein"
    ],
    [
     "Björn",
     "Lindström"
    ],
    [
     "Ove",
     "Till"
    ]
   ],
   "title": "Some aspects on context and response range effects when assessing naturalness of Swedish sentences generated by 4 synthesiser systems",
   "original": "i92_1339",
   "page_count": 4,
   "order": 361,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "Naturalness of 4 Swedish sentence-pairs generated by 4 speech synthesis systems as a function of (a) stimulus range 'context' presented to the Subjects (Ss) and (b) assessment method used (5-point Category Rating (CR) scale, 11-point CR scale or free number Magnitude Estimation (ME)) was assessed. Three stimulus range conditions were created by using natural speech as an internal (hidden) reference condition (only sentences generated by the 4 synthesiser systems systems were presented), using natural speech as a good external reference condition and using natural speech as both a good and bad reference. 72 Ss participated, divided into 3 groups, where group I assessed Naturalness of the 4 systems (no external reference, only internal), group II assessed the 4 systems + natural speech (good external reference) and group III assessed the 4 systems + natural speech + distorted natural speech (good and bad external reference), using both ME and CR scales (5- or 11-point). CRs of Naturalness are sensitive both to changes in response and stimulus range. An 11-point CR scale generates lower ratings for each of the 4 synthesiser systems than a 5-point rating scale (appr. -0.5 unit) regardless of stimulus range. Increasing stimulus range (by introducing natural speech and natural and distorted natural speech as part of the stimulus context), affects the Naturalness ratings of the 4 synthesis systems. For both CR scales (5- and 11-point), decreased ratings (-) as a function of increasing stimulus range were obtained for all 4 systems (up to - 1.0 unit). This decrease is quite contrary to what has been found for Picture Quality (Jones & Marks, 1985; CCIR, 1986), where increase in Picture Quality ratings (+) were obtained as a function of increasing stimulus range using 5-point CR scale as well as ME. The relationship ME of Naturalness and stimulus range was more complex. Naturalness was found to be a metathetic continuum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-359"
  },
  "pelillo92b_icslp": {
   "authors": [
    [
     "Marcello",
     "Pelillo"
    ],
    [
     "Franca",
     "Moro"
    ],
    [
     "Mario",
     "Refice"
    ]
   ],
   "title": "Probabilistic prediction of parts-of-speech from word spelling using decision trees",
   "original": "i92_1343",
   "page_count": 4,
   "order": 362,
   "p1": "1343",
   "pn": "1346",
   "abstract": [
    "The problem of tagging words with their parts-of-speech has received considerable attention in the last few years and several methods for solving it have been developed. Word labeling is usually accomplished by predicting, for each word, the list of its possible labels, and then making a selection on the basis of context. In this paper the use of probabilistic decision trees for part-of-speech prediction is proposed. The tree is automatically constructed using a recent partitioning algorithm that works in linear time, and then pruned with a generalized \"reduced-error\" algorithm. Preliminary experiments conducted over the LOB Corpus are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-360"
  },
  "barschdorff92_icslp": {
   "authors": [
    [
     "D.",
     "Barschdorff"
    ],
    [
     "U.",
     "Gartner"
    ]
   ],
   "title": "Single word detection system with a neural classifier for recognizing speech at variable levels of background noise",
   "original": "i92_1347",
   "page_count": 4,
   "order": 363,
   "p1": "1347",
   "pn": "1350",
   "abstract": [
    "This paper proposes a speech recognition system based on a neural network. The network with good generalising capabilities is used for speaker independent single word classification. The system is able to follow changes in the pronunciation by the same speaker and it can also be adapted to previously unlearnt speakers by a special retraining mode. Speaker independent recognition experiments have been performed using a vocabulary of 21 German words by male and female speakers. Using the proposed, retraining method, a high recognition accuracy of better than 90% is obtained after a few spoken words by an untrained speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-361"
  },
  "oviatt92_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ],
    [
     "Philip",
     "Cohen"
    ],
    [
     "Martin",
     "Fong"
    ],
    [
     "Michael",
     "Frank"
    ]
   ],
   "title": "A rapid semi-automatic simulation technique for investigating interactive speech and handwriting",
   "original": "i92_1351",
   "page_count": 4,
   "order": 364,
   "p1": "1351",
   "pn": "1354",
   "abstract": [
    "As computing moves in the direction of pocket-sized portable devices, speech and pen will become input modalities of choice. However, little currently is known about how people are likely to speak and write to future interactive systems. This paper describes a. new simulation technique designed to support a wide spectrum of empirical studies on the characteristics of spoken, handwritten, and combined pen/voice input to future interactive systems. The simulation technique aims: (1) to provide a tool for investigating interactive handwriting and pen systems, on which no simulation research currently is available, (2) to devise a technique appropriate for comparing people's use of speech and writing, such that differences between these communication modalities and their related technologies can be better understood, and (3) to support a very rapid exchange with simulated speech, pen, and pen/voice systems, such that interactions can be subject-paced. This paper outlines the specifications, general environment, and capabilities of a new semi-automatic simulation technique developed to achieve these goals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-362"
  },
  "chung92_icslp": {
   "authors": [
    [
     "Sang-Hwa",
     "Chung"
    ],
    [
     "Dan",
     "Moldovan"
    ]
   ],
   "title": "Speech understanding on a massively parallel computer",
   "original": "i92_1355",
   "page_count": 4,
   "order": 365,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "In this paper, we present a parallel computational model for the integration of speech and natural language processing (NLP). We have developed a parallel speech understanding system on the semantic network array processor (SNAP), a massively parallel computer developed at the University of Southern California. The parallel speech understanding algorithm is based on a memory-based parsing scheme. The key to the integration of speech and linguistic processing is the construction of a hierarchically structured knowledge base. The processing is carried out by passing markers parallelly through the knowledge base. Speech-specific problems like insertion, deletion, substitution, word boundary problems, and multiple hypotheses problems were analyzed and their parallel solutions were provided. The experimental results show that the processing time increases linearly with the length of target sentences, and increases logarithmically with the size of the knowledge base. This demonstrates that a massively parallel approach provides a viable platform for integrating speech and NLP on larger domains.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-363"
  },
  "lee92_icslp": {
   "authors": [
    [
     "Chan-Do",
     "Lee"
    ]
   ],
   "title": "Rationale for \"performance phonology\"",
   "original": "i92_1359",
   "page_count": 4,
   "order": 366,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "Most traditional theories presuppose abstract underlying representations ( URs) and a set of rules to explain phonological processes. There are, however, a number of questions regarding this approach: Where do URs come from? How are rules formed and related to each other? This paper proposes a new approach, \"performance phonology\", that does not require any explicit rules and URs. In this study, it is hypothesized that rules would emerge as the generalizations a connectionist network abstracts in the process of learning to associate forms with the meanings of the words and URs could emerge as a pattern on the hidden layer. Employing a simple recurrent network, a series of simulations on different types of phonological processes was run. The results show that this network is capable of learning various types of phonological phenomena without URs and explicit rules, thus providing the justification for \"performance phonology\".\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-364"
  },
  "koizumi92_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Jyoji",
     "Urata"
    ],
    [
     "Shuji",
     "Taniguchi"
    ]
   ],
   "title": "The effect of information feedback on the performance of a phoneme recognizer using kohonen map",
   "original": "i92_1363",
   "page_count": 4,
   "order": 367,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "This paper deals with a new phoneme recognition system with information feedback comprising a learning vector quantizer (LVQ) and hidden Markov models (HMMs). This system aims at achieving higher phoneme recognition rates than conventional LVQ-HMM systems, incorporating a feedback of information on the classification of input phoneme which is obtained from the out-put of the LVQ. The ability of this system has been investigated by a phoneme recognition experiment using a number of Japanese words uttered by a native male speaker in a quiet environment. The result of the experiment shows that recognition rates achieved with this system are higher than those accomplished with conventional LVQ-HMM phoneme recognizers which have no information feedback.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-365"
  },
  "asano92_icslp": {
   "authors": [
    [
     "Yasuharu",
     "Asano"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A method of dialogue management for the speech response system",
   "original": "i92_1367",
   "page_count": 4,
   "order": 368,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "A method is developed for the dialogue record management and the response content generation and is implemented on a computer as the dialogue processing subsystem for the speech response system. In order to realize a feasible system for the speech communication between human and machine, several functions are required for the subsystem. Since the current technology for continuous speech recognition is limited, the subsystem is designed to operate only with the information on the content words that may be obtained at a high recognition rate by the word spotting techniques. As for the configuration, the word dictionary and the topic-dependent rules are constructed to suit the topic specified by the knowledge base. Other parts of the subsystem can be constructed independently from the topic. This configuration leads to the high adaptability to the dialogue topic of the subsystem. A prototype for the dialogue processing subsystem is constructed, and smooth dialogue between user and system is performed, indicating the validity of the method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-366"
  },
  "takizawa92_icslp": {
   "authors": [
    [
     "Yumi",
     "Takizawa"
    ],
    [
     "Eiichi",
     "Tsuboka"
    ]
   ],
   "title": "Syllable duration prediction for speech recognition",
   "original": "i92_1371",
   "page_count": 4,
   "order": 369,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "In speech recognition using HMM, several methods have been proposed for controlling the state or word duration and their effectiveness is well known. However these methods model the duration of each state or word only, and don't consider the relation among durations of separate words within a sentence or separate states within a word. This paper proposes a new method of syllable duration control for continuous Japanese speech recognition. It constrains the syllable duration using the relation of among each of syllables, and this method is effective even if the speed of speech changes. At first the syllable duration is predicted by using the matching periods which have already been spotted and using speaker independent factors which affect syllable duration. Next, the matching period of the predicted syllable is constrained using predicted duration. Using 50 sentences and 10 speakers, we evaluate the performance of prediction and recognition. As a result, this method improves sentence recognition rate by 5.2%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-367"
  },
  "canavesio92_icslp": {
   "authors": [
    [
     "F.",
     "Canavesio"
    ],
    [
     "G.",
     "Castagneri"
    ],
    [
     "G. Di",
     "Fabbrizio"
    ],
    [
     "F.",
     "Senia"
    ]
   ],
   "title": "Comparison between two methodologies of testing isolated word speech recognizers",
   "original": "i92_1375",
   "page_count": 4,
   "order": 370,
   "p1": "1375",
   "pn": "1378",
   "abstract": [
    "The goal of this paper is to compare two different methodologies for automatic testing of isolated word speech recognisers. They are usually tested by playing single speech tokens and collecting the device response. The second approach, allows to output speech files containing lists of words and simultaneously to collect recogniser answers. It ensures a more realistic testing of the performance of any kind of isolated word speech recogniser. The results obtained are significantly lower than the ones got with isolated word testing.\n",
    "Furthermore many recognisers, designed to work in noisy environment, cannot be correctly tested in isolation as they need to be continuously fed by background noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-368"
  },
  "jun92_icslp": {
   "authors": [
    [
     "He",
     "Jun"
    ],
    [
     "Henri",
     "Leich"
    ]
   ],
   "title": "Extracting fuzzy features from MLP for recognition of speech",
   "original": "i92_1379",
   "page_count": 4,
   "order": 371,
   "p1": "1379",
   "pn": "1382",
   "abstract": [
    "In this paper, fuzzy information embedded in the MLP classifier is analyzed. Membership function, which represents the degree to which an input pattern belongs to a class, is derived from network output. Analysis show that our membership feature vector can effectively represent the grade of membership without being influenced by the sharp decision boundary formed by MLP trained as classifier. New hybrid system integrating MLP into Hidden Markov Model (HMM) is also proposed 1 and a French vowel recognition experiment has been done based on this hybrid system for comparison of membership feature vector with acoustic feature vector.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-369"
  },
  "fukuzawa92_icslp": {
   "authors": [
    [
     "Keiji",
     "Fukuzawa"
    ],
    [
     "Yoshinaga",
     "Kato"
    ],
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "A fuzzy partition model (FPM) neural network architecture for speaker-independent continuous speech recognition",
   "original": "i92_1383",
   "page_count": 4,
   "order": 372,
   "p1": "1383",
   "pn": "1386",
   "abstract": [
    "This paper proposes a Fuzzy Partition Model (FPM) neural network architecture for speaker-independent continuous speech recognition. Generally speaking, conventional TDNN (Time-Delay Neural Network) architecture in its training stage requires much computation time. Nevertheless, an FPM has a rapid training capability that is over two times faster than TDNN's training speed. FPM architecture is combined with an LR-parser and its recognition performance with 278 Japanese phrases is evaluated. The recognition rate of FPM-LR is higher than that of TDNN-LR. This paper also proposes a Multi-FPM-LR method. Using this method, the recognition rate is 77.5% for open speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-370"
  },
  "ennaji92_icslp": {
   "authors": [
    [
     "A.",
     "Ennaji"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Conception of speech filters based on a neural network",
   "original": "i92_1387",
   "page_count": 4,
   "order": 373,
   "p1": "1387",
   "pn": "1390",
   "abstract": [
    "This paper is concerned with the recognition and the restoration of periodic signal sequences by a neural network. It describes a method for noise reduction using a feed-forward neural network. We will show the abilities of the network to recognize complex and multiple sequences, to regenerate sequences with an appropriate precision and to recognize and generate sequences despite the existence of noise. Moreover, we will discuss the ability of the feed-forward network to generalize the amplitude and the frequency. This ability will be exploited to design filters as low-pass, high-pass, band-pass and I band-stop. The system has been realized with a reduced architecture trained by the fast back propagation algorithm which yields a fast convergence by using short learning sequences made of pure tones (10 ms learning sequences).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-371"
  },
  "kuo92_icslp": {
   "authors": [
    [
     "Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Aaron E.",
     "Rosenberg"
    ]
   ],
   "title": "Speaker set identification through speaker group modeling",
   "original": "i92_1391",
   "page_count": 4,
   "order": 374,
   "p1": "1391",
   "pn": "1394",
   "abstract": [
    "Speaker identification has traditionally been performed by pattern matching an utterance from an unknown speaker to all the speaker models in the database. By clustering speakers into a fixed number of groups and performing speaker group identification instead, the computation required may be much less when the population of enrolled speakers becomes very large in size. This paper introduces the idea of speaker set identification and ways to efficiently accomplish speaker identification over a large population of speakers through speaker set identification and speaker clustering.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-372"
  },
  "springer92_icslp": {
   "authors": [
    [
     "Stephen",
     "Springer"
    ],
    [
     "Sara",
     "Basson"
    ],
    [
     "Judith",
     "Spitz"
    ]
   ],
   "title": "Identification of principal ergonomic requirements for interactive spoken language systems",
   "original": "i92_1395",
   "page_count": 4,
   "order": 375,
   "p1": "1395",
   "pn": "1398",
   "abstract": [
    "Successful deployment of speech understanding systems demands an understanding of all relevant aspects of human/machine interaction. By determining the factors which most significantly affect user behavior, and quantitatively describing the effects of varying these factors, we can more accurately control and predict a system's ultimate performance under real user conditions. This paper describes the design of an automated system, Money Talks, which will determine speech recognition and application requirements in a real user, service-providing environment: a telephone-based information service. We discuss the reasoning and implications underlying our design decisions to investigate certain aspects of interaction, while not investigating others. We also discuss the trade-offs between \"naturalness,\" efficiency, and the effect on speaker compliance associated with wording variations in prompts. Lastly, issues related to the design of such trials, such as the need for barge-in technology, realistic simulations of graceful failure, and operator-handoff procedures, are also addressed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-373"
  },
  "jacobs92_icslp": {
   "authors": [
    [
     "Thomas E.",
     "Jacobs"
    ],
    [
     "Eric R.",
     "Buhrke"
    ]
   ],
   "title": "Performance of the united kingdom intelligent network automatic speech recognition system",
   "original": "i92_1399",
   "page_count": 4,
   "order": 376,
   "p1": "1399",
   "pn": "1402",
   "abstract": [
    "An automatic speech recognition (ASR) system was developed as i part of AT&T's Intelligent Network in the United Kingdom for British Telecom. This work describes the algorithm development and presents the ASR recognition accuracy measured during an independent, country-wide performance trial of the system conducted in England in 1991. The hidden Markov model based system recognizes the words one, two, three, four, yes and no across eighteen English dialects in the UK. The country-wide isolated word performance of the system is 98.3%. Its performance on vocabulary words embedded in extraneous speech and noise is 94.4%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-374"
  },
  "deville92_icslp": {
   "authors": [
    [
     "Guy",
     "Deville"
    ],
    [
     "Pierre",
     "Mousel"
    ]
   ],
   "title": "Evaluation of parsing strategies in natural language spoken man-machine dialogue",
   "original": "i92_1403",
   "page_count": 4,
   "order": 377,
   "p1": "1403",
   "pn": "1406",
   "abstract": [
    "This paper presents a detailed evaluation of the parsing strategies of a syntactic-semantic component (SYNSEM) within a natural language spoken man- machine dialogue system. After giving a brief account of the DIAL dialogue system, we present the evaluation methodology which has been applied to a representative sample of utterances. The sample, the parsing strategies and results are extensively discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-375"
  },
  "niimi92_icslp": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Yutaka",
     "Kobayashi"
    ]
   ],
   "title": "An information retrieval system with a speech interface",
   "original": "i92_1407",
   "page_count": 4,
   "order": 378,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "This paper describes a speech interface to an information retrieval system. It consists of three main components; speech recognition system, command generator and response generator. The speech recognition system accepts a spoken command of a Japanese sentence and passes the recognized sentence to the command generator, which translates it into a formal query command to operate the information retrieval system. The response generator receives retrieved data and produces a response to the user in a written sentence. We proposed a basic strategy in construction of the speech recognition system. It is that the top-down linguistic hypotheses are made at the lexical level while they are verified by using units independent of the word, phonetic strings bounded by robust phones (phones which can reliably be detected) in order to reduce the misrecognition of short function words. In the natural language interface, syntactic and semantic analyses are simultaneously performed. This makes it possible to resolve syntactic ambiguities. The interface was tested by using the speech corpus of 53 sentences spoken by each of three male speakers. The most promising rate of sentence understanding was 89.9 % for a small task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-376"
  },
  "eatock92_icslp": {
   "authors": [
    [
     "J. P.",
     "Eatock"
    ],
    [
     "J. S. D.",
     "Mason"
    ]
   ],
   "title": "Phoneme performance in speaker recognition",
   "original": "i92_1411",
   "page_count": 4,
   "order": 379,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "It is recognised that some phonemes are more useful for speaker recognition than others. For example, it is commonly accepted that unvoiced phonemes are less useful than voiced ones. This paper overviews published work investigating the speaker-discriminating properties of phonemes. Following this, a preliminary experiment is described, in which the performances of nasals and unvoiced fricatives are compared, using an on-line transputer-based speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-377"
  },
  "tzoukermann92_icslp": {
   "authors": [
    [
     "Evelyne",
     "Tzoukermann"
    ],
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Zakhar",
     "Gorelov"
    ]
   ],
   "title": "Natural language processing in the chronus system",
   "original": "i92_1415",
   "page_count": 4,
   "order": 380,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "This paper describes the natural language processing component of a speech understanding system. The CHRONUS system was designed for the ATIS (Air Travel Information Service) task 1. The overall project consists of building a speech understanding system within the domain of airline reservations. Ideally, a user communicates by speaking to a computer to inquire about flights, fares, and other travel information. This task differs drastically from the previous speech understanding projects because the input to the system is spontaneous speech and the vocabulary is unlimited, even though the domain is restricted. In the framework of our system, laid out in Pieraccini et al. [1, 2], the natural language processing front end to a relational database is described. The original aspect of this approach is that statistical-based algorithms are combined with linguistic knowledge to achieve the understanding task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-378"
  },
  "francois92_icslp": {
   "authors": [
    [
     "Dominique",
     "Francois"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Contribution of neural networks for phoneme identification in the APHODEX expert system",
   "original": "i92_1419",
   "page_count": 4,
   "order": 381,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "We propose in this study a way to integrate neural networks by an acoustic-phonetic decoder expert system. The goal of this coupling is to improve the recognition rate of plosive and fricative consonants. We first show that the work we carried out aimed at improving the efficiency of the phonetic knowledge base. In a second step we present a new method based of multi-layer perceptrons. This is being used to recognise plosives or fricatives and to be coupled to the expert system. We conclude by discussing how to make this cooperation possible in a hybrid system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-379"
  },
  "paul92b_icslp": {
   "authors": [
    [
     "Douglas B.",
     "Paul"
    ]
   ],
   "title": "A CSR-NL interface architecture",
   "original": "i92_1423",
   "page_count": 3,
   "order": 382,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "Spoken Language Systems will require integration of continuous speech recognition and natural language processing. This is a proposed architecture for an interface between a continuous speech recognizer (CSR) and a natural language processor (NLP) to form a spoken language system. Both components are integrated with a stack controller and contribute to the search control. The architecture also allows a \"Top-N\" mode in which a \"first part\" outputs a list of top N scored sentences for post-processing by a \"second part\". An additional use for this architecture might be NLP evaluation testing: a common simulated CSR could be interfaced to each site's NLP to provide identical testing environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-380"
  },
  "lefebvre92b_icslp": {
   "authors": [
    [
     "R.",
     "Lefebvre"
    ],
    [
     "F.",
     "Poirier"
    ],
    [
     "G.",
     "Duncan"
    ]
   ],
   "title": "Speech interface for a man-machine dialog with the unix operating system",
   "original": "i92_1427",
   "page_count": 4,
   "order": 383,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "Natural language and graphic interface techniques go some way towards facilitating competent, reliable use of the Unix operating system. Such methods, however, remain insufficient particularly where newcomers to the use of Unix are concerned. This paper presents a novel, multimodal approach to interaction with the Unix operating system. The main aim of this type of interface is to provide for an optimum combination of various communication modalities (voice, keyboard, graphics) within an integrated architecture, so as to promote a more natural man-machine dialog. Here, the role of each mode is defined together with their complementarity within a multimodal architecture. The multimodal event analyser is then presented, together with preferred options in respect of the communication modalities exercised for system response.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-381"
  },
  "bardaud92_icslp": {
   "authors": [
    [
     "P.",
     "Bardaud"
    ],
    [
     "F.",
     "Capman"
    ],
    [
     "C.",
     "Mokbel"
    ],
    [
     "C.",
     "Tadj"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Transformation of databases for the evaluation of speech recognizers",
   "original": "i92_1431",
   "page_count": 4,
   "order": 384,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "This paper proposes two simulation techniques of the Lombard effect. These simulations are based on the Linear Prediction Coding (LPC) and the Linear Multiple Regression (LMR) methods. The LPC model determines a synthesis filter. The LPC coefficients filter are obtained by the processing of the spectral noise signal This transformation simulates both enhancement spectral tilt and a relative amplification of speech spectral frequency bands where maximum of noise energy exists. This approximates the Lombard effect. These experiments are used to test the limits of SAMREC1 (ENST's DTW recognizer system) in presence of Lombard effect. The second technique is based on the learning of the spectral transformation from the database reference without noise to the same database but recorded with the Lombard effect simulated in the laboratory. This treatment is optimized by dynamic programming.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-382"
  },
  "yamashita92_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Dialog management for speech output from concept representation",
   "original": "i92_1435",
   "page_count": 4,
   "order": 385,
   "p1": "1435",
   "pn": "1438",
   "abstract": [
    "Building a general speech interface independent of Intelligent Performance Systems (IPS) is an important technique for efficient construction of spoken dialog systems. This paper describes dialog management for speech output in MASCOTS-II which is a dialog manager in such an interface system. MASCOTS-II carries out dialog management based on two mechanisms; SR-plan analyzes the dialog structure of utterances and TPN (Topic Packet Network) models the topic transition in dialog. In speech output of this framework, output messages from IPS are described with concept representation which is based on the case structure and the phrase patterns. MASCOTS-II receives concept representation from IPS and modifies it according to the dialog context. Modified concept representation is converted into synthetic speech by SOCS, a concept-to-speech conversion system in the interface. MAS-COTS-II extracts emphatic words to add prominence so that the user pays his/her attention to important information in the IPS utterance. It also has a mechanism for restoring missing words in IPS utterance to notify the recognition result to the user. The concept representation is rewritten in terms of phrase pattern named custom template, if it is possible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-383"
  },
  "hangai92_icslp": {
   "authors": [
    [
     "Seiichiro",
     "Hangai"
    ],
    [
     "Shigetoshi",
     "Sugiyama"
    ],
    [
     "Kazuhiro",
     "Miyauchi"
    ]
   ],
   "title": "Speaker verification using locations and sizes of multipulses on neural networks",
   "original": "i92_1439",
   "page_count": 4,
   "order": 386,
   "p1": "1439",
   "pn": "1442",
   "abstract": [
    "Locations and sizes of multipulses, which is used for driving a LPC vocal tract filter in speech synthesis, are clustered and applied to speaker verification. In clustering multipulse information, a modified K-mean algorithm is adopted. Three types of neural networks which consist of input layer with 90 ports, hidden layer with 1, 2 and 5 units and output layer with one port is also considered in verification process. After enough training using multipulse information of 70 speakers' 5 vowels, 94% of correct acceptance is achieved by determining optimum threshold.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-384"
  },
  "teixeira92_icslp": {
   "authors": [
    [
     "Carlos J.",
     "Teixeira"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Word rejection using multiple sink models",
   "original": "i92_1443",
   "page_count": 4,
   "order": 387,
   "p1": "1443",
   "pn": "1446",
   "abstract": [
    "The purpose of this work is two-folded: to improve the robustness of a baseline isolated-word recogniser with a medium-sized vocabulary in terms of word rejection capabilities and to verify these improvements on a multi-application database including different native and non-native English accents. Although the results obtained so far could be biased by the unavailability of realistic corpora, they seem to indicate the usefulness of multiple sink models in this context, relative to the use of a single one. The improvements are particularly evident in multi-accent environments, where our results show that merging two different accents in the training material yields scores which are similar to the ones obtained in a single accent environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-385"
  },
  "lindberg92_icslp": {
   "authors": [
    [
     "Boerge",
     "Lindberg"
    ]
   ],
   "title": "Verification of language specific performance factors from recogniser testing on EUROM.1 CVC material",
   "original": "i92_1447",
   "page_count": 4,
   "order": 388,
   "p1": "1447",
   "pn": "1450",
   "abstract": [
    "This paper reports on the investigation of diagnostic and predictive assessment techniques in testing a recogniser on the EUROM.1 speech database, produced within the ESPRIT Project 2589 SAM.\n",
    "Based on test results obtained on the full set of CVC-type (Consonant-Vowel-Consonant) words contained within the Danish Few Talker Part of the database, three different assessment techniques are investigated in this paper. In the first, the substitution errors encountered are related to an analysis of the Danish phoneme inventory in terms of a representation of the phonetic distinctive features and the results show that many frequent substitutions found in a con-f fusion matrix may be predicted as seen from the distinctive-feature representation.\n",
    "In the second, a statistical analysis is carried out in which the asymmetric confusion matrices are transformed into symmetrical similarity matrices and applied in an individual differences multi-dimensional scaling analysis. This data-driven characterisation of the phoneme-inventory is illustrated in two-dimensional plots, and a high degree of correspondence with the distinctive feature map of phonemes is observed. This indicates possibilities of calibrating the performance of a recogniser against a given language by specifying the limitations of the recogniser in terms of phonetic dimensions specific to the language.\n",
    "In the third, the estimated phoneme distances from testing a target recogniser on the EUROM.1 CVC material are used in a recogniser response model in order to aim at predicting the performance of the target recogniser on a different vocabulary. A limited experiment concludes that the observed performance of the target recogniser on the different vocabulary does not vary significantly from the predicted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-386"
  },
  "cozannet92_icslp": {
   "authors": [
    [
     "Alain",
     "Cozannet"
    ]
   ],
   "title": "Modeling task driven oral dialogue",
   "original": "i92_1451",
   "page_count": 4,
   "order": 389,
   "p1": "1451",
   "pn": "1454",
   "abstract": [
    "Starting from a first dialogue system, using a finite state automaton to perform oral cooperative flight enquiries, a new system has been designed to reach an extended cooperative performance by using a generic model of the task, and a rational use of knowledge to derive the new goal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-387"
  },
  "li92_icslp": {
   "authors": [
    [
     "Wei-ying",
     "Li"
    ],
    [
     "Kechu",
     "Yi"
    ],
    [
     "Zheng",
     "Hu"
    ]
   ],
   "title": "Introducing neural predictor to hidden Markov model for speech recognition",
   "original": "i92_1455",
   "page_count": 4,
   "order": 390,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "In this paper, Neural Predictors (NP) are introduced to Hidden Markov Models (HMM) to form a new model HMM / NP for speech recognition. In HMM/NP, each NP is a Multilayer Perceptron (MLP) used as a separate nonlinear predictor, and corresponds to a state in the model. Training and recognition algorithms are given based on Baum-Welch and Back-Propagation (BP) algorithms.Speaker-dependent Mandarin digit recognition experiments were carried out. The performance of forward prediction HMM / NP model and forward-backward prediction HMM / NP model was comparied and a recognition accurcy of 96.2% and 98.7% was obtained respectively. The result indicates that it is effective to use HMM/NP for Mandarin speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-388"
  },
  "liu92b_icslp": {
   "authors": [
    [
     "Feng",
     "Liu"
    ],
    [
     "Jianxin",
     "Jiang"
    ],
    [
     "Jun",
     "Cheng"
    ],
    [
     "Kechu",
     "Yi"
    ]
   ],
   "title": "A neural network based on subnets - SNN",
   "original": "i92_1459",
   "page_count": 4,
   "order": 391,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "In this paper, we present a new kind of neural network model based on subnets-SNN. The network is composed of a set of subnets and a decision layer. Each subnet is a Multilayer Perceptron that has two outputs units and used as a simple pattern classifier. The decision layer takes all the outputs of the subnets into account, and makes the final decision. We also give an training algorithm. Each pattern can be trained independently, which is possible to train a NN on a personal computer. In the paper, we compared SNN with the MLP, and show that SNN greatly decreases the compl exity of the networks. Evaluation experiments were conducted, using 10 Chinese vowel syllables. The results show that the SNN is effective and has more potential in the speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-389"
  },
  "ziegenhain92_icslp": {
   "authors": [
    [
     "Ute",
     "Ziegenhain"
    ]
   ],
   "title": "Syntactic anaphora resolution in a speech understanding system",
   "original": "i92_1569",
   "page_count": 4,
   "order": 392,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "In this paper a way to treat anaphoric processes in a natural language dialogue system is proposed. It makes extensive use of syntactic and semantic information to find the right antecedents for anaphoric expressions. Using the proposed method it can be shown that the right candidate for an anaphoric expression can be found in 89% out of 300 dialogues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-390"
  },
  "mast92_icslp": {
   "authors": [
    [
     "Marion",
     "Mast"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Noth"
    ]
   ],
   "title": "The dialog module of the speech recognition and dialog system EVAR",
   "original": "i92_1573",
   "page_count": 4,
   "order": 393,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "This article describes the dialog module of EVAR. The dialog is seen as a sequence of dialog acts uttered by the system and the user. From a corpus of real human-human dialogs a model was extracted. The model covers all sequences of dialog acts observed in the corpus. Each dialog act is modelled as a set of pragmatic, semantic and syntactic concepts. The properties of the concepts and the current dialog state are used to identify the actual dialog act. Each user utterance is interpreted by the system and the system reacts with the appropriate dialog act. After the user has defined his request completly, the system starts a database query and finally generates a synthesized natural language answer. The answer generation is realised with sentence masks, where the information from the database enquiry is filled in at the appropriate slots. A dialog memory which allows the interpretation of elliptical sentences and proforms is updated after each utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-391"
  },
  "cheng92b_icslp": {
   "authors": [
    [
     "Yan Ming",
     "Cheng"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Paul",
     "Mermelstein"
    ]
   ],
   "title": "Statistical recovery of wideband speech from narrowband speech",
   "original": "i92_1577",
   "page_count": 4,
   "order": 394,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "We present an algorithm to generate wideband speech from a narrowband version of the same. The main body of the algorithm is a Statistical Recovery Function (SRF), which predicts the highband spectrum based on the narrowband spectrum. Assuming that bandpass portions of the speech are generated completely by a fixed number of random sources, the SRF explores the dependency among the random sources. The performance of the algorithm has been measured both in terms of spectral distortion and spectral signal-to-noise ratio (SNR). We obtained a 3 dB gain in SNR for the reconstructed wideband speech as compared to the narrowband speech. Informal perceptual experiments indicate a significant preference for the reconstructed speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-392"
  },
  "heuvel92_icslp": {
   "authors": [
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Toni",
     "Rietveld"
    ]
   ],
   "title": "Speaker related variability in cepstral representations of dutch speech segments",
   "original": "i92_1581",
   "page_count": 4,
   "order": 395,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "Speaker variation was examined in cepstral vectors extracted from 24 /CVCq/ nonsense words read by five male and five female speakers. Dynamic Time Warping (DTW) was used for segment synchronization and a ratio of inter/intra speaker distances was employed as a measure of speaker specificity. The following rank order of increasing speaker specificity was found: plosives + /r/; fricatives; short vowels; nasals; long vowels. Transition segments proved to be less speaker specific than vowel steady states. The factor speaker sex was not found to be of relevance. These results may contribute to the fields of automatic speaker identification/verification and automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-393"
  },
  "rosenbeck92_icslp": {
   "authors": [
    [
     "Per",
     "Rosenbeck"
    ],
    [
     "Bo",
     "Baungaard"
    ]
   ],
   "title": "Experiences from a real-world telephone application: teledialogue",
   "original": "i92_1585",
   "page_count": 4,
   "order": 396,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "Unpredictable challenges emerge from the task of moving speech technology applied in laboratories to applications working in the telephone network. This paper summarizes experiences and results achieved from a real-world telephone application that makes use of an advanced user-dialogue and recognition of isolated words using the Continuous Hidden Markov Model approach. The service developed at Jydsk Telefon, Denmark, is named teleDialogue. It has been running as a commercial service for a field trial period of one year (May 91 to May 92), serving more than 1 million subscribers. TeleDialogue enables call establishment to six commonly used telecommunication services in Denmark simply by pronouncing the names of these services. In this way teleDialogue integrates these six services into one that can be reached by calling one specific service number (0020). In order to achieve information on the use of teleDialogue, it is designed to give comprehensive statistics on the use of the service. Furthermore it is possible to monitor and even record user dialogues with the service.\n",
    "Even if the recognition performance in the laboratory is higher than 95% (SAM standard), the successful transaction rate in the real-world drops to 85%, meaning that 85 of 100 initiated calls are directed to the desired service.\n",
    "A sample material, obtained by listening to user dialogues, has been selected to reveal the reasons for this performance drop i.e. extraneous speech, background noise, utterance input level and utterances given by children. Each of these subjects are discussed thoroughly in the paper.\n",
    "Jydsk Telefon is, with teleDialogue, among the first in Europe to integrate speech technology into real-world telephone applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-394"
  },
  "lee92b_icslp": {
   "authors": [
    [
     "K. Y.",
     "Lee"
    ],
    [
     "P.",
     "Ha"
    ],
    [
     "J.",
     "Rheem"
    ],
    [
     "S.",
     "Ann"
    ],
    [
     "I.",
     "Song"
    ]
   ],
   "title": "Robust estimation of time-varying LP parameters on speech",
   "original": "i92_1589",
   "page_count": 4,
   "order": 397,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "We investigate a robust parametric approach to the estimation of time-varying AR parameters in speech. The time-varying parameters are assumed to be linear combinations of a set of basis time functions so that the model can be specified by constant parameters. The parameters of the linear combination of functions are obtained by the M-estimator defined by the minimization of an nonquadratic function of normalized residuals. We have demonstrated by computer simulation on synthetic data that the proposed method is less sensitive to variations of the pitch frequency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-395"
  },
  "hernando92_icslp": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "On the AR modelling of the one-sided autocorrelation sequence for noisy speech recognition",
   "original": "i92_1593",
   "page_count": 4,
   "order": 398,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "Speech recognition in noisy environments remains an unsolved problem even in the case of isolated word recognition with small vocabularies. Recently, several techniques have been proposed to alleviate this problem. Concretely, two closely related parameterization techniques based on an AR modelling in the autocorrelation domain called SMC [1] and OSALPC [2] have shown good results using speech contaminated by additive white noise. The aim of this paper is twofold: to compare several techniques based on an AR modelling in the autocorrelation domain, including SMC and OSALPC, and to find the optimum model order and cepstral liftering for noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-396"
  },
  "shimodaira92_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Mitsuru",
     "Nakai"
    ]
   ],
   "title": "Robust pitch detection by narrow band spectrum analysis",
   "original": "i92_1597",
   "page_count": 4,
   "order": 399,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "This paper proposes a new technique for detecting pitch patterns which is useful for automatic speech recognition, by using a narrow band spectrum analysis. The motivation of this approach is that humans perceive some kind of pitch in whispers where no fundamental frequencies can be observed, while most of the pitch determination algorithm (PDA) fails to detect such perceptual pitch. The narrow band spectrum analysis enable us to find pitch structure distributed locally in frequency domain. Incorporating this technique into PDA's is realized to applying the technique to the lag window based PDA. Experimental results show that pitch detection performance could be improved by 4% for voiced sounds and 8% for voiceless sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-397"
  },
  "eady92_icslp": {
   "authors": [
    [
     "S.",
     "Eady"
    ],
    [
     "B. Craig",
     "Dickson"
    ],
    [
     "Roy C.",
     "Snell"
    ],
    [
     "J.",
     "Woolsey"
    ],
    [
     "P.",
     "Ollek"
    ],
    [
     "A.",
     "Wynrib"
    ],
    [
     "J.",
     "Clayards"
    ]
   ],
   "title": "A microcomputer-based system for real-time analysis and display of laryngograph signals",
   "original": "i92_1601",
   "page_count": 4,
   "order": 400,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "This paper describes a microcomputer-based system that is being developed to analyze and display laryngograph signals for clinical applications. The system uses commercially-available hardware to digitize and analyze laryngograph signals. After digitization, the signal is smoothed using a moving-average technique, in order to remove float caused by vertical movement of the larynx during phonation. Digital-signal-processing techniques are then used to analyze and display laryngeal parameters including pitch, open quotient, jitter and periodicity index (as measured by cepstral amplitude). The system provides real-time analysis and display for some of the above parameters, and long-term averaging and statistical analysis for others. The system is designed for use by speech clinicians, and it is being tested with normal and pathological voices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-398"
  },
  "veilleux92_icslp": {
   "authors": [
    [
     "N. M.",
     "Veilleux"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Colin",
     "Wightman"
    ]
   ],
   "title": "Parse scoring with prosodic information",
   "original": "i92_1605",
   "page_count": 4,
   "order": 401,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "The relative size and location of prosodic phrase boundaries provides an important cue for resolving syntactic ambiguity, and can be used to improve the accuracy of automatic speech understanding. This paper describes an approach to scoring candidate sentence hypotheses and associated parses using prosodic phrase cues. Specifically, for each hypothesized parse, prosodic breaks are automatically detected and the probability of these breaks given the parse is computed based on a stochastic model of the prosody/syntax relationship. The parse probability can be used to rank sentence hypotheses and associated parses, optionally in combination with other scores. Both the prosodic break recognition algorithm and the prosody /syntax model can be automatically trained and can therefore be designed specifically for different speaking styles or task domains, given appropriate labeled data. We have demonstrated the potential of this approach in experiments with a corpus of ambiguous sentences spoken by FM radio announcers. Disambiguation performance is comparable to that of human listeners using the algorithm with hand-labeled breaks, although there is some performance degradation with the fully automatic system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-399"
  },
  "cheng92c_icslp": {
   "authors": [
    [
     "Ying",
     "Cheng"
    ],
    [
     "Paul",
     "Fortier"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "Topic identification using a neural network with a keyword-spotting preprocessor",
   "original": "i92_1609",
   "page_count": 4,
   "order": 402,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "This paper proposes a topic indentifier which uses a multi-layer perceptron (MLP) with a keyword-spotting preprocessor. The keyword-spotting preprocessor is designed to locate the keywords that appear in the input sentences and assigns each of them to one class among a pre-determined set of semantic classes. A three layer perceptron is then trained to identify the input sentence's topic, the network's input units represent semantic classes and its output units correspond to the topics.\n",
    "The experiments on AITS0, the pilot corpus from the Air Travel Information System (ATIS) corpora, show 91.01% classifying accuracy for the test set when transcripts are used as input, and 88.76% accuracy when using the output from CRIM's spontaneous speech recognition system[l].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-400"
  },
  "switzer92_icslp": {
   "authors": [
    [
     "Shane",
     "Switzer"
    ],
    [
     "Tim",
     "Anderson"
    ],
    [
     "Matthew",
     "Kabrisky"
    ],
    [
     "Steven K.",
     "Rogers"
    ],
    [
     "Bruce",
     "Suter"
    ]
   ],
   "title": "Frequency domain speech coding",
   "original": "i92_1613",
   "page_count": 4,
   "order": 403,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "This paper describes research undertaken to investigate speech coding techniques that attempt to achieve high quality speech transmittable at 4800 and 2400 bits per second (bps). The approach taken is to code the raw frequency domain representation of speech sampled at 8 kHz. Speech is represented by a sparse set of frequency components. Four frequency selection schemes are implemented and the resulting frequency coefficients (magnitude and phase) are coded in an efficient manner for transmission. Specific techniques involved in the speech coder include: (J) a recurrent neural network to make periodic/noiselike decisions, (2) variable length windows for analysis and synthesis, and (3) representation of noiselike speech using frequency banded energy information. The quality of the reconstructed speech is evaluated using listening tests which compare speech produced using the different frequency selection schemes along with the original and sampled versions of the test utterances. Although the system does not achieve \"toll quality\" speech, the resulting speech is intelligible as shown by the scores on a Modified Rhyme Test (MRT). At various noise levels, MRT scores for the reconstructed speech are not significantly lower than those achieved by the McAulay/Quatieri sinusoidal coder. An attempt is made to lower the transmission bit rate to 2400 bps by decreasing the number of frequency coefficients per unit time. The resulting speech quality suffers, but the MRT results show little degradation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-401"
  },
  "descout92_icslp": {
   "authors": [
    [
     "Raymond",
     "Descout"
    ],
    [
     "Robert",
     "Bergeron"
    ],
    [
     "Bernard",
     "Meriald"
    ]
   ],
   "title": "MEDIATEX-TASF: a closed captioning real-time service in French",
   "original": "i92_1617",
   "page_count": 4,
   "order": 404,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "MEDIATEX-TASF is a recent development undertaken for Canadian Broadcasting Corporation's French network by a joint effort from Canadian Workplace Automation Research Centre and IBM-France Scientific Center. This system generates real-time captions during live broadcast by using an experienced stenotypist who is connected to an automatic computer-based transcription system. Real-time closed captioning for hearing-impaired television viewers has been available in English for 10 years, but until now, none were for French-speaking users.\n",
    "MEDIATEX-TASF adapts the core technology developed by IBM-France (TASF) to the numerous constraints of live operation for providing high quality captioning. Three major improvements were carried out: adaptation of Grandjean method, introduction of specific dictionaries containing more than 6,000 expressions to complement the basic TASF lexicon and post-processing by 500 rules solving lexical problems as well as verb conjugation and word agreement in gender and number. The error rate of MEDIATEX-TASF is now reaching 5%, which could be sufficient for a commercial usage. First on-the-air tests will occur next September in Montreal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-402"
  },
  "wilde92_icslp": {
   "authors": [
    [
     "S. A.",
     "Wilde"
    ],
    [
     "K. M.",
     "Curtis"
    ]
   ],
   "title": "The wavelet transform for speech analysis",
   "original": "i92_1621",
   "page_count": 4,
   "order": 405,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "Spectral analysis is an important technique that is often utilised in speech processing. In the case of speech synthesis it is often used for the extraction of formant information, from natural speech, which is then used to construct a data base of the synthesisers drive parameters for subsequent synthesis. In the case of analysis the extracted frequency information is often used in its own right. Traditionally the Fast Fourier Transform has been the technique adopted to perform the required frequency extraction. The wavelet transform has recently gained favour for carrying out frequency analysis due to its ability to localise a signal in both frequency and time. This paper describes the investigations carried out into the use of the wavelet transform for formant extraction. The resynthesis of the natural speech using a parallel formant speech synthesizer driven using the extracted formant information. Comparisons are drawn as to the effectiveness of the technique when compared to that of a group delay minimum phase FFT technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-403"
  },
  "aibar92_icslp": {
   "authors": [
    [
     "Pablo",
     "Aibar"
    ],
    [
     "Andres",
     "Marzal"
    ],
    [
     "Enrique",
     "Vidal"
    ],
    [
     "Francisco",
     "Casacuberta"
    ]
   ],
   "title": "Problems and algorithms in optimal linguistic decoding: a unified formulation",
   "original": "i92_1625",
   "page_count": 4,
   "order": 406,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "Optimal Linguistic Decoding (OLD) is the task of finding a best description of an observed acoustic signal in terms of certain types of Linguistic Units such as phonemes, words, etc. So far, three main Dynamic Programming algorithms have been presented as alternative solutions to this decoding problem: Two Level, Level Building and One Stage (or Viterbi). It is generally accepted that all these algorithms solve the very same OLD problem and that they differ only in computational complexity aspects; however, this is not in fact the case. In this paper we show that different classes of increasingly difficult and interesting OLD problems can be identified and that not all the mentioned algorithms can actually solve all the problems. Level Building and One Stage can only solve a most restricted (least interesting) class. Two Level is significantly more powerful, properly accounting for a wide class of interesting OLD settings. Finally, no efficient algorithmic (exact) solution is known so far for the most traditional (and interesting) establishment of the OLD task. A formal characterization of these classes of problems is introduced in this paper, along with a discussion on their relative practical interest. Also, this characterization has allowed us to achieve a formal derivation of the above-mentioned classical algorithms along with some new, more efficient versions thereof.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-404"
  },
  "rouat92_icslp": {
   "authors": [
    [
     "Jean",
     "Rouat"
    ],
    [
     "Sylvain",
     "Lemieux"
    ],
    [
     "Alain",
     "Migneault"
    ]
   ],
   "title": "A spectro-temporal analysis of speech based on nonlinear operators",
   "original": "i92_1629",
   "page_count": 4,
   "order": 407,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "This paper proposes a spectro-temporal analysis based on a bank of cochlea filters in combinaison with a nonlinear operator for amplitude modulation enhancement in the medium and high frequency formants. The output of the spectro-temporal analysis is represented as a 3D image where it is possible to observe very short-term speech transitions and formant modulations. With such analysis, it is possible to obtain patterns characteristics of phonemes and transitions betwen phonemes, which can not be obtained by using other speech analysis ( FFT, LPC ) techniques. The paper presents 3D images of vowels, where the amplitude modulation of the formants is clearly visable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-405"
  },
  "berrojo92_icslp": {
   "authors": [
    [
     "Miguel A.",
     "Berrojo"
    ],
    [
     "Javier",
     "Corrales"
    ],
    [
     "Jesus",
     "Macias"
    ],
    [
     "Santiago",
     "Aguilera"
    ]
   ],
   "title": "A PC graphic tool for speech research based on a DSP board",
   "original": "i92_1633",
   "page_count": 4,
   "order": 408,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "We introduce a speech analysis system that performs the complete recording-playback interface, and allows some edition facilities. The system can display parameters such as energy, zero-crossing rate, fundamental frequency (pitch contour), spectrogram and LPC envelope. Another capabilities are filtering of the pre-recorded speech, labelling of the different acoustic segments and the possibility of laser screen hardcopies.\n",
    "The system needs a PC and some specific hardware. The PC must be AT or later, with a VGA graphic card and a serial mouse. The specific hardware is a DSP based board which was completely developed in the Department. The software is divided in two blocks: the software running on the iX86 processor, which controls the system and displays the speech information at screen, and the program running on the DSP, that performs the signal processing, the recording and the playback.\n",
    "Since the system is oriented to not technical people (phoneticians, etc.) we focused on the user interface to make it as easy and friendly as possible. The user can configure the windows in the screen in a wide manner using a keyboard or the mouse driven menu system.\n",
    "This speech analysis device takes advantage of all the standard PC features (video monitor, hard disk, printer, memory, etc.), what results in the lower cost compared with any other analysis tool in the market.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-406"
  },
  "hayamizu92_icslp": {
   "authors": [
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "A spoken language dialogue system for automatic collection of spontaneous speech",
   "original": "i92_1637",
   "page_count": 4,
   "order": 409,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "This paper describes a speech dialogue system for the data collection of spontaneous speech in a transportation guidance domain. As it is difficult to collect spontaneous speech and to use a real system for the collection, the phenomena related with dialogues have not been quantitatively clarified yet. The authors constructed a speech dialogue system which operates in almost real time, with acceptable recognition accuracy and flexible dialogue control. The system was used for spontaneous speech collection in a transportation guidance domain. The system performance evaluated in the domain is the understanding rate of 68.4 % for all the utterances, and the understanding rate of 84.2% for the utterances within the predefined grammar and the lexicon. Some statistics about the spontaneous speech collected are given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-407"
  },
  "nishioka92_icslp": {
   "authors": [
    [
     "Shingo",
     "Nishioka"
    ],
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "A powerful disambiguating mechanism for speech understanding systems based on ATMs",
   "original": "i92_1641",
   "page_count": 4,
   "order": 410,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "A speech understanding system confronts with the ambiguities caused by the acoustic-phonetic errors and multiple-meaning of words. Thus the effective framework is required to resolve the ambiguity. In this paper we propose a generic framework for speech understanding system, which efficiently resolves the ambiguity of input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-408"
  },
  "naja92_icslp": {
   "authors": [
    [
     "Najib",
     "Naja"
    ],
    [
     "Jean Marc",
     "Boucher"
    ],
    [
     "Samir",
     "Saoudi"
    ]
   ],
   "title": "A mixed Gaussian-stochastic code book for CELP coder in LSP speech coding",
   "original": "i92_1645",
   "page_count": 4,
   "order": 411,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "In a last few years, the Code Excited Linear Predictive (CELP) coder has demonstrated that such coders offer considerable promise for producing high quality synthetic speech at bit rates between 4.8 and 16 kbit/s. Tremendous efforts have been made to improve the CELP coder performance or to reduce its implementation cost. The CELP performance has been studied for different kind of code books such as Gaussian code books, algebraic code books, ternary code books ... In this paper, the Gaussian and a mixed Gaussian-stochastic code books were compared at different sizes, while the Line Spectrum Pairs (LSP) were used instead of Log Area Ratios (LAR) which provide efficient representation of the synthesis filter used in the CELP. This new stochasic code book was designed for various sizes, using the K-means algorithm applied to a large training data. Its first half is a Gaussian code book and the other half is obtained from the prediction error (after standardizing to unit variance) of speech signals when the skewness pi and the kurtosis 02 parameters do not correspond to the Gaussian law. For the same size, the new stochastic code book offers a better performance than the Gaussian one, about 0.5 dB for the same number of bits.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-409"
  },
  "kamata92_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Kamata"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "A method to estimate the transfer function of ARMA model of speech wave using prony method and homomorphic analysis",
   "original": "i92_1649",
   "page_count": 4,
   "order": 412,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "This paper proposes a method to perform the continuous analysis of speech signals using the Prony method. The Prony method can estimate the transfer function of autoregressive-moving average (ARMA) model. However, in the estimation based on the Prony method, the zeros are often found out as the unstable roots when the beginning point of sampled data has not been appropriate.\n",
    "In this paper, we first estimate the vocal tract impulse response from speech signals by using homomorphic analysis. Next, the transfer function of ARMA model can be calculated by applying the Prony method to the impulse response. The proposed method can perfectly estimate the poles and zeros as the stable roots.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-410"
  },
  "lindberg92b_icslp": {
   "authors": [
    [
     "Boerge",
     "Lindberg"
    ],
    [
     "Bjarne",
     "Andersen"
    ],
    [
     "Anders",
     "Baekgaard"
    ],
    [
     "Tom",
     "Broendsted"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Jan",
     "Kristiansen"
    ]
   ],
   "title": "An integrated dialogue design and continuous speech recognition system environment",
   "original": "i92_1653",
   "page_count": 4,
   "order": 413,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "Successful and flexible establishment of a spoken dialogue system requires the integration of dialogue design, dialogue management and spoken language processing into a common system environment.\n",
    "The research and development reported in this paper give details from the establishment of such an environment and its necessary development tools and devices as well as preliminary results from testing the integral continuous speech recogniser component on an artificial CAD-application. The application is characterised by a vocabulary containing names of geometrical objects, words by which these objects may be moved, changed in size, coloured etc, such that speech activated manipulation of the objects may be observed on a display. The interactive functionality is guided by reproductive speech output telling the user if, for instance, he/she is asking the system to perform illegal actions as seen from the pragmatic/semantic component of the dialogue system. The continuous speech recogniser part of the spoken language processing module is based on Continuous Hidden Markov Models (CHMM's) of user-configurable speech units, and the search algorithm is a token-passing Viterbi search algorithm constrained by syntactic rules stored in a finite state table (transition network). In addition to this, stub functions are associated with each state in the network, implementing simple conditions and actions and thereby enabling the implementation of more advanced grammars than otherwise available with traditional finite state tables. These stub functions may also be applied in handling of non-static conditions in the recognition process, such as pragmatic constraints. The artificial CAD application has been used to test the system, and the performance is reported for a traditional grammar both with and without stub augmentation. Results from these experiments show that improvements of the recognition rate may be obtained by implementing pragmatic and semantic constraints as directly associated stub functions within the states of the network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-411"
  },
  "marchal92b_icslp": {
   "authors": [
    [
     "Alain",
     "Marchal"
    ],
    [
     "C.",
     "Meunier"
    ],
    [
     "P.",
     "Gavarry"
    ]
   ],
   "title": "The PSH/DISPE helium speech cdrom",
   "original": "i92_1657",
   "page_count": 4,
   "order": 414,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "Gas mixture and pressure modify the spectral characteristics of diver's speech. Additionnally, constraints imposed on jaw movements by wearing a facial mask affect the speech production process. The auditory feedback loop is equally concerned. Furthermore, underwater adverse working conditions are characterised by noise from different sources. As a result, divers' speech is poorly intelligible and communications between divers and surface control need to be enhanced. To this end, «voice unscramblers» are being used. However, the technological state of commercially available equipment is dated and the quality of speech remains insufficient. To help with the design, testing and qualification of new communication devices, a bilingual (French-English) Data-base is set up. It consists of phonetically balanced lists of 200 words read by 17 divers under sea and in chambers at operationnal levels from the surface to 300m.These recordings have been edited, labelled and then stored on a CD-ROM. The PSH/DISPE CDROM complies with the SAM EUROPEC standards (ESPRIT-SAM project n°2589). It consists of acoustic signal files (40 KHz, 16 bits) of hyperbaric speech and of associated files. These later ones provide informations about the speakers and the recording conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1992-412"
  }
 },
 "sessions": [
  {
   "title": "Plenary",
   "papers": [
    "ladefoged92_icslp",
    "mori92_icslp",
    "kuhl92_icslp",
    "hirose92_icslp"
   ]
  },
  {
   "title": "Word Spotting in ASR",
   "papers": [
    "lleida92_icslp",
    "komori92_icslp",
    "clary92_icslp",
    "gish92_icslp",
    "feng92_icslp",
    "copperi92_icslp"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "seza92_icslp",
    "paksoy92_icslp",
    "shoham92_icslp",
    "taniguchi92_icslp"
   ]
  },
  {
   "title": "Speech Production: Coarticulation",
   "papers": [
    "bonafonte92_icslp",
    "leeper92_icslp",
    "shadle92_icslp",
    "slater92_icslp",
    "caldognetto92_icslp"
   ]
  },
  {
   "title": "Auditory Models",
   "papers": [
    "smeele92_icslp",
    "laine92_icslp",
    "gao92_icslp",
    "tajchman92_icslp",
    "holton92_icslp"
   ]
  },
  {
   "title": "Recognition of Telephone Speech",
   "papers": [
    "hermansky92_icslp",
    "ciaramella92_icslp",
    "lennig92_icslp",
    "chigier92_icslp"
   ]
  },
  {
   "title": "Text-to-Speech Synthesis 1, 1",
   "papers": [
    "gulikers92_icslp",
    "willemse92_icslp",
    "bruce92_icslp",
    "sullivan92_icslp",
    "manzara92_icslp",
    "agrawal92_icslp",
    "pols92_icslp",
    "granstrom92_icslp"
   ]
  },
  {
   "title": "Voice Source Characteristics",
   "papers": [
    "strik92_icslp",
    "koreman92_icslp",
    "palmer92_icslp",
    "imaizumi92_icslp",
    "kiritani92_icslp"
   ]
  },
  {
   "title": "Speech Perception: Higher-Order Processes 1, 1",
   "papers": [
    "traum92_icslp",
    "meltzer92_icslp",
    "smyth92_icslp",
    "su92_icslp",
    "minematsu92_icslp",
    "nygaard92_icslp",
    "quene92_icslp",
    "sommers92_icslp",
    "mcqueen92_icslp"
   ]
  },
  {
   "title": "Speaker-Independent Word Recognition",
   "papers": [
    "euler92_icslp",
    "cordoba92_icslp",
    "pitrelli92_icslp",
    "kimura92_icslp",
    "roddeman92_icslp"
   ]
  },
  {
   "title": "Human Factors",
   "papers": [
    "cutler92_icslp",
    "ulagaraj92_icslp",
    "hunnicutt92_icslp",
    "goodine92_icslp",
    "jekosch92_icslp"
   ]
  },
  {
   "title": "Continuous Speech Recognition 1, 2",
   "papers": [
    "kenny92_icslp",
    "boulianne92_icslp",
    "rainion92_icslp",
    "nagai92_icslp",
    "bates92_icslp",
    "shirotsuka92_icslp",
    "yamaguchi92_icslp",
    "kita92_icslp",
    "ito92_icslp",
    "ljolje92_icslp"
   ]
  },
  {
   "title": "Natural Language Processing and Speech Understanding 1-3",
   "papers": [
    "waegner92_icslp",
    "zoltowski92_icslp",
    "jones92_icslp",
    "kai92_icslp",
    "pereira92_icslp",
    "seneff92_icslp",
    "goddeau92_icslp",
    "howells92_icslp",
    "rao92_icslp",
    "jackson92_icslp",
    "pelillo92_icslp",
    "hatazaki92_icslp",
    "morimoto92_icslp",
    "morimoto92b_icslp",
    "pieraccini92_icslp"
   ]
  },
  {
   "title": "Language Learning and Acquisition 1, 2",
   "papers": [
    "robb92_icslp",
    "kojima92_icslp",
    "rochet92_icslp",
    "gasser92_icslp",
    "hurlburt92_icslp",
    "halle92_icslp",
    "deguchi92_icslp"
   ]
  },
  {
   "title": "Speech Perception: Units of Processing",
   "papers": [
    "treurniet92_icslp",
    "riley92_icslp",
    "derwing92_icslp",
    "wiebe92_icslp"
   ]
  },
  {
   "title": "Prosody: The Phrase and Beyond 1, 2",
   "papers": [
    "sluijter92_icslp",
    "sluijter92b_icslp",
    "kaiki92_icslp",
    "mobius92_icslp",
    "ross92_icslp",
    "swerts92_icslp",
    "nakajima92_icslp",
    "grosz92_icslp",
    "fujisaki92_icslp",
    "bishop92_icslp"
   ]
  },
  {
   "title": "Speaker Adaptation",
   "papers": [
    "ohkura92_icslp",
    "matsubka92_icslp",
    "gong92_icslp",
    "hattori92_icslp",
    "kobayashi92_icslp"
   ]
  },
  {
   "title": "Pronunciation Training",
   "papers": [
    "lefevre92_icslp",
    "rooney92_icslp",
    "benedetto92_icslp"
   ]
  },
  {
   "title": "Self-Organizing Systems in ASR 1, 2",
   "papers": [
    "poirier92_icslp",
    "haan92_icslp",
    "bradshaw92_icslp",
    "sorensen92_icslp",
    "caraty92_icslp",
    "monte92_icslp",
    "mantysalo92_icslp",
    "kurimo92_icslp",
    "dalsgaard92_icslp",
    "utela92_icslp"
   ]
  },
  {
   "title": "Speech Synthesis 1-3",
   "papers": [
    "iwahashi92_icslp",
    "sagisaka92_icslp",
    "coile92_icslp",
    "karlsson92_icslp",
    "santen92_icslp",
    "balestri92_icslp",
    "sproat92_icslp",
    "hirokawa92_icslp",
    "williams92_icslp",
    "goldstein92_icslp",
    "jongenburger92_icslp",
    "katoh92_icslp",
    "itoh92_icslp",
    "schoentgen92_icslp",
    "ceder92_icslp",
    "iwata92_icslp",
    "monaghan92_icslp",
    "hessen92_icslp",
    "campbell92_icslp",
    "hertz92_icslp",
    "house92_icslp",
    "tatham92_icslp",
    "belrhali92_icslp",
    "elmlund92_icslp",
    "rentzepopoulos92_icslp",
    "hara92_icslp",
    "breen92_icslp",
    "andrews92_icslp",
    "boogaart92_icslp",
    "boeffard92_icslp",
    "kasuya92_icslp",
    "hill92_icslp",
    "rozsypal92_icslp",
    "portele92_icslp",
    "delogu92_icslp",
    "gussenhoven92_icslp"
   ]
  },
  {
   "title": "Speech Perception: Phonetic Processes 1, 2",
   "papers": [
    "davis92_icslp",
    "stevens92_icslp",
    "huber92_icslp",
    "kato92_icslp",
    "bonneau92_icslp",
    "carlson92_icslp",
    "benedetto92b_icslp",
    "nearey92_icslp",
    "pisoni92_icslp",
    "goldinger92_icslp"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "cheng92_icslp",
    "beattie92_icslp",
    "brown92_icslp",
    "nandkumar92_icslp",
    "moreno92_icslp"
   ]
  },
  {
   "title": "Speech Recognition 1, 2",
   "papers": [
    "anglade92_icslp",
    "rosenberg92_icslp",
    "matsui92_icslp",
    "bennani92_icslp",
    "montacie92_icslp",
    "schiel92_icslp",
    "ederveen92_icslp",
    "yi92_icslp",
    "koo92_icslp",
    "shirai92_icslp",
    "koga92_icslp",
    "vaseghi92_icslp",
    "mcinnes92_icslp",
    "fissore92_icslp",
    "cifuentes92_icslp",
    "ruske92_icslp",
    "goblirsch92_icslp",
    "nagai92b_icslp",
    "sakai92_icslp",
    "mizuta92_icslp",
    "kuroiwa92_icslp",
    "kuwano92_icslp",
    "lennon92_icslp",
    "minami92_icslp",
    "noll92_icslp",
    "fanty92_icslp",
    "abe92_icslp",
    "wooters92_icslp",
    "jacobsen92_icslp",
    "kobayashi92b_icslp",
    "xu92_icslp",
    "chen92_icslp"
   ]
  },
  {
   "title": "Hidden Markov Models",
   "papers": [
    "fallside92_icslp",
    "masai92_icslp",
    "cardin92_icslp",
    "brugnara92_icslp",
    "tridgell92_icslp"
   ]
  },
  {
   "title": "Dialogue",
   "papers": [
    "niedermair92_icslp",
    "andry92_icslp",
    "heisterkamp92_icslp",
    "nagata92_icslp",
    "takebayashi92_icslp"
   ]
  },
  {
   "title": "Prosody and Phonology 1, 2",
   "papers": [
    "yang92_icslp",
    "tsukuma92_icslp",
    "campbell92b_icslp",
    "fant92_icslp",
    "carlson92b_icslp",
    "caspars92_icslp",
    "terken92_icslp",
    "beaugendre92_icslp",
    "liberman92_icslp",
    "shattuckhufnagel92_icslp"
   ]
  },
  {
   "title": "Speaking Styles 1, 2",
   "papers": [
    "morton92_icslp",
    "spring92_icslp",
    "slamacazacu92_icslp",
    "bronstein92_icslp",
    "blaauw92_icslp",
    "eskenazi92_icslp",
    "umeda92_icslp",
    "daly92_icslp",
    "shockey92_icslp"
   ]
  },
  {
   "title": "ASR in Noise",
   "papers": [
    "lefebvre92_icslp",
    "stern92_icslp",
    "kitamura92_icslp",
    "cairns92_icslp",
    "mokbel92_icslp"
   ]
  },
  {
   "title": "Dialogues and Applications",
   "papers": [
    "meyer92_icslp",
    "jack92_icslp",
    "nielsen92_icslp",
    "lerner92_icslp",
    "matsunaga92_icslp"
   ]
  },
  {
   "title": "Sampling of Speech Production 1, 2",
   "papers": [
    "lu92_icslp",
    "oshimat92_icslp",
    "matsumura92_icslp",
    "kiritani92b_icslp",
    "sonoda92_icslp",
    "alku92_icslp",
    "motoki92_icslp"
   ]
  },
  {
   "title": "Labelling of Speech 1, 2",
   "papers": [
    "marchal92_icslp",
    "fujiwara92_icslp",
    "phillips92_icslp",
    "katunobu92_icslp",
    "brugnara92b_icslp",
    "luk92_icslp",
    "bagshaw92_icslp",
    "gong92b_icslp",
    "silverman92_icslp",
    "eisen92_icslp"
   ]
  },
  {
   "title": "Speech Production: Models and Theories 1, 2",
   "papers": [
    "fant92b_icslp",
    "junqua92_icslp",
    "laprie92_icslp",
    "carre92_icslp",
    "stone92_icslp",
    "harris92_icslp",
    "hirayama92_icslp",
    "vatikiotisbateson92_icslp"
   ]
  },
  {
   "title": "Phonetics",
   "papers": [
    "keating92_icslp",
    "byrd92_icslp",
    "ohala92_icslp",
    "erickson92_icslp",
    "tajchman92b_icslp"
   ]
  },
  {
   "title": "Speech Data Bases",
   "papers": [
    "cole92_icslp",
    "muthusamy92_icslp",
    "paul92_icslp",
    "hirschman92_icslp",
    "phillips92b_icslp"
   ]
  },
  {
   "title": "Stochastic ASR",
   "papers": [
    "abrash92_icslp",
    "cohen92_icslp",
    "fink92_icslp"
   ]
  },
  {
   "title": "Analysis of Disfluencies in Speech 1, 2",
   "papers": [
    "lehiste92_icslp",
    "bickley92_icslp",
    "oshaughnessy92_icslp",
    "lickley92_icslp",
    "morin92_icslp",
    "cucchiarini92_icslp",
    "shriberg92_icslp",
    "wade92_icslp"
   ]
  },
  {
   "title": "Linguistic Phonetics: Reduction",
   "papers": [
    "manuel92_icslp",
    "umeda92b_icslp"
   ]
  },
  {
   "title": "Hearing/Speech Impaired 1, 2",
   "papers": [
    "benoit92_icslp",
    "oster92_icslp",
    "seki92_icslp",
    "boothroyd92_icslp",
    "cutugno92_icslp",
    "baum92_icslp",
    "bhatt92_icslp",
    "hosoi92_icslp",
    "nabelek92_icslp",
    "jamieson92_icslp"
   ]
  },
  {
   "title": "Unit-Based ASR",
   "papers": [
    "fuyuan92_icslp",
    "yogo92_icslp",
    "altosaar92_icslp",
    "flammia92_icslp"
   ]
  },
  {
   "title": "Language and Dialect Characterization",
   "papers": [
    "benoit92b_icslp",
    "brousseau92_icslp",
    "muthusamy92b_icslp",
    "nakagawa92_icslp",
    "itahashi92_icslp"
   ]
  },
  {
   "title": "Speech Production, Parception, and Analysis",
   "papers": [
    "boyanov92_icslp",
    "fu92_icslp",
    "rump92_icslp",
    "liu92_icslp",
    "norris92_icslp",
    "slawinski92_icslp",
    "alwan92_icslp",
    "sawallis92_icslp",
    "wrench92_icslp",
    "connell92_icslp",
    "kashino92_icslp",
    "tsuzaki92_icslp",
    "cole92b_icslp",
    "weitzman92_icslp",
    "koopmansvanbeinum92_icslp",
    "gable92_icslp",
    "esling92_icslp",
    "shigeno92_icslp",
    "morohashi92_icslp",
    "nabelek92b_icslp",
    "cheesman92_icslp",
    "jamieson92b_icslp"
   ]
  },
  {
   "title": "Linguistic Phonetics",
   "papers": [
    "hata92_icslp",
    "wang92_icslp",
    "nathan92_icslp",
    "hosaka92_icslp",
    "zhang92_icslp",
    "kvale92_icslp",
    "mcrobbieutasi92_icslp",
    "yamada92_icslp",
    "ottesen92_icslp",
    "heuven92_icslp",
    "elenius92_icslp",
    "strangert92_icslp",
    "taylor92_icslp",
    "weiss92_icslp",
    "elgendy92_icslp",
    "silverman92b_icslp",
    "ohala92b_icslp",
    "hinkelman92_icslp",
    "port92_icslp",
    "ziolkowski92_icslp",
    "sato92_icslp",
    "cao92_icslp",
    "wang92b_icslp"
   ]
  },
  {
   "title": "Perception and Production",
   "papers": [
    "bradford92_icslp",
    "cosi92_icslp",
    "goldstein92b_icslp",
    "pelillo92b_icslp",
    "barschdorff92_icslp",
    "oviatt92_icslp",
    "chung92_icslp",
    "lee92_icslp",
    "koizumi92_icslp",
    "asano92_icslp",
    "takizawa92_icslp",
    "canavesio92_icslp",
    "jun92_icslp",
    "fukuzawa92_icslp",
    "ennaji92_icslp",
    "kuo92_icslp",
    "springer92_icslp",
    "jacobs92_icslp",
    "deville92_icslp",
    "niimi92_icslp",
    "eatock92_icslp",
    "tzoukermann92_icslp",
    "francois92_icslp",
    "paul92b_icslp",
    "lefebvre92b_icslp",
    "bardaud92_icslp",
    "yamashita92_icslp",
    "hangai92_icslp",
    "teixeira92_icslp",
    "lindberg92_icslp",
    "cozannet92_icslp",
    "li92_icslp",
    "liu92b_icslp"
   ]
  },
  {
   "title": "Analysis/Systems",
   "papers": [
    "ziegenhain92_icslp",
    "mast92_icslp",
    "cheng92b_icslp",
    "heuvel92_icslp",
    "rosenbeck92_icslp",
    "lee92b_icslp",
    "hernando92_icslp",
    "shimodaira92_icslp",
    "eady92_icslp",
    "veilleux92_icslp",
    "cheng92c_icslp",
    "switzer92_icslp",
    "descout92_icslp",
    "wilde92_icslp",
    "aibar92_icslp",
    "rouat92_icslp",
    "berrojo92_icslp",
    "hayamizu92_icslp",
    "nishioka92_icslp",
    "naja92_icslp",
    "kamata92_icslp",
    "lindberg92b_icslp",
    "marchal92b_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.1992"
}