{
 "title": "Interactive Dialogue in Multi-Modal Systems (IDS 1999)",
 "location": "Kloster Irsee, Germany",
 "startDate": "22/6/1999",
 "endDate": "25/6/1999",
 "conf": "IDS",
 "year": "1999",
 "name": "ids_1999",
 "series": "IDS",
 "SIG": "",
 "title1": "Interactive Dialogue in Multi-Modal Systems",
 "title2": "(IDS 1999)",
 "date": "22-25 June 1999",
 "papers": {
  "sadek99_ids": {
   "authors": [
    [
     "David",
     "Sadek"
    ]
   ],
   "title": "Design considerations on dialogue systems:from theory to technology- the case of Artimis -",
   "original": "ids9_173",
   "page_count": 15,
   "order": 1,
   "p1": "173",
   "pn": "187",
   "abstract": [
    "Significant features of an interactive system that makes the user view it as a user-friendly di alogue system are introduced and discussed. It is argued that it is onl when they are satisfied jointly and independently from dialogue state that they can appreciably affect the degree of system conviviality. It is also claimed that the way that the satisfaction of these features is handled, namely extensionally or as a consequence of a deeper \"intelligence\" of the system, determines a fundamental division in the approaches of dialogue system d esign. Classical (structural and plan-based) approaches to dialogue are discussed. Then, a new paradigm fo designing and implementing dialogue systems, that of rational dialogue agent, based on rational interaction approach, is introduced. It is shown how this paradigm and approach have led to Artimis, a technology of rational agency, which provides a generic framework to instantiate effective advanced dialogue systems.\n",
    ""
   ]
  },
  "thomson99_ids": {
   "authors": [
    [
     "David L.",
     "Thomson"
    ],
    [
     "Jack J.",
     "Wisowaty"
    ]
   ],
   "title": "User confusion in natural language services",
   "original": "ids9_189",
   "page_count": 8,
   "order": 2,
   "p1": "189",
   "pn": "196",
   "abstract": [
    "This paper examines the difficulty subjects experience in communicating with natural language (NL) speech recognizers in real telephone services. We show that providing an NL interface is not sufficient for creating a comfortable experience for untrained users, and that customer confusion is a key challenge in building NL-based services. We examine three recent NL trials from both the technology perspective and the human perspective and show where improvements must be made before NL services can be broadly deployed.\n",
    ""
   ]
  },
  "strong99_ids": {
   "authors": [
    [
     "Gary W.",
     "Strong"
    ]
   ],
   "title": "Spoken dialogue architectures in the US DARPA Communicator Program",
   "original": "ids9_197",
   "page_count": 1,
   "order": 3,
   "p1": "197 (abstract only)",
   "pn": "",
   "abstract": [
    "The Communicator Program of the United States Defense Advanced Research Projects Agency is a research program dedicated to the exploration of technologies for spoken dialogue between humans and computers. It has adopted an architecture, the MIT Galaxy System, as the reference architecture fo participants in the program. The program has established a policy committee in order to oversee the evolution of the architecture and its movement toward eventual transition into industry. Such processes are not without controversy in any diverse community of researchers. On the other hand, it is seen as essential to not only lower the bar for entry into dialogue research and its evaluation but also an essential step toward setting standards that will be necessary in the transition of the technology to industry. Several models are being explored for industrial participation in a DARPA consortium for the development of a standard dialogue reference architecture.\n",
    ""
   ]
  },
  "cole99_ids": {
   "authors": [
    [
     "Ron",
     "Cole"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "Kal",
     "Shobaki"
    ],
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Michael M.",
     "Cohen"
    ]
   ],
   "title": "Demonstration of dialogue tools in the CSLU toolkit",
   "original": "ids9_155",
   "page_count": 2,
   "order": 4,
   "p1": "155",
   "pn": "156",
   "abstract": [
    "The CSLU Toolkit and accompanying tutorials are designed to provide a platform for researching and developing language technologies and systems, and to engage naïve users in using and experimenting with interactive language systems. We provide a set of demonstrations in this special session that illustrate capabilities. They include rapid prototyping of a spoken dialogue system that integrates an animated talking face, speech recognition and text-to-speech synthesis, and a variety of applications created by practitioners using the rapid application developer.\n",
    ""
   ]
  },
  "brndsted99_ids": {
   "authors": [
    [
     "Tom",
     "Brøndsted"
    ]
   ],
   "title": "Implementing a task specific grammar for recognition and parsing using theCPK NLP Suite for spoken language understanding",
   "original": "ids9_157",
   "page_count": 4,
   "order": 5,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "This paper describes how a task specific grammar can be implemented using a dedicated NLP Augmented Phrase Structure (APS) grammar formalism. The APS is used for generation of appropriate semantic frames to be passed on to the dialogue manager of a spoken dialogue system. In a derived form, conforming to the HTK Standard Lattice format, the same APS may be used for constraining the approved speech recogniser grapHvite by Entropics. APS and HTK (standard lattice) are just two of several NLP and recognition grammar formats supported by the CPK NLP Suite for Spoken Language Understanding. The suite can be downloaded (in C++ source code) for research and other non-commercial use at the web address http://www.cpk.auc.dk/~tb/nlpsuite.\n",
    ""
   ]
  },
  "kawahara99_ids": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Atsushi",
     "Yamada"
    ],
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Japanese Dictation Toolkit - free software repository for speech recognition",
   "original": "ids9_161",
   "page_count": 1,
   "order": 6,
   "p1": "161",
   "pn": "",
   "abstract": [
    "Overview: A sharable software repository Project sponsored by a governmental organization IPA (Information-technology Promotion Agency), Japan Collaboration of researchers of different academic institutes in Japan Available to public freely For further information: http://www.lang.astem.or.jp/dictation-tk/, dictation-tk-request@astem.or.jp\n",
    ""
   ]
  },
  "lee99_ids": {
   "authors": [
    [
     "Lin-shan",
     "Lee"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Keh-Jiann",
     "Chen"
    ]
   ],
   "title": "Preliminary tools and systems with initial experiences forChinese spoken language processing tasks",
   "original": "ids9_163",
   "page_count": 2,
   "order": 7,
   "p1": "163",
   "pn": "164",
   "abstract": [
    "Due to the feature characteristics of Chinese language, the spoken language processing technologies developed for Western alphabetic languages are not necessarily equally efficient when applied to Chinese language, and as a result a whole set of technologies specially for Chinese languages have been developed. This paper summarizes some preliminary tools and systems with initial experiences obtained with these technologies.\n",
    ""
   ]
  },
  "gustafson99_ids": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Kåre",
     "Sjölander"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "Creating web-based exercises for spoken language technology",
   "original": "ids9_165",
   "page_count": 4,
   "order": 8,
   "p1": "165",
   "pn": "166",
   "abstract": [
    "This paper describes the efforts at KTH in creating webbased exercises for speech technology. The World Wide Web was chosen as our platform in order to increase the usability and accessibility of our computer exercises. The aim was to provide dedicated educational software instead of exercises based on complex research tools. Currently, the set of exercises comprises basic speech analysis, multi-modal speech synthesis and spoken dialogue systems. Students access web pages in which the exercises have been embedded as applets. This makes it possible to use them in a classroom setting, as well as from the students home computers.\n",
    ""
   ]
  },
  "potamianos99_ids": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Hong-Kwang",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Antoine",
     "Saad"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "Design principles and tools for multimodal dialog systems",
   "original": "ids9_167",
   "page_count": 4,
   "order": 9,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "We review e\u000borts in defining design principles and creating tools for building multimodal dialog systems with emphasis on the speech modality. General design principles and challenges are outlined. The focus is on system architecture, application and speech interface design, data collection and evaluation tools. We conclude that modularity, flexibility, customizability, domain-independence and automatic dialog generation are some important features of successful dialog systems and design tools.\n",
    ""
   ]
  },
  "sturm99_ids": {
   "authors": [
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Els den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Issues in spoken dialogue systems: experiences with the Dutch ARISEsystem",
   "original": "ids9_001",
   "page_count": 4,
   "order": 10,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "In the ARISE project we developed an experimental spoken dialogue system for access to train timetable information of the Dutch Railways. It is based on an existing system (VIOS), which became operational during the course of the project. This paper discusses a number of issues that came to light during evaluations of the ARISE system. We are able to shorten the dialogue with the system considerably, by using confidence measures, short prompts and a zooming exceptions handling strategy. The problems that we observed are all related to the fact that both the dialogue system and the user are unable to infer each others intentions and capabilities. Also, evaluation of a dialogue system is difficult since the evaluator has little means to infer the real intentions of the user. Working with predefined scenarios is not the optimal solution for this problem.\n",
    ""
   ]
  },
  "kawahara99b_ids": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Katsuaki",
     "Tanaka"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Virtual fitting room with spoken dialogue interface",
   "original": "ids9_005",
   "page_count": 4,
   "order": 11,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "We address an e\u000bective application of spoken dialogue interface by combining with virtual space technologies. Virtual space enables us to do what cannot be done in the real world with feeling as if doing in the real world. Spoken dialogue systems are to realize our intention by understanding expressions that include concept and a nuance. We clarified advantages and disadvantages of speech interface in a multi-modal virtual space, then designed a virtual fitting room, where the speech interface works effectively and the dialogue is enhanced by multi-modal interaction. The system supposes that a user is in a fitting room. The user can select his favorite clothes interactively by stating his preference and checking the virtual mirror. It was observed that the combination of spoken language input and virtually-real image output realizes a natural and robust interaction, thus giving better user satisfaction than conventional platforms.\n",
    ""
   ]
  },
  "fabbrizio99_ids": {
   "authors": [
    [
     "G. di",
     "Fabbrizio"
    ],
    [
     "C.",
     "Kamm"
    ],
    [
     "P.",
     "Ruscitti"
    ],
    [
     "Shri",
     "Narayanan"
    ],
    [
     "B.",
     "Buntschuh"
    ],
    [
     "A.",
     "Abella"
    ],
    [
     "J.",
     "Hubbell"
    ],
    [
     "J.",
     "Wright"
    ]
   ],
   "title": "Extending a standard-based IP and computer telephony platform tosupport multi-modal services",
   "original": "ids9_009",
   "page_count": 4,
   "order": 12,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "Despite recent advances in Computer Telephony (CT) and IP Telephony (IPT) standards at defining flexible architectures to support new technologies, the current CT paradigm does not adequately support the requirements of advanced spoken dialogue systems. This paper describes an application framework based on CT and IPT standards that defines new architectural components for information access, alerting, and multi-modal input/output integration. This framework permits separation of the application logic from low-level resource management in order to facilitate the design and development of advanced, multi-modal voice-enabled services.\n",
    ""
   ]
  },
  "salmen99_ids": {
   "authors": [
    [
     "Angelika",
     "Salmen"
    ],
    [
     "Philipp",
     "Großmann"
    ],
    [
     "Ludwig",
     "Hitzenberger"
    ],
    [
     "Uwe",
     "Creutzburg"
    ]
   ],
   "title": "Dialog systems in traffic environment",
   "original": "ids9_013",
   "page_count": 4,
   "order": 13,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "This paper focuses on specific problems of dialog systems in the car environment which are dealt with in two projects concerning navigation systems and information on the car itself. Drivers' capacities and traffic safety propose speech only dialog systems. We will give insight in some requirements for the design of speech interfaces. As their possibilities are limited, however, graphical add-ons have to be elaborated. We will discuss the problems of the combination on the background of the main modus of speech.\n",
    ""
   ]
  },
  "wyard99_ids": {
   "authors": [
    [
     "Peter J.",
     "Wyard"
    ],
    [
     "Gavin E.",
     "Churcher"
    ]
   ],
   "title": "The MUeSLI Multimodal 3D Retail System",
   "original": "ids9_017",
   "page_count": 4,
   "order": 14,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "This paper describes a multimodal spoken language system, which allows users to furnish a virtual 3D living room. Speech and touch modalities are tightly integrated in a single user input turn. An advanced dialogue manager allows users to have a natural flexible style of interaction with the \"virtual assistant\" (a talking head). The system is mixed initiative, with a system intelligence which allows the assistant to make expert suggestions on room design. This paper concentrates on the architecture and components which were needed to achieve these results.\n",
    ""
   ]
  },
  "streit99_ids": {
   "authors": [
    [
     "Michael",
     "Streit"
    ]
   ],
   "title": "The interaction of speech, deixis and graphics in the multimodaloffice agent Talky",
   "original": "ids9_021",
   "page_count": 4,
   "order": 15,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "If multimodal systems are strictly based on the pattern of natural conversation, problems arise that are related to the transient nature of speech that strongly occupies the users attention. The concept of visual utterance is introduced that allows for a strictly user driven interaction, by preserving a conversational style of communication. Important features of visual utterances are that clarification and modifications of user requests can be based on a structured presentation of the systems interpretation, that the user can interact with these presentations not only by speech, but also by gestures and te xtual input, and that the systems view on focus is pr esented visually b Smileys, attached to the utterance. Certain problems of visual utterances are discussed. Also the problem of the distinction between deictic and manipulative uses of gestures is touched.\n",
    ""
   ]
  },
  "lee99b_ids": {
   "authors": [
    [
     "Mark",
     "Lee"
    ]
   ],
   "title": "Implicit goals in indirect replies",
   "original": "ids9_025",
   "page_count": 4,
   "order": 16,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "Indirection is a common phenomenon in natural language dialogue. Therefore, any system designed to deal with dialogue must be able to handle indirection. Despite this, there has been little work explaining the motivations behind indirection. This is due to an assumption that agents are purely cooperative. In this paper, we present an account of indirect replies to questions based on the concept of agent rationality. This account is implemented in a question-answering system.\n",
    ""
   ]
  },
  "hirose99_ids": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Shinya",
     "Kiriyama"
    ]
   ],
   "title": "Generation of speech reply in a spoken dialogue system for literature retrieval",
   "original": "ids9_029",
   "page_count": 4,
   "order": 17,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "A spoken dialogue system was developed for the literature retrieval, and discussions were conducted mainly from the viewpoints of speech reply generation. The system was designed taking the following three points into consideration: appropriate amount of information included in a sentence, appropriate guidance from the system to users, and inappropriateness of using text-to-speech conversion devices for reply speech generation. In order to generate wide variety of output sentences, they were converted from concept representations. The generated sentences were given as sequences of phone and prosodic labels so that they can be directly fed to the speech synthesizer. System evaluation was conducted to compare the system-oriented and user-oriented dialogue modes, and also to find out appropriate ellipsis levels of output sentences. The results indicated the version at the middle of two extremes was the best for the both cases, though they will be different depending on various factors, such as whether the users having good experiences on dialogue systems, whether the system having a display, and so on.\n",
    ""
   ]
  },
  "esposito99_ids": {
   "authors": [
    [
     "Richard",
     "Esposito"
    ],
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Acoustic correlates of interruptions in spoken dialogue",
   "original": "ids9_033",
   "page_count": 4,
   "order": 18,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "In this paper, we investigate the prosody and functions of interruptions in natural dialogue, and the role of interruption-initiated actions in spoken dialogue systems. The specific forms of prosody are both an indicator of cagnitive state and a reflöection of negotiation strategies to mutually guide a dialogue, with pitch and amplitude values signaling the degree of concordance with the current topic. These prosodic forms are potentially usable in spoken dialogue systems to provide intelligent responding systems that are responsive to homan motivations in dialogues.\n",
    ""
   ]
  },
  "pargellis99_ids": {
   "authors": [
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Automatic application generator matches userexpectations to system capabilities",
   "original": "ids9_037",
   "page_count": 4,
   "order": 19,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "We report on the Application Generator (AG), a system that automatically creates, and then manages, user-customized applications requiring a speech interface. The AG is composed of four modular components: the Automatic Dialogue Generator (ADG), the Profile Manager (PM), the Information and Services Manager (ISM), and the Dialogue Manager (DM). The PM module encodes the users intent with a representation of the services and information topics the user is interested in. Thus, the user defines a customized application; both in topical content and presentation format. The ISM module determines the available system resources by searching databases for the requested information and then delivers the information in a format defined by the user. A key feature of the ADG module is that it generates, in a uniform and consistent manner, a finite state dialogue for any task described by a set of tables. Finally, the DM module uses a set of Voice Interface Language (VIL) commands to carry out the actual dialogue session with the user.\n",
    ""
   ]
  },
  "kurematsu99_ids": {
   "authors": [
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "Development of Japanese spontaneous speech database and meaningextraction based on semantic caseframe on scheduling task",
   "original": "ids9_041",
   "page_count": 4,
   "order": 20,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "This paper describes the issue of analysis of spontaneous speech in Japanese in the scheduling task and the procedure of transcription and segmentation into words. The experiment of meaning extraction was conducted using the corpus of spontaneous speech in the scheduling task which is suitable for the research on spontaneous dialog processing. The framework of semantic lexicon and the structure of semantic caseframe which focuses the verbal words in phrase are described. The results of the preliminary experiment of the meaning extraction of the spontaneous speech in the scheduling task dialogue are shown.\n",
    ""
   ]
  },
  "tanigaki99_ids": {
   "authors": [
    [
     "Kouichi",
     "Tanigaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Robust speech understanding based on word graph interface",
   "original": "ids9_045",
   "page_count": 4,
   "order": 21,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "This paper proposes a stochastic understanding model and its utilization on recognition word-graphs, to enhance the end-to-end performance of spontaneous speech understanding systems. The understanding model is constructed automatically based on Binary Decision Trees, which account for the semantic probability for a word sequence. We apply the model on word-graphs provided by speech recognition, and search the most profitable paths that maximize the acoustic, linguistic, and semantic probabilities for the utterances. Speech understanding experiments show that our approach improves understanding accuracy from 65.6% to 68.5%, compared to the simple understanding results for recognition top-best hypotheses.\n",
    ""
   ]
  },
  "boros99_ids": {
   "authors": [
    [
     "Manuela",
     "Boros"
    ],
    [
     "Paul",
     "Heisterkamp"
    ]
   ],
   "title": "Efficient and robust natural language processing in simple speech application domains",
   "original": "ids9_049",
   "page_count": 4,
   "order": 22,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "Comparing spoken dialogue systems in commercial and research background reveals a great discrepancy in state-of-the-art systems. Whereas research systems tend to be very complex, allowing free and flexible dialogues using unrestricted speech, commercial systems are restricted to far less complex solutions such as menu-driven systems based on simple keyword-spotting, mostly doing completely without natural language processing. In our paper we present an approach to speech processing in dialogue systems based on linguistic phrase spotting, that allows for efficient and robust speech understanding in simple application domains. Misunderstandings or under-specification of user requests will thus be detected and complex dialogue strategies for clarification may be activated. In all other cases, the system may operate like a keyword-spotter. The proposed approach therefore is well suited for simple application domains such as information retrieval in commercial dialogue systems.\n",
    ""
   ]
  },
  "dahlback99_ids": {
   "authors": [
    [
     "Nils",
     "Dahlbäck"
    ],
    [
     "Annika",
     "Flycht-Eriksson"
    ],
    [
     "Arne",
     "Jönsson"
    ],
    [
     "Pernilla",
     "Qvarfordt"
    ]
   ],
   "title": "An architechture for multi-modal natural dialogue systems",
   "original": "ids9_053",
   "page_count": 4,
   "order": 23,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "In this paper we present an architecture for multi-modal dialogue systems. It is illustrated from our development of a multi-modal information system for local bus timetable information. The system is based on a natural language interface for typed interaction that is enhanced to handle also multi-modal interaction. The multi-modal user interface was designed based on empirical investigations and some results from these investigations are presented. We also show how information specification forms can be utilised to handle requests typical for timetable information systems and how spatial and temporal information is integrated and used in the system.\n",
    ""
   ]
  },
  "minker99_ids": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ],
    [
     "Marsal",
     "Gavaldà"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Hidden understanding models for machine translation",
   "original": "ids9_057",
   "page_count": 4,
   "order": 24,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "We demonstrate the portability of a stochastic method for understanding natural language from a setting of human-machine interactions (ATIS - Air Travel Information Services and MASK - Multimodal Multimedia Automated Service Kiosk) into the more open one of human-to-human interactions. The applicationwe use is the English Spontaneous Speech Task (ESST) for multilingual appointment scheduling. Spoken language systems developed for this task translate spontaneous conversational speech among different languages.\n",
    ""
   ]
  },
  "gustafson99b_ids": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Magnus",
     "Lundeberg"
    ],
    [
     "Johan",
     "Liljencrants"
    ]
   ],
   "title": "Experiences from the development of August -a multi-modal spoken dialogue system",
   "original": "ids9_061",
   "page_count": 4,
   "order": 25,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "This paper describes experiences from the development of a Swedish spoken dialogue system with a talking agent, August. The system was exposed to the general public at a very early stage. Apart from describing the system and its components this paper discusses the problems encountered during the set-up and describe possible solutions considered. The system has been used for a period of six months to collect spontaneous speech data, largely from people with no previous experience of speech technology. One of the goals of the August project was to be able to analyze how novice users interact with a multi-modal information kiosk, placed without supervision in a public location. Another goal was to demonstrate how the speech technology modules developed at the department could be put together to rapidly prototype a multi-modal spoken dialogue system.\n",
    ""
   ]
  },
  "carsonberndsen99_ids": {
   "authors": [
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "A feature geometry based lexicon model for speech applications",
   "original": "ids9_065",
   "page_count": 4,
   "order": 26,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "This paper proposes a generic feature geometry based lexical representation of phonological description using inheritance networks which has applications in various areas of speech technology. The feature geometry based lexicon model provides the linguistic knowledge for a generic lexicon tool which allows application-specific lexica to be generated. The relevance of this approach to interactive dialogue in multi-modal systems lies in the representation of parallel information which is structured with respect to temporal relations. This paper first presents the formal modelling of the feature geometry based lexical representation and then provides examples of specific output formats discussing in particular the temporal coordination of multilinear representations for speech technology applications.\n",
    ""
   ]
  },
  "kawahara99c_ids": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Katsuaki",
     "Tanaka"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Domain-independent platform of spoken dialogue interfacesfor information query",
   "original": "ids9_069",
   "page_count": 4,
   "order": 27,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "A domain-independent platform of spoken dialogue systems is presented. It assumes that information query is done by a sequence of key-phrases corresponding to search keys, which can be defined by a domain database. A GUI is incorporated to display typical phrase patterns for guidance and to promptly present recognition and query results for confirmation. The platform consists of general-purpose speech understanding modules and task specification tools that semi-automatically derive a lexicon and dialogue management rules from the domain database. It is applied to two different domains: hotel search system and literature search system. It drastically saves the labor cost and time in developing the prototype systems. And the strategy of key-phrase-based dialogue is even more effective than the conventional tuned system.\n",
    ""
   ]
  },
  "ortmanns99_ids": {
   "authors": [
    [
     "Stefan",
     "Ortmanns"
    ],
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "Wu",
     "Chou"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "An efficient decoding approach for dialogue systems",
   "original": "ids9_073",
   "page_count": 4,
   "order": 28,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "We present a decoder for dialogue systems having both automatic speech recognition (ASR) and natural language (NL) understanding. Accurate and fast decoding of spoken utterances is a key to speech understanding. A good set of multiple word hypotheses produced by a decoder is also crucial for flexible integration of knowledge sources for language understanding. We introduce three contributions to improve the search efficiency and the effectiveness of our decoder, namely: (1) handling of long-term language models with crossword triphone models, (2) speed-up of search and likelihood computation; and (3) construction of high quality word graphs for interfaces between the ASR and NL understanding modules. The search algorithm was successfully applied to a natural language call routing task in a banking domain. As a result, we achieve both, word hypothesis decoding and word graph generation in real time with nearly no loss in word error rate in speech recognition.\n",
    ""
   ]
  },
  "cole99b_ids": {
   "authors": [
    [
     "Ron",
     "Cole"
    ],
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Dan",
     "Jurafsky"
    ],
    [
     "Lecia J.",
     "Barker"
    ]
   ],
   "title": "Interactive learning tools for human language technology",
   "original": "ids9_077",
   "page_count": 4,
   "order": 29,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Developing an accessible curriculum of laboratory courses for undergraduate students is vital to progress in human language technology. In this article, we describe means to provide students with access to leading-edge language technologies, and tools to combine these technologies in spoken dialogue systems of their own design. The tools and technologies used in the proposed laboratory courses will enable students to build interactive dialogue systems for new and exciting applications and to research and perhaps improve the core language technologies. By making them freely available (via the Internet or CD-ROM) with documentation and support, our community can remove some of the main entry barriers to developing new programs in human language technology in our colleges and universities. In this way, students are not only exposed to new technology, they become involved in the process of creating it.\n",
    ""
   ]
  },
  "bell99_ids": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Utterance types in the August dialogues",
   "original": "ids9_081",
   "page_count": 4,
   "order": 30,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "August, a Swedish multi-modal spoken dialogue system featuring an animated agent, was used to collect a database of spontaneous computer-directed speech. The system was designed with several simple domains rather than a single complex one. It was installed in a public location in the center of Stockholm and was available for the general public during six months. The people who interacted with the system were given little or no information on what they could expect the system to understand. In this paper, we draw on the experiences gathered in course of the development of the August system and discuss findings in a database of more than 10,000 utterances. The aim of the paper is to address the issue of how the different types of utterances found in the database reflect the demands and strategies of the users. The categorization of the speech input into utterance types and the subsequent analysis of these types will be discussed, and possible implications for modeling future dialogue systems will be covered briefly.\n",
    ""
   ]
  },
  "kikuchi99_ids": {
   "authors": [
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Controlling dialogue strategy according to performance of processes",
   "original": "ids9_085",
   "page_count": 4,
   "order": 31,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "We aim at the establishment of the method of controlling dialogues according to performance in computation. So that the system takes the most suitable dialogue strategy according to performance in computation, it needs to calculate evaluation functions modeled with performance in computation in the inside all time. In this paper, we propose degree of presenting process status as evaluation function and model it with computation time. Then, we simulate this model by controlling dialogue strategies. We also confirm efficiency of the model by preliminary experiment using the prototype of spoken dialogue system.\n",
    ""
   ]
  },
  "larsen99_ids": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ]
   ],
   "title": "Combining objective and subjective data in evaluation ofspoken dialogues",
   "original": "ids9_089",
   "page_count": 4,
   "order": 32,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "Evaluation of human-computer spoken dialogues is often based on analyses of objective metrics such as task completion rate, turn-taking, time consumption, etc. While these data are easily obtained and processed from log files, they offer or very little information about the actual usability of the given service as perceived by the test subjects.\n",
    "This study combines the evaluation based on objective data obtained from log files with subjective data, where the test subjects express their attitudes to a number issues directly related to the usability of the service. It is shown how the joint analysis can be used to support, but also question findings from either source.\n",
    ""
   ]
  },
  "wessel99_ids": {
   "authors": [
    [
     "Frank",
     "Wessel"
    ],
    [
     "Andrea",
     "Baader"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "A comparison of dialogue-state dependent language models",
   "original": "ids9_093",
   "page_count": 4,
   "order": 33,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "Dialogue-state dependent language models in automatic inquiry systems can be employed to improve speech recognition and understanding. In this paper, the dialogue state is defined by the set of parameters contained in the system prompt. Using this knowledge, a separate language model for each state can be constructed.\n",
    "In order to obtain robust language models we study the linear interpolation of all dialogue-state dependent language models and an automatic text clustering algorithm. In particular, we extend the clustering algorithm so as to automatically determine the optimal number of clusters. These clusters are then be combined with linear interpolation.\n",
    "We present experimental results on a Dutch corpus which has been recorded in the Netherlands with a train timetable information system in the framework of the ARISE project [1]. The perplexity, the word error rate, and the attribute error rate can be reduced significantly with all of these methods.\n",
    ""
   ]
  },
  "janiszek99_ids": {
   "authors": [
    [
     "David",
     "Janiszek"
    ],
    [
     "Renato de",
     "Mori"
    ],
    [
     "Frédéric",
     "Bechet"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Chafik",
     "Mokbel"
    ]
   ],
   "title": "New language model adaptation algorithm based on the definition ofcardinal distance",
   "original": "ids9_097",
   "page_count": 4,
   "order": 34,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "Linear transformations are proposed for transforming vectors of Language Model (LM) probabilities. A separate vector is considered for each word and the j-th element of a vector is the probability of observing the word in the context of its j-th history. If a good general LM is available, it is possible to cluster vectors into classes and to infer a transformation for each class. Probability distributions of words which are not observed or which are observed with a low frequency in the adaptation corpus can be obtained by transforming the distribution they have in the general model using the transformation of the cluster they belong to. Experimental results show that there is a interesting range in the size of the adaptation corpus in which perplexity of the adapted LM is lower than the perplexity of the LM whose probabilities are directly estimated from the adaptation data.\n",
    ""
   ]
  },
  "reichl99_ids": {
   "authors": [
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "Stefan",
     "Ortmanns"
    ]
   ],
   "title": "Integrated natural language call routing",
   "original": "ids9_101",
   "page_count": 4,
   "order": 35,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "We present a new integrated approach for natural language call routing based on stochastic language models. The system learns automatically from examples to direct a call to the appropriate destination within a call center. It employs stochastic language models for each call destination. The language models are generated by a language model adaptation algorithmbased on the minimum discrimination information. The call routing approach is consistent with the stochastic pattern recognition paradigm and can be easily integrated in a automatic speech recognition (ASR) system. It results in over 92% correctly routed calls in a natural language call center application.\n",
    ""
   ]
  },
  "bernsen99_ids": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjær"
    ]
   ],
   "title": "A theory of speech in multimodal systems",
   "original": "ids9_105",
   "page_count": 4,
   "order": 36,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "Increasingly, speech input and/or speech output is being used in combination with other modalities for the representation and exchange of information with, or mediated by, computer systems. Therefore, a growing number of developers of systems and interfaces are faced with the question of whether or not to use speech input and/or speech output in multimodal combinations for the applications they are about to build. This paper presents first results on speech in multimodal systems from a test of a theory-based approach to speech functionality. The test used a large corpus of claims about speech functionality derived from the recent literature.\n",
    ""
   ]
  },
  "parker99_ids": {
   "authors": [
    [
     "Liam",
     "Parker"
    ]
   ],
   "title": "Information overload in personified verbal multimodal interfaces",
   "original": "ids9_109",
   "page_count": 4,
   "order": 37,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "The author presents an experiment which critically examines whether current state of the art text to speech synthesisers and web based VRML style graphics can be utilised to provide a multimodal persona presence in less than ideal user conditions. Audio visual anthropomorphised representations of online agents are becoming more common and their use in commercial systems has to be examined in order to ascertain whether multimodal processes such as the McGurk effect may limit their use. An experiment is outlined that is based around a web tele-shopping service with the users placed under adverse stress through information overload. The ability of the participants to successfully complete their task is objectively measured and conclusions are drawn on the suitability of current day speech synthesisers and VRML modelling for persona based human computer interface output.\n",
    ""
   ]
  },
  "anibaldi99_ids": {
   "authors": [
    [
     "Luca",
     "Anibaldi"
    ],
    [
     "Seán",
     "Ó Nuallián"
    ]
   ],
   "title": "A computational multilayered model for theinterpretation of locative expressions",
   "original": "ids9_113",
   "page_count": 4,
   "order": 38,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "In this paper, we outline some practical solutions to some classical problems in spatial cognition. In pa rticular, we focus on the controversial area of the pragmatic interpretation of spatial preposition and verbs. Our work is given particular urgency by the need to produce solutions which are tractable enough to be implemented immediately in an NL interface to a mult imedia environment whose graphics component is written in VRML.\n",
    "The first step therefore, is to describe this problem. We then proceed to discuss various current controversies, for example, whether the Landau and Jackendoff claim that a single spatial representation suffices for geometric objects and the relations between them is correct. Alternatively, as we believe, it may be the case that an altogether more sophisticated multilayered model is necessary to trace a path from the use of spatial language to viewing its pragmatic interpretation in the real or virtual world. We proceed to outline our approach. Finally, we indicate how this approache works in the context of the SONAS system.\n",
    ""
   ]
  },
  "sowa99_ids": {
   "authors": [
    [
     "Timo",
     "Sowa"
    ],
    [
     "Ipke",
     "Wachsmuth"
    ]
   ],
   "title": "Understanding coverbal dimensional gestures in a virtual designenvironment",
   "original": "ids9_117",
   "page_count": 4,
   "order": 39,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "Today's multimodal systems, which allow full-body (3D) gestures and speech as input modalities, are quite restricted to easily interpretable coverbal gestures with a predefined shape and meaning. In this paper, we propose methods to abstract the concrete shape of gestures by using high-level features and to integrate them with coexpressive words using their phonological attributes. The application of this approach is discussed for a class of gestures useful in virtual design. We sketch our technical environment and first implementation approaches to build a prototype system.\n",
    ""
   ]
  },
  "verlinde99_ids": {
   "authors": [
    [
     "Patrick",
     "Verlinde"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Marc",
     "Acheroy"
    ]
   ],
   "title": "About multi-modal identity verification in interactive dialogue systems",
   "original": "ids9_121",
   "page_count": 4,
   "order": 40,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "The aim of this paper is to introduce multi­modal identity verification techniques to the Interactive Dialogue Systems society, and to identify possible applications. The multi­modal identity veri\u0002cation system presented here is based on two biometric modalities (speech and vision) and it uses 3 experts (using voice, frontal face and profile images), based on these two modalities, in parallel. Each expert delivers as output a scalar number, called score, stating how well the claimed identity is veri\u0002ed. A fusion module receiving as input the 3 scores has to take a binary decision: accept or reject identity. We have solved this fusion problem using a wide range of statistical pattern recognition techniques. The performances of the different fusion modules have been evaluated and compared on a multi­modal database, containing both vocal and visual modalities.\n",
    ""
   ]
  },
  "wolff99_ids": {
   "authors": [
    [
     "Frédéric",
     "Wolff"
    ],
    [
     "Nadia",
     "Bellalem"
    ]
   ],
   "title": "Framework for spontaneous 3D referring gestures analysis",
   "original": "ids9_125",
   "page_count": 3,
   "order": 41,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "This paper presents a framework proposal aiming at analysing 3D spontaneous referring gestures which occur in multimodal interactions. Our approach is co mposed by two main steps: a structural segmentation of 3D continuous gestural signal and a perception or iented interpretation stage. Both modules which already exist have now to be merged, and the whole process validated with actual trajectories.\n",
    ""
   ]
  },
  "nijholt99_ids": {
   "authors": [
    [
     "Anton",
     "Nijholt"
    ],
    [
     "Joris",
     "Hulstijn"
    ],
    [
     "Arjan van",
     "Hessen"
    ]
   ],
   "title": "Speech and language interactions in a web theatre environment",
   "original": "ids9_129",
   "page_count": 4,
   "order": 42,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "We discuss research on interaction in a virtual theatre that can be accessed through Web pages. In the environment we employ several agents. The virtual theatre allows navigation through keyboard and mouse, but there is also a navigation agent which listens to typed input and spoken commands. We also have an information agent which allows a NL dialogue, where input is keyboard- driven and output is by tables and template driven NL generation. In development are talking faces for the agents. A users commitment to this environment is increased by increasing presence.\n",
    ""
   ]
  },
  "brndsted99b_ids": {
   "authors": [
    [
     "Tom",
     "Brøndsted"
    ]
   ],
   "title": "Reference problems in Chameleon",
   "original": "ids9_133",
   "page_count": 4,
   "order": 43,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "This paper discusses the problem of endophoric/deictic references in multimodal dialogues involving speech and pointing gestures. The dialogue system in focus is a building information system implemented using a general workbench architecture \"Chameleon\". Chameleon and the building information system have been developed within the IntelliMedia 2000+ project initiated 1996 by the Institute of Electronic Systems, Aalborg University [1]. Further development and experiments were carried out in the EU Esprit-project 24 493 \"Language and Image Data Fusion using Stochastic Models and Spatial Context Modeling\" together with LIMSI and Bertin (coordinator), France [2]. We focus on two problems (1) cross-media references and (2) crossuser/system references.\n",
    ""
   ]
  },
  "adams99_ids": {
   "authors": [
    [
     "L. J.",
     "Adams"
    ],
    [
     "Robert I.",
     "Damper"
    ],
    [
     "S.",
     "Harnad"
    ],
    [
     "W.",
     "Hall"
    ]
   ],
   "title": "A system design for human factors studies ofspeech-enabled web browsing",
   "original": "ids9_137",
   "page_count": 4,
   "order": 44,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "This paper describes the design of a system which will subsequently be used as the basis of a range of empirical studies aimed at discovering how best to harness speech recognition capabilities in multimodal multimedia computing. Initial work focuses on speech-enabled browsing of the World Wide Web, which was never designed for such use. System design is complete, and is being evaluated via usability testing.\n",
    ""
   ]
  },
  "kuo99_ids": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Information and services manager customizes dialogue-based applications",
   "original": "ids9_141",
   "page_count": 4,
   "order": 45,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "An Information and Services Manager has been developed in order to provide user-customized information and services through a speech interface. The ISM has three main functions: search and download documents of interest to a user, summarize available information according to the users preference, and allow the user to query and search for additional information. The ISM employs a user profile to select services and documents, as well as to define the presentation format. It also provides the information needed for the Auto-Dialogue Generator module to create language models for automatic speech recognition and for text-to-speech system prompts.\n",
    ""
   ]
  },
  "bellik99_ids": {
   "authors": [
    [
     "Yacine",
     "Bellik"
    ],
    [
     "Siwar",
     "Farhat"
    ]
   ],
   "title": "Customizable web access for the blind",
   "original": "ids9_145",
   "page_count": 4,
   "order": 46,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "This article deals with blind user access to the Web. We first present the different existing approaches that facilitate Internet access to blind users and the different ergonomic rules that HTML page designers should respect. Then, we present our approach for a dynamic improvement of the accessibility. This approach uses a customization interface that allows the user to choose the mod ifications he would like to perform on the Web pages and the information to visualize. Our system, SeeWeb, should operate between the user and a traditional navigator (Netscape, Internet Explorer, etc). It allows the analysis of the HTML document and its content. It also enables to modify the page structure, according to users preferences and some ergonomic rules. The user interface coupled to a screen reader allows the restitution of the information in a more comprehensive way for the blind user. This work has been carried out as a coope ation between LIMSI-CNRS (a research laboratory) and TECHNIBRAILLE (an industrial partner).\n",
    ""
   ]
  },
  "chase99_ids": {
   "authors": [
    [
     "Lin L.",
     "Chase"
    ]
   ],
   "title": "Important open questions: telephone-based dialog systems",
   "original": "ids9_149",
   "page_count": 3,
   "order": 47,
   "p1": "149",
   "pn": "151",
   "abstract": [
    "Per a discussion with Paul Heisterkamp in Cambridge on January 21, 1999, I would like to suggest the following poster topic for you \"tea lounge discussion area\". This is simply a list of open questions that I find important in dealing with the development and deployment of live spoken language dialog systems, as situated in actual industrial contexts. This submission is certainly not a scientific contribution in any sense other than it poses som questions that may possibly interest researchers. I would of course be excited to find that some of them have already been answered.\n",
    ""
   ]
  },
  "martin99_ids": {
   "authors": [
    [
     "Jean-Claude",
     "Martin"
    ]
   ],
   "title": "Is your multimodal software better than mine?How to compare multimodal modules from a practical software engineering point of view",
   "original": "ids9_153",
   "page_count": 0,
   "order": 48,
   "p1": "153",
   "pn": "",
   "abstract": [
    "Several multimodal modules combining several input modalities such as speech and gesture have been alread implemented (cf CMC'98, IJCAI'97, CMC'95, IMMI95). Comparison of these multimodal modules can be done on the basis of their evaluation during user studies. Yet, user studies are done with different protocols from one multimodal module to the other. Hence comparison is difficult. Multimodal systems can also be compared on th basis of the number and complexity of modality they process. Yet, it seems weird to take into account the monomodal aspects of multimodal modules in their comparison.\n",
    "In this paper, we suggest two new criteria for comparing multimodal modules from a software-engineering point of view: how well do these multimodal modules address the upgrading and composing problems.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "sadek99_ids",
    "thomson99_ids",
    "strong99_ids"
   ]
  },
  {
   "title": "Tutorial - Dialogue Tools",
   "papers": [
    "cole99_ids",
    "brndsted99_ids",
    "kawahara99_ids",
    "lee99_ids",
    "gustafson99_ids",
    "potamianos99_ids"
   ]
  },
  {
   "title": "Dialogue Applications",
   "papers": [
    "sturm99_ids",
    "kawahara99b_ids",
    "fabbrizio99_ids"
   ]
  },
  {
   "title": "Multimodal Systems",
   "papers": [
    "salmen99_ids",
    "wyard99_ids",
    "streit99_ids"
   ]
  },
  {
   "title": "Spoken Language Generation",
   "papers": [
    "lee99b_ids",
    "hirose99_ids",
    "esposito99_ids"
   ]
  },
  {
   "title": "Spoken Dialogue: Systems and Issues (Poster Session)",
   "papers": [
    "pargellis99_ids",
    "kurematsu99_ids",
    "tanigaki99_ids",
    "boros99_ids",
    "dahlback99_ids",
    "minker99_ids",
    "gustafson99b_ids",
    "carsonberndsen99_ids",
    "kawahara99c_ids",
    "ortmanns99_ids",
    "cole99b_ids"
   ]
  },
  {
   "title": "Dialogue Management and Evaluation",
   "papers": [
    "bell99_ids",
    "kikuchi99_ids",
    "larsen99_ids"
   ]
  },
  {
   "title": "Language Modelling",
   "papers": [
    "wessel99_ids",
    "janiszek99_ids",
    "reichl99_ids"
   ]
  },
  {
   "title": "Multimodal Dialogue: Systems and Issues (Poster)",
   "papers": [
    "bernsen99_ids",
    "parker99_ids",
    "anibaldi99_ids",
    "sowa99_ids",
    "verlinde99_ids",
    "wolff99_ids",
    "nijholt99_ids",
    "brndsted99b_ids"
   ]
  },
  {
   "title": "Dialogue and Web Access",
   "papers": [
    "adams99_ids",
    "kuo99_ids",
    "bellik99_ids"
   ]
  },
  {
   "title": "Open and Unanswered Questions",
   "papers": [
    "chase99_ids",
    "martin99_ids"
   ]
  }
 ]
}