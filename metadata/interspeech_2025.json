{
 "title": "Interspeech 2025",
 "startDate": "17/08/2025",
 "endDate": "21/08/2025",
 "URL": "https://www.interspeech2025.org",
 "location": "Rotterdam, The Netherlands",
 "ISSN": "2958-1796",
 "chair": "Chairs: Odette Scharenborg, Catharine Oertel, Khiet Truong",
 "series": "Interspeech",
 "intro": "./booklet.pdf",
 "conf": "Interspeech",
 "name": "interspeech_2025",
 "year": "2025",
 "SIG": "",
 "title1": "Interspeech 2025",
 "booklet": "./booklet.pdf",
 "date": "17-21 August 2025",
 "month": 8,
 "day": 17,
 "now": 1755182925381162,
 "papers": {
  "xiao25_interspeech": {
   "authors": [
    [
     "Yang",
     "Xiao"
    ],
    [
     "Rohan Kumar",
     "Das"
    ]
   ],
   "title": "TF-Mamba: A Time-Frequency Network for Sound Source Localization",
   "original": "9",
   "order": 196,
   "page_count": 5,
   "abstract": [
    "Sound source localization (SSL) determines the position of sound sources using multi-channel audio data. It is commonly used to improve speech enhancement and separation. Extracting spatial features is crucial for SSL, especially in challenging acoustic environments. Recently, a novel structure referred to as Mamba demonstrated notable performance across various sequence-based modalities. This study introduces the Mamba for SSL tasks. We consider the Mamba-based model to analyze spatial features from speech signals by fusing both time and frequency features, and we develop an SSL system called TF-Mamba. This system integrates time and frequency fusion, with Bidirectional Mamba managing both time-wise and frequency-wise processing. We conduct the experiments on the simulated and real datasets. Experiments show that TF-Mamba significantly outperforms other advanced methods. The code will be publicly released in due course."
   ],
   "p1": 948,
   "pn": 952,
   "doi": "10.21437/Interspeech.2025-9",
   "url": "interspeech_2025/xiao25_interspeech.html"
  },
  "horiguchi25_interspeech": {
   "authors": [
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Naohiro",
     "Tawara"
    ]
   ],
   "title": "Mitigating Non-Target Speaker Bias in Guided Speaker Embedding",
   "original": "10",
   "order": 1062,
   "page_count": 5,
   "abstract": [
    "Obtaining high-quality speaker embeddings in multi-speaker conditions is crucial for many applications. A recently proposed guided speaker embedding framework, which utilizes speech activities of target and non-target speakers as clues, drastically improved embeddings under severe overlap with small degradation in low-overlap cases. However, since extreme overlaps are rare in natural conversations, this degradation cannot be overlooked. This paper first reveals that the degradation is caused by the global-statistics-based modules, widely used in speaker feature extractors, being overly sensitive to intervals containing only non-target speakers. As a countermeasure, we propose an extension of such modules that exploit the target speaker activity clues, to compute statistics from intervals where the target is active. The proposed method improves speaker verification performance in both low and high overlap ratios, and diarization performance on multiple datasets."
   ],
   "p1": 5208,
   "pn": 5212,
   "doi": "10.21437/Interspeech.2025-10",
   "url": "interspeech_2025/horiguchi25_interspeech.html"
  },
  "xiao25b_interspeech": {
   "authors": [
    [
     "Yang",
     "Xiao"
    ],
    [
     "Tianyi",
     "Peng"
    ],
    [
     "Yanghao",
     "Zhou"
    ],
    [
     "Rohan Kumar",
     "Das"
    ]
   ],
   "title": "AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation",
   "original": "15",
   "order": 1103,
   "page_count": 5,
   "abstract": [
    "Spoken keyword spotting (KWS) aims to identify keywords in audio for wide applications, especially on edge devices. Current small-footprint KWS systems focus on efficient model designs. However, their inference performance can decline in unseen environments or noisy backgrounds. Test-time adaptation (TTA) helps models adapt to test samples without needing the original training data. In this study, we present AdaKWS, the first TTA method for robust KWS to the best of our knowledge. Specifically, 1) We initially optimize the model&#x27;s confidence by selecting reliable samples based on prediction entropy minimization and adjusting the normalization statistics in each batch. 2) we introduce pseudo-keyword consistency (PKC) to identify critical, reliable features without overfitting to noise. Our experiments show that AdaKWS outperforms other methods across various conditions, including Gaussian noise and real-scenario noises. The code will be released in due course."
   ],
   "p1": 5408,
   "pn": 5412,
   "doi": "10.21437/Interspeech.2025-15",
   "url": "interspeech_2025/xiao25b_interspeech.html"
  },
  "xiao25c_interspeech": {
   "authors": [
    [
     "Yang",
     "Xiao"
    ],
    [
     "Rohan Kumar",
     "Das"
    ]
   ],
   "title": "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing",
   "original": "16",
   "order": 319,
   "page_count": 5,
   "abstract": [
    "As deepfake speech becomes common and hard to detect, it is vital to trace its source. Recent work on audio deepfake source tracing (ST) aims to find the origins of synthetic or manipulated speech. However, ST models must adapt to learn new deepfake attacks while retaining knowledge of the previous ones. A major challenge is catastrophic forgetting, where models lose the ability to recognize previously learned attacks. Some continual learning methods help with deepfake detection, but multi-class tasks such as ST introduce additional challenges as the number of classes grows. To address this, we propose an analytic class incremental learning method called AnaST. When new attacks appear, the feature extractor remains fixed, and the classifier is updated with a closed-form analytical solution in one epoch. This approach ensures data privacy, optimizes memory usage, and is suitable for online training. The experiments carried out in this work show that our method outperforms the baselines."
   ],
   "p1": 1563,
   "pn": 1567,
   "doi": "10.21437/Interspeech.2025-16",
   "url": "interspeech_2025/xiao25c_interspeech.html"
  },
  "zhang25_interspeech": {
   "authors": [
    [
     "Xiangyu",
     "Zhang"
    ],
    [
     "Daijiao",
     "Liu"
    ],
    [
     "Tianyi",
     "Xiao"
    ],
    [
     "Cihan",
     "Xiao"
    ],
    [
     "Tünde",
     "Szalay"
    ],
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Beena",
     "Ahmed"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "Auto-Landmark: Acoustic Landmark Dataset and Open-Source Toolkit for Landmark Extraction",
   "original": "17",
   "order": 869,
   "page_count": 5,
   "abstract": [
    "In the speech signal, acoustic landmarks identify times when the acoustic manifestations of the linguistically motivated distinctive features are most salient. Acoustic landmarks have been widely applied in various domains, including speech recognition, speech depression detection, clinical analysis of speech abnormalities, and the detection of disordered speech. However, there is currently no dataset available that provides precise timing information for landmarks, which has been proven to be crucial for downstream applications involving landmarks. In this paper, we selected the most useful acoustic landmarks based on previous research and annotated the TIMIT dataset with them, based on a combination of phoneme boundary information and manual inspection. Moreover, previous landmark extraction tools were not open source or benchmarked, so to address this, we developed an open source Python-based landmark extraction tool and established a series of landmark detection baselines. The first of their kinds, the dataset with landmark precise timing information, landmark extraction tool and baselines are designed to support a wide variety of future research"
   ],
   "p1": 4263,
   "pn": 4267,
   "doi": "10.21437/Interspeech.2025-17",
   "url": "interspeech_2025/zhang25_interspeech.html"
  },
  "li25_interspeech": {
   "authors": [
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Haoning",
     "Xu"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Huimeng",
     "Wang"
    ],
    [
     "Youjun",
     "Chen"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Towards One-bit ASR: Extremely Low-bit Conformer Quantization Using Co-training and Stochastic Precision",
   "original": "18",
   "order": 402,
   "page_count": 5,
   "abstract": [
    "Model compression has become an emerging need as the sizes of modern speech systems rapidly increase. In this paper, we study model weight quantization, which directly reduces the memory footprint to accommodate computationally resource-constrained applications. We propose novel approaches to perform extremely low-bit (i.e., 2-bit and 1-bit) quantization of Conformer automatic speech recognition systems using multiple precision model co-training, stochastic precision, and tensor-wise learnable scaling factors to alleviate quantization incurred performance loss. The proposed methods can achieve performance-lossless 2-bit and 1-bit quantization of Conformer ASR systems trained with the 300-hr Switchboard and 960-hr LibriSpeech corpus. Maximum overall performance-lossless compression ratios of 16.2 and 16.6 times are achieved without a statistically significant increase in the word error rate (WER) over the full precision baseline systems, respectively."
   ],
   "p1": 1973,
   "pn": 1977,
   "doi": "10.21437/Interspeech.2025-18",
   "url": "interspeech_2025/li25_interspeech.html"
  },
  "pei25_interspeech": {
   "authors": [
    [
     "Yan Ru",
     "Pei"
    ],
    [
     "Ritik",
     "Shrivastava"
    ],
    [
     "Fnu",
     "Sidharth"
    ]
   ],
   "title": "Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
   "original": "19",
   "order": 12,
   "page_count": 5,
   "abstract": [
    "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network&#x27;s performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments."
   ],
   "p1": 51,
   "pn": 55,
   "doi": "10.21437/Interspeech.2025-19",
   "url": "interspeech_2025/pei25_interspeech.html"
  },
  "muller25_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Müller"
    ],
    [
     "Piotr",
     "Kawa"
    ],
    [
     "Wei-Herng",
     "Choong"
    ],
    [
     "Adriana",
     "Stan"
    ],
    [
     "Aditya Tirumala",
     "Bukkapatnam"
    ],
    [
     "Karla",
     "Pizzi"
    ],
    [
     "Alexander",
     "Wagner"
    ],
    [
     "Philip",
     "Sperl"
    ]
   ],
   "title": "Replay Attacks Against Audio Deepfake Detection",
   "original": "20",
   "order": 460,
   "page_count": 5,
   "abstract": [
    "We show how replay attacks undermine audio deepfake detection: By playing and re-recording deepfake audio through various speakers and microphones, we make spoofed samples appear authentic to the detection model. \n",
    "To study this phenomenon in more detail, we introduce ReplayDF, a dataset of recordings derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations across six languages and four TTS models. It includes diverse acoustic conditions, some highly challenging for detection.\n",
    "Our analysis of six open-source detection models across five datasets reveals significant vulnerability, with the top-performing W2V2-AASIST model&#x27;s Equal Error Rate (EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response (RIR) retraining, performance remains compromised with an 11.0% EER. We release ReplayDF for non-commercial research use."
   ],
   "p1": 2245,
   "pn": 2249,
   "doi": "10.21437/Interspeech.2025-20",
   "url": "interspeech_2025/muller25_interspeech.html"
  },
  "chi25_interspeech": {
   "authors": [
    [
     "Hyung-gun",
     "Chi"
    ],
    [
     "Zakaria",
     "Aldeneh"
    ],
    [
     "Tatiana",
     "Likhomanenko"
    ],
    [
     "Oggi",
     "Rudovic"
    ],
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Li-Wei",
     "Chen"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ]
   ],
   "title": "DiceHuBERT: Distilling HuBERT with a Self-Supervised Learning Objective",
   "original": "29",
   "order": 250,
   "page_count": 5,
   "abstract": [
    "We introduce DiceHuBERT, a knowledge distillation framework for compressing HuBERT, a widely used self-supervised learning (SSL)-based speech foundation model. Unlike existing distillation methods that rely on layer-wise and feature-wise mapping between teacher and student models, DiceHuBERT leverages  HuBERT&#x27;s iterative self-distillation mechanism by directly replacing the original model with a student model. This replacement allows the student to be trained using the same SSL objective used when pre-training HuBERT, eliminating the need for additional modules or architectural constraints. Experimental results on SUPERB show that DiceHuBERT consistently outperforms existing distillation methods, improving phoneme recognition performance by over 21% and ASR performance by more than 14%. Furthermore, DiceHuBERT demonstrates competitive performance across multiple tasks, highlighting its clear advantage."
   ],
   "p1": 1218,
   "pn": 1222,
   "doi": "10.21437/Interspeech.2025-29",
   "url": "interspeech_2025/chi25_interspeech.html"
  },
  "chi25b_interspeech": {
   "authors": [
    [
     "Hyung-gun",
     "Chi"
    ],
    [
     "Florian",
     "Pesce"
    ],
    [
     "Wonil",
     "Chang"
    ],
    [
     "Oggi",
     "Rudovic"
    ],
    [
     "Arturo",
     "Argueta"
    ],
    [
     "Stefan",
     "Braun"
    ],
    [
     "Vineet",
     "Garg"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ]
   ],
   "title": "Adaptive Knowledge Distillation for Device-Directed Speech Detection",
   "original": "30",
   "order": 1179,
   "page_count": 5,
   "abstract": [
    "Device-directed speech detection (DDSD) is a binary classification task that separates the user’s queries to a voice assistant (VA) from background speech or side conversations. This is important for achieving naturalistic user experience. To this end, we propose knowledge distillation (KD) to enhance DDSD accuracy while ensuring efficient deployment. Specifically, we introduce a novel adaptive KD method that transfers knowledge from general representations of an ASR large pre-trained acoustic encoder (teacher). We apply task-specific adapters, on top of the (frozen) teacher encoder, trained jointly with the student model on DDSD. We demonstrate that the proposed adaptive KD outperforms the student model without distillation in the keyword and keyword-free (follow-up) invocations, with an improvement of +26% and +19% in terms of Equal Error Rate, respectively. We also show that this approach generalizes across the transformer and conformer-based model architectures."
   ],
   "p1": 5788,
   "pn": 5792,
   "doi": "10.21437/Interspeech.2025-30",
   "url": "interspeech_2025/chi25b_interspeech.html"
  },
  "whetten25_interspeech": {
   "authors": [
    [
     "Ryan",
     "Whetten"
    ],
    [
     "Lucas",
     "Maison"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Marco",
     "Dinarelli"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "Towards Early Prediction of Self-Supervised Speech Model Performance",
   "original": "34",
   "order": 252,
   "page_count": 5,
   "abstract": [
    "In Self-Supervised Learning (SSL), pre-training and evaluation are resource intensive. In the speech domain, current indicators of the quality of SSL models during pre-training, such as the loss, do not correlate well with downstream performance. Consequently, it is often difficult to gauge the final downstream performance in a cost efficient manner during pre-training. In this work, we propose unsupervised efficient methods that give insights into the pre-training quality of SSL speech models, namely, measuring the cluster quality and rank of the embeddings produced by the SSL model. Results show that measures of cluster quality and rank correlate better with downstream performance than the pre-training loss, reducing the need for GPU hours and labeled data in SSL model evaluation."
   ],
   "p1": 1228,
   "pn": 1232,
   "doi": "10.21437/Interspeech.2025-34",
   "url": "interspeech_2025/whetten25_interspeech.html"
  },
  "zhao25_interspeech": {
   "authors": [
    [
     "Bingliang",
     "Zhao"
    ],
    [
     "Xiyu",
     "Wu"
    ]
   ],
   "title": "Investigating Glottal Stop Coda Loss During Sound Change of Checked Syllables Based on Speech-EGG Voice Offset Alignment",
   "original": "38",
   "order": 603,
   "page_count": 5,
   "abstract": [
    "The glottal stop coda in Wu Chinese checked syllables diachronically weakens into non-modal phonation on the preceding vowel and finally disappears. It&#x27;s crucial to capture the initial stage of this sound change when the glottal stop coda is realized as an abrupt glottal closure. Aiming to clarify the age-related differences in the phonetic realizations of the glottal stop coda, we detected the presence of the abrupt glottal closure at voice offset using speech and EGG signals from Shengzhou Wu speakers. Results show that the voice offsets in checked syllables produced by most old speakers remain aligned, indicating the sound change has not been initiated. However, the glottal stop coda is no longer realized as abrupt glottal closure in the younger generation. This study directly measures the glottal stop coda loss through the physiological basis of articulation, potentially providing insights applicable to abrupt glottal closure detection for other linguistic purposes."
   ],
   "p1": 2960,
   "pn": 2964,
   "doi": "10.21437/Interspeech.2025-38",
   "url": "interspeech_2025/zhao25_interspeech.html"
  },
  "jiang25_interspeech": {
   "authors": [
    [
     "Yiheng",
     "Jiang"
    ],
    [
     "Haoxu",
     "Wang"
    ],
    [
     "Yafeng",
     "Chen"
    ],
    [
     "Gang",
     "Qiao"
    ],
    [
     "Biao",
     "Tian"
    ]
   ],
   "title": "Exploring Efficient Directional and Distance Cues for Regional Speech Separation",
   "original": "40",
   "order": 302,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce a neural network-based method for regional speech separation using a microphone array. This approach leverages novel spatial cues to extract the sound source not only from specified direction but also within defined distance. Specifically, our method employs an improved delay-and-sum technique to obtain directional cues, substantially enhancing the signal from the target direction. We further enhance separation by incorporating the direct-to-reverberant ratio into the input features, enabling the model to better discriminate sources within and beyond a specified distance. Experimental results demonstrate that our proposed method leads to substantial gains across multiple objective metrics. Furthermore, our method achieves state-of-the-art performance on the CHiME-8 MMCSG dataset, which was recorded in real-world conversational scenarios, underscoring its effectiveness for speech separation in practical applications."
   ],
   "p1": 1478,
   "pn": 1482,
   "doi": "10.21437/Interspeech.2025-40",
   "url": "interspeech_2025/jiang25_interspeech.html"
  },
  "kim25_interspeech": {
   "authors": [
    [
     "Yubin",
     "Kim"
    ],
    [
     "Taehan",
     "Kim"
    ],
    [
     "Wonjune",
     "Kang"
    ],
    [
     "Eugene",
     "Park"
    ],
    [
     "Joonsik",
     "Yoon"
    ],
    [
     "Dongjae",
     "Lee"
    ],
    [
     "Xin",
     "Liu"
    ],
    [
     "Daniel",
     "McDuff"
    ],
    [
     "Hyeonhoon",
     "Lee"
    ],
    [
     "Cynthia",
     "Breazeal"
    ],
    [
     "Hae Won",
     "Park"
    ]
   ],
   "title": "VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation",
   "original": "41",
   "order": 940,
   "page_count": 5,
   "abstract": [
    "Vocal health plays a crucial role in peoples&#x27; lives, significantly impacting their communicative abilities and interactions. However, despite the global prevalence of voice disorders, many lack access to convenient diagnosis and treatment. This paper introduces VocalAgent, an audio large language model (LLM) to address these challenges through vocal health diagnosis. We leverage Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital patients, and present a multifaceted evaluation framework encompassing a safety assessment to mitigate diagnostic biases, cross-lingual performance analysis, and modality ablation studies. VocalAgent demonstrates superior accuracy on voice disorder classification compared to state-of-the-art baselines. Its LLM-based  method offers a scalable solution for broader adoption of health diagnostics, while underscoring the importance of ethical and technical validation."
   ],
   "p1": 4618,
   "pn": 4622,
   "doi": "10.21437/Interspeech.2025-41",
   "url": "interspeech_2025/kim25_interspeech.html"
  },
  "chen25_interspeech": {
   "authors": [
    [
     "Zhiyong",
     "Chen"
    ],
    [
     "Shuhang",
     "Wu"
    ],
    [
     "Xinnuo",
     "Li"
    ],
    [
     "Zhiqi",
     "Ai"
    ],
    [
     "Shugong",
     "Xu"
    ]
   ],
   "title": "Towards Robust Speaker Recognition against Intrinsic Variation with Foundation Model Few-shot Tuning and Effective Speech Synthesis",
   "original": "42",
   "order": 230,
   "page_count": 5,
   "abstract": [
    "Speaker recognition is essential for secure authentication and personalized voice assistants in smart home settings, but it faces challenges due to intrinsic speaker variability, such as aging and emotional fluctuations. Existing methods often rely on pretraining and require extensive data. To address these challenges, we propose a framework for time-varying and emotion-robust open-set identification (OSI) for smart home environments, utilizing few-shot foundation enrollment-time tuning and style-rich zero-shot text-to-speech (TTS) systems. We explore best practices for synthetic data selection and suitable open-set outlier-focused loss functions. Our proposed method improves handling emotional and aging variations in target speakers, enhancing robustness to intrinsic variability while maintaining resilience to unknown outliers. Experiments demonstrate strong generalization across multiple time-varying and emotionally rich benchmarks."
   ],
   "p1": 1118,
   "pn": 1122,
   "doi": "10.21437/Interspeech.2025-42",
   "url": "interspeech_2025/chen25_interspeech.html"
  },
  "pan25_interspeech": {
   "authors": [
    [
     "Zexu",
     "Pan"
    ],
    [
     "Wupeng",
     "Wang"
    ],
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Chong",
     "Zhang"
    ],
    [
     "Kun",
     "Zhou"
    ],
    [
     "Yukun",
     "Ma"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Online Audio-Visual Autoregressive Speaker Extraction",
   "original": "43",
   "order": 393,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel online audio-visual speaker extraction model. In the streaming regime, most studies optimize the audio network only, leaving the visual frontend less explored. We first propose a lightweight visual frontend based on depth-wise separable convolution. Then, we propose a lightweight autoregressive acoustic encoder to serve as the second cue, to actively explore the information in the separated speech signal from past steps. Scenario-wise, for the first time, we study how the algorithm performs when there is a change in focus of attention, i.e., the target speaker. Experimental results on LRS3 datasets show that our visual frontend performs comparably to the previous state-of-the-art on both SkiM and ConvTasNet audio backbones with only 0.1 million network parameters and 2.1 MACs per second of processing. The autoregressive acoustic encoder provides an additional 0.9 dB gain in terms of SI-SNRi, and its momentum is robust against the change in attention."
   ],
   "p1": 1928,
   "pn": 1932,
   "doi": "10.21437/Interspeech.2025-43",
   "url": "interspeech_2025/pan25_interspeech.html"
  },
  "getman25_interspeech": {
   "authors": [
    [
     "Yaroslav",
     "Getman"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Tommi",
     "Lehtonen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Is your model big enough? Training and interpreting large-scale monolingual speech foundation models",
   "original": "46",
   "order": 48,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning has been widely used in developing speech foundation models. Most languages, however, are only represented in multilingual foundations. We introduce monolingual self-supervised foundation models pre-trained on more than 150,000 hours of Finnish speech and propose a new interpretation technique to understand their capabilities. To our knowledge, this is the largest monolingual data used for self-supervised non-English speech representation learning. Our models demonstrate superior downstream low-resource ASR performance and improved generalization compared to prior work, with absolute WER reductions of up to 14%. Moreover, our proposed interpretation technique, Layer Utilization Rate (LUR), enables us to assess the percentage of neurons in each layer highly contributing towards the output. Empirical results show that the proposed LUR metric can be used to indicate the potential of the fine-tuned model&#x27;s size and architecture to generalize to unseen domains."
   ],
   "p1": 231,
   "pn": 235,
   "doi": "10.21437/Interspeech.2025-46",
   "url": "interspeech_2025/getman25_interspeech.html"
  },
  "kim25b_interspeech": {
   "authors": [
    [
     "Minsu",
     "Kim"
    ],
    [
     "Pingchuan",
     "Ma"
    ],
    [
     "Honglie",
     "Chen"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis",
   "original": "47",
   "order": 771,
   "page_count": 5,
   "abstract": [
    "This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS) where the voice can be generated from face image, and the characteristics of output speech (e.g., pace, noise level, distance, tone, place) can be controllable with natural text description. Specifically, we aim to mitigate the following three challenges in face-driven TTS systems. 1) To overcome the limited audio quality of audio-visual speech corpora, we propose a training method that additionally utilizes high-quality audio-only speech corpora. 2) To generate voices not only from real human faces but also from artistic portraits, we propose augmenting the input face image with stylization. 3) To consider one-to-many possibilities in face-to-voice mapping and ensure consistent voice generation at the same time, we propose to first employ sampling-based decoding and then use prompting with generated speech samples. Experimental results validate the proposed model&#x27;s effectiveness in face-driven voice synthesis."
   ],
   "p1": 3773,
   "pn": 3777,
   "doi": "10.21437/Interspeech.2025-47",
   "url": "interspeech_2025/kim25b_interspeech.html"
  },
  "yang25_interspeech": {
   "authors": [
    [
     "Xue",
     "Yang"
    ],
    [
     "Guiru",
     "Shen"
    ],
    [
     "Yu",
     "Yang"
    ]
   ],
   "title": "Speaker Separation for an Unknown Number of Speakers with Encoder-Decoder-Based Contextual Information Module",
   "original": "49",
   "order": 296,
   "page_count": 5,
   "abstract": [
    "Many speaker separation methods impractically assume that the number of speakers is known in advance. To tackle this issue, an encoder-decoder-based attractor module was proposed to generate multiple speaker attractors. However, the attractors are compact vectors that discard the contextual information. In this paper, an encoder-decoder-based contextual information module is proposed. During training, the LSTM decoder performs multiple iterations to sequentially derive the frame-level representations of different speakers and the mixed signal. During inference, a sufficiently large number of iterations is assumed. The LSTM decoder can autonomously determine at which iteration to derive the frame-level representation of the mixed signal, thereby determining the appropriate channel for its estimation. The number of speakers can be effectively determined through the similarity computation. Experimental results show that high performance is achieved in both speaker counting and separation."
   ],
   "p1": 1448,
   "pn": 1452,
   "doi": "10.21437/Interspeech.2025-49",
   "url": "interspeech_2025/yang25_interspeech.html"
  },
  "ai25_interspeech": {
   "authors": [
    [
     "Zhiqi",
     "Ai"
    ],
    [
     "Meixuan",
     "Bao"
    ],
    [
     "Zhiyong",
     "Chen"
    ],
    [
     "Zhi",
     "Yang"
    ],
    [
     "Xinnuo",
     "Li"
    ],
    [
     "Shugong",
     "Xu"
    ]
   ],
   "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin",
   "original": "57",
   "order": 746,
   "page_count": 5,
   "abstract": [
    "The performance of speaker verification systems is adversely affected by speaker aging. However, due to challenges in data collection, particularly the lack of sustained and large-scale longitudinal data for individuals, research on speaker aging remains difficult. In this paper, we present VoxAging, a large-scale longitudinal dataset collected from 293 speakers (226 English speakers and 67 Mandarin speakers) over several years, with the longest time span reaching 17 years (approximately 900 weeks). For each speaker, the data were recorded at weekly intervals. We studied the phenomenon of speaker aging and its effects on advanced speaker verification systems, analyzed individual speaker aging processes, and explored the impact of factors such as age group and gender on speaker aging research."
   ],
   "p1": 3648,
   "pn": 3652,
   "doi": "10.21437/Interspeech.2025-57",
   "url": "interspeech_2025/ai25_interspeech.html"
  },
  "lyu25_interspeech": {
   "authors": [
    [
     "Sheng",
     "Lyu"
    ],
    [
     "Yuemin",
     "Yu"
    ],
    [
     "Chenshu",
     "Wu"
    ]
   ],
   "title": "Temporal Modeling of Room Impulse Response Generation via Multi-Scale Autoregressive Learning",
   "original": "60",
   "order": 191,
   "page_count": 5,
   "abstract": [
    "The rise of AIGC has revolutionized multimedia processing, including audio applications. Room Impulse Response (RIR), which models sound propagation in acoustic environments,  plays a critical role in various downstream tasks such as speech synthesis. Existing RIR generation methods, whether based on ray tracing or neural representations, fail to fully exploit the temporal dynamics inherent in RIR. In this work, we propose a novel method for temporal modeling of RIR through autoregressive learning. Our approach captures the sequential evolution of sound propagation by introducing a multi-scale generation mechanism that adaptively scales across varying temporal resolutions. Extensive evaluations demonstrate that our approach achieves respective T60 error rates of 4.1% and 5.3% on two real-world datasets, outperforming existing RIR generation methods. We believe our work opens up new directions for future research."
   ],
   "p1": 923,
   "pn": 927,
   "doi": "10.21437/Interspeech.2025-60",
   "url": "interspeech_2025/lyu25_interspeech.html"
  },
  "chen25b_interspeech": {
   "authors": [
    [
     "Jingyi",
     "Chen"
    ],
    [
     "Ju Seung",
     "Byun"
    ],
    [
     "Micha",
     "Elsner"
    ],
    [
     "Pichao",
     "Wang"
    ],
    [
     "Andrew",
     "Perrault"
    ]
   ],
   "title": "Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback",
   "original": "63",
   "order": 702,
   "page_count": 5,
   "abstract": [
    "Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose Diffusion Loss-Guided Policy Optimization (DLPO), an RLHF framework for TTS diffusion models. DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion model&#x27;s structure, improving speech quality. We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with DLPO audio preferred 67% of the time. These findings demonstrate DLPO&#x27;s potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings."
   ],
   "p1": 3454,
   "pn": 3458,
   "doi": "10.21437/Interspeech.2025-63",
   "url": "interspeech_2025/chen25b_interspeech.html"
  },
  "zhu25_interspeech": {
   "authors": [
    [
     "Yaqi",
     "Zhu"
    ],
    [
     "Lei",
     "Zhou"
    ],
    [
     "Hongqing",
     "Liu"
    ],
    [
     "Liming",
     "Shi"
    ],
    [
     "Lu",
     "Gan"
    ]
   ],
   "title": "A Robust Hybrid ACC-PM Approach for Personal Sound Zones",
   "original": "65",
   "order": 787,
   "page_count": 5,
   "abstract": [
    "The performance of personal sound systems is often degraded by inaccurate acoustic measurements. To achieve robust control while balancing acoustic contrast and signal distortion, this work proposes a robust hybrid optimization method that exploits both acoustic contrast control and pressure matching (ACC-PM). The method addresses perturbations caused by uncertainties in the acoustic transfer functions such as temperature changes, head movement, etc, modeled as norm-bounded uncertainties. Although the resulting worst-case optimization is inherently non-convex, it is reformulated as a second-order cone programming problem, which can be efficiently solved. Numerical simulations demonstrate the effectiveness of the proposed robust ACC-PM algorithm, showing an improvement over 18%  in terms of AC compared to vanilla ACC-PM."
   ],
   "p1": 3853,
   "pn": 3857,
   "doi": "10.21437/Interspeech.2025-65",
   "url": "interspeech_2025/zhu25_interspeech.html"
  },
  "ding25_interspeech": {
   "authors": [
    [
     "Yuting",
     "Ding"
    ],
    [
     "Xuefei",
     "Wang"
    ],
    [
     "Fei",
     "Chen"
    ]
   ],
   "title": "Linguistic Masking and Its Release in Simulated Electric-acoustic Hearing",
   "original": "72",
   "order": 176,
   "page_count": 5,
   "abstract": [
    "Combined electric-acoustic stimulation (EAS) with a cochlear implant (CI) and a hearing aid can significantly improve CI user’s speech-in-noise performance, which is noted as combined stimulation advantage. The present study aimed to investigate this advantage in the context of linguistic masking, i.e., when the target speech and masker were from the same or different languages, and how combined stimulation and different linguistic masker affected linguistic release from masking (LRM). Mandarin sentences were mixed with 2-talker babble maskers spoken in Mandarin, Cantonese or English, processed by noise vocoder models simulating CI or EAS hearing, and presented to normal-hearing listeners to recognize. Experimental results showed the combined stimulation advantage under linguistic masking with all three languages. The EAS condition yielded significant difference between the scores of two LRMs (i.e., English vs. Mandarin, and Cantonese vs. Mandarin). Under linguistic masking with Cantonese, there was significant difference between the scores of CI- and EAS-based LRMs. The findings of the present work provided new insights on the benefits for CI users using combined electric-acoustic stimulation."
   ],
   "p1": 848,
   "pn": 852,
   "doi": "10.21437/Interspeech.2025-72",
   "url": "interspeech_2025/ding25_interspeech.html"
  },
  "hu25_interspeech": {
   "authors": [
    [
     "Rui",
     "Hu"
    ],
    [
     "Xiaolong",
     "Lin"
    ],
    [
     "Jiawang",
     "Liu"
    ],
    [
     "Shixi",
     "Huang"
    ],
    [
     "Zhenpeng",
     "Zhan"
    ]
   ],
   "title": "Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation",
   "original": "76",
   "order": 519,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a method for annotating phonemic and prosodic labels on a given audio-transcript pair, aimed at constructing Japanese text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale pre-trained automatic speech recognition (ASR) model, conditioned on ground truth transcripts, to simultaneously output phrase-level graphemes and annotation labels. To further correct errors in phonemic labeling, we employ a decoding strategy that utilizes dictionary prior knowledge. The objective evaluation results demonstrate that our proposed method outperforms previous approaches relying solely on text or audio. The subjective evaluation results indicate that the naturalness of speech synthesized by the TTS model, trained with labels annotated using our method, is comparable to that of a model trained with manual annotations."
   ],
   "p1": 2540,
   "pn": 2544,
   "doi": "10.21437/Interspeech.2025-76",
   "url": "interspeech_2025/hu25_interspeech.html"
  },
  "xu25_interspeech": {
   "authors": [
    [
     "Manjie",
     "Xu"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Yong",
     "Ren"
    ],
    [
     "Xinyi",
     "Tu"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Wei",
     "Liang"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Towards Diverse and Efficient Audio Captioning via Diffusion Models",
   "original": "79",
   "order": 40,
   "page_count": 5,
   "abstract": [
    "We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive diffusion model tailored for diverse and efficient audio captioning. Although existing captioning models relying on language backbones have achieved remarkable success in various captioning tasks, their insufficient performance in terms of generation speed and diversity impedes progress in audio understanding and multimedia applications. Our diffusion-based framework offers unique advantages stemming from its inherent stochasticity and holistic context modeling in captioning. Through rigorous evaluation, we demonstrate that DAC not only achieves superior performance levels compared to existing benchmarks in the caption quality, but also significantly outperforms them in terms of generation speed and diversity."
   ],
   "p1": 191,
   "pn": 195,
   "doi": "10.21437/Interspeech.2025-79",
   "url": "interspeech_2025/xu25_interspeech.html"
  },
  "inoue25_interspeech": {
   "authors": [
    [
     "Sho",
     "Inoue"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs",
   "original": "80",
   "order": 38,
   "page_count": 5,
   "abstract": [
    "Despite significant progress in neural spoken dialog systems, personality-aware conversation agents---capable of adapting behavior based on personalities---remain underexplored due to the absence of personality annotations in speech datasets. We propose a pipeline that preprocesses raw audio recordings to create a dialogue dataset annotated with timestamps, response types, and emotion/sentiment labels.  We employ an automatic speech recognition (ASR) system to extract transcripts and timestamps, then generate conversation-level annotations. Leveraging these annotations, we design a system that employs large language models to predict conversational personality. Human evaluators were engaged to identify conversational characteristics and assign personality labels. Our analysis demonstrates that the proposed system achieves stronger alignment with human judgments compared to existing approaches."
   ],
   "p1": 181,
   "pn": 185,
   "doi": "10.21437/Interspeech.2025-80",
   "url": "interspeech_2025/inoue25_interspeech.html"
  },
  "liu25_interspeech": {
   "authors": [
    [
     "Yang",
     "Liu"
    ],
    [
     "Li",
     "Wan"
    ],
    [
     "Yiteng",
     "Huang"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Xinhao",
     "Mei"
    ],
    [
     "Xubo",
     "Liu"
    ],
    [
     "Yangyang",
     "Shi"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "MASV: Speaker Verification with Global and Local Context Mamba",
   "original": "82",
   "order": 757,
   "page_count": 5,
   "abstract": [
    "Deep learning models like Convolutional Neural Networks and transformers have shown impressive capabilities in speech verification, gaining considerable attention in the research community. However, CNN-based approaches struggle with modeling long-sequence audio effectively, resulting in suboptimal verification performance. On the other hand, transformer-based methods are often hindered by high computational demands, limiting their practicality. This paper presents the MASV model, a novel architecture that integrates the Mamba module into the ECAPA-TDNN framework. By introducing the Local Context Bidirectional Mamba and Tri-Mamba block, the model effectively captures both global and local context within audio sequences. Experimental results demonstrate that the MASV model substantially enhances verification performance, surpassing existing models in both accuracy and efficiency."
   ],
   "p1": 3703,
   "pn": 3707,
   "doi": "10.21437/Interspeech.2025-82",
   "url": "interspeech_2025/liu25_interspeech.html"
  },
  "niizumi25_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Niizumi"
    ],
    [
     "Daiki",
     "Takeuchi"
    ],
    [
     "Masahiro",
     "Yasuda"
    ],
    [
     "Binh Thien",
     "Nguyen"
    ],
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Noboru",
     "Harada"
    ]
   ],
   "title": "Towards Pre-training an Effective Respiratory Audio Foundation Model",
   "original": "84",
   "order": 206,
   "page_count": 5,
   "abstract": [
    "Recent advancements in foundation models have sparked interest in respiratory audio foundation models. However, the effectiveness of applying conventional pre-training schemes to datasets that are small-sized and lack diversity has not been sufficiently verified. This study aims to explore better pre-training practices for respiratory sounds by comparing numerous pre-trained audio models. Our investigation reveals that models pre-trained on AudioSet, a general audio dataset, are more effective than the models specifically pre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory sound datasets for further pre-training enhances performance, and preserving the frequency-wise information when aggregating features is vital. Along with more insights found in the experiments, we establish a new state-of-the-art for the OPERA benchmark, contributing to advancing respiratory audio foundation models."
   ],
   "p1": 998,
   "pn": 1002,
   "doi": "10.21437/Interspeech.2025-84",
   "url": "interspeech_2025/niizumi25_interspeech.html"
  },
  "yan25_interspeech": {
   "authors": [
    [
     "Yujie",
     "Yan"
    ],
    [
     "Xiran",
     "Xu"
    ],
    [
     "Haolin",
     "Zhu"
    ],
    [
     "Songyi",
     "Li"
    ],
    [
     "Bo",
     "Wang"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Jing",
     "Chen"
    ]
   ],
   "title": "Overestimated performance of auditory attention decoding caused by experimental design in EEG recordings",
   "original": "85",
   "order": 217,
   "page_count": 5,
   "abstract": [
    "Auditory Attention Decoding (AAD) identifies a listener&#x27;s focus in complex auditory scenes based on cortical neural responses. High decoding performance using DNN-based methods has been achieved with public EEG datasets. However, performance may be overestimated as models might learn temporal-autocorrelation features rather than auditory attention-related features. While data splitting risks have been discussed, experimental design risks have not. In this work, we collected a non-block design (NBD) scalp-EEG and ear-EEG joint dataset and compared it to previous block design (BD) datasets using DNN-based models. Results show a significant accuracy drop from BD to NBD dataset, while a linear stimulus reconstruction model remains robust. Inter-trial phase coherence analysis confirms stronger neural phase-locking to attended speech in BD dataset. These findings suggest BD enhances coherence of neural response but risks overestimating AAD accuracy. Code and data are released."
   ],
   "p1": 1053,
   "pn": 1057,
   "doi": "10.21437/Interspeech.2025-85",
   "url": "interspeech_2025/yan25_interspeech.html"
  },
  "ross25_interspeech": {
   "authors": [
    [
     "Alice",
     "Ross"
    ],
    [
     "Cliodhna",
     "Hughes"
    ],
    [
     "Eddie L.",
     "Ungless"
    ],
    [
     "Catherine",
     "Lai"
    ]
   ],
   "title": "Conveying Gender Through Speech: Insights from Trans Men",
   "original": "86",
   "order": 141,
   "page_count": 5,
   "abstract": [
    "Trans people tend to be well aware of the ways in which (perceived) gender is indexically linked to the voice. Focusing on the understudied population of trans men, we present one of the first studies on style shift in trans speakers, considering the phonetic features of trans men&#x27;s speech in different contexts, and their own beliefs about vocal cues to gender perception. Our participants (n = 7) provided read speech samples with contrasting imagined listeners, and discussed their voices and experiences in semi-structured interviews. Acoustic and qualitative analyses show that our participants adopted lower, narrower pitch ranges when they imagined speaking to a stranger, though individuals differed in the ways they altered their vowel formants and /s/ production. The interview data provide further insights into trans men&#x27;s speech styles, self-monitoring, and pursuit of authenticity, highlighting shared concerns about voice training apps, safety, and representation for trans people."
   ],
   "p1": 674,
   "pn": 678,
   "doi": "10.21437/Interspeech.2025-86",
   "url": "interspeech_2025/ross25_interspeech.html"
  },
  "hsieh25_interspeech": {
   "authors": [
    [
     "I-Ting",
     "Hsieh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Dysarthric Speech Recognition Using Curriculum Learning and Multi-stream Architecture",
   "original": "89",
   "order": 453,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech recognition presents significant challenges due to limited resources and the diverse attributes of such speech. Current solutions often involve techniques like data augmentation or domain adaptation. However, applying these methods to a single model frequently results in inconsistent improvements across different speech intelligibility groups. To address this, we propose a novel approach that combines curriculum learning with a multi-stream architecture. Experimental results show the effectiveness of the proposed method, achieving a 10.47% improvement over the baseline."
   ],
   "p1": 2210,
   "pn": 2214,
   "doi": "10.21437/Interspeech.2025-89",
   "url": "interspeech_2025/hsieh25_interspeech.html"
  },
  "botelho25_interspeech": {
   "authors": [
    [
     "Catarina",
     "Botelho"
    ],
    [
     "David",
     "Gimeno-Gómez"
    ],
    [
     "Francisco",
     "Teixeira"
    ],
    [
     "John",
     "Mendonça"
    ],
    [
     "Patrícia",
     "Pereira"
    ],
    [
     "Diogo A. P.",
     "Nunes"
    ],
    [
     "Thomas",
     "Rolland"
    ],
    [
     "Anna",
     "Pompili"
    ],
    [
     "Rubén",
     "Solera-Ureña"
    ],
    [
     "Maria",
     "Ponte"
    ],
    [
     "David Martins de",
     "Matos"
    ],
    [
     "Carlos-D.",
     "Martínez-Hinarejos"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "Acoustic and Linguistic Biomarkers for Cognitive Impairment Detection from Speech",
   "original": "91",
   "order": 290,
   "page_count": 5,
   "abstract": [
    "This work describes a comprehensive approach for the automatic assessment of cognitive decline from spontaneous speech in the context of the PROCESS Challenge 2025. Based on our previous experience on the use of speech and text-derived biomarkers for disease detection, we evaluate here the use of knowledge-based acoustic and text-based feature sets, as well as LLM-based macro-descriptors, and multiple neural representations (e.g., Longformer, ECAPA-TDNN, and Trillsson embeddings). The combination of these feature sets with different classifiers resulted in a large pool of systems, from which, those providing the best balance between train, development, and individual class performance were selected for model ensembling. Our final best-performing systems correspond to combinations of models that are complementary to each other, relying on acoustic and textual information from the three clinical tasks provided in the challenge dataset."
   ],
   "p1": 1418,
   "pn": 1422,
   "doi": "10.21437/Interspeech.2025-91",
   "url": "interspeech_2025/botelho25_interspeech.html"
  },
  "amoniyan25_interspeech": {
   "authors": [
    [
     "Oluwasegun",
     "Amoniyan"
    ]
   ],
   "title": "Is it all about race?: A Cross-examination of /s/ in a Multilingual (Nigerian) Context",
   "original": "94",
   "order": 684,
   "page_count": 5,
   "abstract": [
    "Previous studies on /s/ variation have examined how /s/ production functions as an indexical marker, particularly in relation to gender and race. These studies primarily focus on Western contexts (e.g., North America) to explain that /s/ can signal gendered personas and racialized social meanings. However, in a multilingual context like Nigeria, /s/ has not been explored to identify motivation(s) for /s/ variation.\n",
    "This study, therefore, examines /s/ production in Nigerian English (NigE), focusing on the factors that condition /s/ variation. The study samples 4,056 tokens of /s/ from the ICE-NigEand they are analyzed using center of gravity, zero crossings, duration, and skewness to explain /s/ frontness or backness. \n",
    "The results reveal that ethnicity and year of birth influence /s/ variation. The younger speakers produce fronted /s/, and older speakers realize a more retracted /s/. While /s/ is more fronted among the Igbo NigE speakers, it is more retracted among Yoruba and Hausa NigE speakers, and the absence of gender difference may highlight the importance of ethnicity rather than gender in a multilingual context."
   ],
   "p1": 3364,
   "pn": 3368,
   "doi": "10.21437/Interspeech.2025-94",
   "url": "interspeech_2025/amoniyan25_interspeech.html"
  },
  "lin25_interspeech": {
   "authors": [
    [
     "Yuke",
     "Lin"
    ],
    [
     "Jun",
     "Chen"
    ],
    [
     "Wenjie",
     "Li"
    ],
    [
     "Longshuai",
     "Xiao"
    ],
    [
     "Chao",
     "Weng"
    ]
   ],
   "title": "Robust Personal Voice Activity Detection for Mitigating Domain Mismatch and False Acceptance Scenarios",
   "original": "98",
   "order": 1178,
   "page_count": 5,
   "abstract": [
    "Personal Voice Activity Detection (pVAD), which leverages pre-enrolled speaker information to identify the presence of a specific speaker, has been widely adopted in mobile devices. However, domain mismatches between enrolled and test data are common in real-world scenarios, resulting in significant performance degradation. Additionally, existing pVAD models primarily optimize detection performance for a target speaker but often fail to address the challenge of false acceptance, especially when interfering speakers share similar voice characteristics. To address these limitations, we propose a novel backbone integrated with an auxiliary decoder and utilize an embedding-updating method during the inference phase to enhance performance under domain mismatch conditions. Furthermore, we introduce an on-the-fly hard-sample data simulation strategy, which has been shown to significantly reduce false acceptance rates, as demonstrated by our experimental results."
   ],
   "p1": 5783,
   "pn": 5787,
   "doi": "10.21437/Interspeech.2025-98",
   "url": "interspeech_2025/lin25_interspeech.html"
  },
  "combei25_interspeech": {
   "authors": [
    [
     "David",
     "Combei"
    ],
    [
     "Adriana",
     "Stan"
    ],
    [
     "Dan",
     "Oneata"
    ],
    [
     "Nicolas",
     "Müller"
    ],
    [
     "Horia",
     "Cucu"
    ]
   ],
   "title": "Unmasking real-world audio deepfakes: A data-centric approach",
   "original": "100",
   "order": 1090,
   "page_count": 5,
   "abstract": [
    "The growing prevalence of real-world deepfakes presents a critical challenge for existing detection systems, which are often evaluated on datasets collected just for scientific purposes. To address this gap, we introduce a novel dataset of real-world audio deepfakes. Our analysis reveals that these real-world examples pose significant challenges, even for the most performant detection models. Rather than increasing model complexity or exhaustively search for a better alternative, in this work we focus on a data-centric paradigm, employing strategies like dataset curation, pruning, and augmentation to improve model robustness and generalization. Through these methods, we achieve a 55% relative reduction in EER on the In-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on our newly proposed real-world deepfakes dataset, AI4T. These results highlight the transformative potential of data-centric approaches in enhancing deepfake detection for real-world applications."
   ],
   "p1": 5343,
   "pn": 5347,
   "doi": "10.21437/Interspeech.2025-100",
   "url": "interspeech_2025/combei25_interspeech.html"
  },
  "bentum25_interspeech": {
   "authors": [
    [
     "Martijn",
     "Bentum"
    ],
    [
     "Louis",
     "ten Bosch"
    ],
    [
     "Tomas O.",
     "Lentz"
    ]
   ],
   "title": "Word stress in self-supervised speech models: A cross-linguistic comparison",
   "original": "106",
   "order": 52,
   "page_count": 5,
   "abstract": [
    "In this paper we study word stress representations learned by self-supervised speech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M representations of word stress for five different languages: Three languages with variable or lexical stress (Dutch, English and German) and two languages with fixed or demarcative stress (Hungarian and Polish). We train diagnostic stress classifiers on S3M embeddings and show that they can distinguish between stressed and unstressed syllables in read-aloud short sentences with high accuracy. We also tested language-specificity effects of S3M word stress. \n",
    "The results indicate that the word stress representations are language-specific, with a greater difference between the set of variable versus the set of fixed stressed languages."
   ],
   "p1": 251,
   "pn": 255,
   "doi": "10.21437/Interspeech.2025-106",
   "url": "interspeech_2025/bentum25_interspeech.html"
  },
  "kuhne25_interspeech": {
   "authors": [
    [
     "Nikolai Lund",
     "Kühne"
    ],
    [
     "Jan",
     "Østergaard"
    ],
    [
     "Jesper",
     "Jensen"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement",
   "original": "108",
   "order": 1050,
   "page_count": 5,
   "abstract": [
    "While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A direct comparative analysis reveals that xLSTM---and notably, even LSTM---can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems of similar complexity on the Voicebank+DEMAND dataset."
   ],
   "p1": 5148,
   "pn": 5152,
   "doi": "10.21437/Interspeech.2025-108",
   "url": "interspeech_2025/kuhne25_interspeech.html"
  },
  "cappellazzo25_interspeech": {
   "authors": [
    [
     "Umberto",
     "Cappellazzo"
    ],
    [
     "Minsu",
     "Kim"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Alessio",
     "Brutti"
    ]
   ],
   "title": "Scaling and Enhancing LLM-based AVSR:  A Sparse Mixture of Projectors Approach",
   "original": "111",
   "order": 372,
   "page_count": 5,
   "abstract": [
    "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy environments by integrating visual cues. While recent advances integrate Large Language Models (LLMs) into AVSR, their high computational cost hinders deployment in resource-constrained settings. To address this, we propose Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of Projectors (SMoP) module to scale model capacity without increasing inference costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors, Llama-SMoP enables the use of smaller LLMs while maintaining strong performance. We explore three SMoP configurations and show that Llama-SMoP DEDR (Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation studies confirm its effectiveness in expert activation, scalability, and noise robustness."
   ],
   "p1": 1823,
   "pn": 1827,
   "doi": "10.21437/Interspeech.2025-111",
   "url": "interspeech_2025/cappellazzo25_interspeech.html"
  },
  "chen25c_interspeech": {
   "authors": [
    [
     "Yizhou",
     "Chen"
    ],
    [
     "Xiyu",
     "Wu"
    ]
   ],
   "title": "Perception of Emotional Speech by Individuals with High Borderline Personality Features",
   "original": "113",
   "order": 413,
   "page_count": 5,
   "abstract": [
    "Borderline personality disorder (BPD) is characterized by emotional dysregulation. Prior research on facial emotion recognition has revealed that BPD individuals have emotion perception abnormalities. This study examined whether these findings extended to emotional speech recognition. Mandarin emotional speech (neutral, angry, happy, and sad) was synthesized at varying emotional intensities by adjusting the fundamental frequency. A perceptual experiment was conducted with Chinese university students with high and low borderline personality features (BPF). High-BPF participants showed lower accuracy in identifying neutral speech, more frequently misidentifying it as other emotions, and were less accurate in identifying high-intensity happy speech, tending to misclassify it as neutral. They also exhibited marginally higher confidence in recognizing angry speech. The results contribute to the phenomenological understanding of emotional dysregulation in BPD."
   ],
   "p1": 2028,
   "pn": 2032,
   "doi": "10.21437/Interspeech.2025-113",
   "url": "interspeech_2025/chen25c_interspeech.html"
  },
  "lee25_interspeech": {
   "authors": [
    [
     "Sun-Kyung",
     "Lee"
    ],
    [
     "Jong-Hwan",
     "Kim"
    ]
   ],
   "title": "CAMER: Contribution-Aware Multimodal Emotion Recognition",
   "original": "114",
   "order": 549,
   "page_count": 5,
   "abstract": [
    "Multimodal emotion recognition (MER) integrates data from speech, facial expressions, and text to enhance emotion prediction, with applications in various human-computer interaction scenarios. While recent end-to-end models improved performance, they typically lack interpretability regarding the role of each modality in predicting emotions. This paper introduces a contribution-aware MER (CAMER) with a novel adaptive weighting mechanism that dynamically adjusts the contribution of each modality based on the emotional content using cross-attention between modality features and emotion embeddings. This approach not only enhances the performance of MER by optimizing modality integration but also significantly improves the interpretability of the model&#x27;s predictions. Evaluations on the IEMOCAP and CMU-MOSEI datasets demonstrate the superiority of our method. Additionally, we provide an interactive demo that allows users to explore and visualize the model&#x27;s decision-making process."
   ],
   "p1": 2690,
   "pn": 2694,
   "doi": "10.21437/Interspeech.2025-114",
   "url": "interspeech_2025/lee25_interspeech.html"
  },
  "sadok25_interspeech": {
   "authors": [
    [
     "Samir",
     "Sadok"
    ],
    [
     "Julien",
     "Hauret"
    ],
    [
     "Éric",
     "Bavu"
    ]
   ],
   "title": "Bringing Interpretability to Neural Audio Codecs",
   "original": "115",
   "order": 1025,
   "page_count": 5,
   "abstract": [
    "The advent of neural audio codecs has increased in popularity due to their potential for efficiently modeling audio with transformers. Such advanced codecs represent audio from a highly continuous waveform to low-sampled discrete units. In contrast to semantic units, acoustic units may lack interpretability because their training objectives primarily focus on reconstruction performance. This paper proposes a two-step approach to explore the encoding of speech information within the codec tokens. The primary goal of the analysis stage is to gain deeper insight into how speech attributes such as content, identity, and pitch are encoded. The synthesis stage then trains an AnCoGen network for post-hoc explanation of codecs to extract speech attributes from the respective tokens directly."
   ],
   "p1": 5023,
   "pn": 5027,
   "doi": "10.21437/Interspeech.2025-115",
   "url": "interspeech_2025/sadok25_interspeech.html"
  },
  "ye25_interspeech": {
   "authors": [
    [
     "Chengjia",
     "Ye"
    ],
    [
     "James M.",
     "McQueen"
    ],
    [
     "Hans Rutger",
     "Bosker"
    ]
   ],
   "title": "A Gradient Effect of Hand Beat Timing on Spoken Word Recognition",
   "original": "116",
   "order": 775,
   "page_count": 5,
   "abstract": [
    "Visual cues play a key role in speech perception. Beat gestures (i.e., simple up-and-down hand movements) usually co-occur with prominence in speech. Previous studies found that hand beat timing can indicate word stress. The present study further examines whether hand beat timing influences spoken word recognition in a gradient fashion. On watching videos of a native speaker of Dutch uttering a disyllabic word voornaam while making a hand beat, 40 participants needed to decide if they heard the word with initial (VOORnaam, &quot;first name&quot;) or final stress (voorNAAM, &quot;respectable&quot;). Crucially, nine beat apex timings were equally distributed between the pitch peaks of the two syllables. Results exhibited a gradient effect of hand beat timing on stress perception, which appeared not to be susceptible to brief pretest feedback implying that visual cues should be ignored. Our findings provide novel evidence for audiovisual interaction and can inform gesture generation in conversational agents."
   ],
   "p1": 3793,
   "pn": 3797,
   "doi": "10.21437/Interspeech.2025-116",
   "url": "interspeech_2025/ye25_interspeech.html"
  },
  "gutscher25_interspeech": {
   "authors": [
    [
     "Lorenz",
     "Gutscher"
    ],
    [
     "Michael",
     "Pucher"
    ]
   ],
   "title": "Audio-Based Classification and Geographic Regression of Austrian Dialects",
   "original": "119",
   "order": 564,
   "page_count": 5,
   "abstract": [
    "Dialect classification remains challenging due to regional variability and limited dialect-specific datasets. This study addresses these challenges by leveraging a novel dataset of 304 speakers from 108 locations across Austria for automatic classification of Austrian dialects. To minimize speaker-specific biases and enhance dialectal features, speaker augmentation techniques are applied. Classification is conducted at three levels: location, dialect group, and federal state. Additionally, a regression task predicts the speakers&#x27; geographic coordinates, with the wav2vec 2.0 model architecture achieving an average test-set distance error of 66.7 kilometers. This work represents a unique approach to fine-grained dialect classification and geographic location prediction for Austria. Finally, model explainability is explored using Integrated Gradients (IG), identifying the most relevant speech segments for classification within each dialect group."
   ],
   "p1": 2765,
   "pn": 2769,
   "doi": "10.21437/Interspeech.2025-119",
   "url": "interspeech_2025/gutscher25_interspeech.html"
  },
  "horiguchi25b_interspeech": {
   "authors": [
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Naohiro",
     "Tawara"
    ],
    [
     "Marc",
     "Delcroix"
    ]
   ],
   "title": "Pretraining Multi-Speaker Identification for Neural Speaker Diarization",
   "original": "120",
   "order": 328,
   "page_count": 5,
   "abstract": [
    "End-to-end speaker diarization enables accurate overlap-aware diarization by jointly estimating multiple speakers&#x27; speech activities in parallel. This approach is data-hungry, requiring a large amount of labeled conversational data, which cannot be fully obtained from real datasets alone. To address this issue, large-scale simulated data is often used for pretraining, but it requires enormous storage and I/O capacity, and simulating data that closely resembles real conversations remains challenging. In this paper, we propose pretraining a model to identify multiple speakers from an input fully overlapped mixture as an alternative to pretraining a diarization model. This method eliminates the need to prepare a large-scale simulated dataset while leveraging large-scale speaker recognition datasets for training. Through comprehensive experiments, we demonstrate that the proposed method enables a highly accurate yet lightweight local diarization model without simulated conversational data."
   ],
   "p1": 1608,
   "pn": 1612,
   "doi": "10.21437/Interspeech.2025-120",
   "url": "interspeech_2025/horiguchi25b_interspeech.html"
  },
  "wu25_interspeech": {
   "authors": [
    [
     "Wen",
     "Wu"
    ],
    [
     "Ziyun",
     "Cui"
    ],
    [
     "Chang",
     "Lei"
    ],
    [
     "Yinan",
     "Duan"
    ],
    [
     "Diyang",
     "Qu"
    ],
    [
     "Ji",
     "Wu"
    ],
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Runsen",
     "Chen"
    ],
    [
     "Chao",
     "Zhang"
    ]
   ],
   "title": "The 1st SpeechWellness Challenge: Detecting Suicide Risk Among Adolescents",
   "original": "124",
   "order": 86,
   "page_count": 5,
   "abstract": [
    "The 1st SpeechWellness Challenge (SW1) aims to advance methods for detecting current suicide risk in adolescents using speech analysis techniques. Suicide among adolescents is a critical public health issue globally. Early detection of suicidal tendencies can lead to timely intervention and potentially save lives. Traditional methods of assessment often rely on self-reporting or clinical interviews, which may not always be accessible. The SW1 challenge addresses this gap by exploring speech as a non-invasive and readily available indicator of mental health. We release the SW1 dataset which contains speech recordings from 600 adolescents aged 10-18 years. By focusing on speech generated from natural tasks, the challenge seeks to uncover patterns and markers that correlate with current suicide risk."
   ],
   "p1": 399,
   "pn": 403,
   "doi": "10.21437/Interspeech.2025-124",
   "url": "interspeech_2025/wu25_interspeech.html"
  },
  "funk25_interspeech": {
   "authors": [
    [
     "Riccarda",
     "Funk"
    ],
    [
     "Melanie",
     "Weirich"
    ],
    [
     "Adrian",
     "Simpson"
    ]
   ],
   "title": "How sibilant spectra shape gender perception in prepubertal children: A voice morphing study",
   "original": "125",
   "order": 199,
   "page_count": 5,
   "abstract": [
    "Spectral characteristics of sibilants have been linked to social factors such as gender or sexual orientation. This paper examines the impact of sibilant spectra on gender perception in prepubertal German-speaking children (ages 6-9) through a longitudinal study. Center of Gravity (CoG) and skewness were measured in the sibilant /z/. Two listening experiments assessed gender perception: one with natural stimuli, the other with voice morphed stimuli to isolate the effect of sibilant spectra.\n",
    "No overall gender differences in CoG and skewness were found. While sibilant spectra had no effect on gender perception in natural stimuli, significant effects emerged in morphed stimuli. This suggests that while broader acoustic cues may have a stronger effect on gender perception, sibilants also reflect stereotypical gender associations in isolation."
   ],
   "p1": 963,
   "pn": 967,
   "doi": "10.21437/Interspeech.2025-125",
   "url": "interspeech_2025/funk25_interspeech.html"
  },
  "yang25b_interspeech": {
   "authors": [
    [
     "Zijiang",
     "Yang"
    ],
    [
     "Meishu",
     "Song"
    ],
    [
     "Xin",
     "Jing"
    ],
    [
     "Haojie",
     "Zhang"
    ],
    [
     "Kun",
     "Qian"
    ],
    [
     "Bin",
     "Hu"
    ],
    [
     "Kota",
     "Tamada"
    ],
    [
     "Toru",
     "Takumi"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Yoshiharu",
     "Yamamoto"
    ]
   ],
   "title": "MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge",
   "original": "127",
   "order": 350,
   "page_count": 5,
   "abstract": [
    "The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting Autism Spectrum Disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based model using three different features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection."
   ],
   "p1": 1718,
   "pn": 1722,
   "doi": "10.21437/Interspeech.2025-127",
   "url": "interspeech_2025/yang25b_interspeech.html"
  },
  "ng25_interspeech": {
   "authors": [
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Kun",
     "Zhou"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Thinking Fast and Slow: Robust Speech Recognition via Deep Filter-Tuning",
   "original": "128",
   "order": 735,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) models have revolutionized speech representation by extracting rich acoustic and phonetic features with minimal labeled data. However, their computational demands during fine-tuning and vulnerability to catastrophic forgetting pose challenges for practical deployment. Parameter-Efficient fine-tuning (PEFT) methods, such as prompt tuning, are often employed to address these challenges. While prompt tuning has been successful with large language models in natural language processing (NLP), it struggles to learn effective instructional signals when adapting to speech SSL models, likely due to insufficient a priori knowledge that hinders soft token learning during fine-tuning. We introduce Deep Filter Tuning (DFT), a soft-token adaptation strategy that selectively filters semantic information from noise-distorted representations. By modifying only 0.38% of model weights, DFT achieves a 12% performance gain in noisy environments, offering an efficient solution for robust speech recognition under challenging conditions such as noise adaptation."
   ],
   "p1": 3593,
   "pn": 3597,
   "doi": "10.21437/Interspeech.2025-128",
   "url": "interspeech_2025/ng25_interspeech.html"
  },
  "sudo25_interspeech": {
   "authors": [
    [
     "Yui",
     "Sudo"
    ],
    [
     "Yosuke",
     "Fukumoto"
    ],
    [
     "Muhammad",
     "Shakeel"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Chyi-Jiunn",
     "Lin"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "DYNAC: Dynamic Vocabulary-based Non-Autoregressive Contextualization for Speech Recognition",
   "original": "129",
   "order": 454,
   "page_count": 5,
   "abstract": [
    "Contextual biasing (CB) improves automatic speech recognition for rare and unseen phrases. Recent studies have introduced dynamic vocabulary, which represents context phrases as expandable tokens in autoregressive (AR) models. This method improves CB accuracy but with slow inference speed. While dynamic vocabulary can be applied to non-autoregressive (NAR) models, such as connectionist temporal classification (CTC), the conditional independence assumption fails to capture dependencies between static and dynamic tokens. This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a self-conditioned CTC method that integrates dynamic vocabulary into intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC effectively captures dependencies between static and dynamic tokens while reducing the real-time factor (RTF). Experimental results show that DYNAC reduces RTF by 81% with a 0.1-point degradation in word error rate on the LibriSpeech 960 test-clean set."
   ],
   "p1": 2215,
   "pn": 2219,
   "doi": "10.21437/Interspeech.2025-129",
   "url": "interspeech_2025/sudo25_interspeech.html"
  },
  "xu25b_interspeech": {
   "authors": [
    [
     "Yuetonghui",
     "Xu"
    ],
    [
     "Yiwen",
     "Wang"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Xiaobing",
     "Li"
    ],
    [
     "Feng",
     "Yu"
    ]
   ],
   "title": "Position also matters! Separating Same Instruments in String Quartet using Timbral and Positional Cues",
   "original": "130",
   "order": 632,
   "page_count": 5,
   "abstract": [
    "String quartets inherently face challenges such as fundamental frequency overlap, similar timbres, and complex acoustic environments, making existing source separation methods relying solely on timbral characteristics less effective. This paper proposes a novel approach that enhances separation performance by encoding timbral and spatial positional cues to guide a neural network for source separation. To evaluate our method, we created a dataset of string quartet recordings with realistic reverberation with two different seating arrangements. Experiments demonstrate that our approach outperforms current state-of-the-art (SOTA) methods in source separation, achieving superior performance even in seating arrangements different from training, thus highlighting the importance of our timbral and positional encoding in improving separation quality and generalizability."
   ],
   "p1": 3105,
   "pn": 3109,
   "doi": "10.21437/Interspeech.2025-130",
   "url": "interspeech_2025/xu25b_interspeech.html"
  },
  "weirich25_interspeech": {
   "authors": [
    [
     "Melanie",
     "Weirich"
    ],
    [
     "Adrian",
     "Simpson"
    ]
   ],
   "title": "Investigating effects of sex hormones, cycle phases and age on female fundamental frequency",
   "original": "139",
   "order": 681,
   "page_count": 5,
   "abstract": [
    "Mean fundamental frequency (f0) has been found to be affected by hormones. In females, studies show contradicting results regarding the influence of hormonal changes across the cycle on f0. Some studies have suggested hormonal changes due to menopause are responsible for the decrease in female f0 with age, while others point to an earlier onset of this decrease.\n",
    "Here, the effect of hormones (estradiol, progesterone, testosterone) on mean f0 within speakers across the cycle and between speakers was investigated in 62 German participants. In addition, the factors age, height and speech material were included.\n",
    "Results reveal that mean f0 was not affected by cycle phase. Also, variation in mean f0 across speakers could not be explained by variation in investigated hormones. In contrast, a clear effect of speakers&#x27; age on mean f0 was found, irrespective of hormone levels, height and speech material, pointing to other factors contributing to a lower mean f0 with increasing age in females."
   ],
   "p1": 3349,
   "pn": 3353,
   "doi": "10.21437/Interspeech.2025-139",
   "url": "interspeech_2025/weirich25_interspeech.html"
  },
  "vlasenko25_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Vlasenko"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Multimodal Prosody Modeling: A Use Case for Multilingual Sentence Mode Prediction",
   "original": "143",
   "order": 1099,
   "page_count": 5,
   "abstract": [
    "Prosody modeling has garnered significant attention from the speech processing community. Recent developments in multilingual latent spaces for representing linguistic and acoustic information have become a new trend in various research directions. Therefore, we decided to evaluate the ability of multilingual acoustic neural embeddings and knowledge-based features to preserve sentence-mode-related information at the suprasegmental level. For linguistic information modeling, we selected neural embeddings based on word- and phoneme-level latent space representations. The experimental study was conducted using Italian, French, and German audiobook recordings, as well as emotional speech samples from EMO-DB. Both intra- and inter-language experimental protocols were used to assess classification performance for uni- and multimodal (early fusion approach) features. For comparison, we used a sentence mode prediction system built on top of automatically generated WHISPER-based transcripts."
   ],
   "p1": 5388,
   "pn": 5392,
   "doi": "10.21437/Interspeech.2025-143",
   "url": "interspeech_2025/vlasenko25_interspeech.html"
  },
  "cumani25_interspeech": {
   "authors": [
    [
     "Sandro",
     "Cumani"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Sara",
     "Barahona"
    ],
    [
     "Ladislav",
     "Mošner"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Johan",
     "Rohdin"
    ]
   ],
   "title": "Analysis of the ABC Classification Backends for NIST SRE24",
   "original": "146",
   "order": 812,
   "page_count": 5,
   "abstract": [
    "We present an analysis of the classification backends of the ABC submission for the audio tracks of the NIST 2024 Speaker Recognition Evaluation (SRE24). Our analysis covers embedding pre-processing, classification and score-level normalization, calibration and fusion strategies adopted to cope with the source, language and duration mismatch challenges of SRE24. We show that Pairwise Support Vector Machines provide the best results, which can be further improved, for single frontends, through score-level fusion of additional classifiers. We also show that condition-aware score calibration can mitigate the effects of source mismatch, whereas score normalization methods proved ineffective. Finally, we show that generative calibration is able to achieve competitive results with respect to other approaches."
   ],
   "p1": 3978,
   "pn": 3982,
   "doi": "10.21437/Interspeech.2025-146",
   "url": "interspeech_2025/cumani25_interspeech.html"
  },
  "cumani25b_interspeech": {
   "authors": [
    [
     "Sandro",
     "Cumani"
    ]
   ],
   "title": "A Copula-Based Generative Score-Level Fusion Model for Speaker Verification",
   "original": "147",
   "order": 761,
   "page_count": 5,
   "abstract": [
    "In this work we present a novel generative approach for the score-level fusion of speaker verification systems. The proposed method employs a copula-based representation of the joint score distribution of multiple speaker recognizers that allows decoupling the dependency structure from the characterization of the marginal densities of the scores of different systems. This allows us to combine complex Variance-Gamma marginals with a simple Gaussian copula to obtain a characterization of the joint target and non-target score distribution that can be effectively employed for the score-level combination of multiple recognizers. Our results on NIST SRE 2019 and SITW datasets show that our approach is competitive with respect to state-of-the-art discriminative score fusion techniques, providing both accurate and well-calibrated scores, with a measured Cllr reduction of up to 7% relative with respect to discriminative linear fusion methods."
   ],
   "p1": 3723,
   "pn": 3727,
   "doi": "10.21437/Interspeech.2025-147",
   "url": "interspeech_2025/cumani25b_interspeech.html"
  },
  "xu25c_interspeech": {
   "authors": [
    [
     "Hainan",
     "Xu"
    ],
    [
     "Vladimir",
     "Bataev"
    ],
    [
     "Lilit",
     "Grigoryan"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection",
   "original": "148",
   "order": 134,
   "page_count": 5,
   "abstract": [
    "We propose Windowed Inference for Non-blank Detection (WIND), a novel strategy that significantly accelerates RNN-T inference without compromising model accuracy. During model inference, instead of processing frames sequentially, WIND processes multiple frames simultaneously within a window in parallel, allowing the model to quickly locate non-blank predictions during decoding, resulting in significant speed-ups. We implement WIND for greedy decoding, batched greedy decoding with label-looping techniques, and also propose a novel beam-search decoding method. Experiments on multiple datasets with different conditions show that our method, when operating in greedy modes, speeds up as much as 2.4X compared to the baseline sequential approach while maintaining identical Word Error Rate (WER) performance. Our beam-search algorithm achieves slightly better accuracy than alternative methods, with significantly improved speed. We will open-source our WIND implementation."
   ],
   "p1": 639,
   "pn": 643,
   "doi": "10.21437/Interspeech.2025-148",
   "url": "interspeech_2025/xu25c_interspeech.html"
  },
  "raut25_interspeech": {
   "authors": [
    [
     "Ankush",
     "Raut"
    ],
    [
     "Projna",
     "Paromita"
    ],
    [
     "Sydney",
     "Begerowski"
    ],
    [
     "Suzanne",
     "Bell"
    ],
    [
     "Theodora",
     "Chaspari"
    ]
   ],
   "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions",
   "original": "149",
   "order": 1112,
   "page_count": 5,
   "abstract": [
    "We explore the feasibility of large language models (LLMs) in detecting subtle expressions of micro-behaviors in team conversations using transcripts collected during simulated space missions. Specifically, we examine zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only sequence classification LLMs, as well as few-shot text generation with decoder-only causal language modeling LLMs, to predict the micro-behavior associated with each conversational turn (i.e., dialogue). Our findings indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to detect underrepresented micro-behaviors, particularly discouraging speech, even with weighted fine-tuning. In contrast, the instruction fine-tuned version of Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best models achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. These results have implications for the development of speech technologies aimed at analyzing team communication dynamics and enhancing training interventions in high-stakes environments such as space missions, particularly in scenarios where text is the only accessible data."
   ],
   "p1": 5453,
   "pn": 5457,
   "doi": "10.21437/Interspeech.2025-149",
   "url": "interspeech_2025/raut25_interspeech.html"
  },
  "kommineni25_interspeech": {
   "authors": [
    [
     "Aditya",
     "Kommineni"
    ],
    [
     "Digbalay",
     "Bose"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "So Hyun",
     "Kim"
    ],
    [
     "Helen",
     "Tager-Flusberg"
    ],
    [
     "Somer",
     "Bishop"
    ],
    [
     "Catherine",
     "Lord"
    ],
    [
     "Sudarsana",
     "Kadiri"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Can Multimodal Foundation Models Help Analyze Child-Inclusive Autism Diagnostic Videos?",
   "original": "150",
   "order": 621,
   "page_count": 5,
   "abstract": [
    "Multimodal foundation models have paved the way for a paradigm shift in long video understanding. The extent to which these models can help analyze verbal and non-verbal behaviors in the context of human interactions is underexplored, particularly in the challenging settings of clinical diagnosis and treatment. We investigate the use of foundation models across speech, video, and text modalities to analyze child-focused interactions in the context of autism diagnosis. We evaluate model performance in two related tasks, i.e. activity understanding and atypical behavior detection. We further propose a unified methodology for merging information from audio and video streams by leveraging large language models as reasoning agents. Our experiments reveal that, while models perform relatively well for coarse-grained tasks such as activity recognition and over-activity identification, they fail to generalize to fine-grained tasks such as anxiety detection and activity segmentation."
   ],
   "p1": 3050,
   "pn": 3054,
   "doi": "10.21437/Interspeech.2025-150",
   "url": "interspeech_2025/kommineni25_interspeech.html"
  },
  "ghosh25_interspeech": {
   "authors": [
    [
     "Suhita",
     "Ghosh"
    ],
    [
     "Melanie",
     "Jouaiti"
    ],
    [
     "Jan-Ole",
     "Perschewski"
    ],
    [
     "Sebastian",
     "Stober"
    ]
   ],
   "title": "StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation",
   "original": "151",
   "order": 168,
   "page_count": 5,
   "abstract": [
    "Detecting and segmenting dysfluencies is crucial for effective speech therapy and real-time feedback. However, most methods only classify dysfluencies at the utterance level. We introduce StutterCut, a semi-supervised framework that formulates dysfluency segmentation as a graph partitioning problem, where speech embeddings from overlapping windows are represented as graph nodes. We refine the connections between nodes using a pseudo-oracle classifier trained on weak (utterance-level) labels, with its influence controlled by an uncertainty measure from Monte Carlo dropout. Additionally, we extend the weakly labelled FluencyBank dataset by incorporating frame-level dysfluency boundaries for four dysfluency types. This provides a more realistic benchmark compared to synthetic datasets. Experiments on real and synthetic datasets show that StutterCut outperforms existing methods, achieving higher F1 scores and more precise stuttering onset detection."
   ],
   "p1": 808,
   "pn": 812,
   "doi": "10.21437/Interspeech.2025-151",
   "url": "interspeech_2025/ghosh25_interspeech.html"
  },
  "yazawa25_interspeech": {
   "authors": [
    [
     "Kakeru",
     "Yazawa"
    ],
    [
     "Takayuki",
     "Konishi"
    ]
   ],
   "title": "A Bayesian Approach to L2 Fluency Ratings by Native and Nonnative Listeners",
   "original": "152",
   "order": 23,
   "page_count": 5,
   "abstract": [
    "This study investigates how native and nonnative listeners evaluate the fluency of Japanese speakers&#x27; English using a Bayesian modeling framework. Data were obtained from 16 listeners with diverse linguistic backgrounds (Cantonese, English, French, German, Japanese, Korean, Mandarin, Polish, Punjabi, and Spanish), who rated English read speech samples from 180 Japanese speakers, in the J-AESOP corpus. Utterance fluency measures included speed (syllable- or segment-based articulation rate), breakdown (pause frequency and duration), and re pair (repetitions). Results revealed that nonnative listeners, particularly those with Asian language backgrounds, were generally more lenient and less reliant on speech rate than native listeners, highlighting inter-listener variability previously overlooked. Model comparisons also revealed that segment-based articulation rate better captures utterance speed fluency than the commonly adopted syllable-based articulation rate."
   ],
   "p1": 106,
   "pn": 110,
   "doi": "10.21437/Interspeech.2025-152",
   "url": "interspeech_2025/yazawa25_interspeech.html"
  },
  "gu25_interspeech": {
   "authors": [
    [
     "Yicheng",
     "Gu"
    ],
    [
     "Chaoren",
     "Wang"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Lauri",
     "Juvela"
    ]
   ],
   "title": "Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN",
   "original": "154",
   "order": 257,
   "page_count": 5,
   "abstract": [
    "Pitch manipulation is the process of producers adjusting the pitch of an audio segment to a specific key and intonation, which is essential in music production. Neural-network-based pitch-manipulation systems have been popular in recent years due to their superior synthesis quality compared to classical DSP methods. However, their performance is still limited due to their inaccurate feature disentanglement using source-filter models and the lack of paired in- and out-of-tune training data. This work proposes Neurodyne to address these issues. Specifically, Neurodyne uses adversarial representation learning to learn a pitch-independent latent representation to avoid inaccurate disentanglement and cycle-consistency training to create paired training data implicitly. Experimental results on global-key and template-based pitch manipulation demonstrate the effectiveness of the proposed system, marking improved synthesis quality while maintaining the original singer identity."
   ],
   "p1": 1253,
   "pn": 1257,
   "doi": "10.21437/Interspeech.2025-154",
   "url": "interspeech_2025/gu25_interspeech.html"
  },
  "kim25c_interspeech": {
   "authors": [
    [
     "Miseul",
     "Kim"
    ],
    [
     "Seyun",
     "Um"
    ],
    [
     "Hyeonjin",
     "Cha"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "SpeechMLC: Speech Multi-label Classification",
   "original": "155",
   "order": 109,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a multi-label classification framework to detect multiple speaking styles in a speech sample. Unlike previous studies that have primarily focused on identifying a single target style, our framework effectively captures various speaker characteristics within a unified structure, making it suitable for generalized human-computer interaction applications. The proposed framework integrates cross-attention mechanisms within a transformer decoder to extract salient features associated with each target label from the input speech. To mitigate the data imbalance inherent in multi-label speech datasets, we employ a data augmentation technique based on a speech generation model. We validate our model&#x27;s effectiveness through multiple objective evaluations on seen and unseen corpora. In addition, we provide an analysis of the influence of human perception on classification accuracy by considering the impact of human labeling agreement on model performance."
   ],
   "p1": 514,
   "pn": 518,
   "doi": "10.21437/Interspeech.2025-155",
   "url": "interspeech_2025/kim25c_interspeech.html"
  },
  "hu25b_interspeech": {
   "authors": [
    [
     "Ruofan",
     "Hu"
    ],
    [
     "Yan",
     "Xia"
    ],
    [
     "Minjie",
     "Hong"
    ],
    [
     "Jieming",
     "Zhu"
    ],
    [
     "Bo",
     "Chen"
    ],
    [
     "Xiaoda",
     "Yang"
    ],
    [
     "Minghui",
     "Fang"
    ],
    [
     "Tao",
     "Jin"
    ]
   ],
   "title": "Vela: Scalable Embeddings with Voice Large Language Models for Multimodal Retrieval",
   "original": "159",
   "order": 539,
   "page_count": 5,
   "abstract": [
    "Multimodal large language models (MLLMs) have seen substantial progress in recent years. However, their ability to represent multimodal information in the acoustic domain remains underexplored. In this work, we introduce Vela, a novel framework designed to adapt MLLMs for the generation of universal multimodal embeddings. By leveraging MLLMs with specially crafted prompts and selected in-context learning examples, Vela effectively bridges the modality gap across various modalities. We then propose a single-modality training approach, where the model is trained exclusively on text pairs. Our experiments show that Vela outperforms traditional CLAP models in standard text-audio retrieval tasks. Furthermore, we introduce new benchmarks that expose CLAP models&#x27; limitations in handling long texts and complex retrieval tasks. In contrast, Vela, by harnessing the capabilities of MLLMs, demonstrates robust performance in these scenarios. Our code will soon be available."
   ],
   "p1": 2640,
   "pn": 2644,
   "doi": "10.21437/Interspeech.2025-159",
   "url": "interspeech_2025/hu25b_interspeech.html"
  },
  "ravenscroft25_interspeech": {
   "authors": [
    [
     "William",
     "Ravenscroft"
    ],
    [
     "George",
     "Close"
    ],
    [
     "Kit",
     "Bower-Morris"
    ],
    [
     "Jamie",
     "Stacey"
    ],
    [
     "Dmitry",
     "Sityaev"
    ],
    [
     "Kris Y.",
     "Hong"
    ]
   ],
   "title": "Whilter: A Whisper-based Data Filter for &quot;In-the-Wild&quot; Speech Corpora Using Utterance-level Multi-Task Classification ",
   "original": "160",
   "order": 874,
   "page_count": 5,
   "abstract": [
    "Large-scale in-the-wild speech datasets have become more prevalent in recent years due to increased interest in models that can learn useful features from unlabelled data for tasks such as speech recognition or synthesis. These datasets often contain undesirable features, such as multiple speakers, non-target languages, and music, which may impact model learning. The Whilter model is proposed as a multitask solution to identify these undesirable samples. Whilter uses a Whisper encoder with an attention-based classifier to solve five diverse classification problems at once. In addition, an annotated dataset is published for a subset of two popular in-the-wild corpora. Whilter achieves F1 scores above 85% and equal error rates of 6.5% to 7.8% for three of five subtasks, outperforming a state-of-the-art BEATs classifier on speech-specific classes, with a notable decrease in processing time compared to a combination of single-task alternatives."
   ],
   "p1": 4288,
   "pn": 4292,
   "doi": "10.21437/Interspeech.2025-160",
   "url": "interspeech_2025/ravenscroft25_interspeech.html"
  },
  "bhattacharya25_interspeech": {
   "authors": [
    [
     "Debasmita",
     "Bhattacharya"
    ],
    [
     "Aanya",
     "Tolat"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "From Context to Code-switching: Examining the Interplay of Language Proficiency and Multilingualism in Speech",
   "original": "165",
   "order": 922,
   "page_count": 5,
   "abstract": [
    "Multilingual speakers are known to code-switch across language pairs and in association with various paralinguistic aspects of conversation. We build on prior work by asking: how is a multilingual speaker&#x27;s language proficiency, as shaped by their linguistic background, related to their code-switching (CSW) behavior? To answer this question, we examine the Bangor Miami corpus of spontaneous Spanish-English speech and analyze the linguistic and demographic profiles of its speakers alongside features of their conversational language production. We find that a speaker&#x27;s parents&#x27; primary language, medium of secondary schooling, and self-reported ability are strongly associated with the quantity of CSW and language distribution in code-switched speech. Our work is the first to empirically show a relationship between language proficiency and multilingual speech production and calls for the inclusion of linguistic background features in future paralinguistic studies of CSW."
   ],
   "p1": 4528,
   "pn": 4532,
   "doi": "10.21437/Interspeech.2025-165",
   "url": "interspeech_2025/bhattacharya25_interspeech.html"
  },
  "kang25_interspeech": {
   "authors": [
    [
     "Wonjune",
     "Kang"
    ],
    [
     "Junteng",
     "Jia"
    ],
    [
     "Chunyang",
     "Wu"
    ],
    [
     "Wei",
     "Zhou"
    ],
    [
     "Egor",
     "Lakomkin"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Leda",
     "Sari"
    ],
    [
     "Suyoun",
     "Kim"
    ],
    [
     "Ke",
     "Li"
    ],
    [
     "Jay",
     "Mahadeokar"
    ],
    [
     "Ozlem",
     "Kalinli"
    ]
   ],
   "title": "Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech",
   "original": "166",
   "order": 881,
   "page_count": 5,
   "abstract": [
    "This work studies the capabilities of a large language model (LLM) to understand paralinguistic aspects of speech without fine-tuning its weights. We utilize an end-to-end system with a speech encoder, which is trained to produce token embeddings such that the LLM&#x27;s response to an expressive speech prompt is aligned with its response to a semantically matching text prompt that has also been conditioned on the user&#x27;s speaking style. This framework enables the encoder to generate tokens that capture both linguistic and paralinguistic information and effectively convey them to the LLM, even when the LLM&#x27;s weights remain completely frozen. To the best of our knowledge, our work is the first to explore how to induce a frozen LLM to understand more than just linguistic content from speech inputs in a general interaction setting. Experiments demonstrate that our system is able to produce higher quality and more empathetic responses to expressive speech prompts compared to several baselines."
   ],
   "p1": 4323,
   "pn": 4327,
   "doi": "10.21437/Interspeech.2025-166",
   "url": "interspeech_2025/kang25_interspeech.html"
  },
  "heo25_interspeech": {
   "authors": [
    [
     "Seongsil",
     "Heo"
    ],
    [
     "Christi",
     "Miller"
    ],
    [
     "Calvin",
     "Murdock"
    ],
    [
     "Michael",
     "Proulx"
    ]
   ],
   "title": "Gaze-Enhanced Multimodal Turn-Taking Prediction in Triadic Conversations",
   "original": "167",
   "order": 220,
   "page_count": 5,
   "abstract": [
    "Turn-taking prediction is crucial for seamless interactions. This study introduces a novel, lightweight framework for accurate turn-taking prediction in triadic conversations without relying on computationally intensive methods. Unlike prior approaches that either disregard gaze or treat it as a passive signal, our model integrates gaze with speaker localization, structuring it within a spatial constraint to transform it into a reliable predictive cue. Leveraging egocentric behavioral cues, our experiments demonstrate that incorporating gaze data from a single-user significantly improves prediction performance, while gaze data from multiple-users further enhances it by capturing richer conversational dynamics. This study presents a lightweight and privacy-conscious approach to support adaptive, directional sound control, enhancing speech intelligibility in noisy environments, particularly for hearing assistance in smart glasses."
   ],
   "p1": 1068,
   "pn": 1072,
   "doi": "10.21437/Interspeech.2025-167",
   "url": "interspeech_2025/heo25_interspeech.html"
  },
  "kwok25_interspeech": {
   "authors": [
    [
     "Chin Yuen",
     "Kwok"
    ],
    [
     "Jia Qi",
     "Yip"
    ],
    [
     "Zhen",
     "Qiu"
    ],
    [
     "Chi Hung",
     "Chi"
    ],
    [
     "Kwok Yan",
     "Lam"
    ]
   ],
   "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems",
   "original": "172",
   "order": 457,
   "page_count": 5,
   "abstract": [
    "Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research."
   ],
   "p1": 2230,
   "pn": 2234,
   "doi": "10.21437/Interspeech.2025-172",
   "url": "interspeech_2025/kwok25_interspeech.html"
  },
  "kwok25b_interspeech": {
   "authors": [
    [
     "Chin Yuen",
     "Kwok"
    ],
    [
     "Jia Qi",
     "Yip"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function",
   "original": "173",
   "order": 794,
   "page_count": 5,
   "abstract": [
    "Rare word recognition can be improved by adapting ASR models to synthetic data that includes these words. Further improvements can be achieved through contextual biasing, which trains and adds a biasing module into the model architecture to prioritize rare words. While training the module on synthetic rare word data is more effective than using non-rare-word data, it can lead to overfitting due to artifacts in the synthetic audio. To address this, we enhance the TCPGen-based contextual biasing approach and propose a keyword-aware loss function that additionally focuses on biased words when training biasing modules. This loss includes a masked cross-entropy term for biased word prediction and a binary classification term for detecting biased word positions. These two terms complementarily support the decoding of biased words during inference. By adapting Whisper to 10 hours of synthetic data, our method reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81%."
   ],
   "p1": 3888,
   "pn": 3892,
   "doi": "10.21437/Interspeech.2025-173",
   "url": "interspeech_2025/kwok25b_interspeech.html"
  },
  "lepagnol25_interspeech": {
   "authors": [
    [
     "Pierre",
     "Lepagnol"
    ],
    [
     "Sahar",
     "Ghannay"
    ],
    [
     "Thomas",
     "Gerald"
    ],
    [
     "Christophe",
     "Servan"
    ],
    [
     "Sophie",
     "Rosset"
    ]
   ],
   "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning",
   "original": "175",
   "order": 838,
   "page_count": 5,
   "abstract": [
    "Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.\n",
    "In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length."
   ],
   "p1": 4108,
   "pn": 4112,
   "doi": "10.21437/Interspeech.2025-175",
   "url": "interspeech_2025/lepagnol25_interspeech.html"
  },
  "pawlowski25_interspeech": {
   "authors": [
    [
     "Pawel",
     "Pawlowski"
    ],
    [
     "Krystian",
     "Zawistowski"
    ],
    [
     "Wojciech",
     "Lapacz"
    ],
    [
     "Adam",
     "Wiacek"
    ],
    [
     "Marcin",
     "Skorupa"
    ],
    [
     "Sebastien",
     "Postansque"
    ],
    [
     "Jakub",
     "Hoscilowicz"
    ]
   ],
   "title": "TinyClick: Single-Turn Agent for Empowering GUI Automation",
   "original": "176",
   "order": 618,
   "page_count": 5,
   "abstract": [
    "We present an UI agent for user interface (UI) interaction tasks, using Vision-Language Model Florence-2-Base. The agent&#x27;s primary task is identifying the screen coordinates of the UI element corresponding to the user&#x27;s command. It demonstrates very strong performance on Screenspot and OmniAct annotations, while maintaining a very small size of 0.27B parameters and minimal latency. Moreover, training needs small compute budget of 56 GPU-hours (worth about $40). Relevant improvement comes from vision-specific multi-task training and MLLM-based data augmentation. We hope that decreased needs for expensive compute resources and manually annotated data will allow to facilitate more inclusive and sustainable research of UI agents."
   ],
   "p1": 3035,
   "pn": 3039,
   "doi": "10.21437/Interspeech.2025-176",
   "url": "interspeech_2025/pawlowski25_interspeech.html"
  },
  "maran25_interspeech": {
   "authors": [
    [
     "Matteo",
     "Maran"
    ],
    [
     "Renske",
     "Rötjes"
    ],
    [
     "Anna R. E.",
     "Schreurs"
    ],
    [
     "Hans Rutger",
     "Bosker"
    ]
   ],
   "title": "Beat gestures made by human-like avatars affect speech perception",
   "original": "178",
   "order": 1028,
   "page_count": 5,
   "abstract": [
    "In face-to-face communication, several visual cues support speech perception. Even the timing of simple up-and-down flicks of the hand, called beat gestures, can convey word stress, changing what individuals hear (e.g., CONtent vs. conTENT). While beat gestures have been traditionally investigated in human-to-human communications, nowadays individuals increasingly interact with computer-controlled avatars (e.g., virtual assistants). The present study tested whether beat gestures produced by an avatar affect word stress perception, similarly to human gestures. Furthermore, this study tested whether a minimal visual cue such as a 2D moving disc can also affect speech perception. Beat gestures made by the avatar significantly affected speech perception, albeit slightly less than human-made gestures. The disc condition did not affect speech perception. The present work lays the foundation for the application of (beat) gesturing avatars, which could be used to boost speech intelligibility."
   ],
   "p1": 5038,
   "pn": 5042,
   "doi": "10.21437/Interspeech.2025-178",
   "url": "interspeech_2025/maran25_interspeech.html"
  },
  "zhuang25_interspeech": {
   "authors": [
    [
     "Ziyang",
     "Zhuang"
    ],
    [
     "Tao",
     "Wei"
    ],
    [
     "Ming",
     "Fang"
    ],
    [
     "Ning",
     "Cheng"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Towards Efficiently Whisper Fine-tuning with Monotonic Alignments ",
   "original": "179",
   "order": 736,
   "page_count": 5,
   "abstract": [
    "Whisper is a popular large-scale model developed by OpenAI, which can perform robust speech recognition and speech translation tasks. Fine-tuning Whisper on various domains thus becomes a common approach to improve the model performance on specific tasks. This paper introduces an efficient fine-tuning method that enhances Whisper&#x27;s fine-tuning quality by incorporating a training objective based on monotonic alignments, without modifying the model network. Experimental results show that the proposed fine-tuning approach achieves consistent and considerable improvements over normal fine-tuned models on speech recognition tasks and some of the speech translation tasks. In addition, a new dataset MDCCST is built to validate the performance of Cantonese-to-Mandarin translation."
   ],
   "p1": 3598,
   "pn": 3602,
   "doi": "10.21437/Interspeech.2025-179",
   "url": "interspeech_2025/zhuang25_interspeech.html"
  },
  "patman25_interspeech": {
   "authors": [
    [
     "Chloe",
     "Patman"
    ],
    [
     "Paul",
     "Foulkes"
    ],
    [
     "Kirsty",
     "McDougall"
    ]
   ],
   "title": "Evaluating the suitability of acoustic parameters for capturing breathy voice in non-pathological female speakers",
   "original": "180",
   "order": 856,
   "page_count": 5,
   "abstract": [
    "This study evaluates the suitability of acoustic parameters for capturing breathy voice in non-pathological female speakers. Much existing literature on the acoustic analysis of voice quality (VQ) focuses on pathological speakers and/or sustained vowel productions. The limited work on more naturalistic speech (e.g. as needed for forensic casework) focuses on male speakers. Using studio-quality recordings from the PASR dataset, nine phoneticians (6 male, 3 female) read a passage in their normal (‘default’) and breathy voice multiple times. Acoustic parameters (H1−H2, H1*−H2*, H1−A1, H1*−A1*, CPP, HNR05/15/25/35) were automatically extracted using VoiceSauce. A linear mixed effects regression analysis showed significant interactions between acoustic parameters and speaker sex. In support of, our study confirms CPP as a relatively robust parameter for males and extends its suitability to females, with CPP and HNR particularly suitable for female breathy voice."
   ],
   "p1": 4198,
   "pn": 4202,
   "doi": "10.21437/Interspeech.2025-180",
   "url": "interspeech_2025/patman25_interspeech.html"
  },
  "ducorroy25_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Ducorroy"
    ],
    [
     "Rachid",
     "Riad"
    ]
   ],
   "title": "Robust fine-tuning of speech recognition models via model merging: application to disordered speech",
   "original": "182",
   "order": 667,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) has advanced with Speech Foundation Models (SFMs), yet performance degrades on dysarthric speech due to variability and limited data. This study as part of the submission to the Speech Accessibility challenge, explored model merging to improve ASR generalization using Whisper as the base SFM. We compared fine-tuning with single-trajectory merging, combining models from one fine-tuning path, and multi-run merging, merging independently trained models.  Our best multi-run merging approach achieved a 12% relative decrease of WER over classic fine-tuning, and a 16.2% relative decrease on long-form audios, a major loss contributor in dysarthric ASR. Merging more and more models led to continuous gains, remained effective in low-data regimes, and generalized across model architectures. These results highlight model merging as an easily replicable adaptation method that consistently improves ASR without additional inference cost or hyperparameter tuning."
   ],
   "p1": 3279,
   "pn": 3283,
   "doi": "10.21437/Interspeech.2025-182",
   "url": "interspeech_2025/ducorroy25_interspeech.html"
  },
  "lepage25_interspeech": {
   "authors": [
    [
     "Theo",
     "Lepage"
    ],
    [
     "Reda",
     "Dehak"
    ]
   ],
   "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification",
   "original": "183",
   "order": 226,
   "page_count": 5,
   "abstract": [
    "Self-Supervised Learning (SSL) has led to considerable progress in Speaker Verification (SV). The standard framework uses same-utterance positive sampling and data-augmentation to generate anchor-positive pairs of the same speaker. This is a major limitation, as this strategy primarily encodes channel information from the recording condition, shared by the anchor and positive. We propose a new positive sampling technique to address this bottleneck: Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find an appropriate positive, i.e., of the same speaker identity but a different recording condition, in the latent space using clustering assignments and a memory queue of positive embeddings. SSPS improves SV performance for both SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by lowering intra-speaker variance, providing comparable performance to DINO-SSPS."
   ],
   "p1": 1098,
   "pn": 1102,
   "doi": "10.21437/Interspeech.2025-183",
   "url": "interspeech_2025/lepage25_interspeech.html"
  },
  "wang25_interspeech": {
   "authors": [
    [
     "Yingzhi",
     "Wang"
    ],
    [
     "Anas",
     "Alhmoud"
    ],
    [
     "Muhammad",
     "Alqurishi"
    ]
   ],
   "title": "Open Universal Arabic ASR Leaderboard",
   "original": "184",
   "order": 240,
   "page_count": 5,
   "abstract": [
    "In recent years, the enhanced capabilities of ASR models and the emergence of multi-dialect datasets have increasingly pushed Arabic ASR model development toward an all-dialect-in-one direction. This trend highlights the need for benchmarking studies that evaluate model performance in a multi-dialect setting, providing the community with insights into models&#x27; generalization capabilities.\n",
    "In this paper, we introduce Open Universal Arabic ASR Leaderboard, a continuous benchmark project for open-source universal Arabic ASR models across various multi-dialect datasets. We also provide a comprehensive analysis of the model&#x27;s robustness, speaker adaptation, inference efficiency, and memory consumption. This work aims to offer the Arabic ASR community a reference for models&#x27; general performance and also establish a common evaluation framework for multi-dialectal Arabic ASR models."
   ],
   "p1": 1168,
   "pn": 1172,
   "doi": "10.21437/Interspeech.2025-184",
   "url": "interspeech_2025/wang25_interspeech.html"
  },
  "ren25_interspeech": {
   "authors": [
    [
     "Yong",
     "Ren"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Le",
     "Xu"
    ],
    [
     "Hao",
     "Gu"
    ],
    [
     "Duzhen",
     "Zhang"
    ],
    [
     "Yujie",
     "Chen"
    ],
    [
     "Manjie",
     "Xu"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Shan",
     "Yang"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model",
   "original": "190",
   "order": 33,
   "page_count": 5,
   "abstract": [
    "Humans can intuitively infer sounds from silent videos, but whether multimodal large language models can perform modal-mismatch reasoning without accessing target modalities remains relatively unexplored. Current text-assisted-video-to-audio (VT2A) methods excel in video foley tasks but struggle to acquire audio descriptions during inference. We introduce the task of Reasoning Audio Descriptions from Silent Videos (SVAD) to address this challenge and investigate vision-language models&#x27; (VLMs) capabilities on this task. To further enhance the VLMs&#x27; reasoning capacity for the SVAD task, we construct a CoT-AudioCaps dataset and propose a Chain-of-Thought-based supervised fine-tuning strategy. Experiments on SVAD and subsequent VT2A tasks demonstrate our method&#x27;s effectiveness in two key aspects: significantly improving VLMs&#x27; modal-mismatch reasoning for SVAD and effectively addressing the challenge of acquiring audio descriptions during VT2A inference."
   ],
   "p1": 156,
   "pn": 160,
   "doi": "10.21437/Interspeech.2025-190",
   "url": "interspeech_2025/ren25_interspeech.html"
  },
  "park25_interspeech": {
   "authors": [
    [
     "Chanho",
     "Park"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Semi-Supervised Learning for Automatic Speech Recognition with Word Error Rate Estimation and Targeted Domain Data Selection",
   "original": "191",
   "order": 749,
   "page_count": 5,
   "abstract": [
    "There is a growing demand for leveraging untranscribed multi-domain data in semi-supervised learning (SSL) for automatic speech recognition (ASR) to broaden its applications. However, domain mismatch between source and target data can limit SSL’s performance gains, even when transcript accuracy for training is high. While word error rate (WER) estimation (WE) methods for automatic transcription have advanced, they remain insufficient for handling multi-domain data.\n",
    "This paper proposes a novel data selection method for SSL in ASR that integrates WE and acoustic domain similarity (ADS). For WE, multi-target regression for error rate prediction (MTR-ER) is introduced, while ADS is incorporated as a selection criterion, measured using noise-contrastive estimation. The effectiveness of this approach is demonstrated through comparisons with a confidence-based method. Results show that combining WE and ADS achieves 26.66% of the expected performance improvement of fully supervised learning."
   ],
   "p1": 3663,
   "pn": 3667,
   "doi": "10.21437/Interspeech.2025-191",
   "url": "interspeech_2025/park25_interspeech.html"
  },
  "kim25d_interspeech": {
   "authors": [
    [
     "Dohyun",
     "Kim"
    ],
    [
     "Jiwook",
     "Hwang"
    ]
   ],
   "title": "Fully End-to-end Streaming Open-vocabulary Keyword Spotting with W-CTC Forced Alignment",
   "original": "193",
   "order": 110,
   "page_count": 5,
   "abstract": [
    "In open-vocabulary keyword spotting, an acoustic encoder pre-trained with Connectionist Temporal Classification (CTC) loss is typically used to train a text encoder by aligning audio embedding space with text embedding space. In previous work, word-aligned datasets were created by forced alignment algorithms such as the Montreal Forced Aligner (MFA) to train text encoder and verifier models. In this paper, we propose a new training pipeline for open-vocabulary keyword spotting using the W-CTC forced alignment algorithm, a simple modification of the practical CTC algorithm. Our approach eliminates the need for creating word-aligned datasets, operates in a fully end-to-end manner, and demonstrates superior performance on the Libriphrase hard dataset."
   ],
   "p1": 519,
   "pn": 523,
   "doi": "10.21437/Interspeech.2025-193",
   "url": "interspeech_2025/kim25d_interspeech.html"
  },
  "yao25_interspeech": {
   "authors": [
    [
     "Jixun",
     "Yao"
    ],
    [
     "Hexin",
     "Liu"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "EASY: Emotion-aware Speaker Anonymization via Factorized Distillation",
   "original": "194",
   "order": 655,
   "page_count": 5,
   "abstract": [
    "Emotion plays a significant role in speech interaction, conveyed through tone, pitch, and rhythm, enabling the expression of feelings and intentions beyond words to create a more personalized experience. However, most existing speaker anonymization systems employ parallel disentanglement methods, which only separate speech into linguistic content and speaker identity, often neglecting the preservation of the original emotional state. In this study, we introduce EASY, an emotion-aware speaker anonymization framework. EASY employs a novel sequential disentanglement process to disentangle speaker identity, linguistic content, and emotional representation, modeling each speech attribute in distinct subspaces through a factorized distillation approach. By independently constraining speaker identity and emotional representation, EASY minimizes information leakage, enhancing privacy protection while preserving original linguistic content and emotional state. Experimental results on the VoicePrivacy Challenge official datasets demonstrate that our proposed approach outperforms all baseline systems, effectively protecting speaker privacy while maintaining linguistic content and emotional state."
   ],
   "p1": 3219,
   "pn": 3223,
   "doi": "10.21437/Interspeech.2025-194",
   "url": "interspeech_2025/yao25_interspeech.html"
  },
  "wen25_interspeech": {
   "authors": [
    [
     "Liang",
     "Wen"
    ],
    [
     "Lizhong",
     "Wang"
    ],
    [
     "Yuxing",
     "Zheng"
    ],
    [
     "Weijing",
     "Shi"
    ],
    [
     "Kwang Pyo",
     "Choi"
    ]
   ],
   "title": "SPCODEC: Split and Prediction for Neural Speech Codec",
   "original": "196",
   "order": 1022,
   "page_count": 5,
   "abstract": [
    "Recent advancements in time-domain end-to-end neural speech codecs have significantly improved performance. However, existing codecs fail to fully exploit the correlations across different frequency bands in speech, leading to inefficiencies and reduced interpretability. In this paper, we introduce SPCODEC, a time-domain end-to-end neural speech codec featuring a latent split-and-prediction scheme. The model consists of a fully convolutional encoder-decoder and a group residual vector quantization module enhanced with a split-and-prediction mechanism. This mechanism disentangles low- and high-frequency representations and employs prediction to effectively reduce feature redundancy. SPCODEC achieves state-of-the-art MOS-POLQA scores of 4.0 at 6/8 kbps and 4.5 at 10.66/16 kbps for wideband and super-wideband speech, significantly outperforming both neural and traditional codecs."
   ],
   "p1": 5008,
   "pn": 5012,
   "doi": "10.21437/Interspeech.2025-196",
   "url": "interspeech_2025/wen25_interspeech.html"
  },
  "zhao25b_interspeech": {
   "authors": [
    [
     "Fei",
     "Zhao"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Room Impulse Response as a Prompt for Acoustic Echo Cancellation",
   "original": "197",
   "order": 159,
   "page_count": 5,
   "abstract": [
    "Data-driven acoustic echo cancellation (AEC) methods, predominantly trained on synthetic or constrained real-world datasets, encounter performance declines in unseen echo scenarios, especially in real environments where echo paths are not directly observable. Our proposed method counters this limitation by integrating room impulse response (RIR) as a pivotal training prompt, aiming to improve the generalization of AEC models in such unforeseen conditions. We also explore four RIR prompt fusion methods. Comprehensive evaluations, including both simulated RIR under unknown conditions and recorded RIR in real, demonstrate that the proposed approach significantly improves performance compared to baseline models. These results substantiate the effectiveness of our RIR-guided approach in strengthening the model&#x27;s generalization capabilities."
   ],
   "p1": 763,
   "pn": 767,
   "doi": "10.21437/Interspeech.2025-197",
   "url": "interspeech_2025/zhao25b_interspeech.html"
  },
  "tian25_interspeech": {
   "authors": [
    [
     "Jingguang",
     "Tian"
    ],
    [
     "Haoqin",
     "Sun"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Xinkang",
     "Xu"
    ]
   ],
   "title": "Discrete Audio Representations for Automated Audio Captioning",
   "original": "199",
   "order": 636,
   "page_count": 5,
   "abstract": [
    "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task."
   ],
   "p1": 3125,
   "pn": 3129,
   "doi": "10.21437/Interspeech.2025-199",
   "url": "interspeech_2025/tian25_interspeech.html"
  },
  "parsons25_interspeech": {
   "authors": [
    [
     "Phoebe",
     "Parsons"
    ],
    [
     "Heming Strømholt",
     "Bremnes"
    ],
    [
     "Knut",
     "Kvale"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Effects of Prosodic Information on Dialect Classification Using Whisper Features",
   "original": "200",
   "order": 568,
   "page_count": 5,
   "abstract": [
    "In dialect identification (DID), a model needs to attend to subtle cues to distinguish between highly similar linguistic variants. However, the knowledge of which cues are important and why is limited. Inspired by the literature on human DID, we fine-tuned a Whisper model with modified audio to see how deprivation of various signal components would impact performance. Specifically, the audio manipulation sought to either isolate or remove (tonal) prosodic information, by either low-pass filtering or monotonizing F0, respectively. Results indicate that fine-tuning on low-pass filtered data produces a significant improvement over unmodified data. Utilizing sensitivity maps in the frequency domain, we argue that the low-pass model is able to devote more attention to lower frequency bands, thus exploiting task-relevant pitch dynamics. Though only evaluated with Norwegian, we suggest that our methodology should generalize, encouraging improvement in DID and its downstream applications."
   ],
   "p1": 2785,
   "pn": 2789,
   "doi": "10.21437/Interspeech.2025-200",
   "url": "interspeech_2025/parsons25_interspeech.html"
  },
  "wang25b_interspeech": {
   "authors": [
    [
     "Yingzhi",
     "Wang"
    ],
    [
     "Anas",
     "Alhmoud"
    ],
    [
     "Saad",
     "Alsahly"
    ],
    [
     "Muhammad",
     "Alqurishi"
    ],
    [
     "Mirco",
     "Ravanelli"
    ]
   ],
   "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down",
   "original": "201",
   "order": 694,
   "page_count": 5,
   "abstract": [
    "OpenAI&#x27;s Whisper has achieved significant success in Automatic Speech Recognition. However, it has consistently been found to exhibit hallucination issues, particularly in non-speech segments, which limits its broader application in complex industrial settings. \n",
    "In this paper, we introduce a novel method to reduce Whisper&#x27;s hallucination on non-speech segments without using any pre- or post-possessing techniques. Specifically, we benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder to the hallucination problem by performing a head-wise mask. Our findings reveal that only 3 of the 20 heads account for over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune these three crazy heads using a collection of non-speech data. The results show that our best fine-tuned model, namely Calm-Whisper, achieves over 80% reduction in non-speech hallucination with only less than 0.1% WER degradation on LibriSpeech test-clean and test-other."
   ],
   "p1": 3414,
   "pn": 3418,
   "doi": "10.21437/Interspeech.2025-201",
   "url": "interspeech_2025/wang25b_interspeech.html"
  },
  "pan25b_interspeech": {
   "authors": [
    [
     "Yu",
     "Pan"
    ],
    [
     "Yanni",
     "Hu"
    ],
    [
     "Yuguang",
     "Yang"
    ],
    [
     "Jixun",
     "Yao"
    ],
    [
     "Jianhao",
     "Ye"
    ],
    [
     "Hongbin",
     "Zhou"
    ],
    [
     "Lei",
     "Ma"
    ],
    [
     "Jianjun",
     "Zhao"
    ]
   ],
   "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech",
   "original": "203",
   "order": 933,
   "page_count": 5,
   "abstract": [
    "Despite great advances, achieving high-fidelity emotional voice conversion (EVC) with flexible and interpretable control remains challenging. This paper introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality converted speech driven by natural language prompts or reference speech with adjustable emotion intensity. We first propose EVC-CLAP, an emotional contrastive language-audio pre-training model, guided by natural language prompts and categorical labels, to extract and align fine-grained emotional elements across speech and text modalities. Then, a FuEncoder with an adaptive intensity gate is presented to seamless fuse emotional features with Phonetic PosteriorGrams from a pre-trained ASR model. To further improve emotion expressiveness and speech naturalness, we propose a flow matching model conditioned on these captured features to reconstruct Mel-spectrogram of source speech. Subjective and objective evaluations validate the effectiveness of ClapFM-EVC."
   ],
   "p1": 4583,
   "pn": 4587,
   "doi": "10.21437/Interspeech.2025-203",
   "url": "interspeech_2025/pan25b_interspeech.html"
  },
  "ljubesic25_interspeech": {
   "authors": [
    [
     "Nikola",
     "Ljubešić"
    ],
    [
     "Ivan",
     "Porupski"
    ],
    [
     "Peter",
     "Rupnik"
    ]
   ],
   "title": "Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models",
   "original": "205",
   "order": 1175,
   "page_count": 5,
   "abstract": [
    "Automating primary stress identification has been an active research field due to the role of stress in encoding meaning and aiding speech comprehension. Previous studies relied mainly on traditional acoustic features and English datasets. In this paper we investigate the approach of fine-tuning a pre-trained transformer model with an audio frame classification head. Our experiments use a new Croatian training dataset, with test sets in Croatian, Serbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier using traditional acoustic features with the fine-tuned speech transformer, we demonstrate the transformer&#x27;s superiority across the board, achieving near-perfect results for Croatian and Serbian, with a 10-point performance drop for more distant Chakavian and Slovenian. Finally, we show that only a few hundred multi-syllabic training words suffice for strong performance. We release our datasets and model under permissive licenses."
   ],
   "p1": 5768,
   "pn": 5772,
   "doi": "10.21437/Interspeech.2025-205",
   "url": "interspeech_2025/ljubesic25_interspeech.html"
  },
  "jung25_interspeech": {
   "authors": [
    [
     "Chaeyoung",
     "Jung"
    ],
    [
     "Hojoon",
     "Ki"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Junmo",
     "Kim"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "InfiniteAudio: Infinite-Length Audio Generation with Consistency",
   "original": "209",
   "order": 859,
   "page_count": 5,
   "abstract": [
    "This paper presents InfiniteAudio, a simple yet effective strategy for generating infinite-length audio using diffusion-based text-to-audio methods. Current approaches face memory constraints because the output size increases with input length, making long duration generation challenging. A common workaround is to concatenate short audio segments, but this often leads to inconsistencies due to the lack of shared temporal context. To address this, InfiniteAudio integrates seamlessly into existing pipelines without additional training. It introduces two key techniques: FIFO sampling, a first-in, first-out inference strategy with fixed-size inputs, and curved denoising, which selectively prioritizes key diffusion steps for efficiency. Experiments show that InfiniteAudio achieves comparable or superior performance across all metrics."
   ],
   "p1": 4213,
   "pn": 4217,
   "doi": "10.21437/Interspeech.2025-209",
   "url": "interspeech_2025/jung25_interspeech.html"
  },
  "itani25_interspeech": {
   "authors": [
    [
     "Malek",
     "Itani"
    ],
    [
     "Ashton",
     "Graves"
    ],
    [
     "Sefik",
     "Emre Eskimez"
    ],
    [
     "Shyamnath",
     "Gollakota"
    ]
   ],
   "title": "Neural Speech Extraction with Human Feedback",
   "original": "214",
   "order": 1020,
   "page_count": 5,
   "abstract": [
    "We present the first neural target speech extraction (TSE) system that uses human feedback for iterative refinement. Our approach allows users to mark specific segments of the TSE output, generating an edit mask. The refinement system then improves the marked sections while preserving unmarked regions. Since large-scale datasets of human-marked errors are difficult to collect, we generate synthetic datasets using various automated masking functions and train models on each. Evaluations show that models trained with noise power-based masking (in dBFS) and probabilistic thresholding perform best, aligning with human annotations. In a study with 22 participants, users showed a preference for refined outputs over baseline TSE. Our findings demonstrate that human-in-the-loop refinement is a promising approach for improving the performance of neural speech extraction."
   ],
   "p1": 4998,
   "pn": 5002,
   "doi": "10.21437/Interspeech.2025-214",
   "url": "interspeech_2025/itani25_interspeech.html"
  },
  "shao25_interspeech": {
   "authors": [
    [
     "Yongqi",
     "Shao"
    ],
    [
     "Tao",
     "Fang"
    ]
   ],
   "title": "Alzheimer’s Disease Detection Using Co-Attention Mechanism for Acoustic and ASR-Transcribed Text Features",
   "original": "219",
   "order": 1156,
   "page_count": 5,
   "abstract": [
    "Alzheimer&#x27;s disease (AD) is a progressive disorder that gradually affects memory, language, and reasoning, making early detection crucial for timely intervention. Traditional methods, like medical imaging and clinical evaluations, are costly and limit accessibility. To address this, we propose a speech-based AD detection method that leverages a co-attention mechanism to integrate multilevel acoustic and transcribed text features. These acoustic features include spectrograms, MFCC features, and wav2vec2 embeddings. The mechanism dynamically assigns weights to different features, enhancing their interaction and improving fusion. We also compare early and late fusion strategies to optimize integration. Tested on the ADReSSo dataset, our model achieves 83.15% accuracy, demonstrating the effectiveness of a well-structured integration of acoustic and transcribed text features for accessible and cost-efficient AD detection."
   ],
   "p1": 5673,
   "pn": 5677,
   "doi": "10.21437/Interspeech.2025-219",
   "url": "interspeech_2025/shao25_interspeech.html"
  },
  "loweimi25_interspeech": {
   "authors": [
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Sofia",
     "de la Fuente Garcia"
    ],
    [
     "Saturnino",
     "Luz"
    ]
   ],
   "title": "Zero-Shot Speech-Based Depression and Anxiety Assessment with LLMs",
   "original": "235",
   "order": 104,
   "page_count": 5,
   "abstract": [
    "The use of Large Language Models (LLMs) for psychological state assessment from speech has gained significant interest, particularly in analysing and predicting mental health. In this paper, we explore the potential of eight instruct-tuned LLMs (Llama-3.1-8B, Ministral, Gemma-2-9B, Phi-4, Mistral, DeepSeek-Qwen, QwQ-Preview and Llama-3.3-70B) in a zero-shot setting to predict Hospital Anxiety and Depression Scale (HADS) depression and anxiety scores from one-to-two minute spontaneous speech recordings from the PsyVoiD database. We evaluate how transcript quality affects LLM responses by comparing performance using ground-truth transcriptions versus transcripts generated by Whisper models of different sizes. Spearman correlation coefficients and statistical analysis demonstrate significant and notable potential of the LLMs to predict psychological states in a zero-shot setting."
   ],
   "p1": 489,
   "pn": 493,
   "doi": "10.21437/Interspeech.2025-235",
   "url": "interspeech_2025/loweimi25_interspeech.html"
  },
  "yuen25_interspeech": {
   "authors": [
    [
     "Ivan",
     "Yuen"
    ],
    [
     "Katherine",
     "Demuth"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "How do both phonological and syntactic complexity influence speech planning?",
   "original": "236",
   "order": 74,
   "page_count": 4,
   "abstract": [
    "Wheeldon &amp; Lahiri reported that speech initiation time (RT) was influenced by the phonological complexity of the initial prosodic word (PWd) in an immediate-production task, but by the number of PWds in a delayed-production task that enabled preplanning. Thus, different tasks are sensitive to different types of complexity. The current study explored RT for different complexity types: number of syllables in the PWd (monosyllabic vs disyllabic), location of a complex PWd (early vs late), and number of PWds (3 vs. 4 vs. 5) using a reading aloud task. The presence of a disyllabic noun increased RT only in 4-PWd sentences, where the syntactic complexity of the SUBJECT and OBJECT NPs differed. This suggests that RT may increase in the absence of parallel syntactic structures in SUBJECT and OBJECT position, highlighting the need for further research on the role of complexity at different levels in the speech planning process."
   ],
   "p1": 340,
   "pn": 343,
   "doi": "10.21437/Interspeech.2025-236",
   "url": "interspeech_2025/yuen25_interspeech.html"
  },
  "zhao25c_interspeech": {
   "authors": [
    [
     "Kunlong",
     "Zhao"
    ],
    [
     "Gongping",
     "Huang"
    ],
    [
     "Xudong",
     "Zhao"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Jacob",
     "Benesty"
    ],
    [
     "Zoran",
     "Cvetkovic"
    ]
   ],
   "title": "On the Design of a Robust Superdirective Beamformer and Topology Parameter Optimization with Frustum-Shaped Microphone Arrays Featuring Multiple Rings",
   "original": "238",
   "order": 700,
   "page_count": 5,
   "abstract": [
    "Superdirective beamformers using concentric circular microphone arrays are widely used in various applications for their steering capabilities, high directional gains, and robustness. However, their performance often degrades when the beamformer is steered outside the sensor plane, a limitation common in real-world scenarios due to the array&#x27;s planar configuration. To address this challenge, this paper introduces a novel class of co-axial frustum-shaped concentric circular microphone arrays, where multiple rings are distributed across different horizontal planes. A robust superdirective beamformer is designed by converting beamforming optimization into a quadratic eigenvalue problem. Additionally, an approach is proposed to optimize the array topology parameters using the particle swarm optimization algorithm. Simulation results demonstrate that the proposed method significantly improves beampattern steering and directivity, while effectively controlling the white noise gain."
   ],
   "p1": 3444,
   "pn": 3448,
   "doi": "10.21437/Interspeech.2025-238",
   "url": "interspeech_2025/zhao25c_interspeech.html"
  },
  "lu25_interspeech": {
   "authors": [
    [
     "Shenghui",
     "Lu"
    ],
    [
     "Hukai",
     "Huang"
    ],
    [
     "Jinanglong",
     "Yao"
    ],
    [
     "Kaidi",
     "Wang"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement",
   "original": "242",
   "order": 13,
   "page_count": 5,
   "abstract": [
    "This paper proposes a model that integrates sub-band processing and deep filtering to fully exploit information from the target time-frequency (TF) bin and its surrounding TF bins for single-channel speech enhancement. The sub-band module captures surrounding frequency bin information at the input, while the deep filtering module applies filtering at the output to both the target TF bin and its surrounding TF bins. To further improve the model performance, we decouple deep filtering into temporal and frequency components and introduce a two-stage framework, reducing the complexity of filter coefficient prediction at each stage. Additionally, we propose the TAConv module to strengthen convolutional feature extraction. Experimental results demonstrate that the proposed hierarchical deep filtering network (HDF-Net) effectively utilizes surrounding TF bin information and outperforms other advanced systems while using fewer resources."
   ],
   "p1": 56,
   "pn": 60,
   "doi": "10.21437/Interspeech.2025-242",
   "url": "interspeech_2025/lu25_interspeech.html"
  },
  "zeng25_interspeech": {
   "authors": [
    [
     "Xijie",
     "Zeng"
    ],
    [
     "Frank",
     "Rudzicz"
    ]
   ],
   "title": "How to Recover Long Audio Sequences Through Gradient Inversion Attack With Dynamic Segment-based Reconstruction",
   "original": "244",
   "order": 1044,
   "page_count": 5,
   "abstract": [
    "Recent advancements in gradient inversion attacks have demonstrated the vulnerability of shared gradients in distributed learning systems, particularly in image and text domains. However, applying these techniques to audio data, especially longer speech sequences, remains largely unexplored. Our study introduces a novel approach that builds upon the principles of gradient inversion attacks to retrieve high-quality audio recordings from shared gradients. We propose an optimized spectrogram segmentation technique that enables extracting longer audio sequences with diverse acoustic features, without requiring complex post-processing techniques. Through this study, we overcome the limitations of previous methods that were restricted to short audio clips with simple acoustic features and limited semantic information."
   ],
   "p1": 5118,
   "pn": 5122,
   "doi": "10.21437/Interspeech.2025-244",
   "url": "interspeech_2025/zeng25_interspeech.html"
  },
  "chang25_interspeech": {
   "authors": [
    [
     "Heng-Jui",
     "Chang"
    ],
    [
     "Hongyu",
     "Gong"
    ],
    [
     "Changhan",
     "Wang"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Yu-An",
     "Chung"
    ]
   ],
   "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
   "original": "246",
   "order": 1166,
   "page_count": 5,
   "abstract": [
    "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents SpinHuBERT and Double-Codebook Speaker-invariant Clustering (DC-Spin) to improve speech tokenization for bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. Comparisons of tokenization methods and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, offering insights for designing speech tokenizers for SLMs."
   ],
   "p1": 5723,
   "pn": 5727,
   "doi": "10.21437/Interspeech.2025-246",
   "url": "interspeech_2025/chang25_interspeech.html"
  },
  "ariga25_interspeech": {
   "authors": [
    [
     "Terumichi",
     "Ariga"
    ]
   ],
   "title": "Coping with segmental–prosodic incongruity in spoken word recognition in Japanese",
   "original": "247",
   "order": 475,
   "page_count": 5,
   "abstract": [
    "A spoken word consists of two phonological cues: segments (phonemes) and prosody (e.g., lexical pitch accent). While both segments and prosody contribute to spoken word recognition, it is unknown how listeners interpret a spoken word when these cues contradict each other. For example, a word *tansu HLL is logically interpreted as both a segmental mispronunciation of dansu HLL &quot;dance&quot; and a prosodic mispronunciation of tansu LHH &quot;wardrobe&quot;. The present study investigated how Japanese listeners interpret those segmentally and prosodically incongruent words in spoken word recognition using a cross-modal repetition priming paradigm. When ISI = 0 ms, listeners&#x27; lexical decision was delayed more for prosodic mispronunciation (prime: *tansu HLL; target: tansu LHH &quot;wardrobe&quot;) than for segmental mispronunciation (prime: *dansu LHH; target: tansu LHH). In contrast, when ISI = 750 ms, listeners’ lexical decision was delayed more for segmental mispronunciation than for prosodic mispronunciation. These results demonstrate that lexical access to segmentally and prosodically incongruent words is a dynamic process, in which listeners first try a prosody-oriented interpretation and then shift to a segment-oriented interpretation."
   ],
   "p1": 2320,
   "pn": 2324,
   "doi": "10.21437/Interspeech.2025-247",
   "url": "interspeech_2025/ariga25_interspeech.html"
  },
  "oh25_interspeech": {
   "authors": [
    [
     "Sehyun",
     "Oh"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Multimodal and Multitask Learning for Predicting Multiple Scores in L2 English Speech",
   "original": "248",
   "order": 496,
   "page_count": 5,
   "abstract": [
    "This study presents a novel multimodal and multitask learning model for predicting five proficiency scores of L2 English speeches. The proposed approach integrates speech and text embeddings using multimodal transformer blocks with cross-modal attention to refine features dynamically between modalities, capturing complementary information. A joint loss function, combining MSE and a Trait-Aware (TA) loss, enhances the model by leveraging relationships among proficiency traits. Experiments with different combinations of four embeddings (MFCCs, GloVe, wav2vec 2.0, and BERT) revealed that the proposed model with wav2vec 2.0 and BERT embeddings achieved the best performance, with a mean PCC of 0.734 and a standard deviation of 0.0129 across five criteria. This approach significantly outperforms unimodal and baseline multimodal models, demonstrating the potential of advanced multimodal architectures and task-aware optimization in automated speech assessment systems."
   ],
   "p1": 2425,
   "pn": 2429,
   "doi": "10.21437/Interspeech.2025-248",
   "url": "interspeech_2025/oh25_interspeech.html"
  },
  "oh25b_interspeech": {
   "authors": [
    [
     "Sehyun",
     "Oh"
    ],
    [
     "Minhwa",
     "Chung"
    ],
    [
     "Sunhee",
     "Kim"
    ]
   ],
   "title": "Multilingual Speech Assessment Using Cross-Attention and Multitask Learning",
   "original": "249",
   "order": 1035,
   "page_count": 5,
   "abstract": [
    "Automatic speech assessment plays a vital role in language learning by providing essential feedback on pronunciation, fluency, and overall speaking ability. However, developing effective multilingual speech assessment systems poses significant challenges with the complexity of modeling multiple languages and limited availability of labeled data, especially for languages other than English. In this study, we propose a multilingual speech assessment system for three languages -- English, German, and French, which are produced by Korean learners. Enhanced by cross-attention and multitask learning mechanisms, our model utilizes pre-trained models to capture both language-specific and cross-linguistic features, predicting overall speaking proficiency scores directly from raw speech audio. Experimental results demonstrate that our proposed method, especially with wav2vec 2.0, presents superior performance on both seen and unseen data compared to monolingual models."
   ],
   "p1": 5073,
   "pn": 5077,
   "doi": "10.21437/Interspeech.2025-249",
   "url": "interspeech_2025/oh25b_interspeech.html"
  },
  "goswami25_interspeech": {
   "authors": [
    [
     "Nabarun",
     "Goswami"
    ],
    [
     "Tatsuya",
     "Harada"
    ]
   ],
   "title": "FUSE: Universal Speech Enhancement using Multi‐Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
   "original": "251",
   "order": 183,
   "page_count": 5,
   "abstract": [
    "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach."
   ],
   "p1": 883,
   "pn": 887,
   "doi": "10.21437/Interspeech.2025-251",
   "url": "interspeech_2025/goswami25_interspeech.html"
  },
  "li25b_interspeech": {
   "authors": [
    [
     "Zhipeng",
     "Li"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Jingyuan",
     "Xing"
    ],
    [
     "Hangrui",
     "Hu"
    ],
    [
     "Heng",
     "Lu"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "Long-Context Speech Synthesis with Context-Aware Memory",
   "original": "253",
   "order": 502,
   "page_count": 5,
   "abstract": [
    "In long-text speech synthesis, current approaches typically convert text to speech at the sentence-level and concatenate the results to form pseudo-paragraph-level speech. These methods overlook the contextual coherence of paragraphs, leading to reduced naturalness and inconsistencies in style and timbre across the long-form speech. To address these issues, we propose a Context-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The CAM block integrates and retrieves both long-term memory and local context details, enabling dynamic memory updates and transfers within long paragraphs to guide sentence-level speech synthesis. Furthermore, the prefix mask enhances the in-context learning ability by enabling bidirectional attention on prefix tokens while maintaining unidirectional generation. Experimental results demonstrate that the proposed method outperforms baseline and state-of-the-art long-context methods in terms of prosody expressiveness, coherence and context inference cost across paragraph-level speech."
   ],
   "p1": 2455,
   "pn": 2459,
   "doi": "10.21437/Interspeech.2025-253",
   "url": "interspeech_2025/li25b_interspeech.html"
  },
  "yang25c_interspeech": {
   "authors": [
    [
     "Xue",
     "Yang"
    ],
    [
     "Guiru",
     "Shen"
    ],
    [
     "Yu",
     "Yang"
    ]
   ],
   "title": "Cross-Attention-Based Target Sound Extraction by Fully Leveraging Enrollment in a Shared Latent Space",
   "original": "257",
   "order": 1018,
   "page_count": 5,
   "abstract": [
    "Target sound extraction aims to isolate the target sound event (SE) from a mixture containing undesired interference. The enrollment of the target SE class is often utilized to derive an embedding for guidance. However, this embedding is highly summarized and discards important details, resulting in suboptimal performance. To address this problem, the local dynamics and temporal structures of the enrollment are leveraged in this paper. Specifically, the enrollment and mixture are first processed separately along the frequency and time axes, mapping them into a shared latent space. The processed features are then interacted via a cross-attention mechanism, with the resulting feature serving as guidance. This approach effectively leverages the contextual information in the enrollment and allows for the extraction of SE class that are unseen during training. Experimental results show that our proposed method attains high performance and exhibits certain zero-shot generalization ability."
   ],
   "p1": 4988,
   "pn": 4992,
   "doi": "10.21437/Interspeech.2025-257",
   "url": "interspeech_2025/yang25c_interspeech.html"
  },
  "sung25_interspeech": {
   "authors": [
    [
     "Selina S.",
     "Sung"
    ],
    [
     "Seunghee",
     "Ha"
    ],
    [
     "Tae-Jin",
     "Yoon"
    ],
    [
     "Jungmin",
     "So"
    ]
   ],
   "title": "Multitask Learning with Fused Attention for Improved ASR and Mispronunciation Detection in Children's Speech Sound Disorders",
   "original": "259",
   "order": 1161,
   "page_count": 5,
   "abstract": [
    "This study proposes a multitask learning framework with fused attention to enhance automatic speech recognition (ASR) for pronunciation-based transcription and mispronunciation detection (MPD) in speech sound disorders (SSD). Our approach leverages multitask learning by carrying out ASR and classification tasks concurrently. To further improve performance, we propose a fused attention mechanism that refines hidden states by weighting features relevant to mispronunciations. The classification head and the attention mechanism work synergistically, jointly optimizing transcription and detection performance. Evaluated on a Korean children SSD dataset, our approach outperforms the baseline, achieving lower Character Error Rates (CER) and higher Unweighted Average Recall (UAR), demonstrating the effectiveness of multitask learning with fused attention for mispronunciation detection."
   ],
   "p1": 5698,
   "pn": 5702,
   "doi": "10.21437/Interspeech.2025-259",
   "url": "interspeech_2025/sung25_interspeech.html"
  },
  "halim25_interspeech": {
   "authors": [
    [
     "Jule Valendo",
     "Halim"
    ],
    [
     "Siyi",
     "Wang"
    ],
    [
     "Hong",
     "Jia"
    ],
    [
     "Ting",
     "Dang"
    ]
   ],
   "title": "Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition",
   "original": "261",
   "order": 1111,
   "page_count": 5,
   "abstract": [
    "Emotional intelligence in conversational AI is crucial across domains like human-computer interaction. While numerous models have been developed, they often overlook the complexity and ambiguity inherent in human emotions. In the era of large speech foundation models (SFMs), understanding their capability in recognizing ambiguous emotions is essential for the development of next-generation emotion-aware models. This study examines the effectiveness of SFMs in ambiguous emotion recognition. We designed prompts for ambiguous emotion prediction and introduced two novel approaches to infer ambiguous emotion distributions: one analysing generated text responses and the other examining the internal processing of SFMs through token-level logits. Our findings suggest that while SFMs may not consistently generate accurate text responses for ambiguous emotions, they can interpret such emotions at the token level based on prior knowledge, demonstrating robustness across different prompts."
   ],
   "p1": 5448,
   "pn": 5452,
   "doi": "10.21437/Interspeech.2025-261",
   "url": "interspeech_2025/halim25_interspeech.html"
  },
  "hannan25_interspeech": {
   "authors": [
    [
     "Abdul",
     "Hannan"
    ],
    [
     "Muhammad Arslan",
     "Manzoor"
    ],
    [
     "Shah",
     "Nawaz"
    ],
    [
     "Muhammad Irzam",
     "Liaqat"
    ],
    [
     "Markus",
     "Schedl"
    ],
    [
     "Mubashir",
     "Noman"
    ]
   ],
   "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association",
   "original": "268",
   "order": 553,
   "page_count": 5,
   "abstract": [
    "We study the task of learning association between faces and voices, which is gaining interest in the multimodal community lately. These methods suffer from the deliberate crafting of negative mining procedures as well as the reliance on the distant margin parameter. These issues are addressed by learning a joint embedding space in which orthogonality constraints are applied to the fused embeddings of faces and voices. However, embedding spaces of faces and voices possess different characteristics and require spaces to be aligned before fusing them. To this end, we propose a method that accurately aligns the embedding spaces and fuses them with an enhanced gated fusion thereby improving the performance of face-voice association. Extensive experiments on the VoxCeleb dataset reveals the merits of the proposed approach."
   ],
   "p1": 2710,
   "pn": 2714,
   "doi": "10.21437/Interspeech.2025-268",
   "url": "interspeech_2025/hannan25_interspeech.html"
  },
  "ankita25_interspeech": {
   "authors": [
    [
     "Ankita",
     "Ankita"
    ],
    [
     "Shambhavi",
     "Shambhavi"
    ],
    [
     "Syed",
     "Shahnawazuddin"
    ]
   ],
   "title": "On Enhancing the Performance of Children's ASR Task in Limited Data Scenario",
   "original": "273",
   "order": 577,
   "page_count": 5,
   "abstract": [
    "In this paper, we have documented our efforts towards developing a robust children&#x27;s automatic speech recognition (ASR) system in limited data scenario. At first, we have explored the effect of in-domain data augmentation so as to deal with limitations posed by data scarcity. This also helps in developing a competitive baseline ASR system. Next, we have studied the affect of modeling glottal activity parameters along with spectrum-based front-end acoustic features like the Mel-frequency cepstral coefficients (MFCC). Finally, the impact of feature normalization through feature-space maximum likelihood linear regression (fMLLR) is explored. As a consequence of applying fMLLR and then concatenating the normalized MFCC features with glottal activity parameters, a relative reduction in character error rate by 40% over the baseline is obtained."
   ],
   "p1": 2830,
   "pn": 2834,
   "doi": "10.21437/Interspeech.2025-273",
   "url": "interspeech_2025/ankita25_interspeech.html"
  },
  "luan25_interspeech": {
   "authors": [
    [
     "Kaixuan",
     "Luan"
    ],
    [
     "Xiaoda",
     "Yang"
    ],
    [
     "Shile",
     "Cai"
    ],
    [
     "Ruofan",
     "Hu"
    ],
    [
     "Minghui",
     "Fang"
    ],
    [
     "Wenrui",
     "Liu"
    ],
    [
     "Jialong",
     "Zuo"
    ],
    [
     "Jiaqi",
     "Duan"
    ],
    [
     "Yuhang",
     "Ma"
    ],
    [
     "Junyu",
     "Lu"
    ]
   ],
   "title": "MelRe: Vision-Based Mel-Spectrogram Restoration",
   "original": "274",
   "order": 781,
   "page_count": 5,
   "abstract": [
    "With advancements in visual technology, an increasing number of visual techniques have recently been applied in other fields. Among them, mel spectrograms provide a bridge between audio features and visual models. Previous work has demonstrated that applying image processing methods to mel spectrograms is feasible. However, traditional image-based models operate at a relatively coarse level, focusing primarily on controlling texture and shape. In contrast, mel spectrograms are highly sensitive to detail, containing complex time-frequency information that requires more refined modeling. To address this, we propose MelRe, a visual model specifically designed for mel spectrograms, aimed at tackling complex fine-grained audio degradation issues from a visual perspective. MelRe addresses the need for fine-grained detail through pixel-level restoration methods and employs degradation alignment and noise simulation strategies to achieve high-precision restoration across varying levels of degradation, demonstrating exceptional restoration performance. Experimental results show that MelRe achieves a new state-of-the-art (SOTA) level in complex audio restoration tasks, highlighting its potential for high-quality audio repair."
   ],
   "p1": 3823,
   "pn": 3827,
   "doi": "10.21437/Interspeech.2025-274",
   "url": "interspeech_2025/luan25_interspeech.html"
  },
  "murata25_interspeech": {
   "authors": [
    [
     "Masato",
     "Murata"
    ],
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Tomoki",
     "Koriyama"
    ]
   ],
   "title": "Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity Control",
   "original": "276",
   "order": 893,
   "page_count": 5,
   "abstract": [
    "Cross-speaker emotion intensity control aims to generate emotional speech of a target speaker with desired emotion intensities using only their neutral speech. A recently proposed method, emotion arithmetic, achieves emotion intensity control using a single-speaker emotion vector. Although this prior method has shown promising results in the same-speaker setting, it lost speaker consistency in the cross-speaker setting due to mismatches between the emotion vector of the source and target speakers. To overcome this limitation, we propose a speaker-agnostic emotion vector designed to capture shared emotional expressions across multiple speakers. This speaker-agnostic emotion vector is applicable to arbitrary speakers. Experimental results demonstrate that the proposed method succeeds in cross-speaker emotion intensity control while maintaining speaker consistency, speech quality, and controllability, even in the unseen speaker case."
   ],
   "p1": 4383,
   "pn": 4387,
   "doi": "10.21437/Interspeech.2025-276",
   "url": "interspeech_2025/murata25_interspeech.html"
  },
  "murata25b_interspeech": {
   "authors": [
    [
     "Masato",
     "Murata"
    ],
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Eigenvoice Synthesis based on Model Editing for Speaker Generation",
   "original": "277",
   "order": 1126,
   "page_count": 5,
   "abstract": [
    "Speaker generation task aims to create unseen speaker voice without reference speech. The key to the task is defining a speaker space that represents diverse speakers to determine the generated speaker trait. However, the effective way to define this speaker space remains unclear. Eigenvoice synthesis is one of the promising approaches in the traditional parametric synthesis framework, such as HMM-based methods, which define a low-dimensional speaker space using pre-stored speaker features. This study proposes a novel DNN-based eigenvoice synthesis method via model editing. Unlike prior methods, our method defines a speaker space in the DNN model parameter space. By directly sampling new DNN model parameters in this space, we can create diverse speaker voices. Experimental results showed the capability of our method to generate diverse speakers&#x27; speech. Moreover, we discovered a gender-dominant axis in the created speaker space, indicating the potential to control speaker attributes."
   ],
   "p1": 5523,
   "pn": 5527,
   "doi": "10.21437/Interspeech.2025-277",
   "url": "interspeech_2025/murata25b_interspeech.html"
  },
  "liang25_interspeech": {
   "authors": [
    [
     "Wenrui",
     "Liang"
    ],
    [
     "Rong",
     "Zhang"
    ],
    [
     "Xuezhen",
     "Zhang"
    ],
    [
     "Ying",
     "Ma"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ]
   ],
   "title": "DepressGEN: Synthetic Data Generation Framework for Depression Detection",
   "original": "280",
   "order": 99,
   "page_count": 5,
   "abstract": [
    "Automated depression detection is vital for early diagnosis, but ethical and privacy concerns often limit the availability of sufficient training data, hindering research in depression screening. To address this, we introduce DepressGEN, a novel framework that generates synthetic interview dialogue texts and speech simulating depressed patients to improve training for detection models. By inputting linguistic features associated with depression into a large language model, we create dialogue texts and use a TTS system to generate corresponding speech. We also developed a depression modulation module to modify the synthesized speech, as well as a speech verification module to bridge the gap between synthetic and real data distributions. Our results demonstrate that a GRU/BiLSTM-based model trained with additional synthetic data improves F1 scores by 9.9% compared to the same model trained only on original data, outperforming existing methods on the EATD dataset."
   ],
   "p1": 464,
   "pn": 468,
   "doi": "10.21437/Interspeech.2025-280",
   "url": "interspeech_2025/liang25_interspeech.html"
  },
  "hermes25_interspeech": {
   "authors": [
    [
     "Anne",
     "Hermes"
    ],
    [
     "Ivana",
     "Didirková"
    ],
    [
     "Philipp",
     "Buech"
    ],
    [
     "Gilles",
     "Vannuscorps"
    ]
   ],
   "title": "Acoustic similarities, articulatory uniqueness: Speech production mechanisms in individuals with congenital lip paralysis",
   "original": "281",
   "order": 762,
   "page_count": 5,
   "abstract": [
    "Moebius syndrome is a rare congenital neuromuscular disorder characterized by facial paralysis, which can severely impact speech. Despite these constraints, some individuals with congenital lip paralysis develop very natural speech intelligibility by employing strategies that remain underexplored. This study examines the acoustic-articulatory adaptations in three French-speaking individuals with Moebius syndrome compared to three controls. Using articulatory and acoustic data, we analyzed speech patterns in disyllabic words. Results indicate that Moebius speakers achieve acoustically similar vowel targets to controls despite significant articulatory differences, including increased tongue movement to compensate for lip paralysis. These findings highlight the adaptability of the speech motor system, supporting the notion of motor equivalence in achieving intelligible speech."
   ],
   "p1": 3728,
   "pn": 3732,
   "doi": "10.21437/Interspeech.2025-281",
   "url": "interspeech_2025/hermes25_interspeech.html"
  },
  "wang25c_interspeech": {
   "authors": [
    [
     "Xuying",
     "Wang"
    ],
    [
     "Fang",
     "Hu"
    ]
   ],
   "title": "On Apical Vowels in Eastern Zhenjiang Mandarin",
   "original": "282",
   "order": 968,
   "page_count": 5,
   "abstract": [
    "There is generally no distributional constraint for the apical vowels in Eastern Zhenjiang Mandarin, and the apical vowels contrast with the high front vowels when preceded by an initial consonant [p ph m t th n l ʨ ʨh ɕ] or occurring in an onset-less syllable. This paper analyzes acoustic characteristics and lingual articulation in the production of the apical vowels [ɿ ɥ] in Eastern Zhenjiang Mandarin. The results show that although accompanied by frication noise, the apical vowels [ɿ ɥ] have their own spectral properties that are distinctive to the high front vowel [i y]. The ultrasound data reveal that both the anterior and posterior parts of the tongue are involved in the production of [ɿ ɥ]."
   ],
   "p1": 4758,
   "pn": 4762,
   "doi": "10.21437/Interspeech.2025-282",
   "url": "interspeech_2025/wang25c_interspeech.html"
  },
  "du25_interspeech": {
   "authors": [
    [
     "Changhong",
     "Du"
    ],
    [
     "Fang",
     "Hu"
    ]
   ],
   "title": "Tonal Contrasts in the Malipo Variety of the Mienic Language",
   "original": "283",
   "order": 150,
   "page_count": 4,
   "abstract": [
    "This paper describes tonal contrasts in the Malipo variety of the Mienic language and explores how they are manifested by acoustic cues. Results show that Linear Mixed-Effects Models based on Growth Curve Analysis successfully characterizes the 5 level tones, 3 falling tones, 3 rising or concave tones, and 2 convex tones in the Malipo variety. In addition to F0, duration also plays a role in a complex tonal system."
   ],
   "p1": 719,
   "pn": 722,
   "doi": "10.21437/Interspeech.2025-283",
   "url": "interspeech_2025/du25_interspeech.html"
  },
  "zhang25b_interspeech": {
   "authors": [
    [
     "Zhenrui",
     "Zhang"
    ],
    [
     "Fang",
     "Hu"
    ]
   ],
   "title": "Tonal Perception in Changde Mandarin",
   "original": "284",
   "order": 149,
   "page_count": 5,
   "abstract": [
    "This paper explores perceptual cues of the four tones in Changde Mandarin. Results show that T1 is perceived as a high-level tone, T2 is a low-rising tone in both production and perception, T3 is not phonologically level but falling, and the perception of T4 is influenced by various factors. And results also show that tonal perception might not be categorical, since neither the T2-T3 continuum nor the T2-T4 continuum fulfills the standards for typical categorical perception."
   ],
   "p1": 714,
   "pn": 718,
   "doi": "10.21437/Interspeech.2025-284",
   "url": "interspeech_2025/zhang25b_interspeech.html"
  },
  "saladukha25_interspeech": {
   "authors": [
    [
     "Dzmitry",
     "Saladukha"
    ],
    [
     "Ivan",
     "Koriabkin"
    ],
    [
     "Kanstantsin",
     "Artsiom"
    ],
    [
     "Aliaksei",
     "Rak"
    ],
    [
     "Nikita",
     "Ryzhikov"
    ]
   ],
   "title": "Multichannel Keyword Spotting for Noisy Conditions",
   "original": "285",
   "order": 545,
   "page_count": 5,
   "abstract": [
    "This article presents a method for improving a keyword spotter (KWS) algorithm in noisy environments. Although beamforming (BF) and adaptive noise cancellation (ANC) techniques are robust in some conditions, they may degrade the performance of the activation system by distorting or suppressing useful signals. The authors propose a neural network architecture that uses several input channels and an attention mechanism that allows the network to determine the most useful channel or their combination. The improved quality of the algorithm was demonstrated on two datasets: from a laboratory with controlled conditions and from smart speakers in natural conditions. The proposed algorithm was compared against several baselines in terms of the quality of noise reduction metrics, KWS metrics, and computing resources in comparison with existing solutions."
   ],
   "p1": 2670,
   "pn": 2674,
   "doi": "10.21437/Interspeech.2025-285",
   "url": "interspeech_2025/saladukha25_interspeech.html"
  },
  "liu25b_interspeech": {
   "authors": [
    [
     "Zhe",
     "Liu"
    ]
   ],
   "title": "Unlearning LLM-Based Speech Recognition Models",
   "original": "287",
   "order": 654,
   "page_count": 5,
   "abstract": [
    "Recent advances in Large Language Models (LLMs) have been extended to the field of Automatic Speech Recognition (ASR), leveraging the exceptional generative capabilities of LLMs for speech transcription generation. However, concerns about privacy risks have emerged since both LLMs and large ASR models may unintentionally memorize training examples. After deploying a model, practitioners might be asked to remove any personal data from the model. Re-training the original model every time of these requests is computationally expensive. In this work, we provide the first investigation into the measurement and detection of unintended memorization in LLM-based ASR models and present an efficient solution for unlearning memorized data upon requests. Experiments on the LibriSpeech dataset show that the proposed method is effective and achieves strong privacy-utility trade-offs."
   ],
   "p1": 3214,
   "pn": 3218,
   "doi": "10.21437/Interspeech.2025-287",
   "url": "interspeech_2025/liu25b_interspeech.html"
  },
  "bolanos25_interspeech": {
   "authors": [
    [
     "Cecilia",
     "Bolaños"
    ],
    [
     "Leonardo",
     "Pepino"
    ],
    [
     "Martin",
     "Meza"
    ],
    [
     "Luciana",
     "Ferrer"
    ]
   ],
   "title": "Benchmarking Time-localized Explanations for Audio Classification Models",
   "original": "288",
   "order": 44,
   "page_count": 5,
   "abstract": [
    "Most modern approaches for audio processing are opaque, in the sense that they do not provide an explanation for their decisions. For this reason, various methods have been proposed to explain the outputs generated by these models. Good explanations can result in interesting insights about the data or the model, as well as increase trust in the system. Unfortunately, evaluating the quality of explanations is far from trivial since, for most tasks, there is no clear ground truth explanation to use as reference. In this work, we propose a benchmark for time-localized explanations for audio classification models that uses time annotations of target events as a proxy for ground truth explanations. We use this benchmark to systematically optimize and compare various approaches for model-agnostic post-hoc explanation, obtaining, in some cases, close to perfect explanations. Finally, we illustrate the utility of the explanations for uncovering spurious correlations."
   ],
   "p1": 211,
   "pn": 215,
   "doi": "10.21437/Interspeech.2025-288",
   "url": "interspeech_2025/bolanos25_interspeech.html"
  },
  "tao25_interspeech": {
   "authors": [
    [
     "Liang",
     "Tao"
    ],
    [
     "Maoshen",
     "Jia"
    ],
    [
     "Yonggang",
     "Hu"
    ]
   ],
   "title": "Direct-path Relative Harmonic Coefficients Detection for Multi-source Direction-of-Arrival Estimation in Reverberant Environments",
   "original": "296",
   "order": 512,
   "page_count": 5,
   "abstract": [
    "The identification of direct-path recordings for direction-of-arrival (DOA) estimation has been recognized as a crucial factor in enhancing the localization robustness under reverberant environments, especially for the challenging cases involving multiple acoustic sources. In particular, the recently introduced spatial feature denoted relative harmonic coefficients (RHC) has been extensively investigated for DOA estimation due to a strong association with spatial cues. Building upon this, this paper presents a multi-source localization method that utilizes higher-order RHC vector to detect direct-path dominant (DPD) bins in the time-frequency (TF) domain. Specifically, DPD bins are effectively identified by applying an orthogonal projection matrix (OPM) to the RHC vector. Moreover, we integrate the OPM with a closed-form single source DOA estimator, thus circumventing the exhaustive search over a dense spatial grid. Extensive experimental evaluations using both simulated and real-world recordings confirm the proposed algorithm’s effectiveness, and superiority over baseline methods."
   ],
   "p1": 2505,
   "pn": 2509,
   "doi": "10.21437/Interspeech.2025-296",
   "url": "interspeech_2025/tao25_interspeech.html"
  },
  "liang25b_interspeech": {
   "authors": [
    [
     "Liming",
     "Liang"
    ],
    [
     "Luo",
     "Chen"
    ],
    [
     "Yuehan",
     "Jin"
    ],
    [
     "Xianwei",
     "Zhuang"
    ],
    [
     "Yuxin",
     "Xie"
    ],
    [
     "Yongkang",
     "Yin"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "FoleyMaster: High-Quality Video-to-Audio Synthesis via MLLM-Augmented Prompt Tuning and Joint Semantic-Temporal Adaptation",
   "original": "300",
   "order": 860,
   "page_count": 5,
   "abstract": [
    "We study video-to-audio (V2A) generation, a critical task for automatically creating high-quality sound effects synchronized with silent video. Current V2A methods face three limitations: (1) inadequate textual annotations in existing datasets, (2) over-reliance on global video features, and (3) coarse temporal synchronization. To address these, We propose FoleyMaster with three key innovations: 1) We introduce VGGSound Plus dataset with 197,955 videos annotated by Qwen2-VL-7B for fine-grained event descriptions; 2) We develop a cross-attention semantic adapter integrating token-level text embeddings with global video features via prompt learning, enabling precise alignment between visual events and sound; 3) We develop a probabilistic temporal adapter that adjusts audio generation based on action prominence replacing binary synchronization. Extensive experiments demonstrate that FoleyMaster achieves state-of-the-art V2A performance across all metrics. Demo and dataset are available."
   ],
   "p1": 4218,
   "pn": 4222,
   "doi": "10.21437/Interspeech.2025-300",
   "url": "interspeech_2025/liang25b_interspeech.html"
  },
  "chen25d_interspeech": {
   "authors": [
    [
     "Wei",
     "Chen"
    ],
    [
     "Binzhu",
     "Sha"
    ],
    [
     "Dan",
     "Luo"
    ],
    [
     "Jing",
     "Yang"
    ],
    [
     "Zhuo",
     "Wang"
    ],
    [
     "Fan",
     "Fan"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching",
   "original": "305",
   "order": 259,
   "page_count": 5,
   "abstract": [
    "Singing Voice Conversion (SVC) transfers a source singer&#x27;s timbre to a target while keeping melody and lyrics. The key challenge in any-to-any SVC is adapting unseen speaker timbres to source audio without quality degradation. Existing methods either face timbre leakage or fail to achieve satisfactory timbre similarity and quality in the generated audio. To address these challenges, we propose DAFMSVC, where the self-supervised learning (SSL) features from the source audio are replaced with the most similar SSL features from the target audio to prevent timbre leakage. It also incorporates a dual-cross-attention mechanism for the adaptive fusion of speaker embeddings, melody, and linguistic content. Additionally, we introduce a flow matching module for high-quality audio generation from the fused features. Experimental results show that DAFMSVC significantly enhances timbre similarity and naturalness, outperforming state-of-the-art methods in both subjective and objective evaluations."
   ],
   "p1": 1263,
   "pn": 1267,
   "doi": "10.21437/Interspeech.2025-305",
   "url": "interspeech_2025/chen25d_interspeech.html"
  },
  "harmsen25_interspeech": {
   "authors": [
    [
     "Wieke",
     "Harmsen"
    ],
    [
     "Roeland",
     "van Hout"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Can ASR generate valid measures of child reading fluency?",
   "original": "306",
   "order": 490,
   "page_count": 5,
   "abstract": [
    "Early diagnosis of reading difficulties requires assessment of children’s oral reading fluency. Since obtaining reliable assessments through subjective rating rubrics has proven to be difficult and time consuming, the current study presents a novel procedure incorporating Automatic Speech Recognition (ASR) to compute 15 measures related to the aspects phrasing, smoothness and pacing of oral reading fluency. We investigate the validity of these measures by comparing them to the same measures computed on human transcripts. This comparison was performed on a dataset of 244 recordings of texts read by 131 Dutch primary school children (aged 6-13 years). We found strong correlations for 12 out of 15 measures, which emphasizes the great potential of these measures for more reliable, less time-consuming and sustainable reading assessment."
   ],
   "p1": 2395,
   "pn": 2399,
   "doi": "10.21437/Interspeech.2025-306",
   "url": "interspeech_2025/harmsen25_interspeech.html"
  },
  "sun25_interspeech": {
   "authors": [
    [
     "Siqi",
     "Sun"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Acquiring Pronunciation from Speech Audio via Multi-task Learning",
   "original": "308",
   "order": 466,
   "page_count": 5,
   "abstract": [
    "Recent work has shown the benefit of bootstrapping an integrated sequence-to-sequence (Seq2Seq) linguistic frontend from a traditional pipeline-based frontend for text-to-speech (TTS). To overcome the fixed lexical coverage of bootstrapping training data, previous work has proposed to leverage easily accessible transcribed speech audio as an additional training source for acquiring novel pronunciation knowledge for uncovered words, which relies on cumbersome auxiliary acoustic model training and explicit pronunciation decoding. In this work, we propose an alternative method to leverage transcribed speech audio as an additional training source, based on multi-task learning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend, the proposed MTL-based method improves overall accuracy by 3.7% absolute for those word types covered exclusively in transcribed speech audio, achieving a similar performance to the previous method but with a much simpler implementation flow."
   ],
   "p1": 2275,
   "pn": 2279,
   "doi": "10.21437/Interspeech.2025-308",
   "url": "interspeech_2025/sun25_interspeech.html"
  },
  "kando25_interspeech": {
   "authors": [
    [
     "Shunsuke",
     "Kando"
    ],
    [
     "Yusuke",
     "Miyao"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ]
   ],
   "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
   "original": "310",
   "order": 1167,
   "page_count": 5,
   "abstract": [
    "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding."
   ],
   "p1": 5728,
   "pn": 5732,
   "doi": "10.21437/Interspeech.2025-310",
   "url": "interspeech_2025/kando25_interspeech.html"
  },
  "huang25_interspeech": {
   "authors": [
    [
     "Jiawen",
     "Huang"
    ],
    [
     "Felipe",
     "Sousa"
    ],
    [
     "Emir",
     "Demirel"
    ],
    [
     "Emmanouil",
     "Benetos"
    ],
    [
     "Igor",
     "Gadelha"
    ]
   ],
   "title": "Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss",
   "original": "311",
   "order": 627,
   "page_count": 5,
   "abstract": [
    "Automatic Lyrics Transcription (ALT) aims to recognize lyrics from singing voices, similar to Automatic Speech Recognition (ASR) for spoken language, but faces added complexity due to domain-specific properties of the singing voice. While foundation ASR models show robustness in various speech tasks, their performance degrades on singing voice, especially in the presence of musical accompaniment. This work focuses on this performance gap and explores Low-Rank Adaptation (LoRA) for ALT, investigating both single-domain and dual-domain fine-tuning strategies. We propose using a consistency loss to better align vocal and mixture encoder representations, improving transcription on mixture without relying on singing voice separation. Our results show that while naïve dual-domain fine-tuning underperforms, structured training with consistency loss yields modest but consistent gains, demonstrating the potential of adapting ASR foundation models for music."
   ],
   "p1": 3080,
   "pn": 3084,
   "doi": "10.21437/Interspeech.2025-311",
   "url": "interspeech_2025/huang25_interspeech.html"
  },
  "peng25_interspeech": {
   "authors": [
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Yu",
     "Guan"
    ],
    [
     "Lipeng",
     "Dai"
    ],
    [
     "Zuoliang",
     "Li"
    ]
   ],
   "title": "Parameter-Efficient Fine-tuning with Instance-Aware Prompt and Parallel Adapters for Speaker Verification",
   "original": "312",
   "order": 335,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a parameter-efficient fine-tuning method to tailor a pre-trained model for speaker verification. The proposed method simultaneously considers adapter tuning and prompt tuning in one framework. Instead of conventional static prompts, we first insert a prompt generator between two neighboring transformer layers of the pre-trained model, which can incorporate utterance-specific clues to dynamically generate instance-aware prompts. Meanwhile, we append parallel adapter branches to the multi-head attention and feed-forward modules in the transformer layers in order to capture speaker-related information. Experimental results on the VoxCeleb datasets demonstrate the superiority of our method in case of updating fewer than 10% of the parameters."
   ],
   "p1": 1643,
   "pn": 1647,
   "doi": "10.21437/Interspeech.2025-312",
   "url": "interspeech_2025/peng25_interspeech.html"
  },
  "niebuhr25_interspeech": {
   "authors": [
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "On the cross-modal makeup of charisma: Insights from a field-data analysis",
   "original": "313",
   "order": 926,
   "page_count": 5,
   "abstract": [
    "This study uses expert ratings and prosodic analyses of the most popular 2025 video speeches of the German DAX-40 CEOs to investigate the interplay of visual, verbal, and prosodic factors in perceiving speaker charisma. Results show strong correlations between all factors, point to a special role of prosody, and provide initial evidence for industry-specific speaking styles."
   ],
   "p1": 4548,
   "pn": 4552,
   "doi": "10.21437/Interspeech.2025-313",
   "url": "interspeech_2025/niebuhr25_interspeech.html"
  },
  "kim25e_interspeech": {
   "authors": [
    [
     "Minyoung",
     "Kim"
    ],
    [
     "Sehwan",
     "Park"
    ],
    [
     "Sungmin",
     "Cha"
    ],
    [
     "Paul Hongsuck",
     "Seo"
    ]
   ],
   "title": "Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries",
   "original": "316",
   "order": 1041,
   "page_count": 5,
   "abstract": [
    "Recent advances in voice cloning and lip synchronization models have enabled Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are manipulated to mimic a target speaker. This significantly increases the risk of misinformation by making fake content seem real. To address this issue, existing methods detect or localize manipulations but cannot recover the authentic audio that conveys the semantic content of the message. This limitation reduces their effectiveness in combating audiovisual misinformation. In this work, we introduce the task of Authentic Audio Recovery (AAR) and Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal watermarking framework to embed authentic audio into visuals before manipulation. This enables AAR, TLA, and a robust defense against misinformation. Extensive experiments demonstrate the strong performance of our method in AAR and TLA against various manipulations, including voice cloning and lip synchronization."
   ],
   "p1": 5103,
   "pn": 5107,
   "doi": "10.21437/Interspeech.2025-316",
   "url": "interspeech_2025/kim25e_interspeech.html"
  },
  "higuchi25_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Higuchi"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "End-to-End Speech Translation Guided by Robust Translation Capability of Large Language Model",
   "original": "317",
   "order": 6,
   "page_count": 5,
   "abstract": [
    "We present an end-to-end speech translation (ST) model that uses a large language model (LLM) to guide the translation process. Recent advances in LLMs have shown strong contextual understanding and robustness to noisy text, making them beneficial for mitigating automatic speech recognition (ASR) errors. Building on these strengths, we develop an LLM-driven ST model within an encoder-decoder framework, with the encoder handling an auxiliary ASR task and the decoder incorporating an LLM at its front end. Here, the encoder generates an ASR hypothesis that cues the LLM to perform machine translation. The LLM output is then fed into the decoder to yield the final translation. This two-pass design capitalizes on the LLM&#x27;s robust and accurate translation capabilities, while enabling end-to-end optimization tailored to specific ST tasks. Experimental results on various ST tasks reveal significant performance gains with our LLM integration, and extensive analyses further validate our approach."
   ],
   "p1": 21,
   "pn": 25,
   "doi": "10.21437/Interspeech.2025-317",
   "url": "interspeech_2025/higuchi25_interspeech.html"
  },
  "blaschke25_interspeech": {
   "authors": [
    [
     "Verena",
     "Blaschke"
    ],
    [
     "Miriam",
     "Winkler"
    ],
    [
     "Constantin",
     "Förster"
    ],
    [
     "Gabriele",
     "Wenger-Glemser"
    ],
    [
     "Barbara",
     "Plank"
    ]
   ],
   "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation",
   "original": "318",
   "order": 189,
   "page_count": 5,
   "abstract": [
    "Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions."
   ],
   "p1": 913,
   "pn": 917,
   "doi": "10.21437/Interspeech.2025-318",
   "url": "interspeech_2025/blaschke25_interspeech.html"
  },
  "lu25b_interspeech": {
   "authors": [
    [
     "Ye-Xin",
     "Lu"
    ],
    [
     "Hui-Peng",
     "Du"
    ],
    [
     "Fei",
     "Liu"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
   "original": "319",
   "order": 504,
   "page_count": 5,
   "abstract": [
    "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models."
   ],
   "p1": 2465,
   "pn": 2469,
   "doi": "10.21437/Interspeech.2025-319",
   "url": "interspeech_2025/lu25b_interspeech.html"
  },
  "yu25_interspeech": {
   "authors": [
    [
     "En-Lun",
     "Yu"
    ],
    [
     "Chien-Chun",
     "Wang"
    ],
    [
     "Jeih-Weih",
     "Hung"
    ],
    [
     "Shih-Chieh",
     "Huang"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Flexible VAD-PVAD Transition: A Detachable PVAD Module for Dynamic Encoder RNN VAD",
   "original": "322",
   "order": 1180,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose Flexible Dynamic Encoder RNN (FDE-RNN), an innovative model capable of seamlessly switching between VAD and PVAD without incurring redundant resource consumption. In static PVAD modeling, performing VAD typically requires either merging categories or omitting speaker embeddings, often resulting in excessively large models that are impractical for VAD tasks. In contrast, FDERNN efficiently adapts by removing the personalization module when functioning as VAD, significantly reducing resource demands. Furthermore, on PVAD tasks, FDE-RNN leverages dynamic neural networks with a gating-based skipping mechanism, enabling it to bypass redundant computations during non-speech segments, further optimizing computational efficiency. Extensive experiments demonstrate that FDE-RNN outperforms all other prior arts on both PVAD and VAD tasks in terms of overall performance. Notably, when functioning as a VAD, FDE-RNN merely utilizes 30% of the parameters required by the competitive models, underscoring its remarkable efficiency and scalability."
   ],
   "p1": 5793,
   "pn": 5797,
   "doi": "10.21437/Interspeech.2025-322",
   "url": "interspeech_2025/yu25_interspeech.html"
  },
  "kuan25_interspeech": {
   "authors": [
    [
     "Chun-Yi",
     "Kuan"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples ",
   "original": "324",
   "order": 422,
   "page_count": 5,
   "abstract": [
    "Recent advancements in audio-aware large language models (ALLMs) enable them to process and understand audio inputs. However, these models often hallucinate non-existent sound events, reducing their reliability in real-world applications. To address this, we propose LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method that enhances ALLMs’ ability to distinguish between present and absent sounds using synthesized data from the backbone LLM. Unlike prior approaches, our method requires no modification to LLM parameters and efficiently integrates audio representations via a lightweight adapter. Experiments show that LISTEN effectively mitigates hallucinations while maintaining impressive performance on existing audio question and reasoning benchmarks. At the same time, it is more efficient in both data and computation."
   ],
   "p1": 2073,
   "pn": 2077,
   "doi": "10.21437/Interspeech.2025-324",
   "url": "interspeech_2025/kuan25_interspeech.html"
  },
  "gong25_interspeech": {
   "authors": [
    [
     "Xun",
     "Gong"
    ],
    [
     "Anqi",
     "Lv"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Zhiming",
     "Wang"
    ],
    [
     "Huijia",
     "Zhu"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual Biasing ASR in Speech LLM",
   "original": "326",
   "order": 825,
   "page_count": 5,
   "abstract": [
    "While speech large language models (SpeechLLMs) have advanced standard automatic speech recognition (ASR), contextual biasing for named entities and rare words remains challenging, especially at scale. To address this, we propose BR-ASR: a Bias Retrieval framework for large-scale contextual biasing (up to 200k entries) via two innovations: (1) speech-and-bias contrastive learning to retrieve semantically relevant candidates; (2) dynamic curriculum learning that mitigates homophone confusion which negatively impacts the final performance. The is a general framework that allows seamless integration of the retrieved candidates into diverse ASR systems without finetuning. Experiments on LibriSpeech test-clean/-other achieve state-of-the-art (SOTA) biased word error rates (B-WER) of 2.8%/7.1% with 2000 bias words, delivering 45% relative improvement over prior methods. BR-ASR also demonstrates high scalability: when expanding the bias list to 200k where traditional methods generally fail, it induces only 0.3 / 2.9% absolute WER/ B-WER degradation with a 99.99% pruning rate and only 20ms latency per query on test-other."
   ],
   "p1": 4043,
   "pn": 4047,
   "doi": "10.21437/Interspeech.2025-326",
   "url": "interspeech_2025/gong25_interspeech.html"
  },
  "novitasari25_interspeech": {
   "authors": [
    [
     "Sashi",
     "Novitasari"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Gakuto",
     "Kurata"
    ]
   ],
   "title": "Voice Activity-based Text Segmentation for ASR Text Denormalization ",
   "original": "328",
   "order": 750,
   "page_count": 5,
   "abstract": [
    "We introduce a novel technique for text capitalization and punctuation recovery (CP) systems that learn from voice-activity cues to effectively enhance the output readability of E2E ASR. Commonly E2E ASR systems produce uncapitalized text with no punctuation marks. In such situations, CP systems are introduced as external modules to denormalize the ASR output; however, they suffer from performance degradation due to the difference between the text segmentation used to construct them and those resulting from ASR. ASR systems generally produce decoded text of input speech segments determined by a VAD algorithm, while CP systems are often constructed on grammatically well-segmented full-sentence text. To reduce this gap, we construct a CP system by using pseudo VAD-segmented text given by a text segmentation model designed using voice activity cues. Our method reduces false predictions by 4.5%-18.9% compared with the baseline while appropriately formatting the ASR texts."
   ],
   "p1": 3668,
   "pn": 3672,
   "doi": "10.21437/Interspeech.2025-328",
   "url": "interspeech_2025/novitasari25_interspeech.html"
  },
  "you25_interspeech": {
   "authors": [
    [
     "Jiajun",
     "You"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Xun",
     "Gong"
    ],
    [
     "Xiang",
     "Wan"
    ]
   ],
   "title": "M3L: A Multi-Modal and Multi-Lingual Depression Detection Framework",
   "original": "329",
   "order": 1074,
   "page_count": 5,
   "abstract": [
    "Early diagnosis are essential to reduce costs and improve treatment efficiency. Recently, automatic depression detection (ADD) based on audio and textual features from participant interviews has emerged as a promising approach, attracting significant attention. However, existing models are constrained to monolingual depression datasets, with limited exploration of multi-lingual scenarios. To investigate the effectiveness of multi-lingual data for the ADD task and its transferability in low-resource scenarios, in this paper, we propose a Multi-Modal Multi-Lingual (M3L) depression detection framework and an effective language adaptive fine-tuning (LAFT) to further boost the performance on the target language. M3L utilizes the pretrained speech model Whisper and the text model XLM-RoBERTa to enhance the encoding of multilingual information. Evaluations on the DAIC-WOZ (English) and EATD (Chinese) datasets demonstrate that M3L effectively integrates multi-lingual and multi-modal information, while the proposed LAFT consistently boosts performance across both datasets."
   ],
   "p1": 5268,
   "pn": 5272,
   "doi": "10.21437/Interspeech.2025-329",
   "url": "interspeech_2025/you25_interspeech.html"
  },
  "novitasari25b_interspeech": {
   "authors": [
    [
     "Sashi",
     "Novitasari"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Gakuto",
     "Kurata"
    ]
   ],
   "title": "Improving End-to-end Mixed-case ASR with Knowledge Distillation and Integration of Voice Activity Cues ",
   "original": "330",
   "order": 678,
   "page_count": 5,
   "abstract": [
    "E2E mixed-case (MC) ASR is a more challenging task than unicase (UC) ASR because of the necessity of capitalizing and punctuating the decoded outputs simultaneously. MC models that are simply trained on formatted transcriptions often suffer from various negative impacts, notably a degradation in case-and-punctuation-insensitive performance due to the increased learning complexity. In this paper, we describe novel techniques for training E2E MC ASR models and use them to improve casing-and-punctuation sensitive and insensitive performance. Our approach incorporates knowledge distillation from UC teacher to MC student models not only to improve capitalization and punctuation accuracy but also to maximize phone classification capability in MC ASR. Furthermore, we attempt to integrate voice activity cues into MC ASR to support text formatting tasks. Our method significantly reduces errors by up to 9.2% relative to baseline models that operate at a similar decoding cost."
   ],
   "p1": 3334,
   "pn": 3338,
   "doi": "10.21437/Interspeech.2025-330",
   "url": "interspeech_2025/novitasari25b_interspeech.html"
  },
  "singh25_interspeech": {
   "authors": [
    [
     "Prabhav",
     "Singh"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Count Your Speakers! Multitask Learning for Multimodal Speaker Diarization",
   "original": "334",
   "order": 324,
   "page_count": 5,
   "abstract": [
    "Recent advances in speaker diarization have explored diverse clustering methods, particularly in multimodal frameworks. However, a critical limitation lies in the clustering stage, where heuristic-based methods often fail to leverage the full potential of multimodal data. For example, threshold-based clustering frequently leads to over-clustering, causing incorrect speaker assignments and elevated DER. To address this, we propose CYS-MSD, a novel framework that fuses audio-visual modalities via a trainable cross-modal attention mechanism. The embeddings are fine-tuned with a multitask objective to jointly predict speaker counts and assign speaker labels, enabling data-driven clustering that adapts to varying speaker scenarios. Additionally, a modality-masking mechanism ensures robustness to missing inputs in real-world conditions. We evaluate CYS-MSD on the AVA-AVD corpus, reporting a 5% reduction in DER over the baseline and an average 2% reduction compared to various SOTA systems."
   ],
   "p1": 1588,
   "pn": 1592,
   "doi": "10.21437/Interspeech.2025-334",
   "url": "interspeech_2025/singh25_interspeech.html"
  },
  "benway25_interspeech": {
   "authors": [
    [
     "Nina R",
     "Benway"
    ],
    [
     "Saba",
     "Tabatabaee"
    ],
    [
     "Benjamin",
     "Munson"
    ],
    [
     "Jonathan",
     "Preston"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Subtyping Speech Errors in Childhood Speech Sound Disorders with Acoustic-to-Articulatory Speech Inversion",
   "original": "339",
   "order": 571,
   "page_count": 5,
   "abstract": [
    "Speech inversion holds much potential to describe speech errors in childhood speech sound disorders. However, the clinical interpretability of speech inversion tract variables is unknown. This study is the first to show, through linear mixed modeling, that acoustic-to-articulatory speech inversion can quantify statistically significant articulatory differences between several perceptually salient subtypes of /ɹ/ and /s/ speech sound errors, and correct /ɹ/ and /s/ targets, in American English."
   ],
   "p1": 2800,
   "pn": 2804,
   "doi": "10.21437/Interspeech.2025-339",
   "url": "interspeech_2025/benway25_interspeech.html"
  },
  "visser25_interspeech": {
   "authors": [
    [
     "Nicol",
     "Visser"
    ],
    [
     "Herman",
     "Kamper"
    ]
   ],
   "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units",
   "original": "340",
   "order": 401,
   "page_count": 5,
   "abstract": [
    "Spoken language models (SLMs) operate on acoustic units obtained by discretizing self-supervised speech representations. Although the characteristics of these units directly affect performance, the interaction between codebook size and unit coarseness (i.e., duration) remains unexplored. We investigate SLM performance as we vary codebook size and unit coarseness using the simple duration-penalized dynamic programming (DPDP) method. New analyses are performed across different linguistic levels. At the phone and word levels, coarseness provides little benefit, as long as the codebook size is chosen appropriately. However, when producing whole sentences in a resynthesis task, SLMs perform better with coarser units. In lexical and syntactic language modeling tasks, coarser units also give higher accuracies at lower bitrates. We therefore show that coarser units aren&#x27;t always better, but that DPDP is a simple and efficient way to obtain coarser units for the tasks where they are beneficial."
   ],
   "p1": 1968,
   "pn": 1972,
   "doi": "10.21437/Interspeech.2025-340",
   "url": "interspeech_2025/visser25_interspeech.html"
  },
  "cai25_interspeech": {
   "authors": [
    [
     "Rui",
     "Cai"
    ],
    [
     "Titia",
     "Benders"
    ]
   ],
   "title": "ASR-based segmentation for the analysis of larger child-speech datasets: Performance evaluation on vowels from Australian-English speaking children aged 4 to 11 years",
   "original": "342",
   "order": 871,
   "page_count": 5,
   "abstract": [
    "Annotation of segment boundaries in child speech presents a persistent challenge, particularly with large-scale datasets. While auto-segmentation methods have been developed for adult speech, little attention has been paid to evaluating their performance on child speech. This study evaluates factors contributing to performance of automatic segmentation by analyzing the annotations of two studies into Australian-English-speaking children. The first study assesses human-human reliability in 3- and 12-year-olds. The second compares manual and the Montreal Forced Aligner (MFA) segmentation in 4- to 11-year-olds. The results indicate that the MFA falls short of the human annotator, though discrepancies decrease as children grow older. Systematic discrepancies between the human annotator and the MFA suggest different criteria for placing segment boundaries. These results shed light on how to incorporate adult-based automatic aligners into semi-automatic (and thus: still partially manual) acoustic analysis of child speech."
   ],
   "p1": 4273,
   "pn": 4277,
   "doi": "10.21437/Interspeech.2025-342",
   "url": "interspeech_2025/cai25_interspeech.html"
  },
  "oneata25_interspeech": {
   "authors": [
    [
     "Dan",
     "Oneata"
    ],
    [
     "Leanne",
     "Nortje"
    ],
    [
     "Yevgen",
     "Matusevych"
    ],
    [
     "Herman",
     "Kamper"
    ]
   ],
   "title": "The mutual exclusivity bias of bilingual visually grounded speech models",
   "original": "343",
   "order": 1029,
   "page_count": 5,
   "abstract": [
    "Mutual exclusivity (ME) is a strategy where a novel word is associated with a novel object rather than a familiar one, facilitating language learning in children. Recent work has found an ME bias in a visually grounded speech (VGS) model trained on English speech with paired images. But ME has also been studied in bilingual children, who may employ it less due to cross-lingual ambiguity. We explore this pattern computationally using bilingual VGS models trained on combinations of English, French, and Dutch. We find that bilingual models generally exhibit a weaker ME bias than monolingual models, though exceptions exist. Analyses show that the combined visual embeddings of bilingual models have a smaller variance for familiar data, partly explaining the increase in confusion between novel and familiar concepts. We also provide new insights into why the ME bias exists in VGS models in the first place."
   ],
   "p1": 5043,
   "pn": 5047,
   "doi": "10.21437/Interspeech.2025-343",
   "url": "interspeech_2025/oneata25_interspeech.html"
  },
  "takagi25_interspeech": {
   "authors": [
    [
     "Masato",
     "Takagi"
    ],
    [
     "Miku",
     "Nishihara"
    ],
    [
     "Yukiya",
     "Hono"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "PeriodCodec: A Pitch-Controllable Neural Audio Codec Using Periodic Signals for Singing Voice Synthesis",
   "original": "347",
   "order": 999,
   "page_count": 5,
   "abstract": [
    "Neural audio codecs (NACs) have attracted considerable attention in the field of text-to-speech.  However, previous methods don&#x27;t offer a mechanism for explicit controlling the fundamental frequency (F0), hence they are not suitable for singing voice synthesis. To overcome this limitation, we propose a NAC that can control F0 by introducing explicit periodic signals into the decoder. This architecture enables direct manipulation of F0 during the synthesis process. Experimental results show that our proposed method achieves F0 control and improves synthesis quality compared to previous methods. Furthermore, by including singing voices in the training data set, we showed that both F0 controllability and the quality of singing voices are improved, enabling the construction of a NAC suitable for singing voice synthesis tasks."
   ],
   "p1": 4913,
   "pn": 4917,
   "doi": "10.21437/Interspeech.2025-347",
   "url": "interspeech_2025/takagi25_interspeech.html"
  },
  "xu25d_interspeech": {
   "authors": [
    [
     "Nan",
     "Xu"
    ],
    [
     "Zhaolong",
     "Huang"
    ],
    [
     "Xiaonan",
     "Zhi"
    ]
   ],
   "title": "MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement",
   "original": "350",
   "order": 832,
   "page_count": 5,
   "abstract": [
    "With the development of deep learning, speech enhancement has been greatly optimized in terms of speech quality. Previous methods typically focus on the discriminative supervised learning or generative modeling, which tends to introduce speech distortions or high computational cost. In this paper, we propose MDDM, a Multi-view Discriminative enhanced Diffusion-based Model. Specifically, we take the features of three domains (time, frequency and noise) as inputs of a discriminative prediction network, generating the preliminary spectrogram. Then, the discriminative output can be converted to clean speech by several inference sampling steps. Due to the intersection of the distributions between discriminative output and clean target, the smaller sampling steps can achieve the competitive performance compared to other diffusion-based methods. Experiments conducted on a public dataset and a real-world dataset validate the effectiveness of MDDM, either on subjective or objective metric."
   ],
   "p1": 4078,
   "pn": 4082,
   "doi": "10.21437/Interspeech.2025-350",
   "url": "interspeech_2025/xu25d_interspeech.html"
  },
  "shiota25_interspeech": {
   "authors": [
    [
     "Sayaka",
     "Shiota"
    ],
    [
     "Suzuka",
     "Horie"
    ],
    [
     "Kouta",
     "Kanno"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ]
   ],
   "title": "J-SPAW: Japanese speaker verification and spoofing attacks recorded in-the-wild dataset",
   "original": "352",
   "order": 799,
   "page_count": 5,
   "abstract": [
    "In this paper, we present J-SPAW (Japanese speaker verification and spoofing attacks recorded in-the-wild dataset), a novel speech database designed for speaker verification and spoofing detection in-the-wild environments1.J-SPAW is a unique database that simultaneously evaluates speaker verification and spoofing detection under realistic conditions, focusing on physical access scenarios, including replay attacks. The database includes diverse physical access scenarios, enhancing the variety and applicability of datasets available for anti-spoofing research. Our experimental results demonstrate that J-SPAW enables comprehensive analysis of spoofing detection from various perspectives and can be utilized for both speaker verification and spoofing detection tasks. This contribution is expected to advance state-of-the-art speaker verification and spoofing detection and provide a valuable resource for future research."
   ],
   "p1": 3913,
   "pn": 3917,
   "doi": "10.21437/Interspeech.2025-352",
   "url": "interspeech_2025/shiota25_interspeech.html"
  },
  "xu25e_interspeech": {
   "authors": [
    [
     "Haoning",
     "Xu"
    ],
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Youjun",
     "Chen"
    ],
    [
     "Huimeng",
     "Wang"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Chengxi",
     "Deng"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates",
   "original": "353",
   "order": 404,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel approach for speech foundation models compression that tightly integrates model pruning and parameter update into a single stage. Highly compact layer-level tied self-pinching gates each containing only a single learnable threshold are jointly trained with uncompressed models and used in fine-grained neuron level pruning. Experiments conducted on the LibriSpeech-100hr corpus suggest that our approach reduces the number of parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, while incurring no statistically significant word error rate (WER) increase on the test-clean dataset. Compared to previously published methods on the same task, our approach not only achieves the lowest WER of 7.05% on the test-clean dataset under a comparable model compression ratio of 4.26x, but also operates with at least 25% less model compression time."
   ],
   "p1": 1983,
   "pn": 1987,
   "doi": "10.21437/Interspeech.2025-353",
   "url": "interspeech_2025/xu25e_interspeech.html"
  },
  "tseng25_interspeech": {
   "authors": [
    [
     "Wei-Cheng",
     "Tseng"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "Probing the Robustness Properties of Neural Speech Codecs",
   "original": "355",
   "order": 1023,
   "page_count": 5,
   "abstract": [
    "Neural speech codecs have revolutionized speech coding, achieving higher compression while preserving audio fidelity. Beyond compression, they have emerged as tokenization strategies, enabling language modeling on speech and driving paradigm shifts across various speech processing tasks. Despite these advancements, their robustness in noisy environments remains underexplored, raising concerns about their generalization to real-world scenarios. In this work, we systematically evaluate neural speech codecs under various noise conditions, revealing non-trivial differences in their robustness. We further examine their linearity properties, uncovering non-linear distortions which partly explain observed variations in robustness. Lastly, we analyze their frequency response to identify factors affecting audio fidelity. Our findings provide critical insights into codec behavior and future codec design, as well as emphasizing the importance of noise robustness for their real-world integration1."
   ],
   "p1": 5013,
   "pn": 5017,
   "doi": "10.21437/Interspeech.2025-355",
   "url": "interspeech_2025/tseng25_interspeech.html"
  },
  "li25c_interspeech": {
   "authors": [
    [
     "Shaojie",
     "Li"
    ],
    [
     "Qintuya",
     "Si"
    ],
    [
     "De",
     "Hu"
    ]
   ],
   "title": "Temporal Convolutional Network with Smoothed and Weighted Losses for Distant Voice Activity and Overlapped Speech Detection",
   "original": "357",
   "order": 107,
   "page_count": 5,
   "abstract": [
    "Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key steps in various audio/speech processing tasks. Recent advances in VAD or OSD are moving toward using Temporal Convolutional Networks (TCNs) with frame-independent cross-entropy loss, which may be unable to cope with transient errors or boundary errors (caused by weak recordings at speech boundaries). In this paper, we formulate two novel losses, namely smoothed loss and weighted loss, in which the former copes with transient errors while the latter deals with boundary errors. In addition, we adopt Mel Frequency Cepstral Coefficients (MFCCs) and Instantaneous Correlation Coefficients (ICCs) as the acoustic and spatial features to drive the model. To improve computing efficiency, we also propose a spatial feature extraction module by selecting those frequencies with information-rich ICCs, which delivers good lightweight nature. Numerical experiments validate the efficacy of the proposed method."
   ],
   "p1": 504,
   "pn": 508,
   "doi": "10.21437/Interspeech.2025-357",
   "url": "interspeech_2025/li25c_interspeech.html"
  },
  "pan25c_interspeech": {
   "authors": [
    [
     "Yue",
     "Pan"
    ],
    [
     "Liwei",
     "Liu"
    ],
    [
     "Changxin",
     "Li"
    ],
    [
     "Xingyao",
     "Wang"
    ],
    [
     "Yili",
     "Xia"
    ],
    [
     "Hanyue",
     "Zhang"
    ],
    [
     "Ming",
     "Chu"
    ]
   ],
   "title": "A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification",
   "original": "358",
   "order": 409,
   "page_count": 5,
   "abstract": [
    "Speech is a cost-effective and non-intrusive data source for identifying acute and chronic heart failure (HF). However, there is a lack of research on whether Chinese syllables contain HF-related information, as observed in other well-studied languages. This study presents the first Chinese speech database of HF patients, featuring paired recordings taken before and after hospitalisation. The findings confirm the effectiveness of the Chinese language in HF detection using both standard &#x27;patient-wise&#x27; and personalised &#x27;pair-wise&#x27; classification approaches, with the latter serving as an ideal speaker-decoupled baseline for future research. Statistical tests and classification results highlight individual differences as key contributors to inaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for frequency importance analysis."
   ],
   "p1": 2008,
   "pn": 2012,
   "doi": "10.21437/Interspeech.2025-358",
   "url": "interspeech_2025/pan25c_interspeech.html"
  },
  "trachu25_interspeech": {
   "authors": [
    [
     "Thanapat",
     "Trachu"
    ],
    [
     "Thanathai",
     "Lertpetchpun"
    ],
    [
     "Ekapol",
     "Chuangsuwanich"
    ]
   ],
   "title": "Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing",
   "original": "362",
   "order": 1149,
   "page_count": 5,
   "abstract": [
    "Spoofed utterances always contain artifacts introduced by generative models. While several countermeasures have been proposed to detect spoofed utterances, most primarily focus on architectural improvements. In this work, we investigate how artifacts remain hidden in spoofed speech and how to enhance their presence. We propose a model-agnostic pipeline that amplifies artifacts using speech enhancement and various types of noise. Our approach consists of three key steps: noise addition, noise extraction, and noise amplification. First, we introduce noise into the raw speech. Then, we apply speech enhancement to extract the entangled noise and artifacts. Finally, we amplify these extracted features. Moreover, our pipeline is compatible with different speech enhancement models and countermeasure architectures. Our method improves spoof detection performance by up to 44.44% on ASVspoof2019 and 26.34% on ASVspoof2021."
   ],
   "p1": 5638,
   "pn": 5642,
   "doi": "10.21437/Interspeech.2025-362",
   "url": "interspeech_2025/trachu25_interspeech.html"
  },
  "meng25_interspeech": {
   "authors": [
    [
     "Ying",
     "Meng"
    ],
    [
     "Zhihua",
     "Fang"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "Federated Learning with Feature Space Separation for Speaker Recognition",
   "original": "364",
   "order": 310,
   "page_count": 5,
   "abstract": [
    "The performance of deep speaker models relies on large-scale, high-quality datasets. To improve the generalization of speaker models, federated learning can be used to share the knowledge learned by local models in private data. However, traditional federated learning methods have mapping conflict problems between local models. In this paper, we propose a federal speaker recognition method with feature space separation. Specifically, we introduce feature anchors for each speaker and transmit them when the client and server communicate. Local models use anchor loss to constrain the feature distribution of private data to avoid mapping conflict. In addition, we introduce a dynamic weight aggregation method when the server aggregates the local models, which amplifies the weights of well-performed local models to achieve a better aggregation effect. Extensive experiments and analyses in VoxCeleb and CN-Celeb demonstrate the effectiveness of our proposed method."
   ],
   "p1": 1518,
   "pn": 1522,
   "doi": "10.21437/Interspeech.2025-364",
   "url": "interspeech_2025/meng25_interspeech.html"
  },
  "lee25b_interspeech": {
   "authors": [
    [
     "Seung-jae",
     "Lee"
    ],
    [
     "Paul Hongsuck",
     "Seo"
    ]
   ],
   "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models",
   "original": "366",
   "order": 640,
   "page_count": 5,
   "abstract": [
    "Audiovisual segmentation (AVS) aims to identify visual regions corresponding to sound sources, playing a vital role in video understanding, surveillance, and human-computer interaction. Traditional AVS methods depend on large-scale pixel-level annotations, which are costly and time-consuming to obtain. To address this, we propose a novel zero-shot AVS framework that eliminates task-specific training by leveraging multiple pretrained models. Our approach integrates audio, vision, and text representations to bridge modality gaps, enabling precise sound source segmentation without AVS-specific annotations. We systematically explore different strategies for connecting pretrained models and evaluate their efficacy across multiple datasets. Experimental results demonstrate that our framework achieves state-of-the-art zero-shot AVS performance, highlighting the effectiveness of multimodal model integration for fine-grained audiovisual segmentation1."
   ],
   "p1": 3145,
   "pn": 3149,
   "doi": "10.21437/Interspeech.2025-366",
   "url": "interspeech_2025/lee25b_interspeech.html"
  },
  "dutta25_interspeech": {
   "authors": [
    [
     "Soumya",
     "Dutta"
    ],
    [
     "Smruthi",
     "Balaji"
    ],
    [
     "Varada",
     "R"
    ],
    [
     "Viveka",
     "Salinamakki"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "ABHINAYA - A System for Speech Emotion Recognition In Naturalistic Conditions Challenge ",
   "original": "368",
   "order": 949,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) in naturalistic settings remains a challenge due to the intrinsic variability, diverse recording conditions, and class imbalance. As participants in the Interspeech Naturalistic SER Challenge which focused on these complexities, we present &quot;Abhinaya&quot;, a system integrating speech-based, text-based, and speech-text models. Our approach fine-tunes self-supervised and speech large language models (SLLM) for speech representations, leverages large language models (LLM) for textual context, and employs speech-text modeling with an SLLM to capture nuanced emotional cues. To combat class imbalance, we apply tailored loss functions and generate categorical decisions through majority voting. Despite one model not being fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon completion of training, it achieved state-of-the-art performance among published results, demonstrating the effectiveness of our approach for SER in real-world conditions1."
   ],
   "p1": 4663,
   "pn": 4667,
   "doi": "10.21437/Interspeech.2025-368",
   "url": "interspeech_2025/dutta25_interspeech.html"
  },
  "alip25_interspeech": {
   "authors": [
    [
     "Nurali",
     "Alip"
    ],
    [
     "Tianrui",
     "Wang"
    ],
    [
     "Rui",
     "Cao"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Jingru",
     "Lin"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A Three-Stage Beamforming with Harmonic Guidance for Multi-Channel Speech Enhancement",
   "original": "369",
   "order": 245,
   "page_count": 5,
   "abstract": [
    "With the rapid advancement of multi-channel speech enhancement (MCSE) research, nonlinear spatial filtering methods integrating spatial and spectral processing are prevalent. However, many existing approaches overlook the explicit extraction of speech spectral structure information, leading to insufficient learning of joint spatial-spectral information,  limiting  performance under low signal-to-noise ratio (SNR) conditions. To address this, we propose a three-stage multi-channel speech enhancement framework. The first stage has an acoustic structure extraction module capturing speech spectral patterns from noisy inputs, enabling  spatial-spectral cue exploration and interaction. In the next two stages, the process combines full-band noise reduction with speech structure refinement by decoupling enhancement into coarse enhancement and spectral refinement. Experiments on LibriSpeech-based datasets demonstrate that the proposed method significantly outperforms the reference method."
   ],
   "p1": 1193,
   "pn": 1197,
   "doi": "10.21437/Interspeech.2025-369",
   "url": "interspeech_2025/alip25_interspeech.html"
  },
  "huang25b_interspeech": {
   "authors": [
    [
     "Yuheng",
     "Huang"
    ],
    [
     "Ying",
     "Ren"
    ],
    [
     "Wenjie",
     "Zhang"
    ],
    [
     "Diqun",
     "Yan"
    ]
   ],
   "title": "CBA: Backdoor Attack on Deep Speech Classification via Audio Compression",
   "original": "372",
   "order": 1151,
   "page_count": 5,
   "abstract": [
    "As deep neural networks permeate various aspects of society, research has demonstrated their vulnerability to backdoor attacks, particularly with untrustworthy third-party platforms, enabling attackers to manipulate model outputs using customized triggers. However, most audio-based backdoor methods are either sample-agnostic or audible. In this paper, we propose Compression-based Backdoor Attack (CBA), a novel strategy designed specifically for audio compression scenarios, which also utilizes fusion as well as iterative optimization strategies to generate sample-specific triggers and ensure both the effectiveness and stealthiness of the triggers. Extensive experiments validate the attack efficiency and robustness."
   ],
   "p1": 5648,
   "pn": 5652,
   "doi": "10.21437/Interspeech.2025-372",
   "url": "interspeech_2025/huang25b_interspeech.html"
  },
  "xiang25_interspeech": {
   "authors": [
    [
     "Yang",
     "Xiang"
    ],
    [
     "Canan",
     "Huang"
    ],
    [
     "Desheng",
     "Hu"
    ],
    [
     "Jingguang",
     "Tian"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Chao",
     "Zhang"
    ]
   ],
   "title": "A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model",
   "original": "374",
   "order": 829,
   "page_count": 5,
   "abstract": [
    "Most current speech enhancement (SE) methods recover clean speech from noisy inputs by directly estimating time-frequency masks or spectrums. However, these approaches often neglect the distinct attributes, such as semantic content and acoustic details, inherent in speech signals, which can hinder performance in downstream tasks. Moreover, their effectiveness tends to degrade in complex acoustic environments. To overcome these challenges, we propose a novel, semantic information-based, step-by-step factorized SE method using factorized codec and diffusion model. Unlike traditional SE methods, our hierarchical modeling of semantic and acoustic attributes enables more robust clean speech recovery, particularly in challenging acoustic scenarios. Moreover, this method offers further advantages for downstream TTS tasks. Experimental results demonstrate that our algorithm not only outperforms SOTA baselines in terms of speech quality but also enhances TTS performance in noisy environments."
   ],
   "p1": 4063,
   "pn": 4067,
   "doi": "10.21437/Interspeech.2025-374",
   "url": "interspeech_2025/xiang25_interspeech.html"
  },
  "kim25f_interspeech": {
   "authors": [
    [
     "Jangyeon",
     "Kim"
    ],
    [
     "Ui-Hyeop",
     "Shin"
    ],
    [
     "Jaehyun",
     "Ko"
    ],
    [
     "Hyung-Min",
     "Park"
    ]
   ],
   "title": "Stack Less, Repeat More: A Block Reusing Approach for Progressive Speech Enhancement",
   "original": "376",
   "order": 1052,
   "page_count": 5,
   "abstract": [
    "This paper presents an efficient speech enhancement (SE) approach that reuses a processing block repeatedly instead of conventional stacking. Rather than increasing the number of blocks for learning deep latent representations, repeating a single block leads to progressive refinement while reducing parameter redundancy. We also minimize domain transformation by keeping an encoder and decoder shallow and reusing a single sequence modeling block. Experimental results show that the number of processing stages is more critical to performance than the number of blocks with different weights. Also, we observed that the proposed method gradually refines a noisy input within a single block. Furthermore, with the block reuse method, we demonstrate that deepening the encoder and decoder can be redundant for learning deep complex representation. Therefore, the experimental results confirm that the proposed block reusing enables progressive learning and provides an efficient alternative for SE."
   ],
   "p1": 5158,
   "pn": 5162,
   "doi": "10.21437/Interspeech.2025-376",
   "url": "interspeech_2025/kim25f_interspeech.html"
  },
  "jung25b_interspeech": {
   "authors": [
    [
     "Youngmoon",
     "Jung"
    ],
    [
     "Yong-Hyeok",
     "Lee"
    ],
    [
     "Myunghun",
     "Jung"
    ],
    [
     "Jaeyoung",
     "Roh"
    ],
    [
     "Chang Woo",
     "Han"
    ],
    [
     "Hoon-Young",
     "Cho"
    ]
   ],
   "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting",
   "original": "378",
   "order": 540,
   "page_count": 5,
   "abstract": [
    "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic and text embeddings are typically compared at either the phoneme or utterance level. To facilitate this, we optimize acoustic and text encoders using deep metric learning (DML), enabling direct comparison of multi-modal embeddings in a shared embedding space. However, the inherent heterogeneity between audio and text modalities presents a significant challenge. To address this, we propose Modality Adversarial Learning (MAL), which reduces the domain gap in heterogeneous modality representations. Specifically, we train a modality classifier adversarially to encourage both encoders to generate modality-invariant embeddings. Additionally, we apply DML to achieve phoneme-level alignment between audio and text, and conduct extensive comparisons across various DML objectives. Experiments on the Wall Street Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the proposed approach."
   ],
   "p1": 2645,
   "pn": 2649,
   "doi": "10.21437/Interspeech.2025-378",
   "url": "interspeech_2025/jung25b_interspeech.html"
  },
  "r25_interspeech": {
   "authors": [
    [
     "Kirandevraj",
     "R"
    ],
    [
     "Vinod K",
     "Kurmi"
    ],
    [
     "Vinay",
     "Namboodiri"
    ],
    [
     "CV",
     "Jawahar"
    ]
   ],
   "title": "Multilingual Query-by-Example KWS for Indian Languages using Transliteration",
   "original": "380",
   "order": 187,
   "page_count": 5,
   "abstract": [
    "Query-by-Example Keyword Spotting (QbE KWS) detects query audio within target audio. A common approach for multilingual QbE KWS uses phoneme posteriors as representations, with a shared phoneme dictionary across languages. We propose a novel method that replaces phoneme-based representations with transliteration, unifying transcripts from multiple Indian languages into the Devanagari script, a text script used for Hindi and Marathi. We train a Multilingual ASR model to predict transliterated Devanagari text from audio across 10 Indian languages. The character logits from this ASR serve as both query and target audio features. Using the Kathbath dataset for training and the IndicSUPERB QbE evaluation set, our approach achieves significant improvements. The average MTWV increased from 0.015 (IndicSUPERB) to 0.504, and performance rose from 0.387 to 0.504, surpassing the best-performing Marathi ASR baseline. This demonstrates the effectiveness of transliteration for multilingual KWS."
   ],
   "p1": 903,
   "pn": 907,
   "doi": "10.21437/Interspeech.2025-380",
   "url": "interspeech_2025/r25_interspeech.html"
  },
  "okabe25_interspeech": {
   "authors": [
    [
     "Koji",
     "Okabe"
    ],
    [
     "Hitoshi",
     "Yamamoto"
    ]
   ],
   "title": "Simultaneous Masked and Unmasked Decoding with Speculative Decoding Masking for Fast ASR without Accuracy Loss",
   "original": "382",
   "order": 133,
   "page_count": 5,
   "abstract": [
    "In this paper, we introduce two methods, Simultaneous Masked and Unmasked Decoding (SMUD) and speculative decoding masking, into Partially autoregressive (PAR) decoding. These methods achieve the same recognition accuracy as Autoregressive (AR) decoding while maintaining higher computational efficiency than AR in Automatic Speech Recognition (ASR). SMUD and speculative decoding masking can accurately identify hypotheses where decoder score computation can be omitted. By omitting these computations, they achieve faster processing while obtaining the same search results as AR decoding. In TED-LIUM2 evaluations, SMUD with speculative decoding masking achieved a WER of 7.3% and an RTF of 0.41, as compared to AR&#x27;s WER of 7.3% and RTF of 0.59, showcasing the method’s ability to maintain the same high accuracy as AR while enhancing computational efficiency."
   ],
   "p1": 634,
   "pn": 638,
   "doi": "10.21437/Interspeech.2025-382",
   "url": "interspeech_2025/okabe25_interspeech.html"
  },
  "suda25_interspeech": {
   "authors": [
    [
     "Hitoshi",
     "Suda"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Satoru",
     "Fukayama"
    ]
   ],
   "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora",
   "original": "383",
   "order": 279,
   "page_count": 5,
   "abstract": [
    "Perceived voice likability plays a crucial role in various social interactions, such as partner selection and advertising. A system that provides reference likable voice samples tailored to target audiences would enable users to adjust their speaking style and voice quality, facilitating smoother communication. To this end, we propose a voice conversion method that controls the likability of input speech while preserving both speaker identity and linguistic content. To improve training data scalability, we train a likability predictor on an existing voice likability dataset and employ it to automatically annotate a large speech synthesis corpus with likability ratings. Experimental evaluations reveal a significant correlation between the predictor’s outputs and human-provided likability ratings. Subjective and objective evaluations further demonstrate that the proposed approach effectively controls voice likability while preserving both speaker identity and linguistic content."
   ],
   "p1": 1363,
   "pn": 1367,
   "doi": "10.21437/Interspeech.2025-383",
   "url": "interspeech_2025/suda25_interspeech.html"
  },
  "li25d_interspeech": {
   "authors": [
    [
     "Zixuan",
     "Li"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Jinglin",
     "Bai"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "TF-SkiMNet: Speech Enhancement Based on Inplace Modeling and Skipping Memory in Time-Frequency Domain",
   "original": "391",
   "order": 1049,
   "page_count": 5,
   "abstract": [
    "Neural networks that leverage both full-band and sub-band information have demonstrated exceptional performance across various speech processing tasks. In this paper, we examine the essential factors that allow these architectures to achieve state-of-the-art (SOTA) performance, identifying their &#x27;inplace modeling&#x27; capability as a critical component of this success. Adhering to this principle, we introduce TF-SkiMNet, which employs Skipping Memory (SkiM) to efficiently perform global temporal modeling using full-band information with low computational overhead. For single-channel speech enhancement, TF-SkiMNet achieves comparable performance to the SOTA model TF-CrossNet while reducing MACs by 87%. Furthermore, TF-SkiMNet is evaluated on both single-channel and multi-channel speech enhancement tasks, achieving SOTA performance under similar computational budgets."
   ],
   "p1": 5143,
   "pn": 5147,
   "doi": "10.21437/Interspeech.2025-391",
   "url": "interspeech_2025/li25d_interspeech.html"
  },
  "yuan25_interspeech": {
   "authors": [
    [
     "Xihao",
     "Yuan"
    ],
    [
     "Siqi",
     "Liu"
    ],
    [
     "Yan",
     "Chen"
    ],
    [
     "Hang",
     "Zhou"
    ],
    [
     "Chang",
     "Liu"
    ],
    [
     "Hanting",
     "Chen"
    ],
    [
     "Jie",
     "Hu"
    ]
   ],
   "title": "SaD: A Scenario-Aware Discriminator for Speech Enhancement",
   "original": "392",
   "order": 984,
   "page_count": 5,
   "abstract": [
    "Generative adversarial network-based models have shown remarkable performance in the field of speech enhancement. However, the current optimization strategies for these models predominantly focus on refining the architecture of the generator or enhancing the quality evaluation metrics of the discriminator. This approach often overlooks the rich contextual information inherent in diverse scenarios. In this paper, we propose a scenario-aware discriminator that captures scene-specific features and performs frequency-domain division, thereby enabling a more accurate quality assessment of the enhanced speech generated by the generator. We conducted comprehensive experiments on three representative models using two publicly available datasets. The results demonstrate that our method can effectively adapt to various generator architectures without altering their structure, thereby unlocking further performance gains in speech enhancement across different scenarios."
   ],
   "p1": 4838,
   "pn": 4842,
   "doi": "10.21437/Interspeech.2025-392",
   "url": "interspeech_2025/yuan25_interspeech.html"
  },
  "ahn25_interspeech": {
   "authors": [
    [
     "Hyebin",
     "Ahn"
    ],
    [
     "Kangwook",
     "Jang"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization",
   "original": "397",
   "order": 695,
   "page_count": 5,
   "abstract": [
    "Noise robustness in speech foundation models (SFMs) has been a critical challenge, as most models are primarily trained on clean data and experience performance degradation when the models are exposed to noisy speech. To address this issue, we propose HuBERT-VIC, a noise-robust SFM with variance, invariance, and covariance regularization (VICReg) objectives. These objectives adjust the statistics of noisy speech representations, enabling the model to capture diverse acoustic characteristics and improving the generalization ability across different types of noise. When applied to HuBERT, our model shows relative performance improvements of 23.3% on LibriSpeech test-clean and 13.2% on test-other, compared to the baseline model pre-trained on noisy speech."
   ],
   "p1": 3419,
   "pn": 3423,
   "doi": "10.21437/Interspeech.2025-397",
   "url": "interspeech_2025/ahn25_interspeech.html"
  },
  "zhuo25_interspeech": {
   "authors": [
    [
     "Jianheng",
     "Zhuo"
    ],
    [
     "Yifan",
     "Yang"
    ],
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Yong",
     "Xu"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Kai",
     "Yu"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining",
   "original": "398",
   "order": 239,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR."
   ],
   "p1": 1163,
   "pn": 1167,
   "doi": "10.21437/Interspeech.2025-398",
   "url": "interspeech_2025/zhuo25_interspeech.html"
  },
  "lemaguer25_interspeech": {
   "authors": [
    [
     "Sébastien",
     "Le Maguer"
    ],
    [
     "Gwénolé",
     "Lecorvé"
    ],
    [
     "Damien",
     "Lolive"
    ],
    [
     "Naomi",
     "Harte"
    ],
    [
     "Juraj",
     "Šimko"
    ]
   ],
   "title": "Enabling the replicability of speech synthesis perceptual evaluations",
   "original": "401",
   "order": 520,
   "page_count": 5,
   "abstract": [
    "How speech synthesis is evaluated is nowadays questioned. Not only have conventional listening tests as a whole been proven a poor match for modern synthesis, but more fundamentally, important information (e.g., the question asked to the listener) is frequently missing in the report of the outcome of the evaluation despite the impact on the interpretation of the test results. This can lead to uncertainty about the validity of these evaluations. To address this issue, we propose standardising the structure of any evaluation report. To facilitate this standardisation, our contribution is twofold: an open-source subjective evaluation platform; and a set of reporting guidelines. The platform is designed to enable the development of easily shareable evaluation recipes. The set of guidelines complements the platform to support researchers in reporting their evaluation choices and analysis in more detail while relying on the recipe to describe the actual evaluation process."
   ],
   "p1": 2545,
   "pn": 2549,
   "doi": "10.21437/Interspeech.2025-401",
   "url": "interspeech_2025/lemaguer25_interspeech.html"
  },
  "su25_interspeech": {
   "authors": [
    [
     "Jia-Jyu",
     "Su"
    ],
    [
     "Yen-Ting",
     "Lin"
    ],
    [
     "Wu-Hao",
     "Li"
    ],
    [
     "Chao-Kai",
     "Chang"
    ],
    [
     "Yan-Zhi",
     "Chen"
    ],
    [
     "Chen-Yu",
     "Chiang"
    ]
   ],
   "title": "Lightweight Speech Enhancement for Mandarin Esophageal Speech",
   "original": "404",
   "order": 939,
   "page_count": 5,
   "abstract": [
    "Esophageal speech is an alternative speech production method for people who have undergone laryngectomy and often suffer from reduced intelligibility. This paper proposes three lightweight speech enhancement methods trained with the loss given by end-to-end automatic speech recognition models. The proposed methods are based on frame mask (FM), ideal ratio mask (IRM), and voice conversion (VC) techniques. The evaluations show that the enhanced speech produced by the proposed methods led to an improvement over the original esophagus speech, specifically in terms of speech recognition rates by automatic speech recognition systems and human evaluators, naturalness as assessed by mean opinion scores, and more detected voicing segments."
   ],
   "p1": 4613,
   "pn": 4617,
   "doi": "10.21437/Interspeech.2025-404",
   "url": "interspeech_2025/su25_interspeech.html"
  },
  "levkovitch25_interspeech": {
   "authors": [
    [
     "Alon",
     "Levkovitch"
    ],
    [
     "Julian",
     "Salazar"
    ],
    [
     "Soroosh",
     "Mariooryad"
    ],
    [
     "RJ",
     "Skerry-Ryan"
    ],
    [
     "Nadav",
     "Bar"
    ],
    [
     "Bastiaan",
     "Kleijn"
    ],
    [
     "Eliya",
     "Nachmani"
    ]
   ],
   "title": "Zero-Shot Mono-to-Binaural Speech Synthesis",
   "original": "406",
   "order": 850,
   "page_count": 5,
   "abstract": [
    "We present ZeroBAS, a neural method to synthesize binaural speech from monaural speech recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural speech synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on previous standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset."
   ],
   "p1": 4168,
   "pn": 4172,
   "doi": "10.21437/Interspeech.2025-406",
   "url": "interspeech_2025/levkovitch25_interspeech.html"
  },
  "kawanishi25_interspeech": {
   "authors": [
    [
     "Shoki",
     "Kawanishi"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Yuya",
     "Chiba"
    ],
    [
     "Takashi",
     "Nose"
    ]
   ],
   "title": "Improving User Impression of Spoken Dialogue Systems by Controlling Para-linguistic Expression Based on Intimacy",
   "original": "408",
   "order": 619,
   "page_count": 5,
   "abstract": [
    "The advent of large language models (LLMs) has improved the naturalness of responses in dialogue systems; however, conversations with these systems still differ significantly from those between humans. For instance, humans adjust their manner of speaking based on their relationship with the conversation partner, whereas dialogue systems respond uniformly, even after repeated interactions. By producing responses that consider the progression of relationships, it may be possible to develop dialogue systems that are more appealing to users. Previous studies attempting to achieve such systems have primarily focused on the linguistic expressions of responses,while paralinguistic expressions have been largely overlooked. In this paper, we propose a dialogue system that adapts both linguistic and para-linguistic expressions as the number of interactions increases. We also evaluate its effectiveness through dialogue experiments."
   ],
   "p1": 3040,
   "pn": 3044,
   "doi": "10.21437/Interspeech.2025-408",
   "url": "interspeech_2025/kawanishi25_interspeech.html"
  },
  "hsiao25_interspeech": {
   "authors": [
    [
     "Chi-Yuan",
     "Hsiao"
    ],
    [
     "Ke-Han",
     "Lu"
    ],
    [
     "Kai-Wei",
     "Chang"
    ],
    [
     "Chih-Kai",
     "Yang"
    ],
    [
     "Wei-Chih",
     "Chen"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
   "original": "409",
   "order": 658,
   "page_count": 5,
   "abstract": [
    "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies—model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines."
   ],
   "p1": 3234,
   "pn": 3238,
   "doi": "10.21437/Interspeech.2025-409",
   "url": "interspeech_2025/hsiao25_interspeech.html"
  },
  "bijoy25_interspeech": {
   "authors": [
    [
     "Mehedi Hasan",
     "Bijoy"
    ],
    [
     "Dejan",
     "Porjazovski"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition",
   "original": "418",
   "order": 31,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness."
   ],
   "p1": 146,
   "pn": 150,
   "doi": "10.21437/Interspeech.2025-418",
   "url": "interspeech_2025/bijoy25_interspeech.html"
  },
  "kumar25_interspeech": {
   "authors": [
    [
     "Saurabh",
     "Kumar"
    ],
    [
     "",
     "Amartyaveer"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Jointly Improving Dialect Identification and ASR in Indian Languages using Multimodal Feature Fusion",
   "original": "421",
   "order": 565,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) and Dialect Identification (DID) are crucial for Indian languages, many of which are low-resource and exhibit significant dialectal differences. Existing methods often optimize ASR or DID individually, resulting in performance trade-offs. In this work, we propose a multimodal framework that jointly improves ASR and DID. Our method employs a Bottleneck Encoder to extract dialectal features from Conformer-based speech representations and a RoBERTa encoder to process ASR-generated CTC embeddings. A gating mechanism merges these features, followed by an attention encoder to refine the representations. The learned embeddings are concatenated with Conformer outputs to enhance ASR features. Evaluated on eight Indian languages with thirty-three dialects, our method achieves an average DID accuracy of 81.63% and average CER and WER of 4.65% and 17.73%, respectively. These results highlight the effectiveness of our method for joint ASR-DID modeling."
   ],
   "p1": 2770,
   "pn": 2774,
   "doi": "10.21437/Interspeech.2025-421",
   "url": "interspeech_2025/kumar25_interspeech.html"
  },
  "kim25g_interspeech": {
   "authors": [
    [
     "Seung-bin",
     "Kim"
    ],
    [
     "Hyun-seo",
     "Shin"
    ],
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Chan-yeong",
     "Lim"
    ],
    [
     "Kyo-Won",
     "Koo"
    ],
    [
     "Jisoo",
     "Son"
    ],
    [
     "Sanghyun",
     "Hong"
    ],
    [
     "Souhwan",
     "Jung"
    ],
    [
     "Ha-Jin",
     "Yu"
    ]
   ],
   "title": "Enhancing Audio Deepfake Detection by Improving Representation Similarity of Bonafide Speech",
   "original": "422",
   "order": 461,
   "page_count": 5,
   "abstract": [
    "The key to audio deepfake detection is distinguishing bonafide speech from carefully generated spoofed speech. The more distinguishable they are, the better and more generalizable the detection becomes. In this work, we propose a novel approach to enhance this distinguishability in the latent space. Inspired by one-class classification, we formulate an objective function that encourages the contraction of bonafide samples while dispersing fake speech samples during training. Our objective consists of two key components: Bonafide-Pair Learning (BPL) loss and an Extended One-Class Softmax (EOC-S) loss. The BPL reduces intra-class variance by aligning the embeddings of augmented bonafide pairs, while the EOC-S leverages Adam-based centroid updates and margin constraints to reinforce separability from spoofed data. Experimental results on ASVspoof datasets demonstrate that our proposed approach enhances detection performance across diverse attack scenarios."
   ],
   "p1": 2250,
   "pn": 2254,
   "doi": "10.21437/Interspeech.2025-422",
   "url": "interspeech_2025/kim25g_interspeech.html"
  },
  "yadav25_interspeech": {
   "authors": [
    [
     "Sarthak",
     "Yadav"
    ],
    [
     "Sergios",
     "Theodoridis"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "AxLSTMs: learning self-supervised audio representations with xLSTMs",
   "original": "426",
   "order": 717,
   "page_count": 5,
   "abstract": [
    "While the transformer has emerged as the eminent neural architecture, several independent lines of research have emerged to address its limitations. Recurrent neural approaches have observed a lot of renewed interest, including the extended long short-term memory (xLSTM) architecture, which reinvigorates the original LSTM. However, while xLSTMs have shown competitive performance compared to the transformer, their viability for learning self-supervised general-purpose audio representations has not been evaluated. This work proposes Audio xLSTM (AxLSTM), an approach for learning audio representations from masked spectrogram patches in a self-supervised setting. Pretrained on the AudioSet dataset, the proposed AxLSTM models outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by up to 25% in relative performance across a set of ten diverse downstream tasks while having up to 45% fewer parameters."
   ],
   "p1": 3524,
   "pn": 3528,
   "doi": "10.21437/Interspeech.2025-426",
   "url": "interspeech_2025/yadav25_interspeech.html"
  },
  "gao25_interspeech": {
   "authors": [
    [
     "Yi",
     "Gao"
    ],
    [
     "Hangting",
     "Chen"
    ],
    [
     "Siyu",
     "Zhang"
    ],
    [
     "Qingshan",
     "Yang"
    ],
    [
     "Jingcong",
     "Chen"
    ]
   ],
   "title": "TSDT-Net: Ultra-Low-Complexity Two-Stage Model Combining Dual-Path-Transformer and Transform-Average-Concatenate Network for Speech Enhancement",
   "original": "429",
   "order": 16,
   "page_count": 5,
   "abstract": [
    "This paper introduces TSDT-Net, a dual-stage ultra-low-complexity architecture for speech enhancement which achieves higher denoising performance with limited parameter number and computational cost. Its first stage utilizes a simplified Dual-Path-Transformer(DPT) structure. In the second stage, the first-stage output and original noisy signal are treated as dual-channel inputs, modeled as a beamforming optimization problem. An enhanced Transform-Average-Concatenate (TAC) network processes these channels through spectral filtering and enhancement. Fast linear transformers ensure ultra-low computational overhead, while gated networks in both stages facilitate complex Ideal Ratio Mask (cIRM) construction. Residual connections between stages enable performance synergies. Evaluations on the INTERSPEECH 2020 DNS Challenge demonstrate TSDT-Net&#x27;s superiority, achieving staet-of-the-art DNSMOS and PESQ scores with significant margins over single-stage models under stringent computational constraints (&lt;700K parameters and &lt;500M/ 200M MACs). This efficiency enables deployment across diverse embedded devices."
   ],
   "p1": 71,
   "pn": 75,
   "doi": "10.21437/Interspeech.2025-429",
   "url": "interspeech_2025/gao25_interspeech.html"
  },
  "huang25c_interspeech": {
   "authors": [
    [
     "Sung-Feng",
     "Huang"
    ],
    [
     "Heng-Cheng",
     "Kuo"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Xuesong",
     "Yang"
    ],
    [
     "Pin-Jui",
     "Ku"
    ],
    [
     "Ante",
     "Jukic"
    ],
    [
     "Huck",
     "Yang"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Yu-Chiang Frank",
     "Wang"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Szu-Wei",
     "Fu"
    ]
   ],
   "title": "VoiceNoNG: Robust High-Quality Speech Editing Model without Hallucinations",
   "original": "431",
   "order": 705,
   "page_count": 5,
   "abstract": [
    "Voicebox and VoiceCraft are the current most representative models for non-autoregressive and autoregressive speech editing, respectively. Although both of them can generate high-quality speech edits, we identify their limitations: Voicebox is not good at editing speech with background audio, while VoiceCraft suffers from the hallucination-like problem. To maintain speech quality for varying audio scenarios and address the hallucination issue, we introduce VoiceNoNG, which combines the strengths of both model frameworks. VoiceNoNG utilizes a latent flow-matching framework to model the pre-quantization features of a neural codec. The vector quantizer in the neural codec provides additional robustness against minor prediction errors from the editing model, which enables VoiceNoNG to achieve state-of-the-art performance in both objective and subjective evaluations under diverse audio conditions. Audio examples and ablations can be found on the demo page1 and supplementary material2."
   ],
   "p1": 3469,
   "pn": 3473,
   "doi": "10.21437/Interspeech.2025-431",
   "url": "interspeech_2025/huang25c_interspeech.html"
  },
  "stucki25_interspeech": {
   "authors": [
    [
     "Samuel",
     "Stucki"
    ],
    [
     "Jan",
     "Deriu"
    ],
    [
     "Mark",
     "Cieliebak"
    ]
   ],
   "title": "Voice Adaptation for Swiss German",
   "original": "432",
   "order": 845,
   "page_count": 5,
   "abstract": [
    "This work investigates the performance of Voice Adaptation models for Swiss German dialects, i.e., translating Standard German text to Swiss German dialect speech. For this, we preprocess a large dataset of Swiss podcasts, which we automatically transcribe and annotate with dialect classes, yielding approximately 5000 hours of weakly labeled training material. We fine-tune the XTTSv2 model on this dataset and show that it achieves good scores in human and automated evaluations and can correctly render the desired dialect. Our work shows a step towards adapting Voice Cloning technology to underrepresented languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS scores of 3.8."
   ],
   "p1": 4143,
   "pn": 4147,
   "doi": "10.21437/Interspeech.2025-432",
   "url": "interspeech_2025/stucki25_interspeech.html"
  },
  "kang25b_interspeech": {
   "authors": [
    [
     "Minsu",
     "Kang"
    ],
    [
     "Seolhee",
     "Lee"
    ],
    [
     "Choonghyeon",
     "Lee"
    ],
    [
     "Namhyun",
     "Cho"
    ]
   ],
   "title": "When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds",
   "original": "433",
   "order": 847,
   "page_count": 5,
   "abstract": [
    "Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres."
   ],
   "p1": 4153,
   "pn": 4157,
   "doi": "10.21437/Interspeech.2025-433",
   "url": "interspeech_2025/kang25b_interspeech.html"
  },
  "turavecino25_interspeech": {
   "authors": [
    [
     "Biel",
     "Tura-Vecino"
    ],
    [
     "Subhadeep",
     "Maji"
    ],
    [
     "Aravind",
     "Varier"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Ivan",
     "Valles"
    ],
    [
     "Michael",
     "Owen"
    ],
    [
     "Constantinos",
     "Papayiannis"
    ],
    [
     "Leif",
     "Radel"
    ],
    [
     "Grant",
     "Strimel"
    ],
    [
     "Oluwaseyi",
     "Feyisetan"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ],
    [
     "Ariya",
     "Rastrow"
    ],
    [
     "Volker",
     "Leutnant"
    ],
    [
     "Trevor",
     "Wood"
    ]
   ],
   "title": "Universal Semantic Disentangled Privacy-preserving Speech Representation Learning",
   "original": "437",
   "order": 743,
   "page_count": 5,
   "abstract": [
    "The use of human speech to train LLMs poses privacy concerns due to these models&#x27; ability to generate samples that closely resemble artifacts in the training data. We propose a speaker privacy-preserving representation learning method through the Universal Speech Codec (USC), a computationally efficient codec that disentangles speech into: (i) privacy-preserving semantically rich representations, capturing content and speech paralinguistics, and (ii) residual acoustic and speaker representations that enable high-fidelity reconstruction. Evaluations show that USC&#x27;s semantic representation preserves content, prosody, and sentiment, while removing identifiable traits. Additionally, we present an evaluation methodology for measuring privacy-preserving properties. We compare USC against other speech codecs and demonstrate its effectiveness on privacy-preserving representation learning, showcasing the trade-offs between speaker anonymization and paralinguistics retention.1"
   ],
   "p1": 3633,
   "pn": 3637,
   "doi": "10.21437/Interspeech.2025-437",
   "url": "interspeech_2025/turavecino25_interspeech.html"
  },
  "kamper25_interspeech": {
   "authors": [
    [
     "Herman",
     "Kamper"
    ],
    [
     "Benjamin",
     "van Niekerk"
    ],
    [
     "Julian",
     "Zaïdi"
    ],
    [
     "Marc-André",
     "Carbonneau"
    ]
   ],
   "title": "LinearVC: Linear Transformations of Self-Supervised Features Through the Lens of Voice Conversion",
   "original": "438",
   "order": 286,
   "page_count": 5,
   "abstract": [
    "We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations."
   ],
   "p1": 1398,
   "pn": 1402,
   "doi": "10.21437/Interspeech.2025-438",
   "url": "interspeech_2025/kamper25_interspeech.html"
  },
  "fernandez25_interspeech": {
   "authors": [
    [
     "Andres",
     "Fernandez"
    ],
    [
     "Juan Azcarreta",
     "Ortiz"
    ],
    [
     "Çağdaş",
     "Bilen"
    ],
    [
     "Jesus",
     "Monge Alvarez"
    ]
   ],
   "title": "Efficient Neural and Numerical Methods for High-QualityOnline Speech Spectrogram Inversion via Gradient Theorem",
   "original": "439",
   "order": 701,
   "page_count": 5,
   "abstract": [
    "Recent work in online speech spectrogram inversion effectively combines Deep Learning with the Gradient Theorem to predict phase derivatives directly from magnitudes. Then, phases are estimated from their derivatives via least squares, resulting in a high quality reconstruction. In this work, we introduce three innovations that drastically reduce computational cost, while maintaining high quality: Firstly, we introduce a novel neural network architecture with just 8k parameters, 30 times smaller than previous state of the art. Secondly, increasing latency by 1 hop size allows us to further halve the cost of the neural inference step. Thirdly, we propose a linear-complexity solver for the least squares step that leverages tridiagonality and positive-semidefiniteness, achieving a speedup of several orders of magnitude. We release samples online."
   ],
   "p1": 3449,
   "pn": 3453,
   "doi": "10.21437/Interspeech.2025-439",
   "url": "interspeech_2025/fernandez25_interspeech.html"
  },
  "masztalski25_interspeech": {
   "authors": [
    [
     "Piotr",
     "Masztalski"
    ],
    [
     "Michał",
     "Romaniuk"
    ],
    [
     "Jakub",
     "Żak"
    ],
    [
     "Mateusz",
     "Matuszewski"
    ],
    [
     "Konrad",
     "Kowalczyk"
    ]
   ],
   "title": "Clustering-based Hard Negative Sampling for Supervised Contrastive Speaker Verification",
   "original": "442",
   "order": 756,
   "page_count": 5,
   "abstract": [
    "In speaker verification, contrastive learning is gaining popularity as an alternative to the traditionally used classification-based approaches. Contrastive methods can benefit from an effective use of hard negative pairs, which are different-class samples particularly challenging for a verification model due to their similarity. In this paper, we propose CHNS - a clustering-based hard negative sampling method, dedicated for supervised contrastive speaker representation learning. Our approach clusters embeddings of similar speakers, and adjusts batch composition to obtain an optimal ratio of hard and easy negatives during contrastive loss calculation. Experimental evaluation shows that CHNS outperforms a baseline supervised contrastive approach with and without loss-based hard negative sampling, as well as a state-of-the-art classification-based approach to speaker verification by as much as 18 % relative EER and minDCF on the VoxCeleb dataset using two lightweight model architectures."
   ],
   "p1": 3698,
   "pn": 3702,
   "doi": "10.21437/Interspeech.2025-442",
   "url": "interspeech_2025/masztalski25_interspeech.html"
  },
  "biswas25_interspeech": {
   "authors": [
    [
     "Astik",
     "Biswas"
    ],
    [
     "Oleg",
     "Shevelev"
    ],
    [
     "Amine",
     "Abdaoui"
    ],
    [
     "Vivek",
     "Tyagi"
    ],
    [
     "Abdelmoumene",
     "Boumadane"
    ]
   ],
   "title": "Adapting Whisper for low-resource Hindi-English Code-Mix speech with on-the-fly Augmentation &amp; LLM-Synthesised Data",
   "original": "447",
   "order": 875,
   "page_count": 5,
   "abstract": [
    "Code-switching (CS) automatic speech recognition (ASR) faces challenges due to language confusion from accents, acoustic similarities, and seamless transitions. In multilingual India, CS is prevalent, yet adapting pre-trained Whisper for low-resource Indian CS-ASR remains under-explored. This study explores strategies for adapting Whisper with limited data. First, we propose language prompts for fine-tuning and on-the-fly code-mixed data simulation to handle language switches. Second, we use Llama for few-shot code-switching (CS) text generation, coupled with audio synthesis, to create synthetic data for fine-tuning the Whisper model. Experiments on a Hindi-English CS dataset show promising results, demonstrating the techniques effectiveness. These findings are applicable to other multilingual contexts, aiding Whisper’s adaptation to new domains."
   ],
   "p1": 4293,
   "pn": 4297,
   "doi": "10.21437/Interspeech.2025-447",
   "url": "interspeech_2025/biswas25_interspeech.html"
  },
  "yang25d_interspeech": {
   "authors": [
    [
     "Junqi",
     "Yang"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Xin",
     "Zhao"
    ],
    [
     "Cedar",
     "Lin"
    ]
   ],
   "title": "Band-SCNet: A Causal, Lightweight Model for High-Performance Real-Time Music Source Separation",
   "original": "448",
   "order": 1015,
   "page_count": 5,
   "abstract": [
    "Music source separation (MSS) for real-time applications faces challenges due to significant performance loss. In this paper, we propose Band-SCNet, a lightweight real-time model that bridges the performance gap between real-time and non-real-time models. Band-SCNet combines Sparse Compression with Cross-band and Narrow-band Blocks to reduce model size and complexity while improving performance. We also introduce the Compressed Self-Attention (CSA) Fusion Module, which enhances efficiency by reducing parameters. Experiments on the MUSDB18-HQ dataset show that Band-SCNet outperforms existing real-time models with an SDR of 7.79 dB, 2.59 million parameters by relaxing to a 92 ms latency, confirming its suitability for real-time MSS. The model achieves a balance between performance, latency, and model size, offering a promising solution for real-time music source separation."
   ],
   "p1": 4973,
   "pn": 4977,
   "doi": "10.21437/Interspeech.2025-448",
   "url": "interspeech_2025/yang25d_interspeech.html"
  },
  "makishima25_interspeech": {
   "authors": [
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Naotaka",
     "Kawata"
    ],
    [
     "Taiga",
     "Yamane"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Satoshi",
     "Suzuki"
    ],
    [
     "Shota",
     "Orihashi"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "SOMSRED-SVC: Sequential Output Modeling with Speaker Vector Constraints for Joint Multi-Talker Overlapped ASR and Speaker Diarization",
   "original": "449",
   "order": 795,
   "page_count": 5,
   "abstract": [
    "We have developed a sequential output model with speaker vector constraints for the joint modeling of multi-talker automatic speech recognition (ASR) and speaker diarization. The conventional approach to joint modeling of multi-talker ASR and speaker diarization, called SOMSRED, enables the estimation of speaker embeddings from fully overlapped speech by discretizing the speaker embedding space and treating the speaker embeddings as tokens. However, the predicted speaker embeddings become less distinctive compared to the ones directly obtained from non-overlapping speech due to the discretization. To address this problem, we add a new training objective that optimizes speaker embeddings in continuous space without discretization. Experimental results show that the proposed method avoids overfitting to the discretized speaker tokens and outperforms SOMSRED in both ASR performance and speaker embedding performance."
   ],
   "p1": 3893,
   "pn": 3897,
   "doi": "10.21437/Interspeech.2025-449",
   "url": "interspeech_2025/makishima25_interspeech.html"
  },
  "makishima25b_interspeech": {
   "authors": [
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Naotaka",
     "Kawata"
    ],
    [
     "Taiga",
     "Yamane"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Satoshi",
     "Suzuki"
    ],
    [
     "Shota",
     "Orihashi"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Unified Audio-Visual Modeling for Recognizing Which Face Spoke When and What in Multi-Talker Overlapped Speech and Video",
   "original": "451",
   "order": 375,
   "page_count": 5,
   "abstract": [
    "We have developed a model that jointly recognizes which face spoke “when” and “what” from multi-talker overlapped speech and video of multiple speakers. For understanding of videos in which multiple speakers are speaking, it is important to recognize which face spoke “when” and “what” from the overlapped speech and multiple speakers’ videos. Conventional methods have to combine speech separation, active speaker detection, and audio-visual speech recognition to address this task. However, the combined system makes the system complex and suboptimal. To address this problem, our idea is to serialize “which face spoke when and what” of multiple speakers into a single token sequence, which is recursively estimated with the proposed unified model using multiple speakers’ videos and overlapped speech as input. Experimental results demonstrate the validity of the proposed method."
   ],
   "p1": 1838,
   "pn": 1842,
   "doi": "10.21437/Interspeech.2025-451",
   "url": "interspeech_2025/makishima25b_interspeech.html"
  },
  "ormaechea25_interspeech": {
   "authors": [
    [
     "Lucía",
     "Ormaechea"
    ],
    [
     "Nikos",
     "Tsourakis"
    ],
    [
     "Pierrette",
     "Bouillon"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Didier",
     "Schwab"
    ]
   ],
   "title": "Towards High-Quality LLM-Based Data for French Spontaneous Speech Simplification: an Exo-Refinement Approach",
   "original": "452",
   "order": 824,
   "page_count": 5,
   "abstract": [
    "This study explores the synthetic data generation capabilities of LLMs for French spontaneous speech simplification (S3), a low-resource NLP task. We introduce the exo-refinement approach, which builds on the self-reflect workflow but differs by using separate models for generation and evaluation. To address the limitations of single-model refinement, it integrates external feedback from distinct LLMs as judges, refining outputs based on 3 task-specific dimensions. Comparing expert-based simplifications from gold transcriptions to LLM synthetic outputs, results show that mistral-large outperforms all benchmarked models, including the MUSS baseline, and mistral-small achieves competitive performance with few refinements. SARI results confirm that iterations improve simplicity gain without compromising semantic meaning, as shown by COMET scores. These findings support exo-refinement as a scalable method for synthetic data generation and future S3 model development."
   ],
   "p1": 4038,
   "pn": 4042,
   "doi": "10.21437/Interspeech.2025-452",
   "url": "interspeech_2025/ormaechea25_interspeech.html"
  },
  "yoon25_interspeech": {
   "authors": [
    [
     "Hyungchan",
     "Yoon"
    ],
    [
     "Chanwoo",
     "Lee"
    ],
    [
     "Hoodong",
     "Lee"
    ],
    [
     "Stanley Jungkyu",
     "Choi"
    ]
   ],
   "title": "APTTS: Adversarial Post-training in Latent Flow Matching for Fast and High-fidelity Text-to-Speech",
   "original": "455",
   "order": 1125,
   "page_count": 5,
   "abstract": [
    "Although recent text-to-speech (TTS) models based on flow matching have achieved remarkable generation quality, their reliance on numerous sampling steps hinders practical deployment. In this work, we introduce an adversarial post-training strategy for flow matching TTS that significantly reduces the required sampling steps. Our approach treats a pre-trained flow matching model as a few-step generator, optimizing it with reconstruction and adversarial objectives. We integrate this technique into APTTS, our novel latent flow matching framework for zero-shot TTS, and demonstrate its superiority over state-of-the-art baselines with real-time applicability.  Furthermore, we validate the scalability of our adversarial post-training approach by applying it to Matcha-TTS, a publicly available flow matching model. Evaluations on a multi-speaker dataset show that our method enhances audio quality while reducing inference time, underscoring its potential as a scalable solution for real-time TTS."
   ],
   "p1": 5518,
   "pn": 5522,
   "doi": "10.21437/Interspeech.2025-455",
   "url": "interspeech_2025/yoon25_interspeech.html"
  },
  "marmor25_interspeech": {
   "authors": [
    [
     "Yanir",
     "Marmor"
    ],
    [
     "Yair",
     "Lifshitz"
    ],
    [
     "Yoad",
     "Snapir"
    ],
    [
     "Kinneret",
     "Misgav"
    ]
   ],
   "title": "Building an Accurate Open-Source Hebrew ASR System through Crowdsourcing",
   "original": "460",
   "order": 151,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) for Hebrew faces significant challenges due to limited resources and rich morphology. While recent advances have improved high-resource languages ASR, Hebrew still lacks robust open-source solutions. Through crowdsourcing efforts, we created a dataset of 314 hours of transcribed speech, which we used to train a new Hebrew ASR model based on the Whisper architecture. Our model demonstrates up to 29% reduction in error rates compared to existing Whisper solutions, particularly excelling in producing verbatim transcriptions. Additionally, we introduce a new evaluation dataset designed specifically for Hebrew ASR assessment. By making both the model and methodology freely available, we provide a framework that can be adapted for developing ASR systems in other under-resourced languages. This work represents a step toward making speech technology more accessible in different languages."
   ],
   "p1": 723,
   "pn": 727,
   "doi": "10.21437/Interspeech.2025-460",
   "url": "interspeech_2025/marmor25_interspeech.html"
  },
  "hiruta25_interspeech": {
   "authors": [
    [
     "Komei",
     "Hiruta"
    ],
    [
     "Yosuke",
     "Yamano"
    ],
    [
     "Hideaki",
     "Tamori"
    ]
   ],
   "title": "Hybrid Data Sampling for ASR: Integrating Acoustic Diversity and Transcription Uncertainty",
   "original": "462",
   "order": 873,
   "page_count": 5,
   "abstract": [
    "Efficiently selecting training data is crucial for improving automatic speech recognition (ASR) models while minimizing annotation costs. This research extends TypiClust, a data sampling method originally validated for images, to ASR by jointly considering acoustic diversity and transcription uncertainty. Our method clusters speech embeddings from Wav2Vec2 and prioritizes typical and low transcription-confidence samples within each cluster, ensuring selection of representative and hard-to-train samples. We evaluate our method on Japanese speech datasets, CSJ and ReazonSpeech, demonstrating that it achieves lower recognition error rates than random or single-criterion-based data selection. These results indicate that selecting data based on both diversity and uncertainty enhances speech recognition model performance while reducing annotation costs."
   ],
   "p1": 4283,
   "pn": 4287,
   "doi": "10.21437/Interspeech.2025-462",
   "url": "interspeech_2025/hiruta25_interspeech.html"
  },
  "vaessen25_interspeech": {
   "authors": [
    [
     "Nik",
     "Vaessen"
    ],
    [
     "Roeland",
     "Ordelman"
    ],
    [
     "David A.",
     "van Leeuwen"
    ]
   ],
   "title": "Self-supervised learning of speech representations with Dutch archival data",
   "original": "463",
   "order": 248,
   "page_count": 5,
   "abstract": [
    "This paper explores the use of Dutch archival television broadcast data for self-supervised learning of speech foundation models, specifically wav2vec 2.0. We first study data quality assumptions for pre-training, and show how music, noise and speaker overlap affect SSL convergence and downstream fine-tuning performance. Secondly, we explore effectively pre-processing strategies to convert the noisy broadcast dataset into a qualitative dataset for pre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual and multi-lingual pre-training with equivalent amounts of data, and show that mono-lingual pre-training is more robust to out-of-domain data. Lastly, we achieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a continuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55 k hour archival dataset."
   ],
   "p1": 1208,
   "pn": 1212,
   "doi": "10.21437/Interspeech.2025-463",
   "url": "interspeech_2025/vaessen25_interspeech.html"
  },
  "zhao25d_interspeech": {
   "authors": [
    [
     "Junchuan",
     "Zhao"
    ],
    [
     "Xintong",
     "Wang"
    ],
    [
     "Ye",
     "Wang"
    ]
   ],
   "title": "Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning",
   "original": "464",
   "order": 995,
   "page_count": 5,
   "abstract": [
    "Recent advances in discrete audio codecs have significantly improved speech representation modeling, while codec language models have enabled in-context learning for zero-shot speech synthesis. Inspired by this, we propose a voice conversion (VC) model within the VALLE-X framework, leveraging its strong in-context learning capabilities for speaker adaptation. To enhance prosody control, we introduce a prosody-aware audio codec encoder (PACE) module, which isolates and refines prosody from other sources, improving expressiveness and control. By integrating PACE into our VC model, we achieve greater flexibility in prosody manipulation while preserving speaker timbre. Experimental evaluation results demonstrate that our approach outperforms baseline VC systems in prosody preservation, timbre consistency, and overall naturalness, surpassing baseline VC systems."
   ],
   "p1": 4893,
   "pn": 4897,
   "doi": "10.21437/Interspeech.2025-464",
   "url": "interspeech_2025/zhao25d_interspeech.html"
  },
  "woszczyk25_interspeech": {
   "authors": [
    [
     "Dominika C",
     "Woszczyk"
    ],
    [
     "Ranya",
     "Aloufi"
    ],
    [
     "Soteris",
     "Demetriou"
    ]
   ],
   "title": "ClaritySpeech: Dementia Obfuscation in Speech",
   "original": "465",
   "order": 293,
   "page_count": 5,
   "abstract": [
    "Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.12"
   ],
   "p1": 1433,
   "pn": 1437,
   "doi": "10.21437/Interspeech.2025-465",
   "url": "interspeech_2025/woszczyk25_interspeech.html"
  },
  "wu25b_interspeech": {
   "authors": [
    [
     "Ho-Hsiang",
     "Wu"
    ],
    [
     "Wei-Cheng",
     "Lin"
    ],
    [
     "Abinaya",
     "Kumar"
    ],
    [
     "Luca",
     "Bondi"
    ],
    [
     "Shabnam",
     "Ghaffarzadegan"
    ],
    [
     "Juan Pablo",
     "Bello"
    ]
   ],
   "title": "Towards Few-Shot Training-Free Anomaly Sound Detection",
   "original": "467",
   "order": 688,
   "page_count": 5,
   "abstract": [
    "Anomaly sound detection is an important audio task, with various applications in industrial monitoring, healthcare, security and surveillance, and other domains. Existing methods are often designed with various assumptions for specific types of anomalies and are not suitable when applied to the challenging real-world scenarios, such as domain shifts and data scarcity issues. In this work, we propose a few-shot training-free method, leveraging pre-trained audio models to extract patch-based spatial-temporal representations for few-shot anomaly detection and segmentation. We show that our proposed approach is flexible and applicable even under the extreme low-shot regimes, and can at times outperform models trained with the full datasets. Furthermore, our method is more robust in tackling domain shifts, with the need of only few-shot data points to quickly adapt to various conditions, therefore more suitable for deployment in real-world applications."
   ],
   "p1": 3384,
   "pn": 3388,
   "doi": "10.21437/Interspeech.2025-467",
   "url": "interspeech_2025/wu25b_interspeech.html"
  },
  "li25e_interspeech": {
   "authors": [
    [
     "Jiaqi",
     "Li"
    ],
    [
     "Xiaolong",
     "Lin"
    ],
    [
     "Zhekai",
     "Li"
    ],
    [
     "Shixi",
     "Huang"
    ],
    [
     "Yuancheng",
     "Wang"
    ],
    [
     "Chaoren",
     "Wang"
    ],
    [
     "Zhenpeng",
     "Zhan"
    ],
    [
     "Zhizheng",
     "Wu"
    ]
   ],
   "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
   "original": "468",
   "order": 993,
   "page_count": 5,
   "abstract": [
    "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec."
   ],
   "p1": 4883,
   "pn": 4887,
   "doi": "10.21437/Interspeech.2025-468",
   "url": "interspeech_2025/li25e_interspeech.html"
  },
  "naseem25_interspeech": {
   "authors": [
    [
     "Fatima",
     "Naseem"
    ],
    [
     "Maham",
     "Sajid"
    ],
    [
     "Farah",
     "Adeeba"
    ],
    [
     "Sahar",
     "Rauf"
    ],
    [
     "Asad",
     "Mustafa"
    ],
    [
     "Sarmad",
     "Hussain"
    ]
   ],
   "title": "Developing High-Quality TTS for Punjabi and Urdu: Benchmarking against MMS Models",
   "original": "469",
   "order": 332,
   "page_count": 5,
   "abstract": [
    "Existing Punjabi text-to-speech (TTS) solutions focus on Gurumukhi script, requiring transliteration from Shahmukhi. This leads to letter substitutions and omissions, resulting in pronunciation errors. In this study, speech corpus, phonetic lexicon, and text analysis module for Punjabi Shahmukhi were developed. Two model architectures: Tacotron 1 and Tacotron 2 with WaveGlow were used to build TTS models. In addition to Punjabi, Urdu TTS models were also developed. These models were benchmarked against Urdu and Punjabi Gurumukhi TTS models provided by Meta’s Massively Multilingual Speech (MMS) which is a top profile multilingual speech project. Objective and subjective evaluations indicate that tacotron based Urdu and Punjabi models outperform MMS in intelligibility, naturalness, and phonetic accuracy, enhancing TTS quality for these languages."
   ],
   "p1": 1628,
   "pn": 1632,
   "doi": "10.21437/Interspeech.2025-469",
   "url": "interspeech_2025/naseem25_interspeech.html"
  },
  "stan25_interspeech": {
   "authors": [
    [
     "Adriana",
     "Stan"
    ],
    [
     "David",
     "Combei"
    ],
    [
     "Dan",
     "Oneata"
    ],
    [
     "Horia",
     "Cucu"
    ]
   ],
   "title": "TADA: Training-free Attribution and Out-of-Domain Detection of Audio Deepfakes",
   "original": "472",
   "order": 315,
   "page_count": 5,
   "abstract": [
    "Deepfake detection has gained significant attention across audio, text, and image modalities, with high accuracy in distinguishing real from fake. However, identifying the exact source—such as the system or model behind a deepfake—remains a less studied problem. In this paper, we take a significant step forward in audio deepfake model attribution or source tracing by proposing a training-free, green AI approach based entirely on k-Nearest Neighbors (kNN). Leveraging a pre-trained self-supervised learning (SSL) model, we show that grouping samples from the same generator is straightforward–we obtain an 0.93 F1-score across five deepfake datasets. The method also demonstrates strong out-of-domain (OOD) detection, effectively identifying samples from unseen models at an F1-score of 0.84. We further analyse these results in a multi-dimensional approach and provide additional insights. All code and data protocols used in this work are available in our open repository."
   ],
   "p1": 1543,
   "pn": 1547,
   "doi": "10.21437/Interspeech.2025-472",
   "url": "interspeech_2025/stan25_interspeech.html"
  },
  "dai25_interspeech": {
   "authors": [
    [
     "Lipeng",
     "Dai"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Shengyu",
     "Peng"
    ],
    [
     "Yu",
     "Guan"
    ],
    [
     "Wu",
     "Guo"
    ]
   ],
   "title": "Leveraging Multi-Level Features of ATST with Conformer-Based Dual-Branch Network for Sound Event Detection",
   "original": "474",
   "order": 530,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a Conformer-based dual-branch framework to fully exploit the multi-layer features of pre-trained model (PTM) for sound event detection (SED). The proposed model follows the mainstream framework, consisting of a front-end encoder built upon the pre-trained Audio Teacher-Student Transformer (ATST) model and a back-end context network. For the front-end, we evenly divide the Transformer layers of ATST into shallow and deep parts, each of which is fused using weighted integration to effectively incorporate multi-layer features for SED. In the back-end, dual-branch Conformer is used to extract both high-level and low-level clues from the aggregated features. Furthermore, we adopt adapter modules instead of full fine-tuning the ATST model in the training stage, achieving comparable performance with far fewer parameters. We carry out experiments on the validation set for Task 4 of DCASE 2024 Challenge, and results demonstrate the efficacy of the proposed method."
   ],
   "p1": 2595,
   "pn": 2599,
   "doi": "10.21437/Interspeech.2025-474",
   "url": "interspeech_2025/dai25_interspeech.html"
  },
  "su25b_interspeech": {
   "authors": [
    [
     "Hang",
     "Su"
    ],
    [
     "Yuxiang",
     "Kong"
    ],
    [
     "Lichun",
     "Fan"
    ],
    [
     "Jian",
     "Luan"
    ]
   ],
   "title": "Text-Enhanced Audio Encoder for Large Language Model based Speech Recognition via Cross-Modality Pre-training with Unpaired Audio-Text Data",
   "original": "476",
   "order": 120,
   "page_count": 5,
   "abstract": [
    "Recent advances in Large Language Models (LLM) have brought a new architecture for Automatic Speech Recognition (ASR) tasks, where an audio encoder is followed by a powerful LLM. Refining audio embeddings from the audio encoder to better align textual embeddings can enhance performance of LLM-based ASR. However, current LLM-based ASR research mainly focuses on aligning textual and audio features via paired audio-text data. The use of unpaired audio-text data for such alignment remains under-explored. This paper proposes a cross-modality pre-training method using readily available unpaired audio-text data to better align the audio embeddings to text modality. Experimental results show that using this text-enhanced audio encoder in LLM-based ASR significantly outperforms using the audio encoder pre-trained only with audio data. This method has great potential for further improvement with plentiful easily accessible unpaired audio-text data."
   ],
   "p1": 569,
   "pn": 573,
   "doi": "10.21437/Interspeech.2025-476",
   "url": "interspeech_2025/su25b_interspeech.html"
  },
  "rouas25_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Charles",
     "Brazier"
    ],
    [
     "Leila",
     "Ben Letaifa"
    ],
    [
     "Rafael",
     "Medina"
    ],
    [
     "Pedro",
     "Palacios"
    ],
    [
     "David",
     "Atienza"
    ],
    [
     "Giovanni",
     "Ansaloni"
    ]
   ],
   "title": "Structured pruning for efficient systolic array accelerated cascade Speech-to-Text Translation",
   "original": "478",
   "order": 185,
   "page_count": 5,
   "abstract": [
    "We present in this paper a simple method for pruning tiles of weights in sparse matrices, that do not require fine-tuning or retraining. This method is applied here to the feed-forward layers of transformers. We assess in a first experiment the impact of such pruning on the performances of speech recognition, machine translation, and the cascaded speech-to-text translation, on the MuST-C database, for the English to French direction. Depending on the size of the pruned tiles (from 4x4 to 32x32), we observe that pruning rates from 15 to 40% for speech recognition and from 40 to 70% for machine translation are feasible for a performance degradation of 10%. Applying this pruning method to the systolic array accelerated version of the cascade speech-to-text translation system results in speedups up to 74x compared to the non-accelerated system. Energy consumption also benefits from structured pruning with a maximum reduction of 35%."
   ],
   "p1": 893,
   "pn": 897,
   "doi": "10.21437/Interspeech.2025-478",
   "url": "interspeech_2025/rouas25_interspeech.html"
  },
  "altwlkany25_interspeech": {
   "authors": [
    [
     "Kemal",
     "Altwlkany"
    ],
    [
     "Amar",
     "Kuric"
    ],
    [
     "Emanuel",
     "Lacic"
    ]
   ],
   "title": "On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs",
   "original": "481",
   "order": 276,
   "page_count": 5,
   "abstract": [
    "In recent years, there has been a growing focus on fairness and inclusivity within speech technology, particularly in areas such as automatic speech recognition and speech sentiment analysis. When audio is transcoded prior to processing, as is the case in streaming or real-time applications, any inherent bias in the coding mechanism may result in disparities. This not only affects user experience but can also have broader societal implications by perpetuating stereotypes and exclusion. Thus, it is important that audio coding mechanisms are unbiased. In this work, we contribute towards the scarce research with respect to language and gender biases of audio codecs. By analyzing the speech quality of over 2 million multilingual audio files after transcoding through a representative subset of codecs (PSTN, VoIP and neural), our results indicate that PSTN codecs are strongly biased in terms of gender and that neural codecs introduce language biases."
   ],
   "p1": 1348,
   "pn": 1352,
   "doi": "10.21437/Interspeech.2025-481",
   "url": "interspeech_2025/altwlkany25_interspeech.html"
  },
  "birkholz25_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Tianyi",
     "Zhang"
    ]
   ],
   "title": "Evaluation of a model for sound radiation from the vocal tract wall",
   "original": "482",
   "order": 78,
   "page_count": 5,
   "abstract": [
    "This study evaluated a simulation model for sound radiation from the vocal tract wall in the context of articulatory speech synthesis. In this model, the vocal tract is represented in terms of incremental contiguous tube sections, where the wall of each section reacts to the local sound pressure like a damped spring-mass systems. The surface-radiated sound from this model was compared with that of six real speakers by means of the voicebars of /b,d,g/ in different vowel contexts. Based on this, the wall parameters of the real speakers were estimated by fitting the simulated to the real voicebars. The parameter optimization allowed a close reproduction of the natural voicebar spectra with root-mean-square errors between 2.26 dB and 3.82 dB in the frequency range from 0-800 Hz. Hence, despite its simplicity, the modeling method is very well suited for articulatory speech synthesis."
   ],
   "p1": 359,
   "pn": 363,
   "doi": "10.21437/Interspeech.2025-482",
   "url": "interspeech_2025/birkholz25_interspeech.html"
  },
  "sharma25_interspeech": {
   "authors": [
    [
     "Chandra Mohan",
     "Sharma"
    ],
    [
     "Arnab Kumar",
     "Roy"
    ],
    [
     "Anupam",
     "Mandal"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ],
    [
     "Prasanna Kumar",
     "Kr"
    ]
   ],
   "title": "Boosting StoRM Convergence with Metric Guidance and Non-uniform State-Sampling for Optimal Dereverberation",
   "original": "483",
   "order": 779,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel approach to address late reverberation, which degrades speech intelligibility by convolving clean speech with room impulse response. Our method combines metric-guided training and non-uniform state sampling within the Stochastic Regeneration Model (StoRM) diffusion architecture, enabling better diffusion variability modeling while maintaining computational efficiency. Key metrics such as STFT loss, spectral convergence loss, Mel Frequency Cepstral Coefficient (MFCC) loss and log-magnitude loss guide the regeneration process, improving convergence by reducing training epochs by ~19.6% with slight improvements in dereverberation. Meanwhile, the non-linear state sampling approach enhances training convergence by ~27.2% with practically similar perceptual performance. We evaluate the impact of these modifications on automatic speech recognition and clean speech distortion relative to the baseline, demonstrating optimal speech-quality-aware performance."
   ],
   "p1": 3813,
   "pn": 3817,
   "doi": "10.21437/Interspeech.2025-483",
   "url": "interspeech_2025/sharma25_interspeech.html"
  },
  "han25_interspeech": {
   "authors": [
    [
     "Jiangyu",
     "Han"
    ],
    [
     "Federico",
     "Landini"
    ],
    [
     "Johan",
     "Rohdin"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Jan",
     "Černocký"
    ],
    [
     "Lukáš",
     "Burget"
    ]
   ],
   "title": "Fine-tune Before Structured Pruning: Towards Compact and Accurate Self-Supervised Models for Speaker Diarization",
   "original": "484",
   "order": 323,
   "page_count": 5,
   "abstract": [
    "Self-supervised learning (SSL) models like WavLM can be effectively utilized when building speaker diarization systems but are often large and slow, limiting their use in resource-constrained scenarios. Previous studies have explored compression techniques, but usually for the price of degraded performance at high pruning ratios. In this work, we propose to compress SSL models through structured pruning by introducing knowledge distillation. Different from the existing works, we emphasize the importance of fine-tuning SSL models before pruning. Experiments on far-field single-channel AMI, AISHELL-4, and AliMeeting datasets show that our method can remove redundant parameters of WavLM Base+ and WavLM Large by up to 80% without any performance degradation. After pruning, the inference speeds on a single GPU for the Base+ and Large models are 4.0 and 2.6 times faster, respectively. Our source code is publicly available."
   ],
   "p1": 1583,
   "pn": 1587,
   "doi": "10.21437/Interspeech.2025-484",
   "url": "interspeech_2025/han25_interspeech.html"
  },
  "sun25b_interspeech": {
   "authors": [
    [
     "Qi",
     "Sun"
    ],
    [
     "Ziyue",
     "Qiu"
    ],
    [
     "Yu",
     "Pu"
    ],
    [
     "Jinpeng",
     "Li"
    ],
    [
     "Xuchu",
     "Chen"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ]
   ],
   "title": "PPGs-BERT: Leveraging Phoneme Sequence and BERT for Alzheimer’s Disease Detection from Spontaneous Speech",
   "original": "489",
   "order": 117,
   "page_count": 5,
   "abstract": [
    "Alzheimer&#x27;s Disease (AD) is a neurodegenerative condition characterized by linguistic impairments. While ASR and LLMs show promise in AD detection, ASR often normalizes key AD-related speech patterns and faces cross-lingual challenges due to language dependencies. Besides, ASR training demands extensive matched data. In our paper, however, we employ a phoneme recognizer as a frontend tokenizer. Provided it has comprehensive phoneme coverage, a multitude of linguistic phenomena can be represented via phoneme sequences, including hesitations, repetitions, pauses, mispronunciations, and even distinctions between different language identities that are crucial for AD detection. Furthermore, the BERT model is employed to extract high-dimensional features from the Phonetic PosteriorGrams (PPGs), which are ultimately used to diagnose Alzheimer&#x27;s disease. Our approach offers cross-lingual applicability, achieves competitive accuracy, and maintains computational efficiency."
   ],
   "p1": 554,
   "pn": 558,
   "doi": "10.21437/Interspeech.2025-489",
   "url": "interspeech_2025/sun25b_interspeech.html"
  },
  "tanner25_interspeech": {
   "authors": [
    [
     "James",
     "Tanner"
    ],
    [
     "Morgan",
     "Sonderegger"
    ],
    [
     "Jane",
     "Stuart-Smith"
    ],
    [
     "Jeff",
     "Mielke"
    ],
    [
     "Tyler",
     "Kendall"
    ]
   ],
   "title": "Automatic classification of stop realisation with wav2vec2.0",
   "original": "491",
   "order": 465,
   "page_count": 5,
   "abstract": [
    "Modern phonetic research regularly makes use of automatic tools for the annotation of speech data, however few tools exist for the annotation of many variable phonetic phenomena. At the same time, pre-trained self-supervised models, such as wav2vec2.0, have been shown to perform well at speech classification tasks and latently encode fine-grained phonetic information. We demonstrate that wav2vec2.0 models can be trained to automatically classify stop burst presence with high accuracy in both English and Japanese, robust across both finely-curated and unprepared speech corpora. Patterns of variability in stop realisation are replicated with the automatic annotations, and closely follow those of manual annotations. These results demonstrate the potential of pre-trained speech models as tools for the automatic annotation and processing of speech corpus data, enabling researchers to &#x27;scale-up&#x27; the scope of phonetic research with relative ease."
   ],
   "p1": 2270,
   "pn": 2274,
   "doi": "10.21437/Interspeech.2025-491",
   "url": "interspeech_2025/tanner25_interspeech.html"
  },
  "harrington25_interspeech": {
   "authors": [
    [
     "Lauren",
     "Harrington"
    ],
    [
     "Vincent",
     "Hughes"
    ],
    [
     "Philip",
     "Harrison"
    ],
    [
     "Paul",
     "Foulkes"
    ],
    [
     "Jessica",
     "Wormald"
    ],
    [
     "Finnian",
     "Kelly"
    ],
    [
     "David",
     "van der Vloed"
    ]
   ],
   "title": "Variability in performance across four generations of automatic speaker recognition systems",
   "original": "494",
   "order": 815,
   "page_count": 5,
   "abstract": [
    "The field of automatic speaker recognition (ASR) has seen a series of generational changes to speaker modelling approaches in the last 3 decades. Adoption of new approaches has mainly been driven by improvements observed in overall system-level performance metrics on common datasets. There is now considerable debate within the field around understanding why systems perform better for some speakers than others. In this study, we compare the performance of 4 generations of ASR systems with the same set of forensically-relevant test and calibration data. On a system- and individual speaker-level, we observe improvements from GMM-UBM to i-vector to x-vector but not for ECAPA-TDNN. We find that certain individuals remain difficult to recognise across all systems. Our findings show that both file- and speaker-level factors contribute to the performance of individual speakers and systems overall, which supports calls for more detailed exploration of system performance."
   ],
   "p1": 3993,
   "pn": 3997,
   "doi": "10.21437/Interspeech.2025-494",
   "url": "interspeech_2025/harrington25_interspeech.html"
  },
  "miniconi25_interspeech": {
   "authors": [
    [
     "Natacha",
     "Miniconi"
    ],
    [
     "Meysam",
     "Shamsi"
    ],
    [
     "Anthony",
     "Larcher"
    ]
   ],
   "title": "When The MOS Predictor Asks For Training Annotation In Cross Lingual/Domain Adaptation",
   "original": "496",
   "order": 521,
   "page_count": 5,
   "abstract": [
    "The Mean Opinion Score (MOS) is widely used to assess speech synthesis quality, but requires costly human evaluation. Automatic MOS predictors have been developed to estimate MOS. Training and generalizing these predictors across different languages and domains remain difficult due to the high cost of labeled data. To optimize MOS prediction while minimizing human annotation efforts, we explore active learning. As far as we know, this is the first study to investigate the use of active learning as training sample selection strategies for enhancing MOS prediction. We investigate its effectiveness on two tasks: cross-domain and cross-lingual adaptation, comparing multiple selection strategies that rely on uncertainty or diversity measures. Our sample selection strategies have been compared with random selection. Among these strategies, Monte Carlo (MC) Dropout proved effective for cross-lingual adaptation, while perturbation noise performed well for cross-domain adaptation."
   ],
   "p1": 2550,
   "pn": 2554,
   "doi": "10.21437/Interspeech.2025-496",
   "url": "interspeech_2025/miniconi25_interspeech.html"
  },
  "schuster25_interspeech": {
   "authors": [
    [
     "Jan",
     "Schuster"
    ],
    [
     "Alexander",
     "Wölfel"
    ],
    [
     "Fabian",
     "Brunner"
    ],
    [
     "Christian",
     "Bergler"
    ]
   ],
   "title": "PredTrAD – Prediction-based Transformer for Anomaly Detection in Multivariate Time Series Data",
   "original": "501",
   "order": 791,
   "page_count": 5,
   "abstract": [
    "Anomaly detection in multivariate time series data is an extensive field of research with significant impact on a broad spectrum of real-world applications. Building a reliable anomaly detection system is extremely challenging due to data imbalance, lack of labels, next to the actual definition of anomalies. The current study proposes a new transformer-based approach, together with a publicly available time series dataset – the TIKI data corpus – comprising various performance metrics of a Kubernetes cluster distributed over roughly 500 million timestamps. The proposed network, entitled PredTrAD, is compared with the state-of-the-art anomaly detection models – TranAD and DTAAD – in several experiments: (1) verification on three well-known benchmark corpora, (2) evaluation on the TIKI data, and (3) cross-entity anomaly detection. Across all experimental constellations, the proposed model has shown a significant performance growth, proven by F1 score enhancements of 2.41% up to 16.81%."
   ],
   "p1": 3873,
   "pn": 3877,
   "doi": "10.21437/Interspeech.2025-501",
   "url": "interspeech_2025/schuster25_interspeech.html"
  },
  "hamann25_interspeech": {
   "authors": [
    [
     "Silke",
     "Hamann"
    ],
    [
     "Andrea",
     "Alićehajić"
    ]
   ],
   "title": "Are loan sequences different from foreign sequences? A perception study with Japanese listeners on coronal obstruent – high front vowel sequences",
   "original": "502",
   "order": 24,
   "page_count": 5,
   "abstract": [
    "Native phonotactics influences speech perception, as numerous studies have shown. The present study tackles the question whether there is a difference in perceptual performance if the involved sequence occurs only in loanwords, compared to a sequence that does not occur at all in the native language. This was tested with the native Japanese sequences of palatal affricate plus /i/, compared to /ti/ (accepted only in loanwords) versus /zi/ (not accepted in Japanese) in an online AX discrimination task with 39 Japanese speakers (21-63 years old), who also had to answer three questions on their received English input. Participants performed significantly better at discriminating the accepted loan sequence /ti/, though discrimination of the foreign sequence /zi/ was also quite high (ranging from 40-100% correct). The results indicate that discriminability is only partly guided by native phonotactics. A potential role of amount of English input measured by self-report could not be attested."
   ],
   "p1": 111,
   "pn": 115,
   "doi": "10.21437/Interspeech.2025-502",
   "url": "interspeech_2025/hamann25_interspeech.html"
  },
  "gu25b_interspeech": {
   "authors": [
    [
     "Tianteng",
     "Gu"
    ],
    [
     "Bei",
     "Liu"
    ],
    [
     "Haoyu",
     "Wang"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Ultra-Low Bit Post-Training Quantization of Large Speech Models via K-Means Clustering and Mixed Precision Allocation",
   "original": "503",
   "order": 405,
   "page_count": 5,
   "abstract": [
    "Large speech foundation models like Whisper face significant deployment challenges due to their massive storage requirements. While post-training quantization (PTQ) offers a practical compression solution, existing methods suffer severe performance degradation below 8 bits, particularly for transformer-based architectures with prevalent weight outliers. We propose an ultra-low bit PTQ framework combining three key innovations: 1) K-means clustering for distribution-aware nonlinear quantization, 2) Mixed-precision allocation based on columnwise outlier density, and 3) Selective retention of critical outliers in sparse FP32 format. Evaluated on Whisper-Large-V3 (1.5B parameters), our method achieves 2.12-bit quantization with only 0.17% absolute WER increase on LibriSpeech test-clean. The approach also maintains whisper&#x27;s robust capabilities, showing less than 1% WER degradation across multiple dataset."
   ],
   "p1": 1988,
   "pn": 1992,
   "doi": "10.21437/Interspeech.2025-503",
   "url": "interspeech_2025/gu25b_interspeech.html"
  },
  "fan25_interspeech": {
   "authors": [
    [
     "Xulin",
     "Fan"
    ],
    [
     "Jialu",
     "Li"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Nancy L.",
     "McElwain"
    ]
   ],
   "title": "Band-Split Self-supervised Mamba for Infant-centered Audio Analysis",
   "original": "504",
   "order": 570,
   "page_count": 5,
   "abstract": [
    "Infant-worn audio recorders provide a valuable means to analyze an infant’s home environment and vocal interactions with family members. Recent advances in self-supervised learning on large unlabeled datasets and supervised training on limited annotated data have improved performance in this domain. However, data scarcity remains a challenge. We introduce Band-Split SSAMBA (BS-SSAMBA), a self-supervised representation learning method that incorporates band-specific projections and a band-agnostic Mamba encoder to model temporal relationships across frequency bands. Designed for data-efficient learning, BS-SSAMBA effectively leverages both unlabeled and labeled in-domain data. Through extensive experiments on family audio recordings, we show that BS-SSAMBA outperforms vanilla SSAMBA and wav2vec2-based models, demonstrating its effectiveness for infant-centered audio tagging."
   ],
   "p1": 2795,
   "pn": 2799,
   "doi": "10.21437/Interspeech.2025-504",
   "url": "interspeech_2025/fan25_interspeech.html"
  },
  "pepino25_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Pepino"
    ],
    [
     "Pablo",
     "Riera"
    ],
    [
     "Luciana",
     "Ferrer"
    ]
   ],
   "title": "EnCodecMAE: leveraging neural codecs for universal audio representation learning",
   "original": "506",
   "order": 716,
   "page_count": 5,
   "abstract": [
    "Universal audio representation learning aims to obtain foundational models that are useful for diverse tasks involving speech, music and environmental sounds. To achieve this, often methods inspired by self-supervised models from NLP (e.g., BERT), and vision, like masked autoencoders (MAE), are adapted to the audio domain. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for a MAE-based universal audio model. We evaluate our approach, EnCodecMAE, across various tasks and find that, on average, it outperforms state-of-the-art audio representation models. Moreover, we analyze the impact of several factors on downstream performance, concluding that increasing model size leads to performance improvements, that the optimal input representation depends on the type of task, that self-training is benefical and that diversity in the training dataset is essential to achieve good performance across different audio tasks."
   ],
   "p1": 3519,
   "pn": 3523,
   "doi": "10.21437/Interspeech.2025-506",
   "url": "interspeech_2025/pepino25_interspeech.html"
  },
  "zhou25_interspeech": {
   "authors": [
    [
     "Haoran",
     "Zhou"
    ],
    [
     "Xingchen",
     "Song"
    ],
    [
     "Brendan",
     "Fahy"
    ],
    [
     "Qiaochu",
     "Song"
    ],
    [
     "Binbin",
     "Zhang"
    ],
    [
     "Zhendong",
     "Peng"
    ],
    [
     "Anshul",
     "Wadhawan"
    ],
    [
     "Denglin",
     "Jiang"
    ],
    [
     "Apurv",
     "Verma"
    ],
    [
     "Vinay",
     "Ramesh"
    ],
    [
     "Srivas",
     "Prasad"
    ],
    [
     "Michele M.",
     "Franceschini"
    ]
   ],
   "title": "Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding",
   "original": "511",
   "order": 902,
   "page_count": 5,
   "abstract": [
    "OpenAI Whisper is a family of robust Automatic Speech Recognition (ASR) models trained on 680,000 hours of audio. However, its encoder-decoder architecture, trained with a sequence-to-sequence objective, lacks native support for streaming ASR. In this paper, we fine-tune Whisper for streaming ASR using the WeNet toolkit by adopting a Unified Two-pass (U2) structure. We introduce an additional Connectionist Temporal Classification (CTC) decoder trained with causal attention masks to generate streaming partial transcripts, while the original Whisper decoder reranks these partial outputs. Our experiments on LibriSpeech and an earnings call dataset demonstrate that, with adequate fine-tuning data, Whisper can be adapted into a capable streaming ASR model. We also introduce a hybrid tokenizer approach, which uses a smaller token space for the CTC decoder while retaining Whisper’s original token space for the attention decoder, resulting in improved data efficiency and generalization."
   ],
   "p1": 4428,
   "pn": 4432,
   "doi": "10.21437/Interspeech.2025-511",
   "url": "interspeech_2025/zhou25_interspeech.html"
  },
  "li25f_interspeech": {
   "authors": [
    [
     "Chin-Jou",
     "Li"
    ],
    [
     "Eunjung",
     "Yeo"
    ],
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Masao",
     "Someki"
    ],
    [
     "Rohan Kumar",
     "Das"
    ],
    [
     "Zhengjun",
     "Yue"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "David R.",
     "Mortensen"
    ]
   ],
   "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages",
   "original": "512",
   "order": 433,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics."
   ],
   "p1": 2128,
   "pn": 2132,
   "doi": "10.21437/Interspeech.2025-512",
   "url": "interspeech_2025/li25f_interspeech.html"
  },
  "huo25_interspeech": {
   "authors": [
    [
     "Robin",
     "Huo"
    ],
    [
     "Ewan",
     "Dunbar"
    ]
   ],
   "title": "Iterative Refinement, Not Training Objective, Makes HuBERT Behave Differently from wav2vec 2.0",
   "original": "514",
   "order": 54,
   "page_count": 5,
   "abstract": [
    "Self-supervised models for speech representation learning now see widespread use for their versatility and performance on downstream tasks, but the effect of model architecture on the linguistic information learned in their representations remains under-studied. This study investigates two such models, HuBERT and wav2vec 2.0, and minimally compares two of their architectural differences: training objective and iterative pseudo-label refinement through multiple training iterations. We find that differences in canonical correlation of hidden representations to word identity, phoneme identity, and speaker identity are explained by training iteration, not training objective. We suggest that future work investigate the reason for the effectiveness of iterative refinement in encoding linguistic information in self-supervised speech representations."
   ],
   "p1": 261,
   "pn": 265,
   "doi": "10.21437/Interspeech.2025-514",
   "url": "interspeech_2025/huo25_interspeech.html"
  },
  "goebiowska25_interspeech": {
   "authors": [
    [
     "Magdalena",
     "Gołębiowska"
    ],
    [
     "Piotr",
     "Syga"
    ]
   ],
   "title": "EmoSpeechAuth: Emotion-Aware Speaker Verification",
   "original": "515",
   "order": 1170,
   "page_count": 5,
   "abstract": [
    "Speaker verification is a process of verifying the identity of a user. Research has shown that emotional variability in speech degrades the performance of speaker verification tasks. Prior approaches were more computationally expensive and did not focus on the state-of-the-art speaker representations. In this paper, we propose a novel framework for constructing emotional speaker embeddings. Our framework utilizes pre-trained state-of-the-art feature extractors for speaker and emotion recognition, including both speaker and emotional information in the final embeddings. We present results of speaker verification on emotional speech datasets. We show that fusing ECAPA2 speaker representations and emotional features from emotion2vec with a cross-attention module improves EER by 8.29 percentage points compared to the baseline."
   ],
   "p1": 5743,
   "pn": 5747,
   "doi": "10.21437/Interspeech.2025-515",
   "url": "interspeech_2025/goebiowska25_interspeech.html"
  },
  "cumlin25_interspeech": {
   "authors": [
    [
     "Fredrik",
     "Cumlin"
    ],
    [
     "Xinyu",
     "Liang"
    ],
    [
     "Victor",
     "Ungureanu"
    ],
    [
     "Chandan K.A.",
     "Reddy"
    ],
    [
     "Christian",
     "Schüldt"
    ],
    [
     "Saikat",
     "Chatterjee"
    ]
   ],
   "title": "Multivariate Probabilistic Assessment of Speech Quality",
   "original": "518",
   "order": 1104,
   "page_count": 5,
   "abstract": [
    "The mean opinion score (MOS) is a standard metric for assessing speech quality, but its singular focus fails to identify specific distortions when low scores are observed. The NISQA dataset addresses this limitation by providing ratings across four additional dimensions: noisiness, coloration, discontinuity, and loudness, alongside MOS. In this paper, we extend the explored univariate MOS estimation to a multivariate framework by modeling these dimensions jointly using a multivariate Gaussian distribution. Our approach utilizes Cholesky decomposition to predict covariances without imposing restrictive assumptions, and extends probabilistic affine transformations to a multivariate context. Experimental results show that our model performs on par with state-of-the-art methods in point estimation, while uniquely providing uncertainty and correlation estimates across speech quality dimensions. This enables better diagnosis of poor speech quality and inform targeted improvements."
   ],
   "p1": 5413,
   "pn": 5417,
   "doi": "10.21437/Interspeech.2025-518",
   "url": "interspeech_2025/cumlin25_interspeech.html"
  },
  "escobargrisales25_interspeech": {
   "authors": [
    [
     "Daniel",
     "Escobar-Grisales"
    ],
    [
     "Cristian David",
     "Ríos-Urrego"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Adolfo M.",
     "Garcia"
    ],
    [
     "Yamile",
     "Bocanegra"
    ],
    [
     "Leonardo",
     "Moreno"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ]
   ],
   "title": "Synchronous analysis of abnormal acoustic and linguistic production in Parkinson's speech",
   "original": "521",
   "order": 1078,
   "page_count": 5,
   "abstract": [
    "Parkinson&#x27;s disease is a neurodegenerative disorder involving speech and language deficits. Often, these are separately studied as proxies of motor and non-motor (e.g., cognitive) symptoms, respectively. Conversely, links between both dimensions remain virtually uncharted. This paper introduces a methodology that enables the synchronous study of acoustic and linguistic patterns in Parkinson&#x27;s speech. Our findings show that verbs and nouns provided relevant acoustic and linguistic information not only to model motor impairments but also to understand non-motor symptoms like those that appear when Parkinson&#x27;s disease patients develop mild cognitive impairment."
   ],
   "p1": 5283,
   "pn": 5287,
   "doi": "10.21437/Interspeech.2025-521",
   "url": "interspeech_2025/escobargrisales25_interspeech.html"
  },
  "gao25b_interspeech": {
   "authors": [
    [
     "Kunxiao",
     "Gao"
    ],
    [
     "Anna",
     "Favaro"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Laureano Moro",
     "Velazquez"
    ]
   ],
   "title": "ADCeleb: A Longitudinal Speech Dataset from Public Figures for Early Detection of Alzheimer’s Disease",
   "original": "523",
   "order": 1159,
   "page_count": 5,
   "abstract": [
    "Previous studies on early Alzheimer’s disease (AD) detection using speech have been limited by small sample sizes, scarce prodromal phase recordings, and minimal longitudinal data. Many also rely on standardized tasks that do not reflect natural language use. To address these gaps, we introduce ADCeleb, a longitudinal speech dataset comprising publicly available recordings from individuals who later disclosed an AD diagnosis. It includes samples from 40 individuals with prodromal AD and 40 cognitively normal controls (CNs), matched by age and sex, spanning 1 to 10 years before diagnosis. Classification experiments using multimodal models integrating speech and text achieved 0.72 accuracy in distinguishing AD from CNs in the 6- to 10-year pre-diagnosis window and 0.80 in the 1- to 5-year pre-diagnosis window. These results highlight the potential of speech-based technologies as non-invasive tools for early AD detection in real-world settings or for triage improvement in clinical trials."
   ],
   "p1": 5688,
   "pn": 5692,
   "doi": "10.21437/Interspeech.2025-523",
   "url": "interspeech_2025/gao25b_interspeech.html"
  },
  "reuter25_interspeech": {
   "authors": [
    [
     "Paul M.",
     "Reuter"
    ],
    [
     "Michael",
     "Jessen"
    ]
   ],
   "title": "On the influence of language similarity in non-target speaker verification trials",
   "original": "526",
   "order": 816,
   "page_count": 5,
   "abstract": [
    "In this paper, we investigate the influence of language similarity in cross-lingual non-target speaker verification trials using a state-of-the-art speaker verification system, ECAPA-TDNN, trained on multilingual and monolingual variants of the VoxCeleb dataset. Our analysis of the score distribution patterns on multilingual Globalphone and LDC CTS reveals a clustering effect in speaker comparisons involving a training language, whereby the choice of comparison language only minimally impacts scores. Conversely, we observe a language similarity effect in trials involving languages not included in the training set of the speaker verification system, with scores correlating with language similarity measured by a language classification system, especially when using multilingual training data."
   ],
   "p1": 3998,
   "pn": 4002,
   "doi": "10.21437/Interspeech.2025-526",
   "url": "interspeech_2025/reuter25_interspeech.html"
  },
  "elkheir25_interspeech": {
   "authors": [
    [
     "Yassine",
     "El Kheir"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "BiCrossMamba-ST: Speech Deepfake Detection with Bidirectional Mamba Spectro-Temporal Cross-Attention",
   "original": "527",
   "order": 458,
   "page_count": 5,
   "abstract": [
    "We propose BiCrossMamba-ST, a robust framework for speech deepfake detection that leverages a dual-branch spectro-temporal architecture powered by bidirectional Mamba blocks and mutual cross-attention. By processing spectral sub-bands and temporal intervals separately and then integrating their representations, BiCrossMamba-ST effectively captures the subtle cues of synthetic speech. In addition, our proposed framework leverages a convolution-based 2D attention map to focus on specific spectro-temporal regions, enabling robust deepfake detection. Operating directly on raw features, BiCrossMamba-ST achieves significant performance improvements, a 67.74% and 26.3% relative gain over state-of-the-art AASIST on ASVSpoof LA21 and ASVSpoof DF21 benchmarks, respectively, and a 6.80% improvement over RawBMamba on ASVSpoof DF21. Code and models is made publicly available."
   ],
   "p1": 2235,
   "pn": 2239,
   "doi": "10.21437/Interspeech.2025-527",
   "url": "interspeech_2025/elkheir25_interspeech.html"
  },
  "asali25_interspeech": {
   "authors": [
    [
     "Amro",
     "Asali"
    ],
    [
     "Yehuda",
     "Ben-Shimol"
    ],
    [
     "Itshak",
     "Lapidot"
    ]
   ],
   "title": "ATMM-SAGA: Alternating Training for Multi-Module with Score-Aware Gated Attention SASV system",
   "original": "529",
   "order": 758,
   "page_count": 5,
   "abstract": [
    "The objective of automatic speaker verification (ASV) systems is to determine whether a given test speech utterance corresponds to a claimed enrolled speaker. These systems have a wide range of applications, and ensuring their reliability is crucial. In this paper, we propose a spoofing-robust automatic speaker verification (SASV) system employing a score-aware gated attention (SAGA) fusion scheme, integrating scores from a pre-trained countermeasure (CM) with speaker embeddings from a pre-trained ASV. Specifically, we employ the AASIST and ECAPA-TDNN models. SAGA acts as an adaptive gating mechanism, where the CM score determines how strongly ASV embeddings influence the final SASV decision. Experiments on the ASVspoof2019 logical access dataset demonstrate that the proposed SASV system achieves an SASV equal error rate (SASV-EER) and agnostic detection cost function (a-DCF) of 2.31%, 0.0603 for the development set and 2.18%, 0.0480 for the evaluation set."
   ],
   "p1": 3708,
   "pn": 3712,
   "doi": "10.21437/Interspeech.2025-529",
   "url": "interspeech_2025/asali25_interspeech.html"
  },
  "pendyala25_interspeech": {
   "authors": [
    [
     "Varsha",
     "Pendyala"
    ],
    [
     "Pedro",
     "Morgado"
    ],
    [
     "William",
     "Sethares"
    ]
   ],
   "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation",
   "original": "530",
   "order": 917,
   "page_count": 5,
   "abstract": [
    "Voice interfaces integral to the human-computer interaction systems can benefit from speech emotion recognition (SER) to customize responses based on user emotions. Since humans convey emotions through multi-modal audio-visual cues, developing SER systems using both the modalities is beneficial. However, collecting a vast amount of labeled data for their development is expensive. This paper proposes a knowledge distillation framework called LightweightSER (LiSER) that leverages unlabeled audio-visual data for SER, using large teacher models built on advanced speech and face representation models. LiSER transfers knowledge regarding speech emotions and facial expressions from the teacher models to lightweight student models. Experiments conducted on two benchmark datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence on extensive labeled datasets for SER tasks."
   ],
   "p1": 4503,
   "pn": 4507,
   "doi": "10.21437/Interspeech.2025-530",
   "url": "interspeech_2025/pendyala25_interspeech.html"
  },
  "biswas25b_interspeech": {
   "authors": [
    [
     "Subrata",
     "Biswas"
    ],
    [
     "Mohammad Nur Hossain",
     "Khan"
    ],
    [
     "Bashima",
     "Islam"
    ]
   ],
   "title": "QUADS: Quantized Distillation Framework for Efficient Speech Language Understanding",
   "original": "532",
   "order": 836,
   "page_count": 5,
   "abstract": [
    "Spoken Language Understanding (SLU) systems must bal ance performance and efficiency, particularly in resource-constrained environments. Existing methods apply distillation and quantization separately, leading to suboptimal compression as distillation ignores quantization constraints. We propose QUADS, a unified framework that optimizes both through multi-stage training with a pre-trained model, enhancing adaptability to low-bit regimes while maintaining accuracy. QUADS achieves 71.13% accuracy on SLURP and 99.20% on FSC, with only minor degradations of up to 5.56% compared to state-of-the-art models. Additionally, it reduces computational complexity by 60-73× (GMACs) and model size by 83-700×, demonstrating strong robustness under extreme quantization. These results establish QUADS as a highly efficient solution for real-world, resource-constrained SLU applications."
   ],
   "p1": 4098,
   "pn": 4102,
   "doi": "10.21437/Interspeech.2025-532",
   "url": "interspeech_2025/biswas25b_interspeech.html"
  },
  "lin25b_interspeech": {
   "authors": [
    [
     "Ju",
     "Lin"
    ],
    [
     "Yiteng",
     "Huang"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Directional Speech Recognition with Full-Duplex Capability",
   "original": "535",
   "order": 525,
   "page_count": 5,
   "abstract": [
    "Recent work on directional automatic speech recognition (DASR) has enabled automatic transcription of a conversation partner several feet away via smart glasses. DASR leverages multiple microphones in the glasses by using multiple beamformers simultaneously. We aim to make the DASR insensitive to scenarios that also involve text-to-speech (TTS) playback. This could enable additional future scenarios like simultaneous speech translation. How to prevent ASR from capturing the system&#x27;s own TTS output, while maintaining optimal clarity of the captured conversation partner&#x27;s speech? We experiment with two modern linear acoustic echo cancellation (AEC) algorithms. To remedy accuracy regressions from echo residuals, we propose AEC-aware model training. While AEC alone eliminates most TTS loopback, dramatically improving the word-error rate (WER) by over 70%, AEC-aware model training provides further relative WER boosts of 13% or more."
   ],
   "p1": 2570,
   "pn": 2574,
   "doi": "10.21437/Interspeech.2025-535",
   "url": "interspeech_2025/lin25b_interspeech.html"
  },
  "bn25_interspeech": {
   "authors": [
    [
     "Suhas",
     "BN"
    ],
    [
     "Han-Chin",
     "Shing"
    ],
    [
     "Lei",
     "Xu"
    ],
    [
     "Mitch",
     "Strong"
    ],
    [
     "Jon",
     "Burnsky"
    ],
    [
     "Jessica",
     "Ofor"
    ],
    [
     "Jordan R.",
     "Mason"
    ],
    [
     "Susan",
     "Chen"
    ],
    [
     "Sundararajan",
     "Srinivasan"
    ],
    [
     "Chaitanya",
     "Shivade"
    ],
    [
     "Jack",
     "Moriarty"
    ],
    [
     "Joseph Paul",
     "Cohen"
    ]
   ],
   "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization",
   "original": "537",
   "order": 625,
   "page_count": 5,
   "abstract": [
    "Hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues pose significant risks to patient care and clinical decision-making. However, the phenomenon remains understudied in the clinical domain, with uncertainty surrounding the applicability of general-domain hallucination detectors. The rarity and randomness of hallucinations further complicate their investigation. In this paper, we conduct an evaluation of hallucination detection methods in the medical domain, and construct two datasets for the purpose: A fact-controlled Leave-N-out dataset - generated by systematically removing facts from source dialogues to induce hallucinated content in summaries; and a natural hallucination dataset - arising organically during LLM-based medical summarization. We show that general-domain detectors struggle to detect clinical hallucinations, and that performance on fact-controlled hallucinations does not reliably predict effectiveness on natural hallucinations. We then develop fact-based approaches that count hallucinations, offering explainability not available with existing methods. Notably, our LLM-based detectors, which we developed using fact-controlled hallucinations, generalize well to detecting real-world clinical hallucinations. This research contributes a suite of specialized metrics supported by expertannotated datasets to advance faithful clinical summarization systems."
   ],
   "p1": 3070,
   "pn": 3074,
   "doi": "10.21437/Interspeech.2025-537",
   "url": "interspeech_2025/bn25_interspeech.html"
  },
  "doan25_interspeech": {
   "authors": [
    [
     "Thien-Phuc",
     "Doan"
    ],
    [
     "Kihun",
     "Hong"
    ],
    [
     "Souhwan",
     "Jung"
    ]
   ],
   "title": "VIB-based Real Pre-emphasis Audio Deepfake Source Tracing",
   "original": "538",
   "order": 320,
   "page_count": 5,
   "abstract": [
    "The rise of audio deepfakes presents significant challenges, necessitating effective detection and attribution mechanisms for fabricated speech. This paper addresses the need to identify not only fake speech but also its origins. We develop the audio deepfake source tracing method as follows: First, we establish a protocol using the MLAAD v5 dataset to analyze the acoustic and vocoder processing involved in generating fake speech. Second, we introduce a novel real pre-emphasis method based on the VIB architecture for audio deepfake source tracing. Our evaluation demonstrates that our model outperforms baseline systems, achieving enhancements of 10%. Our approach significantly improves model generalization and reduces complexity while preserving the real pre-emphasis effect established by the baseline."
   ],
   "p1": 1568,
   "pn": 1572,
   "doi": "10.21437/Interspeech.2025-538",
   "url": "interspeech_2025/doan25_interspeech.html"
  },
  "szalay25_interspeech": {
   "authors": [
    [
     "Tünde",
     "Szalay"
    ],
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Tharmakulasingam",
     "Sirojan"
    ],
    [
     "Zheng",
     "Nan"
    ],
    [
     "Renata",
     "Huang"
    ],
    [
     "Kirrie",
     "Ballard"
    ],
    [
     "Beena",
     "Ahmed"
    ]
   ],
   "title": "AusKidTalk: Using Strategic Data Collection and Out-of-Domain Tools to Semi-Automate Novel Corpora Annotation",
   "original": "539",
   "order": 870,
   "page_count": 5,
   "abstract": [
    "Annotating speech corpora for novel populations presents a circular problem: eliminating costly manual transcription requires automatic speech recognition (ASR) tools not yet developed; but developing ASR tools requires annotated speech corpora not available. Manual transcription burden was reduced for AusKidTalk, a novel population due to speaker age and accent, by strategic data collection protocol combined with out-of-domain ASR tools for semi-automatic annotation. The data collection protocol inserted tones and timestamps to automatically segment the recordings. Automatic annotation was conducted by out-of-domain tools for diarisation (NeMo) and orthographic transcription (UNSW ASR). Transcription accuracy with 17% word error rate (WER) for single words and 23% WER for continuous speech allowed for hand-correction instead of transcription, reducing annotation burden. The workflow can be adapted for other corpora and updated with new ASR tools as they become available."
   ],
   "p1": 4268,
   "pn": 4272,
   "doi": "10.21437/Interspeech.2025-539",
   "url": "interspeech_2025/szalay25_interspeech.html"
  },
  "raokoluguri25_interspeech": {
   "authors": [
    [
     "Nithin",
     "Rao Koluguri"
    ],
    [
     "Monica",
     "Sekoyan"
    ],
    [
     "George",
     "Zelenfroynd"
    ],
    [
     "Sasha",
     "Meister"
    ],
    [
     "Shuoyang",
     "Ding"
    ],
    [
     "Sofia",
     "Kostandian"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Nikolay",
     "Karpov#"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Sara",
     "Papi"
    ],
    [
     "Marco",
     "Gaido"
    ],
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages",
   "original": "540",
   "order": 801,
   "page_count": 5,
   "abstract": [
    "Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data."
   ],
   "p1": 3923,
   "pn": 3927,
   "doi": "10.21437/Interspeech.2025-540",
   "url": "interspeech_2025/raokoluguri25_interspeech.html"
  },
  "wan25_interspeech": {
   "authors": [
    [
     "Zixiang",
     "Wan"
    ],
    [
     "Guochang",
     "Zhang"
    ],
    [
     "Yifeng",
     "He"
    ],
    [
     "Jianqiang",
     "Wei"
    ]
   ],
   "title": "SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain",
   "original": "546",
   "order": 126,
   "page_count": 5,
   "abstract": [
    "Neural Audio Codecs (NACs) have gained growing attention in recent years as technologies for audio compression and audio representation in speech language models. While mainstream NACs typically require G-level computation and M-level parameters, the performance of lightweight and streaming NACs remains underexplored. This paper proposes SpecTokenizer, a lightweight streaming codec that operates in the compressed spectral domain. Composed solely of alternating CNN and RNN layers, SpecTokenizer achieves greater efficiency and better representational capability through multi-scale modeling in the compressed spectrum domain. At 4 kbps, the proposed SpecTokenizer achieves comparable or superior performance compared to the codec with state-of-the-art lightweight architecture while requiring only 20% of the computation and 10% of the parameters. Furthermore, it significantly outperforms the codec when using similar computational and storage resources."
   ],
   "p1": 599,
   "pn": 603,
   "doi": "10.21437/Interspeech.2025-546",
   "url": "interspeech_2025/wan25_interspeech.html"
  },
  "sun25c_interspeech": {
   "authors": [
    [
     "Haoqin",
     "Sun"
    ],
    [
     "Jingguang",
     "Tian"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Jiabei",
     "He"
    ],
    [
     "Shiwan",
     "Zhao"
    ],
    [
     "Xiangyu",
     "Kong"
    ],
    [
     "Desheng",
     "Hu"
    ],
    [
     "Xinkang",
     "Xu"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "RA-CLAP: Relation-Augmented Emotional Speaking Style Contrastive Language-Audio Pretraining For Speech Retrieval",
   "original": "548",
   "order": 610,
   "page_count": 5,
   "abstract": [
    "The Contrastive Language-Audio Pretraining (CLAP) model has demonstrated excellent performance in general audio description-related tasks, such as audio retrieval. However, in the emerging field of emotional speaking style description (ESSD), cross-modal contrastive pretraining remains largely unexplored. In this paper, we propose a novel speech retrieval task called emotional speaking style retrieval (ESSR), and ESS-CLAP, an emotional speaking style CLAP model tailored for learning relationship between speech and natural language descriptions. In addition, we further propose relation-augmented CLAP (RA-CLAP) to address the limitation of traditional methods that assume a strict binary relationship between caption and audio. The model leverages self-distillation to learn the potential local matching relationships between speech and descriptions, thereby enhancing generalization ability. The experimental results validate the effectiveness of RA-CLAP, providing valuable reference in ESSD."
   ],
   "p1": 2995,
   "pn": 2999,
   "doi": "10.21437/Interspeech.2025-548",
   "url": "interspeech_2025/sun25c_interspeech.html"
  },
  "kong25_interspeech": {
   "authors": [
    [
     "Yuxiang",
     "Kong"
    ],
    [
     "Fan",
     "Cui"
    ],
    [
     "Liyong",
     "Guo"
    ],
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Lichun",
     "Fan"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Jian",
     "Luan"
    ]
   ],
   "title": "GLCLAP: A Novel Contrastive Learning Pre-trained Model for Contextual Biasing in ASR",
   "original": "550",
   "order": 1055,
   "page_count": 5,
   "abstract": [
    "Recently, Automatic Speech Recognition (ASR) that supports prompts has shown remarkable versatility. For contextual biasing with these systems, a pivotal factor lies in obtaining well-matched prompts. To address this issue, Contrastive Language-Audio Pre-training is exploited to retrieve matched entities from a user-specified list. Instead of only confining contrastive learning to the sentence level, we propose the Global-Local Contrastive Language-Audio Pre-trained model (GLCLAP). On the global scale, semantic information is extracted from audio and text, enabling a holistic understanding of the input. On the local scale, detailed local word information of individual segments is focused. This multi-scale information has led to a remarkable improvement in bias word retrieval accuracy. By using the GLCLAP bias word retrieval system as the prompts generation component, the accuracy of the final ASR decoding result is significantly improved without finetuning."
   ],
   "p1": 5173,
   "pn": 5177,
   "doi": "10.21437/Interspeech.2025-550",
   "url": "interspeech_2025/kong25_interspeech.html"
  },
  "zhang25c_interspeech": {
   "authors": [
    [
     "Yike",
     "Zhang"
    ],
    [
     "Yiming",
     "Li"
    ],
    [
     "Jie",
     "Chen"
    ],
    [
     "Qinghua",
     "Wu"
    ],
    [
     "Songjun",
     "Cao"
    ],
    [
     "Long",
     "Ma"
    ]
   ],
   "title": "Monotonic Attention for Robust Text-to-Speech Synthesis in Large Language Model Frameworks",
   "original": "551",
   "order": 503,
   "page_count": 5,
   "abstract": [
    "Text-to-speech (TTS) synthesis using large language models (LLMs) has demonstrated promising performance and has recently garnered significant attention. Despite their impressive naturalness, these methods often lack monotonic alignment constraints, resulting in issues such as repetition, omissions, and misalignment in the synthesized output. This paper introduces a stepwise monotonic attention algorithm tailored for LLM-based architectures, to enhance the robustness of TTS synthesis and effectively address these issues. Compared with the best existing model, VALL-E R, the proposed approach requires no additional forced aligners and exhibits greater robustness on out-of-domain test sets. Furthermore, experiments show that the proposed method scales well to large model sizes and large-scale training sets."
   ],
   "p1": 2460,
   "pn": 2464,
   "doi": "10.21437/Interspeech.2025-551",
   "url": "interspeech_2025/zhang25c_interspeech.html"
  },
  "zhang25d_interspeech": {
   "authors": [
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Yadong",
     "Niu"
    ],
    [
     "Chenyu",
     "Liu"
    ],
    [
     "Si",
     "Cheng"
    ],
    [
     "Anbei",
     "Zhao"
    ],
    [
     "Jian",
     "Luan"
    ]
   ],
   "title": "X-ARES: A Comprehensive Framework for Assessing Audio Encoder Performance",
   "original": "552",
   "order": 990,
   "page_count": 5,
   "abstract": [
    "We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a novel open-source benchmark designed to systematically assess audio encoder performance across diverse domains. By encompassing tasks spanning speech, environmental sounds, and music, X-ARES provides two evaluation approaches for evaluating audio representations: linear fine-tuning and unparameterized evaluation. The framework includes 22 distinct tasks that cover essential aspects of audio processing, from speech recognition and emotion detection to sound event classification and music genre identification. Our extensive evaluation of state-of-the-art audio encoders reveals significant performance variations across different tasks and domains, highlighting the complexity of general audio representation learning."
   ],
   "p1": 4868,
   "pn": 4872,
   "doi": "10.21437/Interspeech.2025-552",
   "url": "interspeech_2025/zhang25d_interspeech.html"
  },
  "park25b_interspeech": {
   "authors": [
    [
     "Hyun Joon",
     "Park"
    ],
    [
     "Jeongmin",
     "Liu"
    ],
    [
     "Jin Sob",
     "Kim"
    ],
    [
     "Jeong Yeol",
     "Yang"
    ],
    [
     "Sung Won",
     "Han"
    ],
    [
     "Eunwoo",
     "Song"
    ]
   ],
   "title": "RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching",
   "original": "554",
   "order": 499,
   "page_count": 5,
   "abstract": [
    "We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in flow matching (FM) training. Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed. To address this challenge, RapFlow-TTS enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps. Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis. Experimental results show that RapFlow-TTS achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively."
   ],
   "p1": 2440,
   "pn": 2444,
   "doi": "10.21437/Interspeech.2025-554",
   "url": "interspeech_2025/park25b_interspeech.html"
  },
  "ram25_interspeech": {
   "authors": [
    [
     "Ashwin",
     "Ram"
    ],
    [
     "Marisol",
     "Muñoz"
    ],
    [
     "Zoi",
     "Gkalitsiou"
    ],
    [
     "Alexandros G.",
     "Dimakis"
    ]
   ],
   "title": "Bilingual Speakers Exhibit Cognitive Fatigue: A Speech Disfluencies Case Study on Research Talks",
   "original": "555",
   "order": 778,
   "page_count": 5,
   "abstract": [
    "Speech disfluencies are vital for understanding cognitive processes and improving speech recognition systems. We curate a dataset with annotated text and labeled speech disfluencies from more than 20 hours of speech from monolingual and bilingual speakers. Furthermore, we illustrate a large-scale validation of a bilingual cognitive fatigue phenomenon that seems to be independent of the two spoken languages of the speakers. That is, after navigating a lexically complex word, bilingual speakers tend to use a disfluency, such as a filled pause or repair, followed by a phonetically simpler word in order to possibly regain momentum for subsequent utterance segments. We conclude by exploring how our research can help speech pathologists by revealing distinct bilingual cognitive strategies and how they manifest in speaker disfluencies."
   ],
   "p1": 3808,
   "pn": 3812,
   "doi": "10.21437/Interspeech.2025-555",
   "url": "interspeech_2025/ram25_interspeech.html"
  },
  "liu25c_interspeech": {
   "authors": [
    [
     "Rui",
     "Liu"
    ],
    [
     "Pu",
     "Gao"
    ],
    [
     "Jiatian",
     "Xi"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset",
   "original": "559",
   "order": 977,
   "page_count": 5,
   "abstract": [
    "Text-based speech editing (TSE) modifies speech using only text, eliminating re-recording. However, existing TSE methods, mainly focus on the content accuracy and acoustic consistency of synthetic speech segments, and often overlook the emotional shifts or inconsistency issues introduced by text changes. To address this issue, we propose EmoCorrector, a novel postcorrection scheme for TSE. EmoCorrector leverages Retrieval-Augmented Generation (RAG) by extracting the edited text’s emotional features, retrieving speech samples with matching emotions, and synthesizing speech that aligns with the desired emotion while preserving the speaker’s identity and quality. To support the training and evaluation of emotional consistency modeling in TSE, we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The prominent aspect of ECD-TSE is its inclusion of &lt;text, speech&gt; paired data featuring diverse text variations and a range of emotional expressions. Subjective and objective experiments and comprehensive analysis on ECD-TSE confirm that EmoCorrector significantly enhances the expression of intended emotion while addressing emotion inconsistency limitations in current TSE methods."
   ],
   "p1": 4803,
   "pn": 4807,
   "doi": "10.21437/Interspeech.2025-559",
   "url": "interspeech_2025/liu25c_interspeech.html"
  },
  "fang25_interspeech": {
   "authors": [
    [
     "Yuanbo",
     "Fang"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Xueru",
     "Li"
    ],
    [
     "Weibin",
     "Zhang"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "MMLoRA: Multitask Memory Parameter-Efficient Fine-Tuning for Multimodal SER",
   "original": "560",
   "order": 1006,
   "page_count": 5,
   "abstract": [
    "The differences in emotional expression present significant challenges for the development of effective emotion recognition systems. Although large language models (LLMs) have demonstrated strong performance, their generalization in emotion recognition tasks is often limited by variations in individual emotional expression, which remain inadequately addressed even by parameter-efficient fine-tuning techniques such as LoRA. To address these challenges, we proposes a multitask memory parameter-efficient fine-tuning method (MMLoRA) that enhances multimodal SER by incorporating gender as an auxiliary task. The method leverages shared LoRA experts to facilitate information exchange between tasks and employs a mixture of LoRA experts to process task-specific information. Additionally, a memory mechanism propagates task-specific information across layers. Experimental results demonstrate that the proposed MMLoRA significantly improves emotion recognition performance compared to vanilla LoRA."
   ],
   "p1": 4948,
   "pn": 4952,
   "doi": "10.21437/Interspeech.2025-560",
   "url": "interspeech_2025/fang25_interspeech.html"
  },
  "lee25c_interspeech": {
   "authors": [
    [
     "Hoyeon",
     "Lee"
    ],
    [
     "Sejung",
     "Son"
    ],
    [
     "Ye-Eun",
     "Kang"
    ],
    [
     "Jong-Hwan",
     "Kim"
    ]
   ],
   "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model",
   "original": "564",
   "order": 97,
   "page_count": 5,
   "abstract": [
    "Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain."
   ],
   "p1": 454,
   "pn": 458,
   "doi": "10.21437/Interspeech.2025-564",
   "url": "interspeech_2025/lee25c_interspeech.html"
  },
  "lee25d_interspeech": {
   "authors": [
    [
     "Jihwan",
     "Lee"
    ],
    [
     "Kevin",
     "Huang"
    ],
    [
     "Kleanthis",
     "Avramidis"
    ],
    [
     "Simon",
     "Pistrosch"
    ],
    [
     "Monica",
     "Gonzalez-Machorro"
    ],
    [
     "Yoonjeong",
     "Lee"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Articulatory Feature Prediction from Surface EMG during Speech Production",
   "original": "565",
   "order": 70,
   "page_count": 5,
   "abstract": [
    "We present a model for predicting articulatory features from surface electromyography (EMG) signals during speech production. The proposed model integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features. Our approach achieves a high prediction correlation of approximately 0.9 for most articulatory features. Furthermore, we demonstrate that these predicted articulatory features can be decoded into intelligible speech waveforms. To our knowledge, this is the first method to decode speech waveforms from surface EMG via articulatory features, offering a novel approach to EMG-based speech synthesis. Additionally, we analyze the relationship between EMG electrode placement and articulatory feature predictability, providing knowledge-driven insights for optimizing EMG electrode configurations. The source code and decoded speech samples are publicly available."
   ],
   "p1": 320,
   "pn": 324,
   "doi": "10.21437/Interspeech.2025-565",
   "url": "interspeech_2025/lee25d_interspeech.html"
  },
  "zheng25_interspeech": {
   "authors": [
    [
     "Xiuwen",
     "Zheng"
    ],
    [
     "Bornali",
     "Phukon"
    ],
    [
     "Jonghwan",
     "Na"
    ],
    [
     "Ed",
     "Cutrell"
    ],
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Pan-Pan",
     "Jiang"
    ],
    [
     "Aadhrik",
     "Kuila"
    ],
    [
     "Colin",
     "Lea"
    ],
    [
     "Bob",
     "MacDonald"
    ],
    [
     "Gautam",
     "Mantena"
    ],
    [
     "Venkatesh",
     "Ravichandran"
    ],
    [
     "Leda",
     "Sari"
    ],
    [
     "Katrin",
     "Tomanek"
    ],
    [
     "Chang D.",
     "Yoo"
    ],
    [
     "Chris",
     "Zwilling"
    ]
   ],
   "title": "The Interspeech 2025 Speech Accessibility Project Challenge",
   "original": "566",
   "order": 665,
   "page_count": 5,
   "abstract": [
    "While the last decade has witnessed significant advancements in Automatic Speech Recognition (ASR) systems, performance of these systems for individuals with speech disabilities remains inadequate, partly due to limited public training data. To bridge this gap, the 2025 Interspeech Speech Accessibility Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data collected and transcribed from more than 500 individuals with diverse speech disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline, the SAP Challenge evaluates submissions based on Word Error Rate and Semantic Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2 baseline in terms of WER, while 17 teams surpassed the baseline on SemScore. Notably, the top team achieved the lowest WER of 8.11%, and the highest SemScore of 88.44% at the same time, setting new benchmarks for future ASR systems in recognizing impaired speech."
   ],
   "p1": 3269,
   "pn": 3273,
   "doi": "10.21437/Interspeech.2025-566",
   "url": "interspeech_2025/zheng25_interspeech.html"
  },
  "xiong25_interspeech": {
   "authors": [
    [
     "Yan",
     "Xiong"
    ],
    [
     "Visar",
     "Berisha"
    ],
    [
     "Julie",
     "Liss"
    ],
    [
     "Chaitali",
     "Chakrabarti"
    ]
   ],
   "title": "Mitigating Overfitting During Speech Foundation Model Fine-tuning: Applications to Dysarthric Speech Detection",
   "original": "567",
   "order": 434,
   "page_count": 5,
   "abstract": [
    "Speech foundation models have shown significant success in various speech-processing applications. However, fine-tuning these models on dysarthric speech is challenging due to overfitting caused by limited dataset sizes. This work proposes a modified multitask learning framework to mitigate overfitting in foundation model fine-tuning. Specifically, we train the model on a more complex task along with the task of interest and use gradient projection to preserve beneficial updates while resolving conflicts. We demonstrate that using automatic speech recognition as the main task and dysarthria detection as the auxiliary task improves model robustness and dysarthria detection performance. The proposed method1 reduces overfitting and improves in-corpus and cross-corpus detection accuracy by 5.4% to 13.4% compared to standard multi-task learning. These findings highlight the importance of structured multitask training for enhancing foundation model adaptability."
   ],
   "p1": 2133,
   "pn": 2137,
   "doi": "10.21437/Interspeech.2025-567",
   "url": "interspeech_2025/xiong25_interspeech.html"
  },
  "le25_interspeech": {
   "authors": [
    [
     "Chenyang",
     "Le"
    ],
    [
     "Yinfeng",
     "Xia"
    ],
    [
     "Huiyan",
     "Li"
    ],
    [
     "Manhong",
     "Wang"
    ],
    [
     "Yutao",
     "Sun"
    ],
    [
     "Xingyang",
     "Ma"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation",
   "original": "568",
   "order": 188,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech-to-text translation have led to the development of multilingual models capable of handling multiple language pairs simultaneously. However, these unified models often suffer from large parameter sizes, making it challenging to balance inference efficiency and performance, particularly in local deployment scenarios. We propose an innovative Parasitic Dual-Scale Approach, which combines an enhanced speculative sampling method with model compression and knowledge distillation techniques. Building on the Whisper Medium model, we enhance it for multilingual speech translation into whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art (SOTA) performance across six popular languages with improved inference efficiency. KVSPN enables a 40% speedup with no BLEU score degradation. Combined with distillation methods, it represents a 2.6$× speedup over the original Whisper Medium with superior performance."
   ],
   "p1": 908,
   "pn": 912,
   "doi": "10.21437/Interspeech.2025-568",
   "url": "interspeech_2025/le25_interspeech.html"
  },
  "wu25c_interspeech": {
   "authors": [
    [
     "Chenhao",
     "Wu"
    ],
    [
     "Xiangjun",
     "Cai"
    ],
    [
     "Haojie",
     "Zhang"
    ],
    [
     "Tianrui",
     "Jia"
    ],
    [
     "Yilu",
     "Deng"
    ],
    [
     "Kun",
     "Qian"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Yoshiharu",
     "Yamamoto"
    ],
    [
     "Jiang",
     "Liu"
    ]
   ],
   "title": "Exploring the Power of Empirical Mode Decomposition for Sensing the Sound of Silence: A Pilot Study on Mice Autism Detection via Ultrasonic Vocalisation",
   "original": "571",
   "order": 348,
   "page_count": 5,
   "abstract": [
    "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental disorder, and mice models have become essential for studying its genetic and behavioural aspects. Ultrasonic Vocalisations (USVs) emitted by mice provide a promising biomarker for ASD detection, but existing methods relying on spectrogram-based features struggle to capture the complex, non-stationary, and multi-scale nature of USVs. To address this, we propose a novel multi-branch fusion model that integrates spectrogram-based features with multi-scale features extracted using Empirical Mode Decomposition (EMD), which decomposes USVs into Intrinsic Mode Functions (IMFs) to represent their inherent complexity better. Through systematic occlusion experiments, we identify high-frequency components, particularly IMF1, as critical for accurate ASD detection, highlighting the diagnostic relevance of high-frequency USV patterns. Our model achieves an Unweighted Average Recall (UAR) of 0.75 in subject-level classification, significantly outperforming existing methods. These findings provide valuable insights into the importance of multi-scale feature extraction and offer a robust framework for improving ASD diagnostics and research."
   ],
   "p1": 1708,
   "pn": 1712,
   "doi": "10.21437/Interspeech.2025-571",
   "url": "interspeech_2025/wu25c_interspeech.html"
  },
  "setoguchi25_interspeech": {
   "authors": [
    [
     "Ryo",
     "Setoguchi"
    ],
    [
     "Yoshiko",
     "Arimoto"
    ]
   ],
   "title": "Assessment of the synthetic quality and controllability of laughing onset in speech-laugh synthesis",
   "original": "572",
   "order": 522,
   "page_count": 5,
   "abstract": [
    "This study is the first challenge of building a synthetic speech-laugh model via a deep learning technique. To maintain the phonetic intelligibility of synthesized speech-laugh, the model was trained with nonlaughing read speech material for both phones of speech-laugh (SL) and of speech (SP). To control laughing onset in SL, the model was also trained using SL material only for the phones of SL instances. The listening tests revealed that the naturalness score for synthesized female SL was as high as that for human SL and that the laughter-likeness score for synthesized SL was higher than that for synthesized SP in almost all conditions. The dictation test revealed that the training for phonetic intelligibility in SL synthesis was highly effective for synthesized SL. However, the difference between segmented SL onset and correct onset was greater for synthesized SL with phonetic intelligibility training than for that without training."
   ],
   "p1": 2555,
   "pn": 2559,
   "doi": "10.21437/Interspeech.2025-572",
   "url": "interspeech_2025/setoguchi25_interspeech.html"
  },
  "li25g_interspeech": {
   "authors": [
    [
     "Haiyun",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Xiaofeng",
     "Xie"
    ],
    [
     "Jingran",
     "Xie"
    ],
    [
     "Yaoxun",
     "Xu"
    ],
    [
     "Hanyang",
     "Peng"
    ]
   ],
   "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents",
   "original": "575",
   "order": 1042,
   "page_count": 5,
   "abstract": [
    "Voice cloning (VC)-resistant watermarking is an emerging technique for tracing and preventing unauthorized cloning. Existing methods effectively trace traditional VC models by training them on watermarked audio but fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training. To address this, we propose VoiceMark, the first zero-shot VC-resistant watermarking method that leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, we introduce VC-simulated augmentations and VAD-based loss to enhance robustness against distortions. Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods, which only reach around 50%."
   ],
   "p1": 5108,
   "pn": 5112,
   "doi": "10.21437/Interspeech.2025-575",
   "url": "interspeech_2025/li25g_interspeech.html"
  },
  "kim25h_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Kim"
    ],
    [
     "Doyeon",
     "Kim"
    ],
    [
     "Hyewon",
     "Han"
    ],
    [
     "Jinyoung",
     "Lee"
    ],
    [
     "Jonguk",
     "Yoo"
    ],
    [
     "Chang Woo",
     "Han"
    ],
    [
     "Jeongook",
     "Song"
    ],
    [
     "Hoon-Young",
     "Cho"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Quadruple Path Modeling with Latent Feature Transfer for Permutation-free Continuous Speech Separation",
   "original": "576",
   "order": 294,
   "page_count": 5,
   "abstract": [
    "This paper proposes Quadruple Path Modeling (QPM), a permutation-free and generalized continuous speech separation (CSS) model designed to handle varying speaker conditions and efficiently address the permutation problem in chunk-based streaming scenarios. QPM integrates intra-chunk feature modeling, inter-speaker and inter-chunk processing, and latent feature transfer (LFT) modules to enhance separation performance while ensuring speaker consistency across segments. By leveraging a memory-based inter-chunk mechanism and a learnable gating strategy, QPM effectively propagates relevant speaker information across segments, thus reducing speaker permutation errors in streaming CSS tasks. Designed for lightweight and low-latency applications, including live streaming, QPM demonstrates strong performance using a 2-second chunk size. Experimental results confirm the efficacy of the proposed system in resolving the permutation problem, offering a scalable and adaptable solution for CSS."
   ],
   "p1": 1438,
   "pn": 1442,
   "doi": "10.21437/Interspeech.2025-576",
   "url": "interspeech_2025/kim25h_interspeech.html"
  },
  "onda25_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Onda"
    ],
    [
     "Keisuke",
     "Imoto"
    ],
    [
     "Satoru",
     "Fukayama"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data",
   "original": "577",
   "order": 46,
   "page_count": 5,
   "abstract": [
    "In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as &quot;interlanguage speech intelligibility beneﬁt&quot; (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce."
   ],
   "p1": 221,
   "pn": 225,
   "doi": "10.21437/Interspeech.2025-577",
   "url": "interspeech_2025/onda25_interspeech.html"
  },
  "mun25_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Mun"
    ],
    [
     "Minhwa",
     "Chung"
    ],
    [
     "Sunhee",
     "Kim"
    ]
   ],
   "title": "Speech-Based Automatic Chronic Kidney Disease Diagnosis via Transformer Fusion of Glottal and Spectrogram Features",
   "original": "587",
   "order": 1082,
   "page_count": 5,
   "abstract": [
    "Chronic kidney disease (CKD) is a global health concern characterized by a gradual and irreversible decline in kidney function. Early diagnosis and timely intervention are crucial, yet current methods rely primarily on invasive blood and urine tests. Since CKD affects the respiratory system and alters speech production, vocal characteristics may serve as biomarkers for disease detection. This study proposes a deep learning-based approach that integrates spectrogram and glottal features for CKD diagnosis. Spectrograms capture broad acoustic characteristics, whereas glottal features, known to be influenced by CKD, provide complementary phonatory information. To effectively fuse these features, we employ a transformer-like architecture. The proposed method achieves an accuracy and a macro F1 score of 0.96, demonstrating its potential as an objective, non-invasive diagnostic tool. In addition, we analyze attention weights and gradient-based saliency maps to enhance model interpretability."
   ],
   "p1": 5303,
   "pn": 5307,
   "doi": "10.21437/Interspeech.2025-587",
   "url": "interspeech_2025/mun25_interspeech.html"
  },
  "zhang25e_interspeech": {
   "authors": [
    [
     "Yunqi C.",
     "Zhang"
    ],
    [
     "Dhruv",
     "Jagmohan"
    ],
    [
     "Hong Kit",
     "Li"
    ],
    [
     "C. T. Justine",
     "Hui"
    ],
    [
     "Yusuke",
     "Hioka"
    ]
   ],
   "title": "Effect of Noise Floor in Room Impulse Response on Speech Perception Under Spherical Harmonics-based Spatial Sound Reproduction",
   "original": "588",
   "order": 192,
   "page_count": 5,
   "abstract": [
    "The current study investigates the effect of noise floor in measured room impulse responses (RIR) on the reproducibility of speech perception under spherical harmonics-based spatial sound reproduction. Subjective listening test measuring the intelligibility of speech in noise was conducted under the spatial sound reproduction implemented using practically measured RIR with varying level of noise floor. The same test was also conducted in the real rooms where the RIR were measured. The comparison of the experimental results from the spatial sound reproduction and the real room suggests using measured RIR with low noise floor contributes to reproducing speech perception in real rooms accurately when the room is highly reverberant. It also has an effect to improve the reproducibility when the sound sources are located at 5 m but not at 2 m. Truncating RIR to further remove the noise floor mostly did not help improve the reproducibility regardless of the acoustics of the room."
   ],
   "p1": 928,
   "pn": 932,
   "doi": "10.21437/Interspeech.2025-588",
   "url": "interspeech_2025/zhang25e_interspeech.html"
  },
  "onda25b_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Onda"
    ],
    [
     "Keisuke",
     "Imoto"
    ],
    [
     "Satoru",
     "Fukayama"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Prosodically Enhanced Foreign Accent Simulation by Discrete Token-based Resynthesis Only with Native Speech Corpora",
   "original": "590",
   "order": 450,
   "page_count": 5,
   "abstract": [
    "Recently, a method for synthesizing foreign-accented speech only with native speech data using discrete tokens obtained from self-supervised learning (SSL) models was proposed. Considering limited availability of accented speech data, this method is expected to make it much easier to simulate foreign accents. By using the synthesized accented speech as listening materials for humans or training data for automatic speech recognition (ASR), both of them will acquire higher robustness against foreign accents. However, the previous method has a fatal flaw that it cannot reproduce duration-related accents. Durational accents are commonly seen when L2 speakers, whose native language has syllable-timed or mora-timed rhythm, speak stress-timed languages, such as English. In this paper, we integrate duration modification to the previous method to simulate foreign accents more accurately. Experiments show that the proposed method successfully replicates durational accents seen in real L2 speech."
   ],
   "p1": 2195,
   "pn": 2199,
   "doi": "10.21437/Interspeech.2025-590",
   "url": "interspeech_2025/onda25b_interspeech.html"
  },
  "lee25e_interspeech": {
   "authors": [
    [
     "Jaejun",
     "Lee"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Vo-Ve: An Explainable Voice-Vector for Speaker Identity Evaluation",
   "original": "591",
   "order": 814,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose Vo-Ve, a novel voice-vector embedding that captures speaker identity. Unlike conventional speaker embeddings, Vo-Ve is explainable, as it contains the probabilities of explicit voice attribute classes. Through extensive analysis, we demonstrate that Vo-Ve not only evaluates speaker similarity competitively with conventional techniques but also provides an interpretable explanation in terms of voice attributes. We strongly believe that Vo-Ve can enhance evaluation schemes across various speech tasks due to its high-level explainability."
   ],
   "p1": 3988,
   "pn": 3992,
   "doi": "10.21437/Interspeech.2025-591",
   "url": "interspeech_2025/lee25e_interspeech.html"
  },
  "onda25c_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Onda"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Hayato",
     "Futami"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Differentiable K-means for Fully-optimized Discrete Token-based ASR",
   "original": "593",
   "order": 251,
   "page_count": 5,
   "abstract": [
    "Recent studies have highlighted the potential of discrete tokens derived from self-supervised learning (SSL) models for various speech-related tasks. These tokens serve not only as substitutes for text in language modeling but also as intermediate representations for tasks such as automatic speech recognition (ASR). However, discrete tokens are typically obtained via k-means clustering of SSL features independently of downstream tasks, making them suboptimal for specific applications. This paper proposes the use of differentiable k-means, enabling the joint optimization of tokenization and downstream tasks. This approach enables the fine-tuning of the SSL parameters and learning weights for outputs from multiple SSL layers. Experiments were conducted with ASR as a downstream task. ASR accuracy successfully improved owing to the optimized tokens. The acquired tokens also exhibited greater purity of phonetic information, which were found to be useful even in speech resynthesis."
   ],
   "p1": 1223,
   "pn": 1227,
   "doi": "10.21437/Interspeech.2025-593",
   "url": "interspeech_2025/onda25c_interspeech.html"
  },
  "qian25_interspeech": {
   "authors": [
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Rao",
     "Ma"
    ],
    [
     "Stefano",
     "Bannò"
    ],
    [
     "Kate M.",
     "Knill"
    ],
    [
     "Mark J.F.",
     "Gales"
    ]
   ],
   "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction",
   "original": "594",
   "order": 1037,
   "page_count": 5,
   "abstract": [
    "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial for second language learners, teachers and test takers. Traditional SGEC systems rely on a cascaded pipeline consisting of an ASR, a module for disfluency detection (DD) and removal and one for GEC. With the rise of end-to-end (E2E) speech foundation models, we investigate their effectiveness in SGEC and feedback generation. This work introduces a pseudo-labelling process to address the challenge of limited labelled data, expanding the training data size from 77 hours to approximately 2500 hours, leading to improved performance. Additionally, we prompt an E2E Whisper-based SGEC model with fluent transcriptions, showing a slight improvement in SGEC performance, with more significant gains in feedback generation. Finally, we assess the impact of increasing model size, revealing that while pseudo-labelled data does not yield performance gain for a larger Whisper model, training with prompts proves beneficial."
   ],
   "p1": 5083,
   "pn": 5087,
   "doi": "10.21437/Interspeech.2025-594",
   "url": "interspeech_2025/qian25_interspeech.html"
  },
  "mori25_interspeech": {
   "authors": [
    [
     "Kiyotada",
     "Mori"
    ],
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Chaoran",
     "Liu"
    ],
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Angel García",
     "Contreras"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems",
   "original": "595",
   "order": 358,
   "page_count": 5,
   "abstract": [
    "Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at the front end of their pipeline. The role of ASR in SDSs is to recognize information in user speech related to response generation appropriately. Examining selective listening of humans, which refers to the ability to focus on and listen to important parts of a conversation during the speech, will enable us to identify the ASR capabilities required for SDSs and evaluate them. In this study, we experimentally confirmed selective listening when humans generate dialogue responses by comparing human transcriptions for generating dialogue responses and reference transcriptions. Based on our experimental results, we discuss the possibility of a new ASR evaluation method that leverages human selective listening, which can identify the gap between transcription ability between ASR systems and humans."
   ],
   "p1": 1753,
   "pn": 1757,
   "doi": "10.21437/Interspeech.2025-595",
   "url": "interspeech_2025/mori25_interspeech.html"
  },
  "jeon25_interspeech": {
   "authors": [
    [
     "Yejin",
     "Jeon"
    ],
    [
     "Solee",
     "Im"
    ],
    [
     "Youngjae",
     "Kim"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning ",
   "original": "596",
   "order": 429,
   "page_count": 5,
   "abstract": [
    "Dysarthric speakers experience substantial communication challenges due to impaired motor control of the speech apparatus, which leads to reduced speech intelligibility. This creates significant obstacles in dataset curation since actual recording of long, articulate sentences for the objective of training personalized TTS models becomes infeasible. Thus, the limited availability of audio data, in addition to the articulation errors that are present within the audio, complicates personalized speech synthesis for target dysarthric speaker adaptation. To address this, we frame the issue as a domain transfer task and introduce a knowledge anchoring framework that leverages a teacher-student model, enhanced by curriculum learning through audio augmentation. Experimental results show that the proposed zero-shot multi-speaker TTS model effectively generates synthetic speech with markedly reduced articulation errors and high speaker fidelity, while maintaining prosodic naturalness."
   ],
   "p1": 2108,
   "pn": 2112,
   "doi": "10.21437/Interspeech.2025-596",
   "url": "interspeech_2025/jeon25_interspeech.html"
  },
  "martin25_interspeech": {
   "authors": [
    [
     "Vincent P.",
     "Martin"
    ],
    [
     "Charles",
     "Brazier"
    ],
    [
     "Maxime",
     "Amblard"
    ],
    [
     "Michel",
     "Musiol"
    ],
    [
     "Jean-Luc",
     "Rouas"
    ]
   ],
   "title": "Network of acoustic characteristics for the automatic detection of suicide risk from speech. Contribution to the 2025 SpeechWellness challenge by the Semawave team",
   "original": "599",
   "order": 91,
   "page_count": 5,
   "abstract": [
    "Suicide is a leading cause of death among young individuals. Although early detection and intervention are vital for preventing suicide attempts, current suicide risk assessments rely heavily on clinical interviews and questionnaires, both of which are subject to patient biases. In contrast, speech analysis provides several objective advantages for estimating suicide risk. This is the focus of the 2025 SpeechWellness Challenge. This article presents a new paradigm for speech analysis based on network analyses of low-level descriptors. We evaluate the performance of this approach compared to the classical eGeMAPS+SVM model for suicide risk detection. Additionally, we assess the relevance of comparing networks derived from reading and spontaneous speech, and explore different methods for network construction, analyzing their respective performances."
   ],
   "p1": 424,
   "pn": 428,
   "doi": "10.21437/Interspeech.2025-599",
   "url": "interspeech_2025/martin25_interspeech.html"
  },
  "mori25b_interspeech": {
   "authors": [
    [
     "Kiyotada",
     "Mori"
    ],
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Angel García",
     "Contreras"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model",
   "original": "600",
   "order": 620,
   "page_count": 5,
   "abstract": [
    "Prefetching of dialogue responses has been investigated to reduce user-perceived latency (UPL), which refers to the user&#x27;s waiting time before receiving the system&#x27;s response, in spoken dialogue systems. To reduce the UPL, it is necessary to predict complete user utterances before the end of the user&#x27;s speech, typically by language models, to prepare prefetched dialogue responses. In this study, we proposed a prediction confidence model (PCM) that determines whether prefetching is possible or not by estimating the semantic similarity between the predicted complete user utterance and the complete user utterance. We evaluated our PCM based on the differences between the predicted complete user utterance and the complete user utterance."
   ],
   "p1": 3045,
   "pn": 3049,
   "doi": "10.21437/Interspeech.2025-600",
   "url": "interspeech_2025/mori25b_interspeech.html"
  },
  "wu25d_interspeech": {
   "authors": [
    [
     "Yuan-Kuei",
     "Wu"
    ],
    [
     "Juan Azcarreta",
     "Ortiz"
    ],
    [
     "Kashyap",
     "Patel"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "Jung-Suk",
     "Lee"
    ],
    [
     "Sanha",
     "Lee"
    ],
    [
     "Ashutosh",
     "Pandey"
    ]
   ],
   "title": "A Novel Deep Learning Framework for Efficient Multichannel Acoustic Feedback Control",
   "original": "605",
   "order": 164,
   "page_count": 5,
   "abstract": [
    "This study presents a deep-learning framework for controlling multichannel acoustic feedback in audio devices. Traditional digital signal processing methods struggle with convergence when dealing with high correlated noise such as feedback. We introduce a Convolutional Recurrent Network that efficiently combines spatial and temporal processing, significantly enhancing speech enhancement capabilities with lower computational demands. Our approach utilizes three training methods: In-a-Loop Training, Teacher Forcing, and a Hybrid strategy with a Multichannel Wiener Filter, optimizing performance in complex acoustic environments. This scalable framework offers a robust solution for real-world applications, making significant advances in Acoustic Feedback Control technology."
   ],
   "p1": 788,
   "pn": 792,
   "doi": "10.21437/Interspeech.2025-605",
   "url": "interspeech_2025/wu25d_interspeech.html"
  },
  "yang25e_interspeech": {
   "authors": [
    [
     "Zhao",
     "Yang"
    ],
    [
     "Rui",
     "Jiang"
    ],
    [
     "Yue Heng",
     "Yeo"
    ],
    [
     "Xiao",
     "Fu"
    ],
    [
     "Wei",
     "Xi"
    ],
    [
     "Jizhong",
     "Zhao"
    ]
   ],
   "title": "Visually-Adaptive Guided Robust Speech Recognition with Parameter-Efficient Adaptation",
   "original": "606",
   "order": 1004,
   "page_count": 5,
   "abstract": [
    "Recent developments in large-scale speech foundation models have further pushed the boundaries of automatic speech recognition (ASR) capabilities, making them excellent candidates for integration with multi-modality approaches. In this work, we propose AVWhisper-LoRA, an extension of the Whisper model that incorporates an auxiliary visual encoder to enable audio-visual speech recognition (AVSR) with lightweight trainable parameters. Our approach capitalizes on the existing attention mechanisms of the well-trained Whisper model, facilitating the integration of visual information through both self-attention and cross-attention interactions. Additionally, we introduce LoRA-based trainable lightweight adapters into the frozen Whisper model to enable effective adaptation to the multi-modality target domain during training. Experimental results on the LRS3-TED dataset demonstrate that our method consistently outperforms state-of-the-art methods, particularly in challenging speech environments."
   ],
   "p1": 4938,
   "pn": 4942,
   "doi": "10.21437/Interspeech.2025-606",
   "url": "interspeech_2025/yang25e_interspeech.html"
  },
  "wang25d_interspeech": {
   "authors": [
    [
     "Yuyang",
     "Wang"
    ],
    [
     "Yonghui",
     "Liu"
    ],
    [
     "Jianbing",
     "Liu"
    ],
    [
     "Kai",
     "Niu"
    ],
    [
     "Zhiqiang",
     "He"
    ]
   ],
   "title": "CAGCRN: Real-Time Speech Enhancement with a Lightweight Model for Joint Acoustic Echo Cancellation and Noise Suppression",
   "original": "608",
   "order": 160,
   "page_count": 5,
   "abstract": [
    "The rapid advancement of communication technologies has made acoustic echo cancellation (AEC) and noise suppression (NS) increasingly essential. Most existing research tackles these tasks separately, often cascading models in practical systems, which is not suitable for resource-constrained environments. In contrast, a model that simultaneously addresses both tasks with minimal computational resources can significantly reduce system complexity. Furthermore, traditional AEC systems often encounter challenges with audio signal delays, compromising their effectiveness. This paper proposes the cross-attention gated convolutional recurrent network (CAGCRN), which utilizes a gating mechanism to efficiently collaborate AEC and NS tasks, and a cross-attention mechanism to align delays. Experimental results show that CAGCRN excels in both AEC and NS tasks while requiring minimal computational resources, with only 0.07M parameters, making it ideal for devices with limited capabilities."
   ],
   "p1": 768,
   "pn": 772,
   "doi": "10.21437/Interspeech.2025-608",
   "url": "interspeech_2025/wang25d_interspeech.html"
  },
  "geng25_interspeech": {
   "authors": [
    [
     "Haopeng",
     "Geng"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "A Perception-Based L2 Speech Intelligibility Indicator: Leveraging a Rater’s Shadowing and Sequence-to-sequence Voice Conversion",
   "original": "615",
   "order": 495,
   "page_count": 5,
   "abstract": [
    "Evaluating L2 speech intelligibility is crucial for effective computer-assisted language learning (CALL). Conventional ASR-based methods often focus on native-likeness, which may fail to capture the actual intelligibility perceived by human listeners. In contrast, our work introduces a novel, perception-based L2 speech intelligibility indicator that leverages a native rater’s shadowing data within a sequence-to-sequence (seq2seq) voice conversion framework. By integrating an alignment mechanism and acoustic feature reconstruction, our approach simulates the auditory perception of native listeners, identifying segments in L2 speech that are likely to cause comprehension difficulties. Both objective and subjective evaluations indicate that our method aligns more closely with native judgments than traditional ASR-based metrics, offering a promising new direction for CALL systems in a global, multilingual contexts."
   ],
   "p1": 2420,
   "pn": 2424,
   "doi": "10.21437/Interspeech.2025-615",
   "url": "interspeech_2025/geng25_interspeech.html"
  },
  "lu25c_interspeech": {
   "authors": [
    [
     "Ke-Han",
     "Lu"
    ],
    [
     "Chun-Yi",
     "Kuan"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models",
   "original": "619",
   "order": 423,
   "page_count": 5,
   "abstract": [
    "We introduce Speech-IFEval, an evaluation framework designed to assess instruction-following capabilities and quantify catastrophic forgetting in speech-aware language models (SLMs). Recent SLMs integrate speech perception with large language models (LLMs), often degrading textual capabilities due to speech-centric training. Existing benchmarks conflate speech perception with instruction-following, hindering evaluation of these distinct skills. To address this gap, we provide a benchmark for diagnosing the instruction-following abilities of SLMs. Our findings show that most SLMs struggle with even basic instructions, performing far worse than text-based LLMs. Additionally, these models are highly sensitive to prompt variations, often yielding inconsistent and unreliable outputs. We highlight core challenges and provide insights to guide future research, emphasizing the need for evaluation beyond task-level metrics."
   ],
   "p1": 2078,
   "pn": 2082,
   "doi": "10.21437/Interspeech.2025-619",
   "url": "interspeech_2025/lu25c_interspeech.html"
  },
  "kim25i_interspeech": {
   "authors": [
    [
     "Jin Sob",
     "Kim"
    ],
    [
     "Hyun Joon",
     "Park"
    ],
    [
     "Wooseok",
     "Shin"
    ],
    [
     "Sung Won",
     "Han"
    ]
   ],
   "title": "Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification",
   "original": "628",
   "order": 759,
   "page_count": 5,
   "abstract": [
    "Recent speaker verification studies have achieved notable success by leveraging layer-wise output from pre-trained Transformer models. However, few have explored the advancements in aggregating these multi-level features beyond the static weighted average. We present Layer Attentive Pooling (LAP), a novel strategy for aggregating inter-layer representations from pre-trained speech models for speaker verification. LAP assesses the significance of each layer from multiple perspectives time-dynamically, and employs max pooling instead of averaging. Additionally, we propose a lightweight backend speaker model comprising LAP and Attentive Statistical Temporal Pooling (ASTP) to extract speaker embeddings from pre-trained model output. Experiments on the VoxCeleb benchmark reveal that our compact architecture achieves state-of-the-art performance while greatly reducing the training time. We further analyzed LAP design and its dynamic weighting mechanism for capturing speaker characteristics."
   ],
   "p1": 3713,
   "pn": 3717,
   "doi": "10.21437/Interspeech.2025-628",
   "url": "interspeech_2025/kim25i_interspeech.html"
  },
  "zuo25_interspeech": {
   "authors": [
    [
     "Lishi",
     "Zuo"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "Leveraging Ordinal Information for Speech-based Depression Classification",
   "original": "638",
   "order": 103,
   "page_count": 5,
   "abstract": [
    "While depression is inherently ordinal, much of the previous work in depression detection oversimplifies the problem by treating it as a binary classification problem, ignoring the subtle variations and the order in depression severity. We propose creating a latent space that contains ordinal information via an ordinal loss to benefit the learning of depression classification. Specifically, we define K thresholds for the depression scores, thereby creating a series of binary classification tasks on different levels of depression (e.g., mild vs. non-mild). The ordinal loss allows the model to capture the relationships between these levels on top of the binary classification task. Our approach outperforms current state-of-the-art depression detection methods, highlighting the importance of considering the inherent ordinal nature of depression severity."
   ],
   "p1": 484,
   "pn": 488,
   "doi": "10.21437/Interspeech.2025-638",
   "url": "interspeech_2025/zuo25_interspeech.html"
  },
  "xu25f_interspeech": {
   "authors": [
    [
     "Yaoxun",
     "Xu"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Hangting",
     "Chen"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Rongzhi",
     "Gu"
    ],
    [
     "Yi",
     "Luo"
    ]
   ],
   "title": "WAKE: Watermarking Audio with Key Enrichment",
   "original": "642",
   "order": 1039,
   "page_count": 5,
   "abstract": [
    "As deep learning advances in audio generation, challenges in audio security and copyright protection highlight the need for robust audio watermarking. Recent neural network-based methods have made progress but still face three main issues: preventing unauthorized access, decoding initial watermarks after multiple embeddings, and embedding varying lengths of watermarks. To address these issues, we propose WAKE, the first key-controllable audio watermark framework. WAKE embeds watermarks using specific keys and recovers them with corresponding keys, enhancing security by making incorrect key decoding impossible. It also resolves the overwriting issue by allowing watermark decoding after multiple embeddings and supports variable-length watermark insertion. WAKE outperforms existing models in both watermarked audio quality and watermark detection accuracy."
   ],
   "p1": 5093,
   "pn": 5097,
   "doi": "10.21437/Interspeech.2025-642",
   "url": "interspeech_2025/xu25f_interspeech.html"
  },
  "jin25_interspeech": {
   "authors": [
    [
     "Hojun",
     "Jin"
    ],
    [
     "Eunsoo",
     "Hong"
    ],
    [
     "Ziwon",
     "Hyung"
    ],
    [
     "Sungjun",
     "Lim"
    ],
    [
     "Seungjin",
     "Lee"
    ],
    [
     "Keunseok",
     "Cho"
    ]
   ],
   "title": "Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts",
   "original": "643",
   "order": 455,
   "page_count": 5,
   "abstract": [
    "Hard-parameter sharing is a common strategy to train a single model jointly across diverse tasks. However, this often leads to task interference, impeding overall model performance. To address the issue, we propose a simple yet effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of Experts models, S-MoE eliminates the need for training gating functions by utilizing special guiding tokens to route each task to its designated expert. By assigning each task to a separate feedforward network, S-MoE overcomes the limitations of hard-parameter sharing. We further apply S-MoE to a speech-to-text model, enabling the model to process mixed-bandwidth input while jointly performing automatic speech recognition (ASR) and speech translation (ST). Experimental results demonstrate the effectiveness of the proposed S-MoE, achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder."
   ],
   "p1": 2220,
   "pn": 2224,
   "doi": "10.21437/Interspeech.2025-643",
   "url": "interspeech_2025/jin25_interspeech.html"
  },
  "hou25_interspeech": {
   "authors": [
    [
     "Haoxiang",
     "Hou"
    ],
    [
     "Xun",
     "Gong"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Wei",
     "Wang"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Ranking and Selection of Bias Words for Contextual Bias Speech Recognition",
   "original": "646",
   "order": 1057,
   "page_count": 5,
   "abstract": [
    "Contextual Automatic Speech Recognition (ASR) systems have made significant advancements. However, contextual ASR models face challenges when dealing with a large number of bias words. This paper focuses on addressing the limitations of contextual ASR models in handling a substantial number of bias words. First, to guide the model to focus on the most important words, we propose a novel network serving as a scorer for bias word ranking and selection. Second, as an example, we explore the use of the proposed scorer in conjunction with the contextual Whisper model. We create a new bias word list using a named-entity recognition (NER) model, which is closer to real-world scenarios. The results on the LibriSpeech dataset with the IS21 bias words list demonstrate that bias word ranking and selection can significantly enhance the model&#x27;s performance in recognizing bias words, achieving a relative reduction of over 40% in the Biased Word Error Rate."
   ],
   "p1": 5183,
   "pn": 5187,
   "doi": "10.21437/Interspeech.2025-646",
   "url": "interspeech_2025/hou25_interspeech.html"
  },
  "zhao25e_interspeech": {
   "authors": [
    [
     "Fei",
     "Zhao"
    ],
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Zhong-Qiu",
     "Wang"
    ]
   ],
   "title": "Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation",
   "original": "647",
   "order": 132,
   "page_count": 5,
   "abstract": [
    "Acoustic echo cancellation (AEC) is an important speech signal processing technology that can remove echoes from microphone signals to enable natural-sounding full-duplex speech communication. While single-channel AEC is widely adopted, multi-channel AEC can leverage spatial cues afforded by multiple microphones to achieve better performance. Existing multi-channel AEC approaches typically combine beamforming with deep neural networks (DNN). This work proposes a two-stage algorithm that enhances multi-channel AEC by incorporating sound source directional cues. Specifically, a lightweight DNN is first trained to predict the sound source directions, and then the predicted directional information, multi-channel microphone signals, and single-channel far-end signal are jointly fed into an AEC network to estimate the near-end signal. Evaluation results show that the proposed algorithm outperforms baseline approaches and exhibits robust generalization across diverse acoustic environments."
   ],
   "p1": 629,
   "pn": 633,
   "doi": "10.21437/Interspeech.2025-647",
   "url": "interspeech_2025/zhao25e_interspeech.html"
  },
  "cheng25_interspeech": {
   "authors": [
    [
     "Yifan",
     "Cheng"
    ],
    [
     "Ruoyi",
     "Zhang"
    ],
    [
     "Jiatong",
     "Shi"
    ]
   ],
   "title": "MIKU-PAL: An Automated and Standardized Multimodal Method for Speech Paralinguistic and Affect Labeling",
   "original": "648",
   "order": 878,
   "page_count": 5,
   "abstract": [
    "Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss κ score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning."
   ],
   "p1": 4308,
   "pn": 4312,
   "doi": "10.21437/Interspeech.2025-648",
   "url": "interspeech_2025/cheng25_interspeech.html"
  },
  "yamashita25_interspeech": {
   "authors": [
    [
     "Natsuo",
     "Yamashita"
    ],
    [
     "Masaaki",
     "Yamamoto"
    ],
    [
     "Hiroaki",
     "Kokubo"
    ],
    [
     "Yohei",
     "Kawaguchi"
    ]
   ],
   "title": "LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context",
   "original": "649",
   "order": 747,
   "page_count": 5,
   "abstract": [
    "Generative error correction (GER) with large language models (LLMs) has emerged as an effective post-processing approach to improve automatic speech recognition (ASR) performance. However, it often struggles with rare or domain-specific words due to limited training data. Furthermore, existing LLM-based GER approaches primarily rely on textual information, neglecting phonetic cues, which leads to over-correction. To address these issues, we propose a novel LLM-based GER approach that targets rare words and incorporates phonetic information. First, we generate synthetic data to contain rare words for fine-tuning the GER model. Second, we integrate ASR&#x27;s N-best hypotheses along with phonetic context to mitigate over-correction. Experimental results show that our method not only improves the correction of rare words but also reduces the WER and CER across both English and Japanese datasets."
   ],
   "p1": 3653,
   "pn": 3657,
   "doi": "10.21437/Interspeech.2025-649",
   "url": "interspeech_2025/yamashita25_interspeech.html"
  },
  "niu25_interspeech": {
   "authors": [
    [
     "Ben",
     "Niu"
    ],
    [
     "Yangjie",
     "Wei"
    ],
    [
     "Gang",
     "Yang"
    ],
    [
     "Yuqiao",
     "Wang"
    ],
    [
     "Shengling",
     "Yu"
    ]
   ],
   "title": "StarGAN-Aug: A Cross-domain Fault Audio Generation Method for High-performance Fault Diagnosis of Power Transformers",
   "original": "651",
   "order": 691,
   "page_count": 5,
   "abstract": [
    "Although current data generation methods can enhance the classification performance of existing fault diagnosis models by augmenting the limited training set, their performance improvement is still restricted due to the low quality and insufficient diversity of generated data. To tackle this issue, we propose a cross-domain fault audio generation method based on the improved star generative adversarial networks, namely StarGAN-Aug. In the StarGAN-Aug, the model structure is first optimized using a pre-trained JDC network and a style encoder, leading to efficiently extract the specific frequency in fault signals and generate diverse audio samples across domains. Moreover, we add a comprehensive objective to optimize the model training and enhance its ability to ensure that the generated samples have both high fidelity and diversity. Experimental results show that using our generated audio samples, the classification performance of existing diagnostic models can be significantly improved."
   ],
   "p1": 3399,
   "pn": 3403,
   "doi": "10.21437/Interspeech.2025-651",
   "url": "interspeech_2025/niu25_interspeech.html"
  },
  "nguyen25_interspeech": {
   "authors": [
    [
     "Tuan",
     "Nguyen"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages",
   "original": "654",
   "order": 157,
   "page_count": 5,
   "abstract": [
    "Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry."
   ],
   "p1": 753,
   "pn": 757,
   "doi": "10.21437/Interspeech.2025-654",
   "url": "interspeech_2025/nguyen25_interspeech.html"
  },
  "li25h_interspeech": {
   "authors": [
    [
     "Jin",
     "Li"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Johan",
     "Rohdin"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Bayesian Learning for Domain-Invariant Speaker Verification and Anti-Spoofing",
   "original": "655",
   "order": 231,
   "page_count": 5,
   "abstract": [
    "The performance of automatic speaker verification (ASV) and anti-spoofing drops seriously under real-world domain mismatch conditions. The relaxed instance frequency-wise normalization (RFN), which normalizes the frequency components based on the feature statistics along the time and channel axes, is a promising approach to reducing the domain dependence in the feature maps of a speaker embedding network. We advocate that the different frequencies should receive different weights and that the weights&#x27; uncertainty due to domain shift should be accounted for. To these ends, we propose leveraging variational inference to model the posterior distribution of the weights, which results in Bayesian weighted RFN (BWRFN). This approach overcomes the limitations of fixed-weight RFN, making it more effective under domain mismatch conditions. Extensive experiments on cross-dataset ASV, cross-TTS anti-spoofing, and spoofing-robust ASV show that BWRFN is significantly better than WRFN and RFN."
   ],
   "p1": 1123,
   "pn": 1127,
   "doi": "10.21437/Interspeech.2025-655",
   "url": "interspeech_2025/li25h_interspeech.html"
  },
  "geng25b_interspeech": {
   "authors": [
    [
     "Yizhong",
     "Geng"
    ],
    [
     "Wenxin",
     "Fu"
    ],
    [
     "Qihang",
     "Lu"
    ],
    [
     "Bingsong",
     "Bai"
    ],
    [
     "Cong",
     "Wang"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "EEG-based Voice Conversion : Hearing the Voice of Your Brain",
   "original": "656",
   "order": 848,
   "page_count": 5,
   "abstract": [
    "The connection between Electroencephalography (EEG) signals and human voice has gained significant attention, with studies demonstrating the feasibility of speech synthesis from EEG data. However, EEG-based voice conversion (VC) remains largely unexplored. To address this, we present the first EEG-based zero-shot VC system that converts speech into a target speaker’s voice without prior target data. Our method integrates an EEG feature extraction module with an alignment module to map EEG features to speaker-specific voice features. By leveraging an innovative three-stage training strategy and a pre-trained VC model—trained solely on speech data—we achieve zero-shot conversion. Experiments on the Single-Word-Production Dutch-iBIDS dataset confirm the system’s ability to reliably convert speech to a target speaker’s voice. This work highlights the potential of EEG-based VC for advancing assistive communication and brain-computer interfaces. All demos are available in https://doi.org/10.5281/zenodo.15510829."
   ],
   "p1": 4158,
   "pn": 4162,
   "doi": "10.21437/Interspeech.2025-656",
   "url": "interspeech_2025/geng25b_interspeech.html"
  },
  "gao25c_interspeech": {
   "authors": [
    [
     "Lingyun",
     "Gao"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts",
   "original": "658",
   "order": 581,
   "page_count": 5,
   "abstract": [
    "Automatic reading aloud evaluation can provide valuable support to teachers by enabling more efficient scoring of reading exercises. However, research on reading evaluation systems and applications remains limited. We present a novel multimodal approach that leverages audio and knowledge from text resources. In particular, we explored the potential of using Whisper and instruction-tuned large language models (LLMs) with prompts to improve transcriptions for child speech recognition, as well as their effectiveness in downstream reading mistake detection. Our results demonstrate the effectiveness of prompting Whisper and prompting LLM, compared to the baseline Whisper model without prompting. The best performing system achieved state-of-the-art recognition performance in Dutch child read speech, with a word error rate (WER) of 5.1%, improving the baseline WER of 9.4%. Furthermore, it significantly improved reading mistake detection, increasing the F1 score from 0.39 to 0.73."
   ],
   "p1": 2850,
   "pn": 2854,
   "doi": "10.21437/Interspeech.2025-658",
   "url": "interspeech_2025/gao25c_interspeech.html"
  },
  "ohnaka25_interspeech": {
   "authors": [
    [
     "Hien",
     "Ohnaka"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Byeongseon",
     "Park"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ]
   ],
   "title": "Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning",
   "original": "661",
   "order": 516,
   "page_count": 5,
   "abstract": [
    "We propose a model to obtain phonemic and prosodic labels of speech that are coherent with graphemes. Unlike previous methods that simply fine-tune a pre-trained ASR model with the labels, the proposed model conditions the label generation on corresponding graphemes by two methods: 1) Add implicit grapheme conditioning through prompt encoder using pre-trained BERT features. 2) Explicitly prune the label hypotheses inconsistent with the grapheme during inference. These methods enable obtaining parallel data of speech, the labels, and graphemes, which is applicable to various downstream tasks such as text-to-speech and accent estimation from text. Experiments showed that the proposed method significantly improved the consistency between graphemes and the predicted labels. Further, experiments on accent estimation task confirmed that the created parallel data by the proposed method effectively improve the estimation accuracy."
   ],
   "p1": 2525,
   "pn": 2529,
   "doi": "10.21437/Interspeech.2025-661",
   "url": "interspeech_2025/ohnaka25_interspeech.html"
  },
  "lin25c_interspeech": {
   "authors": [
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Huang-Cheng",
     "Chou"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach",
   "original": "662",
   "order": 418,
   "page_count": 5,
   "abstract": [
    "While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a drop of less than 3.6% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential when explicit demographic information is unavailable."
   ],
   "p1": 2053,
   "pn": 2057,
   "doi": "10.21437/Interspeech.2025-662",
   "url": "interspeech_2025/lin25c_interspeech.html"
  },
  "white25_interspeech": {
   "authors": [
    [
     "Hannah",
     "White"
    ],
    [
     "Joshua",
     "Penney"
    ],
    [
     "Felicity",
     "Cox"
    ]
   ],
   "title": "Variability in Intervocalic /t/ and Community Diversity in Australian English ",
   "original": "664",
   "order": 26,
   "page_count": 5,
   "abstract": [
    "The voiceless alveolar stop /t/ exhibits considerable variation in English. Realisations of /t/ vary depending phonetic context and social factors such as gender, age and socioeconomic status. Generally, studies on Australian English have focused on the “mainstream” variety, without acknowledging the wide range of linguistic diversity speakers are exposed to in contemporary multicultural Australian society. In the present paper, we explore intervocalic /t/ variation in data collected from 183 speakers as part of the Multicultural Australian English – Voices of Sydney corpus. Results show that, in certain phonetic contexts, exposure to community linguistic diversity can affect intervocalic /t/ realisation, with speakers from more diverse areas showing a preference for a single variant (the tap) compared to those from less diverse areas. We interpret this as an example of simplification that can occur in diverse communities where there is extreme variability in ambient language exposure."
   ],
   "p1": 121,
   "pn": 125,
   "doi": "10.21437/Interspeech.2025-664",
   "url": "interspeech_2025/white25_interspeech.html"
  },
  "lo25_interspeech": {
   "authors": [
    [
     "Justin J. H.",
     "Lo"
    ],
    [
     "Patrycja",
     "Strycharczuk"
    ],
    [
     "Sam",
     "Kirkham"
    ]
   ],
   "title": "Articulatory Strategy in Vowel Production as a Basis for Speaker Discrimination",
   "original": "666",
   "order": 713,
   "page_count": 5,
   "abstract": [
    "The way speakers articulate is well known to be variable across individuals while at the same time subject to anatomical and biomechanical constraints. In this study, we ask whether articulatory strategy in vowel production can be sufficiently speaker-specific to form the basis for speaker discrimination. We conducted Generalised Procrustes Analyses of tongue shape data from 40 English speakers from the North West of England, and assessed the speaker-discriminatory potential of orthogonal tongue shape features within the framework of likelihood ratios. Tongue size emerged as the individual dimension with the strongest discriminatory power, while tongue shape variation in the more anterior part of the tongue generally outperformed tongue shape variation in the posterior part. When considered in combination, shape-only information may offer comparable levels of speaker specificity to size-and-shape information, but only when features do not exhibit speaker-level co-variation."
   ],
   "p1": 3504,
   "pn": 3508,
   "doi": "10.21437/Interspeech.2025-666",
   "url": "interspeech_2025/lo25_interspeech.html"
  },
  "oconnorrussell25_interspeech": {
   "authors": [
    [
     "Sam",
     "O'Connor Russell"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Visual Cues Support Robust Turn-taking Prediction in Noise",
   "original": "668",
   "order": 221,
   "page_count": 5,
   "abstract": [
    "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic human-robot interaction. However, little is known about their performance in noise. This study therefore explores PTTM performance in types of noise likely to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10 dB music noise. Training with noisy data enables a multimodal PTTM, which includes visual features to better exploit visual cues, with 72% accuracy in 10 dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all noise types and SNRs, highlighting its ability to exploit visual cues; however, this does not always generalise to new types of noise. Analysis also reveals that successful training relies on accurate transcription, limiting the use of ASR-derived transcriptions to clean conditions. We make code publicly available for future research."
   ],
   "p1": 1073,
   "pn": 1077,
   "doi": "10.21437/Interspeech.2025-668",
   "url": "interspeech_2025/oconnorrussell25_interspeech.html"
  },
  "hartuv25_interspeech": {
   "authors": [
    [
     "Nadav",
     "Har-Tuv"
    ],
    [
     "Or",
     "Tal"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
   "original": "669",
   "order": 714,
   "page_count": 5,
   "abstract": [
    "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation."
   ],
   "p1": 3509,
   "pn": 3513,
   "doi": "10.21437/Interspeech.2025-669",
   "url": "interspeech_2025/hartuv25_interspeech.html"
  },
  "han25b_interspeech": {
   "authors": [
    [
     "Seungu",
     "Han"
    ],
    [
     "Sungho",
     "Lee"
    ],
    [
     "Juheon",
     "Lee"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Few-step Adversarial Schrödinger Bridge for Generative Speech Enhancement",
   "original": "673",
   "order": 487,
   "page_count": 5,
   "abstract": [
    "Deep generative models have recently been employed for speech enhancement to generate perceptually valid clean speech on large-scale datasets. Several diffusion models have been proposed, and more recently, a tractable Schrödinger Bridge has been introduced to transport between the clean and noisy speech distributions. However, these models often suffer from an iterative reverse process and require a large number of sampling steps—more than 50. Our investigation reveals that the performance of baseline models significantly degrades when the number of sampling steps is reduced, particularly under low-SNR conditions. We propose integrating Schrödinger Bridge with GANs to effectively mitigate this issue, achieving high-quality outputs on full-band datasets while substantially reducing the required sampling steps. Experimental results demonstrate that our proposed model outperforms existing baselines, even with a single inference step, in both denoising and dereverberation tasks."
   ],
   "p1": 2380,
   "pn": 2384,
   "doi": "10.21437/Interspeech.2025-673",
   "url": "interspeech_2025/han25b_interspeech.html"
  },
  "nguyen25b_interspeech": {
   "authors": [
    [
     "Thai-Binh",
     "Nguyen"
    ],
    [
     "Ngoc-Quan",
     "Pham"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Cocktail-Party Audio-Visual Speech Recognition",
   "original": "676",
   "order": 373,
   "page_count": 5,
   "abstract": [
    "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues."
   ],
   "p1": 1828,
   "pn": 1832,
   "doi": "10.21437/Interspeech.2025-676",
   "url": "interspeech_2025/nguyen25b_interspeech.html"
  },
  "luo25_interspeech": {
   "authors": [
    [
     "Yu-Xiang",
     "Luo"
    ],
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Ming-To",
     "Chuang"
    ],
    [
     "Jia-Hung",
     "Chen"
    ],
    [
     "I-Ning",
     "Tsai"
    ],
    [
     "Pei Xing",
     "Kiew"
    ],
    [
     "Yueh-Hsuan",
     "Huang"
    ],
    [
     "Chien-Feng",
     "Liu"
    ],
    [
     "Yu-Chen",
     "Chen"
    ],
    [
     "Bo-Han",
     "Feng"
    ],
    [
     "Wenze",
     "Ren"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality",
   "original": "679",
   "order": 818,
   "page_count": 5,
   "abstract": [
    "Despite extensive research on toxic speech detection in text, a critical gap remains in handling spoken Mandarin audio. The lack of annotated datasets that capture the unique prosodic cues and culturally specific expressions in Mandarin leaves spoken toxicity underexplored. To address this, we introduce ToxicTone—the largest public dataset of its kind—featuring detailed annotations that distinguish both forms of toxicity (e.g., profanity, bullying) and sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data, sourced from diverse real-world audio and organized into 13 topical categories, mirrors authentic communication scenarios. We also propose a multimodal detection framework that integrates acoustic, linguistic, and emotional features using state-of-the-art speech and emotion encoders. Extensive experiments show our approach outperforms text-only and baseline models, underscoring the essential role of speech-specific cues in revealing hidden toxic expressions."
   ],
   "p1": 4008,
   "pn": 4012,
   "doi": "10.21437/Interspeech.2025-679",
   "url": "interspeech_2025/luo25_interspeech.html"
  },
  "zhao25f_interspeech": {
   "authors": [
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Zexu",
     "Pan"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment",
   "original": "680",
   "order": 607,
   "page_count": 5,
   "abstract": [
    "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech processing toolkit designed to bridge cutting-edge research and practical application. Unlike broad platforms like SpeechBrain and ESPnet, ClearerVoice-Studio focuses on interconnected speech tasks of speech enhancement, separation, super-resolution, and multimodal target speaker extraction. A key advantage is its state-of-the-art pretrained models, including FRCRN (3M+ uses) and MossFormer (2.5M+ uses), optimized for real-world scenarios. It also offers model optimization tools, multi-format audio support, the SpeechScore evaluation toolkit, and user-friendly interfaces, catering to researchers, developers, and end-users. Its rapid adoption (2.8K GitHub stars, 200+ forks) highlights its academic and industrial impact. This paper details ClearerVoice-Studio’s capabilities, architectures, training strategies, benchmarks, community impact, and future plan."
   ],
   "p1": 2980,
   "pn": 2984,
   "doi": "10.21437/Interspeech.2025-680",
   "url": "interspeech_2025/zhao25f_interspeech.html"
  },
  "gaznepoglu25_interspeech": {
   "authors": [
    [
     "Ünal Ege",
     "Gaznepoglu"
    ],
    [
     "Anna",
     "Leschanowsky"
    ],
    [
     "Ahmad",
     "Aloradi"
    ],
    [
     "Prachi",
     "Singh"
    ],
    [
     "Daniel",
     "Tenbrinck"
    ],
    [
     "Emanuël A. P.",
     "Habets"
    ],
    [
     "Nils",
     "Peters"
    ]
   ],
   "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks",
   "original": "681",
   "order": 864,
   "page_count": 5,
   "abstract": [
    "Speaker anonymization systems hide the identity of speakers while preserving other information such as linguistic content and emotions. To evaluate their privacy benefits, attacks in the form of automatic speaker verification (ASV) systems are employed. In this study, we assess the impact of intra-speaker linguistic content similarity in the attacker training and evaluation datasets, by adapting BERT, a language model, as an ASV system. On the VoicePrivacy Attacker Challenge datasets, our method achieves a mean equal error rate (EER) of 35%, with certain speakers attaining EERs as low as 2%, based solely on the textual content of their utterances. Our explainability study reveals that the system decisions are linked to semantically similar keywords within utterances, stemming from how LibriSpeech is curated. Our study suggests reworking the VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge the reliance on global EER for privacy evaluations."
   ],
   "p1": 4238,
   "pn": 4242,
   "doi": "10.21437/Interspeech.2025-681",
   "url": "interspeech_2025/gaznepoglu25_interspeech.html"
  },
  "nam25_interspeech": {
   "authors": [
    [
     "Yunjae",
     "Nam"
    ],
    [
     "Jeong U",
     "Han"
    ],
    [
     "Kiyeon",
     "Kim"
    ],
    [
     "Jaemin",
     "Lim"
    ]
   ],
   "title": "Parameter-efficient Fine-tuning of Conformer-based Streaming Speech Recognition into Non-streaming Models",
   "original": "685",
   "order": 900,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a parameter-efficient fine-tuning method for converting streaming speech recognition models into non-streaming models. We introduce the Full-Context Adapter, a modified residual adapter designed to incorporate full-context information, enabling non-streaming conversion while keeping the pre-trained streaming model. The proposed method integrates the Full-Context Adapter into a Conformer-based RNN-Transducer streaming model, allowing it to operate as a non-streaming model. We conduct experiments to evaluate the effectiveness of our method, demonstrating that it achieves comparable performance to a non-streaming baseline model while requiring &lt;10% additional parameters. Furthermore, we analyze optimal hyperparameter configurations and explore performance enhancement strategies. Our results show that the proposed approach provides a simple yet effective solution for efficiently operating both streaming and non-streaming ASR."
   ],
   "p1": 4418,
   "pn": 4422,
   "doi": "10.21437/Interspeech.2025-685",
   "url": "interspeech_2025/nam25_interspeech.html"
  },
  "lahtinen25_interspeech": {
   "authors": [
    [
     "Kalle",
     "Lahtinen"
    ],
    [
     "Einari",
     "Vaaras"
    ],
    [
     "Liisa",
     "Mustanoja"
    ],
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus",
   "original": "687",
   "order": 808,
   "page_count": 5,
   "abstract": [
    "Study of affect in speech requires suitable data, as emotional expression and perception vary across languages. Until now, no corpus has existed for natural expression of affect in spontaneous Finnish, existing data being acted or from a very specific communicative setting. This paper presents the first such corpus, created by annotating 12,000 utterances for emotional arousal and valence, sampled from three large-scale Finnish speech corpora. To ensure diverse affective expression, sample selection was conducted with an affect mining approach combining acoustic, cross-linguistic speech emotion, and text sentiment features. We compare this method to random sampling in terms of annotation diversity, and conduct post-hoc analyses to identify sampling choices that would have maximized the diversity. As an outcome, the work introduces a spontaneous Finnish affective speech corpus and informs sampling strategies for affective speech corpus creation in other languages or domains."
   ],
   "p1": 3958,
   "pn": 3962,
   "doi": "10.21437/Interspeech.2025-687",
   "url": "interspeech_2025/lahtinen25_interspeech.html"
  },
  "liu25d_interspeech": {
   "authors": [
    [
     "Xin",
     "Liu"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "HWB-Net: A Novel High-Performance and Efficient Hybrid Waveform Bandwidth Extension Method",
   "original": "692",
   "order": 834,
   "page_count": 5,
   "abstract": [
    "Speech Bandwidth Extension (BWE) reconstructs missing high-frequency components in narrowband signals to enhance perceptual quality. Conventional signal-processing methods face performance limitations, while deep-learning approaches require substantial computational resources. To address these issues, we present HWB-Net, a novel Hybrid Waveform Bandwidth network that combines half-wave rectified signals with raw waveforms through a lightweight architecture, with a lightweight architecture, reducing both complexity and cost. Specifically, integrating half-wave rectification into BAE-NET-Lite enhances the model’s understanding of speech signals, leading to improved subjective and objective results, and replacing the decoder with a simple linear layer plus a weighted Gaussian mixture model (WGMM) significantly cuts down parameters (0.20M) and computational complexity (0.013G/s). Evaluations demonstrate HWB-Net’s competitive performance and practical viability for real-time communication through balanced efficiency-accuracy tradeoffs."
   ],
   "p1": 4088,
   "pn": 4092,
   "doi": "10.21437/Interspeech.2025-692",
   "url": "interspeech_2025/liu25d_interspeech.html"
  },
  "vieting25_interspeech": {
   "authors": [
    [
     "Peter",
     "Vieting"
    ],
    [
     "Maximilian",
     "Kannen"
    ],
    [
     "Benedikt",
     "Hilmes"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition",
   "original": "694",
   "order": 1005,
   "page_count": 5,
   "abstract": [
    "Neural front-ends are an appealing alternative to traditional, fixed feature extraction pipelines for automatic speech recognition (ASR) systems since they can be directly trained to fit the acoustic model. However, their performance often falls short compared to classical methods, which we show is largely due to their increased susceptibility to overfitting. This work therefore investigates regularization methods for training ASR models with learnable feature extraction front-ends. First, we examine audio perturbation methods and show that larger relative improvements can be obtained for learnable features. Additionally, we identify two limitations in the standard use of SpecAugment for these front-ends and propose masking in the short time Fourier transform (STFT)-domain as a simple but effective modification to address these challenges. Finally, integrating both regularization approaches effectively closes the performance gap between traditional and learnable features."
   ],
   "p1": 4943,
   "pn": 4947,
   "doi": "10.21437/Interspeech.2025-694",
   "url": "interspeech_2025/vieting25_interspeech.html"
  },
  "damianos25_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Damianos"
    ],
    [
     "Georgios",
     "Paraskevopoulos"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR",
   "original": "695",
   "order": 789,
   "page_count": 5,
   "abstract": [
    "In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training."
   ],
   "p1": 3863,
   "pn": 3867,
   "doi": "10.21437/Interspeech.2025-695",
   "url": "interspeech_2025/damianos25_interspeech.html"
  },
  "cavalcanti25_interspeech": {
   "authors": [
    [
     "Julio Cesar",
     "Cavalcanti"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "``Dyadosyncrasy'', Idiosyncrasy and Demographic Factors in Turn-Taking",
   "original": "697",
   "order": 225,
   "page_count": 5,
   "abstract": [
    "Turn-taking in dialogue follows universal constraints but also varies significantly. This study examines how demographic (sex, age, education) and individual factors shape turn-taking using a large dataset of US English conversations (Fisher). We analyze Transition Floor Offset (TFO) and find notable interspeaker variation. Sex and age have small but significant effects — female speakers and older individuals exhibit slightly shorter offsets — while education shows no effect. Lighter topics correlate with shorter TFOs. However, individual differences have a greater impact, driven by a strong idiosyncratic and an even stronger &quot;dyadosyncratic&quot; component — speakers in a dyad resemble each other more than they resemble themselves in different dyads. This suggests that the dyadic relationship and joint activity are the strongest determinants of TFO, outweighing demographic influences."
   ],
   "p1": 1093,
   "pn": 1097,
   "doi": "10.21437/Interspeech.2025-697",
   "url": "interspeech_2025/cavalcanti25_interspeech.html"
  },
  "chen25e_interspeech": {
   "authors": [
    [
     "Jia-Xin",
     "Chen"
    ],
    [
     "Yi-Ming",
     "Wang"
    ],
    [
     "Ziyu",
     "Zhang"
    ],
    [
     "Jiayang",
     "Han"
    ],
    [
     "Yin-Long",
     "Liu"
    ],
    [
     "Rui",
     "Feng"
    ],
    [
     "Xiuyuan",
     "Liang"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Jia-Hong",
     "Yuan"
    ]
   ],
   "title": "Decoding Speaker-Normalized Pitch from EEG for Mandarin Perception",
   "original": "700",
   "order": 215,
   "page_count": 5,
   "abstract": [
    "The same speech content produced by different speakers exhibits significant differences in pitch contour, yet listeners&#x27; semantic perception remains unaffected. This phenomenon may stem from the brain&#x27;s perception of pitch contours being independent of individual speakers&#x27; pitch ranges. In this work, we recorded electroencephalogram (EEG) while participants listened to Mandarin monosyllables with varying tones, phonemes, and speakers. The CE-ViViT model is proposed to decode raw or speaker-normalized pitch contours directly from EEG. Experimental results demonstrate that the proposed model can decode pitch contours with modest errors, achieving performance comparable to state-of-the-art EEG regression methods. Moreover, speaker-normalized pitch contours were decoded more accurately, supporting the neural encoding of relative pitch."
   ],
   "p1": 1043,
   "pn": 1047,
   "doi": "10.21437/Interspeech.2025-700",
   "url": "interspeech_2025/chen25e_interspeech.html"
  },
  "hu25c_interspeech": {
   "authors": [
    [
     "Jiaxi",
     "Hu"
    ],
    [
     "Leyuan",
     "Qu"
    ],
    [
     "Haoxun",
     "Li"
    ],
    [
     "Taihao",
     "Li"
    ]
   ],
   "title": "Label Semantic-Driven Contrastive Learning for Speech Emotion Recognition",
   "original": "703",
   "order": 886,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) is crucial for human-computer interaction applications. However, SER remains a challenging task due to limited datasets and ambiguous emotion boundaries. While Self-Supervised Learning (SSL) models have demonstrated considerable success in speech processing tasks, existing approaches still struggle to distinguish subtle emotional variations. In this paper, we propose a novel Label Semantic-driven Contrastive Learning framework (LaSCL) that integrates emotion label semantic embeddings into speech representation learning. Our method uses label embeddings as semantic anchors to explicitly model relationships between emotions and employ a label divergence loss to better establish clearer emotion boundaries. Experiments on the widely used IEMOCAP benchmark indicate that LaSCL achieves state-of-the-art performance compared with previous methods."
   ],
   "p1": 4348,
   "pn": 4352,
   "doi": "10.21437/Interspeech.2025-703",
   "url": "interspeech_2025/hu25c_interspeech.html"
  },
  "gao25d_interspeech": {
   "authors": [
    [
     "Changfeng",
     "Gao"
    ],
    [
     "Zhihao",
     "Du"
    ],
    [
     "Shiliang",
     "Zhang"
    ]
   ],
   "title": "Differentiable Reward Optimization for LLM based TTS system",
   "original": "704",
   "order": 501,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system&#x27;s capability to follow instructions effectively. Experimental results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner."
   ],
   "p1": 2450,
   "pn": 2454,
   "doi": "10.21437/Interspeech.2025-704",
   "url": "interspeech_2025/gao25d_interspeech.html"
  },
  "wang25e_interspeech": {
   "authors": [
    [
     "Qiongqiong",
     "Wang"
    ],
    [
     "Hardik B.",
     "Sailor"
    ],
    [
     "Tianchi",
     "Liu"
    ],
    [
     "Ai Ti",
     "Aw"
    ]
   ],
   "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation",
   "original": "706",
   "order": 807,
   "page_count": 5,
   "abstract": [
    "Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM&#x27;s limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities."
   ],
   "p1": 3953,
   "pn": 3957,
   "doi": "10.21437/Interspeech.2025-706",
   "url": "interspeech_2025/wang25e_interspeech.html"
  },
  "tienkamp25_interspeech": {
   "authors": [
    [
     "Thomas",
     "Tienkamp"
    ],
    [
     "Fleur",
     "van Ast"
    ],
    [
     "Roos",
     "van der Veen"
    ],
    [
     "Teja",
     "Rebernik"
    ],
    [
     "Raoul",
     "Buurke"
    ],
    [
     "Nikki",
     "Hoekzema"
    ],
    [
     "Katharina",
     "Polsterer"
    ],
    [
     "Hedwig",
     "Sekeres"
    ],
    [
     "Rob",
     "van Son"
    ],
    [
     "Martijn",
     "Wieling"
    ],
    [
     "Max",
     "Witjes"
    ],
    [
     "Sebastiaan",
     "de Visscher"
    ],
    [
     "Defne",
     "Abur"
    ]
   ],
   "title": "Articulatory clarity and variability before and after surgery for tongue cancer",
   "original": "708",
   "order": 728,
   "page_count": 5,
   "abstract": [
    "Surgical treatment for tongue cancer can negatively affect the mobility and musculature of the tongue, which can influence articulatory clarity and variability. In this study, we investigated articulatory clarity through the vowel articulation index (VAI) and variability through vowel formant dispersion (VFD). Using a sentence reading task, we assessed 11 individuals pre and six months post tongue cancer surgery, alongside 11 sex- and age-matched typical speakers. Our results show that while the VAI was significantly smaller post-surgery compared to pre-surgery, there was no significant difference between patients and typical speakers at either time point. Post-surgery, speakers had higher VFD values for /i/ compared to pre-surgery and typical speakers, signalling higher variability. Taken together, our results suggest that while articulatory clarity remained within typical ranges following surgery for tongue cancer for the speakers in our study, articulatory variability increased."
   ],
   "p1": 3558,
   "pn": 3562,
   "doi": "10.21437/Interspeech.2025-708",
   "url": "interspeech_2025/tienkamp25_interspeech.html"
  },
  "chen25f_interspeech": {
   "authors": [
    [
     "Yafeng",
     "Chen"
    ],
    [
     "Chong",
     "Deng"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Yiheng",
     "Jiang"
    ],
    [
     "Han",
     "Yin"
    ],
    [
     "Qian",
     "Chen"
    ],
    [
     "Wen",
     "Wang"
    ]
   ],
   "title": "Pushing the Frontiers of Self-Distillation Prototypes Network with Dimension Regularization and Score Normalization",
   "original": "715",
   "order": 754,
   "page_count": 5,
   "abstract": [
    "Developing robust speaker verification (SV) systems without speaker labels has been a longstanding challenge. Earlier research has highlighted a considerable performance gap between self-supervised and fully supervised approaches. In this paper, we enhance the non-contrastive self-supervised framework, Self-Distillation Prototypes Network (SDPN), by introducing dimension regularization that explicitly addresses the collapse problem through the application of regularization terms to speaker embeddings. Moreover, we integrate score normalization techniques from fully supervised SV to further bridge the gap toward supervised verification performance. SDPN with dimension regularization and score normalization sets a new state-of-the-art on the VoxCeleb1 speaker verification evaluation benchmark, achieving Equal Error Rate 1.29%, 1.60%, and 2.80% for trial VoxCeleb1-{O,E,H} respectively. These results demonstrate relative improvements of 28.3%, 19.6%, and 22.6% over the current best self-supervised methods, thereby advancing the frontiers of SV technology."
   ],
   "p1": 3688,
   "pn": 3692,
   "doi": "10.21437/Interspeech.2025-715",
   "url": "interspeech_2025/chen25f_interspeech.html"
  },
  "zhou25b_interspeech": {
   "authors": [
    [
     "Qian",
     "Zhou"
    ],
    [
     "Mathilde",
     "Hutin"
    ]
   ],
   "title": "Evaluation of Three Automatic Alignment Tools for the Processing of Non-native French",
   "original": "716",
   "order": 18,
   "page_count": 5,
   "abstract": [
    "The production of non-native speech is known to display &quot;cross-language phonetic interference&quot;, which makes such speech uneasy to align and label automatically. Automatic phonetic alignment refers to an automated process whereby software synchronizes speech with its transcription, usually at the phone and word levels. This method has proven useful and reliable for native speech, yet this reliability usually does not extend to non-native speech. This paper proposes to test three major automatic aligners (WebMAUS, MFA and SPPAS) on non-native French uttered by two native speakers of Chinese by comparing them with two manual segmentations. This paper&#x27;s goal is to offer non-computer linguists a preliminary investigation on which to rely when choosing a tool for their studies in non-native phonetics or language didactics. Results show that the best performing tool for labeling is SPPAS while the best performing tool for both word- and phone-segmentation overall is WebMAUS and MFA the worst."
   ],
   "p1": 81,
   "pn": 85,
   "doi": "10.21437/Interspeech.2025-716",
   "url": "interspeech_2025/zhou25b_interspeech.html"
  },
  "kim25j_interspeech": {
   "authors": [
    [
     "Taewoo",
     "Kim"
    ],
    [
     "Guisik",
     "Kim"
    ],
    [
     "Choongsang",
     "Cho"
    ],
    [
     "Young Han",
     "Lee"
    ]
   ],
   "title": "Naturalness-Aware Curriculum Learning with Dynamic Temperature for Speech Deepfake Detection",
   "original": "717",
   "order": 1085,
   "page_count": 5,
   "abstract": [
    "Recent advances in speech deepfake detection (SDD) have significantly improved artifacts-based detection in spoofed speech. However, most models overlook speech naturalness, a crucial cue for distinguishing bona fide speech from spoofed speech. This study proposes naturalness-aware curriculum learning, a novel training framework that leverages speech naturalness to enhance the robustness and generalization of SDD. This approach measures sample difficulty using both ground-truth labels and mean opinion scores, and adjusts the training schedule to progressively introduce more challenging samples. To further improve generalization, a dynamic temperature scaling method based on speech naturalness is incorporated into the training process. A 23% relative reduction in the EER was achieved in the experiments on the ASVspoof 2021 DF dataset, without modifying the model architecture. Ablation studies confirmed the effectiveness of naturalness-aware training strategies for SDD tasks."
   ],
   "p1": 5318,
   "pn": 5322,
   "doi": "10.21437/Interspeech.2025-717",
   "url": "interspeech_2025/kim25j_interspeech.html"
  },
  "huang25d_interspeech": {
   "authors": [
    [
     "Yi",
     "Huang"
    ],
    [
     "Si",
     "Chen"
    ],
    [
     "Jingyu",
     "Yao"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "Modeling Multi-Turn Spoken Language Understanding with Dynamic Graph Convolutional Networks",
   "original": "718",
   "order": 839,
   "page_count": 5,
   "abstract": [
    "Spoken Language Understanding (SLU) stands as a pivotal element within task-oriented dialogue systems, where it leverages the dialogue context to steer intent detection at the utterance level and slot filling at the token level. Nonetheless, the challenge of judiciously assimilating dialogue context into multi-turn SLU persists as a formidable hurdle. This difficulty is compounded by the inherently dynamic distribution of conversational information in real-world settings, where key details, such as shifts in intent and behavior, can be overshadowed by a deluge of less critical data. To address this issue, we introduce a novel SLU model tailored for intent detection and slot filling in multi-turn interactions. Central to our approach is the incorporation of a dynamic graph convolutional network that selectively amalgamates essential historical information into the dialogue context, thereby enhancing the model&#x27;s sensitivity to contextually relevant cues. We subject our model to rigorous evaluation on three widely recognized benchmark datasets: SIM, CMCC and FewJoint. The experimental outcomes underscore the model&#x27;s superior performance, and further results from real-scene datasets strengthen the effectiveness of the method."
   ],
   "p1": 4113,
   "pn": 4117,
   "doi": "10.21437/Interspeech.2025-718",
   "url": "interspeech_2025/huang25d_interspeech.html"
  },
  "parcollet25_interspeech": {
   "authors": [
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Yuan",
     "Tseng"
    ],
    [
     "Shucong",
     "Zhang"
    ],
    [
     "Rogier C.",
     "van Dalen"
    ]
   ],
   "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use",
   "original": "720",
   "order": 827,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) research is driven by the availability of common datasets between industrial researchers and academics, encouraging comparisons and evaluations. LibriSpeech, despite its long success as an ASR benchmark, is now limited by its size and focus on clean, read speech, leading to near-zero word error rates. More recent datasets, including MOSEL, YODAS, Gigaspeech, OWSM, Libriheavy or People&#x27;s Speech suffer from major limitations including licenses that researchers in the industry cannot use, unreliable transcriptions, incorrect audio data, or the lack of evaluation sets. This work presents the Loquacious Set, a 25,000-hour curated collection of commercially usable English speech. Featuring hundreds of thousands of speakers with diverse accents and a wide range of speech types (read, spontaneous, talks, clean, noisy), the Loquacious Set is designed to work for academics and researchers in the industry to build ASR systems in real-world scenarios."
   ],
   "p1": 4053,
   "pn": 4057,
   "doi": "10.21437/Interspeech.2025-720",
   "url": "interspeech_2025/parcollet25_interspeech.html"
  },
  "hu25d_interspeech": {
   "authors": [
    [
     "Shujie",
     "Hu"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Huimeng",
     "Wang"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Chengxi",
     "Deng"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech Foundation Models for Dysarthric Speech Recognition",
   "original": "721",
   "order": 938,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel MoE-based speaker adaptation framework for foundation models based dysarthric speech recognition. This approach enables zero-shot adaptation and real-time processing while incorporating domain knowledge. Speech impairment severity and gender conditioned adapter experts are dynamically combined using on-the-fly predicted speaker-dependent routing parameters. KL-divergence is used to further enforce diversity among experts and their generalization to unseen speakers. Experimental results on the UASpeech corpus suggest that on-the-fly MoE-based adaptation produces statistically significant WER reductions of up to 1.34% absolute (6.36% relative) over the unadapted baseline HuBERT/WavLM models. Consistent WER reductions of up to 2.55% absolute (11.44% relative) and RTF speedups of up to 7 times are obtained over batch-mode adaptation across varying speaker-level data quantities. The lowest published WER of 16.35% (46.77% on very low intelligibility) is obtained."
   ],
   "p1": 4608,
   "pn": 4612,
   "doi": "10.21437/Interspeech.2025-721",
   "url": "interspeech_2025/hu25d_interspeech.html"
  },
  "lee25f_interspeech": {
   "authors": [
    [
     "Kyowoon",
     "Lee"
    ],
    [
     "Artyom",
     "Stitsyuk"
    ],
    [
     "Gunu",
     "Jho"
    ],
    [
     "Inchul",
     "Hwang"
    ],
    [
     "Jaesik",
     "Choi"
    ]
   ],
   "title": "Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models",
   "original": "723",
   "order": 93,
   "page_count": 5,
   "abstract": [
    "Recent advances in Text-to-Speech (TTS) have significantly improved speech naturalness, increasing the demand for precise prosody control and mispronunciation correction. Existing approaches for prosody manipulation often depend on specialized modules or additional training, limiting their capacity for post-hoc adjustments. Similarly, traditional mispronunciation correction relies on grapheme-to-phoneme dictionaries, making it less practical in low-resource settings. We introduce Counterfactual Activation Editing, a model-agnostic method that manipulates internal representations in a pre-trained TTS model to achieve post-hoc control of prosody and pronunciation. Experimental results show that our method effectively adjusts prosodic features and corrects mispronunciations while preserving synthesis quality. This opens the door to inference-time refinement of TTS outputs without retraining, bridging the gap between pre-trained TTS models and editable speech synthesis."
   ],
   "p1": 434,
   "pn": 438,
   "doi": "10.21437/Interspeech.2025-723",
   "url": "interspeech_2025/lee25f_interspeech.html"
  },
  "mun25b_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Mun"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "A Cascaded Multimodal Framework for Automatic Social Communication Severity Assessment in Children with Autism Spectrum Disorder",
   "original": "726",
   "order": 622,
   "page_count": 5,
   "abstract": [
    "Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by deficits in social communication, affecting both language use and speech patterns. Since assessment relies on behavioral observations rather than standardized medical tests, developing an objective evaluation method is essential. Recognizing that ASD impacts both language and speech production, this study proposes a cascaded multimodal framework for ASD severity assessment. The framework processes raw audio, generates transcriptions via automatic speech recognition, and extracts linguistic and acoustic features using speech-language foundation models. Given the atypical suprasegmental and segmental speech characteristics in ASD, two speech foundation models are employed. A co-attention mechanism then integrates these representations to estimate severity. Achieving a Spearman’s correlation of 0.5629 with human ratings, the proposed approach offers a scalable, fully automated ASD assessment tool."
   ],
   "p1": 3055,
   "pn": 3059,
   "doi": "10.21437/Interspeech.2025-726",
   "url": "interspeech_2025/mun25b_interspeech.html"
  },
  "zhang25f_interspeech": {
   "authors": [
    [
     "Xiaoming",
     "Zhang"
    ],
    [
     "Ke-Yue",
     "Zhang"
    ],
    [
     "Taiping",
     "Yao"
    ],
    [
     "Songjun",
     "Cao"
    ],
    [
     "Shouhong",
     "Ding"
    ],
    [
     "Long",
     "Ma"
    ]
   ],
   "title": "SonarGuard2: Ultrasonic Face Liveness Detection Based on Adaptive Doppler Effect Feature Extraction",
   "original": "728",
   "order": 515,
   "page_count": 5,
   "abstract": [
    "Face anti-spoofing is crucial to security of face recognition systems. Vision-based face liveness detection algorithms often fail when faced with video attacks, such as video replay. However, liveness detection based on sound waves can effectively detect such attacks by relying on the Doppler effect. In order to improve the robustness of liveness detection, we propose a novel framework, named SonarGuard2, adaptively selecting the ultrasonic signals and analyse the Doppler effect. Specifically, we introduce acoustic echo cancellation to filter to get the Doppler effect features, then utilize complex convolutional neural networks to enhance the abilitity to model the Doppler effect features. Furthemore, we propose a novel selection strategy to determine the usability of ultrasonic signals on mobile devices. The performance and visualizaiton on the collected data demonstrate the effectiveness of our framework."
   ],
   "p1": 2520,
   "pn": 2524,
   "doi": "10.21437/Interspeech.2025-728",
   "url": "interspeech_2025/zhang25f_interspeech.html"
  },
  "wang25f_interspeech": {
   "authors": [
    [
     "Junyu",
     "Wang"
    ],
    [
     "Tianrui",
     "Wang"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning",
   "original": "730",
   "order": 1182,
   "page_count": 5,
   "abstract": [
    "In recent advancements in audio self-supervised representation learning, the standard Transformer architecture has emerged as the predominant approach, yet its attention mechanism often allocates a portion of attention weights to irrelevant information, potentially impairing the model’s discriminative ability. To address this, we introduce a differential attention mechanism, which effectively mitigates ineffective attention allocation through the integration of dual-softmax operations and appropriately tuned differential coefficients. Experimental results demonstrate that our ASDA model achieves state-of-the-art (SOTA) performance across multiple benchmarks, including audio classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting (98.3% accuracy on SPC-2), and environmental sound classification (96.1% accuracy on ESC-50). These results highlight ASDA&#x27;s effectiveness in audio tasks, paving the way for broader applications."
   ],
   "p1": 5803,
   "pn": 5807,
   "doi": "10.21437/Interspeech.2025-730",
   "url": "interspeech_2025/wang25f_interspeech.html"
  },
  "rossenbach25_interspeech": {
   "authors": [
    [
     "Nick",
     "Rossenbach"
    ],
    [
     "Benedikt",
     "Hilmes"
    ],
    [
     "Leon",
     "Brackmann"
    ],
    [
     "Moritz",
     "Gunz"
    ],
    [
     "Ralf",
     "Schlüter"
    ]
   ],
   "title": "Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach",
   "original": "731",
   "order": 523,
   "page_count": 5,
   "abstract": [
    "Memristor-based hardware offers new possibilities for energy-efficient machine learning (ML) by providing analog in-memory matrix multiplication. Current hardware prototypes cannot fit large neural networks, and related literature covers only small ML models for tasks like MNIST or single word recognition. Simulation can be used to explore how hardware properties affect larger models, but existing software assumes simplified hardware. We propose a PyTorch-based library based on &quot;Synaptogen&quot; to simulate neural network execution with accurately captured memristor hardware properties. For the first time, we show how an ML system with millions of parameters would behave on memristor hardware, using a Conformer trained on the speech recognition task TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the relative degradation in word error rate to 25% when using a 3-bit weight precision to execute linear operations via simulated analog computation."
   ],
   "p1": 2560,
   "pn": 2564,
   "doi": "10.21437/Interspeech.2025-731",
   "url": "interspeech_2025/rossenbach25_interspeech.html"
  },
  "rong25_interspeech": {
   "authors": [
    [
     "Xiaobin",
     "Rong"
    ],
    [
     "Dahan",
     "Wang"
    ],
    [
     "Qinwen",
     "Hu"
    ],
    [
     "Yushi",
     "Wang"
    ],
    [
     "Yuxiang",
     "Hu"
    ],
    [
     "Jing",
     "Lu"
    ]
   ],
   "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network",
   "original": "734",
   "order": 179,
   "page_count": 5,
   "abstract": [
    "Universal speech enhancement aims to handle input speech with different distortions and input formats. To tackle this challenge, we present TS-URGENet, a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network. To address various distortions, the proposed system employs a novel three-stage architecture consisting of a filling stage, a separation stage, and a restoration stage. The filling stage mitigates packet loss by preliminarily filling lost regions under noise interference, ensuring signal continuity. The separation stage suppresses noise, reverberation, and clipping distortion to improve speech clarity. Finally, the restoration stage compensates for bandwidth limitation, codec artifacts, and residual packet loss distortion, refining the overall speech quality. Our proposed TS-URGENet achieves outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd in Track 1."
   ],
   "p1": 863,
   "pn": 867,
   "doi": "10.21437/Interspeech.2025-734",
   "url": "interspeech_2025/rong25_interspeech.html"
  },
  "gan25_interspeech": {
   "authors": [
    [
     "Chong-Xin",
     "Gan"
    ],
    [
     "Zhe",
     "Li"
    ],
    [
     "Zezhong",
     "Jin"
    ],
    [
     "Zilong",
     "Huang"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Kong Aik",
     "Lee"
    ]
   ],
   "title": "IDIR: Identifying and Distilling Informative Relations for Speaker Verification",
   "original": "736",
   "order": 1173,
   "page_count": 5,
   "abstract": [
    "Traditional feature-based knowledge distillation aligns the student’s features with the teacher’s features. However, these one-to-one alignments overlook the structural relations between speakers in a mini-batch. Also, the large capacity gap between the two networks causes significant discrepancies between their features. To address these limitations, we propose distilling the inter- and intra-speaker relations. Instead of mimicking all pairwise relations between the student&#x27;s and teacher&#x27;s feature vectors, we propose Identifying and Distilling Informative Relations (IDIR), enabling the student network to acquire speakers&#x27; relationships from the teacher. Moreover, a margin is added to the similarity scores of the informative pairs, further reducing intra-speaker variances and increasing inter-speaker separations. Evaluations with a simple x-vector student network demonstrate the method&#x27;s superb performance across three test sets, showcasing its merits and effectiveness."
   ],
   "p1": 5758,
   "pn": 5762,
   "doi": "10.21437/Interspeech.2025-736",
   "url": "interspeech_2025/gan25_interspeech.html"
  },
  "liu25e_interspeech": {
   "authors": [
    [
     "Fan",
     "Liu"
    ],
    [
     "Cheng",
     "Gong"
    ],
    [
     "Boyu",
     "Zhu"
    ],
    [
     "Ruihao",
     "Jing"
    ],
    [
     "Chunyu",
     "Qiang"
    ],
    [
     "Tianrui",
     "Wang"
    ],
    [
     "Xiao-Lei",
     "Zhang"
    ],
    [
     "Xuelong",
     "Li"
    ]
   ],
   "title": "Augment Mandarin to Cantonese Speech Databases via Retrieval-Augmented Generation and Speech Synthesis",
   "original": "737",
   "order": 866,
   "page_count": 5,
   "abstract": [
    "Using large-scale training data has significantly driven recent advances in speech recognition models. However, the lack of corpus for some low-resource languages (e.g., Cantonese) is still a bottleneck for speech processing. With the continuous development of large language models (LLMs) and speech synthesis technologies, it has become possible to expand Cantonese corpora using automatically generated text and speech. In this paper, we propose using LLMs and text-to-speech (TTS) techniques to augment Mandarin data to the Mandarin-Cantonese parallel speech database (MCPSD). We propose a novel retrieval-augmented generation (RAG) method that incorporates a Cantonese knowledge base to enhance both the diversity and the accuracy of text generation. We implement TTS models to generate speech and design a data filter to ensure quality. Experimental results show that the generated parallel database is effective in fine-tuning models in Cantonese speech recognition and translation."
   ],
   "p1": 4248,
   "pn": 4252,
   "doi": "10.21437/Interspeech.2025-737",
   "url": "interspeech_2025/liu25e_interspeech.html"
  },
  "hovsepyan25_interspeech": {
   "authors": [
    [
     "Sevada",
     "Hovsepyan"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Speech power spectra: a window into neural oscillations in Parkinson's disease",
   "original": "738",
   "order": 1077,
   "page_count": 5,
   "abstract": [
    "This study analyzes the power spectral density (PSD) of speech in healthy controls (HC) and Parkinson&#x27;s disease (PD) patients, focusing on the 0-100 Hz range. These low frequency components are below the fundamental frequency and may reflect both motor and neural mechanisms in speech production. We hypothesize that neural oscillations (NOs) involved in speech perception and production - theta (4-8 Hz), beta (15-35 Hz), and gamma (36-80 Hz) - shape the low-frequency PSD. Since NOs are linked to motor control and cognition, and are altered in PD, we expect systematic differences between HC and PD speech. Using multitaper estimation, we found significant differences in beta power, in line with research on beta oscillations and motor dysfunction in PD. Beyond distinguishing HC from PD speech, our results suggest that sub-fundamental frequency information may reflect neural dynamics in speech production, offering new perspectives for speech pathology and neural oscillation research."
   ],
   "p1": 5278,
   "pn": 5282,
   "doi": "10.21437/Interspeech.2025-738",
   "url": "interspeech_2025/hovsepyan25_interspeech.html"
  },
  "peng25b_interspeech": {
   "authors": [
    [
     "Yizhou",
     "Peng"
    ],
    [
     "Yi-Wen",
     "Chao"
    ],
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Yukun",
     "Ma"
    ],
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems",
   "original": "739",
   "order": 37,
   "page_count": 5,
   "abstract": [
    "Full-duplex spoken dialogue systems (FDSDS) enable more natural human–machine interactions by allowing real-time user interruptions and backchanneling, compared to traditional SDS that rely on turn-taking. However, existing benchmarks lack metrics for FD scenes, e.g., evaluating model performance during user interruptions. In this paper, we present a comprehensive FD benchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It assesses FDSDS’s ability to handle user interruptions, manage delays, and maintain robustness in challenging scenarios with diverse novel metrics. We applied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and VITA-1.5) using over 40 hours of generated speech, with 293 simulated conversations and 1,200 interruptions. The results show that all models continue to face challenges, such as failing to respond to user interruptions, under frequent disruptions and noisy conditions. Demonstrations, data, and code will be released."
   ],
   "p1": 176,
   "pn": 180,
   "doi": "10.21437/Interspeech.2025-739",
   "url": "interspeech_2025/peng25b_interspeech.html"
  },
  "kakouros25_interspeech": {
   "authors": [
    [
     "Sofoklis",
     "Kakouros"
    ]
   ],
   "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition",
   "original": "740",
   "order": 76,
   "page_count": 5,
   "abstract": [
    "In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach."
   ],
   "p1": 349,
   "pn": 353,
   "doi": "10.21437/Interspeech.2025-740",
   "url": "interspeech_2025/kakouros25_interspeech.html"
  },
  "liu25f_interspeech": {
   "authors": [
    [
     "Xiaokang",
     "Liu"
    ],
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Yudong",
     "Yang"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Nan",
     "Yan"
    ]
   ],
   "title": "Addressing Task Conflicts in Stuttering Detection via MMoE-Based Multi-Task Learning",
   "original": "743",
   "order": 166,
   "page_count": 5,
   "abstract": [
    "Multi-Task Learning (MTL) is widely used in automatic stuttering detection to identify stuttering symptoms; however, task conflicts can hinder performance. This paper addresses the task conflict issue in MTL-based stuttering detection and proposes a rule-based MTL strategy and a Multi-Mixture-of-Experts (MMoE) MTL framework to alleviate these conflicts. We analyze the inherent conflicts in stuttering detection tasks and develop a rule-based MTL strategy to mitigate them. Additionally, we introduce an MoE-based adaptive multi-task strategy to optimize task allocation. Experimental results show that our approach outperforms current state-of-the-art methods. In the 2024 SLT Stuttering Speech Challenge, the rule-based MTL strategy achieved a 19.9% increase in average F1 score over the baseline, securing first place. The MMoE-MTL strategy further enhanced task collaboration, improving the average F1 score by 7.55%, demonstrating its effectiveness."
   ],
   "p1": 798,
   "pn": 802,
   "doi": "10.21437/Interspeech.2025-743",
   "url": "interspeech_2025/liu25f_interspeech.html"
  },
  "guzik25_interspeech": {
   "authors": [
    [
     "Mateusz",
     "Guzik"
    ],
    [
     "Giulio",
     "Cengarle"
    ],
    [
     "Daniel",
     "Arteaga"
    ]
   ],
   "title": "Deep learning based spatial aliasing reduction in beamforming for audio capture",
   "original": "746",
   "order": 514,
   "page_count": 5,
   "abstract": [
    "Spatial aliasing affects spaced microphone arrays, causing directional ambiguity above certain frequencies, degrading spatial and spectral accuracy of beamformers. Given the limitations of conventional signal processing and the scarcity of deep learning approaches to spatial aliasing mitigation, we propose a novel approach using a U-Net architecture to predict a signal-dependent de-aliasing filter, which reduces aliasing in conventional beamforming for spatial capture. Two types of multichannel filters are considered, one which treats the channels independently and a second one that models cross-channel dependencies. The proposed approach is evaluated in two common spatial capture scenarios: stereo and first-order Ambisonics. The results indicate a very significant improvement, both objective and perceptual, with respect to conventional beamforming. This work shows the potential of deep learning to reduce aliasing in beamforming, leading to improvements in multi-microphone setups."
   ],
   "p1": 2515,
   "pn": 2519,
   "doi": "10.21437/Interspeech.2025-746",
   "url": "interspeech_2025/guzik25_interspeech.html"
  },
  "rittergutierrez25_interspeech": {
   "authors": [
    [
     "Fabian",
     "Ritter-Gutierrez"
    ],
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Jui-Chiang",
     "Wei"
    ],
    [
     "Jeremy H.M",
     "Wong"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Distilling a speech and music encoder with task arithmetic",
   "original": "747",
   "order": 788,
   "page_count": 5,
   "abstract": [
    "Despite the progress in self-supervised learning (SSL) for speech and music, existing models treat these domains separately, limiting their capacity for unified audio understanding. A unified model is desirable for applications that require general representations, e.g. audio large language models. Nonetheless, directly training a general model for speech and music is computationally expensive. Knowledge Distillation of teacher ensembles may be a natural solution, but we posit that decoupling the distillation of the speech and music SSL models allows for more flexibility. Thus, we propose to learn distilled task vectors and then linearly interpolate them to form a unified speech+music model. This strategy enables flexible domain emphasis through adjustable weights and is also simpler to train. Experiments on speech and music benchmarks demonstrate that our method yields superior overall performance compared to ensemble distillation."
   ],
   "p1": 3858,
   "pn": 3862,
   "doi": "10.21437/Interspeech.2025-747",
   "url": "interspeech_2025/rittergutierrez25_interspeech.html"
  },
  "le25b_interspeech": {
   "authors": [
    [
     "Xiaohuai",
     "Le"
    ],
    [
     "Zhuangqi",
     "Chen"
    ],
    [
     "Siyu",
     "Sun"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Chuanzeng",
     "Huang"
    ]
   ],
   "title": "Multistage Universal Speech Enhancement System for URGENT Challenge",
   "original": "749",
   "order": 180,
   "page_count": 5,
   "abstract": [
    "During audio transmission and processing, various distortions may occur. To effectively address this challenge, we developed a multistage universal speech enhancement system, consisting of four submodules, namely audio declipping, packet loss compensation, audio separation, and spectral inpainting. These modules operate across the time, sub-band, and time-frequency domains. We employed a pretrain-finetune training paradigm and introduce a self-distillation method to further improve performance. Experiments on large-scale datasets demonstrate that our system outperforms in multiple evaluation metrics, particularly in improving subjective speech quality. The proposed system ranked 1st in the URGENT 2024 challenge with a MOS of 3.52 and placed 4th in the second track of the URGENT 2025 challenge."
   ],
   "p1": 868,
   "pn": 872,
   "doi": "10.21437/Interspeech.2025-749",
   "url": "interspeech_2025/le25b_interspeech.html"
  },
  "tao25b_interspeech": {
   "authors": [
    [
     "Liang",
     "Tao"
    ],
    [
     "Maoshen",
     "Jia"
    ],
    [
     "Yonggang",
     "Hu"
    ]
   ],
   "title": "Power Spectral Density Estimation for Acoustic Source Separation Using A Spherical Microphone Array",
   "original": "753",
   "order": 301,
   "page_count": 5,
   "abstract": [
    "Acoustic source separation using microphone arrays has garnered increasing attention within the audio signal processing community. Cascading a beamformer and a post-filter for source separation is widely recognized as an effective solution, where accurate estimation of the power spectral density (PSD) is crucial for post-filter. However, estimating the PSD components is a challenging task in the presence of noise. In this paper, we propose an efficient multi-source PSD estimation method using a spherical microphone array (SMA) in noisy environments for source separation. Specifically, the orthogonality of spherical harmonics (SH) inherent in an error-free sampled SMA enables effective suppression of noise components in the spatial covariance matrix (SCM) of the observation signals, facilitating accurate PSD estimation for each individual source, thus the post-filter would be effectively constructed. The source separation problem is then addressed by applying a SH-domain beamformer along with the developed post-filter. Both simulation and experimental results demonstrate the proposed algorithm’s effectiveness over the baseline methods."
   ],
   "p1": 1473,
   "pn": 1477,
   "doi": "10.21437/Interspeech.2025-753",
   "url": "interspeech_2025/tao25b_interspeech.html"
  },
  "li25i_interspeech": {
   "authors": [
    [
     "Haoxun",
     "Li"
    ],
    [
     "Leyuan",
     "Qu"
    ],
    [
     "Jiaxi",
     "Hu"
    ],
    [
     "Taihao",
     "Li"
    ]
   ],
   "title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis",
   "original": "754",
   "order": 890,
   "page_count": 5,
   "abstract": [
    "In recent years, emotional Text-to-Speech (TTS) synthesis and emphasis-controllable speech synthesis have advanced significantly. However, their interaction remains underexplored. We propose Emphasis Meets Emotion TTS (EME-TTS), a novel framework designed to address two key research questions: (1) how to effectively utilize emphasis to enhance the expressiveness of emotional speech, and (2) how to maintain the perceptual clarity and stability of target emphasis across different emotions. EME-TTS employs weakly supervised learning with emphasis pseudo-labels and variance-based emphasis features. Additionally, the proposed Emphasis Perception Enhancement (EPE) block enhances the interaction between emotional signals and emphasis positions. Experimental results show that EME-TTS, when combined with large language models for emphasis position prediction, enables more natural emotional speech synthesis while preserving stable and distinguishable target emphasis across emotions. Synthesized samples are available online."
   ],
   "p1": 4368,
   "pn": 4372,
   "doi": "10.21437/Interspeech.2025-754",
   "url": "interspeech_2025/li25i_interspeech.html"
  },
  "chao25_interspeech": {
   "authors": [
    [
     "Yi-Wen",
     "Chao"
    ],
    [
     "Yizhou",
     "Peng"
    ],
    [
     "Dianwen",
     "Ng"
    ],
    [
     "Yukun",
     "Ma"
    ],
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "A-SMiLE: Affective Sparse Mixture-of-Experts Adapter with Multi-Task Learning for Spoken Dialogue Models",
   "original": "756",
   "order": 1113,
   "page_count": 5,
   "abstract": [
    "Large language models have made remarkable progress in generating coherent and contextually relevant dialogue. However, they still struggle to capture fine-grained paralinguistic cues, limiting emotional coherence and contextual appropriateness. To address this challenge, we propose an Affective Sparse Mixture-of-experts adapter with multi-task Learning (A-SMiLE) to enhance the spoken dialogue model&#x27;s capability for affective perception across multiple dimensions. Our approach integrates Valence, Arousal, and Dominance (VAD) modeling with response generation, thereby ensuring expressive and contextually aware interactions. Evaluations on DailyTalk and a hard-case benchmark demonstrate significant improvements over baselines in both emotion prediction and response generation. It shows the potential of cognitive-inspired affective modeling to enhance intelligent speech interactions on the spoken dialogue models."
   ],
   "p1": 5458,
   "pn": 5462,
   "doi": "10.21437/Interspeech.2025-756",
   "url": "interspeech_2025/chao25_interspeech.html"
  },
  "wepner25_interspeech": {
   "authors": [
    [
     "Saskia",
     "Wepner"
    ],
    [
     "Lucas",
     "Eckert"
    ],
    [
     "Gernot",
     "Kubin"
    ],
    [
     "Barbara",
     "Schuppler"
    ]
   ],
   "title": "What the Filler? Both ASR Systems and Humans Struggle More With Other Kinds of Disfluencies Than With Filler Particles",
   "original": "757",
   "order": 476,
   "page_count": 5,
   "abstract": [
    "Since disfluencies are frequent in conversational speech, they have received notable attention: from speech technologists to make automatic speech recognition (ASR) more robust and from speech scientists to learn more about human speech processing. For ASR, the most established quality measure is the word error rate (WER), while for human recognition, one of the measures is the recall of words or utterance-level semantics. We conduct a transcription experiment in which we present the same disfluent utterances to 54 participants and nine ASR systems. We analyse which factors affect transcription in the context of syntactic disfluencies and filler particles, including well-known factors such as pronunciation variation and articulation rate. We find that, surprisingly, both humans and ASR struggle with largely the same characteristics of conversational speech – despite their mean WERs differing by about 10% – and that the presence or absence of filler particles does not affect the WER."
   ],
   "p1": 2325,
   "pn": 2329,
   "doi": "10.21437/Interspeech.2025-757",
   "url": "interspeech_2025/wepner25_interspeech.html"
  },
  "menezes25_interspeech": {
   "authors": [
    [
     "João",
     "Menezes"
    ],
    [
     "Aubin",
     "Mouras"
    ],
    [
     "Arne-Lukas",
     "Fietkau"
    ],
    [
     "Dani",
     "Kazzy"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Multimodal Silent Recognition of Phonemes Using Radar and Optopalatographic Silent Speech Interfaces",
   "original": "759",
   "order": 1032,
   "page_count": 5,
   "abstract": [
    "Silent Speech Interfaces (SSIs) have a broad range of applications and rely on sensing techniques to measure speech-related bio-signals. To take advantage of the strengths and minimize weaknesses of different sensing techniques, multimodal recordings and pipelines have been investigated lately on SSIs. This study presents a multimodal articulatory recording combining Radar and Optopalatographic (OPG) SSIs and evaluates the performance of each data type and their combination in a phoneme recognition task. A corpus of 26 phonemes was recorded by 3 native German speakers and Support Vector Machines were used to classify the recorded multimodal data into the phonemes. Considering all speakers together, accuracies of 85.40%, 66.99% and 86.55% were obtained with Radar, OPG and the concatenation of Radar and OPG, respectively. The accuracy increase of 1.15% resulting of adding OPG to Radar data is small but statistically significant and points to the potential of such multimodal recordings."
   ],
   "p1": 5058,
   "pn": 5062,
   "doi": "10.21437/Interspeech.2025-759",
   "url": "interspeech_2025/menezes25_interspeech.html"
  },
  "hilmes25_interspeech": {
   "authors": [
    [
     "Benedikt",
     "Hilmes"
    ],
    [
     "Nick",
     "Rossenbach"
    ],
    [
     "Ralf",
     "Schlüter"
    ]
   ],
   "title": "Analyzing the Importance of Blank for CTC-Based Knowledge Distillation",
   "original": "760",
   "order": 407,
   "page_count": 5,
   "abstract": [
    "With the rise of large pre-trained foundation models for automatic speech recognition new challenges appear. While the performance of these models is good, runtime and cost of inference increases. One approach to make use of their strength while retaining efficiency is to distill their knowledge to smaller models during training. In this work, we explore different CTC-based distillation variants, focusing on blank token handling. We show that common approaches like blank elimination do not always work off the shelf. We explore new blank selection patterns as a potential sweet spot between standard knowledge distillation and blank elimination mechanisms. Through the introduction of a symmetric selection method, we are able to remove the CTC loss during knowledge distillation with minimal to no performance degradation. With this, we make the training independent from target labels, potentially allowing for distillation on untranscribed audio data."
   ],
   "p1": 1998,
   "pn": 2002,
   "doi": "10.21437/Interspeech.2025-760",
   "url": "interspeech_2025/hilmes25_interspeech.html"
  },
  "hwang25_interspeech": {
   "authors": [
    [
     "Injune",
     "Hwang"
    ],
    [
     "Jung-Min",
     "Kim"
    ],
    [
     "Ju Seok",
     "Ryu"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Voice-Based Dysphagia Detection: Leveraging Self-Supervised Speech Representation",
   "original": "761",
   "order": 1158,
   "page_count": 5,
   "abstract": [
    "This study introduces a framework for diagnosing dysphagia using self-supervised speech representation learning (SSL) models. Previously reported methods typically rely on mel spectrograms; however, due to the limited amount of medical data, they struggle to accurately diagnose dysphagia from low-dimensional features. However, SSL models, trained on large-scale speech data, are well suited for tasks with smaller dataset. Employing SSL features significantly enhances model performance, allowing for the model’s size reduction while outperforming larger models based on mel spectrograms. Although a decrease in specificity was observed, recall, a crucial metric for disease diagnosis, showed a marked improvement, leading to a general improvement in diagnostic accuracy. Among the SSL models evaluated, the features of the 10th layer of WavLM had the highest performance. Additionally, increasing the size of the filter in the convolutional layers does not contribute to performance gains."
   ],
   "p1": 5683,
   "pn": 5687,
   "doi": "10.21437/Interspeech.2025-761",
   "url": "interspeech_2025/hwang25_interspeech.html"
  },
  "tannander25_interspeech": {
   "authors": [
    [
     "Christina",
     "Tånnander"
    ],
    [
     "David",
     "House"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "Intrasentential English in Swedish TTS: perceived English-accentedness",
   "original": "762",
   "order": 334,
   "page_count": 5,
   "abstract": [
    "English names and expressions are frequently inserted into Swedish text. Humans intuitively adjust the degree of English pronunciation of such insertions. This work aims at a Swedish text-to-speech synthesis (TTS) capable of similar controlled adaptation. We focus on two key aspects: (1) the development of a TTS system with controllable degrees of perceived English-accentedness (PEA); and (2) the exploration of human preferences related to PEA. We trained a Swedish TTS voice on Swedish and English sentences with a conditioning parameter for language (English-accentedness, EA) on a scale from 0 to 1, and estimated a psychometric mapping of the perceived effect of EA to a perceptual scale (PEA) through perception tests. PEA was then used in Best-Worst listening tests presenting English insertions with varying PEA. The results confirm the effectiveness of the training and the PEA scale, and that listener preferences change with different insertions."
   ],
   "p1": 1638,
   "pn": 1642,
   "doi": "10.21437/Interspeech.2025-762",
   "url": "interspeech_2025/tannander25_interspeech.html"
  },
  "fong25_interspeech": {
   "authors": [
    [
     "Seraphina",
     "Fong"
    ],
    [
     "Marco",
     "Matassoni"
    ],
    [
     "Alessio",
     "Brutti"
    ]
   ],
   "title": "Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages",
   "original": "764",
   "order": 408,
   "page_count": 5,
   "abstract": [
    "Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality."
   ],
   "p1": 2003,
   "pn": 2007,
   "doi": "10.21437/Interspeech.2025-764",
   "url": "interspeech_2025/fong25_interspeech.html"
  },
  "yang25f_interspeech": {
   "authors": [
    [
     "Chenyu",
     "Yang"
    ],
    [
     "Hangting",
     "Chen"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Haina",
     "Zhu"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "TVC-MusicGen: Time-Varying Structure Control for Background Music Generation via Self-Supervised Training",
   "original": "766",
   "order": 254,
   "page_count": 5,
   "abstract": [
    "Current text-to-music generation models typically are not involved in generating music with specific structures, thus not meeting some customized needs in practical applications. To address this limitation, we propose a self-supervised Time-Varying Control method (TVC-MusicGen). By providing the temporal boundary and text descriptions for each segment, it can effectively generate music adhering to the corresponding structures. TVC-MusicGen supports generation from both text (text-to-music) and existing music clips (music-to-music), enabling structure editing or local style transfer. Additionally, we propose a generation-based approach to bridge the gap between text and audio modalities in cross-modal models, which are typically used as feature extractors in text-to-music systems. Experiments on both language and diffusion-based models demonstrate that our approach achieves effective control without compromising overall quality."
   ],
   "p1": 1238,
   "pn": 1242,
   "doi": "10.21437/Interspeech.2025-766",
   "url": "interspeech_2025/yang25f_interspeech.html"
  },
  "li25j_interspeech": {
   "authors": [
    [
     "Peiran",
     "Li"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Xixin",
     "Wu"
    ]
   ],
   "title": "EEG-based Speech Decoding Based on Multi-mode Joint Modeling",
   "original": "769",
   "order": 1141,
   "page_count": 5,
   "abstract": [
    "Electroencephalography (EEG)-based speech decoding enables the development of non-invasive speech brain-computer interfaces (BCIs) for restoring communication of individuals with speech impairments. Previous work achieves much better performance in decoding spoken and intended speech from EEG signals, with imagined speech decoding lagging far behind. This paper proposes a novel framework to train a unified multi-mode decoding model for EEG signals of imagined, intended and spoken speech modes using a dynamic masking mechanism. Our multi-mode model achieves significantly better four-vowel decoding accuracies than baselines (34.95% vs. 29.18% for imagined speech). Training a single-mode model with a subset of EEG channels selected according to a multi-mode model as inputs provides superior performance than training a single-mode model from all channels. The accuracy improvements and channel selection capability demonstrate the effectiveness of the proposed joint modeling framework."
   ],
   "p1": 5598,
   "pn": 5602,
   "doi": "10.21437/Interspeech.2025-769",
   "url": "interspeech_2025/li25j_interspeech.html"
  },
  "paierl25_interspeech": {
   "authors": [
    [
     "Michael",
     "Paierl"
    ],
    [
     "Martin",
     "Hagmüller"
    ],
    [
     "Barbara",
     "Schuppler"
    ]
   ],
   "title": "Continuous prediction of backchannel timing for human-robot interaction",
   "original": "770",
   "order": 615,
   "page_count": 5,
   "abstract": [
    "This paper explores the continuous prediction of backchannel timing in conversational speech, with the aim to make turn-taking in human-robot interaction more natural. To assure real-time prediction, we present regression-based models based exclusively on acoustic features that can be extracted continuously from the user&#x27;s speech. Comparing different machine learning models, we found lightGBM models to perform best with respect to accuracy (mean absolute error: approx. 130 ms) and efficiency, while meeting the real-time requirement. Our analysis of feature importances revealed that speaking duration, intensity and fundamental frequency are among the most important predictors of backchannel timing, when extracted in the window from 275-875 ms before a backchannel in the interlocutor&#x27;s turn. Given the strong predictive performance of our models, this work provides a foundation for implementing more natural and responsive conversational agents."
   ],
   "p1": 3020,
   "pn": 3024,
   "doi": "10.21437/Interspeech.2025-770",
   "url": "interspeech_2025/paierl25_interspeech.html"
  },
  "taylor25_interspeech": {
   "authors": [
    [
     "James",
     "Taylor"
    ],
    [
     "Wolfgang",
     "Mack"
    ]
   ],
   "title": "Improving Audio Classification by Transitioning from Zero- to Few-Shot ",
   "original": "771",
   "order": 528,
   "page_count": 5,
   "abstract": [
    "State-of-the-art audio classification often employs a zero-shot approach, which involves comparing audio embeddings with embeddings from text describing the respective audio class. These embeddings are usually generated by neural networks trained through contrastive learning to align audio and text representations. Identifying the optimal text description for an audio class is challenging, particularly when the class comprises a wide variety of sounds. This paper examines few-shot methods designed to improve classification accuracy beyond the zero-shot approach. Specifically, audio embeddings are grouped by class and processed to replace the inherently noisy text embeddings. Our results demonstrate that few-shot classification typically outperforms the zero-shot baseline."
   ],
   "p1": 2585,
   "pn": 2589,
   "doi": "10.21437/Interspeech.2025-771",
   "url": "interspeech_2025/taylor25_interspeech.html"
  },
  "han25c_interspeech": {
   "authors": [
    [
     "Lu",
     "Han"
    ],
    [
     "Junqi",
     "Zhao"
    ],
    [
     "Renhua",
     "Peng"
    ]
   ],
   "title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation",
   "original": "773",
   "order": 244,
   "page_count": 5,
   "abstract": [
    "Current multi-channel speech enhancement systems mainly adopt single-output architecture, which face significant challenges in preserving spatio-temporal signal integrity during multiple-input multiple-output (MIMO) processing. To address this limitation, we propose a novel neural network, termed WTFormer, for MIMO speech enhancement that leverages the multi-resolution characteristics of wavelet transform and multi-dimensional collaborative attention to effectively capture globally distributed spatial features, while using Conformer for time-frequency modeling. A multi task loss strategy accompanying MUSIC algorithm is further proposed for optimization training to protect spatial information to the greatest extent. Experimental results on the LibriSpeech dataset show that WTFormer can achieve comparable denoising performance to advanced systems while preserving more spatial information with only 0.98M parameters."
   ],
   "p1": 1188,
   "pn": 1192,
   "doi": "10.21437/Interspeech.2025-773",
   "url": "interspeech_2025/han25c_interspeech.html"
  },
  "bagat25_interspeech": {
   "authors": [
    [
     "Raphaël",
     "Bagat"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Emmanuel",
     "Vincent"
    ]
   ],
   "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition",
   "original": "775",
   "order": 235,
   "page_count": 5,
   "abstract": [
    "We aim to improve the robustness of Automatic Speech Recognition (ASR) systems against non-native speech, particularly in low-resourced multi-accent settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA) experts, each specialized in a specific accent. This method can be used when the accent is known or unknown at inference time, without the need to fine-tune the model again. Our experiments, conducted using Whisper on the L2-ARCTIC corpus, demonstrate significant improvements in Word Error Rate compared to regular LoRA and full fine-tuning when the accent is unknown. When the accent is known, the results further improve. Furthermore, MAS-LoRA shows less catastrophic forgetting than the other fine-tuning methods. To the best of our knowledge, this is the first use of a mixture of LoRA experts for non-native multi-accent ASR."
   ],
   "p1": 1143,
   "pn": 1147,
   "doi": "10.21437/Interspeech.2025-775",
   "url": "interspeech_2025/bagat25_interspeech.html"
  },
  "wu25e_interspeech": {
   "authors": [
    [
     "Yihan",
     "Wu"
    ],
    [
     "Yichen",
     "Lu"
    ],
    [
     "Yijing",
     "Chen"
    ],
    [
     "Jiaqi",
     "Song"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Ruihua",
     "Song"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "GALAXY: A Large-Scale Open-Domain Dataset for Multimodal Learning",
   "original": "776",
   "order": 36,
   "page_count": 5,
   "abstract": [
    "Humans naturally use multimodal information, with vision, speech, and text working together to understand the world and solve problems. For artificial intelligence to achieve human-level capability, it must process multimodal information in a similar manner. However, there is a lack of large-scale open-domain datasets that support all three modalities—vision, speech, and text—with high-quality speech transcriptions. To address this gap, we introduce GALAXY, a large-scale, open-domain dataset designed for multimodal learning, containing 8,270 hours of videos, speech, and transcriptions across 16 diverse domains. We describe the data creation pipeline and provide detailed statistics and analyses of the dataset. Using multimodal speech recognition as a case study, we validate GALAXY’s effectiveness and evaluate baseline models’ performance across different data volumes and domains. The results highlight GALAXY’s potential as a valuable resource for advancing multimodal understanding."
   ],
   "p1": 171,
   "pn": 175,
   "doi": "10.21437/Interspeech.2025-776",
   "url": "interspeech_2025/wu25e_interspeech.html"
  },
  "park25c_interspeech": {
   "authors": [
    [
     "Seohyun",
     "Park"
    ],
    [
     "Chitralekha",
     "Gupta"
    ],
    [
     "Michelle",
     "Kah Yian Kwan"
    ],
    [
     "Xinhui",
     "Fung"
    ],
    [
     "Alexander Wenjun",
     "Yip"
    ],
    [
     "Suranga",
     "Nanayakkara"
    ]
   ],
   "title": "Towards Temporally Explainable Dysarthric Speech Clarity Assessment",
   "original": "777",
   "order": 435,
   "page_count": 5,
   "abstract": [
    "Dysarthria, a motor speech disorder, affects intelligibility and requires targeted interventions for effective communication. In this work, we investigate automated mispronunciation feedback by collecting a dysarthric speech dataset from six speakers reading two passages, annotated by a speech therapist with temporal markers and mispronunciation descriptions. We design a three-stage framework for explainable mispronunciation evaluation: (1) overall clarity scoring, (2) mispronunciation localization, and (3) mispronunciation type classification. We systematically analyze pretrained Automatic Speech Recognition (ASR) models in each stage, assessing their effectiveness in dysarthric speech evaluation. Our findings offer clinically relevant insights for automating actionable feedback for pronunciation assessment, which could enable independent practice for patients and help therapists deliver more effective interventions."
   ],
   "p1": 2138,
   "pn": 2142,
   "doi": "10.21437/Interspeech.2025-777",
   "url": "interspeech_2025/park25c_interspeech.html"
  },
  "roychowdhury25_interspeech": {
   "authors": [
    [
     "Sujoy",
     "Roychowdhury"
    ],
    [
     "Ranjani",
     "H.G."
    ],
    [
     "Sumit",
     "Soman"
    ],
    [
     "Nishtha",
     "Paul"
    ],
    [
     "Subhadip",
     "Bandyopadhyay"
    ],
    [
     "Siddhanth",
     "Iyengar"
    ]
   ],
   "title": "Intelligibility of Text-to-Speech Systems for Mathematical Expressions",
   "original": "779",
   "order": 467,
   "page_count": 5,
   "abstract": [
    "There has been limited evaluation of advanced Text-to-Speech (TTS) models with Mathematical eXpressions (MX) as inputs. In this work, we design experiments to evaluate quality and intelligibility of five TTS models through listening and transcribing tests for various categories of MX. We use two Large Language Models (LLMs) to generate English pronunciation from LaTeX MX as TTS models cannot process LaTeX directly. We use Mean Opinion Score from user ratings and quantify intelligibility through transcription correctness using three metrics. We also compare listener preference of TTS outputs with respect to human expert rendition of same MX. Results establish that output of TTS models for MX is not necessarily intelligible, the gap in intelligibility varies across TTS models and MX category. For most categories, performance of TTS models is significantly worse than that of expert rendition. The effect of choice of LLM is limited. This establishes the need to improve TTS models for MX."
   ],
   "p1": 2280,
   "pn": 2284,
   "doi": "10.21437/Interspeech.2025-779",
   "url": "interspeech_2025/roychowdhury25_interspeech.html"
  },
  "zuo25b_interspeech": {
   "authors": [
    [
     "Ruichen",
     "Zuo"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Zilong",
     "Huang"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "The Sub-3Sec Problem: From Text-Independent to Text-Dependent Corpus",
   "original": "782",
   "order": 817,
   "page_count": 5,
   "abstract": [
    "This paper introduces the sub-3sec problem in speaker verification, a short-duration task rarely explored. The issue arises from labor-intensive annotations and costly recordings for text-dependent speaker verification (TD-SV) corpora. To address this issue, we propose an automatic pipeline to extract short phrases from text-independent speaker verification (TI-SV) corpora. An ASR model identifies phrases and timestamps, with N-gram analysis ensuring phrases are common across speakers, enabling sufficient trials. Using this pipeline, we created Sub3Vox, a TD-SV corpus from VoxCeleb1, containing 3.7 million short utterances from 1,250 speakers—far larger than existing TD-SV corpora. Results show that matching enrollment and test phrases in TD-SV reduces EER by up to 45.23%. Additionally, shortening test utterances causes significant TI-SV performance drops but only minor reduction for TD-SV, offering the first analysis of phrase length effects on sub-3-second performance."
   ],
   "p1": 4003,
   "pn": 4007,
   "doi": "10.21437/Interspeech.2025-782",
   "url": "interspeech_2025/zuo25b_interspeech.html"
  },
  "kakouros25b_interspeech": {
   "authors": [
    [
     "Sofoklis",
     "Kakouros"
    ],
    [
     "Haoyu",
     "Chen"
    ]
   ],
   "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews",
   "original": "783",
   "order": 854,
   "page_count": 5,
   "abstract": [
    "This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues—such as pitch variability—remain strong indicators of victory."
   ],
   "p1": 4188,
   "pn": 4192,
   "doi": "10.21437/Interspeech.2025-783",
   "url": "interspeech_2025/kakouros25b_interspeech.html"
  },
  "lobato25_interspeech": {
   "authors": [
    [
     "Thiago Henrique Gomes",
     "Lobato"
    ],
    [
     "Magnus",
     "Schäfer"
    ]
   ],
   "title": "Gradual modeling of the Lombard effect by modifying speaker embeddings from a Text-To-Speech model",
   "original": "787",
   "order": 846,
   "page_count": 5,
   "abstract": [
    "This work proposes to modify clean speech into Lombard-like speech by modifying speaker embeddings used to condition a text-to-speech model. A feedforward network learns to map embeddings of plain speech to those of Lombard speech using paired data from the Audio-Visual Lombard Grid corpus. Signal level is then increased as per ITU-T P.1150, and a neural vocoder performs time stretching. We show that the resulting speech retains most of the speaker’s identity while incorporating relevant Lombard characteristics. Additionally, by properly interpolating embeddings, we propose an approach to gradually model Lombard speech as a function of the background noise level. Listening tests show about a 1.12-point Mean opinion score (MOS) increase in speech plausibility in a loud background context, with only a 0.5-point MOS decrease in speaker similarity compared to an ideal Lombard speech interpolation."
   ],
   "p1": 4148,
   "pn": 4152,
   "doi": "10.21437/Interspeech.2025-787",
   "url": "interspeech_2025/lobato25_interspeech.html"
  },
  "gimenogomez25_interspeech": {
   "authors": [
    [
     "David",
     "Gimeno-Gómez"
    ],
    [
     "Rubén",
     "Solera-Ureña"
    ],
    [
     "Anna",
     "Pompili"
    ],
    [
     "Carlos-D.",
     "Martínez-Hinarejos"
    ],
    [
     "Rita",
     "Cardoso"
    ],
    [
     "Isabel",
     "Guimarães"
    ],
    [
     "Joaquim J.",
     "Ferreira"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "On the Relevance of Clinical Assessment Tasks for the Automatic Detection of Parkinson’s Disease Medication State from Speech",
   "original": "793",
   "order": 1076,
   "page_count": 5,
   "abstract": [
    "The automatic identification of medication states of Parkinson&#x27;s disease (PD) patients can assist clinicians in monitoring and scheduling personalized treatments, as well as studying the effects of medication in alleviating the motor symptoms that characterize the disease. This paper explores speech as a non-invasive and accessible biomarker for identifying PD medication states, introducing a novel approach that addresses this task from a speaker-independent perspective. While traditional machine learning models achieve competitive results, self-supervised speech representations prove essential for optimal performance, significantly surpassing knowledge-based acoustic descriptors. Experiments across diverse speech assessment tasks highlight the relevance of prosody and continuous speech in distinguishing medication states, reaching an F1-score of 88.2%. These findings may streamline clinicians&#x27; work and reduce patient effort in voice recordings."
   ],
   "p1": 5273,
   "pn": 5277,
   "doi": "10.21437/Interspeech.2025-793",
   "url": "interspeech_2025/gimenogomez25_interspeech.html"
  },
  "nam25b_interspeech": {
   "authors": [
    [
     "Kihyun",
     "Nam"
    ],
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Gangin",
     "Park"
    ],
    [
     "Chaeyoung",
     "Jung"
    ],
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "SEED: Speaker Embedding Enhancement Diffusion Model",
   "original": "794",
   "order": 760,
   "page_count": 5,
   "abstract": [
    "A primary challenge when deploying speaker recognition systems in real-world applications is performance degradation caused by environmental mismatch. We propose a diffusion-based method that takes speaker embeddings extracted from a pre-trained speaker recognition model and generates refined embeddings. For training, our approach progressively adds Gaussian noise to both clean and noisy speaker embeddings extracted from clean and noisy speech, respectively, via forward process of a diffusion model, and then reconstructs them to clean embeddings in the reverse process. While inferencing, all embeddings are regenerated via diffusion process. Our method needs neither speaker label nor any modification to the existing speaker recognition pipeline. Experiments on evaluation sets simulating environment mismatch scenarios show that our method can improve recognition accuracy by up to 19.6% over baseline models while retaining performance on conventional scenarios. We publish our code here."
   ],
   "p1": 3718,
   "pn": 3722,
   "doi": "10.21437/Interspeech.2025-794",
   "url": "interspeech_2025/nam25b_interspeech.html"
  },
  "sun25d_interspeech": {
   "authors": [
    [
     "Zhihang",
     "Sun"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Tong",
     "Lei"
    ],
    [
     "Rilin",
     "Chen"
    ],
    [
     "Meng",
     "Yu"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Yi",
     "Zhou"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Scaling beyond Denoising: Submitted System and Findings in URGENT Challenge 2025",
   "original": "795",
   "order": 181,
   "page_count": 5,
   "abstract": [
    "Although deep neural networks have remarkably facilitated speech enhancement (SE), current studies focus on noise suppression and narrow benchmarks, with limited exploration of realistic scenarios. This paper presents a system that jointly addresses multiple degradations through an end-to-end framework. Specifically, a variant of dual-path architecture in the time-frequency domain is proposed, involving a fast band-processing strategy to enable parallel band split and merge operations, and a channel-mixing module based on Fourier Analysis Networks to facilitate target estimation. Besides, to enable training with limited GPU resources, we propose a simple yet effective progressive block extension strategy to support model scaling training with limited GPU resources. We participate in the URGENT Challenge and our submitted system ranked first on Track 1. The related findings and analysis of scaling effects provide diverse insights for building universal SE systems."
   ],
   "p1": 873,
   "pn": 877,
   "doi": "10.21437/Interspeech.2025-795",
   "url": "interspeech_2025/sun25d_interspeech.html"
  },
  "han25d_interspeech": {
   "authors": [
    [
     "Runduo",
     "Han"
    ],
    [
     "Yanxin",
     "Hu"
    ],
    [
     "Yihui",
     "Fu"
    ],
    [
     "Zihan",
     "Zhang"
    ],
    [
     "Yukai",
     "Jv"
    ],
    [
     "Li",
     "Chen"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-car Speech Separation with Distributed Heterogeneous Arrays",
   "original": "800",
   "order": 1016,
   "page_count": 5,
   "abstract": [
    "Separating overlapping speech from multiple speakers is crucial for effective human-vehicle interaction. This paper proposes CabinSep, a lightweight neural mask-based minimum variance distortionless response (MVDR) speech separation approach, to reduce speech recognition errors in back-end automatic speech recognition (ASR) models. Our contributions are threefold: First, we utilize channel information to extract spatial features, which improves the estimation of speech and noise masks. Second, we employ MVDR during inference, reducing speech distortion to make it more ASR-friendly. Third, we introduce a data augmentation method combining simulated and real-recorded impulse responses (IRs), improving speaker localization at zone boundaries and further reducing speech recognition errors. With a computational complexity of only 0.4 GMACs, CabinSep achieves a 17.5% relative reduction in speech recognition error rate in a real-recorded dataset compared to the state-of-the-art DualSep model."
   ],
   "p1": 4978,
   "pn": 4982,
   "doi": "10.21437/Interspeech.2025-800",
   "url": "interspeech_2025/han25d_interspeech.html"
  },
  "postma25_interspeech": {
   "authors": [
    [
     "Emmy",
     "Postma"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ]
   ],
   "title": "Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data",
   "original": "801",
   "order": 937,
   "page_count": 5,
   "abstract": [
    "Speech impairments are prevalent biomarkers for Parkinson&#x27;s Disease (PD), motivating the development of diagnostic techniques using speech data for clinical applications. Although deep acoustic features have shown promise for PD classification, their effectiveness often varies due to individual speaker differences, a factor that has not been thoroughly explored in the existing literature. This study investigates the effectiveness of three pre-trained audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification. Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK) and listen and repeat (LR) tasks, capturing critical acoustic features for PD detection. Only Wav2Vec2.0 shows significant gender bias, achieving more favorable results for male speakers, in DDK tasks. The misclassified cases reveal challenges with atypical speech patterns, highlighting the need for improved feature extraction and model robustness in PD detection."
   ],
   "p1": 4603,
   "pn": 4607,
   "doi": "10.21437/Interspeech.2025-801",
   "url": "interspeech_2025/postma25_interspeech.html"
  },
  "bao25_interspeech": {
   "authors": [
    [
     "Hongtao",
     "Bao"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Frequency-Domain Enhanced Extreme Bandwidth Extension Network with ICCRN for Superior Speech Quality",
   "original": "806",
   "order": 835,
   "page_count": 5,
   "abstract": [
    "Bandwidth extension is a critical task in speech processing that enhances narrowband audio quality by reconstructing essential high-frequency components, which are vital for natural and intelligible speech. Recent advancements like EBEN (Extreme Bandwidth Extension Network) achieve notable results with lightweight generators and real-time operations. However, down-sampling limitations degrade frequency-space information, causing speech distortion as up-sampling struggles to recover lost details. To address this, we propose replacing the generator with an ICCRN in the frequency domain. Leveraging cepstral-domain features and inplace convolutions, our approach mitigates information loss during down-sampling and preserves global spectral structure. Evaluations on the French LibriSpeech dataset show significant improvements, including clearer spectrograms, fewer generator parameters, and lower computational costs, advancing the state of the art in bandwidth extension."
   ],
   "p1": 4093,
   "pn": 4097,
   "doi": "10.21437/Interspeech.2025-806",
   "url": "interspeech_2025/bao25_interspeech.html"
  },
  "choi25_interspeech": {
   "authors": [
    [
     "Ho-Young",
     "Choi"
    ],
    [
     "Jae-Heung",
     "Cho"
    ],
    [
     "Pil Moo",
     "Byun"
    ],
    [
     "Won-Gook",
     "Choi"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Temp4Cap: Temporally-aligned Automated Audio Captioning",
   "original": "808",
   "order": 638,
   "page_count": 5,
   "abstract": [
    "Automated audio captioning (AAC) is a crucial task in machine perception within the audio domain. AAC struggles to interpret and incorporate temporal relationships of sound events in captions. However, existing studies often fail to capture the temporal relationship, leading to incorrect captions. Some recent studies leverage sound event detection models to extract temporal relationships but remain limited by their dependence on independent pre-trained models. In this study, we propose Temp4Cap, a novel AAC framework that directly trains temporal alignment via contrastive learning, using the “temporal caption” generated by a large language model. To capture temporal relationships, we apply a temporal negative sampling strategy, which includes event- and order-level shuffle and random substitution when generating negative samples during contrastive learning. Experimental results on Clotho and AudioCaps show that Temp4Cap significantly improves both captioning and temporal metrics."
   ],
   "p1": 3135,
   "pn": 3139,
   "doi": "10.21437/Interspeech.2025-808",
   "url": "interspeech_2025/choi25_interspeech.html"
  },
  "liu25g_interspeech": {
   "authors": [
    [
     "Zehua",
     "Liu"
    ],
    [
     "Xiaolou",
     "Li"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge",
   "original": "812",
   "order": 552,
   "page_count": 5,
   "abstract": [
    "This paper presents the second Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The challenge evaluates two test scenarios: reading in recording studios and Internet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC 2023, which involves CN-CVS for training and CNVSRC-Single/Multi for development and evaluation. However, CNVSRC 2024 introduced two key improvements: (1) a stronger baseline system, and (2) an additional dataset, CN-CVS2-P1, for open tracks to improve data volume and diversity. The new challenge has demonstrated several important innovations in data preprocessing, feature extraction, model design, and training strategies, further pushing the state-of-the-art in Chinese LVC-VSR. More details and resources are available at the official website."
   ],
   "p1": 2705,
   "pn": 2709,
   "doi": "10.21437/Interspeech.2025-812",
   "url": "interspeech_2025/liu25g_interspeech.html"
  },
  "akti25_interspeech": {
   "authors": [
    [
     "Şeymanur",
     "Akti"
    ],
    [
     "Tuan-Nam",
     "Nguyen"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion",
   "original": "815",
   "order": 278,
   "page_count": 5,
   "abstract": [
    "Expressive voice conversion aims to transfer both speaker identity and expressive attributes from a target speech to a given source speech. In this work, we improve over a self-supervised, non-autoregressive framework with a conditional variational autoencoder, focusing on reducing source timbre leakage and improving linguistic-acoustic disentanglement for better style transfer. To minimize style leakage, we use multilingual discrete speech units for content representation and reinforce embeddings with augmentation-based similarity loss and mix-style layer normalization. To enhance expressivity transfer, we incorporate local F0 information via cross-attention and extract style embeddings enriched with global pitch and energy features. Experiments show our model outperforms baselines in emotion and speaker similarity, demonstrating superior style adaptation and reduced source style leakage."
   ],
   "p1": 1358,
   "pn": 1362,
   "doi": "10.21437/Interspeech.2025-815",
   "url": "interspeech_2025/akti25_interspeech.html"
  },
  "liu25h_interspeech": {
   "authors": [
    [
     "Mingda",
     "Liu"
    ],
    [
     "Jiatong",
     "Shi"
    ]
   ],
   "title": "Bridging Speech and Singing: Multi-stage Speech-Prompted Singing Voice Conversion with Speaker Embedding Adaptation",
   "original": "816",
   "order": 256,
   "page_count": 5,
   "abstract": [
    "Singing voice conversion (SVC) technology holds immense potential to transform the entertainment industry and advance human-computer interaction. Unlike previous SVC studies that primarily focus on the conversion between different singing voices, this work specifically investigates the conversion of speech timbre into singing. In this paper, we introduce a novel Singing Speech Alignment Network (SSAN) designed to align speaker and singer embeddings derived from input data. To effectively train the SVC model, we also propose a cycle training strategy that ensures the performance of SVC with speech prompts. Experimental results demonstrate that the proposed method (SSANSVC) achieves superior performance in both the naturalness of the synthesized singing and its similarity to the target speech."
   ],
   "p1": 1248,
   "pn": 1252,
   "doi": "10.21437/Interspeech.2025-816",
   "url": "interspeech_2025/liu25h_interspeech.html"
  },
  "franzreb25_interspeech": {
   "authors": [
    [
     "Carlos",
     "Franzreb"
    ],
    [
     "Arnab",
     "Das"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Private kNN-VC: Interpretable Anonymization of Converted Speech",
   "original": "820",
   "order": 656,
   "page_count": 5,
   "abstract": [
    "Speaker anonymization seeks to conceal a speaker&#x27;s identity while preserving the utility of their speech. The achieved privacy is commonly evaluated with a speaker recognition model trained on anonymized speech. Although this represents a strong attack, it is unclear which aspects of speech are exploited to identify the speakers. Our research sets out to unveil these aspects. It starts with kNN-VC, a powerful voice conversion model that performs poorly as an anonymization system, presumably because of prosody leakage. To test this hypothesis, we extend kNN-VC with two interpretable components that anonymize the duration and variation of phones. These components increase privacy significantly, proving that the studied prosodic factors encode speaker identity and are exploited by the privacy attack. Additionally, we show that changes in the target selection algorithm considerably influence the outcome of the privacy attack."
   ],
   "p1": 3224,
   "pn": 3228,
   "doi": "10.21437/Interspeech.2025-820",
   "url": "interspeech_2025/franzreb25_interspeech.html"
  },
  "casanova25_interspeech": {
   "authors": [
    [
     "Edresson",
     "Casanova"
    ],
    [
     "Paarth",
     "Neekhara"
    ],
    [
     "Ryan",
     "Langman"
    ],
    [
     "Shehzeen",
     "Hussain"
    ],
    [
     "Subhankar",
     "Ghosh"
    ],
    [
     "Xuesong",
     "Yang"
    ],
    [
     "Ante",
     "Jukic"
    ],
    [
     "Jason",
     "Li"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference",
   "original": "827",
   "order": 1026,
   "page_count": 5,
   "abstract": [
    "Large Language Models (LLMs) have significantly advanced audio processing by leveraging audio codecs to discretize audio into tokens, enabling the application of language modeling techniques to speech data. However, existing audio codecs often operate at high frame rates, leading to slow training and inference, particularly for autoregressive models. To address this, there is growing interest in low frame-rate audio codecs, which reduce the number of autoregressive steps required to generate one second of audio. In this paper, we conduct ablation studies to examine the impact of frame rate, bitrate, and causality on codec reconstruction quality. Based on our findings, we introduce NanoCodec, a state-of-the-art audio codec that achieves high-quality compression at just 12.5 frames per second (FPS). NanoCodec outperforms related works across various bitrate ranges, establishing a new benchmark for low-latency and efficient Speech LLM training and inference."
   ],
   "p1": 5028,
   "pn": 5032,
   "doi": "10.21437/Interspeech.2025-827",
   "url": "interspeech_2025/casanova25_interspeech.html"
  },
  "parikh25_interspeech": {
   "authors": [
    [
     "Aditya Kamlesh",
     "Parikh"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge",
   "original": "829",
   "order": 1034,
   "page_count": 5,
   "abstract": [
    "Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research."
   ],
   "p1": 5068,
   "pn": 5072,
   "doi": "10.21437/Interspeech.2025-829",
   "url": "interspeech_2025/parikh25_interspeech.html"
  },
  "fang25b_interspeech": {
   "authors": [
    [
     "Shi-Xin",
     "Fang"
    ],
    [
     "Liang-Yeh",
     "Shen"
    ],
    [
     "Yi-Cheng",
     "Lin"
    ],
    [
     "Huang-Cheng",
     "Chou"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning",
   "original": "832",
   "order": 29,
   "page_count": 5,
   "abstract": [
    "This paper introduces Meta-PerSER, a novel meta-learning framework that personalizes Speech Emotion Recognition (SER) by adapting to each listener’s unique way of interpreting emotion. Conventional SER systems rely on aggregated annotations, often overlooking individual subtleties and leading to inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training, Derivative Annealing, and per-layer per-step learning rates, enabling rapid adaptation with only a few labeled examples. By integrating robust representations from pre-trained self-supervised models, our framework first captures general emotional cues and then fine-tunes itself to personal annotation styles. Experiments on the IEMOCAP corpus demonstrate that Meta-PerSER significantly outperforms baseline methods in both seen and unseen data scenarios, highlighting its promise for personalized emotion recognition."
   ],
   "p1": 136,
   "pn": 140,
   "doi": "10.21437/Interspeech.2025-832",
   "url": "interspeech_2025/fang25b_interspeech.html"
  },
  "chien25_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Po-Chun",
     "Huang"
    ]
   ],
   "title": "CAPR: Confidence-Aware Prompt Refinement in Large Language Models",
   "original": "834",
   "order": 664,
   "page_count": 5,
   "abstract": [
    "Mitigating hallucination in large language models (LLMs) is crucial to ensure trustworthy generation. A meaningful solution to tackle this issue involves eliciting a reliable confidence score in the generation. However, the previous methods only focused on using white-box models for short-form generation. The solutions to long-form sentences with black-box models remain very limited. This study proposes a reliable black-box confidence elicitation for long-form generation by leveraging the external knowledge, inspired by retrieval augmented generation. In particular, this paper proposes the confidence-aware prompt refinement (CAPR) to modify user prompts to ensure trustworthy responses. The prompt refiner is trained by maximizing the rewards based on the confidence and the accuracy of generated sentences. Experiments show that CAPR effectively elicits reliable confidence scores. The refined prompts enable LLMs to produce reliable responses with calibrated confidence."
   ],
   "p1": 3264,
   "pn": 3268,
   "doi": "10.21437/Interspeech.2025-834",
   "url": "interspeech_2025/chien25_interspeech.html"
  },
  "cohen25_interspeech": {
   "authors": [
    [
     "Kfir",
     "Cohen"
    ],
    [
     "Lior",
     "Wolf"
    ],
    [
     "Bracha",
     "Laufer-Goldshtein"
    ]
   ],
   "title": "Discovering Directions of Uncertainty in Speech Inpainting",
   "original": "835",
   "order": 858,
   "page_count": 5,
   "abstract": [
    "Speech inpainting aims to restore missing segments in audio signals. While both classical signal processing and deep learning approaches have been developed for this task, they typically generate a single output, despite the existence of multiple plausible reconstructions. In this paper, we adapt Neural Posterior Principal Component (NPPC) to recover the principal directions of variation in the posterior distribution of the original signal given its masked version. Our empirical results demonstrate that these directions capture diverse and meaningful variations in both speech content and style, while more precisely capturing the predictive error compared to a more costly Bayesian deep learning approach."
   ],
   "p1": 4208,
   "pn": 4212,
   "doi": "10.21437/Interspeech.2025-835",
   "url": "interspeech_2025/cohen25_interspeech.html"
  },
  "yang25g_interspeech": {
   "authors": [
    [
     "Chih-Kai",
     "Yang"
    ],
    [
     "Neo",
     "Ho"
    ],
    [
     "Yen-Ting",
     "Piao"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information",
   "original": "839",
   "order": 365,
   "page_count": 5,
   "abstract": [
    "Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness, but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs’ multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research."
   ],
   "p1": 1788,
   "pn": 1792,
   "doi": "10.21437/Interspeech.2025-839",
   "url": "interspeech_2025/yang25g_interspeech.html"
  },
  "lee25g_interspeech": {
   "authors": [
    [
     "Geonyoung",
     "Lee"
    ],
    [
     "Geonhee",
     "Han"
    ],
    [
     "Paul Hongsuck",
     "Seo"
    ]
   ],
   "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization",
   "original": "840",
   "order": 1017,
   "page_count": 5,
   "abstract": [
    "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound separation via natural language queries. While existing methods rely on task-specific training, we explore whether pretrained diffusion models, originally designed for audio generation, can inherently perform separation without further training. In this study, we introduce a training-free framework leveraging generative priors for zero-shot LASS. Analyzing naïve adaptations, we identify key limitations arising from modality-specific challenges. To address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework that refines spectrogram masks for precise, input-aligned separation. Our approach effectively repurposes pretrained diffusion models for source separation, achieving competitive performance without task-specific supervision. This work expands the application of diffusion models beyond generation, establishing a new paradigm for zero-shot audio separation."
   ],
   "p1": 4983,
   "pn": 4987,
   "doi": "10.21437/Interspeech.2025-840",
   "url": "interspeech_2025/lee25g_interspeech.html"
  },
  "shabtay25_interspeech": {
   "authors": [
    [
     "Nimrod",
     "Shabtay"
    ],
    [
     "Zvi",
     "Kons"
    ],
    [
     "Avihu",
     "Dekel"
    ],
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Assaf",
     "Arbelle"
    ]
   ],
   "title": "Spoken Question Answering for Visual Queries",
   "original": "843",
   "order": 981,
   "page_count": 5,
   "abstract": [
    "Question answering (QA) systems are designed to answer natural language questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA system to accept visual and spoken input respectively. This work aims to create a system that enables user interaction through both speech and images. That is achieved through the fusion of text, speech, and images modalities to tackle the task of spoken VQA (SVQA). The resulting multi-modal model has textual, visual, and spoken inputs and can answer spoken questions on images. Training and evaluating SVQA models requires a dataset for all three modalities, but no such dataset currently exists. We address this problem by synthesizing VQA datasets using two zero-shot TTS models. Our initial findings indicate that a model trained only with synthesized speech nearly reaches the performance of the upper-bounding model trained on textual QAs. In addition, we show that the choice of the TTS model has a minor impact on accuracy."
   ],
   "p1": 4823,
   "pn": 4827,
   "doi": "10.21437/Interspeech.2025-843",
   "url": "interspeech_2025/shabtay25_interspeech.html"
  },
  "avidan25_interspeech": {
   "authors": [
    [
     "Tzlil",
     "Avidan"
    ],
    [
     "Bracha",
     "Laufer-Goldshtein"
    ]
   ],
   "title": "Deep-Simplex Multichannel Speech Separation",
   "original": "844",
   "order": 299,
   "page_count": 5,
   "abstract": [
    "Numerous methods exist for sound source separation, leveraging either classical signal processing or deep learning approaches. While deep-learning-based models often outperform conventional methods, they require large training datasets and struggle to generalize to new settings. To address this, we propose Deep-Simplex, a deep prior-based method that reconstructs the probability simplex of speaker activity over time. This global activity probability guides the estimation of a local mask per frequency, identifying the dominant speaker in each time-frequency (TF) bin. We then use this mask for both spatial and spectral separation. Experimental results demonstrate that Deep-Simplex outperforms competing baselines in different reverberation conditions."
   ],
   "p1": 1463,
   "pn": 1467,
   "doi": "10.21437/Interspeech.2025-844",
   "url": "interspeech_2025/avidan25_interspeech.html"
  },
  "liang25c_interspeech": {
   "authors": [
    [
     "Ruoxuan",
     "Liang"
    ],
    [
     "Xiangjian",
     "Zeng"
    ],
    [
     "Zhen",
     "Liu"
    ],
    [
     "Qingqiang",
     "Wu"
    ],
    [
     "RuiChen",
     "Zhang"
    ],
    [
     "Le",
     "Ren"
    ]
   ],
   "title": "WhisperMSS: A Two-Stage Framework for Mandarin Singing Transcription and Segmentation Using Pretrained Models",
   "original": "847",
   "order": 633,
   "page_count": 5,
   "abstract": [
    "This paper addresses the challenges of Mandarin singing transcription and segmentation by proposing a two-stage framework based on Whisper. Our research makes three key contributions: First,enhancing transcription accuracy via WhisperMLT, which incorporates a Chinese-specific text embedding layer, a CTC branch atop the encoder, and a Transformer-based contextual network; Second, optimizing CTC posterior probabilities through syllable-aligned pseudo-labeling, which generates one-hot frame-level labels from timestamp-annotated datasets;Finally, achieving precise segmentation with CTC-Vseg, which implements silence label insertion, constrained state transitions, and dynamic programming-based path optimization.Experiments demonstrate superior performance in Mandarin singing segmentation, offering novel solutions for audio processing tasks."
   ],
   "p1": 3110,
   "pn": 3114,
   "doi": "10.21437/Interspeech.2025-847",
   "url": "interspeech_2025/liang25c_interspeech.html"
  },
  "zalkow25_interspeech": {
   "authors": [
    [
     "Frank",
     "Zalkow"
    ],
    [
     "Paolo",
     "Sani"
    ],
    [
     "Kishor",
     "Kayyar Lakshminarayana"
    ],
    [
     "Emanuël A. P.",
     "Habets"
    ],
    [
     "Nicola",
     "Pia"
    ],
    [
     "Christian",
     "Dittmar"
    ]
   ],
   "title": "Bridging the Training–Inference Gap in TTS: Training Strategies for Robust Generative Postprocessing for Low-Resource Speakers",
   "original": "854",
   "order": 505,
   "page_count": 5,
   "abstract": [
    "Modern text-to-speech synthesis systems usually consist of an acoustic model generating speech features, e.g., mel spectrograms, and a vocoder converting them into speech waveforms. The vocoder is typically trained with ground-truth features but receives features from the acoustic model during inference, leading to a mismatch between training and inference. To address this issue, previous work proposed employing generative postprocessing models to make the synthetic features appear more natural. While such systems can produce speech nearly indistinguishable from real speech when sufficient training data is available, their performance degrades with limited data. To mitigate this limitation, we propose a training data generation procedure using a subsampling strategy and multiple acoustic models. We evaluate it through listening tests, demonstrating consistent improvements in the naturalness of the synthetic speech across different postprocessing models and low-resource target speakers."
   ],
   "p1": 2470,
   "pn": 2474,
   "doi": "10.21437/Interspeech.2025-854",
   "url": "interspeech_2025/zalkow25_interspeech.html"
  },
  "phaye25_interspeech": {
   "authors": [
    [
     "Saisamarth Rajesh",
     "Phaye"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Andrew",
     "Harper"
    ]
   ],
   "title": "Model as Loss: A Self-Consistent Training Paradigm",
   "original": "855",
   "order": 485,
   "page_count": 5,
   "abstract": [
    "Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.  The Model as Loss paradigm leverages the encoder&#x27;s task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder&#x27;s learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets."
   ],
   "p1": 2370,
   "pn": 2374,
   "doi": "10.21437/Interspeech.2025-855",
   "url": "interspeech_2025/phaye25_interspeech.html"
  },
  "ou25_interspeech": {
   "authors": [
    [
     "Jiale",
     "Ou"
    ],
    [
     "Hongying",
     "Zan"
    ]
   ],
   "title": "CMSP-ST: Cross-modal Mixup with Speech Purification for End-to-End Speech Translation",
   "original": "858",
   "order": 5,
   "page_count": 5,
   "abstract": [
    "End-to-end speech translation (E2E ST) aims to directly convert speech in a source language into text in a target language, and its performance is constrained by the inherent modality gap. Existing methods attempt to align speech and text representations to perform cross-modal mixup at the token level, which overlooks the impact of redundant speech information. In this paper, we propose cross-modal mixup with speech purification for speech translation (CMSP-ST) to address this issue. Specifically, we remove the non-content features from speech through orthogonal projection and extract the purified speech features for cross-modal mixup. Additionally, we employ adversarial training under the Soft Alignment (S-Align) to relax the alignment granularity and improve robustness. Experimental results on the MuST-C En-De, CoVoST-2 Fr-En, and CoVoST-2 De-En benchmarks demonstrate that CMSP-ST effectively improves the speech translation performance of existing cross-modal mixup methods."
   ],
   "p1": 16,
   "pn": 20,
   "doi": "10.21437/Interspeech.2025-858",
   "url": "interspeech_2025/ou25_interspeech.html"
  },
  "dong25_interspeech": {
   "authors": [
    [
     "Wenwei",
     "Dong"
    ],
    [
     "Alif",
     "Silpachai"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Multitalker Babble in English Vowel Perception Training: A Comparison between Humans and Neural Models",
   "original": "859",
   "order": 262,
   "page_count": 5,
   "abstract": [
    "Perceptual training with multitalker babble can benefit first language listeners; however, it is unclear whether such training is beneficial for second language (L2) listeners and whether there is an optimal number of talkers for creating babble. Since perception experiments with humans are complex and time-consuming and neural models are inspired by the human brain, we explore the use of neural models and study how well their performance aligns with the results for human listeners. In this study, we first investigated how babble produced by 2 and 6 talkers affected the perceptual learning of English vowels by L2 listeners. We then fine-tuned Automatic Speech Recognition (ASR) models using the same babble datasets. The results showed that babble regardless of the number of talkers benefited listeners in speech-shaped noise, and the Wav2Vec2.0 model also improved the accuracy with babble training and exhibited trends more similarly to humans than the TDNN model."
   ],
   "p1": 1278,
   "pn": 1282,
   "doi": "10.21437/Interspeech.2025-859",
   "url": "interspeech_2025/dong25_interspeech.html"
  },
  "mcghee25_interspeech": {
   "authors": [
    [
     "Charles",
     "McGhee"
    ],
    [
     "Mark J.F.",
     "Gales"
    ],
    [
     "Kate M.",
     "Knill"
    ]
   ],
   "title": "Training Articulatory Inversion Models for Interspeaker Consistency",
   "original": "860",
   "order": 1138,
   "page_count": 5,
   "abstract": [
    "Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse mapping from speech to articulation. Exact articulatory prediction from speech alone may be impossible, as speakers can choose different forms of articulation seemingly without reference to their vocal tract structure. However, once a speaker has selected an articulatory form, their productions vary minimally. Recent works in AAI have proposed adapting Self-Supervised Learning (SSL) models to single-speaker datasets, claiming that these single-speaker models provide a universal articulatory template. In this paper, we investigate whether SSL-adapted models trained on single and multi-speaker data produce articulatory targets which are consistent across speaker identities for English and Russian. We do this through the use of a novel evaluation method which extracts articulatory targets using minimal pair sets. We also present a training method which can improve interspeaker consistency using only speech data."
   ],
   "p1": 5583,
   "pn": 5587,
   "doi": "10.21437/Interspeech.2025-860",
   "url": "interspeech_2025/mcghee25_interspeech.html"
  },
  "yan25b_interspeech": {
   "authors": [
    [
     "Yuyang",
     "Yan"
    ],
    [
     "Sami O.",
     "Simons"
    ],
    [
     "Visara",
     "Urovi"
    ]
   ],
   "title": "Developing a LeFF Transformer Model for Exacerbated Speech Detection in COPD and Asthma ",
   "original": "861",
   "order": 205,
   "page_count": 5,
   "abstract": [
    "The acoustic features of speech exhibit variations across different respiratory conditions, highlighting the potential of voice analysis as a valuable tool for non-invasive monitoring systems. Early detection of exacerbations plays a critical role in the effective management of chronic respiratory diseases, such as chronic obstructive pulmonary disease (COPD) and asthma. This paper presents the utilization of fused acoustic features from multiple domains, integrated with a Locally-enhanced Feed-Forward Network (LeFF) Transformer model, to classify exacerbated and stable speech in COPD and asthma patients. The proposed methodology is evaluated on the TACTICAS dataset, demonstrating superior performance compared to current state-of-the-art approaches, underscoring its potential for exacerbations monitoring in COPD and asthma patients."
   ],
   "p1": 993,
   "pn": 997,
   "doi": "10.21437/Interspeech.2025-861",
   "url": "interspeech_2025/yan25b_interspeech.html"
  },
  "li25k_interspeech": {
   "authors": [
    [
     "Zhaoyang",
     "Li"
    ],
    [
     "Jie",
     "Wang"
    ],
    [
     "XiaoXiao",
     "Li"
    ],
    [
     "Wangjie",
     "Li"
    ],
    [
     "Longjie",
     "Luo"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm",
   "original": "862",
   "order": 1063,
   "page_count": 5,
   "abstract": [
    "In speaker diarization, traditional clustering-based methods remain widely used in real-world applications. However, these methods struggle with the complex distribution of speaker embeddings and overlapping speech segments. To address these limitations, we propose an Overlapping Community Detection method based on Graph Attention networks and the Label Propagation Algorithm (OCDGALP). The proposed framework comprises two key components: (1) a graph attention network that refines speaker embeddings and node connections by aggregating information from neighboring nodes, and (2) a label propagation algorithm that assigns multiple community labels to each node, enabling simultaneous clustering and overlapping community detection. Experimental results show that the proposed method significantly reduces the Diarization Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07% with oracle VAD."
   ],
   "p1": 5213,
   "pn": 5217,
   "doi": "10.21437/Interspeech.2025-862",
   "url": "interspeech_2025/li25k_interspeech.html"
  },
  "chen25g_interspeech": {
   "authors": [
    [
     "Shuwen",
     "Chen"
    ],
    [
     "Qingke",
     "Sun"
    ],
    [
     "Yue",
     "Huang"
    ],
    [
     "Yingyi",
     "Luo"
    ]
   ],
   "title": "The Prosodic Characteristics of Standard Chinese Rhetorical Questions in Naturalistic Settings",
   "original": "863",
   "order": 1094,
   "page_count": 5,
   "abstract": [
    "The current study investigated the prosodic features of rhetorical questions in Standard Chinese within naturalistic settings, utilizing data collected from 103 native Mandarin speakers. String-identical information-seeking questions (ISQs) and rhetorical questions (RQs) were recorded through an online platform, ensuring a diverse and representative sample. The results revealed that, when freely reading aloud to the phone, speakers tended to convey rhetorical meaning by either shifting prominence toward or enhancing the prominence of the verb or modal verb. Acoustically, the prominent verbs were characterized by higher pitch and longer duration. In other positions, phonetic markers varied depending on the sentence structure. Our large-sample findings highlight the interface of prosody and syntax in conveying communicative intention. This research also addresses a critical gap by moving beyond laboratory-based data to examine speech phenomena in more ecologically valid contexts."
   ],
   "p1": 5363,
   "pn": 5367,
   "doi": "10.21437/Interspeech.2025-863",
   "url": "interspeech_2025/chen25g_interspeech.html"
  },
  "simko25_interspeech": {
   "authors": [
    [
     "Juraj",
     "Šimko"
    ],
    [
     "Benjamin",
     "Elie"
    ],
    [
     "Alice",
     "Turk"
    ]
   ],
   "title": "Self-supervised Optimality-Guided Learning of Speech Articulation",
   "original": "866",
   "order": 213,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel approach for modeling speech articulatory planning based on Optimal Control Theory. The presented approach uses an internal feed-forward controller model that learns to predict optimal articulatory commands minimizing a context-dependent objective function. This objective function combines conflicting tasks of minimizing articulatory effort and maximizing the recognition probability of a target vowel based on acoustic characteristics. We present a self-supervised optimality-guided architecture for training the feedforward internal model that directly uses the objective function as a training loss. Simulations involving isolated vowels of American-English show that online training of the internal model enables feedforward estimation of near-optimal articulatory parameters."
   ],
   "p1": 1033,
   "pn": 1037,
   "doi": "10.21437/Interspeech.2025-866",
   "url": "interspeech_2025/simko25_interspeech.html"
  },
  "hu25e_interspeech": {
   "authors": [
    [
     "Ke",
     "Hu"
    ],
    [
     "Krishna",
     "Puvvada"
    ],
    [
     "Elena",
     "Rastorgueva"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Shuoyang",
     "Ding"
    ],
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "Hainan",
     "Xu"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Word Level Timestamp Generation for Automatic Speech Recognition and Translation",
   "original": "869",
   "order": 524,
   "page_count": 5,
   "abstract": [
    "We introduce a data-driven approach for enabling word-level timestamp prediction in the Canary model. Accurate timestamp information is crucial for a variety of downstream tasks such as speech content retrieval and timed subtitles. While traditional hybrid systems and end-to-end (E2E) models may employ external modules for timestamp prediction, our approach eliminates the need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner (NFA) as a teacher model, we generate word-level timestamps and train the Canary model to predict timestamps directly. We introduce a new &lt;|timestamp|&gt; token, enabling the Canary model to predict start and end timestamps for each word. Our method demonstrates precision and recall rates between 80% and 90%, with timestamp prediction errors ranging from 20 to 120 ms across four languages, with minimal WER degradation. Additionally, we extend our system to automatic speech translation (AST) tasks, achieving timestamp prediction errors around 200 milliseconds. Our code is open-sourced through NeMo, and the checkpoint is released at Hugging Face."
   ],
   "p1": 2565,
   "pn": 2569,
   "doi": "10.21437/Interspeech.2025-869",
   "url": "interspeech_2025/hu25e_interspeech.html"
  },
  "mansi25_interspeech": {
   "authors": [
    [
     "",
     "Mansi"
    ],
    [
     "Anastasios",
     "Lepipas"
    ],
    [
     "Dominika C",
     "Woszczyk"
    ],
    [
     "Yiying",
     "Guan"
    ],
    [
     "Soteris",
     "Demetriou"
    ]
   ],
   "title": "Understanding Dementia Speech Alignment with Diffusion-Based Image Generation",
   "original": "871",
   "order": 292,
   "page_count": 5,
   "abstract": [
    "Text-to-image models generate highly realistic images based on natural language descriptions and millions of users use them to create and share images online. While it is expected that such models can align input text and generated image in the same latent space little has been done to understand whether this alignment is possible between pathological speech and generated images. In this work, we examine the ability of such models to align dementia-related speech information with the generated images and develop methods to explain this alignment. Surprisingly, we found that dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset. We then leverage explainability methods to show which parts of the language contribute to the detection."
   ],
   "p1": 1428,
   "pn": 1432,
   "doi": "10.21437/Interspeech.2025-871",
   "url": "interspeech_2025/mansi25_interspeech.html"
  },
  "hu25f_interspeech": {
   "authors": [
    [
     "Ke",
     "Hu"
    ],
    [
     "Ehsan",
     "Hosseini-Asl"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Edresson",
     "Casanova"
    ],
    [
     "Subhankar",
     "Ghosh"
    ],
    [
     "Piotr",
     "Żelasko"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Jason",
     "Li"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model",
   "original": "874",
   "order": 554,
   "page_count": 5,
   "abstract": [
    "Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility."
   ],
   "p1": 2715,
   "pn": 2719,
   "doi": "10.21437/Interspeech.2025-874",
   "url": "interspeech_2025/hu25f_interspeech.html"
  },
  "li25l_interspeech": {
   "authors": [
    [
     "Zhaoyang",
     "Li"
    ],
    [
     "Haodong",
     "Zhou"
    ],
    [
     "Longjie",
     "Luo"
    ],
    [
     "XiaoXiao",
     "Li"
    ],
    [
     "Yongxin",
     "Chen"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Cross-attention and Self-attention for Audio-visual Speaker Diarization in MISP-Meeting Challenge",
   "original": "875",
   "order": 386,
   "page_count": 5,
   "abstract": [
    "This paper presents the system developed for Task 1 of the Multi-modal Information-based Speech Processing (MISP) 2025 Challenge. We introduce CASA-Net, an embedding fusion method designed for end-to-end audio-visual speaker diarization (AVSD) systems. CASA-Net incorporates a cross-attention (CA) module to effectively capture cross-modal interactions in audio-visual signals and employs a self-attention (SA) module to learn contextual relationships among audio-visual frames. To further enhance performance, we adopt a training strategy that integrates pseudo-label refinement and retraining, improving the accuracy of timestamp predictions. Additionally, median filtering and overlap averaging are applied as post-processing techniques to eliminate outliers and smooth prediction labels. Our system achieved a diarization error rate (DER) of 8.18% on the evaluation set, representing a relative improvement of 47.3% over the baseline DER of 15.52%."
   ],
   "p1": 1893,
   "pn": 1897,
   "doi": "10.21437/Interspeech.2025-875",
   "url": "interspeech_2025/li25l_interspeech.html"
  },
  "elleuch25_interspeech": {
   "authors": [
    [
     "Haroun",
     "Elleuch"
    ],
    [
     "Salima",
     "Mdhaffar"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Fethi",
     "Bougares"
    ]
   ],
   "title": "ADI-20: Arabic Dialect Identification dataset and models",
   "original": "884",
   "order": 566,
   "page_count": 5,
   "abstract": [
    "We present ADI-20, an extension of the previously published ADI-17 Arabic Dialect Identification (ADI) dataset. ADI-20 covers all Arabic-speaking countries&#x27; dialects. It comprises 3,556 hours from 19 Arabic dialects in addition to Modern Standard Arabic (MSA). We used this dataset to train and evaluate various state-of-the-art ADI systems. We explored fine-tuning pre-trained ECAPA-TDNN-based models, as well as Whisper encoder blocks coupled with an attention pooling layer and a classification dense layer. We investigated the effect of (i) training data size and (ii) the model&#x27;s number of parameters on identification performance. Our results show a small decrease in F1 score while using only 30% of the original training data. We open-source our collected data and trained models to enable the reproduction of our work, as well as support further research in ADI."
   ],
   "p1": 2775,
   "pn": 2779,
   "doi": "10.21437/Interspeech.2025-884",
   "url": "interspeech_2025/elleuch25_interspeech.html"
  },
  "acevedo25_interspeech": {
   "authors": [
    [
     "Emiliano",
     "Acevedo"
    ],
    [
     "Martín",
     "Rocamora"
    ],
    [
     "Magdalena",
     "Fuentes"
    ]
   ],
   "title": "Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical Sound Classification",
   "original": "886",
   "order": 272,
   "page_count": 5,
   "abstract": [
    "Audio-text models are widely used in zero-shot environmental sound classification as they alleviate the need for annotated data. However, we show that their performance severely drops in the presence of background sound sources. Our analysis reveals that this degradation is primarily driven by SNR levels of background soundscapes, and independent of background type. To address this, we propose a novel method that quantifies and integrates the contribution of background sources into the classification process, improving performance without requiring model retraining. Our domain adaptation technique enhances accuracy across various backgrounds and SNR conditions. Moreover, we analyze the modality gap between audio and text embeddings, showing that narrowing this gap improves classification performance. The method generalizes effectively across state-of-the-art prototypical approaches, showcasing its scalability and robustness for diverse environments."
   ],
   "p1": 1328,
   "pn": 1332,
   "doi": "10.21437/Interspeech.2025-886",
   "url": "interspeech_2025/acevedo25_interspeech.html"
  },
  "mohammadamini25_interspeech": {
   "authors": [
    [
     "Mohammad",
     "Mohammadamini"
    ],
    [
     "Aghilas",
     "Sini"
    ],
    [
     "Marie",
     "Tahon"
    ],
    [
     "Antoine",
     "Laurent"
    ]
   ],
   "title": "Scaling pseudo-labeling data for end-to-end low-resource speech translation (the case of Kurdish language)",
   "original": "887",
   "order": 186,
   "page_count": 5,
   "abstract": [
    "In this paper we propose a pseudo-labeling pipeline to generate End-to-End Speech to Text Translation (E2E S2TT) data for low-resource languages. This pipeline allows us to achieve very promising results on S2TT task without having any parallel speech corpora. The proposed pipeline is composed of a speech segmentation, followed by a speech recognition system and a machine translation system. Our study is performed on Kurdish language which doesn&#x27;t have resources for E2E S2TT. In our study, we firstly fine-tune and evaluate the ASR and MT systems to achieve a degree of reliability on the these components. The pipeline is used to pseudo-label 3,200 hours of Kurdish speech aligned with English translation. The pseudo-labeled data is extensively evaluated with different E2E S2TT systems such as Seq2Seq Transfomers and Whisper model. Achieving a 20.68 BLEU score on Fleurs benchmark shows the effectiveness of the our approach and its potential for other low-resource languages. The parallel pseudo-labeled data can be retrieved from: https://lium.univ-lemans.fr/en/ckbens2tt/"
   ],
   "p1": 898,
   "pn": 902,
   "doi": "10.21437/Interspeech.2025-887",
   "url": "interspeech_2025/mohammadamini25_interspeech.html"
  },
  "phukon25_interspeech": {
   "authors": [
    [
     "Bornali",
     "Phukon"
    ],
    [
     "Xiuwen",
     "Zheng"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches",
   "original": "891",
   "order": 1163,
   "page_count": 5,
   "abstract": [
    "Traditional ASR metrics like WER and CER fail to capture intelligibility, especially for dysarthric and dysphonic speech, where semantic alignment matters more than exact word matches. ASR systems struggle with these speech types, often producing errors like phoneme repetitions and imprecise consonants, yet the meaning remains clear to human listeners. We identify two key challenges: (1) Existing metrics do not adequately reflect intelligibility, and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR transcripts of dysarthric speech remains underexplored. To address this, we propose a novel metric integrating Natural Language Inference (NLI) scores, semantic similarity, and phonetic similarity. Our ASR evaluation metric achieves a 0.890 correlation with human judgments on Speech Accessibility Project data, surpassing traditional methods and emphasizing the need to prioritize intelligibility over error-based measures."
   ],
   "p1": 5708,
   "pn": 5712,
   "doi": "10.21437/Interspeech.2025-891",
   "url": "interspeech_2025/phukon25_interspeech.html"
  },
  "janse25_interspeech": {
   "authors": [
    [
     "Esther",
     "Janse"
    ],
    [
     "Chen",
     "Shen"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Prediction of listening effort ratings for habitual and clear-Lombard speech presented in noise",
   "original": "892",
   "order": 264,
   "page_count": 5,
   "abstract": [
    "Earlier research developed the high-energy glimpse proportion metric (HEGP) to capture those speech-dominant spectro-temporal regions, or glimpses, that survive energetic masking when speech is presented in noise. Whereas the HEGP metric has been shown to correlate with intelligibility of speech presented in noise, this study investigated whether HEGP predicts listening effort (LE) ratings, and if so, whether it does so differentially for habitual and clear-Lombard speech. Secondly, the study investigated whether acoustic measures of spectral balance, median F0 and F0 range, and articulation rate, explained additional variance in LE. Results showed that HEGP predicted LE ratings equally strongly across speaking styles. Beyond HEGP, wider F0 range and lower articulation rate were also associated with decreased LE, particularly in the habitual speech condition. Thus, the HEGP release-from-masking metric, complemented by speaking-style aspects, can help explain LE ratings in noise."
   ],
   "p1": 1288,
   "pn": 1292,
   "doi": "10.21437/Interspeech.2025-892",
   "url": "interspeech_2025/janse25_interspeech.html"
  },
  "hu25g_interspeech": {
   "authors": [
    [
     "De",
     "Hu"
    ],
    [
     "Qilong",
     "Li"
    ]
   ],
   "title": "Joint Rate Allocation and Sensor Selection for Speech Enhancement in Wireless Acoustic Sensor Networks",
   "original": "893",
   "order": 784,
   "page_count": 5,
   "abstract": [
    "In wireless acoustic sensor networks (WASNs) with high spatial resolution, energy efficiency is crucial for speech enhancement (SE) due to limited node energy. To optimize energy efficiency under predefined performance constraints, sensor selection (SS), rate allocation (RA), or a joint RA and SS (RASS) approach can be employed. However, since the above criteria were developed in a frequency-dependent manner, it may lead to conflicts among SS results over different frequencies. In this work, we propose a novel RASS model by promoting bit rate sparsity in RA, eliminating NP-hard Boolean programming, and extending it to a frequency-invariant RASS (FI-RASS) for consistent SS results across frequencies. In addition, we introduce a re-weighting strategy to further reduce energy consumption in the WASN. Compared with existing RA, SS, and RASS approaches, the FI-RASS provides a remarkable superiority in terms of energy efficiency. Numerical experiments demonstrate the effectiveness of the proposed method."
   ],
   "p1": 3838,
   "pn": 3842,
   "doi": "10.21437/Interspeech.2025-893",
   "url": "interspeech_2025/hu25g_interspeech.html"
  },
  "kim25k_interspeech": {
   "authors": [
    [
     "Bongjun",
     "Kim"
    ],
    [
     "Arindam",
     "Ghosh"
    ],
    [
     "Mark C.",
     "Fuhs"
    ],
    [
     "Anurag",
     "Chowdhury"
    ],
    [
     "Deblin",
     "Bagchi"
    ],
    [
     "Monika",
     "Woszczyna"
    ]
   ],
   "title": "A Hybrid Approach to Combining Role Diarization with ASR for Professional Conversations",
   "original": "895",
   "order": 1069,
   "page_count": 5,
   "abstract": [
    "In professional settings, conversations often involve persons with defined roles (doctor, patient, lawyer, client, etc.), and the intelligibility of a conversational transcript may be improved by annotating conversational turns with the role of the speaker, e.g. &quot;Doctor: How are you feeling? Patient: I sprained my ankle.&quot; We propose a novel hybrid architecture that combines an ASR model augmented to label the speaker&#x27;s role at each speaker change point with a d-vector-based diarization system. This system outperforms modular and fully integrated baselines by 12% and 28%, respectively. We also show that, when an ASR transducer model is trained to predict role or speaker-change tokens as part of the transcript, these token timings can improve diarization more than the adjacent word token timings can, despite there being no explicit training signal conveying precise speaker change points."
   ],
   "p1": 5243,
   "pn": 5247,
   "doi": "10.21437/Interspeech.2025-895",
   "url": "interspeech_2025/kim25k_interspeech.html"
  },
  "li25m_interspeech": {
   "authors": [
    [
     "Haoyang",
     "Li"
    ],
    [
     "Yuchen",
     "Hu"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Songting",
     "Liu"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based Methodology",
   "original": "896",
   "order": 1051,
   "page_count": 5,
   "abstract": [
    "Deep neural network (DNN)-based speech enhancement (SE) usually uses conventional activation functions, which lack the expressiveness to capture complex multiscale structures needed for high-fidelity SE. Group-Rational KAN (GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN’s expressiveness while improving scalability on complex tasks. We adapt GR-KAN to existing DNN-based SE by replacing dense layers with GR-KAN layers in the time-frequency (T-F) domain MP-SENet and adapting GR-KAN&#x27;s activations into the 1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that GR-KAN requires up to 4× fewer parameters while improving PESQ by up to 0.1. In contrast, KAN, facing scalability issues, outperforms MLP on a small-scale signal modeling task but fails to improve MP-SENet. We demonstrate the first successful use of KAN-based methods for consistent improvement in both time- and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE."
   ],
   "p1": 5153,
   "pn": 5157,
   "doi": "10.21437/Interspeech.2025-896",
   "url": "interspeech_2025/li25m_interspeech.html"
  },
  "vanbemmel25_interspeech": {
   "authors": [
    [
     "Loes",
     "van Bemmel"
    ],
    [
     "Lauren G",
     "Reinders"
    ],
    [
     "Folkert",
     "Brijker"
    ],
    [
     "Bas",
     "Holverda"
    ],
    [
     "Frits M.E.",
     "Franssen"
    ],
    [
     "Hanneke",
     "van Helvoort"
    ],
    [
     "Visara",
     "Urovi"
    ],
    [
     "Marieke",
     "Spreeuwenberg"
    ],
    [
     "Sami O.",
     "Simons"
    ]
   ],
   "title": "SPEAKtoCOPD: a flashmob study to collect COPD speech",
   "original": "899",
   "order": 204,
   "page_count": 5,
   "abstract": [
    "Chronic Obstructive Pulmonary Disease (COPD) is the most common chronic respiratory disease in the world. Fluctuations in symptoms frequently occur in COPD and might require additional treatments. Automatic monitoring or diagnosis of these fluctuations may aid the management of people with COPD. Speech analysis is a possible remote non-invasive methods to achieve this. However, this analysis is disease and possibly language specific and the development of speech models requires high-quality and high-quantity data, which are often not available. In this study we introduce SPEAKtoCOPD: a dataset of Dutch speech collected with a flash mob study targeting people with a respiratory disease (specifically COPD). In this paper, we describe the flash mob methodology, our collected data, and perform an automatic quality check to assess whether the speech tasks have been performed correctly. We aim to publish our gathered data for researchers to improve speech analysis for respiratory health."
   ],
   "p1": 988,
   "pn": 992,
   "doi": "10.21437/Interspeech.2025-899",
   "url": "interspeech_2025/vanbemmel25_interspeech.html"
  },
  "chao25b_interspeech": {
   "authors": [
    [
     "Rong",
     "Chao"
    ],
    [
     "Rauf",
     "Nasretdinov"
    ],
    [
     "Yu-Chiang Frank",
     "Wang"
    ],
    [
     "Ante",
     "Jukic"
    ],
    [
     "Szu-Wei",
     "Fu"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "Universal Speech Enhancement with Regression and Generative Mamba",
   "original": "900",
   "order": 184,
   "page_count": 5,
   "abstract": [
    "The Interspeech 2025 URGENT Challenge aimed to advance universal, robust, and generalizable speech enhancement by unifying speech enhancement tasks across a wide variety of conditions, including seven different distortion types and five languages. We present Universal Speech Enhancement Mamba (USEMamba), a state-space speech enhancement model designed to handle long-range sequence modeling, time-frequency structured processing, and sampling frequency-independent feature extraction. Our approach primarily relies on regression-based modeling, which performs well across most distortions. However, for packet loss and bandwidth extension, where missing content must be inferred, a generative variant of the proposed USEMamba proves more effective. Despite being trained in only a subset of the full training data, USEMamba achieved 2nd place in Track 1 during the blind test phase, demonstrating strong generalization across diverse conditions."
   ],
   "p1": 888,
   "pn": 892,
   "doi": "10.21437/Interspeech.2025-900",
   "url": "interspeech_2025/chao25b_interspeech.html"
  },
  "lameris25_interspeech": {
   "authors": [
    [
     "Harm",
     "Lameris"
    ],
    [
     "Joakim",
     "Gustafsson"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "VoiceQualityVC: A Voice Conversion System for Studying the Perceptual Effects of Voice Quality in Speech",
   "original": "902",
   "order": 470,
   "page_count": 5,
   "abstract": [
    "Voice quality is an often overlooked aspect of speech with many communicative functions. Voice quality conveys both paralinguistic and pragmatic information, such as signalling speaker stance and aids in grounding. In this paper, we present VoiceQualityVC, a tool that can manipulate the voice quality of both natural and synthesized speech using voice quality features including CPPS, H1-H2, and H1-A3. VoiceQualityVC is a research tool for perceptual experiments into voice quality and UX experiments for voice design. We perform an objective evaluation demonstrating the control of these features as well as subjective listening tests of the paralinguistic attributes of intimacy, valence, and investment. In these listening tests breathy voice was rated as more intimate and more invested than modal voice and creaky voice was rated as less intimate and less positive. The code and models can be found at https://github.com/Hfkml/VQVC."
   ],
   "p1": 2295,
   "pn": 2299,
   "doi": "10.21437/Interspeech.2025-902",
   "url": "interspeech_2025/lameris25_interspeech.html"
  },
  "vandalen25_interspeech": {
   "authors": [
    [
     "Rogier C.",
     "van Dalen"
    ],
    [
     "Shucong",
     "Zhang"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Sourav",
     "Bhattacharya"
    ]
   ],
   "title": "Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes",
   "original": "903",
   "order": 1007,
   "page_count": 5,
   "abstract": [
    "Speech recognisers usually perform optimally only in a specific environment and need to be adapted to work well in another. For adaptation to a new speaker, there is often too little data for fine-tuning to be robust, and that data is usually unlabelled. This paper proposes a combination of approaches to make adaptation to a single minute of data robust. First, instead of estimating the adaptation parameters with cross-entropy on a single error-prone hypothesis or &quot;pseudo-label&quot;, this paper proposes a novel loss function, the conditional entropy over complete hypotheses. Using multiple hypotheses makes adaptation more robust to errors in the initial recognition. Second, a &quot;speaker code&quot; characterises a speaker in a vector short enough that it requires little data to estimate. On a far-field noise-augmented version of Common Voice, the proposed scheme yields a 20 % relative improvement in word error rate on one minute of adaptation data, increasing on 10 minutes to 29 %."
   ],
   "p1": 4953,
   "pn": 4957,
   "doi": "10.21437/Interspeech.2025-903",
   "url": "interspeech_2025/vandalen25_interspeech.html"
  },
  "hu25h_interspeech": {
   "authors": [
    [
     "Junsheng",
     "Hu"
    ],
    [
     "Shaojie",
     "Li"
    ],
    [
     "Qintuya",
     "Si"
    ],
    [
     "De",
     "Hu"
    ]
   ],
   "title": "D-GAT: Dual Graph Attention Network for Global HRTF Interpolation",
   "original": "905",
   "order": 513,
   "page_count": 5,
   "abstract": [
    "To achieve 3D audio rendering, high-quality Head-Related Transfer Functions (HRTFs) are essential. As measuring HRTFs is time-consuming and tedious, spatial interpolation is often adopted to generate high-resolution HRTFs from low-resolution ones. In this paper, we propose a Dual-Graph Attention Network (D-GAT) for HRTF upsampling. Specifically, we first design a branch of GAT to learn the relationship among HRTFs from adjacent points. In addition, we introduce another branch of GAT to find a mapping from physical features (including the absolute target position and the anthropometric characteristics) to HRTFs. By combining such two GATs in a parallel architecture, the D-GAT is built. Furthermore, a dynamic edge weighting mechanism is adopted in the D-GAT, which allows the model to learn geometry relationships among vertices more flexibly. Experimental results demonstrate the efficacy of the proposed D-GAT in accurately predicting HRTFs, yielding state-of-the-art performance."
   ],
   "p1": 2510,
   "pn": 2514,
   "doi": "10.21437/Interspeech.2025-905",
   "url": "interspeech_2025/hu25h_interspeech.html"
  },
  "fang25c_interspeech": {
   "authors": [
    [
     "Minghui",
     "Fang"
    ],
    [
     "Shengpeng",
     "Ji"
    ],
    [
     "Jialong",
     "Zuo"
    ],
    [
     "Xize",
     "Cheng"
    ],
    [
     "Wenrui",
     "Liu"
    ],
    [
     "Xiaoda",
     "Yang"
    ],
    [
     "Ruofan",
     "Hu"
    ],
    [
     "Jieming",
     "Zhu"
    ],
    [
     "Zhou",
     "Zhao"
    ]
   ],
   "title": "GTA: Towards Generative Text-To-Audio Retrieval via Multi-Scale Tokenizer",
   "original": "908",
   "order": 541,
   "page_count": 5,
   "abstract": [
    "Text-to-audio retrieval is a fundamental task in acoustic signal processing. Currently, mainstream approaches primarily employ a dual-tower architecture, independently encoding text and audio while performing similarity score matching. However, these methods struggle to maintain a uniform embedding space and the latency of score matching increases with corpus size. In this light, we propose GTA to move towards a generative text-to-audio retrieval paradigm. Specifically, we utilize a multi-scale audio tokenizer to embed audio semantics into identifiers, while incorporating a dual-alignment strategy to ensure consistency between audio and text semantics. Furthermore, we implement curriculum learning to bridge the gap between training and inference, guiding the model toward precise token-level generation. Extensive experiments demonstrate the effectiveness and robustness of GTA, further validating the feasibility of the generative text-to-audio retrieval paradigm."
   ],
   "p1": 2650,
   "pn": 2654,
   "doi": "10.21437/Interspeech.2025-908",
   "url": "interspeech_2025/fang25c_interspeech.html"
  },
  "reinders25_interspeech": {
   "authors": [
    [
     "Lauren G",
     "Reinders"
    ],
    [
     "Loes",
     "van Bemmel"
    ],
    [
     "Alexander",
     "Mackay"
    ],
    [
     "David",
     "Nobbs"
    ],
    [
     "Frits M.E.",
     "Franssen"
    ],
    [
     "Hester",
     "Gietema"
    ],
    [
     "Simona",
     "Schäfer"
    ],
    [
     "Sami O.",
     "Simons"
    ]
   ],
   "title": "Effect of physical exercise on voice in people living with COPD",
   "original": "910",
   "order": 207,
   "page_count": 5,
   "abstract": [
    "Previous research on the effect of exercise on voice has been mainly performed in a healthy population. However, the competition between breathing and speaking might be more pronounced in people with COPD. We aimed to evaluate voice features change in people living with COPD after exercise. A prolonged vowel /a/ and paced reading task were performed before and after an exercise task. In this exploratory study, differences were found in the sustained phonation for shimmer (apq5 and local), vowel duration, F1 frequency, and H1-A3 harmonic difference, when taking sex into account. Also, we saw a trend that voice features change by increasing COPD severity for HNR (mean) in the sustained phonation, and in the forced reading task for F2 frequency, H1-A3 harmonic difference (mean) and Hammarberg index. More research is needed using voice recordings from a larger sample size to confirm these findings, and possibly implement this in an exacerbation model using voice."
   ],
   "p1": 1003,
   "pn": 1007,
   "doi": "10.21437/Interspeech.2025-910",
   "url": "interspeech_2025/reinders25_interspeech.html"
  },
  "ambikairajah25_interspeech": {
   "authors": [
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Jingyao",
     "Wu"
    ],
    [
     "Ting",
     "Dang"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ]
   ],
   "title": "A Study of Speech Embedding Similarities Between Australian Aboriginal and High-Resource Languages",
   "original": "911",
   "order": 306,
   "page_count": 5,
   "abstract": [
    "Low-resource languages, such as Australian Aboriginal Languages, are underrepresented in the AI landscape due to limited availability of digital data, which in turn hinders speech processing model development. Leveraging sufficiently similar high-resource languages may help bridge this gap. This study examines the similarities between speech embeddings of aboriginal languages and 107 high-resource languages, including English, Spanish, and Mandarin, using Wav2Vec2 and VoxLingua107-ECAPA-TDNN. Through three language identification tasks, we analyze Warlpiri, Dalabon, and Light Warlpiri alongside 107 other languages. Our results reveal that aboriginal languages are most frequently identified as Māori, suggesting phonetic or structural similarities, while showing significant differences from globally dominant languages. Additionally, we also observe that Warlpiri and Dalabon exhibited closer matches with Hindi and Malayalam, than with other languages."
   ],
   "p1": 1498,
   "pn": 1502,
   "doi": "10.21437/Interspeech.2025-911",
   "url": "interspeech_2025/ambikairajah25_interspeech.html"
  },
  "dong25b_interspeech": {
   "authors": [
    [
     "Wenwei",
     "Dong"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Roeland",
     "van Hout"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Evaluating Progress of CALL System Users on Accentedness and Comprehensibility: An Acoustic and ASR-Based Approach",
   "original": "914",
   "order": 904,
   "page_count": 5,
   "abstract": [
    "Accentedness and comprehensibility scales are widely used to evaluate the effectiveness of Computer-Assisted Language Learning (CALL) tools. Such evaluations mainly rely on subjective expert assessment to measure accentedness and comprehensibility. In this study, we applied Automatic Speech Recognition (ASR) measures and used acoustic features to investigate the importance of objective features and measures that may underlie accentedness and comprehensibility. Furthermore, we combined the practice length of CALL tool users with the pre- and post-test data to gain insight into how practice affects progress. The experimental results showed that ASR measures, loudness, and formant-related features were most relevant for accentedness and comprehensibility. Additionally, practice time was positively correlated with progress in accentedness and comprehensibility. We also found that longer practice time contributes more to the changes in the acoustic features selected by the Lasso regression."
   ],
   "p1": 4438,
   "pn": 4442,
   "doi": "10.21437/Interspeech.2025-914",
   "url": "interspeech_2025/dong25b_interspeech.html"
  },
  "fucci25_interspeech": {
   "authors": [
    [
     "Dennis",
     "Fucci"
    ],
    [
     "Marco",
     "Gaido"
    ],
    [
     "Matteo",
     "Negri"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Luisa",
     "Bentivogli"
    ]
   ],
   "title": "Echoes of Phonetics:  Unveiling Relevant Acoustic Cues for ASR via Feature Attribution",
   "original": "918",
   "order": 43,
   "page_count": 5,
   "abstract": [
    "Despite significant advances in ASR, the specific acoustic cues models rely on remain unclear. Prior studies have examined such cues on a limited set of phonemes and outdated models. In this work, we apply a feature attribution technique to identify the relevant acoustic cues for a modern Conformer-based ASR system. By analyzing plosives, fricatives, and vowels, we assess how feature attributions align with their acoustic properties in the time and frequency domains, also essential for human speech perception. Our findings show that the ASR model relies on vowels’ full time spans, particularly their first two formants, with greater saliency in male speech. It also better captures the spectral characteristics of sibilant fricatives than non-sibilants and prioritizes the release phase in plosives, especially burst characteristics. These insights enhance the interpretability of ASR models and highlight areas for future research to uncover potential gaps in model robustness."
   ],
   "p1": 206,
   "pn": 210,
   "doi": "10.21437/Interspeech.2025-918",
   "url": "interspeech_2025/fucci25_interspeech.html"
  },
  "weizman25_interspeech": {
   "authors": [
    [
     "Avishai",
     "Weizman"
    ],
    [
     "Yehuda",
     "Ben-Shimol"
    ],
    [
     "Itshak",
     "Lapidot"
    ]
   ],
   "title": "ASVspoof2019 vs. ASVspoof5: Assessment and Comparison",
   "original": "920",
   "order": 930,
   "page_count": 5,
   "abstract": [
    "ASVspoof challenges are designed to advance the understanding of spoofing speech attacks and encourage the development of robust countermeasure systems. These challenges provide a standardized database for assessing and comparing spoofing-robust automatic speaker verification solutions. The ASVspoof5 challenge introduces a shift in database conditions compared to ASVspoof2019. While ASVspoof2019 has mismatched conditions only in spoofing attacks in the evaluation set, ASVspoof5 incorporates mismatches in both bonafide and spoofed speech statistics. This paper examines the impact of these mismatches, presenting qualitative and quantitative comparisons within and between the two databases. We show the increased difficulty for genuine and spoofed speech and demonstrate that in ASVspoof5, not only are the attacks more challenging, but the genuine speech also shifts toward spoofed speech compared to ASVspoof2019."
   ],
   "p1": 4568,
   "pn": 4572,
   "doi": "10.21437/Interspeech.2025-920",
   "url": "interspeech_2025/weizman25_interspeech.html"
  },
  "wu25f_interspeech": {
   "authors": [
    [
     "Haibin",
     "Wu"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Sefik",
     "Emre Eskimez"
    ],
    [
     "Jinyu",
     "Li"
    ]
   ],
   "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec",
   "original": "921",
   "order": 127,
   "page_count": 5,
   "abstract": [
    "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NACs are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers and linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources."
   ],
   "p1": 604,
   "pn": 608,
   "doi": "10.21437/Interspeech.2025-921",
   "url": "interspeech_2025/wu25f_interspeech.html"
  },
  "bressensdorf25_interspeech": {
   "authors": [
    [
     "Lilian von",
     "Bressensdorf"
    ],
    [
     "Pia",
     "Greca"
    ],
    [
     "Jonathan",
     "Harrington"
    ]
   ],
   "title": "Agent-based modelling, sound change, and metaphony in Southern Italian varieties of Italo-Romance.",
   "original": "924",
   "order": 601,
   "page_count": 5,
   "abstract": [
    "The study uses an agent-based computational model to test the hypothesis that contact between two dialects that are conservative and innovative as far as a sound change is concerned produces an asymmetric shift of the conservative speakers towards the innovative ones. The computational model, which is initialized with speech data from real speakers, is tested for the first time on a morpho-phonological sound change by which cues to inflectional morphology are being transferred from a suffix to a stem vowel in two Italo-Romance dialects of Southern Italy. The results based on quantifying the extent of diphthongization and the emergence of categorical contrasts marking morphological inflection provide some support for the proposed asymmetric shift in dialect contact. The analysis is more generally consistent with feedback models of sound change in which production is stochastically updated as a consequence of memorizing speech signals in speech perception."
   ],
   "p1": 2950,
   "pn": 2954,
   "doi": "10.21437/Interspeech.2025-924",
   "url": "interspeech_2025/bressensdorf25_interspeech.html"
  },
  "cr25_interspeech": {
   "authors": [
    [
     "Lekshmi",
     "C R"
    ],
    [
     "Rajeev",
     "Rajan"
    ]
   ],
   "title": "Focal Modulation Network: A Novel Solution for Polyphonic Music Instrument Recognition without Attention and Aggregation Strategy ",
   "original": "930",
   "order": 630,
   "page_count": 5,
   "abstract": [
    "This study introduces a novel framework for recognizing multiple predominant instruments in polyphonic music using Focal Modulation Networks (FMN). Unlike traditional methods that rely on sliding window analysis and aggregation strategy, FMN leverages a focal modulation module for hierarchical contextualization and gated aggregation, leading to improved accuracy and interpretability. Trained on single-instrument data, the model effectively recognizes multiple instruments in unseen polyphonic compositions. Extensive experiments show our proposed FMN outperforms existing transformer architectures, including Vision Transformers (ViT), Swin Transformers (SwinT), and Compact Convolutional Transformers (CCT), with improved accuracy and interpretability, achieving state-of-the-art results. The proposed scheme reports micro and macro F1 scores of 0.66 and 0.58, representing 10% and 16% improvements over the state-of-the-art Han&#x27;s model, respectively."
   ],
   "p1": 3095,
   "pn": 3099,
   "doi": "10.21437/Interspeech.2025-930",
   "url": "interspeech_2025/cr25_interspeech.html"
  },
  "young25_interspeech": {
   "authors": [
    [
     "Sophie",
     "Young"
    ],
    [
     "Fuxiang",
     "Tao"
    ],
    [
     "Bahman",
     "Mirheidari"
    ],
    [
     "Madhurananda",
     "Pahar"
    ],
    [
     "Markus",
     "Reuber"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Can Speech Accurately Detect Depression in Patients With Comorbid Dementia? An Approach for Mitigating Confounding Effects of Depression and Dementia",
   "original": "933",
   "order": 106,
   "page_count": 5,
   "abstract": [
    "Approximately 15.9% of people living with dementia experience co-occurring major depressive disorder. Both disorders cause similar early clinical symptoms in older people but treatment options and patient outcomes differ. While it is challenging, it is therefore critical for clinicians to be able to distinguish between them. We build on existing research into objective markers of depression in speech, testing their generalizability to a more complex population. On a novel, comorbidity dataset, we demonstrate that existing depression classification methods perform worse for participants with dementia than they do for those with no cognitive decline. We also propose a method of applying Wasserstein distance-based weight vectors to emphasize depression-related information which is robust against the effect of dementia. This improves performance for users with dementia, without requiring changes to the model architectures. Our best performing model achieves an overall F1-score of 81.0%."
   ],
   "p1": 499,
   "pn": 503,
   "doi": "10.21437/Interspeech.2025-933",
   "url": "interspeech_2025/young25_interspeech.html"
  },
  "wang25g_interspeech": {
   "authors": [
    [
     "Shiyao",
     "Wang"
    ],
    [
     "Jiaming",
     "Zhou"
    ],
    [
     "Shiwan",
     "Zhao"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition",
   "original": "934",
   "order": 671,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech recognition (DSR) enhances the accessibility of smart devices for dysarthric speakers with limited mobility. Previously, DSR research was constrained by the fact that existing datasets typically consisted of isolated words, command phrases, and a limited number of sentences spoken by a few individuals. This constrained research to command-interaction systems and speaker adaptation. The Speech Accessibility Project (SAP) changed this by releasing a large and diverse English dysarthric dataset, leading to the SAP Challenge to build speaker- and text-independent DSR systems. We enhanced the Whisper model&#x27;s performance on long dysarthric speech via a novel self-training method. This method increased training data and adapted the model to handle potentially incomplete speech segments encountered during inference. Our system achieved second place in both Word Error Rate and Semantic Score in the SAP Challenge."
   ],
   "p1": 3299,
   "pn": 3303,
   "doi": "10.21437/Interspeech.2025-934",
   "url": "interspeech_2025/wang25g_interspeech.html"
  },
  "vidal25_interspeech": {
   "authors": [
    [
     "Jazmín",
     "Vidal"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Juan Esteban",
     "Kamienkowski"
    ],
    [
     "Pablo",
     "Riera"
    ]
   ],
   "title": "Improving Automatic Speech Recognition for Children's Reading Assessment with Disfluency-aware Language Models",
   "original": "936",
   "order": 582,
   "page_count": 5,
   "abstract": [
    "We present an approach to improve automatic speech recognition (ASR) quality for children’s reading assessment in Spanish by incorporating disfluencies into the decoding process. Existing ASR-based assessment methods use hybrid or end-to-end models, with some methods relying on task-specific lattices or n-gram models for modeling potentially disfluent speech. In this work, we compare both families of approaches, in combination with in-domain fine-tuning of the acoustic model, on a dataset of Spanish-speaking children reading aloud. The task-specific n-gram model is learned on a synthetic dataset of disfluent text generated automatically from the reference text with our disfluentES toolkit. Results show that modeling disfluencies improves ASR performance as well as fluency assessment performance."
   ],
   "p1": 2855,
   "pn": 2859,
   "doi": "10.21437/Interspeech.2025-936",
   "url": "interspeech_2025/vidal25_interspeech.html"
  },
  "zhang25g_interspeech": {
   "authors": [
    [
     "You",
     "Zhang"
    ],
    [
     "Baotong",
     "Tian"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Zhiyao",
     "Duan"
    ]
   ],
   "title": "PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing  ",
   "original": "942",
   "order": 1092,
   "page_count": 5,
   "abstract": [
    "Neural speech editing enables seamless partial edits to speech utterances, allowing modifications to selected content while preserving the rest of the audio unchanged. This useful technique, however, also poses new risks of deepfakes. To encourage research on detecting such partially edited deepfake speech, we introduce PartialEdit, a deepfake speech dataset curated using advanced neural editing techniques. We explore both detection and localization tasks on PartialEdit. Our experiments reveal that models trained on the existing PartialSpoof dataset fail to detect partially edited speech generated by neural speech editing models. As recent speech editing models almost all involve neural audio codecs, we also provide insights into the artifacts the model learned on detecting these deepfakes. Further information about the PartialEdit dataset and audio samples can be found on the project page."
   ],
   "p1": 5353,
   "pn": 5357,
   "doi": "10.21437/Interspeech.2025-942",
   "url": "interspeech_2025/zhang25g_interspeech.html"
  },
  "he25_interspeech": {
   "authors": [
    [
     "Jiajun",
     "He"
    ],
    [
     "Naoki",
     "Sawada"
    ],
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models",
   "original": "943",
   "order": 526,
   "page_count": 5,
   "abstract": [
    "In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM’s prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios."
   ],
   "p1": 2575,
   "pn": 2579,
   "doi": "10.21437/Interspeech.2025-943",
   "url": "interspeech_2025/he25_interspeech.html"
  },
  "ashihara25_interspeech": {
   "authors": [
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Kohei",
     "Matsuura"
    ],
    [
     "Shota",
     "Horiguchi"
    ]
   ],
   "title": "Analysis of Semantic and Acoustic Token Variability Across Speech, Music, and Audio Domains",
   "original": "945",
   "order": 47,
   "page_count": 5,
   "abstract": [
    "Techniques for discrete audio representation, which convert an audio signal into a sequence of audio tokens using neural audio codecs or self-supervised speech models, have gained attention for offering the possibility of modeling audio with large language models (LM) efficiently. While these audio tokens have been studied in various domains (e.g., speech, music, and general sound), their encoding properties across domains remain unclear. This paper examines several audio token types to analyze cross-domain variations. Our major findings include that audio tokens exhibit consistent statistical structures and probabilistic predictability deduced from rank-frequency distribution and perplexity, regardless of the domain. However, the token usage pattern is somewhat domain-dependent. This result underpins the steady success of the versatile audio LM, while also suggesting that domain-aware LM could further optimize performance by better capturing domain-specific token usage distributions."
   ],
   "p1": 226,
   "pn": 230,
   "doi": "10.21437/Interspeech.2025-945",
   "url": "interspeech_2025/ashihara25_interspeech.html"
  },
  "bakkouche25_interspeech": {
   "authors": [
    [
     "Linda",
     "Bakkouche"
    ],
    [
     "Charles",
     "McGhee"
    ],
    [
     "Emily",
     "Lau"
    ],
    [
     "Stephanie",
     "Cooper"
    ],
    [
     "Xinbing",
     "Luo"
    ],
    [
     "Madeleine",
     "Rees"
    ],
    [
     "Kai",
     "Alter"
    ],
    [
     "Brechtje",
     "Post"
    ],
    [
     "Julia",
     "Schwarz"
    ]
   ],
   "title": "Finding the Human Voice in AI: Insights on the Perception of AI-Voice Clones from Naturalness and Similarity Ratings",
   "original": "947",
   "order": 449,
   "page_count": 5,
   "abstract": [
    "AI-generated voice clones are important tools in language learning, audiobooks, and assistive technology, but often struggle to replicate key prosodic features such as dynamic F0 variation. The impact of these differences on speech perception remain underexplored. To address this, we conducted two behavioural tasks, evaluating listeners’ ratings of naturalness and similarity for human speech, three AI voice clones (ElevenLabs, StyleTTS-2, XTTS-v2), and a 30% F0 variation condition. ElevenLabs was rated comparably to human speech, while StyleTTS-2 and XTTS-v2 received lower ratings. Reduced F0 variation also led to lower ratings, suggesting that prosody is key to perceived naturalness and similarity. Listener ratings were further influenced by speaker accent and sex, but not by AI tool experience. These findings suggest that prosodic features and speaker-specific characteristics could be drivers for the varying performance of AI-voice clones."
   ],
   "p1": 2190,
   "pn": 2194,
   "doi": "10.21437/Interspeech.2025-947",
   "url": "interspeech_2025/bakkouche25_interspeech.html"
  },
  "qi25_interspeech": {
   "authors": [
    [
     "Tianhua",
     "Qi"
    ],
    [
     "Shiyan",
     "Wang"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Tengfei",
     "Song"
    ],
    [
     "Hao",
     "Yang"
    ],
    [
     "Zhanglin",
     "Wu"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "PromptEVC: Controllable Emotional Voice Conversion with Natural Language Prompts",
   "original": "948",
   "order": 934,
   "page_count": 5,
   "abstract": [
    "Controllable emotional voice conversion (EVC) aims to manipulate emotional expressions to increase the diversity of synthesized speech. Existing methods typically rely on predefined labels, reference audios, or prespecified factor values, often overlooking individual differences in emotion perception and expression. In this paper, we introduce PromptEVC that utilizes natural language prompts for precise and flexible emotion control. To bridge text descriptions with emotional speech, we propose emotion descriptor and prompt mapper to generate fine-grained emotion embeddings, trained jointly with reference embeddings. To enhance naturalness, we present a prosody modeling and control pipeline that adjusts the rhythm based on linguistic content and emotional cues. Additionally, a speaker encoder is incorporated to preserve identity. Experimental results demonstrate that PromptEVC outperforms state-of-the-art controllable EVC methods in emotion conversion, intensity control, mixed emotion synthesis, and prosody manipulation."
   ],
   "p1": 4588,
   "pn": 4592,
   "doi": "10.21437/Interspeech.2025-948",
   "url": "interspeech_2025/qi25_interspeech.html"
  },
  "karpov25_interspeech": {
   "authors": [
    [
     "Nikolay",
     "Karpov"
    ],
    [
     "Sofia",
     "Kostandian"
    ],
    [
     "Nune",
     "Tadevosyan"
    ],
    [
     "Alexan",
     "Ayrapetyan"
    ],
    [
     "Andrei",
     "Andrusenko"
    ],
    [
     "Ara",
     "Yeroyan"
    ],
    [
     "Mher",
     "Yerznkanyan"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ]
   ],
   "title": "From Scarcity to Sufficiency: Speech Recognition Pipeline for Zero-resource Language",
   "original": "950",
   "order": 877,
   "page_count": 5,
   "abstract": [
    "The quality of Automatic Speech Recognition (ASR) systems largely depends on the availability of training data, which is predominantly accessible for either high-resource or low-resource languages. In contrast, languages such as Armenian face significant challenges due to the almost zero availability of public speech and text corpora. In this paper, we introduce a comprehensive framework that elevates data availability for a zero-resource language to a new level, thereby enabling the development of a fully operational online ASR model. Our approach involves data collection and processing through diverse resources, including audiobooks, paid crowdsourcing, and leveraging the volunteer platform to assemble a labeled dataset totaling 149 hours. This data made it possible to apply pseudo-labeling techniques on additional 145 hours of public audio data, achieving a new state-of-the-art Word Error Rate (WER) of 9.90% on Common Voice test. All datasets and ASR models are open-sourced."
   ],
   "p1": 4303,
   "pn": 4307,
   "doi": "10.21437/Interspeech.2025-950",
   "url": "interspeech_2025/karpov25_interspeech.html"
  },
  "bakkouche25b_interspeech": {
   "authors": [
    [
     "Linda",
     "Bakkouche"
    ],
    [
     "Brechtje",
     "Post"
    ]
   ],
   "title": "Influence of Proficiency and L2 Experience on Dynamic Spectral Cue Utilization in L2 Vowel Perception and Production",
   "original": "954",
   "order": 22,
   "page_count": 5,
   "abstract": [
    "The acquisition of English vowels as an L2 is complex, yet most studies focus on static measures, with little attention to dynamic spectral cues like Vowel-Inherent Spectral Change (VISC). It remains unclear how language experience and length of residence (LOR) in immersion-rich environments affect perception-production alignment. This study examines Polish learners’ perception and production of /e-æ/ (DRESS-TRAP) and /i-I/ (FLEECE-KIT). These contrasts are challenging due to phonetic similarity and category overlap as predicted by L2 models. Advanced learners showed greater perceptual accuracy and more consistent production, especially for /i-I/, while /e-æ/ remained difficult. With higher proficiency, learners exhibited greater formant movement (20-40% of vowel duration), but LOR and language experience were not significant predictors. These findings provide insight into phonetic similarity in theoretical models of L2 vowel acquisition."
   ],
   "p1": 101,
   "pn": 105,
   "doi": "10.21437/Interspeech.2025-954",
   "url": "interspeech_2025/bakkouche25b_interspeech.html"
  },
  "bataev25_interspeech": {
   "authors": [
    [
     "Vladimir",
     "Bataev"
    ],
    [
     "Andrei",
     "Andrusenko"
    ],
    [
     "Lilit",
     "Grigoryan"
    ],
    [
     "Aleksandr",
     "Laptev"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding",
   "original": "955",
   "order": 135,
   "page_count": 5,
   "abstract": [
    "Statistical n-gram language models are widely used for context-biasing tasks in Automatic Speech Recognition (ASR). However, existing implementations lack computational efficiency due to poor parallelization, making context-biasing less appealing for industrial use. This work rethinks data structures for statistical n-gram language models to enable fast and parallel operations for GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable greedy decoding for all major ASR model types - including transducers, attention encoder-decoder models, and CTC - with less than 7% computational overhead. The proposed approach can eliminate more than 50% of the accuracy gap between greedy and beam search for out-of-domain scenarios while avoiding significant slowdown caused by beam search. The implementation of the proposed NGPU-LM will be open-sourced."
   ],
   "p1": 644,
   "pn": 648,
   "doi": "10.21437/Interspeech.2025-955",
   "url": "interspeech_2025/bataev25_interspeech.html"
  },
  "charuau25_interspeech": {
   "authors": [
    [
     "Delphine",
     "Charuau"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Multimodal Dynamics of Hand Gestures and Pauses in Multiparty Interactions",
   "original": "959",
   "order": 617,
   "page_count": 5,
   "abstract": [
    "This study examines multimodal patterns linking hand gestures and pauses in multiparty interactions, distinguishing between within-speaker pauses and between-speaker pauses. Using the MULTISIMO corpus, which includes annotated audio-visual recordings of collaborative dialogues, we analysed, for each category of gesture, their distribution, pause duration, and the timing of gesture onset and offset relative to the pause. Results showed distinct gestural timing patterns involving within-speaker and between-speaker pauses. Self-adaptors were associated with longer pauses, possibly reflecting increased cognitive demands or speech planning. While syntactic position influenced pause duration, with shorter pauses at utterance endings, it did not impact gesture onset and offset. Additionally, most pauses containing gestures occurred within utterances, regardless of gesture type."
   ],
   "p1": 3030,
   "pn": 3034,
   "doi": "10.21437/Interspeech.2025-959",
   "url": "interspeech_2025/charuau25_interspeech.html"
  },
  "sun25e_interspeech": {
   "authors": [
    [
     "Jingyi",
     "Sun"
    ],
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Yaru",
     "Wu"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Corpus-Based Insights into Mandarin Neutral Tone: Effects of Tonal Context and Structural Patterns in Spontaneous Speech ",
   "original": "960",
   "order": 852,
   "page_count": 5,
   "abstract": [
    "This study examines the realization of Mandarin neutral tone (NT) in spontaneous conversational speech. Drawing on 8,550 phrase-medial NT tokens surrounded by citation tones, we investigate whether robust coarticulatory influences from adjacent tones persist in natural discourse and compare two subtypes of NT—one with a nasal offset and one without—thereby testing the combined effects of syllable structure and grammatical function. Our analysis confirms that adjacent tones exert a dominant influence on NT realization: ultrashort NT syllables largely continue the preceding tone’s contour, while normal- to long-duration syllables exhibit clearer transitions toward the subsequent tone. NT with a nasal offset maintains a higher pitch and greater F0 dynamics while remaining stable across durations, likely link to its stronger prosodic and grammatical cues. These findings highlight the interplay of tonal coarticulation, duration, and structural patterns in shaping NT variation."
   ],
   "p1": 4178,
   "pn": 4182,
   "doi": "10.21437/Interspeech.2025-960",
   "url": "interspeech_2025/sun25e_interspeech.html"
  },
  "mai25_interspeech": {
   "authors": [
    [
     "Long",
     "Mai"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning",
   "original": "962",
   "order": 662,
   "page_count": 5,
   "abstract": [
    "While Large Language Models (LLMs) have made significant strides in replicating human-like abilities, there are concerns about a reduction in the linguistic diversity of their outputs. This results in the homogenization of viewpoints and perspectives, as well as the underrepresentation of specific demographic groups. Although several fine-tuning and prompting techniques have been suggested to tackle the issue, they are often tailored to specific tasks or come with a substantial increase in computational cost and latency. This makes them challenging to apply to applications that demand very low latency, such as spoken chatbots or virtual assistants. We propose Possibility Exploration Fine-Tuning (PoExFT), a task-agnostic framework that enhances response diversity of LLMs without increasing latency. Given the same prompt, models fine-tuned with PoExFT can simultaneously generate multiple diverse responses. Experiments on dialogue and story generation tasks show that PoExFT significantly enhances the diversity of LLMs, as evidenced by lower similarity between candidate responses. As PoExFT emphasizes semantic diversity over lexical diversity, it can also notably reduce demographic bias in dialogue systems."
   ],
   "p1": 3254,
   "pn": 3258,
   "doi": "10.21437/Interspeech.2025-962",
   "url": "interspeech_2025/mai25_interspeech.html"
  },
  "azzouz25_interspeech": {
   "authors": [
    [
     "Sofiane",
     "Azzouz"
    ],
    [
     "Pierre-André",
     "Vuissoz"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Reconstruction of the Complete Vocal Tract Contour Through Acoustic to Articulatory Inversion Using Real-Time MRI Data",
   "original": "963",
   "order": 202,
   "page_count": 5,
   "abstract": [
    "Articulatory to acoustic inversion has often been limited to a small part of the vocal tract because the data are generally EMA (ElectroMagnetic Articulography) data requiring sensors to be glued to easily accessible articulators. The presented acoustic to articulation model focuses on the inversion of the entire vocal tract from the glottis, the complete tongue, the velum, to the lips. It relies on a realtime dynamic MRI database of more than 3 hours of speech. The data are the denoised speech signal and the automatically segmented articulator contours. Several bidirectional LSTM-based approaches have been used, either inverting each articulator individually or inverting all articulators simultaneously. To our knowledge, this is the first complete inversion of the vocal tract. The average RMSE precision on the test set is 1.65 mm to be compared with the pixel size which is 1.62 mm."
   ],
   "p1": 978,
   "pn": 982,
   "doi": "10.21437/Interspeech.2025-963",
   "url": "interspeech_2025/azzouz25_interspeech.html"
  },
  "kommagouni25_interspeech": {
   "authors": [
    [
     "Priyanka",
     "Kommagouni"
    ],
    [
     "Pragya",
     "Khanna"
    ],
    [
     "Vamshiraghusimha",
     "Narasinga"
    ],
    [
     "Anirudh",
     "Bocha"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Towards Classification of Typical and Atypical Disfluencies: A Self Supervised Representation Approach ",
   "original": "964",
   "order": 1080,
   "page_count": 5,
   "abstract": [
    "This paper investigates the nuanced distinctions between typical and atypical speech disfluencies, focusing on features captured by self-supervised models. Typical disfluencies are natural, non-pathological irregularities in speech, while atypical disfluencies are linked to speech disorders like stuttering, characterized by more frequent and severe disruptions. Despite progress in automatic disfluency detection, limited research addresses the direct classification of these two types. This study leverages intermediate representations from four pretrained models Wav2Vec2.0, HuBERT, WavLM, and TERA, to analyze and classify typical and stuttered disfluencies. The experiments utilize two novel Indian English datasets, IIITH-IEDE and IIITH-TISA, enabling a comprehensive analysis of disfluency patterns in a linguistically diverse context. Classification experiments with support vector machines (SVM) and convolutional neural networks (CNN) reveal that features from HuBERT’s 5th layer—balancing low-level acoustic and high-level semantic information—achieve a peak F1 score of 0.97. These findings highlight the importance of intermediate layer representations of self-supervised models in distinguishing nuanced speech variations and contribute to robust and interpretable systems for speech disfluency classification."
   ],
   "p1": 5293,
   "pn": 5297,
   "doi": "10.21437/Interspeech.2025-964",
   "url": "interspeech_2025/kommagouni25_interspeech.html"
  },
  "popescu25_interspeech": {
   "authors": [
    [
     "Anisia",
     "Popescu"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Marc",
     "Evrard"
    ],
    [
     "Ioana",
     "Vasilescu"
    ]
   ],
   "title": "Tracking /r/ Deletion: Forced Alignment of Pronunciation Variants and Sociophonetic Insights into Post-Obstruent Final /r/ in French",
   "original": "967",
   "order": 600,
   "page_count": 5,
   "abstract": [
    "This paper investigates post-obstruent final /r/ deletion in French (e.g., livre [liv(K)], votre [vot(K)]) using two large corpora and phoneme-level forced alignment to analyze both pronounced and deleted /r/ variants. The study examines factors influencing /r/ deletion, including phonetic context, speaker profession, gender, age, and speaking style. Results show that right-phone context, age, and speaking style significantly affect /r/ deletion, with specific patterns also observed for the preceding phone and speaker profession. Gender has no effect. With over 390 speakers producing 14,167 individual /r/ tokens, the study provides robust insights into phonetic and social factors shaping final post-obstruent /r/ deletion. The combination of speech technology, NLP methods, and linguistic analysis of large-scale naturalistic data improves our understanding of this phenomenon, challenging assumptions about gender’s relevance in this phenomenon."
   ],
   "p1": 2945,
   "pn": 2949,
   "doi": "10.21437/Interspeech.2025-967",
   "url": "interspeech_2025/popescu25_interspeech.html"
  },
  "xiang25b_interspeech": {
   "authors": [
    [
     "Yunzhuo",
     "Xiang"
    ],
    [
     "Jingyi",
     "Sun"
    ]
   ],
   "title": "Modeling Formant Dynamics in Mandarin /ai/: Effects of Speech Style and Speech Rate",
   "original": "968",
   "order": 80,
   "page_count": 5,
   "abstract": [
    "This study examines the relationship between speech clarity and speech rate by modeling F1/F2 variation in the Mandarin diphthong /ai/ across different durations and speech styles. The corpus includes 20 hours of conversational and 6 hours of read speech. Vowel durations were manually verified, and formant values were extracted using auto-correlation. Generalized additive mixed models (GAMMs) were used to examine the interaction between duration and formants. Results show that both read and slow speeches exhibit higher F1 onset and F2 offset for /ai/. However, read speech has larger formant frequency range, and even at the shortest (20 ms) or longest (200 ms) durations, diphthongs in read speech remained more clearly articulated than those in conversational speech of the same length. This suggests that speech clarity and speech rate may be distinct dimensions rather than interchangeable factors."
   ],
   "p1": 369,
   "pn": 373,
   "doi": "10.21437/Interspeech.2025-968",
   "url": "interspeech_2025/xiang25b_interspeech.html"
  },
  "navon25_interspeech": {
   "authors": [
    [
     "Aviv",
     "Navon"
    ],
    [
     "Aviv",
     "Shamsian"
    ],
    [
     "Yael",
     "Segal-Feldman"
    ],
    [
     "Neta",
     "Glazer"
    ],
    [
     "Gil",
     "Hetz"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "FlowTSE: Target Speaker Extraction with Flow Matching",
   "original": "970",
   "order": 604,
   "page_count": 5,
   "abstract": [
    "Target speaker extraction (TSE) aims to isolate a specific speaker&#x27;s speech from a mixture using speaker enrollment as a reference. While most existing approaches are discriminative, recent generative methods for TSE achieve strong results. However, generative methods for TSE remain underexplored, with most existing approaches relying on complex pipelines and pretrained components, leading to computational overhead. In this work, we present FlowTSE, a simple yet effective TSE approach based on conditional flow matching. Our model receives an enrollment audio sample and a mixed speech signal, both represented as mel-spectrograms, with the objective of extracting the target speaker&#x27;s clean speech. Furthermore, for tasks where phase reconstruction is crucial, we propose a novel vocoder conditioned on the complex STFT of the mixed signal, enabling improved phase estimation. Experimental results on standard TSE benchmarks show that FlowTSE matches or outperforms strong baselines."
   ],
   "p1": 2965,
   "pn": 2969,
   "doi": "10.21437/Interspeech.2025-970",
   "url": "interspeech_2025/navon25_interspeech.html"
  },
  "sosawelford25_interspeech": {
   "authors": [
    [
     "Alejandro",
     "Sosa Welford"
    ],
    [
     "Leonardo",
     "Pepino"
    ]
   ],
   "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish",
   "original": "973",
   "order": 979,
   "page_count": 5,
   "abstract": [
    "This work addresses the development of a database for the automatic assessment of text‐to‐speech (TTS) systems in Spanish, aiming to improve the accuracy of naturalness prediction models. The dataset consists of 4,326 audio samples from 52 different TTS systems and human voices and is, up to our knowledge, the first of its kind in Spanish. To label the audios, a subjective test was designed based on the ITU‐T Rec. P.807 standard and completed by 92 participants. Furthermore, the utility of the collected dataset was validated by training automatic naturalness prediction systems. We explored two approaches: fine-tuning an existing model originally trained for English, and training small downstream networks on top of frozen self-supervised speech models. Our models achieve a mean absolute error of 0.8 on a five-point MOS scale. Further analysis demonstrates the quality and diversity of the developed dataset, and its potential to advance TTS research in Spanish."
   ],
   "p1": 4813,
   "pn": 4817,
   "doi": "10.21437/Interspeech.2025-973",
   "url": "interspeech_2025/sosawelford25_interspeech.html"
  },
  "choi25b_interspeech": {
   "authors": [
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Masao",
     "Someki"
    ],
    [
     "Emma",
     "Strubell"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "On-device Streaming Discrete Speech Units",
   "original": "975",
   "order": 901,
   "page_count": 5,
   "abstract": [
    "Discrete speech units (DSUs) are derived from clustering the features of self-supervised speech models (S3Ms). DSUs offer significant advantages for on-device streaming speech applications due to their rich phonetic information, high transmission efficiency, and seamless integration with large language models. However, conventional DSU-based approaches are impractical as they require full-length speech input and computationally expensive S3Ms. In this work, we reduce both the attention window and the model size while preserving the effectiveness of DSUs. Our results demonstrate that we can reduce floating-point operations (FLOPs) by 50% with only a relative increase of 6.5% in character error rate (CER) on the ML-SUPERB 1h dataset. These findings highlight the potential of DSUs for real-time speech processing in resource-constrained environments."
   ],
   "p1": 4423,
   "pn": 4427,
   "doi": "10.21437/Interspeech.2025-975",
   "url": "interspeech_2025/choi25b_interspeech.html"
  },
  "sage25_interspeech": {
   "authors": [
    [
     "Agata",
     "Sage"
    ],
    [
     "Zuzanna",
     "Miodońska"
    ],
    [
     "Michał",
     "Kręcichwost"
    ],
    [
     "Ewa",
     "Kwaśniok"
    ],
    [
     "Paweł",
     "Badura"
    ]
   ],
   "title": "Visual features of the oral region in Polish sibilants produced by children with various sibilance patterns",
   "original": "979",
   "order": 414,
   "page_count": 5,
   "abstract": [
    "This paper analyzes the 2D and 3D visual features of Polish sibilants /s/ and /ʂ/ articulated by 189 children aged 4;11-8;0. Our goal was to (1) analyze differences in shape image features of delineated articulators (lips, tongue, and mouth - lips and space between them) in selected sibilants and (2) investigate a relationship between visual parameters and sibilance pattern (the character of a gap between teeth that allows the articulation of sibilant sounds). We employed 2D and 3D shape image features and linear mixed-effect models. The results proved significant differences between sibilants in 2D and 3D visual features of lips and mouth. Different sibilance patterns considered in the study also significantly impacted visual parameters distribution. Our findings indicate the potential of video data in computer-aided speech diagnosis."
   ],
   "p1": 2033,
   "pn": 2037,
   "doi": "10.21437/Interspeech.2025-979",
   "url": "interspeech_2025/sage25_interspeech.html"
  },
  "hsieh25b_interspeech": {
   "authors": [
    [
     "Wen-Wei",
     "Hsieh"
    ],
    [
     "Hao-Wei",
     "Chi"
    ],
    [
     "Kuan-Chen",
     "Wang"
    ],
    [
     "Ping-Cheng",
     "Yeh"
    ],
    [
     "Te-hsin",
     "Liu"
    ],
    [
     "Chen-Yu",
     "Chiang"
    ]
   ],
   "title": "OMPAL: Bridging Speech and Learning with an Open-Source Mandarin Pronunciation Assessment Corpus for Global Learners",
   "original": "983",
   "order": 494,
   "page_count": 5,
   "abstract": [
    "This paper introduces OMPAL, a new open-source Mandarin corpus specifically designed for non-native pronunciation assessment. This corpus comprises 1,768 Mandarin utterances from French L1 speakers learning Mandarin, each meticulously annotated by four experts with professional Mandarin teaching experience at both the word and sentence levels. We also provide a manual scoring system to assist researchers in constructing related corpora. Furthermore, a baseline model for pronunciation assessment, which is publicly accessible, is provided alongside our corpus. The OMPAL corpus, available for commercial and non-commercial use, is designed to support and enhance speech research across various applications. We believe that OMPAL will be a valuable resource for the speech research community."
   ],
   "p1": 2415,
   "pn": 2419,
   "doi": "10.21437/Interspeech.2025-983",
   "url": "interspeech_2025/hsieh25b_interspeech.html"
  },
  "leschanowsky25_interspeech": {
   "authors": [
    [
     "Anna",
     "Leschanowsky"
    ],
    [
     "Kishor",
     "Kayyar Lakshminarayana"
    ],
    [
     "Anjana",
     "Rajasekhar"
    ],
    [
     "Lyonel",
     "Behringer"
    ],
    [
     "Ibrahim",
     "Kilinc"
    ],
    [
     "Guillaume",
     "Fuchs"
    ],
    [
     "Emanuël A. P.",
     "Habets"
    ]
   ],
   "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
   "original": "984",
   "order": 1119,
   "page_count": 5,
   "abstract": [
    "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI – not WER – significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations."
   ],
   "p1": 5488,
   "pn": 5492,
   "doi": "10.21437/Interspeech.2025-984",
   "url": "interspeech_2025/leschanowsky25_interspeech.html"
  },
  "rustagi25_interspeech": {
   "authors": [
    [
     "Arnav",
     "Rustagi"
    ],
    [
     "Satvik",
     "Bajpai"
    ],
    [
     "Nimrat",
     "Kaur"
    ],
    [
     "Siddharth",
     "Siddharth"
    ]
   ],
   "title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi",
   "original": "986",
   "order": 442,
   "page_count": 5,
   "abstract": [
    "Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani---a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi&#x27;s highly phonetic orthography to analyze mispronounced speech and provide targeted feedback."
   ],
   "p1": 2155,
   "pn": 2159,
   "doi": "10.21437/Interspeech.2025-986",
   "url": "interspeech_2025/rustagi25_interspeech.html"
  },
  "aggarwal25_interspeech": {
   "authors": [
    [
     "Pranjal",
     "Aggarwal"
    ],
    [
     "Ghritachi",
     "Mahajani"
    ],
    [
     "Pavan Kumar",
     "Malasani"
    ],
    [
     "Vaibhav",
     "Jamadagni"
    ],
    [
     "Caroline J.",
     "Wendt"
    ],
    [
     "Ehsanul Haque",
     "Nirjhar"
    ],
    [
     "Theodora",
     "Chaspari"
    ]
   ],
   "title": "Investigating the Reasoning Abilities of Large Language Models for Understanding Spoken Language in Interpersonal Interactions",
   "original": "988",
   "order": 920,
   "page_count": 5,
   "abstract": [
    "This study evaluates large language models’ (LLMs) reasoning capabilities in spoken language understanding (SLU) during interpersonal interactions. We incorporated several factors into LLM prompts: instructing via examples (IE), integrating domain knowledge (DK), and including context (IC). Experiments with Gemini-1.5-pro, GPT-3.5-turbo, and GPT-4o were conducted on an SLU task that classifies the degree of explanation (i.e., under-explained, succinct, comprehensive, over-explained) in job interview responses—an important step toward developing automatic interview training systems. Results demonstrate the feasibility of few-shot (1- to 4-shot) learning, with ablation studies confirming that modifications to prompts, especially when combining IE, DK, and IC, lead to further performance improvements."
   ],
   "p1": 4518,
   "pn": 4522,
   "doi": "10.21437/Interspeech.2025-988",
   "url": "interspeech_2025/aggarwal25_interspeech.html"
  },
  "langman25_interspeech": {
   "authors": [
    [
     "Ryan",
     "Langman"
    ],
    [
     "Xuesong",
     "Yang"
    ],
    [
     "Paarth",
     "Neekhara"
    ],
    [
     "Shehzeen",
     "Hussain"
    ],
    [
     "Edresson",
     "Casanova"
    ],
    [
     "Evelina",
     "Bakhturina"
    ],
    [
     "Jason",
     "Li"
    ]
   ],
   "title": "HiFiTTS-2: A Large-Scale High Bandwidth Speech Dataset",
   "original": "989",
   "order": 972,
   "page_count": 5,
   "abstract": [
    "This paper introduces HiFiTTS-2, a large-scale speech dataset designed for high-bandwidth speech synthesis. The dataset is derived from LibriVox audiobooks, and contains approximately 36.7k hours of English speech for 22.05 kHz training, and 31.7k hours for 44.1 kHz training. We present our data processing pipeline, including bandwidth estimation, segmentation, text preprocessing, and multi-speaker detection. The dataset is accompanied by detailed utterance and audiobook metadata generated by our pipeline, enabling researchers to apply data quality filters to adapt the dataset to various use cases. Experimental results demonstrate that our data pipeline and resulting dataset can facilitate the training of high-quality, zero-shot text-to-speech (TTS) models at high bandwidths."
   ],
   "p1": 4778,
   "pn": 4782,
   "doi": "10.21437/Interspeech.2025-989",
   "url": "interspeech_2025/langman25_interspeech.html"
  },
  "bashir25_interspeech": {
   "authors": [
    [
     "Aymen",
     "Bashir"
    ],
    [
     "Haolan",
     "Wang"
    ],
    [
     "Amin",
     "Edraki"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Jesper",
     "Jensen"
    ]
   ],
   "title": "Intelligibility Prediction for Time-Modified Speech Signals Using Spectro-Temporal Modulation Features",
   "original": "991",
   "order": 1117,
   "page_count": 5,
   "abstract": [
    "Reference-based speech intelligibility prediction algorithms (RB-SIPAs) are limited to speech degradations that maintain the time alignment between the clean and degraded speech signals. We address this limitation by augmenting the existing RB-SIPAs framework with time alignment. To achieve robust time alignment at low signal-to-noise ratios, we propose using spectro-temporal modulations (STM) of the speech signals for dynamic time warping (DTW). Our experiments demonstrate that in DTW, the use of specific STM components as features for clean and time-modified degraded speech achieves the baseline time alignment obtained using clean speech and noise-free time-modified speech. Moreover, we propose two methods to incorporate time alignment into existing RB-SIPAs. Using these methods, we compare the output scores of the RB-SIPA with listening test scores and show better correlation results using the STM features as compared to MFCCs."
   ],
   "p1": 5478,
   "pn": 5482,
   "doi": "10.21437/Interspeech.2025-991",
   "url": "interspeech_2025/bashir25_interspeech.html"
  },
  "byun25_interspeech": {
   "authors": [
    [
     "Kyungguen",
     "Byun"
    ],
    [
     "Jason",
     "Filos"
    ],
    [
     "Erik",
     "Visser"
    ],
    [
     "Sunkuk",
     "Moon"
    ]
   ],
   "title": "Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice Conversion Framework",
   "original": "998",
   "order": 830,
   "page_count": 5,
   "abstract": [
    "We propose a speech enhancement system that combines speaker-agnostic speech restoration with voice conversion (VC) to obtain a studio-level quality speech signal. While voice conversion models are typically used to change speaker characteristics, they can also serve as a means of speech restoration when the target speaker is the same as the source speaker. However, since VC models are vulnerable to noisy conditions, we have included a generative speech restoration (GSR) model at the front end of our proposed system. The GSR model performs noise suppression and restores speech damage incurred during that process without knowledge about the target speaker. The VC stage then uses guidance from clean speaker embeddings to further restore the output speech. By employing this two-stage approach, we have achieved speech quality objective metric scores comparable to state-of-the-art (SOTA) methods across multiple datasets."
   ],
   "p1": 4068,
   "pn": 4072,
   "doi": "10.21437/Interspeech.2025-998",
   "url": "interspeech_2025/byun25_interspeech.html"
  },
  "tabatabaee25_interspeech": {
   "authors": [
    [
     "Saba",
     "Tabatabaee"
    ],
    [
     "Jing",
     "Liu"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "FT-Boosted SV: Towards Noise Robust Speaker Verification for English Speaking Classroom Environments ",
   "original": "1002",
   "order": 574,
   "page_count": 5,
   "abstract": [
    "Creating Speaker Verification (SV) systems for classroom settings that are robust to classroom noises such as babble noise is crucial for the development of AI tools that assist educational environments. In this work, we study the efficacy of finetuning with augmented children datasets to adapt the x-vector and ECAPA-TDNN to classroom environments. We demonstrate that finetuning with augmented children&#x27;s datasets is powerful in that regard and reduces the Equal Error Rate (EER) of x-vector and ECAPA-TDNN models for both classroom datasets and children speech datasets. Notably, this method reduces EER of the ECAPA-TDNN model on average by half (a 5% improvement) for classrooms in the MPT dataset compared to the ECAPA-TDNN baseline model. The x-vector model shows an 8% average improvement for classrooms in the NCTE dataset compared to its baseline."
   ],
   "p1": 2815,
   "pn": 2819,
   "doi": "10.21437/Interspeech.2025-1002",
   "url": "interspeech_2025/tabatabaee25_interspeech.html"
  },
  "zhu25b_interspeech": {
   "authors": [
    [
     "Pai",
     "Zhu"
    ],
    [
     "Quan",
     "Wang"
    ],
    [
     "Dhruuv",
     "Agarwal"
    ],
    [
     "Kurt",
     "Partridge"
    ]
   ],
   "title": "LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable Data for Custom Keyword Spotting",
   "original": "1005",
   "order": 546,
   "page_count": 5,
   "abstract": [
    "Custom keyword spotting (KWS) allows detecting user-defined spoken keywords from streaming audio. This is achieved by comparing the embeddings from voice enrollments and input audio. State-of-the-art custom KWS models are typically trained contrastively using  utterances whose keywords are randomly sampled from training dataset. These KWS models often struggle with confusing keywords, such as ``blue&#x27;&#x27; versus ``glue&#x27;&#x27;. This paper introduces an effective way to augment the training with confusable utterances where keywords are generated and grouped from large language models (LLMs), and speech signals are synthesized with diverse speaking styles from text-to-speech (TTS) engines. To better measure user experience on confusable KWS, we define a new northstar metric using the average area under DET curve from confusable groups (c-AUC). Featuring high scalability and zero labor cost, the proposed method improves AUC by 3.7% and c-AUC by 11.3% on the Speech Commands testing set."
   ],
   "p1": 2675,
   "pn": 2679,
   "doi": "10.21437/Interspeech.2025-1005",
   "url": "interspeech_2025/zhu25b_interspeech.html"
  },
  "gong25b_interspeech": {
   "authors": [
    [
     "Ziwei",
     "Gong"
    ],
    [
     "Pengyuan",
     "Shi"
    ],
    [
     "Kaan",
     "Donbekci"
    ],
    [
     "Lin",
     "Ai"
    ],
    [
     "Run",
     "Chen"
    ],
    [
     "David",
     "Sasu"
    ],
    [
     "Zehui",
     "Wu"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Learning More with Less: Self-Supervised Approaches forLow-Resource Speech Emotion Recognition",
   "original": "1006",
   "order": 32,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) has seen significant progress with deep learning, yet remains challenging for Low-Resource Languages (LRLs) due to the scarcity of annotated data. In this work, we explore unsupervised learning to improve SER in low-resource settings. Specifically, we investigate contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches to enhance cross-lingual generalization. Our methods achieve notable F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating their effectiveness in LRLs. Additionally, we analyze model behavior to provide insights on key factors influencing performance across languages, and also highlighting challenges in low-resource SER. This work provides a foundation for developing more inclusive, explainable, and robust emotion recognition systems for underrepresented languages."
   ],
   "p1": 151,
   "pn": 155,
   "doi": "10.21437/Interspeech.2025-1006",
   "url": "interspeech_2025/gong25b_interspeech.html"
  },
  "wen25b_interspeech": {
   "authors": [
    [
     "Chuan",
     "Wen"
    ],
    [
     "Sarah",
     "Verhulst"
    ]
   ],
   "title": "Individualized speech enhancement for hearing-impaired listeners",
   "original": "1009",
   "order": 785,
   "page_count": 5,
   "abstract": [
    "Despite significant progress in speech enhancement (SE), improving speech intelligibility and perceptual quality in noisy environments for hearing-impaired individuals remains challenging. This paper presents an individualized speech-enhancement (ISE) framework that integrates noise reduction (NR) and sound amplification for hearing loss compensation within a unified system. Our fully differentiable, closed-loop design incorporates biophysically realistic auditory models. The framework features two pathways: one simulating the auditory response of a normal-hearing (NH) system to denoised speech and the other modeling a hearing-impaired (HI) system’s response to noisy speech. The ISE model is trained by minimizing the difference between NH and HI auditory responses. Experimental results show that the ISE model enhances speech intelligibility and perceptual quality in noisy conditions for HI listeners, offering a promising foundation for advancing personalized noise reduction strategies."
   ],
   "p1": 3843,
   "pn": 3847,
   "doi": "10.21437/Interspeech.2025-1009",
   "url": "interspeech_2025/wen25b_interspeech.html"
  },
  "shams25_interspeech": {
   "authors": [
    [
     "Siavash",
     "Shams"
    ],
    [
     "Richard",
     "Antonello"
    ],
    [
     "Gavin",
     "Mischler"
    ],
    [
     "Stephan",
     "Bickel"
    ],
    [
     "Ashesh",
     "Mehta"
    ],
    [
     "Nima",
     "Mesgarani"
    ]
   ],
   "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG",
   "original": "1010",
   "order": 595,
   "page_count": 5,
   "abstract": [
    "Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data,  outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies."
   ],
   "p1": 2920,
   "pn": 2924,
   "doi": "10.21437/Interspeech.2025-1010",
   "url": "interspeech_2025/shams25_interspeech.html"
  },
  "masson25_interspeech": {
   "authors": [
    [
     "Margot",
     "Masson"
    ],
    [
     "Isabelle",
     "Ferrané"
    ],
    [
     "Julie",
     "Mauclair"
    ]
   ],
   "title": "Identification of Pathological Pronunciation Profiles in ASR Transcription Errors",
   "original": "1011",
   "order": 380,
   "page_count": 5,
   "abstract": [
    "Despite recent developments, ASR systems still struggle to handle atypical speech. This difference in performance has been leveraged to use ASR to automatically assess speech quality in the context of speech disorders. In the first part of this paper, we confirm the correlation between ASR sensitivity and speech quality. At the same time, ASR systems remain black-box and difficult to interpret. Therefore, we explore ASR performance for Head and Neck Cancer and Parkinson&#x27;s disease speech disorders in the second part of this paper. We hypothesise that ASR errors are representative of pronunciation patterns caused by these disorders. We build pronunciation profiles from ASR transcription errors for each category and assess the representativeness of these profiles by synthesising speech variants including the variations identified in the profiles. Analysis of these variants shows that ASR transcription errors are characteristic of pronunciation patterns caused by speech disorders."
   ],
   "p1": 1863,
   "pn": 1867,
   "doi": "10.21437/Interspeech.2025-1011",
   "url": "interspeech_2025/masson25_interspeech.html"
  },
  "parikh25b_interspeech": {
   "authors": [
    [
     "Aditya Kamlesh",
     "Parikh"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Evaluating Logit-Based GOP Scores for Mispronunciation Detection",
   "original": "1012",
   "order": 492,
   "page_count": 5,
   "abstract": [
    "Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment."
   ],
   "p1": 2405,
   "pn": 2409,
   "doi": "10.21437/Interspeech.2025-1012",
   "url": "interspeech_2025/parikh25b_interspeech.html"
  },
  "chen25h_interspeech": {
   "authors": [
    [
     "William",
     "Chen"
    ],
    [
     "Chutong",
     "Meng"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Martijn",
     "Bartelds"
    ],
    [
     "Shih-Heng",
     "Wang"
    ],
    [
     "Hsiu-Hsuan",
     "Wang"
    ],
    [
     "Rafael",
     "Mosquera"
    ],
    [
     "Sara",
     "Hincapie"
    ],
    [
     "Dan",
     "Jurafsky"
    ],
    [
     "Antonis",
     "Anastasopoulos"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties",
   "original": "1013",
   "order": 426,
   "page_count": 5,
   "abstract": [
    "Recent improvements in multilingual ASR have not been equally distributed across languages and language varieties. To advance state-of-the-art (SOTA) ASR models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a new test suite that consists of data from 200+ languages, accents, and dialects to evaluate SOTA multilingual speech models. The challenge also introduces an online evaluation server based on DynaBench, allowing for flexibility in model design and architecture for participants. The challenge received 5 submissions from 3 teams, all of which outperformed our baselines. The best-performing submission achieved an absolute improvement in LID accuracy of 23% and a reduction in CER of 18% when compared to the best baseline on a general multilingual test set. On accented and dialectal data, the best submission obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance of community challenges in making speech technologies more inclusive."
   ],
   "p1": 2093,
   "pn": 2097,
   "doi": "10.21437/Interspeech.2025-1013",
   "url": "interspeech_2025/chen25h_interspeech.html"
  },
  "hsieh25c_interspeech": {
   "authors": [
    [
     "Pei-Chin",
     "Hsieh"
    ],
    [
     "Yih-Liang",
     "Shen"
    ],
    [
     "Ngoc-Son",
     "Tran"
    ],
    [
     "Tai-Shih",
     "Chi"
    ]
   ],
   "title": "Tonality-Based Accompaniment-Guided Automatic Singing Evaluation",
   "original": "1015",
   "order": 628,
   "page_count": 5,
   "abstract": [
    "Automatic singing evaluation is highly desirable in industrial performance evaluation scenarios, including education and entertainment, given the high cost of human judges. Current singing evaluation systems assess a singer&#x27;s performance by comparing reference vocals to the singing track. Unfortunately, reference vocals are not always available. Moreover, the similarity measure may not provide a reliable evaluation due to the inherently variable nature of singing performances. This paper proposes a tonality-based method for song-independent automatic singing evaluation. Experimental results show the proposed method outperforms other non-intrusive evaluation algorithms."
   ],
   "p1": 3085,
   "pn": 3089,
   "doi": "10.21437/Interspeech.2025-1015",
   "url": "interspeech_2025/hsieh25c_interspeech.html"
  },
  "fischbach25_interspeech": {
   "authors": [
    [
     "Lea",
     "Fischbach"
    ],
    [
     "Akbar",
     "Karimi"
    ],
    [
     "Caroline",
     "Kleen"
    ],
    [
     "Alfred",
     "Lameli"
    ],
    [
     "Lucie",
     "Flek"
    ]
   ],
   "title": "Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion",
   "original": "1017",
   "order": 567,
   "page_count": 5,
   "abstract": [
    "Deep learning models for dialect identification are often limited by the scarcity of dialectal data. To address this challenge, we propose to use Retrieval-based Voice Conversion (RVC) as an effective data augmentation method for a low-resource German dialect classification task. By converting audio samples to a uniform target speaker, RVC minimizes speaker-related variability, enabling models to focus on dialect-specific linguistic and phonetic features. Our experiments demonstrate that RVC enhances classification performance when utilized as a standalone augmentation method. Furthermore, combining RVC with other augmentation methods such as frequency masking and segment removal leads to additional performance gains, highlighting its potential for improving dialect classification in low-resource scenarios."
   ],
   "p1": 2780,
   "pn": 2784,
   "doi": "10.21437/Interspeech.2025-1017",
   "url": "interspeech_2025/fischbach25_interspeech.html"
  },
  "balajishankar25_interspeech": {
   "authors": [
    [
     "Natarajan",
     "Balaji Shankar"
    ],
    [
     "Zilai",
     "Wang"
    ],
    [
     "Kaiyuan",
     "Zhang"
    ],
    [
     "Mohan",
     "Shi"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "CHSER: A Dataset and Case Study on Generative Speech Error Correction for Child ASR",
   "original": "1019",
   "order": 590,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems struggle with child speech due to its distinct acoustic and linguistic variability and limited availability of child speech datasets, leading to high transcription error rates. While ASR error correction (AEC) methods have improved adult speech transcription, their effectiveness on child speech remains largely unexplored. To address this, we introduce CHSER, a Generative Speech Error Correction (GenSEC) dataset for child speech, comprising 200K hypothesis-transcription pairs spanning diverse age groups and speaking styles. Results demonstrate that fine-tuning on the CHSER dataset achieves up to a 28.5% relative WER reduction in a zero-shot setting and a 13.3% reduction when applied to fine-tuned ASR systems. Additionally, our error analysis reveals that while GenSEC improves substitution and deletion errors, it struggles with insertions and child-specific disfluencies. These findings highlight the potential of GenSEC for improving child ASR."
   ],
   "p1": 2895,
   "pn": 2899,
   "doi": "10.21437/Interspeech.2025-1019",
   "url": "interspeech_2025/balajishankar25_interspeech.html"
  },
  "portes25_interspeech": {
   "authors": [
    [
     "David",
     "Porteš"
    ],
    [
     "Aleš",
     "Horák"
    ]
   ],
   "title": "Learning Optimal Prosody Embedding Codebook based on F0 and Energy",
   "original": "1020",
   "order": 962,
   "page_count": 5,
   "abstract": [
    "Both the Fundamental frequency (F0) and Energy are prominent features of prosody. Together, they have been used across a wide variety of speech-processing tasks. However, there is a lack of freely available pre-trained vector representations of these features. Therefore, in this paper, we provide the research community with high-quality joint embeddings of the frame-level F0 and Energy features, using the VQ-VAE architecture. By converting the F0 and Energy into a single stream of vector embeddings, we make it possible to seamlessly use prosody in modern architectures, such as multimodal LLMs. In order to ensure maximum embedding quality, we conduct a large-scale hyperparameter search, totaling over 150 experiments on the LibriTTS dataset. We outperform previous works on F0 embeddings, reaching FFE error below 1 percent, while simultaneously embedding the additional feature of Energy. We publish our best-performing models on the HuggingFace website."
   ],
   "p1": 4728,
   "pn": 4732,
   "doi": "10.21437/Interspeech.2025-1020",
   "url": "interspeech_2025/portes25_interspeech.html"
  },
  "chang25b_interspeech": {
   "authors": [
    [
     "Andrew",
     "Chang"
    ],
    [
     "Yike",
     "Li"
    ],
    [
     "Iran R.",
     "Roman"
    ],
    [
     "David",
     "Poeppel"
    ]
   ],
   "title": "Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds",
   "original": "1021",
   "order": 45,
   "page_count": 5,
   "abstract": [
    "Audio DNNs have demonstrated impressive performance on various machine listening tasks; however, most of their representations are computationally costly and uninterpretable, leaving room for optimization. Here, we propose a novel approach centered on spectrotemporal modulation (STM) features, a signal processing method that mimics the neurophysiological representation in the human auditory cortex. The classification performance of our STM-based model, without any pretraining, is comparable to that of pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds, which are essential categories for both human cognition and machine perception. These results show that STM is an efficient and interpretable feature representation for audio classification, advancing the development of machine listening and unlocking exciting new possibilities for basic understanding of speech and auditory sciences, as well as developing audio BCI and cognitive computing."
   ],
   "p1": 216,
   "pn": 220,
   "doi": "10.21437/Interspeech.2025-1021",
   "url": "interspeech_2025/chang25b_interspeech.html"
  },
  "meyer25_interspeech": {
   "authors": [
    [
     "Sarina",
     "Meyer"
    ],
    [
     "Ekaterina",
     "Kolos"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "First Steps Towards Voice Anonymization for Code-Switching Speech",
   "original": "1027",
   "order": 1045,
   "page_count": 5,
   "abstract": [
    "The goal of voice anonymization is to modify an audio such that the true identity of its speaker is hidden. Research on this task is typically limited to the same English read speech datasets, thus the efficacy of current methods for other types of speech data remains unknown. In this paper, we present the first investigation of voice anonymization for the multilingual phenomenon of code-switching speech. We prepare two corpora for this task and propose adaptations to a multilingual anonymization model to make it applicable for code-switching speech. By testing the anonymization performance of this and two language-independent methods on the datasets, we find that only the multilingual system performs well in terms of privacy and utility preservation. Furthermore, we observe challenges in performing utility evaluations on this data because of its spontaneous character and the limited code-switching support by the multilingual speech recognition model."
   ],
   "p1": 5123,
   "pn": 5127,
   "doi": "10.21437/Interspeech.2025-1027",
   "url": "interspeech_2025/meyer25_interspeech.html"
  },
  "christodoulidou25_interspeech": {
   "authors": [
    [
     "Polychronia",
     "Christodoulidou"
    ],
    [
     "James",
     "Tanner"
    ],
    [
     "Jane",
     "Stuart-Smith"
    ],
    [
     "Michael",
     "McAuliffe"
    ],
    [
     "Mridhula",
     "Murali"
    ],
    [
     "Amy",
     "Smith"
    ],
    [
     "Lauren",
     "Taylor"
    ],
    [
     "Joanne",
     "Cleland"
    ],
    [
     "Anja",
     "Kuschmann"
    ]
   ],
   "title": "A semi-automatic pipeline for transcribing and segmenting child speech",
   "original": "1030",
   "order": 872,
   "page_count": 5,
   "abstract": [
    "This study evaluates both automated transcription (WhisperX) and forced alignment (MFA) in developing a semi-automated pipeline for obtaining  acoustic vowel measures from field recordings from 275 children speaking a non-standard, English dialect, Scottish English. As expected, manual correction of speech transcriptions before forced alignment improves the quality of acoustic vowel measures with respect to manually-annotated data, though speech style and recording environment present some challenges for both tools. Adaptation of the MFA pre-trained english_us_arpa acoustic model towards the children&#x27;s speech also improves the quality of acoustic measures, though greater improvement was not found by increasing training sample size."
   ],
   "p1": 4278,
   "pn": 4282,
   "doi": "10.21437/Interspeech.2025-1030",
   "url": "interspeech_2025/christodoulidou25_interspeech.html"
  },
  "smith25_interspeech": {
   "authors": [
    [
     "Griffin",
     "Smith"
    ],
    [
     "Dianna",
     "Yee"
    ],
    [
     "Jennifer King",
     "Chen"
    ],
    [
     "Leah",
     "Findlater"
    ]
   ],
   "title": "Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection",
   "original": "1031",
   "order": 396,
   "page_count": 5,
   "abstract": [
    "Identifying mistakes (i.e., miscues) made while reading aloud is commonly approached post-hoc by comparing automatic speech recognition (ASR) transcriptions to the target reading text. However, post-hoc methods perform poorly when ASR inaccurately transcribes verbatim speech. To improve on current methods for reading error annotation, we propose a novel end-to-end architecture that incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection. Our contributions include: first, demonstrating that incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning, and second, showing that it is feasible to augment speech recognition tasks for end-to-end miscue detection. We conducted two case studies—children’s read-aloud and adult atypical speech—and found that our proposed strategies improve verbatim transcription and miscue detection compared to current state-of-the-art."
   ],
   "p1": 1943,
   "pn": 1947,
   "doi": "10.21437/Interspeech.2025-1031",
   "url": "interspeech_2025/smith25_interspeech.html"
  },
  "yang25h_interspeech": {
   "authors": [
    [
     "Mu",
     "Yang"
    ],
    [
     "Bowen",
     "Shi"
    ],
    [
     "Matthew",
     "Le"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Andros",
     "Tjandra"
    ]
   ],
   "title": "Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation",
   "original": "1032",
   "order": 255,
   "page_count": 5,
   "abstract": [
    "This work focuses on improving Text-To-Audio (TTA) generation on zero-shot and few-shot settings (i.e. generating unseen or uncommon audio events). Inspired by the success of Retrieval-Augmented Generation (RAG) in Large Language Models, we propose Audiobox TTA-RAG, a novel retrieval-augmented TTA approach based on Audiobox, a flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution that generates audio conditioned on text only, we extend the TTA process by augmenting the conditioning input with both text and retrieved audio samples. Our retrieval method does not require the external database to have labeled audio, offering more practical use cases. We show that the proposed model can effectively leverage the retrieved audio samples and significantly improve zero-shot and few-shot TTA performance, with large margins on multiple evaluation metrics, while maintaining the ability to generate semantically aligned audio for the in-domain setting."
   ],
   "p1": 1243,
   "pn": 1247,
   "doi": "10.21437/Interspeech.2025-1032",
   "url": "interspeech_2025/yang25h_interspeech.html"
  },
  "berger25_interspeech": {
   "authors": [
    [
     "Noe",
     "Berger"
    ],
    [
     "Siqi",
     "Sun"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Non-Standard Accent TTS Support via Large Multi-Accent Frontend Pronunciation Knowledge Transfer",
   "original": "1034",
   "order": 517,
   "page_count": 5,
   "abstract": [
    "Mainstream text-to-speech (TTS) applications rarely offer non-standard accented voice options, perhaps in part due to practical challenges associated with the acquisition of prerequisite training data. Building on prior research, this work demonstrates that a large multi-accent neural frontend model can reduce pronunciation training data requirements by 95% for robust performance in low-resource accents. We further show that pronunciation knowledge transfer is weakly influenced by accent similarity, quantified using a Levenshtein distance-based metric. The large multi-accent paradigm thus emerges as an effective strategy for improving non-standard accent voice-building accessibility, provided source accents are selected to maximize similarity with target accents where possible."
   ],
   "p1": 2530,
   "pn": 2534,
   "doi": "10.21437/Interspeech.2025-1034",
   "url": "interspeech_2025/berger25_interspeech.html"
  },
  "gomezzaragoza25_interspeech": {
   "authors": [
    [
     "Lucía",
     "Gómez-Zaragozá"
    ],
    [
     "Javier",
     "Marín-Morales"
    ],
    [
     "Mariano",
     "Alcañiz"
    ],
    [
     "Mohammad",
     "Soleymani"
    ]
   ],
   "title": "Speech and Text Foundation Models for Depression Detection: Cross-Task and Cross-Language Evaluation",
   "original": "1035",
   "order": 1071,
   "page_count": 5,
   "abstract": [
    "Automated depression detection is gaining attention due to its potential to improve psychiatric care. This study compares the performance of foundation models (FMs) in two datasets: an extended Distress Analysis Interview Corpus (DAIC+) in English, and Depressive Indicators during Casual Talks (DEPTALK) dataset in Spanish. HuBERT models and their fine-tuned versions for emotion recognition (ER) are used for speech. RoBERTa models and their ER variants are applied for text. Representations from FMs are grouped into context windows and processed by a Gated Recurrent Unit. Early fusion is used for multimodal analysis. Speech models perform similarly across datasets (F1≈0.60). Text models perform better on DAIC+ than on DEPTALK (F1=0.70 vs 0.45). Multimodal models using FMs fine-tuned for ER perform best for both (F1=0.75 in DAIC+, 0.69 in DEPTALK), showing effectiveness across tasks and languages. Fairness evaluation reveals gender bias, which motivates future research on its alleviation."
   ],
   "p1": 5253,
   "pn": 5257,
   "doi": "10.21437/Interspeech.2025-1035",
   "url": "interspeech_2025/gomezzaragoza25_interspeech.html"
  },
  "zapata25_interspeech": {
   "authors": [
    [
     "Julian",
     "Zapata"
    ],
    [
     "Lara",
     "Hanna"
    ]
   ],
   "title": "Text Entry for All: Towards Speech-based Multimodal Interaction for Inclusion, Accessibility and the Preservation of the World’s Linguistic Heritage ",
   "original": "1036",
   "order": 371,
   "page_count": 5,
   "abstract": [
    "This paper describes an emerging project that aims at better understanding how speakers of different languages, with diverse needs, produce texts today, and rethinking how they should do so in the future. Text entry (TE), the process of inputting text into a computer, is a crucial part of our interaction with technology. However, research has shown that the conventional TE method--typing on a keyboard--is not the most efficient or appropriate way of inputting text in various contexts, for diverse users. Now, how can we design more effective, accessible and inclusive TE methods that consider the uniqueness of different languages and diverse users and use cases? This fascinating question is likely to motivate speech technology, writing-process and human-computer interaction researchers alike, particularly in the generative AI age. But we are also living in the age of speech and multimodal interactions, which offer unprecedented opportunities to reinvent how we learn, write and communicate."
   ],
   "p1": 1818,
   "pn": 1822,
   "doi": "10.21437/Interspeech.2025-1036",
   "url": "interspeech_2025/zapata25_interspeech.html"
  },
  "jin25b_interspeech": {
   "authors": [
    [
     "Kun",
     "Jin"
    ],
    [
     "Siva",
     "Penke"
    ],
    [
     "Srinivasa",
     "Algubelli"
    ]
   ],
   "title": "VoiceNet: Multilingual On-Device Phoneme-To-Audio Alignment",
   "original": "1037",
   "order": 463,
   "page_count": 5,
   "abstract": [
    "Phoneme-to-Audio Alignment has many applications and is generally considered as an important task in the lip-sync system where an avatar&#x27;s lip shape is synchronized with the corresponding speech signal. In this work, we propose such a novel end-to-end on-device multilingual model, VoiceNet, which learns both phoneme recognition and text-independent forced alignment. VoiceNet supports on-device inference in Real-Time as well as in Non Real-time. Moreover, in the Non-RealTime scenario, we show that the performance can be further enhanced when text is given. Our experiments demonstrate competitive performance of VoiceNet compared with state-of-the-art Phoneme Recognition and Forced Alignment results on LibriSpeech and multilingual dataset. Benchmarked on a set of Galaxy phone devices, VoiceNet achieves the average phoneme inference latency of 6ms on CPU, demonstrating the high computational efficiency. Furthermore, VoiceNet can achieve 2x speedup on GPU and 10x speedup on NPU."
   ],
   "p1": 2260,
   "pn": 2264,
   "doi": "10.21437/Interspeech.2025-1037",
   "url": "interspeech_2025/jin25b_interspeech.html"
  },
  "zhang25h_interspeech": {
   "authors": [
    [
     "Harry",
     "Zhang"
    ],
    [
     "Kurt",
     "Partridge"
    ],
    [
     "Pai",
     "Zhu"
    ],
    [
     "Neng",
     "Chen"
    ],
    [
     "Hyun Jin",
     "Park"
    ],
    [
     "Dhruuv",
     "Agarwal"
    ],
    [
     "Quan",
     "Wang"
    ]
   ],
   "title": "GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples",
   "original": "1038",
   "order": 547,
   "page_count": 5,
   "abstract": [
    "Spoken Keyword Spotting (KWS) is the task of distinguishing between the presence and absence of a keyword in audio. The accuracy of a KWS model hinges on its ability to correctly classify examples close to the keyword and non-keyword boundary. These boundary examples are often scarce in training data, limiting model performance. In this paper, we propose a method to systematically generate adversarial examples close to the decision boundary by making insertion/deletion/substitution edits on the keyword&#x27;s graphemes. We evaluate this technique on held-out data for a popular keyword and show that the technique improves AUC on a dataset of synthetic hard negatives by 61% while maintaining quality on positives and ambient negative audio data."
   ],
   "p1": 2680,
   "pn": 2684,
   "doi": "10.21437/Interspeech.2025-1038",
   "url": "interspeech_2025/zhang25h_interspeech.html"
  },
  "hsu25_interspeech": {
   "authors": [
    [
     "Haley",
     "Hsu"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Khalil",
     "Iskarous"
    ],
    [
     "Louis",
     "Goldstein"
    ]
   ],
   "title": "Instantaneous changes in acoustic signals reflect syllable progression and cross-linguistic syllable variation",
   "original": "1040",
   "order": 21,
   "page_count": 5,
   "abstract": [
    "While abstract speech representations often exploit sequenced syllable units, how exactly syllables as abstract cognitive compositional structure relate to observable patterns in the articulatory and acoustic signals remains opaque. Previous work suggests oscillatory acoustic properties link such linguistic representations to physical events. We probe this relationship by testing temporal coordination between changes in spectral energy and amplitude with syllable boundary locations through phase-locking analyses. Results for syllabic nuclei demonstrate these phase-locking values (PLVs) track syllable progression in both English and Tashlhiyt. Further, cross-language preferences for different syllable nucleus types are found to be reflected in their respective PLVs. Overall, the findings demonstrate a tight coordination between abstract syllable units and quantifiable signal properties and additionally provide novel dynamical grounding for cross-linguistic syllable nucleus preferences."
   ],
   "p1": 96,
   "pn": 100,
   "doi": "10.21437/Interspeech.2025-1040",
   "url": "interspeech_2025/hsu25_interspeech.html"
  },
  "jon25_interspeech": {
   "authors": [
    [
     "Hyo Jin",
     "Jon"
    ],
    [
     "Longbin",
     "Jin"
    ],
    [
     "Hyuntaek",
     "Jung"
    ],
    [
     "Hyunseo",
     "Kim"
    ],
    [
     "Donghun",
     "Min"
    ],
    [
     "Eun Yi",
     "Kim"
    ]
   ],
   "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition",
   "original": "1041",
   "order": 951,
   "page_count": 5,
   "abstract": [
    "This paper presents our contributions to the Speech Emotion Recognition in Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion recognition and emotional attribute prediction. To handle the complexities of natural speech, including intra- and inter-subject variability, we propose Multi-level Acoustic-Textual Emotion Representation (MATER)--a novel hierarchical framework that integrates acoustic and textual features at the word, utterance, and embedding levels. By fusing low-level lexical and acoustic cues with high-level contextualized representations, MATER effectively captures both fine-grained prosodic variations and semantic nuances. Additionally, we introduce an uncertainty-aware ensemble strategy to mitigate annotator inconsistencies, improving robustness in ambiguous emotional expressions. MATER ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of 0.5928, securing second place in valence prediction with an impressive CCC of 0.6941."
   ],
   "p1": 4673,
   "pn": 4677,
   "doi": "10.21437/Interspeech.2025-1041",
   "url": "interspeech_2025/jon25_interspeech.html"
  },
  "hui25_interspeech": {
   "authors": [
    [
     "C. T. Justine",
     "Hui"
    ],
    [
     "Jenice",
     "Kuzhikombil"
    ],
    [
     "Isabella",
     "Shields"
    ],
    [
     "Hiraia",
     "Haami-Wells"
    ],
    [
     "Catherine I.",
     "Watson"
    ],
    [
     "Peter [J.]",
     "Keegan"
    ]
   ],
   "title": "Perception of Long and Short Vowel Contrast in Te Reo Māori in Clean and Everyday Listening Environments",
   "original": "1043",
   "order": 472,
   "page_count": 5,
   "abstract": [
    "Te reo Māori (the Māori language) is the language of the indigenous people in New Zealand and has a long-short vowel contrast. This study first investigates the cues used to perceive vowel length for Māori fluent users and learners. Secondly, it explores the effect of an everyday listening environment in the form of a traditional meeting house (wharenui) on cue-weighting between duration and stress for identifying long and short vowels when the stimuli are rendered with the wharenui room acoustics.  An identification test was carried out with three pairs of words that differed in stress location and were manipulated in vowel duration. We found both groups to mainly use duration as a cue, even though the advanced listeners commented that they were listening for &#x27;intonation&#x27;. For the wharenui acoustics, there were no differences observed between the groups and only a small categorical shift for the /a:/ vowel for the learners."
   ],
   "p1": 2305,
   "pn": 2309,
   "doi": "10.21437/Interspeech.2025-1043",
   "url": "interspeech_2025/hui25_interspeech.html"
  },
  "ong25_interspeech": {
   "authors": [
    [
     "Michael",
     "Ong"
    ],
    [
     "Sean",
     "Robertson"
    ],
    [
     "Leo",
     "Peckham"
    ],
    [
     "Alba",
     "Jorquera Jimenez de Aberasturi"
    ],
    [
     "Paula",
     "Arkhangorodsky"
    ],
    [
     "Robin",
     "Huo"
    ],
    [
     "Aman",
     "Sakhardande"
    ],
    [
     "Mark",
     "Hallap"
    ],
    [
     "Naomi",
     "Nagy"
    ],
    [
     "Ewan",
     "Dunbar"
    ]
   ],
   "title": "The Faetar Speech Recognition Benchmark",
   "original": "1045",
   "order": 822,
   "page_count": 5,
   "abstract": [
    "We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a  Franco-Provençal variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Provençal. The corpus comes from field recordings, most of which are noisy, for which only 5 hours have matching transcriptions, and for which transcriptions are inconsistent. The corpus contains an additional 20 hours of unlabelled speech. We report baseline results from multilingual speech foundation models with a best phone error rate of 30.5%, using a pipeline that continues pre-training on the foundation model using the unlabelled set."
   ],
   "p1": 4028,
   "pn": 4032,
   "doi": "10.21437/Interspeech.2025-1045",
   "url": "interspeech_2025/ong25_interspeech.html"
  },
  "mcallister25_interspeech": {
   "authors": [
    [
     "Tara",
     "McAllister"
    ],
    [
     "Collin",
     "Eagen"
    ],
    [
     "Yi",
     "Shan"
    ],
    [
     "Peter",
     "Traver"
    ],
    [
     "Daphna",
     "Harel"
    ],
    [
     "Tae Hong",
     "Park"
    ],
    [
     "Vesna",
     "Novak"
    ]
   ],
   "title": "Web-Based Application for Real-Time Biofeedback of Vocal Resonance in Gender-Affirming Voice Training: Design and Usability Evaluation",
   "original": "1050",
   "order": 139,
   "page_count": 5,
   "abstract": [
    "Gender-affirming voice training (GAVT) can reduce voice-gender incongruence for transgender and gender-diverse individuals, but access is often limited by high costs and a lack of qualified providers. Interactive software could expand access, but existing GAVT apps are typically limited in functionality. This paper describes the development and testing of software to address one of the most challenging aspects of GAVT: modifying vocal tract resonance. We introduce a biofeedback system that uses a real-time linear predictive coding (LPC) spectrum to visualize changes in resonance as learners adjust their vocal tract shape. Visual targets for brighter (feminine) and darker (masculine) resonances help guide users toward their desired voice characteristics. In-lab user testing with 10 trans women yielded an average System Usability Scale (SUS) score of 75.25, supporting the acceptability of the tool as an adjunct resonance training in the GAVT context."
   ],
   "p1": 664,
   "pn": 668,
   "doi": "10.21437/Interspeech.2025-1050",
   "url": "interspeech_2025/mcallister25_interspeech.html"
  },
  "komatsu25_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Komatsu"
    ],
    [
     "Hokuto",
     "Munakata"
    ],
    [
     "Yuchi",
     "Ishikawa"
    ]
   ],
   "title": "Leveraging Unlabeled Audio for Audio-Text Contrastive Learning via Audio-Composed Text Features",
   "original": "1053",
   "order": 531,
   "page_count": 5,
   "abstract": [
    "We propose a novel approach to audio-text contrastive learning that leverages unlabeled audio by introducing audio-composed text features. First, we generate composed audio by additively combining labeled and unlabeled audio. To obtain a text feature aligned with this newly composed audio, we introduce an audio-to-text (a2t) module that transforms the features of unlabeled audio into the corresponding text feature. The newly generated text feature is then concatenated with the original text of the labeled audio and passed through a text encoder to produce the audio-composed text features. By pairing these features with the composed audio for contrastive learning, our approach effectively integrates information from both labeled and unlabeled data. In audio-text retrieval experiments on Clotho and AudioCaps, the proposed method achieves notable improvements in Recall@1, with relative gains of 9.3% and 13.6%, respectively, compared to those trained solely with labeled audio."
   ],
   "p1": 2600,
   "pn": 2604,
   "doi": "10.21437/Interspeech.2025-1053",
   "url": "interspeech_2025/komatsu25_interspeech.html"
  },
  "ishikawa25_interspeech": {
   "authors": [
    [
     "Yuchi",
     "Ishikawa"
    ],
    [
     "Shota",
     "Nakada"
    ],
    [
     "Hokuto",
     "Munakata"
    ],
    [
     "Kazuhiro",
     "Saito"
    ],
    [
     "Tatsuya",
     "Komatsu"
    ],
    [
     "Yoshimitsu",
     "Aoki"
    ]
   ],
   "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos ",
   "original": "1054",
   "order": 532,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning. LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual masked autoencoders, enabling the model to learn across audio, visual and text modalities. To train LG-CAV-MAE, we introduce an automatic method to generate audio-visual-text triplets from unlabeled videos. We first generate frame-level captions using an image captioning model and then apply CLAP-based filtering to ensure strong alignment between audio and captions. This approach yields high-quality audio-visual-text triplets without requiring manual annotations. We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an audio-visual classification task. Our method significantly outperforms existing approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks and a 3.2% improvement for the classification task."
   ],
   "p1": 2605,
   "pn": 2609,
   "doi": "10.21437/Interspeech.2025-1054",
   "url": "interspeech_2025/ishikawa25_interspeech.html"
  },
  "kim25l_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "Bongsu",
     "Jung"
    ]
   ],
   "title": "DLF-EEND: Dynamic Layer Fusion for End-to-End Speaker Diarization",
   "original": "1059",
   "order": 344,
   "page_count": 5,
   "abstract": [
    "In this study, we propose Dynamic Layer Fusion for EEND (DLF-EEND), a novel approach for integrating Transformer layer information in end-to-end speaker diarization. The model introduces an auxiliary branch during training, using dynamic routing to adaptively fuse multi-resolution representations at each time step. Applying Permutation-Invariant Training (PIT) loss to the fused features enhances hierarchical learning, from low-level acoustic cues to high-level speaker separation. This preserves distinct layer-specific information and improves diarization accuracy, particularly in overlapping speech and speaker transitions. During inference, only the main branch is used, reducing computation while maintaining inter-layer fusion benefits. Experiments show DLF-EEND reduces Diarization Error Rate (DER) by 59.18% on simulated datasets and improves CALLHOME performance by 14.26%, outperforming SA-EEND."
   ],
   "p1": 1688,
   "pn": 1692,
   "doi": "10.21437/Interspeech.2025-1059",
   "url": "interspeech_2025/kim25l_interspeech.html"
  },
  "peng25c_interspeech": {
   "authors": [
    [
     "Yifan",
     "Peng"
    ],
    [
     "Muhammad",
     "Shakeel"
    ],
    [
     "Yui",
     "Sudo"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Chyi-Jiunn",
     "Lin"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning",
   "original": "1062",
   "order": 456,
   "page_count": 5,
   "abstract": [
    "The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit."
   ],
   "p1": 2225,
   "pn": 2229,
   "doi": "10.21437/Interspeech.2025-1062",
   "url": "interspeech_2025/peng25c_interspeech.html"
  },
  "shi25_interspeech": {
   "authors": [
    [
     "Jiacheng",
     "Shi"
    ],
    [
     "Yanfu",
     "Zhang"
    ],
    [
     "Ye",
     "Gao"
    ]
   ],
   "title": "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning",
   "original": "1064",
   "order": 916,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) is fundamental to affective computing and human-computer interaction, yet existing models struggle to generalize across diverse acoustic conditions. While Contrastive Language-Audio Pretraining (CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for capturing emotional cues, making it suboptimal for SER. To address this, we propose CLEP-DG, a framework that enhances CLAP’s robustness in emotion recognition. First, we fine-tune CLAP to obtain CLEP, adapting it on large-scale emotional speech datasets to better encode emotion-relevant features. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a text-driven augmentation strategy that optimizes learnable prompt vectors to model diverse acoustic environments without additional labeled audio. Finally, leveraging cross-modal transferability, we train a classifier on text-derived embeddings and apply it to the audio encoder during inference, mitigating domain shifts between textual supervision and audio-based emotion recognition. Experiments across five benchmark datasets show that CLEP-DG outperforms prior CLAP-based approaches, achieving state-of-the-art performance in both supervised and domain generalization settings."
   ],
   "p1": 4498,
   "pn": 4502,
   "doi": "10.21437/Interspeech.2025-1064",
   "url": "interspeech_2025/shi25_interspeech.html"
  },
  "szalay25b_interspeech": {
   "authors": [
    [
     "Tünde",
     "Szalay"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Amelia",
     "Gully"
    ],
    [
     "Tharinda",
     "Piyadasa"
    ],
    [
     "Craig",
     "Jin"
    ],
    [
     "David",
     "Waddington"
    ],
    [
     "Naeim",
     "Sanaei"
    ],
    [
     "Sheryl",
     "Foster"
    ],
    [
     "Kirrie",
     "Ballard"
    ]
   ],
   "title": "Lateral Channel Formation in Australian English /l/: Insights from Magnetic Resonance Imaging",
   "original": "1065",
   "order": 710,
   "page_count": 5,
   "abstract": [
    "Goals of lateral approximant production are imperfectly understood, partly because of the limitations of most existing data, restricted to the midsagittal plane. To provide more complete information about the configuration of the vocal tract for laterals, /l/-production in three vowel contexts by three Australian English speakers was examined for the first time using a combination of real-time and volumetric Magnetic Resonance Imaging (MRI). Laterals produced by all speakers were characterised by bilateral parasagittal airflow, with some asymmetries in side channel geometries. Central occlusion of the oral airway varied in location, timing, and duration across speakers and vowel contexts, but was consistently associated with reduction in overall acoustic intensity relative to context vowels. These data provide further insights into the complex relationships between articulatory, coarticulatory and acoustic properties of lateral approximants, and their realisation in Australian English."
   ],
   "p1": 3489,
   "pn": 3493,
   "doi": "10.21437/Interspeech.2025-1065",
   "url": "interspeech_2025/szalay25b_interspeech.html"
  },
  "sun25f_interspeech": {
   "authors": [
    [
     "Wanli",
     "Sun"
    ],
    [
     "Anton",
     "Ragni"
    ]
   ],
   "title": "Score-Based Training for Energy-Based TTS Models",
   "original": "1066",
   "order": 1127,
   "page_count": 5,
   "abstract": [
    "Noise contrastive estimation (NCE) is a popular method for training energy-based models (EBM) with intractable normalisation terms. The key idea of NCE is to learn by comparing unnormalised log-likelihoods of the reference and noisy samples, thus avoiding explicitly computing normalisation terms. However, NCE critically relies on the quality of noisy samples. Recently, sliced score matching (SSM) has been popularised by closely related diffusion models (DM). Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning distribution of its projections on randomly chosen directions. However, both NCE and SSM disregard the form of log-likelihood function, which is problematic given that EBMs and DMs make use of first-order optimisation during inference. This paper proposes a new criterion that learns scores more suitable for first-order schemes. Experiments contrasts these approaches for training EBMs."
   ],
   "p1": 5528,
   "pn": 5532,
   "doi": "10.21437/Interspeech.2025-1066",
   "url": "interspeech_2025/sun25f_interspeech.html"
  },
  "jing25_interspeech": {
   "authors": [
    [
     "Jiabo",
     "Jing"
    ],
    [
     "Ying",
     "Hu"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Liang",
     "He"
    ],
    [
     "Zhijian",
     "Ou"
    ]
   ],
   "title": "A Joint Network for Singing Melody Extraction from Polyphonic Music with Attention Aggregation and Self-Consistency Training",
   "original": "1070",
   "order": 631,
   "page_count": 5,
   "abstract": [
    "Singing melody extraction (SME) is an important task in music information retrieval (MIR). In this paper, we propose a separate spectrum-based SME model and a joint network that combines the pre-trained and spectrum-based models. In the joint network, we design an attention aggregation module (AAM) consisting of cross-attention (CA) and adaptive decision fusion (ADF) to effectively fuse the intermediate features from two models. Furthermore, we introduce a self-consistency training strategy, which utilizes hard and soft labels to supervise two separate models to better obtain the SME task-relevant information. Experimental results show that our proposed method, Joint Network, outperforms six compared state-of-the-art methods, achieving overall accuracy (OA) scores of 91.6%, 92.5%, and 78.9% on the ADC 2004, MIREX 05, and MEDLEY DB datasets, respectively. Visualized results show that the Joint Network can reduce the octave and melody detection errors."
   ],
   "p1": 3100,
   "pn": 3104,
   "doi": "10.21437/Interspeech.2025-1070",
   "url": "interspeech_2025/jing25_interspeech.html"
  },
  "kim25m_interspeech": {
   "authors": [
    [
     "Taesoo",
     "Kim"
    ],
    [
     "Yongsik",
     "Jo"
    ],
    [
     "Hyunmin",
     "Song"
    ],
    [
     "Taehwan",
     "Kim"
    ]
   ],
   "title": "Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech",
   "original": "1075",
   "order": 982,
   "page_count": 5,
   "abstract": [
    "Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel multi-sensory conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering para-lingual information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech."
   ],
   "p1": 4828,
   "pn": 4832,
   "doi": "10.21437/Interspeech.2025-1075",
   "url": "interspeech_2025/kim25m_interspeech.html"
  },
  "liu25i_interspeech": {
   "authors": [
    [
     "Hongbin",
     "Liu"
    ],
    [
     "Lun",
     "Wang"
    ],
    [
     "Om",
     "Thakkar"
    ],
    [
     "Abhradeep",
     "Thakurta"
    ],
    [
     "Arun",
     "Narayanan"
    ]
   ],
   "title": "Differentially Private Parameter-Efficient Fine-tuning for Large ASR Models",
   "original": "1076",
   "order": 311,
   "page_count": 5,
   "abstract": [
    "Large ASR models can inadvertently leak sensitive information, which can be mitigated by formal privacy measures like differential privacy (DP). However, traditional DP training is computationally expensive, and can hurt model performance. Our study explores DP parameter-efficient fine-tuning as a way to mitigate privacy risks with smaller computation and performance costs for ASR models. Through extensive experimentation and progressive optimization, we achieve 4.6%/8.1% word error rate on LibriSpeech clean/other test-sets, setting a new performance benchmark while maintaining (10, 3.52e-6)-DP in fine-tuning a large ASR model with over 600M parameters."
   ],
   "p1": 1523,
   "pn": 1527,
   "doi": "10.21437/Interspeech.2025-1076",
   "url": "interspeech_2025/liu25i_interspeech.html"
  },
  "mena25_interspeech": {
   "authors": [
    [
     "Carlos",
     "Mena"
    ],
    [
     "Pol",
     "Serra"
    ],
    [
     "Jacobo",
     "Romero"
    ],
    [
     "Abir",
     "Messaoudi"
    ],
    [
     "Jose",
     "Giraldo"
    ],
    [
     "Carme",
     "Armentano-Oller"
    ],
    [
     "Rodolfo",
     "Zevallos"
    ],
    [
     "Ivan",
     "Meza"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
   "original": "1078",
   "order": 876,
   "page_count": 5,
   "abstract": [
    "Code-switching (CS), the alternating use of two or more languages, challenges automatic speech recognition (ASR) due to scarce training data and linguistic similarities. The lack of dedicated CS datasets limits ASR performance, as most models rely on monolingual or mixed-language corpora that fail to reflect real-world CS patterns. This issue is critical in multilingual societies where CS occurs in informal and formal settings. A key example is Catalan-Spanish CS, widely used in media and parliamentary speeches. In this work, we improve ASR for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data with language tokens. We extract CS data from Catalan speech corpora and fine-tune OpenAI’s Whisper models, making them available on Hugging Face. Results show that combining a modest amount of synthetic CS data with the dominant language token yields the best transcription performance."
   ],
   "p1": 4298,
   "pn": 4302,
   "doi": "10.21437/Interspeech.2025-1078",
   "url": "interspeech_2025/mena25_interspeech.html"
  },
  "moriya25_interspeech": {
   "authors": [
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Kiyoaki",
     "Matsui"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Kohei",
     "Matsuura"
    ]
   ],
   "title": "Attention-Free Dual-Mode ASR with Latency-Controlled Selective State Spaces",
   "original": "1079",
   "order": 734,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel encoder architecture designed to enhance transducer-based dual-mode automatic speech recognition (ASR). Our approach leverages the selective state-space model, Mamba, to enable attention-free dual-mode ASR. While bidirectional Mamba (BiMamba) captures full context and enables constant-time inference, unlike attention-based models with quadratic complexity, it is limited to offline processing. In contrast, using only unidirectional Mamba for the dual-mode ASR degrades ASR performance in both offline and streaming modes due to its restricted access to future contexts. To address this issue, we propose the latency-controlled BiMamba (LC-BiMamba); it enables chunk-wise processing in streaming mode while still accessing future contexts and functioning as standard BiMamba does in offline mode. Experimental results demonstrate that LC-BiMamba outperforms the baseline Conformer system, achieving faster and more accurate decoding in our dual-mode ASR framework."
   ],
   "p1": 3588,
   "pn": 3592,
   "doi": "10.21437/Interspeech.2025-1079",
   "url": "interspeech_2025/moriya25_interspeech.html"
  },
  "ke25_interspeech": {
   "authors": [
    [
     "Xiaoquan",
     "Ke"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Optimizing Pause Context in Fine-Tuning Pre-trained Large Language Models for Dementia Detection",
   "original": "1080",
   "order": 288,
   "page_count": 5,
   "abstract": [
    "Speech pauses serve as a valuable and non-invasive biomarker for the early detection of dementia. Our study aims to examine abnormal pauses, specifically their durations, for improving the detection performance. Inspired by the proven performance of the Transformer-based models in dementia detection, we opted for integrating the abnormal pauses into these models. Specifically, we enriched the inputs for the Transformer-based models by fusing between-segment pause context into the automated transcriptions. We performed the experiments on our Cantonese elderly corpus called CU-Marvel. To improve the detection performance, we optimized the pause durations when infusing the pause context into the transcriptions. Our findings suggest that the between-segment pauses could also serve as promising biomarkers. We emphasize the importance of optimizing pause patterns across different languages or datasets. Our findings indicate that various across different languages or datasets."
   ],
   "p1": 1408,
   "pn": 1412,
   "doi": "10.21437/Interspeech.2025-1080",
   "url": "interspeech_2025/ke25_interspeech.html"
  },
  "hu25i_interspeech": {
   "authors": [
    [
     "Desheng",
     "Hu"
    ],
    [
     "Yang",
     "Xiang"
    ],
    [
     "Jian",
     "Lu"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Xinkang",
     "Xu"
    ]
   ],
   "title": "Speaker Normalization and Content Restoration for Zero-Shot Voice Conversion with Attention-Enhanced Discriminator",
   "original": "1081",
   "order": 287,
   "page_count": 5,
   "abstract": [
    "Zero-shot voice conversion can be achieved by extracting the source linguistic content and the unseen target speaker information, then reconstructing mel-spectrograms from these representations. In this paper, we propose a novel zero-shot VC method. First, we disentangle content and speaker information by training the content encoder from scratch, integrating a supervised phoneme classification network with speaker normalization and content restoration modules. Second, we enhance the speaker encoder by applying consistency loss, ensuring the extraction of accurate and robust speaker representations. Finally, we introduce an attention-enhanced discriminator for adversarial training to generate high-fidelity mel-spectrograms. Experimental results demonstrate that our proposed method demonstrates outstanding VC performance in terms of both speech quality and speaker similarity for unseen speakers."
   ],
   "p1": 1403,
   "pn": 1407,
   "doi": "10.21437/Interspeech.2025-1081",
   "url": "interspeech_2025/hu25i_interspeech.html"
  },
  "lertpetchpun25_interspeech": {
   "authors": [
    [
     "Thanathai",
     "Lertpetchpun"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction",
   "original": "1082",
   "order": 946,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) in naturalistic conditions presents a significant challenge for the speech processing community. Challenges include disagreement in labeling among annotators and imbalanced data distributions. This paper presents a reproducible framework that achieves superior (top 1) performance in the Emotion Recognition in Naturalistic Conditions Challenge (IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system is designed to tackle the aforementioned challenges through multimodal learning, multi-task learning, and imbalanced data handling. Specifically, our best system is trained by adding text embeddings, predicting gender, and including ``Other&#x27;&#x27; (O) and ``No Agreement&#x27;&#x27; (X) samples in the training set. Our system&#x27;s results secured both first and second places in the IS25-SER Challenge, and the top performance was achieved by a simple two-system ensemble."
   ],
   "p1": 4648,
   "pn": 4652,
   "doi": "10.21437/Interspeech.2025-1082",
   "url": "interspeech_2025/lertpetchpun25_interspeech.html"
  },
  "lee25h_interspeech": {
   "authors": [
    [
     "Joun Yeop",
     "Lee"
    ],
    [
     "Sangjun",
     "Park"
    ],
    [
     "Byoung Jin",
     "Choi"
    ],
    [
     "Ji-Hyun",
     "Lee"
    ],
    [
     "Min-Kyung",
     "Kim"
    ],
    [
     "Hoon-Young",
     "Cho"
    ]
   ],
   "title": "Efficient Streaming TTS Acoustic Model with Depthwise RVQ Decoding Strategies in a Mamba Framework",
   "original": "1084",
   "order": 1124,
   "page_count": 5,
   "abstract": [
    "Recent advances in neural codec-based text-to-speech (TTS) systems have achieved remarkable synthesized speech quality. However, their reliance on large model size and heavy computational requirements limits CPU-based on-device deployment. In this work, we present a Mamba-based streaming acoustic model with two novel depthwise decoding strategies for residual vector quantization (RVQ)-based codec: a Masked Language Model (MLM) approach and an Implicit Neural Representation (INR) approach. The MLM strategy iteratively refines tokens along the code depth axis to enhance speech quality, whereas the INR approach predicts all quantization levels in parallel to reduce computational costs. We further incorporate a speaker embedding conditioning mechanism for a zero-shot scenario, enabling robust performance on unseen speakers. Experimental results demonstrate comparable or even superior improvements in both objective and subjective metrics compared to other larger TTS baseline models."
   ],
   "p1": 5513,
   "pn": 5517,
   "doi": "10.21437/Interspeech.2025-1084",
   "url": "interspeech_2025/lee25h_interspeech.html"
  },
  "si25_interspeech": {
   "authors": [
    [
     "Yongjie",
     "Si"
    ],
    [
     "Yanxiong",
     "Li"
    ],
    [
     "Jiaxin",
     "Tan"
    ],
    [
     "Qianhua",
     "He"
    ],
    [
     "Il-Youp",
     "Kwak"
    ]
   ],
   "title": "Fully Few-shot Class-incremental Audio Classification Using Multi-level Embedding Extractor and Ridge Regression Classifier",
   "original": "1085",
   "order": 270,
   "page_count": 5,
   "abstract": [
    "In the task of Few-shot Class-incremental Audio Classification (FCAC), training samples of each base class are required to be abundant to train model. However, it is not easy to collect abundant training samples for many base classes due to data scarcity and high collection cost. We discuss a more realistic issue, Fully FCAC (FFCAC), in which training samples of both base and incremental classes are only a few. Furthermore, we propose a FFCAC method using a model which is decoupled into a multi-level embedding extractor and a ridge regression classifier. The embedding extractor consists of an encoder of audio spectrogram Transformer and a fusion module, and is trained in the base session but frozen in all incremental sessions. The classifier is updated continually in each incremental session. Results on three public datasets show that our method exceeds current methods in accuracy, and has advantage over most of them in complexity."
   ],
   "p1": 1318,
   "pn": 1322,
   "doi": "10.21437/Interspeech.2025-1085",
   "url": "interspeech_2025/si25_interspeech.html"
  },
  "xu25g_interspeech": {
   "authors": [
    [
     "Anfeng",
     "Xu"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "So Hyun",
     "Kim"
    ],
    [
     "Somer",
     "Bishop"
    ],
    [
     "Catherine",
     "Lord"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Large Language Models based ASR Error Correction for Child Conversations",
   "original": "1088",
   "order": 579,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) has recently shown remarkable progress, but accurately transcribing children&#x27;s speech remains a significant challenge. Recent developments in Large Language Models (LLMs) have shown promise in improving ASR transcriptions. However, their applications in child speech including conversational scenarios are under-explored. In this study, we explore the use of LLMs in correcting ASR errors for conversational child speech. We demonstrate the promises and challenges of LLMs through experiments on two children&#x27;s conversational speech datasets with both zero-shot and fine-tuned ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs to improve ASR performance when incorporating contextual information or when using fine-tuned autoregressive ASR (e.g., Whisper) outputs."
   ],
   "p1": 2840,
   "pn": 2844,
   "doi": "10.21437/Interspeech.2025-1088",
   "url": "interspeech_2025/xu25g_interspeech.html"
  },
  "lin25d_interspeech": {
   "authors": [
    [
     "Yu-Sheng",
     "Lin"
    ],
    [
     "Ching-Yu",
     "Yang"
    ],
    [
     "Hsing-Hang",
     "Chou"
    ],
    [
     "Ya-Tse",
     "Wu"
    ],
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Defend for Self-Vocoding: A Novel Enhanced Decoder Network for Watermark Recovery",
   "original": "1091",
   "order": 1040,
   "page_count": 5,
   "abstract": [
    "Recent advances in voice cloning technology have raised security concerns due to its ability to generate highly realistic synthetic speech, making it challenging to detect malicious usage. Proactive watermarking approaches embed authentication information in target voices to prevent unauthorized synthesis. While existing methods show resilience against traditional preprocessing attacks, we identify a novel threat, self-vocoding, which reconstructs audio using neural vocoders, can cause severe watermark degradation but preserve high audio fidelity. To address this, we propose an enhanced decoding framework to handle self-vocoding distortions on watermarks. In addition to general vocoder distortions, we systematically categorize them into two vocoder types for further analysis. Experimental results demonstrate that our approach significantly improves watermark decoding accuracy, offering an effective defense against self-vocoding attacks."
   ],
   "p1": 5098,
   "pn": 5102,
   "doi": "10.21437/Interspeech.2025-1091",
   "url": "interspeech_2025/lin25d_interspeech.html"
  },
  "huang25e_interspeech": {
   "authors": [
    [
     "Wen",
     "Huang"
    ],
    [
     "Xuechen",
     "Liu"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "From Sharpness to Better Generalization for Speech Deepfake Detection",
   "original": "1095",
   "order": 1089,
   "page_count": 5,
   "abstract": [
    "Generalization remains a critical challenge in speech deepfake detection (SDD). While various approaches aim to improve robustness, generalization is typically assessed through performance metrics like equal error rate without a theoretical framework to explain model performance. This work investigates sharpness as a theoretical proxy for generalization in SDD. We analyze how sharpness responds to domain shifts and find it increases in unseen conditions, indicating higher model sensitivity. Based on this, we apply Sharpness-Aware Minimization (SAM) to reduce sharpness explicitly, leading to better and more stable performance across diverse unseen test sets. Furthermore, correlation analysis confirms a statistically significant relationship between sharpness and generalization in most test settings. These findings suggest that sharpness can serve as a theoretical indicator for generalization in SDD and that sharpness-aware training offers a promising strategy for improving robustness."
   ],
   "p1": 5338,
   "pn": 5342,
   "doi": "10.21437/Interspeech.2025-1095",
   "url": "interspeech_2025/huang25e_interspeech.html"
  },
  "hu25j_interspeech": {
   "authors": [
    [
     "Chenguang",
     "Hu"
    ],
    [
     "Yaqian",
     "Hao"
    ],
    [
     "Fulin",
     "Zhang"
    ],
    [
     "Xiaoxue",
     "Luo"
    ],
    [
     "Yao",
     "Shen"
    ],
    [
     "Yingying",
     "Gao"
    ],
    [
     "Chao",
     "Deng"
    ],
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "Privacy-Preserving Speaker Verification via End-to-End Secure Representation Learning",
   "original": "1096",
   "order": 308,
   "page_count": 5,
   "abstract": [
    "The widespread adoption of cloud-based speaker verification raises privacy concerns, as unauthorized access to speaker voice can be exploited for fraudulent purposes. In response, privacy preserving speaker verification (PPSV) schemes have emerged to safeguard speaker information and prevent unauthorized access. However, existing PPSV methods often compromise accuracy in their efforts to protect privacy. To enhance both privacy and accuracy, we propose an end-to-end framework for speaker template encryption that simultaneously optimizes verification and encryption tasks. Our approach enhances the security and accuracy of speaker verification by introducing quintuple loss and sharpness-aware minimization, respectively, eliminating texture details that could be exploited by attackers. We distill our methodologies into a PPSV method, Secure-RawNet. Experimental results demonstrate that Secure-RawNet achieves high accuracy and robust privacy protection."
   ],
   "p1": 1508,
   "pn": 1512,
   "doi": "10.21437/Interspeech.2025-1096",
   "url": "interspeech_2025/hu25j_interspeech.html"
  },
  "ogura25_interspeech": {
   "authors": [
    [
     "Tadashi",
     "Ogura"
    ],
    [
     "Takuma",
     "Okamoto"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "GST-BERT-TTS: Prosody Prediction Without Accentual Labels For Multi-Speaker TTS Using BERT With Global Style Tokens",
   "original": "1098",
   "order": 95,
   "page_count": 5,
   "abstract": [
    "Prosody prediction is crucial for pitch-accent languages like Japanese in text-to-speech (TTS) synthesis. Traditional methods rely on accent labels, which are often incomplete and do not generalize well. BERT-based models, such as fo-BERT, enable fundamental frequency prediction without accent labels but have been limited to single-speaker TTS. We propose GST-BERT-TTS, a novel method for multi-speaker TTS that integrates speaker-specific style embeddings from global style tokens (GST) into the token embeddings in BERT. The proposed method can realize speaker-aware fundamental frequency (fo) prediction in an accent label-free setting. Additionally, we extend fo-BERT to predict not only log fo but also energy and duration, improving speech expressiveness. Experiments using a Japanese multi-speaker TTS corpus demonstrate that GST-BERT-TTS improves the prosody prediction accuracy and synthesis quality compared with fo-BERT."
   ],
   "p1": 444,
   "pn": 448,
   "doi": "10.21437/Interspeech.2025-1098",
   "url": "interspeech_2025/ogura25_interspeech.html"
  },
  "zhou25c_interspeech": {
   "authors": [
    [
     "Junyu",
     "Zhou"
    ],
    [
     "Yanxiong",
     "Li"
    ],
    [
     "Haolin",
     "Yu"
    ]
   ],
   "title": "Infant Cry Emotion Recognition Using Improved ECAPA-TDNN with Multi-scale Feature Fusion and Attention Enhancement",
   "original": "1100",
   "order": 912,
   "page_count": 5,
   "abstract": [
    "Infant cry emotion recognition is crucial for parenting and medical applications. It faces many challenges, such as subtle emotional variations, noise interference, and limited data. The existing methods lack the ability to effectively integrate multi-scale features and temporal-frequency relationships. In this study, we propose a method for infant cry emotion recognition using an improved Emphasized Channel Attention, Propagation and Aggregation in Time Delay Neural Network (ECAPA-TDNN) with both multi-scale feature fusion and attention enhancement. Experiments on a public dataset show that the proposed method achieves accuracy of 82.20%, number of parameters of 1.43 MB and FLOPs of 0.32 Giga. Moreover, our method has advantage over the baseline methods in terms of accuracy. The code is at https://github.com/kkpretend/IETMA."
   ],
   "p1": 4478,
   "pn": 4482,
   "doi": "10.21437/Interspeech.2025-1100",
   "url": "interspeech_2025/zhou25c_interspeech.html"
  },
  "chou25_interspeech": {
   "authors": [
    [
     "Hsing-Hang",
     "Chou"
    ],
    [
     "Yun-Shao",
     "Lin"
    ],
    [
     "Ching-Chin",
     "Sung"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "ZSDEVC: Zero-Shot Diffusion-based Emotional Voice Conversion with Disentangled Mechanism",
   "original": "1101",
   "order": 896,
   "page_count": 5,
   "abstract": [
    "The human voice conveys not just words but also emotional states and individuality. Emotional voice conversion (EVC) modifies emotional expressions while preserving linguistic content and speaker identity, improving applications like human-machine interaction. While deep learning has advanced EVC models for specific target speakers on well-crafted emotional datasets, existing methods often face issues with emotion accuracy and speech distortion. In addition, the zero-shot scenario, in which emotion conversion is applied to unseen speakers, remains underexplored. This work introduces a novel diffusion framework with disentangled mechanisms and expressive guidance, trained on a large emotional speech dataset and evaluated on unseen speakers across in-domain and out-of-domain datasets. Experimental results show that our method produces expressive speech with high emotional accuracy, naturalness, and quality, showcasing its potential for broader EVC applications."
   ],
   "p1": 4398,
   "pn": 4402,
   "doi": "10.21437/Interspeech.2025-1101",
   "url": "interspeech_2025/chou25_interspeech.html"
  },
  "jahan25_interspeech": {
   "authors": [
    [
     "Maliha",
     "Jahan"
    ],
    [
     "Yinglun",
     "Sun"
    ],
    [
     "Priyam",
     "Mazumdar"
    ],
    [
     "Zsuzsanna",
     "Fagyal"
    ],
    [
     "Thomas",
     "Thebaud"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Laureano Moro",
     "Velazquez"
    ]
   ],
   "title": "FaiST: A Benchmark Dataset for Fairness in Speech Technology",
   "original": "1102",
   "order": 275,
   "page_count": 5,
   "abstract": [
    "Fairness in speech processing systems is a critical challenge, especially regarding performance disparities based on speakers&#x27; backgrounds. To help combat this problem, we are introducing FaiST (Fairness in Speech Technology), a novel speech dataset from American English speakers of various racial, ethnic, and national origin groups. The goal is to evaluate and mitigate possible biases across speech technologies using conversational speech from podcasts and interviews online. In FaiST&#x27;s current version, speakers self-identified as Asian American and African American, and future iterations will include other groups. White American speakers&#x27; speech was extracted from VoxCeleb, and their demographic labels were obtained online. In addition to identifiers of race, ethnicity, and national origins, FaiST is also marked for the exact instances in the conversation where self-identifications occurred. We experimented with FaiST and found racial bias in eighteen Automatic Speech Recognition systems."
   ],
   "p1": 1343,
   "pn": 1347,
   "doi": "10.21437/Interspeech.2025-1102",
   "url": "interspeech_2025/jahan25_interspeech.html"
  },
  "kc25_interspeech": {
   "authors": [
    [
     "Bhasi",
     "K.C."
    ],
    [
     "Rajeev",
     "Rajan"
    ]
   ],
   "title": "A Siamese Network-Based Framework for Voice Mimicry Proficiency Assessment Using X-Vector Embeddings",
   "original": "1103",
   "order": 340,
   "page_count": 5,
   "abstract": [
    "This paper evaluates the quality of mimicked speech by computing X-vector based speaker embeddings derived from spectral and prosodic features. These embeddings are processed through a long short term memory (LSTM)-based Siamese Network.The best mimicry artist is first identified through a perception test, and we then examine whether the Siamese Network predicts the same artist. A correct prediction is counted as a hit when the model assigns the highest similarity score (rank-1) to the artist identified by the mean opinion score (MOS). Performance evaluation is conducted on the MIMICz data set using Top-K hit rate. The results highlight the effectiveness of score-level fusion of spectral and prosodic speaker embeddings in assessing voice mimickry competency, with the fusion approach achieving a Top-1 hit rate of 72% and a Top-2 hit rate of 88%."
   ],
   "p1": 1668,
   "pn": 1672,
   "doi": "10.21437/Interspeech.2025-1103",
   "url": "interspeech_2025/kc25_interspeech.html"
  },
  "emezue25_interspeech": {
   "authors": [
    [
     "Chris",
     "Emezue"
    ],
    [
     "",
     "The NaijaVoices Community"
    ],
    [
     "Busayo",
     "Awobade"
    ],
    [
     "Abraham Toluwase",
     "Owodunni"
    ],
    [
     "Handel",
     "Emezue"
    ],
    [
     "Gloria Monica Tobechukwu",
     "Emezue"
    ],
    [
     "Nefertiti Nneoma",
     "Emezue"
    ],
    [
     "Sewade",
     "Ogun"
    ],
    [
     "Bunmi",
     "Akinremi"
    ],
    [
     "David Ifeoluwa",
     "Adelani"
    ],
    [
     "Chris",
     "Pal"
    ]
   ],
   "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages",
   "original": "1104",
   "order": 274,
   "page_count": 5,
   "abstract": [
    "The development of high-performing, robust, and reliable speech technologies depends on large, high-quality datasets. However, African languages -- including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to insufficient data. Popular voice-enabled technologies do not support any of the 2000+ African languages, limiting accessibility for circa one billion people. While previous dataset efforts exist for the target languages, they lack the scale and diversity needed for robust speech models. To bridge this gap, we introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+ speakers. We outline our unique data collection approach, analyze its acoustic diversity, and demonstrate its impact through finetuning experiments on automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices&#x27; potential to advance multilingual speech processing for African languages."
   ],
   "p1": 1338,
   "pn": 1342,
   "doi": "10.21437/Interspeech.2025-1104",
   "url": "interspeech_2025/emezue25_interspeech.html"
  },
  "dong25c_interspeech": {
   "authors": [
    [
     "Boya",
     "Dong"
    ],
    [
     "Wentao",
     "Lei"
    ],
    [
     "Li",
     "Liu"
    ]
   ],
   "title": "FFD: Fine-Finger Diffusion Model for Music to Fine-grained Finger Dance Generation",
   "original": "1105",
   "order": 39,
   "page_count": 5,
   "abstract": [
    "Finger dance is an emerging social media trend using finger gesture motions for expression. Music to finger dance generation is challenging due to its fine-grained movements. Existing music-driven methods often fail to model subtle finger motions, yielding poor performances. We propose Fine-Finger Diffusion (FFD), the first end-to-end framework for music to finger dance generation. Our method employs a diffusion model to create rhythmically aligned finger movements while ensuring motion stability. A novel detail-aware loss (DAL) enhances temporal coherence by constraining inter-frame motion fluctuations. We introduce DanceFingers-4K, the first large-scale finger dance dataset containing 4007 video clips with music-motion pairs. Comprehensive evaluations demonstrate FFD&#x27;s superiority over existing approaches across objective metrics and user study."
   ],
   "p1": 186,
   "pn": 190,
   "doi": "10.21437/Interspeech.2025-1105",
   "url": "interspeech_2025/dong25c_interspeech.html"
  },
  "guo25_interspeech": {
   "authors": [
    [
     "Yiwei",
     "Guo"
    ],
    [
     "Zhihan",
     "Li"
    ],
    [
     "Chenpeng",
     "Du"
    ],
    [
     "Hankun",
     "Wang"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
   "original": "1106",
   "order": 1024,
   "page_count": 5,
   "abstract": [
    "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework."
   ],
   "p1": 5018,
   "pn": 5022,
   "doi": "10.21437/Interspeech.2025-1106",
   "url": "interspeech_2025/guo25_interspeech.html"
  },
  "tran25_interspeech": {
   "authors": [
    [
     "Minh",
     "Tran"
    ],
    [
     "Debjyoti",
     "Paul"
    ],
    [
     "Yutong",
     "Pang"
    ],
    [
     "Laxmi",
     "Pandey"
    ],
    [
     "Jinxi",
     "Guo"
    ],
    [
     "Ke",
     "Li"
    ],
    [
     "Shun",
     "Zhang"
    ],
    [
     "Xuedong",
     "Zhang"
    ],
    [
     "Xin",
     "Lei"
    ]
   ],
   "title": "R2S: Real-to-Synthetic Representation Learning for Training Speech Recognition Models on Synthetic Data",
   "original": "1109",
   "order": 650,
   "page_count": 5,
   "abstract": [
    "We investigate the use of synthetic speech to enhance the performance of Automatic Speech Recognition (ASR) systems. While pre-trained ASR models have demonstrated impressive capabilities, their performance can still vary across different conditions and speakers. Conversely, text-to-speech technology allows for precise control over factors such as environmental noise and speaker accents, producing clean speech that poses fewer challenges for ASR systems. Building on this insight, we propose a novel method called R2S (Real-to-Synthetic), which aligns the representation spaces of real and synthetic speech. Our approach incorporates a Gradient Reversal Layer to promote invariant representations between real and synthetic speech, and a Residual-Vector Quantization module to generate pseudo-labels from synthetic speech, guiding the representations of real speech. Our experimental results on three datasets demonstrate that the proposed method can boost ASR performance by 4-5% and successfully align the representation space of real and synthetic speech. Our qualitative results further demonstrate that R2S can suppress speaker-dependent features thanks to the alignment with synthetic speech."
   ],
   "p1": 3194,
   "pn": 3198,
   "doi": "10.21437/Interspeech.2025-1109",
   "url": "interspeech_2025/tran25_interspeech.html"
  },
  "hao25_interspeech": {
   "authors": [
    [
     "Fengyuan",
     "Hao"
    ],
    [
     "Brian C. J.",
     "Moore"
    ],
    [
     "Huiyong",
     "Zhang"
    ],
    [
     "Xiaodong",
     "Li"
    ],
    [
     "Chengshi",
     "Zheng"
    ]
   ],
   "title": "L3C-DeepMFC: Low-Latency Low-Complexity Deep Marginal Feedback Cancellation with Closed-Loop Fine Tuning for Hearing Aids",
   "original": "1111",
   "order": 174,
   "page_count": 5,
   "abstract": [
    "Feedback control in hearing aids mitigates acoustic feedback caused by the coupling between the receiver and microphone. While DNN-based methods have achieved progress, they remain computationally intensive with relatively high latency. This paper introduces L3C-DeepMFC, a low-latency and low-complexity time-frequency (T-F) domain method that employs complex spectrum mapping to estimate the magnitude and phase components of the desired speech. This method integrates full- and sub-band recurrent modeling to capture spectro-temporal patterns and modifies the overlap-add method for low-latency processing. Moreover, we utilize closed-loop fine tuning with dynamically generated feedback mixtures to minimize the mismatch between training and estimation. Evaluations using the AISHELL-3 dataset confirm its competitive performance across various gains, significantly improving the maximum stable gain (MSG). Integration with traditional methods shows better performance of feedback suppression."
   ],
   "p1": 838,
   "pn": 842,
   "doi": "10.21437/Interspeech.2025-1111",
   "url": "interspeech_2025/hao25_interspeech.html"
  },
  "chen25i_interspeech": {
   "authors": [
    [
     "Youjun",
     "Chen"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Haoning",
     "Xu"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Chengxi",
     "Deng"
    ],
    [
     "Huimeng",
     "Wang"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable Emotion Recognition",
   "original": "1112",
   "order": 943,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel end-to-end LLM-empowered explainable speech emotion recognition (SER) approach. Fine-grained speech emotion descriptor (SED) features, e.g., pitch, tone and emphasis, are disentangled from HuBERT SSL representations via alternating LLM fine-tuning to joint SER-SED prediction and ASR tasks. VAE compressed HuBERT features derived via Information Bottleneck (IB) are used to adjust feature granularity. Experiments on the IEMOCAP and MELD benchmarks demonstrate that our approach consistently out performs comparable LLaMA-based SER baselines, including those using either (a) alternating multi-task fine-tuning alone or (b) feature disentanglement only. Statistically significant increase of SER unweighted accuracy by up to 4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly, emotion descriptors offer further explainability for SER."
   ],
   "p1": 4633,
   "pn": 4637,
   "doi": "10.21437/Interspeech.2025-1112",
   "url": "interspeech_2025/chen25i_interspeech.html"
  },
  "wu25g_interspeech": {
   "authors": [
    [
     "Zhichao",
     "Wu"
    ],
    [
     "Yueteng",
     "Kang"
    ],
    [
     "Songjun",
     "Cao"
    ],
    [
     "Long",
     "Ma"
    ],
    [
     "Qiulin",
     "Li"
    ],
    [
     "Qun",
     "Yang"
    ]
   ],
   "title": "MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt",
   "original": "1115",
   "order": 897,
   "page_count": 5,
   "abstract": [
    "Most existing Zero-Shot Text-To-Speech (ZS-TTS) systems generate the unseen speech based on single prompt, such as reference speech or text descriptions, which limits their flexibility. We propose a customized emotion ZS-TTS system based on multi-modal prompt. The system disentangles speech into the content, timbre, emotion and prosody, allowing emotion prompts to be provided as text, image or speech. To extract emotion information from different prompts, we propose a multi-modal prompt emotion encoder. Additionally, we introduce an prosody predictor to fit the distribution of prosody and propose an emotion consistency loss to preserve emotion information in the predicted prosody. A diffusion-based acoustic model is employed to generate the target mel-spectrogram. Both objective and subjective experiments demonstrate that our system outperforms existing systems in terms of naturalness and similarity."
   ],
   "p1": 4403,
   "pn": 4407,
   "doi": "10.21437/Interspeech.2025-1115",
   "url": "interspeech_2025/wu25g_interspeech.html"
  },
  "kondo25_interspeech": {
   "authors": [
    [
     "Yuto",
     "Kondo"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Takuhiro",
     "Kaneko"
    ]
   ],
   "title": "JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles",
   "original": "1116",
   "order": 973,
   "page_count": 5,
   "abstract": [
    "We construct Japanese Idol Speech Corpus (JIS) to advance research in speech generation AI, including text-to-speech synthesis (TTS) and voice conversion (VC). JIS will facilitate more rigorous evaluations of speaker similarity in TTS and VC systems since all speakers in JIS belong to a highly specific category: “young female live idols&quot; in Japan, and each speaker is identified by a stage name, enabling researchers to recruit listeners familiar with these idols for listening experiments. With its unique speaker attributes, JIS will foster compelling research, including generating voices tailored to listener preferences—an area not yet widely studied. JIS will be distributed free of charge to promote research in speech generation AI, with usage restricted to non-commercial, basic research. We describe the construction of JIS, provide an overview of Japanese live idol culture to support effective and ethical use of JIS, and offer a basic analysis to guide application of JIS."
   ],
   "p1": 4783,
   "pn": 4787,
   "doi": "10.21437/Interspeech.2025-1116",
   "url": "interspeech_2025/kondo25_interspeech.html"
  },
  "baihaqi25_interspeech": {
   "authors": [
    [
     "Muhammad Yeza",
     "Baihaqi"
    ],
    [
     "Angel García",
     "Contreras"
    ],
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "Rapport-Building Dialogue Strategies for Deeper Connection: Integrating Proactive Behavior, Personalization, and Aizuchi Backchannels",
   "original": "1117",
   "order": 223,
   "page_count": 5,
   "abstract": [
    "Proactive behavior, dialogue personalization, and backchannels are essential elements for enhancing rapport, fostering engagement, preventing conversation stalls, and supporting a natural dialogue flow. This study integrates these behaviors into a rapport-building dialogue strategy using the CO-STAR and few-shot frameworks to prompt large language models (LLMs) within a human-robot interaction. The success of the integration was assessed through metrics such as conversation stall frequency, dialogue similarity, and backchannel usage by the robot. We then observed the effect of integrating these elements on participant-observed behavioral outcomes, including participant backchannel usage, self-disclosure, and subjective questionnaire responses. Experimental results indicated that incorporating these behaviors into the dialogue strategy positively impacts human-robot rapport both behaviorally and subjectively."
   ],
   "p1": 1083,
   "pn": 1087,
   "doi": "10.21437/Interspeech.2025-1117",
   "url": "interspeech_2025/baihaqi25_interspeech.html"
  },
  "jia25_interspeech": {
   "authors": [
    [
     "Kaichen",
     "Jia"
    ],
    [
     "Jinpeng",
     "Li"
    ],
    [
     "Ke",
     "Li"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ]
   ],
   "title": "Whisper-Based Multilingual Alzheimer's Disease Detection and Improvements for Low-Resource Language",
   "original": "1118",
   "order": 116,
   "page_count": 5,
   "abstract": [
    "Alzheimer&#x27;s Disease (AD) poses a growing global health challenge due to population aging. Using spontaneous speech for the early diagnosis of AD has emerged as a notable area of research. In response to the global trend of AD, our study proposes a speech-based multilingual AD detection method. In our study, we utilize Whisper for transfer learning to build a multilingual pre-trained AD diagnostic model that achieves 81.38% accuracy on a test set comprising multiple languages. To enhance low-resource language performance, we fine-tune the pre-trained model with multilingual data and full transcripts as prompts, achieving a 4-7% accuracy improvement. Additionally, we incorporate the speaker&#x27;s background information, enhancing the accuracy of low-resource languages by 11-13%. The results demonstrate the validity of our work in multilingual Alzheimer&#x27;s detection tasks and also illustrate the feasibility of our approach in addressing the global need for Alzheimer&#x27;s detection."
   ],
   "p1": 549,
   "pn": 553,
   "doi": "10.21437/Interspeech.2025-1118",
   "url": "interspeech_2025/jia25_interspeech.html"
  },
  "hu25k_interspeech": {
   "authors": [
    [
     "Yuchen",
     "Hu"
    ],
    [
     "Yu",
     "Gu"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Rilin",
     "Chen"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Video-to-Audio Generation with Fine-grained Temporal Semantics",
   "original": "1119",
   "order": 861,
   "page_count": 5,
   "abstract": [
    "With recent advances of AIGC, video generation gains a surge of research interest in both academia and industry (e.g., Sora). However, it remains a challenge to produce temporally aligned audio to match the generated video, considering its complex semantic information. In this work, inspired by the recent success of text-to-audio generation, we investigate the video-to-audio (VTA) generation based on latent diffusion model (LDM). Similar to pioneering explorations, our preliminary results show great potentials of LDM in VTA task, but challenges still exist in temporal consistence. To this end, we propose to enhance the temporal alignment of VTA with frame-level semantic information. With the popular grounding segment anything model (Grounding SAM), we extract the fine-grained semantics in video frames to enable VTA to produce better-aligned audio signal. Experiments show its effectiveness in both objective and subjective evaluation, improving both audio quality and temporal alignment."
   ],
   "p1": 4223,
   "pn": 4227,
   "doi": "10.21437/Interspeech.2025-1119",
   "url": "interspeech_2025/hu25k_interspeech.html"
  },
  "xu25h_interspeech": {
   "authors": [
    [
     "Anqi",
     "Xu"
    ],
    [
     "Yu-yin",
     "Hsu"
    ]
   ],
   "title": "When focus shapes the flow: prosodic restructuring in Mandarin complex nominals",
   "original": "1121",
   "order": 75,
   "page_count": 5,
   "abstract": [
    "While extensive studies have explored acoustic focus realization in Mandarin, little is known about how focus affects the prosodic phrasing of Mandarin complex nominals. This study examined how contrastive focus influences syllable duration of Mandarin numeral-classifier-noun phrases. Using a mini-dialogue paradigm, we elicited contrastive focus of different spans, alongside a baseline no-focus condition. Two production experiments revealed that when focus was placed on the numeral, the default prosodic grouping was disrupted, dissimilar to when focus encompasses the entire phrase or in neutral contexts, with tonal factors amplifying the reorganization. Our results indicate that prosodic organization in Mandarin is shaped by tones, morphosyntactic structures, focus marking and their interplay. Crucially, the results challenge rigid models of boundary phrasing and disyllabic footing, highlighting a multilevel interaction among phonetics, prosodic phrasing, and syntax in tonal languages."
   ],
   "p1": 344,
   "pn": 348,
   "doi": "10.21437/Interspeech.2025-1121",
   "url": "interspeech_2025/xu25h_interspeech.html"
  },
  "kawamura25_interspeech": {
   "authors": [
    [
     "Masaya",
     "Kawamura"
    ],
    [
     "Takuya",
     "Hasumi"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ]
   ],
   "title": "BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing",
   "original": "1122",
   "order": 1129,
   "page_count": 5,
   "abstract": [
    "This paper proposes a highly compact, lightweight text-to-speech (TTS) model for on-device applications. To reduce the model size, the proposed model introduces two techniques. First, we introduce quantization-aware training (QAT), which quantizes model parameters during training to as low as 1.58-bit. In this case, most of 32-bit model parameters are quantized to ternary values {-1, 0, 1}. Second, we propose a method named weight indexing. In this method, we save a group of 1.58-bit weights as a single int8 index. This allows for efficient storage of model parameters, even on hardware that treats values in units of 8-bit. Experimental results demonstrate that the proposed method achieved 83 % reduction in model size, while outperforming the baseline of similar model size without quantization in synthesis quality."
   ],
   "p1": 5538,
   "pn": 5542,
   "doi": "10.21437/Interspeech.2025-1122",
   "url": "interspeech_2025/kawamura25_interspeech.html"
  },
  "miyahara25_interspeech": {
   "authors": [
    [
     "Genzo",
     "Miyahara"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Akihiro",
     "Tamura"
    ]
   ],
   "title": "Stuttering Detection Based on Self-Attention Weights of Temporal Acoustic Vector Sequence",
   "original": "1124",
   "order": 1081,
   "page_count": 5,
   "abstract": [
    "Stuttering detection is gaining attention to enable automatic monitoring of the condition of persons who stutter and to develop inclusive automatic speech recognition. Stuttering has various symptoms, i.e., repetitions of sounds, syllables, or words, prolongation of sounds, blocks, and interjections, and the acoustic properties are highly individual. Although detection performances have been improved by introducing deep learning techniques and creating larger datasets of stuttered speech, the datasets are still much smaller than those of normal speech, which leads to overfitting of the models. We propose a self-attention weight feature of a temporal acoustic vector sequence. This feature can efficiently extract the temporal structure specific to stuttered speech with less regard for the varied acoustic properties. Experimental results showed that a model utilizing multi-layer self-attention weight features of wav2vec 2.0 outperformed a previous attention-based model using wav2vec 2.0."
   ],
   "p1": 5298,
   "pn": 5302,
   "doi": "10.21437/Interspeech.2025-1124",
   "url": "interspeech_2025/miyahara25_interspeech.html"
  },
  "guo25b_interspeech": {
   "authors": [
    [
     "Zhe-chen",
     "Guo"
    ],
    [
     "Bharath",
     "Chandrasekaran"
    ]
   ],
   "title": "Extended High-frequency Cues to Phoneme Recognition: Insights from ASR",
   "original": "1125",
   "order": 214,
   "page_count": 5,
   "abstract": [
    "There is emerging evidence that extended high frequencies (EHFs; &gt;8 kHz) improve speech perception in noise. Yet, the mechanisms underlying this benefit remain unclear. We investigated whether EHFs contribute to phoneme recognition using an automatic speech recognition (ASR) model. A neural network model was trained to decode phonemes from cochleagrams of broadband speech and speech low-pass filtered at 8 or 6 kHz in quiet and masked conditions with varying target-to-masker ratios (TMRs) and target-masker spatial separations. Compared with filtered speech, broadband speech improved phoneme recognition accuracy in masked conditions, particularly at lower TMRs, but showed no benefit in quiet. Removing EHFs increased the probability of the model omitting a phoneme more for consonants than vowels. The findings suggest that the EHF benefit in adverse conditions may partly arise from enhanced phoneme processing, highlighting the potential of improving audiometry and ASR by including EHFs."
   ],
   "p1": 1038,
   "pn": 1042,
   "doi": "10.21437/Interspeech.2025-1125",
   "url": "interspeech_2025/guo25b_interspeech.html"
  },
  "xu25i_interspeech": {
   "authors": [
    [
     "Jingjing",
     "Xu"
    ],
    [
     "Zijian",
     "Yang"
    ],
    [
     "Albert",
     "Zeyer"
    ],
    [
     "Eugen",
     "Beck"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Dynamic Acoustic Model Architecture Optimization in Training for ASR",
   "original": "1126",
   "order": 737,
   "page_count": 5,
   "abstract": [
    "Architecture design is inherently complex. Existing approaches rely on either handcrafted rules, which demand extensive empirical expertise, or automated methods like neural architecture search, which are computationally intensive. In this paper, we introduce DMAO, an architecture optimization framework that employs a grow-and-drop strategy to automatically reallocate parameters during training.This reallocation shifts resources from less-utilized areas to those parts of the model where they are most beneficial. Notably, DMAO only introduces negligible training overhead at a given model complexity. We evaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and Switchboard datasets. The results show that, using the same amount of training resources, our proposed DMAO consistently improves WER by up to ~6% relatively across various architectures, model sizes, and datasets. Furthermore, we analyze the pattern of parameter redistribution and uncover insightful findings."
   ],
   "p1": 3603,
   "pn": 3607,
   "doi": "10.21437/Interspeech.2025-1126",
   "url": "interspeech_2025/xu25i_interspeech.html"
  },
  "halpern25_interspeech": {
   "authors": [
    [
     "Bence Mark",
     "Halpern"
    ],
    [
     "Thomas",
     "Tienkamp"
    ],
    [
     "Teja",
     "Rebernik"
    ],
    [
     "Rob J.J.H.",
     "van Son"
    ],
    [
     "Martijn",
     "Wieling"
    ],
    [
     "Defne",
     "Abur"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Relationship between objective and subjective perceptual measures of speech in individuals with head and neck cancer",
   "original": "1127",
   "order": 763,
   "page_count": 5,
   "abstract": [
    "Meaningful speech assessment is vital in clinical phonetics and therapy monitoring. This study examined the link between perceptual speech assessments and objective acoustic measures in a large head and neck cancer (HNC) dataset. Trained listeners provided ratings of intelligibility, articulation, voice quality, phonation, speech rate, nasality, and background noise on speech. Strong correlations were found between subjective intelligibility, articulation, and voice quality, likely due to a shared underlying cause of speech symptoms in our speaker population. Objective measures of intelligibility and speech rate aligned with their subjective counterpart. Our results suggest that a single intelligibility measure may be sufficient for the clinical monitoring of speakers treated for HNC using concomitant chemoradiation."
   ],
   "p1": 3733,
   "pn": 3737,
   "doi": "10.21437/Interspeech.2025-1127",
   "url": "interspeech_2025/halpern25_interspeech.html"
  },
  "shi25b_interspeech": {
   "authors": [
    [
     "Yu-Fei",
     "Shi"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Universal Preference-Score-based Pairwise Speech Quality Assessment",
   "original": "1131",
   "order": 480,
   "page_count": 5,
   "abstract": [
    "To compare the performance of two speech generation systems, one of the most effective approaches is estimating the preference score between their generated speech. This paper proposes a novel universal preference-score-based pairwise speech quality assessment (UPPSQA) model, aimed at predicting the preference score between paired speech samples to determine which one has better quality. The model first predicts the absolute mean opinion score (MOS) for the two speech samples separately, and then aggregates them into a relative preference score using a preference function. To address the scarcity of preference data, we also construct a new pairwise speech dataset based on a MOS dataset for experiments. Experimental results confirm that, whether in training scenarios with different data types and label conditions, or in both in-domain and out-of-domain test scenarios, the prediction accuracy of UPPSQA outperforms that of the baseline models, demonstrating its universality."
   ],
   "p1": 2345,
   "pn": 2349,
   "doi": "10.21437/Interspeech.2025-1131",
   "url": "interspeech_2025/shi25b_interspeech.html"
  },
  "narain25_interspeech": {
   "authors": [
    [
     "Jaya",
     "Narain"
    ],
    [
     "Vasudha",
     "Kowtha"
    ],
    [
     "Colin",
     "Lea"
    ],
    [
     "Lauren",
     "Tooley"
    ],
    [
     "Dianna",
     "Yee"
    ],
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Zifang",
     "Huang"
    ],
    [
     "Miquel",
     "Espi Marques"
    ],
    [
     "Jon",
     "Huang"
    ],
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Shirley",
     "Ren"
    ]
   ],
   "title": "Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect",
   "original": "1133",
   "order": 942,
   "page_count": 5,
   "abstract": [
    "Perceptual voice quality dimensions describe key characteristics of atypical speech and other speech modulations. Here we develop and evaluate voice quality models for seven voice and speech dimensions (intelligibility, imprecise consonants, harsh voice, naturalness, monoloudness, monopitch, and breathiness). Probes were trained on the public Speech Accessibility (SAP) project dataset with 11,184 samples from 434 speakers, using embeddings from frozen pre-trained models as features. We found that our probes had both strong performance and strong generalization across speech elicitation categories in the SAP dataset. We further validated zero-shot performance on additional datasets, encompassing unseen languages and tasks: Italian atypical speech, English atypical speech, and affective speech. The strong zero-shot performance and the interpretability of results across an array of evaluations suggests the utility of using voice quality dimensions in speaking style-related tasks."
   ],
   "p1": 4628,
   "pn": 4632,
   "doi": "10.21437/Interspeech.2025-1133",
   "url": "interspeech_2025/narain25_interspeech.html"
  },
  "lin25e_interspeech": {
   "authors": [
    [
     "Yi",
     "Lin"
    ],
    [
     "Shumeng",
     "Ni"
    ],
    [
     "Yangfan",
     "Lu"
    ]
   ],
   "title": "Age-related changes in multisensory integration of emotions in an audiovisual face-prosody-semantics Stroop task",
   "original": "1136",
   "order": 680,
   "page_count": 5,
   "abstract": [
    "The present study examined age-related changes in multisensory integration of emotions across facial, prosodic, and semantic channels. Older and younger adults performed an audiovisual Stroop task, in which happy or sad emotions were expressed simultaneously through the three channels. They selectively attended to one of the emotional channels while ignoring congruent or incongruent emotions from other channels. Results indicated that older adults showed an overall decline in emotion integration with greater deficits in the nonverbal channels (particularly prosody) compared to verbal semantics. These channel-specific patterns of age differences were more prominent when information was incongruent across channels compared to the cross-channel congruent condition. To sum up, age differences in multisensory emotion perception are shaped by the complex interactions between channel asymmetry and information congruity, which may be linked to age-related declines in cognitive control."
   ],
   "p1": 3344,
   "pn": 3348,
   "doi": "10.21437/Interspeech.2025-1136",
   "url": "interspeech_2025/lin25e_interspeech.html"
  },
  "hai25_interspeech": {
   "authors": [
    [
     "Jiarui",
     "Hai"
    ],
    [
     "Yong",
     "Xu"
    ],
    [
     "Hao",
     "Zhang"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Helin",
     "Wang"
    ],
    [
     "Mounya",
     "Elhilali"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer",
   "original": "1137",
   "order": 863,
   "page_count": 5,
   "abstract": [
    "We introduce EzAudio, a text-to-audio (T2A) generation framework designed to produce high-quality, natural-sounding sound effects. Core designs include: (1) We propose EzAudio-DiT, an optimized Diffusion Transformer (DiT) designed for audio latent representations, improving convergence speed, as well as parameter and memory efficiency. (2) We apply a classifier-free guidance (CFG) rescaling technique to mitigate fidelity loss at higher CFG scores and enhancing prompt adherence without compromising audio quality. (3) We propose a synthetic caption generation strategy leveraging recent advances in audio understanding and LLMs to enhance T2A pretraining. We show that EzAudio, with its computationally efficient architecture and fast convergence, is a competitive open-source model that excels in both objective and subjective evaluations by delivering highly realistic listening experiences."
   ],
   "p1": 4233,
   "pn": 4237,
   "doi": "10.21437/Interspeech.2025-1137",
   "url": "interspeech_2025/hai25_interspeech.html"
  },
  "song25_interspeech": {
   "authors": [
    [
     "Yuchen",
     "Song"
    ],
    [
     "Yucong",
     "Zhang"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "Exploring Pre-trained models on Ultrasound Modeling for Mice Autism Detection with Uniform Filter Bank and Attentive Scoring",
   "original": "1139",
   "order": 349,
   "page_count": 5,
   "abstract": [
    "Genetically engineered mice, whose behaviors resemble those of individuals with Autism Spectrum Disorder (ASD), serve as valuable models for studying ASD through ultrasound vocalization (USV) analysis. In this paper, we investigate the effectiveness of pre-trained models in learning features of the USVs by fine-tuning. To bridge the gap between the pre-trained model and the inductive bias of the ultrasonic signal, we design a uniformly-spaced filter bank to reduce the dimension in the frequency domain. The extracted filter-bank energies of the ultrasonic spectrogram form a pseudo-spectrogram for pre-trained models. In the back-end, we employ an attentive frame-wise scoring method for classification, resulting in a comprehensive judgment. Experimental results demonstrate the effectiveness of our approach, achieving a segment-level Unweighted Average Recall (UAR) of 0.729 and a subject-level UAR of 0.882 on the validation set provided by the MADUV 2025 Challenge."
   ],
   "p1": 1713,
   "pn": 1717,
   "doi": "10.21437/Interspeech.2025-1139",
   "url": "interspeech_2025/song25_interspeech.html"
  },
  "singh25b_interspeech": {
   "authors": [
    [
     "Prabhav",
     "Singh"
    ],
    [
     "Jesus",
     "Villalba"
    ]
   ],
   "title": "EmoJudge: LLM Based Post-Hoc Refinement for Multimodal Speech Emotion Recognition",
   "original": "1141",
   "order": 957,
   "page_count": 5,
   "abstract": [
    "In SER, a significant challenge lies in building systems that can accurately interpret emotions in naturalistic conditions. To address this, we present EMOJUDGE, our submission to the SER in Naturalistic Conditions Challenge. For the categorical SER task, we propose a novel LLM-refined multimodal approach, while for the dimensional SER task, we propose a robust multimodal architecture. In both submissions, WavLM-Large is combined with attentive pooling aided by residual networks to extract acoustic features. For text, RoBERTa-Large captures linguistic nuances. Experimentation identifies late fusion with logistic regression as the optimal method for integrating modalities. For the categorical challenge, our novel contribution includes using transcripts, speaker indicators, and audio descriptions as input to an LLM for post-hoc correction of conflicting predictions. Results demonstrate improvements over the baseline in both tasks, highlighting the effectiveness of our proposed approach."
   ],
   "p1": 4703,
   "pn": 4707,
   "doi": "10.21437/Interspeech.2025-1141",
   "url": "interspeech_2025/singh25b_interspeech.html"
  },
  "zhou25d_interspeech": {
   "authors": [
    [
     "Dengjian",
     "Zhou"
    ],
    [
     "Jianghan",
     "Hai"
    ],
    [
     "Sijia",
     "Liao"
    ],
    [
     "Yue Ivan",
     "Wu"
    ],
    [
     "Kainam Thomas",
     "Wong"
    ],
    [
     "Xiujuan",
     "Zheng"
    ]
   ],
   "title": "Acoustic Detection of UAV Abnormality Using One Ground-Based Acoustic Vector Sensor",
   "original": "1142",
   "order": 690,
   "page_count": 5,
   "abstract": [
    "Defective unmanned aerial vehicles (UAVs), like other malfunctioning machinery, often emit abnormal sounds. Early detection of such acoustic abnormalities enables timely countermeasures to prevent equipment loss and human casualties. This paper proposes a deep learning scheme to passively detect abnormal sounds emitted from UAVs, using only a single ground-based acoustic vector sensor (AVS). This is because lone AVS measured sound intensity spectrogram can already provide information on both the sound source’s polar-azimuthal direction and abnormality time-frequency signature. This paper integrates a &quot;convolutional autoencoder&quot; with an &quot;attention module&quot;, for UAV abnormality detection. Field experiments, with a UAV in flight and a single AVS on the ground, validate the effectiveness of the proposed method, achieving an F1-score (a machine-learning accuracy metric) exceeding 95% for successful anomaly detection."
   ],
   "p1": 3394,
   "pn": 3398,
   "doi": "10.21437/Interspeech.2025-1142",
   "url": "interspeech_2025/zhou25d_interspeech.html"
  },
  "yin25_interspeech": {
   "authors": [
    [
     "Han",
     "Yin"
    ],
    [
     "Yang",
     "Xiao"
    ],
    [
     "Rohan Kumar",
     "Das"
    ],
    [
     "Jisheng",
     "Bai"
    ],
    [
     "Haohe",
     "Liu"
    ],
    [
     "Wenwu",
     "Wang"
    ],
    [
     "Mark D",
     "Plumbley"
    ]
   ],
   "title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection",
   "original": "1143",
   "order": 42,
   "page_count": 5,
   "abstract": [
    "Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains."
   ],
   "p1": 201,
   "pn": 205,
   "doi": "10.21437/Interspeech.2025-1143",
   "url": "interspeech_2025/yin25_interspeech.html"
  },
  "kim25n_interspeech": {
   "authors": [
    [
     "Minu",
     "Kim"
    ],
    [
     "Kangwook",
     "Jang"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification with Parallel Joint Learning of Speech Enhancement and Noise Extraction",
   "original": "1145",
   "order": 227,
   "page_count": 5,
   "abstract": [
    "Noise-robust speaker verification leverages joint learning of speech enhancement (SE) and speaker verification (SV) to improve robustness. However, prevailing approaches rely on implicit noise suppression, which struggles to separate noise from speaker characteristics as they do not explicitly distinguish noise from speech during training. Although integrating SE and SV helps, it remains limited in handling noise effectively. Meanwhile, recent SE studies suggest that explicitly modeling noise, rather than merely suppressing it, enhances noise resilience. Reflecting this, we propose ParaNoise-SV, with dual U-Nets combining a noise extraction (NE) network and a speech enhancement (SE) network. The NE U-Net explicitly models noise, while the SE U-Net refines speech with guidance from NE through parallel connections, preserving speaker-relevant features. Experimental results show that ParaNoise-SV achieves a relatively 8.4% lower equal error rate (EER) than previous joint SE-SV models."
   ],
   "p1": 1103,
   "pn": 1107,
   "doi": "10.21437/Interspeech.2025-1145",
   "url": "interspeech_2025/kim25n_interspeech.html"
  },
  "hasumi25_interspeech": {
   "authors": [
    [
     "Takuya",
     "Hasumi"
    ],
    [
     "Yusuke",
     "Fujita"
    ]
   ],
   "title": "DnR-nonverbal: Cinematic Audio Source Separation DatasetContaining Non-Verbal Sounds",
   "original": "1148",
   "order": 1019,
   "page_count": 5,
   "abstract": [
    "We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio."
   ],
   "p1": 4993,
   "pn": 4997,
   "doi": "10.21437/Interspeech.2025-1148",
   "url": "interspeech_2025/hasumi25_interspeech.html"
  },
  "broughton25_interspeech": {
   "authors": [
    [
     "Samuel J.",
     "Broughton"
    ],
    [
     "Lahiru",
     "Samarakoon"
    ]
   ],
   "title": "Pushing the Limits of End-to-End Diarization",
   "original": "1150",
   "order": 1064,
   "page_count": 5,
   "abstract": [
    "In this paper, we present state-of-the-art diarization error rates (DERs) on multiple publicly available datasets, including AliMeeting-far, AliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging EEND-TA, a single unified non-autoregressive model for end-to-end speaker diarization, we achieve new benchmark results, most notably a DER of 14.49% on DIHARD III. Our approach scales pre-training through 8-speaker simulation mixtures, ensuring each generated speaker mixture configuration is sufficiently represented. These experiments highlight that EEND-based architectures possess a greater capacity for learning than previously explored, surpassing many existing diarization solutions while maintaining efficient speeds during inference."
   ],
   "p1": 5218,
   "pn": 5222,
   "doi": "10.21437/Interspeech.2025-1150",
   "url": "interspeech_2025/broughton25_interspeech.html"
  },
  "zhang25i_interspeech": {
   "authors": [
    [
     "Yubin",
     "Zhang"
    ],
    [
     "Prakash",
     "Kumar"
    ],
    [
     "Ye",
     "Tian"
    ],
    [
     "Ziwei",
     "Zhao"
    ],
    [
     "Xuan",
     "Shi"
    ],
    [
     "Kevin",
     "Huang"
    ],
    [
     "Kevin",
     "Lee"
    ],
    [
     "Haley",
     "Hsu"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Krishna",
     "Nayak"
    ],
    [
     "Louis",
     "Goldstein"
    ]
   ],
   "title": "Co-registration of real-time MRI and respiration for speech research ",
   "original": "1151",
   "order": 203,
   "page_count": 5,
   "abstract": [
    "In the past, research on speech respiration and articulation has mostly been conducted independently of each other. Due to the limited availability of methods for simultaneously measuring respiration and articulation, the coordination of these two subsystems to encode linguistic structures is yet to be fully examined. In this study, we present a protocol for performing co-registration of real-time vocal tract MRI and respiratory plethysmography for speech research. The procedures for data acquisition and analysis are first presented. Then, the preliminary results from two sample speech production experiments are reported. The first experiment is on the timing between articulation and respiration, and the second one examines articulatory and respiratory aspects of linguistic focus encoding. The implications of the method for speech research are discussed."
   ],
   "p1": 983,
   "pn": 987,
   "doi": "10.21437/Interspeech.2025-1151",
   "url": "interspeech_2025/zhang25i_interspeech.html"
  },
  "zhong25_interspeech": {
   "authors": [
    [
     "Tao",
     "Zhong"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition",
   "original": "1152",
   "order": 428,
   "page_count": 5,
   "abstract": [
    "Accurate recognition of dysarthric and elderly speech remains challenging to date. While privacy concerns have driven a shift from centralized approaches to federated learning (FL) to ensure data confidentiality, this further exacerbates the challenges of data scarcity, imbalanced data distribution and speaker heterogeneity. To this end, this paper conducts a systematic investigation of regularized FL techniques for privacy-preserving dysarthric and elderly speech recognition, addressing different levels of the FL process by 1) parameter-based, 2) embedding-based and 3) novel loss-based regularization. Experiments on the benchmark UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest that regularized FL systems consistently outperform the baseline FedAvg system by statistically significant WER reductions of up to 0.55% absolute (2.13% relative). Further increasing communication frequency to one exchange per batch approaches centralized training performance."
   ],
   "p1": 2103,
   "pn": 2107,
   "doi": "10.21437/Interspeech.2025-1152",
   "url": "interspeech_2025/zhong25_interspeech.html"
  },
  "ji25_interspeech": {
   "authors": [
    [
     "Jinxin",
     "Ji"
    ],
    [
     "Yiying",
     "Hu"
    ],
    [
     "Xiaohu",
     "Yang"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Acoustic Features of Mandarin Tone Production in Noise: A Comparison Between Chinese Native Speakers and Korean L2 Learners",
   "original": "1159",
   "order": 906,
   "page_count": 5,
   "abstract": [
    "Research on Lombard speech produced by Chinese learners remains underdeveloped. This study investigates how native Chinese Mandarin speakers and Korean second language (L2) learners adjust the production of lexical tones under noise conditions. Fifty-two participants (26 Chinese native speakers and 26 higher-level Korean L2 learners) produced 28 syllables under three conditions: babble noise 80 dB SPL, white noise 80 dB SPL, and quiet. Acoustic features, including intensity, duration and F0 contours were analysed. Results indicated that Korean L2 learners significantly increased both intensity and duration across all four tones in the two noise conditions, whereas Chinese native speakers significantly increased durations only for Tone 3 and Tone 4 under white noise. Both groups adjusted F0 contours for all tones in noise, with a notable group difference in the F0 contour of Tone 3. These findings were discussed in association with feedback control theory in second language learners, showing that L2 learners rely more heavily on feedback control and are more detrimentally affected by noise conditions than native speakers."
   ],
   "p1": 4448,
   "pn": 4452,
   "doi": "10.21437/Interspeech.2025-1159",
   "url": "interspeech_2025/ji25_interspeech.html"
  },
  "pham25_interspeech": {
   "authors": [
    [
     "Long-Khanh",
     "Pham"
    ],
    [
     "Thanh V. T.",
     "Tran"
    ],
    [
     "Minh-Tan",
     "Pham"
    ],
    [
     "Van",
     "Nguyen"
    ]
   ],
   "title": "RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling",
   "original": "1160",
   "order": 1144,
   "page_count": 5,
   "abstract": [
    "Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues, faces challenges in accuracy and naturalness due to limited supervision in capturing linguistic content, accents, and prosody. In this paper, we propose RESOUND, a novel L2S system that generates intelligible and expressive speech from silent talking face videos. Leveraging source-filter theory, our method involves two components: an acoustic path to predict prosody and a semantic path to extract linguistic features. This separation simplifies learning, allowing independent optimization of each representation. Additionally, we enhance performance by integrating speech units, a proven unsupervised speech representation technique, into waveform generation alongside mel-spectrograms. This allows RESOUND to synthesize prosodic speech while preserving content and speaker identity. Experiments conducted on two standard L2S benchmarks confirm the effectiveness of the proposed method across various metrics."
   ],
   "p1": 5613,
   "pn": 5617,
   "doi": "10.21437/Interspeech.2025-1160",
   "url": "interspeech_2025/pham25_interspeech.html"
  },
  "bendom25_interspeech": {
   "authors": [
    [
     "Itay",
     "Ben-Dom"
    ],
    [
     "Catherine I.",
     "Watson"
    ],
    [
     "Clare M.",
     "McCann"
    ]
   ],
   "title": "Introducing EMOPARKNZ: the Emotional Speech Database from New Zealand English Speakers with Parkinson’s Disease",
   "original": "1162",
   "order": 382,
   "page_count": 5,
   "abstract": [
    "This paper introduces EMOPARKNZ, a new speech database designed to study emotional speech production in New Zealand English speakers with Parkinson&#x27;s disease. The database features 13 speakers (7 male, 6 female) and 1,950 speech recordings expressing five primary emotions: excited, angry, happy, sad, and neutral. The design considerations, participant selection, and recording procedures are detailed. Acoustic analysis of the database revealed significant differences in fundamental frequency, intensity, and speech rate across emotional states. A preliminary perception test with 22 New Zealand English listeners achieved 63% classification accuracy across the five emotions, aligning with the performance reported for similar emotional speech databases. The results indicate EMOPARKNZ is a valuable resource for advancing research in emotion communication for individuals with Parkinson&#x27;s disease and speech emotion classification."
   ],
   "p1": 1873,
   "pn": 1877,
   "doi": "10.21437/Interspeech.2025-1162",
   "url": "interspeech_2025/bendom25_interspeech.html"
  },
  "feng25_interspeech": {
   "authors": [
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Thanathai",
     "Lertpetchpun"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Developing a Top-tier Framework in Naturalistic Conditions Challenge for Categorized Emotion Prediction: From Speech Foundation Models and Learning Objective to Data Augmentation and Engineering Choices",
   "original": "1163",
   "order": 945,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER), particularly for naturally expressed emotions, remains a challenging computational task. Key challenges include the inherent subjectivity in emotion annotation and the imbalanced distribution of emotion labels in datasets. This paper introduces the SAILER system developed for participation in the INTERSPEECH 2025 Emotion Recognition Challenge (Task 1). The challenge dataset, which contains natural emotional speech from podcasts, serves as a valuable resource for studying imbalanced and subjective emotion annotations. Our system is designed to be simple, reproducible, and effective, highlighting critical choices in modeling, learning objectives, data augmentation, and engineering choices. Results show that even a single system (without ensembling) can outperform more than 95% of the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of three systems further improves performance, achieving a competitively ranked score (top-3 performing team). Our model is at: https://github.com/tiantiaf0627/vox-profile-release."
   ],
   "p1": 4643,
   "pn": 4647,
   "doi": "10.21437/Interspeech.2025-1163",
   "url": "interspeech_2025/feng25_interspeech.html"
  },
  "subramanian25_interspeech": {
   "authors": [
    [
     "Aswin Shanmugam",
     "Subramanian"
    ],
    [
     "Harveen",
     "Chadha"
    ],
    [
     "Vikas",
     "Joshi"
    ],
    [
     "Shubham",
     "Bansal"
    ],
    [
     "Jian",
     "Xue"
    ],
    [
     "Rupeshkumar",
     "Mehta"
    ],
    [
     "Jinyu",
     "Li"
    ]
   ],
   "title": "Length Aware Speech Translation for Video Dubbing",
   "original": "1166",
   "order": 3,
   "page_count": 5,
   "abstract": [
    "In video dubbing, aligning translated audio with the source audio is a significant challenge. Our focus is on achieving this efficiently, tailored for real-time, on-device video dubbing scenarios. We developed a phoneme-based end-to-end length-sensitive speech translation (LSST) model, which generates translations of varying lengths - short, normal, and long - using predefined tags. Additionally, we introduced length-aware beam search (LABS), an efficient approach to generate translations of different lengths in a single decoding pass. This approach maintained comparable BLEU scores compared to a baseline without length awareness while significantly enhancing synchronization quality between source and target audio, achieving a mean opinion score (MOS) gain of 0.34 for Spanish and 0.65 for Korean, respectively."
   ],
   "p1": 6,
   "pn": 10,
   "doi": "10.21437/Interspeech.2025-1166",
   "url": "interspeech_2025/subramanian25_interspeech.html"
  },
  "fan25b_interspeech": {
   "authors": [
    [
     "Zixia",
     "Fan"
    ],
    [
     "Ronny",
     "Ibrahim"
    ],
    [
     "Joshua",
     "Penney"
    ],
    [
     "Felicity",
     "Cox"
    ]
   ],
   "title": "Creaky Voice Facilitates More Efficient Phonological Processing of Mandarin Tone 3",
   "original": "1168",
   "order": 267,
   "page_count": 5,
   "abstract": [
    "Though tone recognition is primarily influenced by pitch cues, phonation type may also play a role. For example, creaky voice, a non-modal phonation, can enhance Mandarin Tone 3 (T3) recognition. While most studies explore creaky voice in tone recognition behaviorally, its neural processing remains unclear. This study aims to investigate tone processing using Electroencephalography to examine how the brain integrates pitch and creaky voice to process T3. We measured Mismatch Negativity (MMN) responses of 10 native Mandarin speakers exposed to an oddball paradigm with real-word single syllable stimuli in two conditions: creak and non-creak. Results showed stronger MMN responses in the creak condition, with larger mean MMN amplitude, shorter peak MMN latency, and larger negative peak MMN amplitude. These findings suggest that creaky voice enhances T3 distinctiveness and facilitates more efficient phonological processing, contributing to a deeper understanding of tone processing mechanisms."
   ],
   "p1": 1303,
   "pn": 1307,
   "doi": "10.21437/Interspeech.2025-1168",
   "url": "interspeech_2025/fan25b_interspeech.html"
  },
  "wang25h_interspeech": {
   "authors": [
    [
     "Zheng",
     "Wang"
    ],
    [
     "Xiaobin",
     "Rong"
    ],
    [
     "Yu",
     "Sun"
    ],
    [
     "Tianchi",
     "Sun"
    ],
    [
     "Zhibin",
     "Lin"
    ],
    [
     "Jing",
     "Lu"
    ]
   ],
   "title": "A Lightweight Hybrid Dual Channel Speech Enhancement System under Low-SNR Conditions",
   "original": "1171",
   "order": 242,
   "page_count": 5,
   "abstract": [
    "Although deep learning based multi-channel speech enhancement has achieved significant advancements, its practical deployment is often limited by constrained computational resources, particularly in low signal-to-noise ratio (SNR) conditions. In this paper, we propose a lightweight hybrid dual-channel speech enhancement system that combines independent vector analysis (IVA) with a modified version of the dual-channel grouped temporal convolutional recurrent network (GTCRN). IVA functions as a coarse estimator, providing auxiliary information for both speech and noise, while the modified GTCRN further refines the speech quality. We investigate several modifications to ensure the comprehensive utilization of both original and auxiliary information. Experimental results demonstrate the effectiveness of the proposed system, achieving enhanced speech with minimal parameters and low computational complexity."
   ],
   "p1": 1178,
   "pn": 1182,
   "doi": "10.21437/Interspeech.2025-1171",
   "url": "interspeech_2025/wang25h_interspeech.html"
  },
  "dong25d_interspeech": {
   "authors": [
    [
     "Xiao",
     "Dong"
    ],
    [
     "Fengming",
     "Liu"
    ],
    [
     "Chien-Jer",
     "Lin"
    ],
    [
     "Monica",
     "Nesbitt"
    ],
    [
     "Shuju",
     "Shi"
    ]
   ],
   "title": "Neutral Tone Variation in Beijing Mandarin: Is Neutral Tone Toneless?",
   "original": "1173",
   "order": 145,
   "page_count": 5,
   "abstract": [
    "Neutral tone (NT) is a distinctive feature of Beijing Mandarin, traditionally described as toneless and entirely dependent on the preceding tone. Recent studies suggest that NT may retain specific phonetic targets and exists on a continuum of reduction, challenging the strict neutral versus full-tone dichotomy. This study examines the phonetic realization of NT as influenced by three factors - preceding tone, underlying tone, and NT type - using word-list reading data from 36 Beijing Mandarin speakers. Our findings confirm a robust effect of the preceding tone. In the meantime, we identify a significant impact of the underlying tone, indicating that NT is not entirely toneless but retains some degree of phonological specification. Moreover, the differences observed between optional and forbidden NT words suggest that NT should be conceptualized as part of a gradient system influenced by contextual factors, rather than as a simple neutral versus full-tone contrast."
   ],
   "p1": 694,
   "pn": 698,
   "doi": "10.21437/Interspeech.2025-1173",
   "url": "interspeech_2025/dong25d_interspeech.html"
  },
  "li25n_interspeech": {
   "authors": [
    [
     "Jingting",
     "Li"
    ],
    [
     "Keyi",
     "Feng"
    ],
    [
     "Xinran",
     "Zhao"
    ],
    [
     "Yan",
     "Wang"
    ],
    [
     "Su-Jing",
     "Wang"
    ]
   ],
   "title": "Synthetic Dysarthric Speech: A Supplement, Not a Substitute for Authentic Data in Dysarthric Speech Recognition",
   "original": "1174",
   "order": 562,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech recognition (DSR) is an emerging field that can enhance social interactions and mental health for individuals with dysarthria. However, the lack of sufficient Chinese dysarthric speech data and challenges like ambiguity and individual differences hinder performance improvements. Text-to-speech (TTS) technology is well-established in normal speech recognition and can also supplement dysarthric speech data. This study explores the impact of TTS-based Chinese dysarthric speech generation on DSR performance. Speaker-dependent experiments show that synthetic dysarthric speech alone does not effectively improve DSR performance. Through statistical analysis of acoustic features, we reveal the disparities between synthetic and authentic speech in dysarthria and highlight the limitations of synthetic data for DSR. These findings provide insights for future improvements in speech generation methods."
   ],
   "p1": 2755,
   "pn": 2759,
   "doi": "10.21437/Interspeech.2025-1174",
   "url": "interspeech_2025/li25n_interspeech.html"
  },
  "b25_interspeech": {
   "authors": [
    [
     "Chidambar",
     "B"
    ],
    [
     "Hanumanth Rao",
     "Naidu"
    ]
   ],
   "title": "Structured Codebook Based Hierarchical Framework for DNN for Computationally Efficient Speech Enhancement",
   "original": "1176",
   "order": 17,
   "page_count": 5,
   "abstract": [
    "Deep Neural Networks (DNN) based single-channel speech enhancement techniques have surpassed traditional techniques in handling non-stationary noise, however, they are computationally demanding. In this work, we introduce a novel Hierarchical Framework for DNNs (HF-DNN) for speech enhancement that replaces a single complex and computationally expensive DNN model with multiple simpler and less complex DNNs that are hierarchically connected. This is achieved by using structured codebooks of speech parameters, like log power spectra, that are generated by exploiting hierarchical relation between the speech training data. The proposed HF-DNN reduces computational complexity significantly compared to a large DNN while maintaining speech enhancement performance. Importantly, such a framework can be extended to other speech processing tasks, such as speech recognition, speaker verification, etc., where parametric models of speech data are utilized."
   ],
   "p1": 76,
   "pn": 80,
   "doi": "10.21437/Interspeech.2025-1176",
   "url": "interspeech_2025/b25_interspeech.html"
  },
  "shen25_interspeech": {
   "authors": [
    [
     "Pengjie",
     "Shen"
    ],
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Zhong-Qiu",
     "Wang"
    ]
   ],
   "title": "ARiSE: Auto-Regressive Multi-Channel Speech Enhancement",
   "original": "1178",
   "order": 243,
   "page_count": 5,
   "abstract": [
    "We propose ARiSE, an auto-regressive algorithm for multi-channel speech enhancement. ARiSE improves existing deep neural network (DNN) based frame-online multi-channel speech enhancement models by introducing auto-regressive connections, where the estimated target speech at previous frames is leveraged as extra input features to help the DNN estimate the target speech at the current frame. The extra input features can be derived from (a) the estimated target speech in previous frames; and (b) a beamformed mixture with the beamformer computed based on the previous estimated target speech. On the other hand, naively training the DNN in an auto-regressive manner is very slow. To deal with this, we propose a parallel training mechanism to speed up the training. Evaluation results in noisy-reverberant conditions show the effectiveness and potential of the proposed algorithms."
   ],
   "p1": 1183,
   "pn": 1187,
   "doi": "10.21437/Interspeech.2025-1178",
   "url": "interspeech_2025/shen25_interspeech.html"
  },
  "prakrankamanant25_interspeech": {
   "authors": [
    [
     "Patawee",
     "Prakrankamanant"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Ekapol",
     "Chuangsuwanich"
    ]
   ],
   "title": "Explainable Depression Detection using Masked Hard Instance Mining",
   "original": "1181",
   "order": 101,
   "page_count": 5,
   "abstract": [
    "This paper addresses the critical need for improved explainability in text-based depression detection. While offering predictive outcomes, current solutions often overlook the understanding of model predictions which can hinder trust in the system. We propose the use of Masked Hard Instance Mining (MHIM) to enhance the explainability in the depression detection task. MHIM strategically masks attention weights within the model, compelling it to distribute attention across a wider range of salient features. We evaluate MHIM on two datasets representing distinct languages: Thai (Thai-Maywe) and English (DAIC-WOZ). Our results demonstrate that MHIM significantly improves performance in terms of both prediction accuracy and explainability metrics."
   ],
   "p1": 474,
   "pn": 478,
   "doi": "10.21437/Interspeech.2025-1181",
   "url": "interspeech_2025/prakrankamanant25_interspeech.html"
  },
  "inoue25b_interspeech": {
   "authors": [
    [
     "Masakazu",
     "Inoue"
    ],
    [
     "Motoshige",
     "Sato"
    ],
    [
     "Kenichi",
     "Tomeoka"
    ],
    [
     "Nathania",
     "Nah"
    ],
    [
     "Eri",
     "Hatakeyama"
    ],
    [
     "Kai",
     "Arulkumaran"
    ],
    [
     "Ilya",
     "Horiguchi"
    ],
    [
     "Shuntaro",
     "Sasai"
    ]
   ],
   "title": "A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations",
   "original": "1183",
   "order": 1142,
   "page_count": 5,
   "abstract": [
    "Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients."
   ],
   "p1": 5603,
   "pn": 5607,
   "doi": "10.21437/Interspeech.2025-1183",
   "url": "interspeech_2025/inoue25b_interspeech.html"
  },
  "tian25b_interspeech": {
   "authors": [
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Shikhar",
     "Bharadwaj"
    ],
    [
     "Takashi",
     "Maekaku"
    ],
    [
     "Yusuke",
     "Shinohara"
    ],
    [
     "Keita",
     "Goto"
    ],
    [
     "Xiang",
     "Yue"
    ],
    [
     "Huck",
     "Yang"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "OpusLM: A Family of Open Unified Speech Language Models",
   "original": "1184",
   "order": 663,
   "page_count": 5,
   "abstract": [
    "This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We demonstrate our OpusLMs achieve comparable (or even superior) performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities. Technically, this paper articulates our SpeechLM designs on tokenization, multi-stream language models, and multi-stage training strategies. We experimentally demonstrate the importance of model size scaling and the effect of annealing data selection. The OpusLMs are all built from publicly available materials and are fully transparent models. We release our code, data, checkpoints, and training logs to facilitate open SpeechLM research."
   ],
   "p1": 3259,
   "pn": 3263,
   "doi": "10.21437/Interspeech.2025-1184",
   "url": "interspeech_2025/tian25b_interspeech.html"
  },
  "zhu25c_interspeech": {
   "authors": [
    [
     "Tong",
     "Zhu"
    ],
    [
     "Xiaoke",
     "Yang"
    ],
    [
     "Jian",
     "Zhou"
    ],
    [
     "Lu",
     "Li"
    ],
    [
     "Zhao",
     "Lv"
    ],
    [
     "Cunhang",
     "Fan"
    ]
   ],
   "title": "SSF-DST: A Spectro-Spatial Features Enhanced Deep Spatiotemporal Network for EEG-Based Auditory Attention Detection",
   "original": "1187",
   "order": 216,
   "page_count": 5,
   "abstract": [
    "In a cocktail party scenario, humans can selectively attend to different speakers. EEG-based auditory attention detection (AAD) aims to identify the attended speaker from brain signals. Existing AAD methods enhance features through temporal-frequency fusion, but frequency features incur heavy computational cost with limited contribution. Furthermore, many methods utilize attention mechanisms to capture temporal dynamics, which may not fully capture the complex interplay of spatial and temporal information. To address these issues, we propose a spectro-spatial features enhanced deep spatiotemporal network (SSF-DST). SSF-DST efficiently extracts spectro-spatial features from frequency domain to enhance spatial patterns of EEG, and constructs deep spatiotemporal features to capture long-term dependencies and spatial patterns. Experimental results demonstrate that SSF-DST outperforms the state-of-the-art model in classification accuracy while reducing the trainable parameters by 50%."
   ],
   "p1": 1048,
   "pn": 1052,
   "doi": "10.21437/Interspeech.2025-1187",
   "url": "interspeech_2025/zhu25c_interspeech.html"
  },
  "dong25e_interspeech": {
   "authors": [
    [
     "Gaoyang",
     "Dong"
    ],
    [
     "Zhicheng",
     "Zhang"
    ],
    [
     "Ping",
     "Sun"
    ],
    [
     "Minghui",
     "Zhang"
    ]
   ],
   "title": "Adaptive Differential Denoising for Respiratory Sounds Classification",
   "original": "1190",
   "order": 208,
   "page_count": 5,
   "abstract": [
    "Automated respiratory sound classification faces practical challenges from background noise and insufficient denoising in existing systems. We propose Adaptive Differential Denoising network, that integrates noise suppression and pathological feature preservation via three innovations: 1) Adaptive Frequency Filter with learnable spectral masks and soft shrink to eliminate noise while retaining diagnostic high-frequency components; 2) A Differential Denoise Layer using differential attention to reduce noise-induced variations through augmented sample comparisons; 3) A bias denoising loss jointly optimizing classification and robustness without clean labels.  Experiments on the ICBHI2017 dataset show that our method achieves 65.53% of the Score, which is improved by 1.99% over the previous sota method. The code is available in https://github.com/deegy666/ADD-RSC"
   ],
   "p1": 1008,
   "pn": 1012,
   "doi": "10.21437/Interspeech.2025-1190",
   "url": "interspeech_2025/dong25e_interspeech.html"
  },
  "fujita25_interspeech": {
   "authors": [
    [
     "Kenichi",
     "Fujita"
    ],
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Yusuke",
     "Ijima"
    ]
   ],
   "title": "Voice Impression Control in Zero-Shot TTS",
   "original": "1192",
   "order": 889,
   "page_count": 5,
   "abstract": [
    "Para-/non-linguistic information in speech is pivotal in shaping the listeners&#x27; impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-/non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark–bright). The results of both objective and subjective evaluations have demonstrated our method&#x27;s effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization."
   ],
   "p1": 4363,
   "pn": 4367,
   "doi": "10.21437/Interspeech.2025-1192",
   "url": "interspeech_2025/fujita25_interspeech.html"
  },
  "someki25_interspeech": {
   "authors": [
    [
     "Masao",
     "Someki"
    ],
    [
     "Shikhar",
     "Bharadwaj"
    ],
    [
     "Atharva Anand",
     "Joshi"
    ],
    [
     "Chyi-Jiunn",
     "Lin"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Markus",
     "Müller"
    ],
    [
     "Nathan",
     "Susanj"
    ],
    [
     "Jing",
     "Liu"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Context-Driven Dynamic Pruning for Large Speech Foundation Models",
   "original": "1193",
   "order": 406,
   "page_count": 5,
   "abstract": [
    "Speech foundation models achieve strong generalization across languages and acoustic conditions, but require significant computational resources for inference. In the context of speech foundation models, pruning techniques have been studied that dynamically optimize model structures based on the target audio leveraging external context. In this work, we extend this line of research and propose context-driven dynamic pruning, a technique that optimizes the model computation depending on the context between different input frames and additional context during inference. We employ the Open Whisper-style Speech Model (OWSM) and incorporate speaker embeddings, acoustic event embeddings, and language information as additional context. By incorporating the speaker embedding, our method achieves a reduction of 56.7 GFLOPs while improving BLEU scores by a relative 25.7% compared to the fully fine-tuned OWSM model."
   ],
   "p1": 1993,
   "pn": 1997,
   "doi": "10.21437/Interspeech.2025-1193",
   "url": "interspeech_2025/someki25_interspeech.html"
  },
  "chandra25_interspeech": {
   "authors": [
    [
     "Shreeram Suresh",
     "Chandra"
    ],
    [
     "Lucas",
     "Goncalves"
    ],
    [
     "Junchen",
     "Lu"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Berrak",
     "Sisman"
    ]
   ],
   "title": "EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast",
   "original": "1198",
   "order": 611,
   "page_count": 5,
   "abstract": [
    "Current emotion-based contrastive language-audio pretraining (CLAP) methods typically learn by naïvely aligning audio samples with corresponding text prompts. Consequently, this approach fails to capture the ordinal nature of emotions, hindering inter-emotion understanding and often resulting in a wide modality gap between the audio and text embeddings due to insufficient alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a supervised contrastive learning approach that uses dimensional attributes of emotional speech and natural language prompts to jointly capture fine-grained emotion variations and improve cross-modal alignment. Our approach utilizes a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space. EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities, measured via a cross-modal retrieval task.1"
   ],
   "p1": 3000,
   "pn": 3004,
   "doi": "10.21437/Interspeech.2025-1198",
   "url": "interspeech_2025/chandra25_interspeech.html"
  },
  "wang25i_interspeech": {
   "authors": [
    [
     "Shaowen",
     "Wang"
    ],
    [
     "Xinyuan",
     "Chen"
    ],
    [
     "Yao",
     "Xu"
    ]
   ],
   "title": "Self-Improvement for Audio Large Language Model using Unlabeled Speech",
   "original": "1200",
   "order": 11,
   "page_count": 5,
   "abstract": [
    "Recent audio LLMs have emerged rapidly, demonstrating strong generalization across various speech tasks. However, given the inherent complexity of speech signals, these models inevitably suffer from performance degradation in specific target domains. To address this, we focus on enhancing audio LLMs in target domains without any labeled data. We propose a self-improvement method called SI-SDA, leveraging the information embedded in large-model decoding to evaluate the quality of generated pseudo labels and then perform domain adaptation based on reinforcement learning optimization. Experimental results show that our method consistently and significantly improves audio LLM performance, outperforming existing baselines in WER and BLEU across multiple public datasets of automatic speech recognition (ASR), spoken question-answering (SQA), and speech-to-text translation (S2TT). Furthermore, our approach exhibits high data efficiency, underscoring its potential for real-world deployment."
   ],
   "p1": 46,
   "pn": 50,
   "doi": "10.21437/Interspeech.2025-1200",
   "url": "interspeech_2025/wang25i_interspeech.html"
  },
  "wei25_interspeech": {
   "authors": [
    [
     "Peidong",
     "Wei"
    ],
    [
     "Shiyu",
     "Miao"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound Classification",
   "original": "1209",
   "order": 209,
   "page_count": 5,
   "abstract": [
    "Deep neural networks have been applied to audio spectrograms for respiratory sound classification, but it is still remains challenging to achieve satisfactory performance due to the scarcity of available data. Moreover, domain mismatch may be introduced into the trained models as a result of the respiratory sound samples being collected from various electronic stethoscopes, patient demographics, and recording environments. To tackle this issue, we proposed a modified Masked Autoencoder (MAE) model, named Disentangling Dual-Encoder MAE (DDE-MAE) for respiratory sound classification. Two independent encoders were designed to capture disease-related and disease-irrelevant information separately, achieving feature disentanglement to reduce the domain mismatch. Our method achieves a competitive performance on the ICBHI dataset."
   ],
   "p1": 1013,
   "pn": 1017,
   "doi": "10.21437/Interspeech.2025-1209",
   "url": "interspeech_2025/wei25_interspeech.html"
  },
  "su25c_interspeech": {
   "authors": [
    [
     "Xiaosu",
     "Su"
    ],
    [
     "BoWen",
     "Yang"
    ],
    [
     "Xiaowei",
     "Yi"
    ],
    [
     "Yun",
     "Cao"
    ]
   ],
   "title": "DiffEmotionVC: A Dual-Granularity Disentangled Diffusion Framework for Any-to-Any Emotional Voice Conversion",
   "original": "1210",
   "order": 895,
   "page_count": 5,
   "abstract": [
    "Emotional Voice Conversion (EVC) plays a vital role in improving human-computer interaction but faces challenges due to the complexity of emotion features, which are entangled with speaker and content characteristics. To overcome these challenges, we propose DiffEmotionVC, a diffusion-based framework for any-to-any EVC. Our approach integrates a dual-granularity emotion encoder that captures both utterance-level emotional context and frame-level acoustic details. It also employs an orthogonality-constrained condition encoder that disentangles emotion features through gated cross-attention while preserving feature independence with an orthogonal loss. Additionally, multi-objective diffusion training enhances both reconstruction fidelity and emotion discriminability via contrastive learning. Experimental results show a UTMOS score of 4.04 and 80% emotion recognition accuracy, demonstrating the framework&#x27;s effectiveness in speech quality and optimizing emotional expression."
   ],
   "p1": 4393,
   "pn": 4397,
   "doi": "10.21437/Interspeech.2025-1210",
   "url": "interspeech_2025/su25c_interspeech.html"
  },
  "geng25c_interspeech": {
   "authors": [
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Patrick",
     "Littell"
    ],
    [
     "Aidan",
     "Pine"
    ],
    [
     "Robbie",
     "Jimerson"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Vishwa",
     "Gupta"
    ],
    [
     "Rolando",
     "Coto-Solano"
    ],
    [
     "Anna",
     "Kazantseva"
    ],
    [
     "Marc",
     "Tessier"
    ],
    [
     "Delaney",
     "Lothian"
    ],
    [
     "Akwiratékha'",
     "Martin"
    ],
    [
     "Eric",
     "Joanis"
    ],
    [
     "Samuel",
     "Larkin"
    ],
    [
     "Roland",
     "Kuhn"
    ]
   ],
   "title": "Evaluating Speech Foundation Models for Automatic Speech Recognition in the Low-Resource Kanyen’kéha Language",
   "original": "1215",
   "order": 420,
   "page_count": 5,
   "abstract": [
    "Despite recent progress in automatic speech recognition (ASR) and speech foundation models (SFMs) for widely spoken languages, their application to low-resource Indigenous languages remains limited. To this end, this paper presents a systematic evaluation of SFMs for ASR development in Kanyen&#x27;kéha, a polysynthetic Iroquoian language structurally and typologically distinct from mainstream languages. To address challenges posed by limited data and extensive vocabulary variation, we further investigate the impact of incorporating in-domain synthesized data and external language models during cross-lingual transfer learning. Experiments on the low-resource Kanyen&#x27;kéha corpus, under various train/test splits, show that the best system obtains a WER of 13.73% and a CER of 2.21% on the test set with a 59.2% OOV rate. Excluding easily correctable errors further reduces the WER and CER to 10.36% and 1.76%, demonstrating its potential to support language documentation and revitalization."
   ],
   "p1": 2063,
   "pn": 2067,
   "doi": "10.21437/Interspeech.2025-1215",
   "url": "interspeech_2025/geng25c_interspeech.html"
  },
  "feng25b_interspeech": {
   "authors": [
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Anfeng",
     "Xu"
    ],
    [
     "Xuan",
     "Shi"
    ],
    [
     "Somer",
     "Bishop"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling",
   "original": "1216",
   "order": 578,
   "page_count": 5,
   "abstract": [
    "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by challenges in social communication, repetitive behavior, and sensory processing. One important research area in ASD is evaluating children&#x27;s behavioral changes over time during treatment. The standard protocol with this objective is BOSCC, which involves dyadic interactions between a child and clinicians performing a pre-defined set of activities. A fundamental aspect of understanding children&#x27;s behavior in these interactions is automatic speech understanding, particularly identifying who speaks and when. Conventional methods in this area heavily rely on audio recorded from a spectator perspective, and there is limited research on egocentric speech modeling. Here, we experiment with wearable sensors to perform speech sampling in BOSCC interviews from an egocentric perspective. Our findings highlight the potential of egocentric speech collection and pre-training to improve speaker classification accuracy."
   ],
   "p1": 2835,
   "pn": 2839,
   "doi": "10.21437/Interspeech.2025-1216",
   "url": "interspeech_2025/feng25b_interspeech.html"
  },
  "yang25i_interspeech": {
   "authors": [
    [
     "Yifan",
     "Yang"
    ],
    [
     "Zhiheng",
     "Qian"
    ]
   ],
   "title": "Temporal organization of prenuclear glides in Hefei Mandarin",
   "original": "1219",
   "order": 970,
   "page_count": 5,
   "abstract": [
    "This study investigates the temporal organization of prenuclear glides [j] and [w] in Hefei Mandarin using acoustic measurements. While the status of prenuclear glides in Chinese syllables remains debated, we employ a recently developed acoustic approach to analyze how CjV and CwV syllables are coordinated. Our results suggest that prenuclear glides in Hefei Mandarin are more likely to be part of the rime rather than forming complex onsets. The findings align with previous research on the status of Mandarin prenuclear [j] but reveal inconsistencies for prenuclear [w] compared to prior studies. This research contributes to our understanding of Chinese syllable structure and highlights the utility of acoustic methods in inferring abstract phonological structures."
   ],
   "p1": 4768,
   "pn": 4772,
   "doi": "10.21437/Interspeech.2025-1219",
   "url": "interspeech_2025/yang25i_interspeech.html"
  },
  "kumar25b_interspeech": {
   "authors": [
    [
     "Ankit",
     "Kumar"
    ],
    [
     "Munir",
     "Georges"
    ]
   ],
   "title": "DRI-GAN: A Novel Dual Real Input GAN with Triplet Loss for Cross-Lingual and Noisy SLU",
   "original": "1220",
   "order": 840,
   "page_count": 5,
   "abstract": [
    "Intent detection is a critical task in building spoken language understanding (SLU) systems. We propose a novel semi-supervised Dual Real Input Generative Adversarial Network (DRI-GAN) with triplet loss to enhance the performance of this task. This method effectively leverages both labeled and unlabeled data to achieve superior representation learning. We extract and fuse text embeddings from three locally deployed pre-trained Large Language Models (LLMs) and adapt these embeddings for training the DRI-GAN with triplet loss. Our experiments demonstrate three key findings: (i) In noisy SLU environments, the proposed method outperforms the state-of-the-art by +1.44%. (ii) In zero-shot cross-lingual scenarios, our approach yields substantial accuracy improvements, achieving an absolute gain of 4.19% on MultiATIS++ dataset and 14.60% on MASSIVE dataset. (iii) it achieves higher accuracy without fine-tuning, significantly reducing computational load."
   ],
   "p1": 4118,
   "pn": 4122,
   "doi": "10.21437/Interspeech.2025-1220",
   "url": "interspeech_2025/kumar25b_interspeech.html"
  },
  "park25d_interspeech": {
   "authors": [
    [
     "Chanwoo",
     "Park"
    ],
    [
     "Anna Seo Gyeong",
     "Choi"
    ],
    [
     "Sunghye",
     "Cho"
    ],
    [
     "Chanwoo",
     "Kim"
    ]
   ],
   "title": "Reasoning-Based Approach with Chain-of-Thought for Alzheimer’s Detection Using Speech and Large Language Models",
   "original": "1226",
   "order": 448,
   "page_count": 5,
   "abstract": [
    "Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimer&#x27;s disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches."
   ],
   "p1": 2185,
   "pn": 2189,
   "doi": "10.21437/Interspeech.2025-1226",
   "url": "interspeech_2025/park25d_interspeech.html"
  },
  "bhattacharya25b_interspeech": {
   "authors": [
    [
     "Debarpan",
     "Bhattacharya"
    ],
    [
     "Apoorva",
     "Kulkarni"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning",
   "original": "1228",
   "order": 421,
   "page_count": 5,
   "abstract": [
    "The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA). We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.1"
   ],
   "p1": 2068,
   "pn": 2072,
   "doi": "10.21437/Interspeech.2025-1228",
   "url": "interspeech_2025/bhattacharya25b_interspeech.html"
  },
  "liu25j_interspeech": {
   "authors": [
    [
     "Yihan",
     "Liu"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Leying",
     "Zhang"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "E2E-BPVC: End-to-End Background-Preserving Voice Conversion via In-Context Learning",
   "original": "1229",
   "order": 282,
   "page_count": 5,
   "abstract": [
    "Voice conversion (VC) systems are commonly trained on clean speech and are unable to function properly in the presence of background sound. However, in many instances, the background sound of speech and the context in which the speech occurs are highly semantically relevant and should also be retained. Existing approaches address this issue by introducing a denoising module to separate speech from background sound before applying voice conversion, which increases complexity and may lead to extra distortion. In this paper, we propose an end-to-end background-preserving voice conversion (E2E-BPVC) framework for the first time. By leveraging in-context learning (ICL), our model simultaneously modifies speech timbre and retains background sounds without requiring a separate denoising step. Both objective and subjective evaluations demonstrate that our method achieves performance comparable to denoising-based BPVC frameworks while maintaining a more streamlined and efficient system design."
   ],
   "p1": 1378,
   "pn": 1382,
   "doi": "10.21437/Interspeech.2025-1229",
   "url": "interspeech_2025/liu25j_interspeech.html"
  },
  "du25b_interspeech": {
   "authors": [
    [
     "Hongfei",
     "Du"
    ],
    [
     "Sidi",
     "Lu"
    ],
    [
     "Gang",
     "Zhou"
    ],
    [
     "Ye",
     "Gao"
    ]
   ],
   "title": "EAA: Emotion-Aware Audio Large Language Models with Dual Cross-Attention and Context-Aware Instruction Tuning",
   "original": "1232",
   "order": 1108,
   "page_count": 5,
   "abstract": [
    "Understanding speech emotion through artificial intelligence (AI) is crucial for human-computer interaction and mental health monitoring. While audio large language models (ALLMs) excel in speech comprehension, they face challenges in accurately integrating emotional signals from acoustic and semantic features. Moreover, emotions often span dialogues, making sole reliance on current audio insufficient for comprehensive understanding. To address these challenges, we propose a novel emotion-aware audio large language model (EAA). Specifically, we design a dual cross-attention mechanism to fuse acoustic and semantic information for a more comprehensive emotional representation. Furthermore, we use context-aware instruction tuning by incorporating the current and immediately preceding utterances as contextual information, enhancing task understanding and emotion recognition. Our experimental results show that EAA outperforms existing ALLMs on the MELD dataset, improving accuracy by 11.4%."
   ],
   "p1": 5433,
   "pn": 5437,
   "doi": "10.21437/Interspeech.2025-1232",
   "url": "interspeech_2025/du25b_interspeech.html"
  },
  "ryu25_interspeech": {
   "authors": [
    [
     "Minji",
     "Ryu"
    ],
    [
     "Ji-Hyeon",
     "Hur"
    ],
    [
     "Sung Heuk",
     "Kim"
    ],
    [
     "Gahgene",
     "Gweon"
    ]
   ],
   "title": "Pitch Contour Model (PCM) with Transformer Cross-Attention for Speech Emotion Recognition",
   "original": "1233",
   "order": 887,
   "page_count": 5,
   "abstract": [
    "Pitch is important for distinguishing emotional states through intonation. To incorporate pitch contour patterns into Speech Emotion Recognition (SER) task, we propose the Pitch Contour Model (PCM), which integrates pitch features with Transformer-based speech representations. PCM processes pitch features via linear embedding and combines them with Wav2Vec 2.0 extracted features using cross-attention. Experimental results show that PCM enhances SER performance, achieving state-of-the-art (SOTA) Valence-Arousal-Dominance (V-A-D) scores with V:0.627, avg:0.571 in MSP-Podcast v1.11 and V:0.646, A:0.744, D:0.557 in IEMOCAP datasets. We observe that the effect of z-score normalization on pitch varies across datasets, with lower pitch variability conditions benefiting more from raw pitch values. Furthermore, our study suggests how pretraining and finetuning language mismatches, between English and Korean, affect the choice between CNN-based and linear embeddings for pitch representation."
   ],
   "p1": 4353,
   "pn": 4357,
   "doi": "10.21437/Interspeech.2025-1233",
   "url": "interspeech_2025/ryu25_interspeech.html"
  },
  "mahapatra25_interspeech": {
   "authors": [
    [
     "Aurosweta",
     "Mahapatra"
    ],
    [
     "Ismail R.",
     "Ulgen"
    ],
    [
     "Abinay",
     "Reddy Naini"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Berrak",
     "Sisman"
    ]
   ],
   "title": "Can Emotion Fool Anti-spoofing?",
   "original": "1234",
   "order": 1147,
   "page_count": 5,
   "abstract": [
    "Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset."
   ],
   "p1": 5628,
   "pn": 5632,
   "doi": "10.21437/Interspeech.2025-1234",
   "url": "interspeech_2025/mahapatra25_interspeech.html"
  },
  "choi25c_interspeech": {
   "authors": [
    [
     "Jeongsoo",
     "Choi"
    ],
    [
     "Zhikang",
     "Niu"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Chunhui",
     "Wang"
    ],
    [
     "Joon Son",
     "Chung"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "Accelerating Diffusion-based Text-to-Speech Model Trainingwith Dual Modality Alignment",
   "original": "1236",
   "order": 703,
   "page_count": 5,
   "abstract": [
    "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines."
   ],
   "p1": 3459,
   "pn": 3463,
   "doi": "10.21437/Interspeech.2025-1236",
   "url": "interspeech_2025/choi25c_interspeech.html"
  },
  "cao25_interspeech": {
   "authors": [
    [
     "Mengxue",
     "Cao"
    ],
    [
     "Tianxin",
     "Zheng"
    ],
    [
     "Jiewen",
     "Zheng"
    ]
   ],
   "title": "Pitch Target Realization in Putonghua Tone Production of Children from Dialect-Speaking Regions",
   "original": "1237",
   "order": 908,
   "page_count": 5,
   "abstract": [
    "Tonal production can be understood as a target realization process. In a hybrid linguistic environment, tonal targets may interfere with one another. This study examines the production of Putonghua tones from both on- and off-target perspectives, using data from 139 children (aged 35–71 months) with Changli dialect exposure, drawn from the CL-CHILD corpus. Results reveal that: (1) children&#x27;s pitch target realization is universally constrained by physiological limitations, exhibiting cross-linguistic commonalities; (2) dialect exposure causes persistent pitch target interference over an extended period; (3) off-target pronunciations stem from phonetic similarities across tonal categories and emerge through mutual interference between multiple targets. These findings underscore that children&#x27;s tonal development is a dynamic process of target approximation, shaped by both physiological constraints and language experience."
   ],
   "p1": 4458,
   "pn": 4462,
   "doi": "10.21437/Interspeech.2025-1237",
   "url": "interspeech_2025/cao25_interspeech.html"
  },
  "male25_interspeech": {
   "authors": [
    [
     "Prabash Reddy",
     "Male"
    ],
    [
     "Swayambhu Nath",
     "Ray"
    ],
    [
     "Harish",
     "Arsikere"
    ],
    [
     "Akshat",
     "Jaiswal"
    ],
    [
     "Prakhar",
     "Swarup"
    ],
    [
     "Prantik",
     "Sen"
    ],
    [
     "Debmalya",
     "Chakrabarty"
    ],
    [
     "K V Vijay",
     "Girish"
    ],
    [
     "Nikhil",
     "Bhave"
    ],
    [
     "Frederick",
     "Weber"
    ],
    [
     "Sambuddha",
     "Bhattacharya"
    ],
    [
     "Sri",
     "Garimella"
    ]
   ],
   "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
   "original": "1242",
   "order": 1183,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most research has focused on either causal or full-context speech encoders, there&#x27;s limited exploration to effectively handle both streaming and non-streaming applications, while achieving state-of-the-art performance. We introduce DuRep, a Dual-mode Speech Representation learning setup, which enables a single speech encoder to function efficiently in both offline and online modes without additional parameters or mode-specific adjustments, across downstream tasks. DuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6% improvements in streaming and non-streaming modes, over baseline encoders on Multilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new performance benchmarks across ASR and non-ASR tasks. Our analysis reveals interesting trade-offs between acoustic and semantic information across encoder layers."
   ],
   "p1": 5808,
   "pn": 5812,
   "doi": "10.21437/Interspeech.2025-1242",
   "url": "interspeech_2025/male25_interspeech.html"
  },
  "hirano25_interspeech": {
   "authors": [
    [
     "Yuta",
     "Hirano"
    ],
    [
     "Sakriani",
     "Sakti"
    ]
   ],
   "title": "SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition",
   "original": "1243",
   "order": 1001,
   "page_count": 5,
   "abstract": [
    "We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an enhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT handles overlapped speech, and we found the decoder performs implicit speaker separation. We hypothesize this implicit separation is often insufficient due to ambiguous acoustic cues in overlapping regions. To address this, SC-SOT explicitly conditions the decoder on speaker information, providing detailed information about &quot;who spoke when&quot;. Specifically, we enhance the decoder by incorporating: (1) speaker embeddings, which allow the model to focus on the acoustic characteristics of the target speaker, and (2) speaker activity information, which guides the model to suppress non-target speakers. The speaker embeddings are derived from a jointly trained E2E speaker diarization model, mitigating the need for speaker enrollment. Experimental results demonstrate the effectiveness of our conditioning approach on overlapped speech."
   ],
   "p1": 4923,
   "pn": 4927,
   "doi": "10.21437/Interspeech.2025-1243",
   "url": "interspeech_2025/hirano25_interspeech.html"
  },
  "raman25_interspeech": {
   "authors": [
    [
     "Sneha",
     "Raman"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Oral Reading Errors by Grade 3 Children in Indian Schools: A Hindi-English Perspective",
   "original": "1245",
   "order": 583,
   "page_count": 5,
   "abstract": [
    "We present an analysis of reading errors on a manually transcribed dataset of Grade 3 students (N=595) who read aloud level-appropriate passages in English and Hindi. Substitutions are categorised as word or non-word errors. Further, substitutions are analysed using grapheme/phoneme sequence matching and assigned typical reading error types such as initial part matches, final part matches and scaffolding errors. We compare the distribution of error categories for the two languages and discuss underlying language-dependent decoding strategies. We also apply the analysis methodology to identify the percentage scaffolding errors per utterance given its usefulness for reading assessment. Finally, we extend our work to its practical application for diagnostics by testing an automatic phone recognition system on our task."
   ],
   "p1": 2860,
   "pn": 2864,
   "doi": "10.21437/Interspeech.2025-1245",
   "url": "interspeech_2025/raman25_interspeech.html"
  },
  "zhang25j_interspeech": {
   "authors": [
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Samuele",
     "Cornell"
    ],
    [
     "Robin",
     "Scheibler"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Zhaoheng",
     "Ni"
    ],
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Marvin",
     "Sach"
    ],
    [
     "Wei",
     "Wang"
    ],
    [
     "Yihui",
     "Fu"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Lessons Learned from the URGENT 2024 Speech Enhancement Challenge",
   "original": "1246",
   "order": 177,
   "page_count": 5,
   "abstract": [
    "The URGENT 2024 Challenge aims to foster speech enhancement (SE) techniques with great universality, robustness, and generalizability, featuring a broader task definition, large-scale multi-domain data, and comprehensive evaluation metrics. Nourished by the challenge outcomes, this paper presents an in-depth analysis of two key, yet understudied, issues in SE system development: data cleaning and evaluation metrics. We highlight several overlooked problems in traditional SE pipelines: (1) mismatches between declared and effective audio bandwidths, along with label noise even in various &quot;high-quality&quot; speech corpora; (2) lack of both effective SE systems to conquer the hardest conditions (e.g., speech overlap, strong noise / reverberation) and reliable measure of speech sample difficulty; (3) importance of combining multifaceted metrics for a comprehensive evaluation correlating well with human judgment. We hope that this endeavor can inspire improved SE pipeline designs in the future."
   ],
   "p1": 853,
   "pn": 857,
   "doi": "10.21437/Interspeech.2025-1246",
   "url": "interspeech_2025/zhang25j_interspeech.html"
  },
  "chae25_interspeech": {
   "authors": [
    [
     "Yunkee",
     "Chae"
    ],
    [
     "Eunsik",
     "Shin"
    ],
    [
     "Suntae",
     "Hwang"
    ],
    [
     "Seungryeol",
     "Paik"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control",
   "original": "1247",
   "order": 261,
   "page_count": 5,
   "abstract": [
    "Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints."
   ],
   "p1": 1273,
   "pn": 1277,
   "doi": "10.21437/Interspeech.2025-1247",
   "url": "interspeech_2025/chae25_interspeech.html"
  },
  "chae25b_interspeech": {
   "authors": [
    [
     "Yunkee",
     "Chae"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Towards Bitrate-Efficient and Noise-Robust Speech Coding with Variable Bitrate RVQ",
   "original": "1253",
   "order": 128,
   "page_count": 5,
   "abstract": [
    "Residual Vector Quantization (RVQ) has become a dominant approach in neural speech and audio coding, providing high-fidelity compression. However, speech coding presents additional challenges due to real-world noise, which degrades compression efficiency. Standard codecs allocate bits uniformly, wasting bitrate on noise components that do not contribute to intelligibility. This paper introduces a Variable Bitrate RVQ (VRVQ) framework for noise-robust speech coding, dynamically adjusting bitrate per frame to optimize rate-distortion trade-offs. Unlike constant bitrate (CBR) RVQ, our method prioritizes critical speech components while suppressing residual noise. Additionally, we integrate a feature denoiser to further improve noise robustness. Experimental results show that VRVQ improves rate-distortion trade-offs over conventional methods, achieving better compression efficiency and perceptual quality in noisy conditions. Samples are available at our project page."
   ],
   "p1": 609,
   "pn": 613,
   "doi": "10.21437/Interspeech.2025-1253",
   "url": "interspeech_2025/chae25b_interspeech.html"
  },
  "suen25_interspeech": {
   "authors": [
    [
     "King Yiu",
     "Suen"
    ],
    [
     "Rudolf",
     "Chow"
    ],
    [
     "Albert Y.S.",
     "Lam"
    ]
   ],
   "title": "Cantonese Punctuation Restoration using LLM Annotated Data",
   "original": "1254",
   "order": 398,
   "page_count": 5,
   "abstract": [
    "One of the main challenges for punctuation restoration for a low-resource language such as Cantonese is data scarcity. While its spoken and written forms are very different, current Cantonese datasets are mostly from formal written text. Naturally spoken data are very scarce. To address this gap, we leverage LLM to annotate naturally spoken Cantonese transcripts sourced from YouTube. Then, we fine-tune pre-trained language models for punctuation restoration using the LLM-annotated transcripts. Our experiments show that models trained on LLM-annotated transcripts outperform those trained solely on formal written text, despite the smaller dataset size. Our best-performing model achieves performance on par with the strongest LLM evaluated on a benchmark dataset, while being significantly smaller. These findings highlight the potential of LLM-generated data for improving NLP tasks in low-resource languages. Our data and code are publicly available."
   ],
   "p1": 1953,
   "pn": 1957,
   "doi": "10.21437/Interspeech.2025-1254",
   "url": "interspeech_2025/suen25_interspeech.html"
  },
  "manakul25_interspeech": {
   "authors": [
    [
     "Potsawee",
     "Manakul"
    ],
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Warit",
     "Sirichotedumrong"
    ],
    [
     "Kasima",
     "Tharnpipitchai"
    ],
    [
     "Kunat",
     "Pipatanakul"
    ]
   ],
   "title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models",
   "original": "1258",
   "order": 424,
   "page_count": 5,
   "abstract": [
    "Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai."
   ],
   "p1": 2083,
   "pn": 2087,
   "doi": "10.21437/Interspeech.2025-1258",
   "url": "interspeech_2025/manakul25_interspeech.html"
  },
  "cheng25b_interspeech": {
   "authors": [
    [
     "Ming",
     "Cheng"
    ],
    [
     "Fei",
     "Su"
    ],
    [
     "Cancan",
     "Li"
    ],
    [
     "Juan",
     "Liu"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "Multi-Channel Sequence-to-Sequence Neural Diarization: Experimental Results for The MISP 2025 Challenge",
   "original": "1262",
   "order": 387,
   "page_count": 5,
   "abstract": [
    "This paper describes the speaker diarization system developed for the Multimodal Information-Based Speech Processing (MISP) 2025 Challenge. First, we utilize the Sequence-to-Sequence Neural Diarization (S2SND) framework to generate initial predictions using single-channel audio. Then, we extend the original S2SND framework to create a new version, Multi-Channel Sequence-to-Sequence Neural Diarization (MC-S2SND), which refines the initial results using multi-channel audio. The final system achieves a diarization error rate (DER) of 8.09% on the evaluation set of the competition database, ranking first place in the speaker diarization task of the MISP 2025 Challenge."
   ],
   "p1": 1898,
   "pn": 1902,
   "doi": "10.21437/Interspeech.2025-1262",
   "url": "interspeech_2025/cheng25b_interspeech.html"
  },
  "deng25_interspeech": {
   "authors": [
    [
     "Chengxi",
     "Deng"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Yicong",
     "Jiang"
    ],
    [
     "Jiankun",
     "Zhao"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Youjun",
     "Chen"
    ],
    [
     "Huimeng",
     "Wang"
    ],
    [
     "Haoning",
     "Xu"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly Speech Recognition",
   "original": "1263",
   "order": 1003,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel Mixture of Prompt-Experts based Speaker Adaptation approach (MOPSA) for elderly speech recognition. It allows zero-shot, real-time adaptation to unseen speakers, and leverages domain knowledge tailored to elderly speakers. Top-K most distinctive speaker prompt clusters derived using K-means serve as experts. A router network is trained to dynamically combine clustered prompt-experts. Acoustic and language level variability among elderly speakers are modelled using separate encoder and decoder prompts for Whisper. Experiments on the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets suggest that online MOPSA adaptation outperforms the speaker-independent (SI) model by statistically significant word error rate (WER) or character error rate (CER) reductions of 0.86% and 1.47% absolute (4.21% and 5.40% relative). Real-time factor (RTF) speed-up ratios of up to 16.12 times are obtained over offline batch-mode adaptation."
   ],
   "p1": 4933,
   "pn": 4937,
   "doi": "10.21437/Interspeech.2025-1263",
   "url": "interspeech_2025/deng25_interspeech.html"
  },
  "klein25_interspeech": {
   "authors": [
    [
     "Nicholas",
     "Klein"
    ],
    [
     "Hemlata",
     "Tak"
    ],
    [
     "Elie",
     "Khoury"
    ]
   ],
   "title": "Open-Set Source Tracing of Audio Deepfake Systems",
   "original": "1269",
   "order": 322,
   "page_count": 5,
   "abstract": [
    "Existing research on source tracing of audio deepfake systems has focused primarily on the closed-set scenario, while studies that evaluate open-set performance are limited to a small number of unseen systems. Due to the large number of emerging audio deepfake systems, robust open-set source tracing is critical. We leverage the protocol of the Interspeech 2025 special session on source tracing to evaluate methods for improving open-set source tracing performance. We introduce a novel adaptation to the energy score for out-of-distribution (OOD) detection, softmax energy (SME). We find that replacing the typical temperature-scaled energy score with SME provides a relative average improvement of 31% in the standard FPR95 (false positive rate at true positive rate of 95%) measure. We further explore SME-guided training as well as copy synthesis, codec, and reverberation augmentations, yielding an FPR95 of 8.3%."
   ],
   "p1": 1578,
   "pn": 1582,
   "doi": "10.21437/Interspeech.2025-1269",
   "url": "interspeech_2025/klein25_interspeech.html"
  },
  "sun25g_interspeech": {
   "authors": [
    [
     "Xingwei",
     "Sun"
    ],
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Yadong",
     "Niu"
    ],
    [
     "Linzhang",
     "Wang"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Jian",
     "Luan"
    ]
   ],
   "title": "Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders",
   "original": "1270",
   "order": 986,
   "page_count": 5,
   "abstract": [
    "Recent research has delved into speech enhancement (SE) approaches that leverage audio embeddings from pre-trained models, diverging from time-frequency masking or signal prediction techniques. This paper introduces an efficient and extensible SE method. Our approach involves initially extracting audio embeddings from noisy speech using a pre-trained audioencoder, which are then denoised by a compact encoder network. Subsequently, a vocoder synthesizes the clean speech from denoised embeddings. An ablation study substantiates the parameter efficiency of the denoise encoder with a pre-trained audioencoder and vocoder. Experimental results on both speech enhancement and speaker fidelity demonstrate that our generative audioencoder-based SE system outperforms models utilizing discriminative audioencoders. Furthermore, subjective listening tests validate that our proposed system surpasses an existing state-of-the-art SE model in terms of perceptual quality."
   ],
   "p1": 4848,
   "pn": 4852,
   "doi": "10.21437/Interspeech.2025-1270",
   "url": "interspeech_2025/sun25g_interspeech.html"
  },
  "cui25_interspeech": {
   "authors": [
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Yifan",
     "Yang"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Jiawen",
     "Kang"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
   "original": "1280",
   "order": 247,
   "page_count": 5,
   "abstract": [
    "This paper investigates discrete tokens based cross-utterance speech contexts modelling for Zipformer-Transducer (Z-T) systems. Their efficacy and efficiency in modelling preceding, current and future speech utterance contexts using concatenation or pooling projection of Z-T encoder embeddings are extensively shown on the 1000-hr GigaSpeech-M and DementiaBank Pitt elderly speech datasets over comparable contextual Z-T baselines using filterbank or continuous WavLM features}. The best performing discrete tokens based contextual Z-T system outperforms the non-contextual baseline by statistically significant average WER reductions of 0.39% and 1.41% absolute (3.4% and 3.4% relative) on the two tasks, respectively. Model training time speedup ratios up to 4.36x is obtained over continuous WavLM feature-based contextual Z-T systems, while retaining up to 98.0% of their WER reductions over non-contextual baselines."
   ],
   "p1": 1203,
   "pn": 1207,
   "doi": "10.21437/Interspeech.2025-1280",
   "url": "interspeech_2025/cui25_interspeech.html"
  },
  "uniyal25_interspeech": {
   "authors": [
    [
     "Drishya",
     "Uniyal"
    ],
    [
     "Vinayak",
     "Abrol"
    ]
   ],
   "title": "From Pretraining to Performance: Benchmarking Self-Supervised Speech Models for Interspeech-25 SER Challenge",
   "original": "1283",
   "order": 944,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) in naturalistic conditions remains a challenging task due to the variability of emotional expression and class imbalances in the real world. As part of the Interspeech-25 SER challenge, we benchmark state-of-the-art large-scale self-supervised speech models on the MSP-Podcast corpus. To extract rich and expressive representations, we systematically investigate fine-tuning strategies, loss functions tailored to mitigate class imbalance, and pre-trained encoder layer freezing techniques to optimize performance. Our findings highlight the impact of these design choices on model robustness and generalization, offering practical guidance for developing SER systems that excel in real-world scenarios."
   ],
   "p1": 4638,
   "pn": 4642,
   "doi": "10.21437/Interspeech.2025-1283",
   "url": "interspeech_2025/uniyal25_interspeech.html"
  },
  "jin25c_interspeech": {
   "authors": [
    [
     "Longbin",
     "Jin"
    ],
    [
     "Donghun",
     "Min"
    ],
    [
     "Jung Eun",
     "Shin"
    ],
    [
     "Eun Yi",
     "Kim"
    ]
   ],
   "title": "Contrastive Learning-based Syllable-Level Mispronunciation Detection and Diagnosis for Speech Audiometry",
   "original": "1285",
   "order": 172,
   "page_count": 5,
   "abstract": [
    "Speech audiometry assesses hearing disorders, typically relies on audiologists, making the process subjective and requiring in-person evaluation. In this paper, we introduce SylPh, a novel automatic syllable-level mispronunciation detection and diagnosis (MDD) model that generalizes across open-set syllables while also offering phonemic analysis. To capture a wide range of mispronunciation patterns, we construct positive and pseudo-negative bags to extract in-distribution and out-of-distribution features from input audio. Our model aligns audio features with adaptive text embeddings using a contrastive objective, dynamically adjusting decision boundaries for each syllable within a single model. Extensive experiments on a large-scale dataset demonstrate its effectiveness in both closed-set and open-set syllables. Notably, despite training only on syllable-level labels, the Sylph has the capability to localize phoneme-level abnormalities, providing detailed diagnostic insights."
   ],
   "p1": 828,
   "pn": 832,
   "doi": "10.21437/Interspeech.2025-1285",
   "url": "interspeech_2025/jin25c_interspeech.html"
  },
  "li25o_interspeech": {
   "authors": [
    [
     "Xingyuan",
     "Li"
    ],
    [
     "Kenny",
     "Zhu"
    ],
    [
     "Mengyue",
     "Wu"
    ]
   ],
   "title": "Dog2vec: Self-Supervised Pre-Training for Canine Vocal Representation",
   "original": "1287",
   "order": 346,
   "page_count": 5,
   "abstract": [
    "Previous generalized biological voice models were trained on large amounts of data from multiple species. However, on average, there is very little training data on species-specific voices, while large differences between the vocalizations of species may even be a barrier to encoding vocal features. This leads to potentially large errors in using generic models for species-specific vocalization studies. We collected over 6000 hours of dog barking videos and presented the first animal-specific bioacoustic embedding model, Dog2vec.1 The results indicate that Dog2vec outperforms species-independent pre-trained models and achieves state-of-the-art results on a series of dog-related tasks, including dog bark type recognition and dog sound event detection, and obtain a relative 8.2% performance increase."
   ],
   "p1": 1698,
   "pn": 1702,
   "doi": "10.21437/Interspeech.2025-1287",
   "url": "interspeech_2025/li25o_interspeech.html"
  },
  "zhang25k_interspeech": {
   "authors": [
    [
     "Hanglei",
     "Zhang"
    ],
    [
     "Yiwei",
     "Guo"
    ],
    [
     "Zhihan",
     "Li"
    ],
    [
     "Xiang",
     "Hao"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
   "original": "1289",
   "order": 1021,
   "page_count": 5,
   "abstract": [
    "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks."
   ],
   "p1": 5003,
   "pn": 5007,
   "doi": "10.21437/Interspeech.2025-1289",
   "url": "interspeech_2025/zhang25k_interspeech.html"
  },
  "kwok25c_interspeech": {
   "authors": [
    [
     "Chin Yuen",
     "Kwok"
    ],
    [
     "Jia Qi",
     "Yip"
    ]
   ],
   "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition",
   "original": "1290",
   "order": 138,
   "page_count": 5,
   "abstract": [
    "Contextual biasing improves rare word recognition of ASR models by prioritizing the output of rare words during decoding. A common approach is Trie-based biasing, which gives ``bonus scores&quot; to partial hypothesis (e.g. ``Bon&quot;) that may lead to the generation of the rare word (e.g. ``Bonham&quot;). If the full word (``Bonham&quot;) isn’t ultimately recognized, the system revokes those earlier bonuses. This revocation is limited to beam search and is computationally expensive, particularly for models with large decoders. To overcome these limitations, we propose adapting ASR models to look ahead and predict multiple steps at once. This avoids the revocation step entirely by better estimating whether a partial hypothesis will lead to the generation of the full rare word. By fine-tuning Whisper with only 10 hours of synthetic data, our method reduces the word error rate on the NSC Part 2 test set from 30.86% to 12.19%."
   ],
   "p1": 659,
   "pn": 663,
   "doi": "10.21437/Interspeech.2025-1290",
   "url": "interspeech_2025/kwok25c_interspeech.html"
  },
  "dao25_interspeech": {
   "authors": [
    [
     "Alan",
     "Dao"
    ],
    [
     "Dinh Bach",
     "Vu"
    ],
    [
     "Huy Hoang",
     "Ha"
    ],
    [
     "Tuan Le Duc",
     "Anh"
    ],
    [
     "Shreyas",
     "Gopal"
    ],
    [
     "Yue Heng",
     "Yeo"
    ],
    [
     "Warren Keng Hoong",
     "Low"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Jia Qi",
     "Yip"
    ]
   ],
   "title": "Speechless: Speech Instruction Training Without Speech for Low Resource Languages",
   "original": "1292",
   "order": 659,
   "page_count": 5,
   "abstract": [
    "The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages."
   ],
   "p1": 3239,
   "pn": 3243,
   "doi": "10.21437/Interspeech.2025-1292",
   "url": "interspeech_2025/dao25_interspeech.html"
  },
  "toikkanen25_interspeech": {
   "authors": [
    [
     "Miika",
     "Toikkanen"
    ],
    [
     "June-Woo",
     "Kim"
    ]
   ],
   "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles",
   "original": "1295",
   "order": 211,
   "page_count": 5,
   "abstract": [
    "Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Ensemble models help but inevitably increase compute cost at inference time. Soft label training distills knowledge efficiently with extra cost only at training. In this study, we explore soft labels for respiratory sound classification as an architecture-agnostic approach to distill an ensemble of teacher models into a student model. We examine different variations of our approach and find that even a single teacher, identical to the student, considerably improves performance beyond its own capability, with optimal gains achieved using only a few teachers. We achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average Scores across architectures by more than 1.16. Our results highlight the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of size or architecture."
   ],
   "p1": 1023,
   "pn": 1027,
   "doi": "10.21437/Interspeech.2025-1295",
   "url": "interspeech_2025/toikkanen25_interspeech.html"
  },
  "chen25j_interspeech": {
   "authors": [
    [
     "Xuanjun",
     "Chen"
    ],
    [
     "I-Ming",
     "Lin"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Jiawei",
     "Du"
    ],
    [
     "Haibin",
     "Wu"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
   "original": "1297",
   "order": 314,
   "page_count": 5,
   "abstract": [
    "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation."
   ],
   "p1": 1538,
   "pn": 1542,
   "doi": "10.21437/Interspeech.2025-1297",
   "url": "interspeech_2025/chen25j_interspeech.html"
  },
  "maeda25_interspeech": {
   "authors": [
    [
     "Chikara",
     "Maeda"
    ],
    [
     "Muhammad",
     "Shakeel"
    ],
    [
     "Yui",
     "Sudo"
    ]
   ],
   "title": "Joint Target-Speaker ASR and Activity Detection",
   "original": "1299",
   "order": 343,
   "page_count": 5,
   "abstract": [
    "Target-speaker automatic speech recognition (TS-ASR) has shown promise in transcribing speech in multi-speaker scenarios by focusing on a specific speaker. However, existing approaches employ a cascaded design, where voice activity detection (VAD) and TS-ASR are optimized separately. This separation leads to unstable training and error accumulation, limiting overall performance. We address these issues by proposing TS-ASR-AD, a joint end-to-end model that integrates VAD with TS-ASR, enabling stable training and reducing error accumulation. Moreover, improved training stability leads to better connectionist temporal classification (CTC) alignment in token probabilities, further enhancing transcription accuracy. Our approach outperforms previous studies and achieves a word error rate (WER) of 6.61% and 14.81%, and a diarization error rate (DER) of 1.23% and 2.66% on the Libri2Mix and Libri3Mix datasets, respectively."
   ],
   "p1": 1683,
   "pn": 1687,
   "doi": "10.21437/Interspeech.2025-1299",
   "url": "interspeech_2025/maeda25_interspeech.html"
  },
  "nakagome25_interspeech": {
   "authors": [
    [
     "Yu",
     "Nakagome"
    ],
    [
     "Michael",
     "Hentschel"
    ]
   ],
   "title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing",
   "original": "1300",
   "order": 1056,
   "page_count": 5,
   "abstract": [
    "Despite recent advances in end-to-end speech recognition methods, the output tends to be biased to the training data’s vocabulary, resulting in inaccurate recognition of proper nouns and other unknown terms. To address this issue, we propose a method to improve recognition accuracy of such rare words in CTC-based models without additional training or text-to-speech systems. Specifically, keyword spotting is performed using acoustic features of intermediate layers during inference, and a bias is applied to the subsequent layers of the acoustic model for detected keywords. For keyword detection, we adopt a wildcard CTC that is both fast and tolerant of ambiguous matches, allowing flexible handling of words that are difficult to match strictly. Since this method does not require retraining of existing models, it can be easily applied to even large-scale models. In experiments on Japanese speech recognition, the proposed method achieved a 29% improvement in the F1 score for unknown words."
   ],
   "p1": 5178,
   "pn": 5182,
   "doi": "10.21437/Interspeech.2025-1300",
   "url": "interspeech_2025/nakagome25_interspeech.html"
  },
  "dasilva25_interspeech": {
   "authors": [
    [
     "Dashanka",
     "Da Silva"
    ],
    [
     "Siqi",
     "Cai"
    ],
    [
     "Saurav",
     "Pahuja"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "NeuroSpex+: Dual-Task Training of Neuro-Guided Speaker Extraction with Speech Envelope and Waveform",
   "original": "1304",
   "order": 1135,
   "page_count": 5,
   "abstract": [
    "Neuro-guided speaker extraction, i.e. NeuroSpex, aims to isolate the speech signal a listener is attending to in a multi-talker environment using reference cues derived from cortical activity, such as electroencephalography (EEG). Despite remarkable progress, there remains untapped potential. In this study, we propose NeuroSpex+, a novel neuro-guided speaker extraction model that integrates an additional task of reconstructing the target speech envelope. By simultaneously optimizing the model for both the target speech envelope and speech waveform, NeuroSpex+ reinforces the mask generation for speaker extraction. Experimental results demonstrate that the proposed model significantly outperforms baselines, improving overall signal quality."
   ],
   "p1": 5568,
   "pn": 5572,
   "doi": "10.21437/Interspeech.2025-1304",
   "url": "interspeech_2025/dasilva25_interspeech.html"
  },
  "kumar25c_interspeech": {
   "authors": [
    [
     "Vishal",
     "Kumar"
    ],
    [
     "Vinayak",
     "Abrol"
    ]
   ],
   "title": "ArticulateX: End-to-End Monolingual Speech Translation in Articulator Space",
   "original": "1305",
   "order": 4,
   "page_count": 5,
   "abstract": [
    "We present ArticulateX, the first non-autoregressive direct speech-to-speech translation (S2ST) model that operates through an articulatory latent space, offering an efficient alternative to existing cascaded models. It consists of a direct speech-to-articulator encoder, a latent articulator-to-MelSpectrogram mapper, and a vocoder for high-fidelity speech synthesis. By leveraging articulatory representations, which are inherently language-agnostic, our model effectively captures speech dynamics, preserving speaker identity, prosody and expressiveness across languages. Unlike prior autoregressive models, ArticulateX eliminates the need for intermediate text, discrete units and/or complex self-supervised objectives, enabling faster inference, stable training, and improved translation quality. We demonstrate the efficacy of the proposed model in fr-en and de-en speech-to-speech translation on the CVSS dataset, achieving BLEU scores better or comparable to existing models."
   ],
   "p1": 11,
   "pn": 15,
   "doi": "10.21437/Interspeech.2025-1305",
   "url": "interspeech_2025/kumar25c_interspeech.html"
  },
  "fujita25b_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Tomoya",
     "Mizumoto"
    ],
    [
     "Atsushi",
     "Kojima"
    ],
    [
     "Lianbo",
     "Liu"
    ],
    [
     "Yui",
     "Sudo"
    ]
   ],
   "title": "AC/DC: LLM-based Audio Comprehension via Dialogue Continuation",
   "original": "1308",
   "order": 533,
   "page_count": 5,
   "abstract": [
    "We propose an instruction-following audio comprehension model that leverages the dialogue continuation ability of large language models (LLMs). Instead of directly generating target captions in training data, the proposed method trains a model to produce responses as if the input caption triggered a dialogue. This dialogue continuation training mitigates the caption variation problem. Learning to continue a dialogue effectively captures the caption&#x27;s meaning beyond its surface-level words. As a result, our model enables zero-shot instruction-following capability without multitask instruction tuning, even trained solely on audio captioning datasets. Experiments on AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene question-answering tests demonstrate our model&#x27;s ability to follow various unseen instructions."
   ],
   "p1": 2610,
   "pn": 2614,
   "doi": "10.21437/Interspeech.2025-1308",
   "url": "interspeech_2025/fujita25b_interspeech.html"
  },
  "ko25_interspeech": {
   "authors": [
    [
     "Ye-Eun",
     "Ko"
    ],
    [
     "Mun-Hak",
     "Lee"
    ],
    [
     "Dong-Hyun",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Improving Generalization of End-to-End ASR through Diversity and Independence Regularization",
   "original": "1309",
   "order": 732,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) has been driven by representative end-to-end model architectures, including connectionist temporal classification (CTC), attention-based encoder-decoder (AED), and recurrent neural network transducer (RNN-T). However, these models are prone to overfitting during training, which degrades their generalization performance. In this paper, we propose a novel regularization technique applicable to various ASR models: diversity loss and independence loss. Diversity loss reduces the similarity between feature representations, encouraging the model to learn diverse patterns. Independence loss minimizes the covariance between feature vectors, ensuring that they contain independent information and reducing redundancy. We apply these techniques to CTC, AED, and RNN-T models and demonstrate that the proposed regularization method effectively improves the model generalization performance and robustness through extensive experiments."
   ],
   "p1": 3578,
   "pn": 3582,
   "doi": "10.21437/Interspeech.2025-1309",
   "url": "interspeech_2025/ko25_interspeech.html"
  },
  "ahn25b_interspeech": {
   "authors": [
    [
     "Seyun",
     "Ahn"
    ],
    [
     "Pil Moo",
     "Byun"
    ],
    [
     "Won-Gook",
     "Choi"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Optimizing CLAP Reward with LLM Feedback for Semantically Aligned and Diverse Automated Audio Captioning",
   "original": "1313",
   "order": 639,
   "page_count": 5,
   "abstract": [
    "Deep learning-based automated audio captioning (AAC) systems describe audio well, yet they often overfit to reference styles. To address this, reinforcement learning (RL) techniques have been adopted to directly optimize evaluation metrics, but these methods often suffer from word repetition and contextual distortion. Embedding-based rewards, such as those derived from contrastive language-audio pretraining (CLAP), may bias the model toward specific words or phrases that human evaluators find unnatural. In this paper, we propose a novel reward system that combines a CLAP-based reward with a repetition penalty (CRRP) and a large language model (LLM) evaluator. CRRP computes rewards using CLAP similarity, applies a repetition penalty and reward clipping to stabilize training, and uses LLM feedback to enhance naturalness. Our method shows outstanding performance in semantic evaluations and both human and AI-based assessments."
   ],
   "p1": 3140,
   "pn": 3144,
   "doi": "10.21437/Interspeech.2025-1313",
   "url": "interspeech_2025/ahn25b_interspeech.html"
  },
  "wang25j_interspeech": {
   "authors": [
    [
     "Haoxu",
     "Wang"
    ],
    [
     "Yiheng",
     "Jiang"
    ],
    [
     "Gang",
     "Qiao"
    ],
    [
     "Pengteng",
     "Shi"
    ],
    [
     "Biao",
     "Tian"
    ]
   ],
   "title": "FLASepformer: Efficient Speech Separation with Gated Focused Linear Attention Transformer",
   "original": "1315",
   "order": 300,
   "page_count": 5,
   "abstract": [
    "Speech separation always faces the challenge of handling prolonged time sequences. Past methods try to reduce sequence lengths and use the Transformer to capture global information. However, due to the quadratic time complexity of the attention module, memory usage and inference time still increase significantly with longer segments. To tackle this, we introduce Focused Linear Attention and build FLASepformer with linear complexity for efficient speech separation. Inspired by SepReformer and TF-Locoformer, we have two variants: FLA-SepReformer and FLA-TFLocoformer. We also add a new Gated module to improve performance further. Experimental results on various datasets show that FLASepformer matches state-of-the-art performance with less memory consumption and faster inference. FLA-SepReformer-T/B/L increases speed by 2.29x, 1.91x, and 1.49x, with 15.8%, 20.9%, and 31.9% GPU memory usage, proving our model&#x27;s effectiveness."
   ],
   "p1": 1468,
   "pn": 1472,
   "doi": "10.21437/Interspeech.2025-1315",
   "url": "interspeech_2025/wang25j_interspeech.html"
  },
  "fukunaga25_interspeech": {
   "authors": [
    [
     "Yoshinori",
     "Fukunaga"
    ],
    [
     "Ryota",
     "Nishimura"
    ],
    [
     "Kengo",
     "Ohta"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "Backchannel prediction for natural spoken dialog systems  using general speaker and listener information ",
   "original": "1316",
   "order": 222,
   "page_count": 5,
   "abstract": [
    "Backchannel responses are a crucial component of conversations enabling more effective communication through listener feedback. Current backchannel prediction models classify these responses into just three categories, using speech, text, and listener IDs. These IDs, which contain detailed personal information, cannot be applied in real-world dialog systems however, and three-category classification limits response generation capabilities. Therefore, we propose a model for predicting a backchannel&#x27;s &#x27;surface form&#x27; using only general speaker and listener embeddings. Our experiments show a 1.3% improvement in prediction accuracy when performing 3-category classification, and a 0.9% improvement when performing 11-category classification, compared to conventional ID embeddings, demonstrating an enhancement in performance that is deployable in real-world systems."
   ],
   "p1": 1078,
   "pn": 1082,
   "doi": "10.21437/Interspeech.2025-1316",
   "url": "interspeech_2025/fukunaga25_interspeech.html"
  },
  "lay25_interspeech": {
   "authors": [
    [
     "Bunlong",
     "Lay"
    ],
    [
     "Rostilav",
     "Makarov"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency",
   "original": "1317",
   "order": 165,
   "page_count": 5,
   "abstract": [
    "Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement."
   ],
   "p1": 793,
   "pn": 797,
   "doi": "10.21437/Interspeech.2025-1317",
   "url": "interspeech_2025/lay25_interspeech.html"
  },
  "wu25h_interspeech": {
   "authors": [
    [
     "Wenxuan",
     "Wu"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction",
   "original": "1321",
   "order": 1031,
   "page_count": 5,
   "abstract": [
    "Audio-visual target speaker extraction (AV-TSE) models primarily rely on target visual cues to isolate the target speaker&#x27;s voice from others. We know that humans leverage linguistic knowledge, such as syntax and semantics, to support speech perception. Inspired by this, we explore the potential of pre-trained speech-language models (PSLMs) and pre-trained language models (PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE model as additional supervision signals. Without introducing any extra computational cost during inference, the proposed approach consistently improves speech quality and intelligibility. Furthermore, we evaluate our method in multi-language settings and visual cue-impaired scenarios and show robust performance gains."
   ],
   "p1": 5053,
   "pn": 5057,
   "doi": "10.21437/Interspeech.2025-1321",
   "url": "interspeech_2025/wu25h_interspeech.html"
  },
  "chuang25_interspeech": {
   "authors": [
    [
     "Yu-Ying",
     "Chuang"
    ],
    [
     "Sheng-Fu",
     "Wang"
    ]
   ],
   "title": "Tonal Variation and Word Meaning in Taiwanese",
   "original": "1325",
   "order": 853,
   "page_count": 5,
   "abstract": [
    "Tone sandhi is extensive in Taiwanese, where all but the final syllable in a morphosyntactically defined unit undergo sandhi. Previous studies have primarily focused on comparing sandhi tonal realizations to their corresponding citation ones to determine whether neutralization occurs. However, such a high-level comparison overlooks the variability in how tones are actually realized. This study aims to examine tonal variation in Taiwanese, focusing on whether word meaning influences tonal realization. Using spontaneous speech data, we analyzed the realizations of the high-falling tone. Our findings indicate that word meaning does contribute to tonal variability. Moreover, when meaning-induced tonal variation is accounted for, no remaining differences between sandhi and citation tones can be observed. These results suggest that tonal realizations systematically vary with word meaning and that the effect of meaning on tonal realizations should be considered when discussing neutralization."
   ],
   "p1": 4183,
   "pn": 4187,
   "doi": "10.21437/Interspeech.2025-1325",
   "url": "interspeech_2025/chuang25_interspeech.html"
  },
  "chen25k_interspeech": {
   "authors": [
    [
     "Hang",
     "Chen"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Juan",
     "Xie"
    ],
    [
     "Shi-Fu",
     "XIong"
    ]
   ],
   "title": "A Study of Real-world Audio-Visual Corpus Design and Production: A Perspective from MISP Challenges",
   "original": "1329",
   "order": 797,
   "page_count": 5,
   "abstract": [
    "With the increasing proliferation of speech-driven applications, the challenges in their deployment environments are becoming more prominent. Accordingly, audio-visual speech processing (AVSP) proposed integrating audio and visual information to enhance performance across many speech-processing tasks. In this context, the MISP challenges were organized at ICASSP 2022, 2023, and 2024, respectively. These challenges released audio-visual corpora to support four core tasks: audio-visual wakeup, diarization, speech enhancement, and recognition. The datasets have garnered attention from the global research community, with over 110 teams downloading the corpora. This paper provides a comprehensive analysis of the MISP corpus design from various perspectives, including scenario selection, recording equipment and processes, as well as manual transcription and alignment, highlighting its strengths and limitations and offering insights and recommendations for the design of future AVSP corpora."
   ],
   "p1": 3903,
   "pn": 3907,
   "doi": "10.21437/Interspeech.2025-1329",
   "url": "interspeech_2025/chen25k_interspeech.html"
  },
  "alradhi25_interspeech": {
   "authors": [
    [
     "Mohammed",
     "Al-Radhi"
    ],
    [
     "Géza",
     "Németh"
    ],
    [
     "Branislav",
     "Gerazov"
    ]
   ],
   "title": "MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based Prosody Prediction and Neural Phase Reconstruction",
   "original": "1334",
   "order": 597,
   "page_count": 5,
   "abstract": [
    "Speech synthesis from intracranial EEG (iEEG) signals offers a promising avenue for restoring communication in individuals with severe speech impairments. However, achieving intelligible and natural speech remains challenging due to limitations in feature representation, prosody modeling, and phase reconstruction. We introduce MiSTR, a deep-learning framework that integrates: 1) Wavelet-based feature extraction to capture fine-grained temporal, spectral, and neurophysiological representations of iEEG signals, 2) A Transformer-based decoder for prosody-aware spectrogram prediction, and 3) A neural phase vocoder enforcing harmonic consistency via adaptive spectral correction. Evaluated on a public iEEG dataset, MiSTR achieves state-of-the-art speech intelligibility, with a mean Pearson correlation of 0.91 between reconstructed and original Mel spectrograms, improving over existing neural speech synthesis baselines."
   ],
   "p1": 2930,
   "pn": 2934,
   "doi": "10.21437/Interspeech.2025-1334",
   "url": "interspeech_2025/alradhi25_interspeech.html"
  },
  "zhang25l_interspeech": {
   "authors": [
    [
     "Bowen",
     "Zhang"
    ],
    [
     "Ian",
     "McLoughlin"
    ],
    [
     "Xiaoxiao",
     "Miao"
    ],
    [
     "AS",
     "Madhukumar"
    ]
   ],
   "title": "LSPnet: an ultra-low bitrate hybrid neural codec",
   "original": "1335",
   "order": 129,
   "page_count": 5,
   "abstract": [
    "This paper presents an ultra-low bitrate speech codec that achieves high-fidelity speech coding at 1.2kbps while maintaining low computational complexity. Building upon the LPCNet framework, combined with a parametric encoder, we introduce several key improvements by incorporating line spectral pairs (LSP) to improve quantization error performance and eliminate explicit LPC estimation by directly predicting the probability distribution of audio samples using a deep neural network, and employing a joint time-frequency training strategy combining short-time Fourier transform (STFT) loss with cross-entropy (CE) loss. The codec is suitable for real-time applications in resource-constrained environments. Experimental results show that the proposed codec not only outperforms traditional speech codecs but also achieves superior speech quality compared to state-of-the-art end-to-end codecs, offering a compelling balance between quality and computational cost."
   ],
   "p1": 614,
   "pn": 618,
   "doi": "10.21437/Interspeech.2025-1335",
   "url": "interspeech_2025/zhang25l_interspeech.html"
  },
  "hoang25_interspeech": {
   "authors": [
    [
     "Long-Vu",
     "Hoang"
    ],
    [
     "Tuan",
     "Nguyen"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment",
   "original": "1336",
   "order": 536,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries."
   ],
   "p1": 2625,
   "pn": 2629,
   "doi": "10.21437/Interspeech.2025-1336",
   "url": "interspeech_2025/hoang25_interspeech.html"
  },
  "kano25_interspeech": {
   "authors": [
    [
     "Takatomo",
     "Kano"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Ryo",
     "Fukuda"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Pick and Summarize: Integrating Extractive and Abstractive Speech Summarization",
   "original": "1341",
   "order": 58,
   "page_count": 5,
   "abstract": [
    "Speech summarization condenses long speech while preserving essential content. Recently, there has been growing interest in end-to-end (E2E) abstractive speech summarization, which directly generates a text summary from spoken input. However, abstractive summarization of lengthy speech sequences presents challenges, such as identifying key information within very long speech. In this paper, we hypothesize that first addressing the simpler task of extractive summarization can help with these aforementioned long-sequence challenges and improve overall summarization performance. To this end, we introduce an extractive-abstractive summarization model that exploits auxiliary information from extractive summaries generated directly from raw speech input to enhance abstractive speech summarization.  Experiments on a web presentation corpus demonstrate consistent gains with our proposed method, achieving up to 1.4-point gains in METEOR score over a strong abstractive summarization baseline."
   ],
   "p1": 281,
   "pn": 285,
   "doi": "10.21437/Interspeech.2025-1341",
   "url": "interspeech_2025/kano25_interspeech.html"
  },
  "song25b_interspeech": {
   "authors": [
    [
     "Jie",
     "Song"
    ],
    [
     "Wang",
     "Xiang"
    ],
    [
     "Jian",
     "Zhou"
    ],
    [
     "Cunhang",
     "Fan"
    ],
    [
     "Zhao",
     "Lv"
    ]
   ],
   "title": "REB-former: RWKV-enhanced E-branchformer for Speech Recognition",
   "original": "1343",
   "order": 790,
   "page_count": 5,
   "abstract": [
    "Transformer-based architectures have achieved significant success in automatic speech recognition (ASR). However, the quadratic complexity of their self-attention mechanisms limits processing efficiency for speech sequences. To address this issue, this paper proposes the Receptance Weighted Key Value (RWKV)-enhanced E-Branchformer (REB-former). Specifically, the REB-former interleaves the E-Branchformer and RWKV layers, combining different attention mechanisms to reduce computational complexity and enhance speech modeling. To overcome RWKV&#x27;s unidirectional limitation, we introduce the GroupBiRWKV module for efficient contextual feature capture. The results show that the REB-former outperforms the E-Branchformer in terms of computational efficiency and inference speed, achieving a relative reduction of up to 7.1% in the word error rate (WER). On the LibriSpeech 100h dataset, our model achieves WER of 6.0%/15.8% on test-clean/test-other, setting a new state-of-the-art performance."
   ],
   "p1": 3868,
   "pn": 3872,
   "doi": "10.21437/Interspeech.2025-1343",
   "url": "interspeech_2025/song25b_interspeech.html"
  },
  "kwon25_interspeech": {
   "authors": [
    [
     "Ki-Joong",
     "Kwon"
    ],
    [
     "Jun-Ho",
     "So"
    ],
    [
     "Sang-Hoon",
     "Lee"
    ]
   ],
   "title": "Parameter-Efficient Fine-Tuning for Low-Resource Text-to-Speech via Cross-Lingual Continual Learning",
   "original": "1344",
   "order": 329,
   "page_count": 5,
   "abstract": [
    "As generative models gain attention, it is crucial to adapt these models efficiently even with limited high-quality data and computational resources. In this work, we investigate a parameter-efficient fine-tuning (PEFT) for low-resource text-to-speech to transfer pre-trained knowledge to a new language leveraging only a single-speaker dataset and a single NVIDIA TITAN RTX GPU. We propose three types of adapters: Conditioning Adapter, Prompt Adapter, and DiT LoRA Adapter, where Conditioning Adapter enhances text embeddings, Prompt Adapter refines input representations, and DiT LoRA Adapter enables speech generation efficiency. We further explore the respective optimal configuration of adapters for single-speaker and multi-speaker scenarios. Consequently, under resource constraints, we successfully achieve effective adaptation to a new language using only 1.72% of the total parameters. Audio samples, source code and checkpoints will be available."
   ],
   "p1": 1613,
   "pn": 1617,
   "doi": "10.21437/Interspeech.2025-1344",
   "url": "interspeech_2025/kwon25_interspeech.html"
  },
  "chung25_interspeech": {
   "authors": [
    [
     "Soo-Whan",
     "Chung"
    ],
    [
     "Min-Seok",
     "Choi"
    ]
   ],
   "title": "Listen through the Sound: Generative Speech Restoration Leveraging Acoustic Context Representation",
   "original": "1352",
   "order": 985,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel approach to speech restoration by integrating a context-related conditioning strategy. Specifically, we employ the diffusion-based generative restoration model, UNIVERSE++, as a backbone to evaluate the effectiveness of contextual representations. We incorporate acoustic context embeddings extracted from the CLAP model, which capture the environmental attributes of input audio. Additionally, we propose an Acoustic Context (ACX) representation that refines CLAP embeddings to better handle various distortion factors and their intensity in speech signals. Unlike content-based approaches that rely on linguistic and speaker attributes, ACX provides contextual information that enables the restoration model to distinguish and mitigate distortions better. Experimental results indicate that context-aware conditioning improves both restoration performance and its stability across diverse distortion conditions, reducing variability compared to content-based methods."
   ],
   "p1": 4843,
   "pn": 4847,
   "doi": "10.21437/Interspeech.2025-1352",
   "url": "interspeech_2025/chung25_interspeech.html"
  },
  "ryu25b_interspeech": {
   "authors": [
    [
     "Myeonghoon",
     "Ryu"
    ],
    [
     "Hongseok",
     "Oh"
    ],
    [
     "Suji",
     "Lee"
    ],
    [
     "Han",
     "Park"
    ]
   ],
   "title": "Unified Microphone Conversion: Many-to-Many Device Mapping via Feature-wise Linear Modulation",
   "original": "1356",
   "order": 273,
   "page_count": 5,
   "abstract": [
    "We present Unified Microphone Conversion, a unified generative framework designed to bolster sound event classification (SEC) systems against device variability. While our prior CycleGAN-based methods effectively simulate device characteristics, they require separate models for each device pair, limiting scalability. Our approach overcomes this constraint by conditioning the generator on frequency response data, enabling many-to-many device mappings through unpaired training. We integrate frequency-response information via Feature-wise Linear Modulation, further enhancing scalability. Additionally, incorporating synthetic frequency response differences improves the applicability of our framework for real-world application. Experimental results show that our method outperforms the state-of-the-art by 2.6% and reduces variability by 0.8% in macro-average F1 score."
   ],
   "p1": 1333,
   "pn": 1337,
   "doi": "10.21437/Interspeech.2025-1356",
   "url": "interspeech_2025/ryu25b_interspeech.html"
  },
  "tzeng25_interspeech": {
   "authors": [
    [
     "Jing-Tong",
     "Tzeng"
    ],
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Ya-Tse",
     "Wu"
    ],
    [
     "Hsing-Hang",
     "Chou"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Lessons Learnt: Revisit Key Training Strategies for Effective Speech Emotion Recognition in the Wild",
   "original": "1357",
   "order": 958,
   "page_count": 5,
   "abstract": [
    "In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild."
   ],
   "p1": 4708,
   "pn": 4712,
   "doi": "10.21437/Interspeech.2025-1357",
   "url": "interspeech_2025/tzeng25_interspeech.html"
  },
  "zhang25m_interspeech": {
   "authors": [
    [
     "Bowen",
     "Zhang"
    ],
    [
     "Nur Afiqah Abdul",
     "Latiff"
    ],
    [
     "Justin",
     "Kan"
    ],
    [
     "Rong",
     "Tong"
    ],
    [
     "Donny",
     "Soh"
    ],
    [
     "Xiaoxiao",
     "Miao"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "Automated evaluation of children's speech fluency for low-resource languages",
   "original": "1358",
   "order": 397,
   "page_count": 5,
   "abstract": [
    "Assessment of children&#x27;s speaking fluency in education is well researched for majority languages, but remains highly challenging for low resource languages. This paper proposes a system to automatically assess fluency by combining a fine-tuned multilingual ASR model, an objective metrics extraction stage, and a generative pre-trained transformer (GPT) network. The objective metrics include phonetic and word error rates, speech speed, and speech-pause duration ratio. These are interpreted by a GPT-based classifier guided by a small set of human-evaluated ground truth examples, to score fluency. We evaluate the proposed system on a dataset of children&#x27;s speech in two low-resource languages, Tamil and Malay and compare the classification performance against Random Forest and XGBoost, as well as using ChatGPT-4o to predict fluency directly from speech input. Results demonstrate that the proposed approach achieves significantly higher accuracy than multimodal GPT or other methods."
   ],
   "p1": 1948,
   "pn": 1952,
   "doi": "10.21437/Interspeech.2025-1358",
   "url": "interspeech_2025/zhang25m_interspeech.html"
  },
  "mimura25_interspeech": {
   "authors": [
    [
     "Masato",
     "Mimura"
    ],
    [
     "Jaeyoung",
     "Lee"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Switch Conformer with Universal Phonetic Experts for Multilingual ASR",
   "original": "1359",
   "order": 232,
   "page_count": 5,
   "abstract": [
    "Multilingual end-to-end ASR presents significant challenges due to the need to accommodate diverse writing systems, lexicons, and grammatical structures. Existing methods often rely on large models with high computational costs for adequate cross-language performance. To address this, we propose the switch Conformer, which enhances model capacity while maintaining nearly the same inference cost as a standard Conformer. Our approach replaces the FFN module in each Conformer block with a sparse mixture of independent experts, activating only one expert per input to enable efficient language-specific feature learning. In addition, a shared expert trained with phonetic supervision captures language-universal speech characteristics. Experiments on streaming ASR using the CommonVoice dataset demonstrate that these experts work synergistically to achieve better performance than the baseline Conformer, with minimal additional active parameters."
   ],
   "p1": 1128,
   "pn": 1132,
   "doi": "10.21437/Interspeech.2025-1359",
   "url": "interspeech_2025/mimura25_interspeech.html"
  },
  "panda25_interspeech": {
   "authors": [
    [
     "Ashish",
     "Panda"
    ],
    [
     "Sunil Kumar",
     "Kopparapu"
    ]
   ],
   "title": "EmbedAug: An Augmentation Scheme for End-to-End Automatic Speech Recognition",
   "original": "1360",
   "order": 697,
   "page_count": 5,
   "abstract": [
    "Data augmentation plays a significant role in making automatic speech recognition (ASR) systems robust against unseen test data. Most of the existing data augmentation techniques are designed to work on the speech features. Augmentation of speech embeddings, within the neural network, e.g., the inputsto encoders, are relatively unexplored. We present a simple yet effective augmentation scheme, EmbedAug, which works by replacing a set of randomly selected speech embeddings by either zeros or Gaussian noise during training. EmbedAug does not require additional data, works online during training and adds very little to the overall computational cost. Using Librispeech 100h, Librispeech 960h and MUCS21 multilingual dataset, we show that the proposed EmbedAug is very effective in improving the robustness of ASR systems. Moreover, EmbedAug can be fine tuned on the development set with the help of just one hyperparameter."
   ],
   "p1": 3429,
   "pn": 3433,
   "doi": "10.21437/Interspeech.2025-1360",
   "url": "interspeech_2025/panda25_interspeech.html"
  },
  "hojo25_interspeech": {
   "authors": [
    [
     "Naoki",
     "Hojo"
    ],
    [
     "Ryoichi",
     "Takashima"
    ],
    [
     "Chihiro",
     "Sugiyama"
    ],
    [
     "Nobukazu",
     "Tanaka"
    ],
    [
     "Kanji",
     "Nohara"
    ],
    [
     "Kazunori",
     "Nozaki"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ]
   ],
   "title": "Revisiting WFST-based Hybrid Japanese Speech Recognition System for Individuals with Organic Speech Disorders",
   "original": "1362",
   "order": 383,
   "page_count": 5,
   "abstract": [
    "End-to-end automatic speech recognition (ASR) technology has advanced significantly, yet it remains ineffective for individuals with speech disorders. In particular, Japanese ASR faces unique challenges due to its diverse character set, including kanji (Chinese characters), hiragana, and katakana (Japanese phonetic syllabary). Adapting an end-to-end ASR model to this task requires extensive training data. In this paper, we revisit a WFST-based hybrid ASR system that decomposes the system into an acoustic model, a pronunciation dictionary, and a language model. This approach is effective when speech data is limited because it uses speech data only to train the acoustic model, while the other components learn only from text data. In addition, to enhance the acoustic model, we introduce multi-step model adaptation using synthetic speech. Experimental results with speakers with organic speech disorders demonstrated that the proposed system outperformed Whisper."
   ],
   "p1": 1878,
   "pn": 1882,
   "doi": "10.21437/Interspeech.2025-1362",
   "url": "interspeech_2025/hojo25_interspeech.html"
  },
  "saijo25_interspeech": {
   "authors": [
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Samuele",
     "Cornell"
    ],
    [
     "Robin",
     "Scheibler"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Zhaoheng",
     "Ni"
    ],
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Marvin",
     "Sach"
    ],
    [
     "Yihui",
     "Fu"
    ],
    [
     "Wei",
     "Wang"
    ],
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Interspeech 2025 URGENT Speech Enhancement Challenge",
   "original": "1363",
   "order": 178,
   "page_count": 5,
   "abstract": [
    "There has been a growing effort to develop universal speech enhancement (SE) to handle inputs with various speech distortions and recording conditions. The URGENT Challenge series aims to foster such universal SE by embracing a broad range of distortion types, increasing data diversity, and incorporating extensive evaluation metrics. This work introduces the Interspeech 2025 URGENT Challenge, the second edition of the series, to explore several aspects that have received limited attention so far: language dependency, universality for more distortion types, data scalability, and the effectiveness of using noisy training data. We received 32 submissions, where the best system uses a discriminative model, while most other competitive ones are hybrid methods. Analysis reveals some key findings: (i) some generative or hybrid approaches are preferred in subjective evaluations over the top discriminative model, and (ii) purely generative SE models can exhibit language dependency."
   ],
   "p1": 858,
   "pn": 862,
   "doi": "10.21437/Interspeech.2025-1363",
   "url": "interspeech_2025/saijo25_interspeech.html"
  },
  "zhao25g_interspeech": {
   "authors": [
    [
     "Zijing",
     "Zhao"
    ],
    [
     "Kai",
     "Wang"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Ying",
     "Hu"
    ],
    [
     "Liang",
     "He"
    ],
    [
     "Jichen",
     "Yang"
    ]
   ],
   "title": "VS-Singer: Vision-Guided Stereo Singing Voice Synthesis with Consistency Schrödinger Bridge",
   "original": "1364",
   "order": 258,
   "page_count": 5,
   "abstract": [
    "To explore the potential advantages of utilizing spatial cues from images for generating stereo singing voices with room reverberation, we introduce VS-Singer, a vision-guided model designed to produce stereo singing voices with room reverberation from scene images. VS-Singer comprises three modules: firstly, a modal interaction network integrates spatial features into text encoding to create a linguistic representation enriched with spatial information. Secondly, the decoder employs a consistency Schrödinger bridge to facilitate one-step sample generation. Moreover, we utilize the SFE module to improve the consistency of audio-visual matching. To our knowledge, this study is the first to combine stereo singing voice synthesis with visual acoustic matching within a unified framework. Experimental results demonstrate that VS-Singer can effectively generate stereo singing voices that align with the scene perspective in a single step."
   ],
   "p1": 1258,
   "pn": 1262,
   "doi": "10.21437/Interspeech.2025-1364",
   "url": "interspeech_2025/zhao25g_interspeech.html"
  },
  "kim25o_interspeech": {
   "authors": [
    [
     "Byeong Hyeon",
     "Kim"
    ],
    [
     "Hyungseob",
     "Lim"
    ],
    [
     "Inseon",
     "Jang"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Towards an Ultra-Low-Delay Neural Audio Coding with Computational Efficiency",
   "original": "1369",
   "order": 125,
   "page_count": 5,
   "abstract": [
    "Recent studies on neural audio codecs (NACs) have primarily focused on improving audio quality in extremely low bit-rate scenarios. However, they have not thoroughly explored the impact of latency. In this work, we first demonstrate that NACs can achieve high-quality reconstruction with an algorithmic delay below 1 ms, albeit at substantial computational costs. To address this challenge, we propose DualStream, a novel framework designed to significantly reduce computational costs in ultra-low-delay settings. DualStream integrates a lightweight encoding module with a small down-sampling ratio to maintain low algorithmic delay, combined with a larger module with a higher down-sampling ratio that processes time-delayed inputs to improve efficiency without introducing additional delay. Experimental results demonstrate that DualStream, with an algorithmic delay of 0.7 ms, achieves comparable performance to conventional NACs while reducing computational costs by approximately 40%."
   ],
   "p1": 594,
   "pn": 598,
   "doi": "10.21437/Interspeech.2025-1369",
   "url": "interspeech_2025/kim25o_interspeech.html"
  },
  "choi25d_interspeech": {
   "authors": [
    [
     "Woongjib",
     "Choi"
    ],
    [
     "Byeong Hyeon",
     "Kim"
    ],
    [
     "Hyungseob",
     "Lim"
    ],
    [
     "Inseon",
     "Jang"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Neural Spectral Band Generation for Audio Coding ",
   "original": "1370",
   "order": 131,
   "page_count": 5,
   "abstract": [
    "Spectral band replication (SBR) enables bit-efficient coding by generating high-frequency bands from the low-frequency ones. However, it only utilizes coarse spectral features upon a subband-wise signal replication, limiting adaptability to diverse acoustic signals. In this paper, we explore the efficacy of a deep neural network (DNN)-based generative approach for coding the high-frequency bands, which we call neural spectral band generation (n-SBG). Specifically, we propose a DNN-based encoder-decoder structure to extract and quantize the side information related to the high-frequency components and generate the components given both the side information and the decoded core-band signals. The whole coding pipeline is optimized with generative adversarial criteria to enable the generation of perceptually plausible sound. From experiments using AAC as the core codec, we show that the proposed method achieves a better perceptual quality than HE-AAC-v1 with much less side information."
   ],
   "p1": 624,
   "pn": 628,
   "doi": "10.21437/Interspeech.2025-1370",
   "url": "interspeech_2025/choi25d_interspeech.html"
  },
  "pan25d_interspeech": {
   "authors": [
    [
     "Zexu",
     "Pan"
    ],
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Tingting",
     "Wang"
    ],
    [
     "Kun",
     "Zhou"
    ],
    [
     "Yukun",
     "Ma"
    ],
    [
     "Chong",
     "Zhang"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
   "original": "1371",
   "order": 394,
   "page_count": 5,
   "abstract": [
    "Audio-visual speaker extraction isolates a target speaker&#x27;s speech from a mixture speech signal conditioned on a visual cue, typically using the target speaker&#x27;s face recording. However, in real-world scenarios, other co-occurring faces are often present on-screen, providing valuable speaker activity cues in the scene. In this work, we introduce a plug-and-play inter-speaker attention module to process these flexible numbers of co-occurring faces, allowing for more accurate speaker extraction in complex multi-person environments. We integrate our module into two prominent models: the AV-DPRNN and the state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets, including the highly overlapped VoxCeleb2 and sparsely overlapped MISP, demonstrate that our approach consistently outperforms baselines. Furthermore, cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and generalizability of our method."
   ],
   "p1": 1933,
   "pn": 1937,
   "doi": "10.21437/Interspeech.2025-1371",
   "url": "interspeech_2025/pan25d_interspeech.html"
  },
  "kim25p_interspeech": {
   "authors": [
    [
     "June-Woo",
     "Kim"
    ],
    [
     "Wonkyo",
     "Oh"
    ],
    [
     "Haram",
     "Yoon"
    ],
    [
     "Sung-Hoon",
     "Yoon"
    ],
    [
     "Dae-Jin",
     "Kim"
    ],
    [
     "Dong-Ho",
     "Lee"
    ],
    [
     "Sang-Yeol",
     "Lee"
    ],
    [
     "Chan-Mo",
     "Yang"
    ]
   ],
   "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models",
   "original": "1372",
   "order": 90,
   "page_count": 5,
   "abstract": [
    "Suicidal risk detection in adolescents is a critical challenge, yet existing methods rely on language-specific models, limiting scalability and generalization. This study introduces a novel language-agnostic framework for suicidal risk assessment with large language models (LLMs). We generate Chinese transcripts from speech using an ASR model and then employ LLMs with prompt-based queries to extract suicidal risk-related features from these transcripts. The extracted features are retained in both Chinese and English to enable cross-linguistic analysis and then used to fine-tune corresponding pretrained language models independently. Experimental results show that our method achieves performance comparable to direct fine-tuning with ASR results or to models trained solely on Chinese suicidal risk-related features, demonstrating its potential to overcome language constraints and improve the robustness of suicidal risk assessment."
   ],
   "p1": 419,
   "pn": 423,
   "doi": "10.21437/Interspeech.2025-1372",
   "url": "interspeech_2025/kim25p_interspeech.html"
  },
  "xiang25c_interspeech": {
   "authors": [
    [
     "Shanshan",
     "Xiang"
    ],
    [
     "Hankiz",
     "Yilahun"
    ],
    [
     "Askar",
     "Hamdulla"
    ]
   ],
   "title": "Speech Mutil-label Emotion Recognition Using Asymmetric Class Loss Function Based on Effective Samples",
   "original": "1373",
   "order": 913,
   "page_count": 5,
   "abstract": [
    "Although significant progress has been made in Speech Emotion Recognition (SER), most current research focuses on single-label emotion classification, while multi-label SER faces low classification accuracy due to the long-tail data distribution (e.g., many happiness samples and few fear samples). To address this, we propose an Asymmetric Category Loss Function based on Effective Sample Selection (ER-ASCL) to alleviate the data imbalance. This method adjusts the asymmetric loss by calculating class weights from effective samples and introduces a label-guided matrix to enhance the relationships between emotion categories, improving classification accuracy. Experimental results show that on the CMU-MOSEI multimodal dataset, using the audio modality, the model achieved 53.5% accuracy, a 2.1% improvement over the best existing method. On the Chinese multi-label emotion dataset CNSCED, the model achieved 69.23% accuracy, outperforming existing multi-label classification loss functions."
   ],
   "p1": 4483,
   "pn": 4487,
   "doi": "10.21437/Interspeech.2025-1373",
   "url": "interspeech_2025/xiang25c_interspeech.html"
  },
  "li25p_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Li"
    ],
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Jianheng",
     "Zhuo"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Liliang",
     "Tang"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts",
   "original": "1374",
   "order": 234,
   "page_count": 5,
   "abstract": [
    "Recent advancements in deep learning have significantly enhanced multilingual automatic speech recognition (ASR) due to the development of advanced model architectures and available large-scale multilingual datasets. Despite that, multilingual ASR still suffers from the curse of multilinguality in that different languages tend to interfere with each other, making it difficult for the ASR model to identify multiple languages effectively while sharing model capacity across them. This paper proposes an efficient finetuning framework for customized multilingual ASR via prepared LoRA language experts based on Whisper. Through LoRA expert fusion or knowledge distillation, our approach achieves better recognition performance on target languages than standard fine-tuning methods. Experimental results demonstrate that the proposed models yield approximately 10% and 15% relative performance gains in language-aware and language-agnostic scenarios, respectively."
   ],
   "p1": 1138,
   "pn": 1142,
   "doi": "10.21437/Interspeech.2025-1374",
   "url": "interspeech_2025/li25p_interspeech.html"
  },
  "phan25_interspeech": {
   "authors": [
    [
     "Nhan",
     "Phan"
    ],
    [
     "Mikko",
     "Kuronen"
    ],
    [
     "Maria",
     "Kautonen"
    ],
    [
     "Riikka",
     "Ullakonoja"
    ],
    [
     "Anna",
     "von Zansen"
    ],
    [
     "Yaroslav",
     "Getman"
    ],
    [
     "Ekaterina",
     "Voskoboinik"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish",
   "original": "1375",
   "order": 498,
   "page_count": 5,
   "abstract": [
    "Mispronunciation detection (MD) models are the cornerstones of many language learning applications. Unfortunately, most systems are built for English and other major languages, while low-resourced language varieties, such as Finland Swedish (FS), lack such tools. In this paper, we introduce our MD model for FS, trained on 89 hours of first language (L1) speakers&#x27; spontaneous speech and tested on 33 minutes of L2 transcribed read-aloud speech. We trained a multilingual wav2vec 2.0 model with entropy regularization, followed by temperature scaling and top-k normalization after the inference to better adapt it for MD. The main novelty of our method lies in its simplicity, requiring minimal L2 data. The process is also language-independent, making it suitable for other low-resource languages. Our proposed algorithm allows us to balance Recall (43.2%) and Precision (29.8%), compared with the baseline model&#x27;s Recall (77.5%) and Precision (17.6%)."
   ],
   "p1": 2435,
   "pn": 2439,
   "doi": "10.21437/Interspeech.2025-1375",
   "url": "interspeech_2025/phan25_interspeech.html"
  },
  "nie25_interspeech": {
   "authors": [
    [
     "Jingping",
     "Nie"
    ],
    [
     "Tien Dung",
     "Tran"
    ],
    [
     "Karan",
     "Thakkar"
    ],
    [
     "Vasudha",
     "Kowtha"
    ],
    [
     "Jon",
     "Huang"
    ],
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Erdrin",
     "Azemi"
    ],
    [
     "Vikramjit",
     "Mitra"
    ]
   ],
   "title": "Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation",
   "original": "1376",
   "order": 411,
   "page_count": 5,
   "abstract": [
    "Auscultation, particularly heart sound, is a non-invasive technique that provides essential vital sign information. Recently, self-supervised acoustic representation foundation models (FMs) have been proposed to offer insights into acoustics-based vital signs. However, there has been little exploration of the extent to which auscultation is encoded in these pre-trained FM representations. In this work, using a publicly available phonocardiogram (PCG) dataset and a heart rate (HR) estimation model, we conduct a layer-wise investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM, Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP model against acoustic features as baseline. Notably, HR estimation using the representations from the audio encoder of the in-house CLAP model outperforms the results obtained from the baseline, achieving a lower mean absolute error (MAE) across various train/validation/test splits despite the domain mismatch."
   ],
   "p1": 2018,
   "pn": 2022,
   "doi": "10.21437/Interspeech.2025-1376",
   "url": "interspeech_2025/nie25_interspeech.html"
  },
  "yamamoto25_interspeech": {
   "authors": [
    [
     "Katsuhiko",
     "Yamamoto"
    ],
    [
     "Koichi",
     "Miyazaki"
    ]
   ],
   "title": "Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners",
   "original": "1377",
   "order": 1114,
   "page_count": 5,
   "abstract": [
    "Speech intelligibility prediction (SIP) models have been used as objective metrics to assess intelligibility for hearing-impaired (HI) listeners. In the Clarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based on transformers showed high prediction accuracy. However, the self-attention mechanism theoretically incurs high computational and memory costs, making it a bottleneck for low-latency, power-efficient devices. This may also degrade the temporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP models instead of transformers for the temporal processing blocks. Experimental results show that our proposed SIP model achieves competitive performance compared to the baseline while maintaining a relatively small number of parameters. Our analysis suggests that the SIP model based on bidirectional Mamba effectively captures contextual and spatial speech information from binaural signals."
   ],
   "p1": 5463,
   "pn": 5467,
   "doi": "10.21437/Interspeech.2025-1377",
   "url": "interspeech_2025/yamamoto25_interspeech.html"
  },
  "yang25j_interspeech": {
   "authors": [
    [
     "Zijian",
     "Yang"
    ],
    [
     "Minh-Nghia",
     "Phan"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": " Label-Context-Dependent Internal Language Model Estimation for CTC",
   "original": "1378",
   "order": 1059,
   "page_count": 5,
   "abstract": [
    "Although connectionist temporal classification (CTC) has the label context independence assumption, it can still implicitly learn a context-dependent internal language model (ILM) due to modern powerful encoders. In this work, we investigate the implicit context dependency modeled in the ILM of CTC. To this end, we propose novel context-dependent ILM estimation methods for CTC based on knowledge distillation (KD) with theoretical justifications. Furthermore, we introduce two regularization methods for KD. We conduct experiments on Librispeech and TED-LIUM Release 2 datasets for in-domain and cross-domain evaluation, respectively. Experimental results show that context-dependent ILMs outperform the context-independent priors in cross-domain evaluation, indicating that CTC learns a context-dependent ILM. The proposed label-level KD with smoothing method surpasses other ILM estimation approaches, with more than 13% relative improvement in word error rate compared to shallow fusion."
   ],
   "p1": 5193,
   "pn": 5197,
   "doi": "10.21437/Interspeech.2025-1378",
   "url": "interspeech_2025/yang25j_interspeech.html"
  },
  "chen25l_interspeech": {
   "authors": [
    [
     "Tuochao",
     "Chen"
    ],
    [
     "D",
     "Shin"
    ],
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Sinan",
     "Hersek"
    ]
   ],
   "title": "SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction",
   "original": "1379",
   "order": 195,
   "page_count": 5,
   "abstract": [
    "This paper introduces SoundSculpt, a neural network designed to extract target sound fields from ambisonic recordings. SoundSculpt employs an ambisonic-in-ambisonic-out architecture and is conditioned on both spatial information (e.g., target direction obtained by pointing at an immersive video) and semantic embeddings (e.g., derived from image segmentation and captioning). Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt demonstrates superior performance compared to various signal processing baselines. Our results further reveal that while spatial conditioning alone can be effective, the combination of spatial and semantic information is beneficial in scenarios where there are secondary sound sources spatially close to the target. Additionally, we compare two different semantic embeddings derived from a text description of the target sound using text encoders."
   ],
   "p1": 943,
   "pn": 947,
   "doi": "10.21437/Interspeech.2025-1379",
   "url": "interspeech_2025/chen25l_interspeech.html"
  },
  "wang25k_interspeech": {
   "authors": [
    [
     "Honghong",
     "Wang"
    ],
    [
     "Jing",
     "Deng"
    ],
    [
     "Fanqin",
     "Meng"
    ],
    [
     "Rong",
     "Zheng"
    ]
   ],
   "title": "Enhancing Speech Emotion Recognition with Multi-Task Learning and Dynamic Feature Fusion",
   "original": "1380",
   "order": 959,
   "page_count": 5,
   "abstract": [
    "This study investigates fine-tuning self-supervised learning (SSL) models using multi-task learning (MTL) to enhance speech emotion recognition (SER). The framework simultaneously handles four related tasks: emotion recognition, gender recognition, speaker verification, and automatic speech recognition. An innovative co-attention module is introduced to dynamically capture the interactions between features from the primary emotion classification task and auxiliary tasks, enabling context-aware fusion. Moreover, We introduce the Sample Weighted Focal Contrastive (SWFC) loss function to address class imbalance and semantic confusion by adjusting sample weights for difficult and minority samples. The method is validated on the Categorical Emotion Recognition task of the Speech Emotion Recognition in Naturalistic Conditions Challenge, showing significant performance improvements."
   ],
   "p1": 4713,
   "pn": 4717,
   "doi": "10.21437/Interspeech.2025-1380",
   "url": "interspeech_2025/wang25k_interspeech.html"
  },
  "birkholz25b_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Dominik",
     "Schäfer"
    ],
    [
     "Patrick",
     "Häsner"
    ],
    [
     "Jihyeon",
     "Yun"
    ],
    [
     "Iris",
     "Kruppke"
    ],
    [
     "Rémi",
     "Blandin"
    ]
   ],
   "title": "Influence of wall coverings of 3D-printed vocal tract models on measured transfer functions ",
   "original": "1381",
   "order": 708,
   "page_count": 5,
   "abstract": [
    "Physical replicas of the vocal tract are often produced by 3D-printing. A proven method to obtain the acoustic transfer function of these models is based on the principle of reciprocity, where an external sound source is used and the acoustic response is measured inside the model at the glottal end. However, earlier investigations showed that for some model geometries the measured transfer functions deviated from the expected resonance pattern due to spurious poles and zeros. These artifacts are probably caused by sound transmission through the model walls, or by whole-body vibrations of the replicas. In this empirical study, two methods to reduce these artifacts were compared, namely wrapping the models with sound-absorbing fabric and embedding the models in sand. Axisymmetric 3D-printed tubes for ten different vowels were analyzed. The results showed that both methods reduce artifacts and at the same time improve the repeatability of the measurements."
   ],
   "p1": 3479,
   "pn": 3483,
   "doi": "10.21437/Interspeech.2025-1381",
   "url": "interspeech_2025/birkholz25b_interspeech.html"
  },
  "luo25b_interspeech": {
   "authors": [
    [
     "Longjie",
     "Luo"
    ],
    [
     "Shenghui",
     "Lu"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge",
   "original": "1382",
   "order": 384,
   "page_count": 5,
   "abstract": [
    "This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE models&#x27; training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place."
   ],
   "p1": 1883,
   "pn": 1887,
   "doi": "10.21437/Interspeech.2025-1382",
   "url": "interspeech_2025/luo25b_interspeech.html"
  },
  "grigoryan25_interspeech": {
   "authors": [
    [
     "Lilit",
     "Grigoryan"
    ],
    [
     "Vladimir",
     "Bataev"
    ],
    [
     "Andrei",
     "Andrusenko"
    ],
    [
     "Hainan",
     "Xu"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Pushing the Limits of Beam Search Decoding  for Transducer-based ASR models",
   "original": "1388",
   "order": 136,
   "page_count": 5,
   "abstract": [
    "Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced."
   ],
   "p1": 649,
   "pn": 653,
   "doi": "10.21437/Interspeech.2025-1388",
   "url": "interspeech_2025/grigoryan25_interspeech.html"
  },
  "cho25_interspeech": {
   "authors": [
    [
     "Deok-Hyeon",
     "Cho"
    ],
    [
     "Hyung-Seok",
     "Oh"
    ],
    [
     "Seung-Bin",
     "Kim"
    ],
    [
     "Seong-Whan",
     "Lee"
    ]
   ],
   "title": "EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification",
   "original": "1391",
   "order": 947,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition predicts a speaker&#x27;s emotional state from speech signals using discrete labels or continuous dimensions such as arousal, valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that integrates spherical VAD region classification to guide VAD regression for improved emotion prediction. In our framework, VAD values are transformed into spherical coordinates that are divided into multiple spherical regions, and an auxiliary classification task predicts which spherical region each point belongs to, guiding the regression process. Additionally, we incorporate a dynamic weighting scheme and a style pooling layer with multi-head self-attention to capture spectral and temporal dynamics, further boosting performance. This combined training strategy reinforces structured learning and improves prediction consistency. Experimental results show that our approach exceeds baseline methods, confirming the validity of the proposed framework."
   ],
   "p1": 4653,
   "pn": 4657,
   "doi": "10.21437/Interspeech.2025-1391",
   "url": "interspeech_2025/cho25_interspeech.html"
  },
  "cho25b_interspeech": {
   "authors": [
    [
     "Deok-Hyeon",
     "Cho"
    ],
    [
     "Hyung-Seok",
     "Oh"
    ],
    [
     "Seung-Bin",
     "Kim"
    ],
    [
     "Seong-Whan",
     "Lee"
    ]
   ],
   "title": "DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech",
   "original": "1394",
   "order": 891,
   "page_count": 5,
   "abstract": [
    "Cross-speaker emotion transfer in speech synthesis relies on extracting speaker-independent emotion embeddings for accurate emotion modeling without retaining speaker traits. However, existing timbre compression methods fail to fully separate speaker and emotion characteristics, causing speaker leakage and degraded synthesis quality. To address this, we propose DiEmo-TTS, a self-supervised distillation method to minimize emotional information loss and preserve speaker identity. We introduce cluster-driven sampling and information perturbation to preserve emotion while removing irrelevant factors. To facilitate this process, we propose an emotion clustering and matching approach using emotional attribute prediction and speaker embeddings, enabling generalization to unlabeled data. Additionally, we designed a dual conditioning transformer to integrate style features better. Experimental results confirm the effectiveness of our method in learning speaker-irrelevant emotion embeddings."
   ],
   "p1": 4373,
   "pn": 4377,
   "doi": "10.21437/Interspeech.2025-1394",
   "url": "interspeech_2025/cho25b_interspeech.html"
  },
  "tan25_interspeech": {
   "authors": [
    [
     "Fengyun",
     "Tan"
    ],
    [
     "Tao",
     "Wei"
    ],
    [
     "Kun",
     "Zou"
    ],
    [
     "Ning",
     "Cheng"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Enhancing Serialized Output Training for Multi-Talker ASR with Soft Monotonic Alignment and Utterance-level Timestamp",
   "original": "1396",
   "order": 327,
   "page_count": 5,
   "abstract": [
    "Multi-talker ASR has gained significant attention due to its broad applications in conference settings. The previously proposed SOT stands out among many approaches. However, when multiple speakers talk simultaneously, predicting the speaker change symbol becomes more challenging. Boundary-Aware SOT (BA-SOT) uses multi-task to learn speaker change point, increasing complexity and training cost, without timestamp prediction. Therefore, we propose enhanced SOT, called Soft Monotonic Alignment SOT (SMA-SOT), which introduces SMA Loss and utterance-level timestamps. These two components complement each other, not only utilizing timestamps to promote monotonic alignment constraint learning but also, in turn, making timestamp prediction more accurate through the SMA Loss. Experimental results on AliMeeting test set show that SMA-SOT achieves a 5.4% and 0.35% relative CER reduction compared to the SOT and BA-SOT respectively and achieves a Speaker Change Accuracy (SCA) of 81.3%."
   ],
   "p1": 1603,
   "pn": 1607,
   "doi": "10.21437/Interspeech.2025-1396",
   "url": "interspeech_2025/tan25_interspeech.html"
  },
  "choi25e_interspeech": {
   "authors": [
    [
     "Joon-Seung",
     "Choi"
    ],
    [
     "Dong-Min",
     "Byun"
    ],
    [
     "Hyung-Seok",
     "Oh"
    ],
    [
     "Seong-Whan",
     "Lee"
    ]
   ],
   "title": "VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion",
   "original": "1397",
   "order": 253,
   "page_count": 5,
   "abstract": [
    "Controlling singing style is crucial for achieving an expressive and natural singing voice. Among the various style factors, vibrato plays a key role in conveying emotions and enhancing musical depth. However, modeling vibrato remains challenging due to its dynamic nature, making it difficult to control in singing voice conversion. To address this, we propose VibE-SVC, a controllable singing voice conversion model that explicitly extracts and manipulates vibrato using discrete wavelet transform. Unlike previous methods that model vibrato implicitly, our approach decomposes the F0 contour into frequency components, enabling precise transfer. This allows vibrato control for enhanced flexibility. Experimental results show that VibE-SVC effectively transforms singing styles while preserving speaker similarity. Both subjective and objective evaluations confirm high-quality conversion."
   ],
   "p1": 1233,
   "pn": 1237,
   "doi": "10.21437/Interspeech.2025-1397",
   "url": "interspeech_2025/choi25e_interspeech.html"
  },
  "mitsumori25_interspeech": {
   "authors": [
    [
     "Shunsuke",
     "Mitsumori"
    ],
    [
     "Sara",
     "Kashiwagi"
    ],
    [
     "Keitaro",
     "Tanaka"
    ],
    [
     "Shigeo",
     "Morishima"
    ]
   ],
   "title": "Cross-lingual Data Selection Using Clip-level Acoustic Similarity for Enhancing Low-resource Automatic Speech Recognition",
   "original": "1399",
   "order": 674,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel donor data selection method to enhance low-resource automatic speech recognition (ASR). While ASR performs well in high-resource languages, its accuracy declines in low-resource settings due to limited training data. A common solution is to leverage multilingual self-supervised learning (SSL) models with donor languages. However, existing methods rely on language-level similarity, overlooking clip-level variations. To address this limitation, we propose clip-wise acoustic token distribution similarity (CATDS), a fine-grained selection method that identifies acoustically relevant donor clips for better alignment with the target language. Unlike existing clip-level selection methods, our method aligns with the representation of SSL models and offers more challenging yet valuable samples. Experimental results show that CATDS outperforms traditional selection methods and can even utilize donor languages previously considered detrimental."
   ],
   "p1": 3314,
   "pn": 3318,
   "doi": "10.21437/Interspeech.2025-1399",
   "url": "interspeech_2025/mitsumori25_interspeech.html"
  },
  "khanday25_interspeech": {
   "authors": [
    [
     "Owais Mujtaba",
     "Khanday"
    ],
    [
     "Pablo Rodríguez San",
     "Esteban"
    ],
    [
     "Zubair Ahmad",
     "Lone"
    ],
    [
     "Marc",
     "Ouellet"
    ],
    [
     "Jose A.",
     "Gonzalez-Lopez"
    ]
   ],
   "title": "Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings",
   "original": "1400",
   "order": 1132,
   "page_count": 5,
   "abstract": [
    "Understanding how neural activity encodes speech and language production is a fundamental challenge in neuroscience and artificial intelligence. This study investigates whether embeddings from large-scale, self-supervised language and speech models can effectively reconstruct high-gamma neural activity characteristics, key indicators of cortical processing, recorded during speech production. We use pre-trained embeddings from deep learning models on linguistic and acoustic data to map high-level speech features onto high-gamma signals. We analyze the extent to which these embeddings preserve the spatio-temporal dynamics of brain activity. Reconstructed neural signals are evaluated against high-gamma ground-truth activity using correlation metrics and signal reconstruction quality assessments. The results indicate High-gamma activity was effectively reconstructed using language and speech model embeddings, yielding Pearson correlation coefficients of 0.79-0.99 across all participants."
   ],
   "p1": 5553,
   "pn": 5557,
   "doi": "10.21437/Interspeech.2025-1400",
   "url": "interspeech_2025/khanday25_interspeech.html"
  },
  "puhach25_interspeech": {
   "authors": [
    [
     "Dariia",
     "Puhach"
    ],
    [
     "Amir H.",
     "Payberah"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM",
   "original": "1402",
   "order": 419,
   "page_count": 5,
   "abstract": [
    "Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark’s speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations."
   ],
   "p1": 2058,
   "pn": 2062,
   "doi": "10.21437/Interspeech.2025-1402",
   "url": "interspeech_2025/puhach25_interspeech.html"
  },
  "nguyen25c_interspeech": {
   "authors": [
    [
     "Tuan-Nam",
     "Nguyen"
    ],
    [
     "Ngoc-Quan",
     "Pham"
    ],
    [
     "Şeymanur",
     "Akti"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement ",
   "original": "1403",
   "order": 849,
   "page_count": 5,
   "abstract": [
    "We propose a first streaming accent conversion (AC) model that transforms non-native speech into a native-like accent while preserving speaker identity, prosody and improving pronunciation. Our approach enables stream processing by modifying a previous AC architecture with an Emformer encoder and an optimized inference mechanism. Additionally, we integrate a native text-to-speech (TTS) model to generate ideal ground-truth data for efficient training. Our streaming AC model achieves comparable performance to the top AC models while maintaining stable latency, making it the first AC system capable of streaming."
   ],
   "p1": 4163,
   "pn": 4167,
   "doi": "10.21437/Interspeech.2025-1403",
   "url": "interspeech_2025/nguyen25c_interspeech.html"
  },
  "du25c_interspeech": {
   "authors": [
    [
     "Zhou",
     "Du"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Huijun",
     "Ding"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Zhen",
     "Chen"
    ]
   ],
   "title": "Hybrid Expert Knowledge and Self-Supervised Learning for Diagnostic Modeling of Adductor Spasmodic and Primary Myotonic Dysphonia",
   "original": "1406",
   "order": 725,
   "page_count": 5,
   "abstract": [
    "Dysphonia encompasses a broad spectrum of vocal disorders with diverse etiologies, among which adductor spasmodic dysphonia (ADSD) and primary muscle tension dysphonia (pMTD) are particularly challenging to diagnose. Currently, the primary diagnostic method relies on subjective auditory perception by highly experienced clinicians. To alleviate the scarcity of diagnostic resources, this study develops a deep learning-based approach for automatically diagnosing ADSD and pMTD using patients’ speech data. Our contributions are: (1) designing a convolutional neural network (CNN)-based diagnostic model that leverages handcrafted features derived from expert knowledge and (2) incorporating self-supervised learning (SSL) to extract more discriminative representations as input from raw waveforms adaptively. This marks the first application of deep learning techniques to ADSD and pMTD diagnostic modeling, achieving a classification accuracy of 83.3% on our newly constructed dataset."
   ],
   "p1": 3543,
   "pn": 3547,
   "doi": "10.21437/Interspeech.2025-1406",
   "url": "interspeech_2025/du25c_interspeech.html"
  },
  "guo25c_interspeech": {
   "authors": [
    [
     "Yao",
     "Guo"
    ],
    [
     "Yang",
     "Ai"
    ],
    [
     "Rui-Chen",
     "Zheng"
    ],
    [
     "Hui-Peng",
     "Du"
    ],
    [
     "Xiao-Hang",
     "Jiang"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Vision-Integrated High-Quality Neural Speech Coding",
   "original": "1409",
   "order": 130,
   "page_count": 5,
   "abstract": [
    "This paper proposes a novel vision-integrated neural speech codec (VNSC), which aims to enhance speech coding quality by leveraging visual modality information. In VNSC, the image analysis-synthesis module extracts visual features from lip images, while the feature fusion module facilitates interaction between the image analysis-synthesis module and the speech coding module, transmitting visual information to assist the speech coding process. Depending on whether visual information is available during the inference stage, the feature fusion module integrates visual features into the speech coding module using either explicit integration or implicit distillation strategies. Experimental results confirm that integrating visual information effectively improves the quality of the decoded speech and enhances the noise robustness of the neural speech codec, without increasing the bitrate."
   ],
   "p1": 619,
   "pn": 623,
   "doi": "10.21437/Interspeech.2025-1409",
   "url": "interspeech_2025/guo25c_interspeech.html"
  },
  "phuong25_interspeech": {
   "authors": [
    [
     "Tuan Dat",
     "Phuong"
    ],
    [
     "Long-Vu",
     "Hoang"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models",
   "original": "1411",
   "order": 1148,
   "page_count": 5,
   "abstract": [
    "Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection."
   ],
   "p1": 5633,
   "pn": 5637,
   "doi": "10.21437/Interspeech.2025-1411",
   "url": "interspeech_2025/phuong25_interspeech.html"
  },
  "kong25b_interspeech": {
   "authors": [
    [
     "Xiangzhu",
     "Kong"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Zhijian",
     "Ou"
    ]
   ],
   "title": "Lightweight and Robust Multi-Channel End-to-End Speech Recognition with Spherical Harmonic Transform",
   "original": "1415",
   "order": 699,
   "page_count": 5,
   "abstract": [
    "This paper presents SHTNet, a lightweight spherical harmonic transform (SHT) based framework, which is designed to address cross-array generalization challenges in multi-channel automatic speech recognition (ASR) through three key innovations. First, SHT based spatial sound field decomposition converts microphone signals into geometry-invariant spherical harmonic coefficients, isolating signal processing from array geometry. Second, the Spatio-Spectral Attention Fusion Network (SSAFN) combines coordinate-aware spatial modeling, refined self-attention channel combinator, and spectral noise suppression without conventional beamforming. Third, Rand-SHT training enhances robustness through random channel selection and array geometry reconstruction. The system achieves 39.26% average CER across heterogeneous arrays (e.g., circular, square, and binaural) on datasets including Aishell-4, Alimeeting, and XMOS, with 97.1% fewer computations than conventional neural beamformers."
   ],
   "p1": 3439,
   "pn": 3443,
   "doi": "10.21437/Interspeech.2025-1415",
   "url": "interspeech_2025/kong25b_interspeech.html"
  },
  "wu25i_interspeech": {
   "authors": [
    [
     "Chia-Hua",
     "Wu"
    ],
    [
     "Wanying",
     "Ge"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "A Comparative Study on Proactive and Passive Detection  of Deepfake Speech",
   "original": "1419",
   "order": 1087,
   "page_count": 5,
   "abstract": [
    "Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github."
   ],
   "p1": 5328,
   "pn": 5332,
   "doi": "10.21437/Interspeech.2025-1419",
   "url": "interspeech_2025/wu25i_interspeech.html"
  },
  "dang25_interspeech": {
   "authors": [
    [
     "Shaoxiang",
     "Dang"
    ],
    [
     "Li",
     "Li"
    ],
    [
     "Shogo",
     "Seki"
    ],
    [
     "Hiroaki",
     "Kudo"
    ]
   ],
   "title": "First Analyze Then Enhance: A Task-Aware System for Speech Separation, Denoising, and Dereverberation",
   "original": "1424",
   "order": 786,
   "page_count": 5,
   "abstract": [
    "This paper presents the First Analyze Then Enhance (FATE) framework for speech enhancement. In FATE, observed signals are initially classified based on their specific degradation types and then enhanced using appropriate modules tailored to each type. This design prevents the overprocessing of clean signals and reduces process complexity by eliminating unnecessary procedures. This paper focuses on commonly encountered degradations, including additive noise, reverberation, speech mixing, and their combinations. To address these degradations, FATE includes a separation submodule and a denoising/dereverberation submodule the enhancement models. Degradation types are predicted using features extracted from pretrained models based on automatic speech recognition and self-supervised learning. Experiments show that FATE can accurately identify undegraded signals and achieve comparable enhancement performance for degraded signals in each scenario while optimizing processing complexity."
   ],
   "p1": 3848,
   "pn": 3852,
   "doi": "10.21437/Interspeech.2025-1424",
   "url": "interspeech_2025/dang25_interspeech.html"
  },
  "zhang25n_interspeech": {
   "authors": [
    [
     "Xiaocan",
     "Zhang"
    ],
    [
     "Weiwei",
     "Jiang"
    ],
    [
     "Guibin",
     "Zheng"
    ],
    [
     "Chenhao",
     "Jing"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Tieran",
     "Zheng"
    ]
   ],
   "title": "Knowledge Distillation Method for Pruned RNN-T Models via Pruning Bounds Sharing and Losses Confusion",
   "original": "1425",
   "order": 738,
   "page_count": 5,
   "abstract": [
    "Although the advantages of large models have been widely proved in speech recognition, small models are still required in several applications due to the limited computational resources or training data. The recognition accuracy of small models has always been a challenging issue. This paper proposes a distillation method for the pruned RNN-T structure to enhance the generalization ability of small models by leveraging information from large models, where the small model shares the pruning bounds of the large model as well as the decoder and connector structures, and multi-loss fusion is used to distill. Utilizing the Chinese speech dataset Aishell-1, experimental results demonstrated that the small model distilled from pre-trained large model significantly outperforms the directly trained model of the same size by a notable relative reduction of 30.4% in Character Error Rate (CER), thereby validating the effectiveness of the proposed knowledge distillation method."
   ],
   "p1": 3608,
   "pn": 3612,
   "doi": "10.21437/Interspeech.2025-1425",
   "url": "interspeech_2025/zhang25n_interspeech.html"
  },
  "mak25_interspeech": {
   "authors": [
    [
     "Timothy Shin Heng",
     "Mak"
    ],
    [
     "King Yiu",
     "Suen"
    ],
    [
     "Albert Y.S.",
     "Lam"
    ]
   ],
   "title": "Speech-guided Grapheme-to-Phoneme Conversion for Cantonese Text-to-Speech",
   "original": "1428",
   "order": 518,
   "page_count": 5,
   "abstract": [
    "Grapheme-to-Phoneme (G2P) conversion is a crucial component in neural Text-to-Speech. Cantonese G2P is especially difficult and suffers from two pain points. First, a large number of commonly used characters have multiple correct pronunciations (jyutpings) that cannot be distinguished based on textual context. Secondly, there is a lack of accurate jyutping-labelled data that can be used for the training of character-to-jyutping (C2J) models. In this study, we propose a speech-guided C2J method based on augmenting an off-the-shelf Automatic Speech Recognition model with a speech-guided C2J module. To overcome the data scarcity problem, we trained the model on speech generated from a commercial Text-to-Speech model. We show that this simple approach achieved a jyutping error rate (JER) of 2% in unseen, clean, speech, improving the best text-based C2J method by 2.6% absolutely. For common polyphonic characters, the improvement was even greater, reducing the JER from 10.6% to 2.0%."
   ],
   "p1": 2535,
   "pn": 2539,
   "doi": "10.21437/Interspeech.2025-1428",
   "url": "interspeech_2025/mak25_interspeech.html"
  },
  "attia25_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Attia"
    ],
    [
     "Dorottya",
     "Demszky"
    ],
    [
     "Jing",
     "Liu"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data",
   "original": "1432",
   "order": 752,
   "page_count": 5,
   "abstract": [
    "Recent progress in speech recognition has relied on models trained on vast amounts of labeled data. However, classroom Automatic Speech Recognition (ASR) faces the real-world challenge of abundant weak transcripts paired with only a small amount of accurate, gold-standard data. In such low-resource settings, high transcription costs make re-transcription impractical. To address this, we ask: what is the best approach when abundant inexpensive weak transcripts coexist with limited gold-standard data, as is the case for classroom speech data? We propose Weakly Supervised Pretraining (WSP), a two-step process where models are first pretrained on weak transcripts in a supervised manner, and then fine-tuned on accurate data. Our results, based on both synthetic and real weak transcripts, show that WSP outperforms alternative methods, establishing it as an effective training methodology for low-resource ASR in real-world scenarios."
   ],
   "p1": 3678,
   "pn": 3682,
   "doi": "10.21437/Interspeech.2025-1432",
   "url": "interspeech_2025/attia25_interspeech.html"
  },
  "shi25c_interspeech": {
   "authors": [
    [
     "Xiaohan",
     "Shi"
    ],
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Who, When, and What: Leveraging the ``Three Ws'' Concept for Emotion Recognition in Conversation",
   "original": "1433",
   "order": 360,
   "page_count": 5,
   "abstract": [
    "Emotion Recognition in Conversation (ERC) is essential for dialogue systems in human-computer interaction. Most existing studies primarily focus on modeling contextual information from historical interactions but often overlook the effective integration of speaker and content information. To address these challenges, we propose the &quot;Three Ws&quot; concept -Who, When, and What, representing speaker, context, and content information- to comprehensively capture emotional cues from historical interactions. Building on this concept, we further introduce a novel model for ERC. Additionally, we incorporate a speaker similarity loss to enhance speaker information. Experimental results show that our model outperforms baselines, with each component making significant contributions—especially context information. Additionally, the speaker similarity loss further improves ERC performance. Notably, the &quot;Three Ws&quot;concept demonstrates robustness across both single-modal and multimodal scenarios."
   ],
   "p1": 1763,
   "pn": 1767,
   "doi": "10.21437/Interspeech.2025-1433",
   "url": "interspeech_2025/shi25c_interspeech.html"
  },
  "biyani25_interspeech": {
   "authors": [
    [
     "Ishan D.",
     "Biyani"
    ],
    [
     "Nirmesh J.",
     "Shah"
    ],
    [
     "Ashishkumar P.",
     "Gudmalwar"
    ],
    [
     "Pankaj",
     "Wasnik"
    ],
    [
     "Rajiv R.",
     "Shah"
    ]
   ],
   "title": "REWIND: Speech Time Reversal for Enhancing Speaker Representations in Diffusion-based Voice Conversion",
   "original": "1434",
   "order": 280,
   "page_count": 5,
   "abstract": [
    "Speech time reversal refers to the process of reversing the entire speech signal in time, causing it to play backward. Such signals are completely unintelligible since the fundamental structures of phonemes and syllables are destroyed. However, they still retain tonal patterns that enable perceptual speaker identification despite losing linguistic content. In this paper, we propose leveraging speaker representations learned from time reversed speech as an augmentation strategy to enhance speaker representation. Notably, speaker and language disentanglement in voice conversion (VC) is essential to accurately preserve a speaker&#x27;s unique vocal traits while minimizing interference from linguistic content. The effectiveness of the proposed approach is evaluated in the context of state-of-the-art diffusion-based VC models. Experimental results indicate that the proposed approach significantly improves speaker similarity-related scores while maintaining high speech quality."
   ],
   "p1": 1368,
   "pn": 1372,
   "doi": "10.21437/Interspeech.2025-1434",
   "url": "interspeech_2025/biyani25_interspeech.html"
  },
  "hu25l_interspeech": {
   "authors": [
    [
     "Cheng Hung",
     "Hu"
    ],
    [
     "Yusuke",
     "Yasuda"
    ],
    [
     "Akifumi",
     "Yoshimoto"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Unifying Listener Scoring Scales: Comparison Learning Framework for Speech Quality Assessment and Continuous Speech Emotion Recognition",
   "original": "1435",
   "order": 1107,
   "page_count": 5,
   "abstract": [
    "Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition (CSER) are two key tasks in speech technology, both relying on listener ratings. However, these ratings are inherently biased due to individual listener factors. Previous approaches have introduced a mean listener scoring scale and modeled all listener scoring scales in the training set. However, the mean listener approach is prone to distortion from averaging ordinal data, leading to potential biases. Moreover, learning multiple listener scoring scales while inferring based only on the mean listener scale limits effectiveness. In contrast, our method focuses on modeling a unified listener scoring scale, using comparison scores to correctly capture the scoring relationships between utterances. Experimental results show that our method effectively improves prediction performance in both SQA and CSER tasks, proving its effectiveness and robustness."
   ],
   "p1": 5428,
   "pn": 5432,
   "doi": "10.21437/Interspeech.2025-1435",
   "url": "interspeech_2025/hu25l_interspeech.html"
  },
  "alabi25_interspeech": {
   "authors": [
    [
     "Jesujoba O.",
     "Alabi"
    ],
    [
     "Xuechen",
     "Liu"
    ],
    [
     "Dietrich",
     "Klakow"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "AfriHuBERT: A self-supervised speech representation model for African languages",
   "original": "1437",
   "order": 821,
   "page_count": 5,
   "abstract": [
    "In this work, we present AfriHuBERT, an extension of mHuBERT-147, a compact self-supervised learning (SSL) model pretrained on 147 languages. While mHuBERT-147 covered 16 African languages, we expand this to 1,226 through continued pretraining on 10K+ hours of speech data from diverse sources, benefiting an African population of over 600M. We evaluate AfriHuBERT on two key speech tasks, Spoken Language Identification (SLID) and Automatic Speech Recognition (ASR), using the FLEURS benchmark. Our results show a +3.6% F1 score improvement for SLID and a -2.1% average Word Error Rate (WER) reduction for ASR over mHuBERT-147, and demonstrates competitiveness with larger SSL models such as MMS and XEUS. Further analysis shows that ASR models trained on AfriHuBERT exhibit improved cross-corpus generalization and are competitive in extremely low-resource ASR scenarios."
   ],
   "p1": 4023,
   "pn": 4027,
   "doi": "10.21437/Interspeech.2025-1437",
   "url": "interspeech_2025/alabi25_interspeech.html"
  },
  "white25b_interspeech": {
   "authors": [
    [
     "Lauren",
     "White"
    ],
    [
     "Ewan",
     "Carr"
    ],
    [
     "Judith",
     "Dineley"
    ],
    [
     "Catarina",
     "Botelho"
    ],
    [
     "Pauline",
     "Conde"
    ],
    [
     "Faith",
     "Matcham"
    ],
    [
     "Carolin",
     "Oetzmann"
    ],
    [
     "Amos",
     "Folarin"
    ],
    [
     "George",
     "Fairs"
    ],
    [
     "Agnes",
     "Norbury"
    ],
    [
     "Stefano",
     "Goria"
    ],
    [
     "Srinivasan",
     "Vairavan"
    ],
    [
     "Til",
     "Wykes"
    ],
    [
     "Richard",
     "Dobson"
    ],
    [
     "Vaibhav",
     "Naraya"
    ],
    [
     "Matthew",
     "Hotopf"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Nicholas",
     "Cummins"
    ]
   ],
   "title": "Speech Reference Intervals: An Assessment of Feasibility in Depression Symptom Severity Prediction",
   "original": "1438",
   "order": 98,
   "page_count": 5,
   "abstract": [
    "Major Depressive Disorder (MDD) is a prevalent mental disorder. Combining speech features and machine learning has promise for predicting MDD, but interpretability is crucial for clinical applications. Reference intervals (RIs) represent a typical range for a speech feature in a population. RIs could increase interpretability and help clinicians identify deviations from norms. They could also replace conventional speech features in machine learning models. However, no work has yet assessed the feasibility of speech RIs in MDD. We generated and compared RIs from three reference datasets varying in size, elicitation prompt, and health information. We then calculated deviations from each RI set for people with MDD to compare performance on a depression symptom severity prediction task. Our RI-based models trained with demographic data performed similarly to each other and equivalent models using conventional features or demographics only, demonstrating the value of RI-derived features."
   ],
   "p1": 459,
   "pn": 463,
   "doi": "10.21437/Interspeech.2025-1438",
   "url": "interspeech_2025/white25b_interspeech.html"
  },
  "shi25d_interspeech": {
   "authors": [
    [
     "Xiaohan",
     "Shi"
    ],
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Speaker-Aware Multi-Task Learning for Speech Emotion Recognition",
   "original": "1439",
   "order": 883,
   "page_count": 5,
   "abstract": [
    "Speaker representations play a crucial role in achieving accurate speech emotion recognition (SER). Previous studies have primarily relied on generic speaker recognition (SR) models to extract speaker representations. However, these approaches struggle in speaker-dependent SER tasks, as they fail to capture speaker-specific characteristics effectively. To address this limitation, we propose a Speaker-Aware Multi-Task (SAMT) model, which is designed to effectively model speaker-specific and emotion-specific representations for SER. Additionally, we introduce a speaker-emotion disentanglement loss to explicitly separate speaker and emotion information, further enhancing speaker representation. Extensive experiments demonstrate the effectiveness of our approach, achieving performance gains of 1.76% in speaker-dependent and 1.99% in speaker-independent settings over the baseline. Moreover, the speaker-emotion disentanglement loss further improves SER performance."
   ],
   "p1": 4333,
   "pn": 4337,
   "doi": "10.21437/Interspeech.2025-1439",
   "url": "interspeech_2025/shi25d_interspeech.html"
  },
  "zheng25b_interspeech": {
   "authors": [
    [
     "Youqiang",
     "Zheng"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Yueteng",
     "Kang"
    ],
    [
     "Jie",
     "Chen"
    ],
    [
     "Yike",
     "Zhang"
    ],
    [
     "Li",
     "Xiao"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Long",
     "Ma"
    ]
   ],
   "title": "FreeCodec: A Disentangled Neural Speech Codec with Fewer Tokens",
   "original": "1440",
   "order": 992,
   "page_count": 5,
   "abstract": [
    "Neural speech codec is a crucial component in generative tasks such as speech resynthesis and zero-shot TTS. However, most works exhibit degraded performance with fewer tokens due to low coding efficiency in modeling complex coupled information. In this paper, we propose a self-supervised disentangled neural speech codec named FreeCodec. It employs distinct frame-level encoders to decompose intrinsic speech properties into separate components and adopts enhanced decoders to reconstruct speech signals. By encoding and quantizing the different frame-level information with dedicated quantizers, FreeCodec gets higher encoding efficiency with 57 tokens. Furthermore, our proposed method can be applied flexibly in reconstruction and disentanglement scenarios with different training strategies. Subjective and objective experimental results demonstrate that our framework outperforms existing methods in both reconstruction and disentanglement tasks."
   ],
   "p1": 4878,
   "pn": 4882,
   "doi": "10.21437/Interspeech.2025-1440",
   "url": "interspeech_2025/zheng25b_interspeech.html"
  },
  "glazer25_interspeech": {
   "authors": [
    [
     "Neta",
     "Glazer"
    ],
    [
     "David",
     "Chernin"
    ],
    [
     "Idan",
     "Achituve"
    ],
    [
     "Sharon",
     "Gannot"
    ],
    [
     "Ethan",
     "Fetaya"
    ]
   ],
   "title": "Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes",
   "original": "1442",
   "order": 459,
   "page_count": 5,
   "abstract": [
    "Recent advancements in Text-to-Speech (TTS) models, particularly in voice cloning, have intensified the demand for adaptable and efficient deepfake detection methods. As TTS systems continue to evolve, detection models must be able to efficiently adapt to previously unseen generation models with minimal data. This paper introduces AADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show how the combination of a powerful deep embedding model with the Gaussian processes flexibility can achieve strong performance and adaptability. Additionally, we show this approach can also be used for personalized detection, with greater robustness to new TTS models and one-shot adaptability. To support our evaluation, a benchmark dataset is constructed for this task using new state-of-the-art voice cloning models."
   ],
   "p1": 2240,
   "pn": 2244,
   "doi": "10.21437/Interspeech.2025-1442",
   "url": "interspeech_2025/glazer25_interspeech.html"
  },
  "rautenberg25_interspeech": {
   "authors": [
    [
     "Frederik",
     "Rautenberg"
    ],
    [
     "Fritz",
     "Seebauer"
    ],
    [
     "Jana",
     "Wiechmann"
    ],
    [
     "Michael",
     "Kuhlmann"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Synthesizing Speech with Selected Perceptual Voice Qualities – A Case Study with Creaky Voice",
   "original": "1443",
   "order": 333,
   "page_count": 5,
   "abstract": [
    "The control of perceptual voice qualities in a text-to-speech (TTS) system is of interest for applications where unmanipulated and manipulated speech probes can serve to illustrate phonetic concepts that are otherwise difficult to grasp. Here, we show that a TTS system, that is augmented with a global speaker attribute manipulation block based on normalizing flows, is capable of correctly manipulating the non-persistent, localized quality of creaky voice, thus avoiding the necessity of a, typically unreliable, frame-wise creak predictor. Subjective listening tests confirm successful creak manipulation at a slightly reduced MOS score compared to the original recording."
   ],
   "p1": 1633,
   "pn": 1637,
   "doi": "10.21437/Interspeech.2025-1443",
   "url": "interspeech_2025/rautenberg25_interspeech.html"
  },
  "shi25e_interspeech": {
   "authors": [
    [
     "Xiaohan",
     "Shi"
    ],
    [
     "Jinyi",
     "Mi"
    ],
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Advancing Emotion Recognition via Ensemble Learning: Integrating Speech, Context, and Text Representations",
   "original": "1445",
   "order": 955,
   "page_count": 5,
   "abstract": [
    "Speech Emotion Recognition (SER) in real-world scenarios aims to identify a speaker&#x27;s emotional states from spontaneous speech. While prior research has focused on noise reduction techniques within individual domains, integrating multi-domain noise-robust representations for SER remains underexplored. To address this challenge, we propose a novel Speech-Context-Text (SCT) model, which integrates speech, context, and text representations via ensemble learning. Specifically, we introduce the Mamba method for speech representation, employ a layer adapter to capture context representation, and adopt ASR correction to refine text representation. Extensive experiments demonstrate the effectiveness of SCT, achieving a 7.4% Macro-F1 improvement over the official baseline of the Speech Emotion Recognition in Naturalistic Conditions Challenge at INTERSPEECH 2025, securing 6th place in the competition. Additionally, SCT yields 7.37% and 7.95% gains on MSP-Podcast and IEMOCAP, respectively."
   ],
   "p1": 4693,
   "pn": 4697,
   "doi": "10.21437/Interspeech.2025-1445",
   "url": "interspeech_2025/shi25e_interspeech.html"
  },
  "yu25b_interspeech": {
   "authors": [
    [
     "Cheng",
     "Yu"
    ],
    [
     "Vahid",
     "Ahmadi Kalkhorani"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Online AV-CrossNet: a Causal and Efficient Audiovisual System for Speech Enhancement and Target Speaker Extraction",
   "original": "1448",
   "order": 608,
   "page_count": 5,
   "abstract": [
    "This paper presents online AV-CrossNet, a computationally efficient audiovisual speech enhancement/extraction system capable of causal and real-time processing. We aim to improve the state-of-the-art AV-CrossNet by enabling causal, frame-by-frame processing. To achieve this, we incorporate causal layers and compression techniques, reduce model size, and employ only one-frame look-ahead, thereby substantially enhancing real-world applicability. Additionally, we analyze compression ratio in both audio and visual modules, providing valuable insights into audiovisual model compression. Experimental results demonstrate an inference latency of 4.73 ms, capable of real-time processing. Moreover, the system maintains competitive performance while reducing size by a factor of 10. These findings highlight the efficiency and effectiveness of the proposed system, offering a promising solution for real-time audiovisual speech enhancement and speaker extraction in acoustically adverse environments."
   ],
   "p1": 2985,
   "pn": 2989,
   "doi": "10.21437/Interspeech.2025-1448",
   "url": "interspeech_2025/yu25b_interspeech.html"
  },
  "jiang25b_interspeech": {
   "authors": [
    [
     "Nan",
     "Jiang"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Qing",
     "Gu"
    ],
    [
     "Haoyu",
     "Song"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "An Effective Anomalous Sound Detection Method Based on Global and Local Attribute Mining",
   "original": "1449",
   "order": 535,
   "page_count": 5,
   "abstract": [
    "Recent anomalous sound detection (ASD) methods have achieved impressive performance by leveraging attribute information. However, domain shift and imbalance issues caused by inconsistent attributes between source and target domains remain a key challenge. In this paper, we propose a novel global and local attribute mining method for ASD (GLAM-ASD) with a Transformer based encoder consisting of teacher and student branches to exploit semantic-rich information. The student mines contextual information via masked patch reconstruction while the teacher obtains weights from the student via exponential moving average. Global and local attributes are then derived in an unsupervised way. The student branch is further optimized by global self-distillation and pseudo-label prediction via learned local attributes. Evaluations on the DCASE2023 Task2 benchmark demonstrate the superiority of GLAM-ASD, especially in its target domain generalization performance."
   ],
   "p1": 2620,
   "pn": 2624,
   "doi": "10.21437/Interspeech.2025-1449",
   "url": "interspeech_2025/jiang25b_interspeech.html"
  },
  "terashima25_interspeech": {
   "authors": [
    [
     "Ryo",
     "Terashima"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Masaya",
     "Kawamura"
    ]
   ],
   "title": "SLASH: Self-Supervised Speech Pitch Estimation Leveraging DSP-derived Absolute Pitch",
   "original": "1453",
   "order": 353,
   "page_count": 5,
   "abstract": [
    "We present SLASH, a pitch estimation method of speech signals based on self-supervised learning (SSL). To enhance the performance of conventional SSL-based approaches that primarily depend on the relative pitch difference derived from pitch shifting, our method incorporates absolute pitch values by 1) introducing a prior pitch distribution derived from digital signal processing (DSP), and 2) optimizing absolute pitch through gradient descent with a loss between the target and differentiable DSP-derived spectrograms. To stabilize the optimization, a novel spectrogram generation method is used that skips complicated waveform generation. In addition, the aperiodic components in speech are accurately predicted through differentiable DSP, enhancing the method&#x27;s applicability to speech signal processing. Experimental results showed that the proposed method outperformed both baseline DSP and SSL-based pitch estimation methods, attributed to the effective integration of SSL and DSP."
   ],
   "p1": 1733,
   "pn": 1737,
   "doi": "10.21437/Interspeech.2025-1453",
   "url": "interspeech_2025/terashima25_interspeech.html"
  },
  "wang25l_interspeech": {
   "authors": [
    [
     "Man",
     "Wang"
    ],
    [
     "Yixin",
     "Ding"
    ],
    [
     "Niels",
     "Schiller"
    ]
   ],
   "title": "Semantic Processing During Spoken Word Production by Children with Cochlear Implants",
   "original": "1455",
   "order": 175,
   "page_count": 5,
   "abstract": [
    "Research that investigates the speech production of children with cochlear implants (CIs) mostly focuses on the characteristics of their speech sounds. Few studies have looked at the psycholinguistic processes during speech production of children with CI. Our study examines the semantic processing during speech production of this group of speakers, compared to their normal hearing (NH) peers. Using the picture-word interference paradigm, we manipulated the semantic relatedness between target picture names and distractor words. We observed the typical semantic interference effect in the NH group but not in the CI group, suggesting that the semantic network may be organized differently in the CI group than in their NH peers and the CI group may have difficulties accessing the semantic categories. Furthermore, our results are in line with the suggestion that the CI group may rely more on a top-down strategy or attentional cognitive processing than a bottom-up semantic activation."
   ],
   "p1": 843,
   "pn": 847,
   "doi": "10.21437/Interspeech.2025-1455",
   "url": "interspeech_2025/wang25l_interspeech.html"
  },
  "yao25b_interspeech": {
   "authors": [
    [
     "Shanshan",
     "Yao"
    ],
    [
     "Dianlong",
     "Liu"
    ],
    [
     "Tian",
     "Li"
    ]
   ],
   "title": "SCD-Conformer: Semantic Content Disentanglement for Text-Independent Speaker Verification",
   "original": "1456",
   "order": 742,
   "page_count": 5,
   "abstract": [
    "Text-independent speaker verification (TISV) identifies a specific speaker without relying on any particular semantic content. In order to eliminate the influence of semantic content in utterances on speaker feature, we propose a SCD-Conformer for semantic content disentanglement. Firstly, a dual-branch Conformer is used to extract the speaker feature and semantic content feature respectively, in which the content feature is directly extracted by a pre-trained model without increasing training parameters and computational complexity. Then, both the frame-level and utterance-level disentanglement methods are used for disentangling the speaker feature and content feature, including dimension matching module, aggregation module and similarity module. Experimental results show that disentangling at utterance level is more effective than that at frame level, whereas the combination of the two is the best, which averagely improved the performance by 11% compared to the best baseline."
   ],
   "p1": 3628,
   "pn": 3632,
   "doi": "10.21437/Interspeech.2025-1456",
   "url": "interspeech_2025/yao25b_interspeech.html"
  },
  "yang25k_interspeech": {
   "authors": [
    [
     "Yujie",
     "Yang"
    ],
    [
     "Bing",
     "Yang"
    ],
    [
     "Xiaofei",
     "Li"
    ]
   ],
   "title": "Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech Enhancement",
   "original": "1462",
   "order": 241,
   "page_count": 5,
   "abstract": [
    "Online multichannel speech enhancement has been intensively studied recently. Though Mel-scale frequency is more matched with human auditory perception and computationally efficient than linear frequency, few works are implemented in a Mel-frequency domain. To this end, this work proposes a Mel-scale framework (namely Mel-McNet). It processes spectral and spatial information with two key components: an effective STFT-to-Mel module compressing multi-channel STFT features into Mel-frequency representations, and a modified McNet backbone directly operating in the Mel domain to generate enhanced LogMel spectra. The spectra can be directly fed to vocoders for waveform reconstruction or ASR systems for transcription. Experiments on CHiME-3 show that Mel-McNet can reduce computational complexity by 60% while maintaining comparable enhancement and ASR performance to the original McNet. Mel-McNet also outperforms other SOTA methods, verifying the potential of Mel-scale speech enhancement."
   ],
   "p1": 1173,
   "pn": 1177,
   "doi": "10.21437/Interspeech.2025-1462",
   "url": "interspeech_2025/yang25k_interspeech.html"
  },
  "li25q_interspeech": {
   "authors": [
    [
     "Zhengyang",
     "Li"
    ],
    [
     "Pascal",
     "Reichert"
    ],
    [
     "Thomas",
     "Graave"
    ],
    [
     "Patrick",
     "Blumenberg"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "Efficient Noise-Robust Hybrid Audiovisual Encoder  with Joint Distillation and Pruning for Audiovisual Speech Recognition",
   "original": "1464",
   "order": 374,
   "page_count": 5,
   "abstract": [
    "Powered by self-supervised learning (SSL) on vast amounts of unlabeled data, a computationally intensive audiovisual encoder -a hybrid architecture combining ResNet and transformer in series- achieves state-of-the-art performance in audiovisual speech recognition (AV-ASR). In this work, we are the first to apply joint distillation and pruning (DP) with a teacher-student model for an efficient and noise-robust audiovisual encoder. First, we compress the transformer of the AV encoder. Second, we extend joint DP to both the ResNet and transformer of the hybrid AV encoder. In addition, we provide analyses on the teacher and the final student, respectively. With a similar number of parameters, our proposed student outperforms the previous state-of-the-art in clean condition (word error rate of 3.1% vs. 4.6%) and across all noisy conditions, while at the same time reducing computational complexity by 31.8%. Our code is at GitHub."
   ],
   "p1": 1833,
   "pn": 1837,
   "doi": "10.21437/Interspeech.2025-1464",
   "url": "interspeech_2025/li25q_interspeech.html"
  },
  "agrawal25_interspeech": {
   "authors": [
    [
     "Neeraj",
     "Agrawal"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Spoken Language Understanding on Unseen Tasks With In-Context Learning",
   "original": "1467",
   "order": 837,
   "page_count": 5,
   "abstract": [
    "Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out-of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs."
   ],
   "p1": 4103,
   "pn": 4107,
   "doi": "10.21437/Interspeech.2025-1467",
   "url": "interspeech_2025/agrawal25_interspeech.html"
  },
  "zhang25o_interspeech": {
   "authors": [
    [
     "Zhe",
     "Zhang"
    ],
    [
     "Wen-Chin",
     "Huang"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Xiaoxiao",
     "Miao"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Mitigating Language Mismatch in SSL-Based Speaker Anonymization",
   "original": "1469",
   "order": 1047,
   "page_count": 5,
   "abstract": [
    "Speaker anonymization aims to protect speaker identity while preserving content information and the intelligibility of speech. However, most speaker anonymization systems (SASs) are developed and evaluated using only English, resulting in degraded utility for other languages. This paper investigates language mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune a self-supervised learning (SSL)-based content encoder with Japanese speech to verify effective language adaptation. Then, we propose fine-tuning a multilingual SSL model with Japanese speech and evaluating the SAS in Japanese and Mandarin. Downstream experiments show that fine-tuning an English-only SSL model with the target language enhances intelligibility while maintaining privacy and that multilingual SSL further extends SASs&#x27; utility across different languages. These findings highlight the importance of language adaptation and multilingual pre-training of SSLs for robust multilingual speaker anonymization."
   ],
   "p1": 5133,
   "pn": 5137,
   "doi": "10.21437/Interspeech.2025-1469",
   "url": "interspeech_2025/zhang25o_interspeech.html"
  },
  "gogate25_interspeech": {
   "authors": [
    [
     "Mandar",
     "Gogate"
    ],
    [
     "Kia",
     "Dashtipour"
    ],
    [
     "Amir",
     "Hussain"
    ]
   ],
   "title": "Towards Personalised Audio Visual Speech Enhancement",
   "original": "1471",
   "order": 987,
   "page_count": 5,
   "abstract": [
    "Personalised speech enhancement (PSE) and audio-visual (AV) speech enhancement (SE) have emerged as promising approaches to improve speech quality and intelligibility in challenging acoustic environments. PSE leverages individual-specific vocal characteristics to address the label permutation problem, while AV SE incorporates visual cues, particularly lip movements, to complement auditory signals in noisy conditions where speech is degraded by competing noise sources. This paper presents a novel framework that unifies these two, advancing towards personalised AV SE. By integrating raw enrolment audio for adaptive target speaker representation with AV inputs the proposed system aims to achieve robust SE in real-world environments. Experimental results demonstrate significant improvements in speech intelligibility and noise suppression on the COG-MHEAR Audio-Visual Speech Enhancement Challenge dataset, outperforming state-of-the-art PSE and AV SE models."
   ],
   "p1": 4853,
   "pn": 4857,
   "doi": "10.21437/Interspeech.2025-1471",
   "url": "interspeech_2025/gogate25_interspeech.html"
  },
  "yosha25_interspeech": {
   "authors": [
    [
     "Iddo",
     "Yosha"
    ],
    [
     "Dorin",
     "Shteyman"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "WhiStress: Enriching Transcriptions with Sentence Stress Detection",
   "original": "1475",
   "order": 960,
   "page_count": 5,
   "abstract": [
    "Spoken language conveys meaning not only through words but also through intonation, emotion, and emphasis. Sentence stress, the emphasis placed on specific words within a sentence, is crucial for conveying speaker intent and has been extensively studied in linguistics. In this work, we introduce WHISTRESS, an alignment-free approach for enhancing transcription systems with sentence stress detection. We propose TINYSTRESS-15K, a scalable, synthetic training data for the task of sentence stress detection which resulted from a fully automated dataset creation process. We train WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive baselines. Our results show that WHISTRESS outperforms existing methods while requiring no additional input priors during training or inference. Notably, despite being trained on synthetic data, WHISTRESS demonstrates strong zero-shot generalization across diverse benchmarks."
   ],
   "p1": 4718,
   "pn": 4722,
   "doi": "10.21437/Interspeech.2025-1475",
   "url": "interspeech_2025/yosha25_interspeech.html"
  },
  "kim25q_interspeech": {
   "authors": [
    [
     "Se-Ha",
     "Kim"
    ],
    [
     "Tae-Gyeong",
     "Kim"
    ],
    [
     "Chang-Jae",
     "Chun"
    ]
   ],
   "title": "Mamba-based Hybrid Model for Speech Enhancement",
   "original": "1476",
   "order": 1053,
   "page_count": 5,
   "abstract": [
    "In this study, we propose MH-SENet, which is designed for speech enhancement by extracting the temporal and spectral features of speech signals in parallel. MH-SENet, which is based on the U-Net architecture, has an encoder and decoder consisting of a bi-directional Mamba and processes it more precisely by considering all the context of the input sequence. Furthermore, a cross-domain Mamba-Transformer block is constructed between the encoder and decoder to effectively fuse information between each time and frequency domains. We evaluated the performance of our proposed MH-SENet on the VCTK + DEMAND dataset and thus it outperformed existing methods by achieving the highest PESQ score. Despite being a hybrid model, the proposed MH-SENet has a lower number of parameters compared to the conventional models."
   ],
   "p1": 5163,
   "pn": 5167,
   "doi": "10.21437/Interspeech.2025-1476",
   "url": "interspeech_2025/kim25q_interspeech.html"
  },
  "jeong25_interspeech": {
   "authors": [
    [
     "Seung Gyu",
     "Jeong"
    ],
    [
     "Seong Eun",
     "Kim"
    ]
   ],
   "title": "Patient-Aware Feature Alignment for Robust Lung Sound Classification: Cohesion-Separation and Global Alignment Losses",
   "original": "1477",
   "order": 210,
   "page_count": 5,
   "abstract": [
    "Lung sound classification is vital for early diagnosis of respiratory diseases. However, biomedical signals often exhibit inter-patient variability even among patients with the same symptoms, requiring a learning approach that considers individual differences. We propose a Patient-Aware Feature Alignment (PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL) and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same patient while separating those from other patients to capture patient variability, whereas GPAL draws each patient&#x27;s centroid toward a global center, preventing feature space fragmentation. Our method achieves outstanding results on the ICBHI dataset with a score of 64.84% for four-class and 72.08% for two-class classification. These findings highlight PAFA&#x27;s ability to capture individualized patterns and demonstrate performance gains in distinct patient clusters, offering broader applications for patient-centered healthcare."
   ],
   "p1": 1018,
   "pn": 1022,
   "doi": "10.21437/Interspeech.2025-1477",
   "url": "interspeech_2025/jeong25_interspeech.html"
  },
  "liang25d_interspeech": {
   "authors": [
    [
     "Yifan",
     "Liang"
    ],
    [
     "Kang",
     "Yang"
    ],
    [
     "Fangkun",
     "Liu"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Xiaodong",
     "Li"
    ],
    [
     "Chengshi",
     "Zheng"
    ]
   ],
   "title": "LightL2S: Ultra-Low Complexity Lip-to-Speech Synthesis for Multi-Speaker Scenarios",
   "original": "1478",
   "order": 773,
   "page_count": 5,
   "abstract": [
    "Lip-to-speech synthesis in the wild remains challenging due to the limited visual information. While self-supervised models have shown promising results in relatively high-quality lip-to-speech synthesis, their computational demands make them impractical for edge devices. To address this issue, we introduce LightL2S, a novel multi-speaker lip-to-speech system designed to achieve ultra-low complexity for edge deployment. To reduce the computational cost, we adopt a much more efficient architecture MoViNet for visual encoder instead of using the conventional ResNet-18. Furthermore, we introduce Zipformer blocks to efficiently learn prosodic information and quantized self-supervised audio representations from the output features generated by MoViNet. Finally, we employ the differentiable digital signal processing vocoder to synthesize speech. Experimental results demonstrate that LightL2S can generate reasonable speech even with a computational complexity of only 0.8 GMacs."
   ],
   "p1": 3783,
   "pn": 3787,
   "doi": "10.21437/Interspeech.2025-1478",
   "url": "interspeech_2025/liang25d_interspeech.html"
  },
  "marchini25_interspeech": {
   "authors": [
    [
     "Gilly",
     "Marchini"
    ],
    [
     "Jeremy",
     "Steffman"
    ]
   ],
   "title": "Data-driven approaches to pitch modelling in two Mexican Spanish ethnolects: K-means Clustering &amp; GAMMs",
   "original": "1479",
   "order": 599,
   "page_count": 5,
   "abstract": [
    "This study models F0 data for documentation purposes. Using K-means clustering and Generative Additive Mixed Models, it documents differences in F0 contours between two Mexican Spanish ethnolects: Afro-Mexican Spanish and Altiplateau Mexican Spanish. Both dialects distinguish focus condition through contour shape and pitch range: rising contours and high frequency ranges are associated with in-focus conditions, and lower frequency plateaus with post-focus. AMS however shows delayed peak alignment in open syllables and overall higher pitch contours for those with post-vocalic /s/. We posit that these fine-grained differences in segmental alignment and pitch range would not be captured through traditional methods, e.g., Sp_ToBI. With this study&#x27;s focus on phonetic detail, the ToBI model risks both the absence of phonetic variation and the assumption of underlying phonological structure. We therefore argue that this variation is best captured through clustering and GAMMs."
   ],
   "p1": 2940,
   "pn": 2944,
   "doi": "10.21437/Interspeech.2025-1479",
   "url": "interspeech_2025/marchini25_interspeech.html"
  },
  "ho25_interspeech": {
   "authors": [
    [
     "Luong",
     "Ho"
    ],
    [
     "Khanh",
     "Le"
    ],
    [
     "Vinh",
     "Pham"
    ],
    [
     "Bao",
     "Nguyen"
    ],
    [
     "Tan",
     "Tran"
    ],
    [
     "Duc",
     "Chau"
    ]
   ],
   "title": "Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization",
   "original": "1480",
   "order": 903,
   "page_count": 5,
   "abstract": [
    "Inverse Text Normalization (ITN) is crucial for converting spoken Automatic Speech Recognition (ASR) outputs into well-formatted written text, enhancing both readability and usability. Despite its importance, the integration of streaming ITN within streaming ASR remains largely unexplored due to challenges in accuracy, efficiency, and adaptability, particularly in low-resource and limited-context scenarios. In this paper, we introduce a streaming pretrained language model for ITN, leveraging pretrained linguistic representations for improved robustness. To address streaming constraints, we propose Dynamic Context-Aware during training and inference, enabling adaptive chunk size adjustments and the integration of right-context information. Experimental results demonstrate that our method achieves accuracy comparable to non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese dataset, all while maintaining low latency, ensuring seamless integration into ASR systems."
   ],
   "p1": 4433,
   "pn": 4437,
   "doi": "10.21437/Interspeech.2025-1480",
   "url": "interspeech_2025/ho25_interspeech.html"
  },
  "gao25e_interspeech": {
   "authors": [
    [
     "Yifan",
     "Gao"
    ],
    [
     "Jiao",
     "Fu"
    ],
    [
     "Long",
     "Guo"
    ],
    [
     "Hong",
     "Liu"
    ]
   ],
   "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection",
   "original": "1483",
   "order": 87,
   "page_count": 5,
   "abstract": [
    "Early identification of suicide risk is crucial for preventing suicidal behaviors. As a result, the identification and study of patterns and markers related to suicide risk have become a key focus of current research. In this paper, we present the results of our work in the 1st SpeechWellness Challenge (SW1), which aims to explore speech as a non-invasive and easily accessible mental health indicator for identifying adolescents at risk of suicide. Our approach leverages large language model (LLM) as the primary tool for feature extraction, alongside conventional acoustic and semantic features. The proposed method achieves an accuracy of 74% on the test set, ranking first in the SW1 challenge. These findings demonstrate the potential of LLM-based methods for analyzing speech in the context of suicide risk assessment."
   ],
   "p1": 404,
   "pn": 408,
   "doi": "10.21437/Interspeech.2025-1483",
   "url": "interspeech_2025/gao25e_interspeech.html"
  },
  "takahashi25_interspeech": {
   "authors": [
    [
     "Kaito",
     "Takahashi"
    ],
    [
     "Keigo",
     "Hojo"
    ],
    [
     "Toshimitsu",
     "Sakai"
    ],
    [
     "Yukoh",
     "Wakabayashi"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "Fine-tuning Parakeet-TDT for Dysarthric Speech Recognition in the Speech Accessibility Project Challenge",
   "original": "1484",
   "order": 672,
   "page_count": 5,
   "abstract": [
    "We present our dysarthric speech recognition system submitted to the Interspeech 2025 Speech Accessibility Project Challenge. This challenge is a competition aimed at improving the recognition accuracy of dysarthric speech. In this challenge, we submitted a speech recognition system with high accuracy for dysarthric speech and achieved first place. In dysarthric speech recognition, models based on self-supervised learning are commonly used. However, we hypothesized that fine-tuning pre-trained model with inherently high recognition accuracy would achieve the best performance. To this end, we developed a model that combines various techniques, including data preprocessing to expand training datasets, data augmentation to enhance generalization during fine-tuning, and decoding acceleration to optimize inference speed. As a result, our speech recognizer greatly improved accuracy, reducing WER to 8.11 from the baseline Whisper large v2&#x27;s 17.82 provided by the organizers."
   ],
   "p1": 3304,
   "pn": 3308,
   "doi": "10.21437/Interspeech.2025-1484",
   "url": "interspeech_2025/takahashi25_interspeech.html"
  },
  "wu25j_interspeech": {
   "authors": [
    [
     "Rongshuai",
     "Wu"
    ],
    [
     "Debasish Ray",
     "Mohapatra"
    ],
    [
     "Sidney",
     "Fels"
    ]
   ],
   "title": "2D Immersed Boundary Method in Vocal Tract Acoustics: An Eulerian–Lagrangian Model for Simulation of Diphthongs",
   "original": "1486",
   "order": 201,
   "page_count": 5,
   "abstract": [
    "Modeling dynamic vocal tracts requires accurate interpolation between target vowels for diphthong synthesis. High-fidelity Finite Element models are computationally expensive, often needing complete remeshing of the computational domain to maintain interpolation stability. In contrast, Digital Waveguide models (DWM) suffer from discretization errors due to staircased approximations of tract geometries. We model dynamic tracts using the 2D Immersed Boundary Method, which represents tract contours in the Lagrangian domain and uses a 2D Finite-Difference Time-Domain scheme to solve wave equations in the Eulerian domain. This framework enables free boundary motion within a fixed Cartesian grid, avoiding remeshing and staircasing. We synthesized three diphthongs and compared their spectral features with a 3D DWM model and recorded speech. Results show that the F1-F2 trajectories closely align with those from the 3D DWM and recorded speech, achieving correlation coefficients exceeding 0.8."
   ],
   "p1": 973,
   "pn": 977,
   "doi": "10.21437/Interspeech.2025-1486",
   "url": "interspeech_2025/wu25j_interspeech.html"
  },
  "mohsin25_interspeech": {
   "authors": [
    [
     "Aarish Shah",
     "Mohsin"
    ],
    [
     "Mohammad",
     "Nadeem"
    ],
    [
     "Shahab Saquib",
     "Sohail"
    ],
    [
     "Tughrul",
     "Arsalan"
    ],
    [
     "Mandar",
     "Gogate"
    ],
    [
     "Nasir",
     "Saleem"
    ],
    [
     "Amir",
     "Hussain"
    ]
   ],
   "title": "Investigating Gender Bias in Text-to-Audio Generation Models",
   "original": "1488",
   "order": 685,
   "page_count": 5,
   "abstract": [
    "Text-to-Audio (TTA) generation models have demonstrated significant advancements in generating quality audio content from textual prompts. However, these models may inherit and propagate gender biases present in their training data potentially resulting audio outputs that reinforce harmful stereotypes. To address this concern, we systematically analyzed the presence of gender bias in TTA models by employing a comprehensive taxonomy of gender-associated terms. We utilized three state-of-the-art TTA generation models (AudioGen, AudioLDM and Stable Audio) for generating audio samples and applied a gender identification tool to classify their perceived gender. Furthermore, we proposed a novel metric to quantitatively measure the extent of gender bias in audio outputs. Our findings reveal that TTA models frequently exhibit gender bias, often reflecting existing societal stereotypes. The study highlights the need for robust bias evaluation frameworks in text-to-audio generation systems."
   ],
   "p1": 3369,
   "pn": 3373,
   "doi": "10.21437/Interspeech.2025-1488",
   "url": "interspeech_2025/mohsin25_interspeech.html"
  },
  "negroni25_interspeech": {
   "authors": [
    [
     "Viola",
     "Negroni"
    ],
    [
     "Davide",
     "Salvi"
    ],
    [
     "Paolo",
     "Bestagini"
    ],
    [
     "Stefano",
     "Tubaro"
    ]
   ],
   "title": " Source Verification for Speech Deepfakes ",
   "original": "1490",
   "order": 316,
   "page_count": 5,
   "abstract": [
    "With the proliferation of speech deepfake generators, it becomes crucial not only to assess the authenticity of synthetic audio but also to trace its origin. While source attribution models attempt to address this challenge, they often struggle in open-set conditions against unseen generators. In this paper, we introduce the source verification task, which, inspired by speaker verification, determines whether a test track was produced using the same model as a set of reference signals. Our approach leverages embeddings from a classifier trained for source attribution, computing distance scores between tracks to assess whether they originate from the same source. We evaluate multiple models across diverse scenarios, analyzing the impact of speaker diversity, language mismatch, and post-processing operations. This work provides the first exploration of source verification, highlighting its potential and vulnerabilities, and offers insights for real-world forensic applications."
   ],
   "p1": 1548,
   "pn": 1552,
   "doi": "10.21437/Interspeech.2025-1490",
   "url": "interspeech_2025/negroni25_interspeech.html"
  },
  "choi25f_interspeech": {
   "authors": [
    [
     "Changin",
     "Choi"
    ],
    [
     "Sungjun",
     "Lim"
    ],
    [
     "Wonjong",
     "Rhee"
    ]
   ],
   "title": "Enhancing Retrieval-Augmented Audio Captioning with Generation-Assisted Multimodal Querying and Progressive Learning",
   "original": "1493",
   "order": 542,
   "page_count": 5,
   "abstract": [
    "Retrieval-augmented generation can improve audio captioning by incorporating relevant audio-text pairs from a knowledge base. Existing methods typically rely solely on the input audio as a unimodal retrieval query. In contrast, we propose Generation-Assisted Multimodal Querying, which generates a text description of the input audio to enable multimodal querying. This approach aligns the query modality with the audio-text structure of the knowledge base, leading to more effective retrieval. Furthermore, we introduce a novel progressive learning strategy that gradually increases the number of interleaved audio-text pairs to enhance the training process. Our experiments on AudioCaps, Clotho, and Auto-ACD demonstrate that our approach achieves state-of-the-art results across these benchmarks."
   ],
   "p1": 2655,
   "pn": 2659,
   "doi": "10.21437/Interspeech.2025-1493",
   "url": "interspeech_2025/choi25f_interspeech.html"
  },
  "que25_interspeech": {
   "authors": [
    [
     "Shumin",
     "Que"
    ],
    [
     "Anton",
     "Ragni"
    ]
   ],
   "title": "VisualSpeech: Enhancing Prosody Modeling in TTS Using Video",
   "original": "1494",
   "order": 772,
   "page_count": 5,
   "abstract": [
    "Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech."
   ],
   "p1": 3778,
   "pn": 3782,
   "doi": "10.21437/Interspeech.2025-1494",
   "url": "interspeech_2025/que25_interspeech.html"
  },
  "villani25_interspeech": {
   "authors": [
    [
     "Filippo",
     "Villani"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Jan",
     "Østergaard"
    ],
    [
     "Jesper",
     "Jensen"
    ]
   ],
   "title": "Analysis and Extension of a Near-End Listening Enhancement Method Based on Long-Term Fractile Noise Statistics",
   "original": "1496",
   "order": 163,
   "page_count": 5,
   "abstract": [
    "This paper addresses the problem of near-end listening enhancement (NELE), where a clean speech signal is modified prior to playback and under an energy constraint to improve intelligibility in noise. We analyze a recently proposed NELE method, optimized using a Speech Intelligibility Index that has been modified to incorporate temporal aspects of the noise via long-term fractile noise statistics. Specifically, we explain the energy allocation strategy adopted by the algorithm, and show that, in contrast to many existing methods, the spectral energy distribution of the modified speech is a function of that of the background noise, but not that of the input speech. Our simulation experiments show that this simple method outperforms well-established spectral shaping NELE methods. In addition, we extend the algorithm by appending an off-the-shelf dynamic range compressor, and show that it performs generally better than state-of-the-art methods for NELE."
   ],
   "p1": 783,
   "pn": 787,
   "doi": "10.21437/Interspeech.2025-1496",
   "url": "interspeech_2025/villani25_interspeech.html"
  },
  "elkheir25b_interspeech": {
   "authors": [
    [
     "Yassine",
     "El Kheir"
    ],
    [
     "Omnia",
     "Ibrahim"
    ],
    [
     "Amit",
     "Meghanani"
    ],
    [
     "Nada",
     "Almarwani"
    ],
    [
     "Hawau",
     "Toyin"
    ],
    [
     "Sadeen",
     "Alharbi"
    ],
    [
     "Modar",
     "Alfadly"
    ],
    [
     "Lamya",
     "Alkanhal"
    ],
    [
     "Ibrahim",
     "Selim"
    ],
    [
     "Shehab",
     "Elbatal"
    ],
    [
     "Salima",
     "Mdhaffar"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Yasser",
     "Hifny"
    ],
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Ahmed",
     "Ali"
    ]
   ],
   "title": "Towards a Unified Benchmark for Arabic Pronunciation Assessment: Qur’anic Recitation as Case Study",
   "original": "1497",
   "order": 493,
   "page_count": 5,
   "abstract": [
    "We present a unified benchmark for mispronunciation detection in Modern Standard Arabic (MSA) using Qur&#x27;anic recitation as a case study. Our approach lays the groundwork for advancing Arabic pronunciation assessment by providing a comprehensive pipeline that spans data processing, the development of a specialized phoneme set tailored to the nuances of MSA pronunciation, and the creation of the first publicly available test set for this task, which we term as the Qur&#x27;anic Mispronunciation Benchmark (QuranMB.v1). Furthermore, we evaluate several baseline models to provide initial performance insights, thereby highlighting both the promise and the challenges inherent in assessing MSA pronunciation. By establishing this standardized framework, we aim to foster further research and development in pronunciation assessment in Arabic language technology and related applications."
   ],
   "p1": 2410,
   "pn": 2414,
   "doi": "10.21437/Interspeech.2025-1497",
   "url": "interspeech_2025/elkheir25b_interspeech.html"
  },
  "qin25_interspeech": {
   "authors": [
    [
     "Chengyuan",
     "Qin"
    ],
    [
     "Wenmeng",
     "Xiong"
    ],
    [
     "Jing",
     "Zhou"
    ],
    [
     "Maoshen",
     "Jia"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Speech Enhancement with Dual-path Multi-Channel Linear Prediction Filter and Multi-norm Beamforming",
   "original": "1502",
   "order": 246,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a speech enhancement method using dual-path Multi-Channel Linear Prediction (MCLP) filters and multi-norm beamforming. Specifically, the MCLP part in the proposed method is designed with dual-path filters in both time and frequency dimensions. For the beamforming part, we minimize the power of the microphone array output as well as the l1 norm of the denoised signals while preserving source signals from the target directions. An efficient method to select the prediction orders in the dual-path filters is also proposed, which is robust for signals with different reverberation time (T60) values and can be applied to other MCLP-based methods. Evaluations demonstrate that our proposed method outperforms the baseline methods for speech enhancement, particularly in high reverberation scenarios."
   ],
   "p1": 1198,
   "pn": 1202,
   "doi": "10.21437/Interspeech.2025-1502",
   "url": "interspeech_2025/qin25_interspeech.html"
  },
  "ma25_interspeech": {
   "authors": [
    [
     "Te",
     "Ma"
    ],
    [
     "Min",
     "Bi"
    ],
    [
     "Saierdaer",
     "Yusuyin"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Zhijian",
     "Ou"
    ]
   ],
   "title": "LLM-based phoneme-to-grapheme for phoneme-based speech recognition",
   "original": "1503",
   "order": 118,
   "page_count": 5,
   "abstract": [
    "In automatic speech recognition (ASR), phoneme-based multilingual pre-training and crosslingual fine-tuning is attractive for its high data efficiency and competitive results compared to subword-based models. However, Weighted Finite State Transducer (WFST) based decoding is limited by its complex pipeline and inability to leverage large language models (LLMs). Therefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for phoneme-based ASR, consisting of speech-to-phoneme (S2P) and phoneme-to-grapheme (P2G). A challenge is that there seems to have information loss in cascading S2P and P2G. To address this challenge, we propose two training strategies: data augmentation with noisy phonemes (DANP), and randomized top-K marginalized (TKM) training and decoding. Our experimental results show that LLM-P2G outperforms WFST-based systems in crosslingual ASR for Polish and German, by relative WER reductions of 3.6% and 6.9% respectively."
   ],
   "p1": 559,
   "pn": 563,
   "doi": "10.21437/Interspeech.2025-1503",
   "url": "interspeech_2025/ma25_interspeech.html"
  },
  "mojarad25_interspeech": {
   "authors": [
    [
     "Hamid",
     "Mojarad"
    ],
    [
     "Kevin",
     "Tang"
    ]
   ],
   "title": "Automatic Speech Recognition of African American English: Lexical and Contextual Effects",
   "original": "1511",
   "order": 793,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) models often struggle with the phonetic, phonological, and morphosyntactic features found in African American English (AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction (CCR) and ING-reduction. It examines whether the presence of CCR and ING-reduction increases ASR misrecognition. Subsequently, it investigates whether end-to-end ASR systems without an external Language Model (LM) are more influenced by lexical neighborhood effect and less by contextual predictability compared to systems with an LM. The Corpus of Regional African American Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR and ING-reduction were detected using the Montreal Forced Aligner (MFA) with pronunciation expansion. The analysis reveals a small but significant effect of CCR and ING on Word Error Rate (WER) and indicates a stronger presence of lexical neighborhood effect in ASR systems without LMs."
   ],
   "p1": 3883,
   "pn": 3887,
   "doi": "10.21437/Interspeech.2025-1511",
   "url": "interspeech_2025/mojarad25_interspeech.html"
  },
  "gu25c_interspeech": {
   "authors": [
    [
     "Qing",
     "Gu"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Haoyu",
     "Song"
    ],
    [
     "Nan",
     "Jiang"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "A Domain Robust Pre-Training Method with Local Prototypes for Speaker Verification",
   "original": "1512",
   "order": 755,
   "page_count": 5,
   "abstract": [
    "Existing self-supervised methods for speaker verification (SV) have demonstrated strong potential by training on large-scale unlabeled speech data to learn effective speaker embeddings. However, most rely on utterance-level contrastive learning or self-distillation, which fails to adequately account for domain shifts caused by different styles and languages. In this paper, we propose a novel domain-robust pre-training method with local prototypes for SV. Specifically, we employ a transformer-based encoder to introduce a self-distillation framework for local feature learning. Domain-agnostic pre-training is used to derive local prototypes through online clustering. Furthermore, domain-aware alignment is applied to learn domain-robust local features. Fine-tuning with utterance-level supervision demonstrates the effectiveness of our proposed method on the CNCeleb and VoxCeleb benchmarks."
   ],
   "p1": 3693,
   "pn": 3697,
   "doi": "10.21437/Interspeech.2025-1512",
   "url": "interspeech_2025/gu25c_interspeech.html"
  },
  "luo25c_interspeech": {
   "authors": [
    [
     "Longjie",
     "Luo"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition",
   "original": "1513",
   "order": 692,
   "page_count": 5,
   "abstract": [
    "Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art."
   ],
   "p1": 3404,
   "pn": 3408,
   "doi": "10.21437/Interspeech.2025-1513",
   "url": "interspeech_2025/luo25c_interspeech.html"
  },
  "chochlakis25_interspeech": {
   "authors": [
    [
     "Georgios",
     "Chochlakis"
    ],
    [
     "Turab",
     "Iqbal"
    ],
    [
     "Woo Hyun",
     "Kang"
    ],
    [
     "Zhaocheng",
     "Huang"
    ]
   ],
   "title": "Modality-Agnostic Multimodal Emotion Recognition using a Contrastive Masked Autoencoder",
   "original": "1514",
   "order": 612,
   "page_count": 5,
   "abstract": [
    "Multimodal deep learning methods have greatly accelerated research in emotion recognition and have become the state of the art. However, in many scenarios, not all modalities are readily available, leading to either failure of traditional algorithms or the need for multiple models. In this work, we advance the state of the art in emotion recognition by proposing a unified, modality-agnostic transformer-based model that is inherently robust to missing modalities. To better exploit the multimodality of the data, we propose to use contrastive learning for modality alignment and masked autoencoding for multimodal reconstruction. Experimental results on the MSP-Podcast corpus show that our unified model achieves state-of-the-art performance, and improves both unimodal and multimodal baselines by 1-5% relative in respective evaluation metrics with the capability to handle missing modalities for two emotion recognition tasks in a more compact model."
   ],
   "p1": 3005,
   "pn": 3009,
   "doi": "10.21437/Interspeech.2025-1514",
   "url": "interspeech_2025/chochlakis25_interspeech.html"
  },
  "kienegger25_interspeech": {
   "authors": [
    [
     "Jakob",
     "Kienegger"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios",
   "original": "1515",
   "order": 609,
   "page_count": 5,
   "abstract": [
    "Recent speaker extraction methods using deep non-linear spatial filtering perform exceptionally well when the target direction is known and stationary. However, spatially dynamic scenarios are considerably more challenging due to time-varying spatial features and arising ambiguities, e.g. when moving speakers cross. While in a static scenario it may be easy for a user to point to the target&#x27;s direction, manually tracking a moving speaker is impractical. Instead of relying on accurate time-dependent directional cues, which we refer to as strong guidance, in this paper we propose a weakly guided extraction method solely depending on the target’s initial position to cope with spatial dynamic scenarios. By incorporating our own deep tracking algorithm and developing a joint training strategy on a synthetic dataset, we demonstrate the proficiency of our approach in resolving spatial ambiguities and even outperform a mismatched, but strongly guided extraction method."
   ],
   "p1": 2990,
   "pn": 2994,
   "doi": "10.21437/Interspeech.2025-1515",
   "url": "interspeech_2025/kienegger25_interspeech.html"
  },
  "he25b_interspeech": {
   "authors": [
    [
     "Yuxuan",
     "He"
    ],
    [
     "Xiaoran",
     "Yang"
    ],
    [
     "Ningning",
     "Pan"
    ],
    [
     "Gongping",
     "Huang"
    ]
   ],
   "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation",
   "original": "1516",
   "order": 862,
   "page_count": 5,
   "abstract": [
    "Most existing text-to-audio (TTA) generation methods produce mono outputs, neglecting essential spatial information for immersive auditory experiences. To address this issue, we propose a cascaded method for text-to-multisource binaural audio generation (TTMBA) with both temporal and spatial control. First, a pretrained large language model (LLM) segments the text into a structured format with time and spatial details for each sound event. Next, a pretrained mono audio generation network creates multiple mono audios with varying durations for each event. These mono audios are transformed into binaural audios using a binaural rendering neural network based on spatial data from the LLM. Finally, the binaural audios are arranged by their start times, resulting in multisource binaural audio. Experimental results demonstrate the superiority of the proposed method in terms of both audio generation quality and spatial perceptual accuracy."
   ],
   "p1": 4228,
   "pn": 4232,
   "doi": "10.21437/Interspeech.2025-1516",
   "url": "interspeech_2025/he25b_interspeech.html"
  },
  "bandekar25_interspeech": {
   "authors": [
    [
     "Jesuraj",
     "Bandekar"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Enhancing Acoustic-to-Articulatory Inversion with Multi-Target Pretraining for Low-Resource Settings",
   "original": "1519",
   "order": 1139,
   "page_count": 5,
   "abstract": [
    "Acoustic-to-Articulatory Inversion (AAI) estimates vocal tract articulator movements from speech, benefiting tasks like ASR, speech synthesis, and speaker verification. While deep learning-based methods (CNNs, RNNs, Transformers) have advanced AAI, recent studies show that Self-Supervised Learning (SSL) features further enhance performance, particularly in low-resource settings. However, SSL feature extractors introduce inference latency and computational overhead. To address this, we propose a novel pretraining method leveraging three target representations - Phoneme Labels, Articulatory Feature Labels, and Critical-articulator Labels - eliminating the need for an SSL extractor during inference. We evaluate our approach against both baseline and SSL-based models across various data conditions. Results demonstrate that our method consistently improves AAI performance, particularly in low-resource scenarios, while significantly reducing inference costs without sacrificing accuracy."
   ],
   "p1": 5588,
   "pn": 5592,
   "doi": "10.21437/Interspeech.2025-1519",
   "url": "interspeech_2025/bandekar25_interspeech.html"
  },
  "wang25m_interspeech": {
   "authors": [
    [
     "Ziqian",
     "Wang"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "U-SAM: An Audio Language Model for Unified Speech, Audio, and Music Understanding",
   "original": "1524",
   "order": 555,
   "page_count": 5,
   "abstract": [
    "The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pretrained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that USAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available."
   ],
   "p1": 2720,
   "pn": 2724,
   "doi": "10.21437/Interspeech.2025-1524",
   "url": "interspeech_2025/wang25m_interspeech.html"
  },
  "deheerkloots25_interspeech": {
   "authors": [
    [
     "Marianne",
     "de Heer Kloots"
    ],
    [
     "Hosein",
     "Mohebbi"
    ],
    [
     "Charlotte",
     "Pouw"
    ],
    [
     "Gaofei",
     "Shen"
    ],
    [
     "Willem",
     "Zuidema"
    ],
    [
     "Martijn",
     "Bentum"
    ]
   ],
   "title": "What do self-supervised speech models know about Dutch?  Analyzing advantages of language-specific pre-training",
   "original": "1526",
   "order": 53,
   "page_count": 5,
   "abstract": [
    "How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it&#x27;s less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pretraining exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition."
   ],
   "p1": 256,
   "pn": 260,
   "doi": "10.21437/Interspeech.2025-1526",
   "url": "interspeech_2025/deheerkloots25_interspeech.html",
   "erratum": "<p>\nCorrection to Section 2 (Models): This archival publication contains misreported numbers on the CommonVoice data included in pre-training the Wav2Vec2-NL model. The correct number of hours sampled from CommonVoice is <b>83</b>, making the total number of hours in Wav2Vec2-NL pre-training <b>831</b>. Across the full training set, audio samples ranged between <b>2</b> and <b>20</b> seconds in length. \n</p>\n"
  },
  "ozer25_interspeech": {
   "authors": [
    [
     "Yigitcan",
     "Özer"
    ],
    [
     "Woosung",
     "Choi"
    ],
    [
     "Joan",
     "Serrà"
    ],
    [
     "Mayank Kumar",
     "Singh"
    ],
    [
     "Wei-Hsiang",
     "Liao"
    ],
    [
     "Yuki",
     "Mitsufuji"
    ]
   ],
   "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?",
   "original": "1530",
   "order": 1043,
   "page_count": 5,
   "abstract": [
    "We present the Robust Audio Watermarking Benchmark (RAW-Bench) to foster the evaluation of deep learning-based audio watermarking algorithms, establishing a standardized benchmark and allowing systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline featuring various distortions such as compression, background noise, and reverberation and propose a diverse test dataset, including speech, environmental sounds, and music recordings. By assessing the performance of four existing watermarking algorithms on our framework, two main insights stand out: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain algorithms. Our contributions strengthen the robustness and perceptual assessment of audio watermarking algorithms across a wide range of applications while ensuring a fair and consistent evaluation approach."
   ],
   "p1": 5113,
   "pn": 5117,
   "doi": "10.21437/Interspeech.2025-1530",
   "url": "interspeech_2025/ozer25_interspeech.html"
  },
  "zhou25e_interspeech": {
   "authors": [
    [
     "Wangjin",
     "Zhou"
    ],
    [
     "Tianjiao",
     "Du"
    ],
    [
     "Chenglin",
     "Xu"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Yi",
     "Zhao"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Simple and Effective Content Encoder for Singing Voice Conversion via SSL-Embedding Dimension Reduction",
   "original": "1531",
   "order": 260,
   "page_count": 5,
   "abstract": [
    "In any-to-any singing voice conversion (SVC), singing content can be encoded using either token-based or embedding-based approaches. Token-based methods often struggle with accurate content reconstruction, while embedding-based methods face significant timbre leakage. To address this trade-off, we propose a novel self-supervised learning (SSL)-based content representation method. By randomly selecting a subset of channels during training to serve as the new embedding and fixing them for subsequent SVC training, our approach achieves superior content modeling compared to token-based methods while mitigating timbre leakage typically observed in embedding-based approaches. We validate the effectiveness and generalizability of our method across SSL-based embeddings, SSL-based soft embeddings, and ContentVec."
   ],
   "p1": 1268,
   "pn": 1272,
   "doi": "10.21437/Interspeech.2025-1531",
   "url": "interspeech_2025/zhou25e_interspeech.html"
  },
  "huttner25_interspeech": {
   "authors": [
    [
     "Lena-Marie",
     "Huttner"
    ],
    [
     "Jeppe H.",
     "Christensen"
    ],
    [
     "Gitte",
     "Keidser"
    ],
    [
     "Tobias",
     "May"
    ],
    [
     "Torsten",
     "Dau"
    ],
    [
     "Sergi",
     "Rotger-Griful"
    ]
   ],
   "title": "Does effortful speech production indicate communication difficulty caused by noise and hearing aid support?",
   "original": "1535",
   "order": 224,
   "page_count": 5,
   "abstract": [
    "Hearing impairment affects a person’s ability to communicate with others. Communication difficulty remains a little understood concept in research. In this study we assess how parameters of speech affected by noise, hearing aid support, and hearing status reflect a participant’s experience of communication difficulty. We paired 44 participants into dyads consisting of one normal-hearing (NH) and one hearing-impaired (HI) participant. Participants engaged in task-based conversation in quiet as well as in 70dB background noise. HI participants completed the task both with and without hearing aid. After each conversation participants indicated their communication difficulty on a questionnaire. We then analyze how F1, vocal level, and turn taking variability predict communication difficulty. For HI participants, increased vocal level as well as higher variability in turn taking are associated with greater difficulty. For NH participants, increased F1 is associated with greater difficulty."
   ],
   "p1": 1088,
   "pn": 1092,
   "doi": "10.21437/Interspeech.2025-1535",
   "url": "interspeech_2025/huttner25_interspeech.html"
  },
  "m25_interspeech": {
   "authors": [
    [
     "Anuprabha",
     "M"
    ],
    [
     "Krishna",
     "Gurugubelli"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS",
   "original": "1536",
   "order": 561,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech poses significant challenges in developing assistive technologies, primarily due to the limited availability of data. Recent advances in neural speech synthesis, especially zero-shot voice cloning, facilitate synthetic speech generation for data augmentation; however, they may introduce biases towards dysarthric speech. In this paper, we investigate the effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using TORGO dataset, focusing on intelligibility, speaker similarity, and prosody preservation. We also analyze potential biases using fairness metrics like Disparate Impact and Parity Difference to assess disparities across dysarthric severity levels. Results show that F5-TTS exhibits a strong bias toward speech intelligibility over speaker and prosody preservation in dysarthric speech synthesis. Insights from this study can help integrate fairness-aware dysarthric speech synthesis, fostering the advancement of more inclusive speech technologies."
   ],
   "p1": 2750,
   "pn": 2754,
   "doi": "10.21437/Interspeech.2025-1536",
   "url": "interspeech_2025/m25_interspeech.html"
  },
  "shah25_interspeech": {
   "authors": [
    [
     "Neil",
     "Shah"
    ],
    [
     "Shirish",
     "Karande"
    ],
    [
     "Vineet",
     "Gandhi"
    ]
   ],
   "title": "NAM-to-Speech Conversion with Multitask-Enhanced Autoregressive Models",
   "original": "1537",
   "order": 1143,
   "page_count": 5,
   "abstract": [
    "We propose an alignment-free, end-to-end Non-Audible Murmur (NAM)-to-Speech conversion model. Existing methods rely on large NAM-text pairs per speaker to generate high-quality alignments for training non-autoregressive models. However, alignment quality deteriorates when trained on multi-speaker data, limiting their ability to generalize and effectively utilize the available training data. To address this, we introduce a streamlined autoregressive approach that eliminates the need for explicit alignment learning. By leveraging multi-speaker samples, synthetic training pairs, and multitask character recognition training, our method reduces the word error rate (WER) by 59.19% compared to the state-of-the-art (SOTA) on two public datasets. We demonstrate the model’s zero-shot capability and validate the effectiveness of multitask training through ablation studies."
   ],
   "p1": 5608,
   "pn": 5612,
   "doi": "10.21437/Interspeech.2025-1537",
   "url": "interspeech_2025/shah25_interspeech.html"
  },
  "li25r_interspeech": {
   "authors": [
    [
     "Fengjin",
     "Li"
    ],
    [
     "Jie",
     "Wang"
    ],
    [
     "Yadong",
     "Niu"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Meng",
     "Meng"
    ],
    [
     "Jian",
     "Luan"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion",
   "original": "1538",
   "order": 935,
   "page_count": 5,
   "abstract": [
    "Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS)."
   ],
   "p1": 4593,
   "pn": 4597,
   "doi": "10.21437/Interspeech.2025-1538",
   "url": "interspeech_2025/li25r_interspeech.html"
  },
  "xia25_interspeech": {
   "authors": [
    [
     "Yinfeng",
     "Xia"
    ],
    [
     "Huiyan",
     "Li"
    ],
    [
     "Chenyang",
     "Le"
    ],
    [
     "Manhong",
     "Wang"
    ],
    [
     "Yutao",
     "Sun"
    ],
    [
     "Xingyang",
     "Ma"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition",
   "original": "1541",
   "order": 898,
   "page_count": 5,
   "abstract": [
    "Applying large pre-trained speech models like Whisper has shown promise in reducing training costs for various speech tasks. However, integrating these models into streaming systems remains a challenge. This paper presents a novel prefix-to-prefix training framework for streaming recognition by fine-tuning the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to establish a quasi-monotonic alignment between continuous speech sequences and discrete text tokens. Additionally, we design Monotonic Finite Look-ahead Attention, allowing each token to attend to infinite left-context and finite right-context from the speech sequences. We also employ the wait-k decoding strategy to simplify the decoding process while ensuring consistency between training and testing. Our theoretical analysis and experiments demonstrate that this approach achieves a controllable trade-off between latency and quality, making it suitable for various streaming applications."
   ],
   "p1": 4408,
   "pn": 4412,
   "doi": "10.21437/Interspeech.2025-1541",
   "url": "interspeech_2025/xia25_interspeech.html"
  },
  "xiao25d_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Xiao"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Layer-Wise Decision Fusion for Fake Audio Detection Using XLS-R",
   "original": "1543",
   "order": 1145,
   "page_count": 5,
   "abstract": [
    "Recent fake audio detection methods often leverage large speech models to achieve robust speech representations.  These models are typically very deep, providing multiple layer-wise representations.  However, current works often rely solely on single layer representation or feature fusion to extract one utterance-level representation for decision making.  These methods risk underutilizing rich information from multiple layers and might induce feature collapse.  We propose a novel layer-wise decision fusion method that applies fusion after per-layer decision making and achieves the best cross-dataset performance on In-the-Wild dataset (EER 6.90%) compared to other strong baselines. Our model design also makes the model more transparent, allowing us to conduct detailed analysis to reveal the underlying mechanism of decision making."
   ],
   "p1": 5618,
   "pn": 5622,
   "doi": "10.21437/Interspeech.2025-1543",
   "url": "interspeech_2025/xiao25d_interspeech.html"
  },
  "shao25b_interspeech": {
   "authors": [
    [
     "Mingchen",
     "Shao"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Chengyou",
     "Wang"
    ],
    [
     "Bingshen",
     "Mu"
    ],
    [
     "Hai",
     "Li"
    ],
    [
     "Ying",
     "Yan"
    ],
    [
     "Junhui",
     "Liu"
    ],
    [
     "Danming",
     "Xie"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR",
   "original": "1548",
   "order": 156,
   "page_count": 5,
   "abstract": [
    "Despite remarkable achievements, automatic speech recognition (ASR) in low-resource scenarios still faces two challenges: high-quality data scarcity and high computational demands. This paper proposes EThai-ASR, the first to apply large language models (LLMs) to Thai ASR and create an efficient LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module and a Thai LLM decoder. To address the data scarcity and obtain a powerful speech encoder, EThai-ASR introduces a self-evolving data refinement strategy to refine weak labels, yielding an enhanced speech encoder. Moreover, we propose a pluggable sequence compression module used in the connection module with three modes designed to reduce the sequence length, thus decreasing computational demands while maintaining decent performance. Extensive experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy in multiple datasets. We release our refined text transcripts to promote further research."
   ],
   "p1": 748,
   "pn": 752,
   "doi": "10.21437/Interspeech.2025-1548",
   "url": "interspeech_2025/shao25b_interspeech.html"
  },
  "lu25d_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Peng",
     "Shen"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR",
   "original": "1549",
   "order": 679,
   "page_count": 5,
   "abstract": [
    "Transferring linguistic knowledge from a pretrained language model (PLM) to acoustic feature learning has proven effective in enhancing end-to-end automatic speech recognition (E2E-ASR). However, aligning representations between linguistic and acoustic modalities remains a challenge due to inherent modality gaps. Optimal transport (OT) has shown promise in mitigating these gaps by minimizing the Wasserstein distance (WD) between linguistic and acoustic feature distributions. However, previous OT-based methods overlook structural relationships, treating feature vectors as unordered sets. To address this, we propose GraphMatching Optimal Transport (GM-OT), which models linguistic and acoustic sequences as structured graphs. Nodes represent feature embeddings, while edges capture temporal and sequential relationships. GM-OT minimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD) (between edges), leading to a fused Gromov Wasserstein distance (FGWD) formulation. This enables structured alignment and more efficient knowledge transfer compared to existing OT-based approaches. Theoretical analysis further shows that prior OT-based method in linguistic knowledge transfer can be viewed as a special case within our GMOT framework. We evaluate GM-OT on Mandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge transfer. Experimental results demonstrate significant performance gains over state-of-the-art models, validating the effectiveness of our approach."
   ],
   "p1": 3339,
   "pn": 3343,
   "doi": "10.21437/Interspeech.2025-1549",
   "url": "interspeech_2025/lu25d_interspeech.html"
  },
  "toyin25_interspeech": {
   "authors": [
    [
     "Hawau",
     "Toyin"
    ],
    [
     "Rufael",
     "Marew"
    ],
    [
     "Humaid",
     "Alblooshi"
    ],
    [
     "Samar M.",
     "Magdy"
    ],
    [
     "Hanan",
     "Aldarmaki"
    ]
   ],
   "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis",
   "original": "1550",
   "order": 978,
   "page_count": 5,
   "abstract": [
    "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech corpus with diacritized transcriptions, intended for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a new professionally recorded set from six voice talents with diverse demographics, (2) a modified subset of the Arabic Speech Corpus; and (3) high-quality synthetic speech from two commercial systems. The complete corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. We train three open-source TTS and two voice conversion systems to illustrate the use cases of the dataset. The corpus is available for research use."
   ],
   "p1": 4808,
   "pn": 4812,
   "doi": "10.21437/Interspeech.2025-1550",
   "url": "interspeech_2025/toyin25_interspeech.html"
  },
  "jing25b_interspeech": {
   "authors": [
    [
     "Kangqi",
     "Jing"
    ],
    [
     "Wenbin",
     "Zhang"
    ],
    [
     "Yu",
     "Gao"
    ]
   ],
   "title": "End-to-End DOA-Guided Speech Extraction in Noisy Multi-Talker Scenarios",
   "original": "1552",
   "order": 295,
   "page_count": 5,
   "abstract": [
    "Target Speaker Extraction (TSE) plays a critical role in enhancing speech signals in noisy and multi-speaker environments. This paper presents an end-to-end TSE model that incorporates Direction of Arrival (DOA) and beamwidth embeddings to extract speech from a specified spatial region centered around the DOA. Our approach efficiently captures spatial and temporal features, enabling robust performance in highly complex scenarios with multiple simultaneous speakers. Experimental results demonstrate that the proposed model not only significantly enhances the target speech within the defined beamwidth but also effectively suppresses interference from other directions, producing a clear and isolated target voice. Furthermore, the model achieves remarkable improvements in downstream Automatic Speech Recognition (ASR) tasks, making it particularly suitable for real-world applications."
   ],
   "p1": 1443,
   "pn": 1447,
   "doi": "10.21437/Interspeech.2025-1552",
   "url": "interspeech_2025/jing25b_interspeech.html"
  },
  "laquatra25_interspeech": {
   "authors": [
    [
     "Moreno",
     "La Quatra"
    ],
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Valerio Mario",
     "Salerno"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ]
   ],
   "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition",
   "original": "1553",
   "order": 668,
   "page_count": 5,
   "abstract": [
    "Despite the remarkable progress in end-to-end Automatic Speech Recognition (ASR) engines, accurately transcribing dysarthric speech remains a major challenge. In this work, we proposed a two-stage framework for the Speech Accessibility Project Challenge at INTERSPEECH 2025, which combines cutting-edge speech recognition models with LLM-based generative error correction (GER). We assess different configurations of model scales and training strategies, incorporating specific hypothesis selection to improve transcription accuracy. Experiments on the Speech Accessibility Project dataset demonstrate the strength of our approach on structured and spontaneous speech, while highlighting challenges in single-word recognition. Through comprehensive analysis, we provide insights into the complementary roles of acoustic and linguistic modeling in dysarthric speech recognition."
   ],
   "p1": 3284,
   "pn": 3288,
   "doi": "10.21437/Interspeech.2025-1553",
   "url": "interspeech_2025/laquatra25_interspeech.html"
  },
  "dai25b_interspeech": {
   "authors": [
    [
     "Wang",
     "Dai"
    ],
    [
     "Archontis",
     "Politis"
    ],
    [
     "Tuomas",
     "Virtanen"
    ]
   ],
   "title": "Inter-Speaker Relative Cues for Text-Guided Target Speech Extraction",
   "original": "1554",
   "order": 391,
   "page_count": 5,
   "abstract": [
    "We propose a novel approach that utilize inter-speaker relative cues for distinguishing target speakers and extracting their voices from mixtures. Continuous cues (e.g., temporal order, age, pitch level) are grouped by relative differences, while discrete cues (e.g., language, gender, emotion) retain their categories. Relative cues offers greater flexibility than fixed speech attribute classification, facilitating much easier expansion of text-guided target speech extraction datasets. Our experiments show that combining all relative cues yields better performance than random subsets, with gender and temporal order being the most robust across languages and reverberant conditions. Additional cues like pitch level, loudness, distance, speaking duration, language, and pitch range also demonstrate notable benefit in complex scenarios. Fine-tuning pre-trained WavLM Base+ CNN encoders improves overall performance over the baseline of using only a Conv1d encoder."
   ],
   "p1": 1918,
   "pn": 1922,
   "doi": "10.21437/Interspeech.2025-1554",
   "url": "interspeech_2025/dai25b_interspeech.html"
  },
  "das25_interspeech": {
   "authors": [
    [
     "Arnab",
     "Das"
    ],
    [
     "Yassine",
     "El Kheir"
    ],
    [
     "Carlos",
     "Franzreb"
    ],
    [
     "Tim",
     "Herzig"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Generalizable Audio Spoofing Detection using Non-Semantic Representations",
   "original": "1555",
   "order": 927,
   "page_count": 5,
   "abstract": [
    "Rapid advancements in generative modeling have made synthetic audio generation easy, making speech-based services vulnerable to spoofing attacks. Consequently, there is a dire need for robust countermeasures more than ever. Existing solutions for deepfake detection are often criticized for lacking generalizability and fail drastically when applied to real-world data. This study proposes a novel method for generalizable spoofing detection leveraging non-semantic universal audio representations. Extensive experiments have been performed to find suitable non-semantic features using TRILL and TRILLsson models. The results indicate that the proposed method achieves comparable performance on the in-domain test set while significantly outperforming state-of-the-art approaches on out-of-domain test sets. Notably, it demonstrates superior generalization on public-domain data, surpassing methods based on hand-crafted features, semantic embeddings, and end-to-end architectures."
   ],
   "p1": 4553,
   "pn": 4557,
   "doi": "10.21437/Interspeech.2025-1555",
   "url": "interspeech_2025/das25_interspeech.html"
  },
  "klejch25_interspeech": {
   "authors": [
    [
     "Ondřej",
     "Klejch"
    ],
    [
     "William",
     "Lamb"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "A Practitioner’s Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic",
   "original": "1557",
   "order": 152,
   "page_count": 5,
   "abstract": [
    "An effective approach to the development of ASR systems for low-resource languages is to fine-tune an existing multilingual end-to-end model. When the original model has been trained on large quantities of data from many languages, fine-tuning can be effective with limited training data, even when the language in question was not present in the original training data. The fine-tuning approach has been encouraged by the availability of public-domain E2E models and is widely believed to lead to state-of-the-art results. This paper, however, challenges that belief. We show that an approach combining hybrid HMMs with self-supervised models can yield substantially better performance with limited training data. This combination allows better utilisation of all available speech and text data through continued self-supervised pre-training and semi-supervised training. We benchmark our approach on Scottish Gaelic, achieving WER reductions of 32% relative over our best fine-tuned Whisper model."
   ],
   "p1": 728,
   "pn": 732,
   "doi": "10.21437/Interspeech.2025-1557",
   "url": "interspeech_2025/klejch25_interspeech.html"
  },
  "nguyen25d_interspeech": {
   "authors": [
    [
     "Thai-Binh",
     "Nguyen"
    ],
    [
     "Thi Van",
     "Nguyen"
    ],
    [
     "Quoc Truong",
     "Do"
    ],
    [
     "Chi Mai",
     "Luong"
    ]
   ],
   "title": "ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition",
   "original": "1559",
   "order": 35,
   "page_count": 5,
   "abstract": [
    "Audio-Visual Speech Recognition (AVSR) has gained significant attention recently due to its robustness against noise, which often challenges conventional speech recognition systems that rely solely on audio features. Despite this advantage, AVSR models remain limited by the scarcity of extensive datasets, especially for most languages beyond English. Automated data collection offers a promising solution. This work presents a practical approach to generate AVSR datasets from raw video, refining existing techniques for improved efficiency and accessibility. We demonstrate its broad applicability by developing a baseline AVSR model for Vietnamese. Experiments show the automatically collected dataset enables a strong baseline, achieving competitive performance with robust ASR in clean conditions and significantly outperforming them in noisy environments like cocktail parties. This efficient method provides a pathway to expand AVSR to more languages, particularly under-resourced ones."
   ],
   "p1": 166,
   "pn": 170,
   "doi": "10.21437/Interspeech.2025-1559",
   "url": "interspeech_2025/nguyen25d_interspeech.html"
  },
  "sankala25_interspeech": {
   "authors": [
    [
     "Sreekanth",
     "Sankala"
    ],
    [
     "Venkatesh",
     "Parvathala"
    ],
    [
     "Ramesh",
     "Gundluru"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "Adversarial Attacks on Text-dependent Speaker Verification System",
   "original": "1560",
   "order": 928,
   "page_count": 5,
   "abstract": [
    "Adversarial attacks against text-independent speaker verification (TI-SV) systems assume access to genuine speaker&#x27;s enrollment speech (e[n]). This assumption is self-defeating because if an attacker has e[n], they can bypass the system directly, making adversarial examples unnecessary. In contrast, we observe that the text-dependent SV (TD-SV) system, where the genuine speaker must say a password, offers a more practically relevant attack scenario. In reality, the attacker may not have access to the password spoken by a genuine speaker, but they can likely obtain normal speech from the genuine speaker. Therefore, generating adversarial noise that, when added to the genuine speaker&#x27;s normal speech, can bypass the password requirement of a TD-SV system constitutes a potential realistic attack. This work investigates the feasibility of such a practical attack and shows that even the state-of-the-art TD-SV system is vulnerable with an attack success rate of 64.28 %."
   ],
   "p1": 4558,
   "pn": 4562,
   "doi": "10.21437/Interspeech.2025-1560",
   "url": "interspeech_2025/sankala25_interspeech.html"
  },
  "wang25n_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Tieran",
     "Zheng"
    ],
    [
     "Guibin",
     "Zheng"
    ],
    [
     "Yongjun",
     "He"
    ]
   ],
   "title": "Dual Orthogonality Sub-center Loss for Enhanced Anomalous Sound Detection",
   "original": "1563",
   "order": 686,
   "page_count": 5,
   "abstract": [
    "Anomalous Sound Detection (ASD) requires modeling a compact and discriminative normal sound distribution. Recently, angular margin loss with multiple sub-centers has been shown to be effective for ASD by extending sub-centers to capture intra-class diversity and maximizing their orthogonality to enhance model discriminability. However, existing methods do not consider that the orthogonality of intra-class sub-centers needs to be optimized based on the inherent data structure to avoid over-extension of the representation space due to over-orthogonality. To address this issue, we propose a Dual Orthogonality Sub-Center Loss (DOSCL) that enforces strict orthogonality of inter-class sub-centers to improve anomaly discrimination while applying relaxed constraints on intra-class sub-centers to capture the data structure. Experiments on the DCASE2023 Challenge Task2 dataset show that DOSCL achieves 1.74% AUC and 0.62% pAUC improvements over a strong baseline, validating its effectiveness."
   ],
   "p1": 3374,
   "pn": 3378,
   "doi": "10.21437/Interspeech.2025-1563",
   "url": "interspeech_2025/wang25n_interspeech.html"
  },
  "liu25k_interspeech": {
   "authors": [
    [
     "Yin-Long",
     "Liu"
    ],
    [
     "Yuanchao",
     "Li"
    ],
    [
     "Rui",
     "Feng"
    ],
    [
     "Liu",
     "He"
    ],
    [
     "Jia-Xin",
     "Chen"
    ],
    [
     "Yi-Ming",
     "Wang"
    ],
    [
     "Yu-Ang",
     "Chen"
    ],
    [
     "Yan-Han",
     "Peng"
    ],
    [
     "Jia-Hong",
     "Yuan"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Leveraging Cascaded Binary Classification and Multimodal Fusion for Dementia Detection through Spontaneous Speech",
   "original": "1564",
   "order": 115,
   "page_count": 5,
   "abstract": [
    "This paper presents our submission to the PROCESS Challenge 2025, focusing on spontaneous speech analysis for early dementia detection. For the three-class classification task (Healthy Control, Mild Cognitive Impairment, and Dementia), we propose a cascaded binary classification framework that fine-tunes pre-trained language models and incorporates pause encoding to better capture disfluencies. This design streamlines multi-class classification and addresses class imbalance by restructuring the decision process. For the Mini-Mental State Examination score regression task, we develop an enhanced multimodal fusion system that combines diverse acoustic and linguistic features. Separate regression models are trained on individual feature sets, with ensemble learning applied through score averaging. Experimental results on the test set outperform the baselines provided by the organizers in both tasks, demonstrating the robustness and effectiveness of our approach."
   ],
   "p1": 544,
   "pn": 548,
   "doi": "10.21437/Interspeech.2025-1564",
   "url": "interspeech_2025/liu25k_interspeech.html"
  },
  "battula25_interspeech": {
   "authors": [
    [
     "Harish",
     "Battula"
    ],
    [
     "Gauri",
     "Deshpande"
    ],
    [
     "Yagna",
     "Gudipalli"
    ],
    [
     "Sachin",
     "Patel"
    ]
   ],
   "title": "Heart Rate as a Proxy Measure to Assess Human Confidence in Spoken Speech",
   "original": "1570",
   "order": 410,
   "page_count": 5,
   "abstract": [
    "Human confidence reflects a positive self-perception and balanced autonomic nervous system response. In this paper, we present a three stage approach to detect human confidence level by computing the heart rate from speech signals. First stage involves extraction of breathing patterns from speech using a pre-trained model followed by stage 2, where the heart rate is extracted using Independent Component Analysis (ICA) on the breathing patterns. Finally in stage 3, an analysis of heart rate values indicating human confidence levels is done. To the best of our knowledge, this is the first time ever, with our experiments, empirically it is found that heart rate extracted from speech carries information related to the confidence of the candidate. We observe that, on an average, confident speakers have 10 beats per minute lower heart rate as compared to non confident speakers."
   ],
   "p1": 2013,
   "pn": 2017,
   "doi": "10.21437/Interspeech.2025-1570",
   "url": "interspeech_2025/battula25_interspeech.html"
  },
  "wang25o_interspeech": {
   "authors": [
    [
     "Jinfu",
     "Wang"
    ],
    [
     "Ziteng",
     "Wang"
    ],
    [
     "Xin",
     "Liu"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Qing",
     "Shi"
    ],
    [
     "Zhengqiang",
     "Luo"
    ],
    [
     "Feiran",
     "Yang"
    ]
   ],
   "title": "Exploiting Echo Path Priors for Enhanced Stereo Acoustic Echo Cancellation",
   "original": "1572",
   "order": 161,
   "page_count": 5,
   "abstract": [
    "Adaptive filters have long served to model echo paths in stereo acoustic echo cancellation (SAEC). In modern devices such as mobile phones and smart speakers, the fixed geometry of loudspeakers and microphones allows a priori echo paths to be identified. However, this prior knowledge remains underexplored. In this paper, we propose an enhanced multichannel state-space frequency-domain adaptive filtering (MCSSFDAF) algorithm for SAEC, which is informed by a priori echo path energy. By dynamically adjusting the process noise covariance in MCSSFDAF based on tracked misalignment between estimated and prior echo paths, our method achieves faster convergence and lower misalignment. Experiments with both simulated and real-world recordings validate the algorithm’s efficacy, demonstrating accelerated reconvergence during echo path changes and superior performance across diverse scenarios."
   ],
   "p1": 773,
   "pn": 777,
   "doi": "10.21437/Interspeech.2025-1572",
   "url": "interspeech_2025/wang25o_interspeech.html"
  },
  "takeuchi25_interspeech": {
   "authors": [
    [
     "Daiki",
     "Takeuchi"
    ],
    [
     "Binh Thien",
     "Nguyen"
    ],
    [
     "Masahiro",
     "Yasuda"
    ],
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Daisuke",
     "Niizumi"
    ],
    [
     "Noboru",
     "Harada"
    ]
   ],
   "title": "CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer",
   "original": "1573",
   "order": 637,
   "page_count": 5,
   "abstract": [
    "Automated Audio Captioning (AAC) aims to describe the semantic contexts of general sounds, including acoustic events and scenes, by leveraging effective acoustic features. To enhance performance, an AAC method, EnCLAP, employed discrete tokens from EnCodec as an effective input for fine-tuning a language model BART. However, EnCodec is designed to reconstruct waveforms rather than capture the semantic contexts of general sounds, which AAC should describe. To address this issue, we propose CLAP-ART, an AAC method that utilizes &quot;semantic-rich and discrete&quot; tokens as input. CLAP-ART computes semantic-rich discrete tokens from pre-trained audio representations through vector quantization. We experimentally confirmed that CLAP-ART outperforms baseline EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens derived from semantically rich AR are beneficial for AAC."
   ],
   "p1": 3130,
   "pn": 3134,
   "doi": "10.21437/Interspeech.2025-1573",
   "url": "interspeech_2025/takeuchi25_interspeech.html"
  },
  "li25s_interspeech": {
   "authors": [
    [
     "Sirui",
     "Li"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Zhijun",
     "Liu"
    ],
    [
     "Zhongjie",
     "Jiang"
    ],
    [
     "Yannan",
     "Wang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "SpeechRefiner: Towards Perceptual Quality Refinement for Front-End Algorithms",
   "original": "1581",
   "order": 782,
   "page_count": 5,
   "abstract": [
    "Speech pre-processing techniques such as denoising, de-reverberation, and separation, are commonly employed as front-ends for various downstream speech processing tasks. However, these methods can sometimes be inadequate, resulting in residual noise or the introduction of new artifacts. Such deficiencies are typically not captured by metrics like SI-SNR but are noticeable to human listeners. To address this, we introduce SpeechRefiner, a post-processing tool that utilizes Conditional Flow Matching (CFM) to improve the perceptual quality of speech. In this study, we benchmark SpeechRefiner against recent task-specific refinement methods and evaluate its performance within our internal processing pipeline, which integrates multiple front-end algorithms. Experiments show that SpeechRefiner exhibits strong generalization across diverse impairment sources, significantly enhancing speech perceptual quality."
   ],
   "p1": 3828,
   "pn": 3832,
   "doi": "10.21437/Interspeech.2025-1581",
   "url": "interspeech_2025/li25s_interspeech.html"
  },
  "wang25p_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Guibin",
     "Zheng"
    ],
    [
     "Tieran",
     "Zheng"
    ],
    [
     "Yongjun",
     "He"
    ]
   ],
   "title": "Adaptive Across-Subcenter Representation Learning for Imbalanced Anomalous Sound Detection",
   "original": "1584",
   "order": 687,
   "page_count": 5,
   "abstract": [
    "Anomalous Sound Detection requires constructing a distribution using only normal sounds. However, collecting sufficient normal samples across diverse conditions is challenging, leading to sample imbalance within subclasses. Existing subcenter angular margin loss methods use multiple subcenters to capture intra-class diversity but still suffer from under-representation or overfitting. To address this issue, we propose Adaptive Across-Subcenter Representation Learning (AASRL). Unlike existing methods that use either a single or all subcenters, AASRL adaptively selects subcenters based on the representation quality of samples and optimizes their representation across the most relevant subcenters. This ensures efficient representation of each sample and prevents the majority subclass from dominating the representation space. Experiments on the DCASE2023 Challenge Task2 dataset and a constructed imbalanced dataset demonstrate the effectiveness of AASRL."
   ],
   "p1": 3379,
   "pn": 3383,
   "doi": "10.21437/Interspeech.2025-1584",
   "url": "interspeech_2025/wang25p_interspeech.html"
  },
  "xu25j_interspeech": {
   "authors": [
    [
     "Le",
     "Xu"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Yong",
     "Ren"
    ],
    [
     "Yujie",
     "Chen"
    ],
    [
     "Yu",
     "Gu"
    ],
    [
     "Ruibo",
     "Fu"
    ],
    [
     "Shan",
     "Yang"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning",
   "original": "1593",
   "order": 34,
   "page_count": 5,
   "abstract": [
    "Current vision-guided audio captioning systems frequently fail to address audiovisual misalignment in real-world scenarios, such as dubbed content or off-screen sounds. To bridge this critical gap, we present an entropy-aware gated fusion framework that dynamically modulates visual information flow through cross-modal uncertainty quantification. Our novel approach employs attention entropy analysis in cross-attention layers to automatically identify and suppress misleading visual cues during modal fusion. Complementing this architecture, we develop a batch-wise audiovisual shuffling technique that generates synthetic mismatched training pairs, greatly enhancing model resilience against alignment noise. Evaluations on the AudioCaps benchmark demonstrate our system&#x27;s superior performance over existing baselines, especially in mismatched modality scenarios. Furthermore, our solution demonstrates an approximately 6x improvement in inference speed compared to the baseline."
   ],
   "p1": 161,
   "pn": 165,
   "doi": "10.21437/Interspeech.2025-1593",
   "url": "interspeech_2025/xu25j_interspeech.html"
  },
  "yang25l_interspeech": {
   "authors": [
    [
     "Mingru",
     "Yang"
    ],
    [
     "Yanmei",
     "Gu"
    ],
    [
     "Qianhua",
     "He"
    ],
    [
     "Yanxiong",
     "Li"
    ],
    [
     "Peirong",
     "Zhang"
    ],
    [
     "Yongqiang",
     "Chen"
    ],
    [
     "Zhiming",
     "Wang"
    ],
    [
     "Huijia",
     "Zhu"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Weiqiang",
     "Wang"
    ]
   ],
   "title": "Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincaré sphere",
   "original": "1594",
   "order": 462,
   "page_count": 5,
   "abstract": [
    "Audio deepfake detection (ADD) faces critical generalization challenges due to diverse real-world spoofing attacks and domain variations. However, existing methods primarily rely on Euclidean distances, failing to adequately capture the intrinsic hierarchical structures associated with attack categories and domain factors. To address these issues, we design a novel framework Poin-HierNet to construct domain-invariant hierarchical representations in the Poincar´e sphere. Poin-HierNet includes three key components: 1) Poincar´e Prototype Learning (PPL) with several data prototypes aligning sample features and capturing multilevel hierarchies beyond human labels; 2) Hierarchical Structure Learning (HSL) leverages top prototypes to establish a tree-like hierarchical structure from data prototypes; and 3) Poincar´e Feature Whitening (PFW) enhances domain invariance by applying feature whitening to suppress domainsensitive features. We evaluate our approach on four datasets: ASVspoof 2019 LA, ASVspoof 2021 LA, ASVspoof 2021 DF, and In The-Wild. Experimental results demonstrate that Poin-HierNet exceeds state-of-the-art methods in Equal Error Rate."
   ],
   "p1": 2255,
   "pn": 2259,
   "doi": "10.21437/Interspeech.2025-1594",
   "url": "interspeech_2025/yang25l_interspeech.html"
  },
  "futami25_interspeech": {
   "authors": [
    [
     "Hayato",
     "Futami"
    ],
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Yuki",
     "Ito"
    ],
    [
     "Hassan",
     "Shahmohammadi"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs",
   "original": "1595",
   "order": 9,
   "page_count": 5,
   "abstract": [
    "Speech-to-speech translation (S2ST) has been advanced with large language models (LLMs), which are fine-tuned on discrete speech units. In such approaches, modality adaptation from text to speech has been an issue. LLMs are trained on text-only data, which presents challenges to adapt them to speech modality with limited speech-to-speech data. To address the training difficulty, we propose scheduled interleaved speech-text training in this study. We use interleaved speech-text units instead of speech units during training, where aligned text tokens are interleaved at the word level. We gradually decrease the ratio of text as training progresses, to facilitate progressive modality adaptation from text to speech. We conduct experimental evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show that the proposed method consistently improves the translation performances, especially for languages with limited training data."
   ],
   "p1": 36,
   "pn": 40,
   "doi": "10.21437/Interspeech.2025-1595",
   "url": "interspeech_2025/futami25_interspeech.html"
  },
  "zhou25f_interspeech": {
   "authors": [
    [
     "Yuqiu",
     "Zhou"
    ],
    [
     "Yongjie",
     "Zhou"
    ],
    [
     "Yudong",
     "Yang"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Jun",
     "Huang"
    ],
    [
     "Shuzhi",
     "Zhao"
    ],
    [
     "Rongfeng",
     "Su"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Nan",
     "Yan"
    ]
   ],
   "title": "Emotion-Guided Graph Attention Networks for Speech-Based Depression Detection under Emotion-Inducting Tasks",
   "original": "1597",
   "order": 100,
   "page_count": 5,
   "abstract": [
    "Depression affects emotional expression and perception. As a non-invasive and privacy-preserving method, speech is widely used for automatic depression detection. However, existing models often focus only on depressive features in speech, ignoring the differential emotion expression patterns across different emotion-inducing tasks. To address this, we propose an emotion-guided graph attention network (emoGAT) for depression detection. By collecting speech-text data from depressed individuals and healthy controls during emotion-inducing tasks, we construct graph embeddings using sentiment cues from both speech and text. Experimental results show our method reduces the standard deviation by 1.8% and improves accuracy by 4.36%. Graph attention visualization also reveals depression-specific characteristics, such as flattened prosody in neutral picture description tasks and cognitive biases toward negative information, offering deeper insights into emotional relational expressions."
   ],
   "p1": 469,
   "pn": 473,
   "doi": "10.21437/Interspeech.2025-1597",
   "url": "interspeech_2025/zhou25f_interspeech.html"
  },
  "liu25l_interspeech": {
   "authors": [
    [
     "Yin-Long",
     "Liu"
    ],
    [
     "Rui",
     "Feng"
    ],
    [
     "Jia-Xin",
     "Chen"
    ],
    [
     "Yi-Ming",
     "Wang"
    ],
    [
     "Jia-Hong",
     "Yuan"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ]
   ],
   "title": "Beyond Manual Transcripts: The Potential of Automated Speech Recognition Errors in Improving Alzheimer’s Disease Detection",
   "original": "1598",
   "order": 1157,
   "page_count": 5,
   "abstract": [
    "Recent breakthroughs in Automatic Speech Recognition (ASR) have enabled fully automated Alzheimer’s Disease (AD) detection using ASR transcripts. Nonetheless, the impact of ASR errors on AD detection remains poorly understood. This paper fills the gap. We conduct a comprehensive study on AD detection using transcripts from various ASR models and their synthesized speech on the ADReSS dataset. Experimental results reveal that certain ASR transcripts (ASR-synthesized speech) outperform manual transcripts (manual-synthesized speech) in detection accuracy, suggesting that ASR errors may provide valuable cues for improving AD detection. Additionally, we propose a cross-attention-based interpretability model that not only identifies these cues but also achieves superior or comparable performance to the baseline. Furthermore, we utilize this model to unveil AD-related patterns within pre-trained embeddings. Our study offers novel insights into the potential of ASR models for AD detection."
   ],
   "p1": 5678,
   "pn": 5682,
   "doi": "10.21437/Interspeech.2025-1598",
   "url": "interspeech_2025/liu25l_interspeech.html"
  },
  "zhou25g_interspeech": {
   "authors": [
    [
     "Haoshuai",
     "Zhou"
    ],
    [
     "Changgeng",
     "Mo"
    ],
    [
     "Boxuan",
     "Cao"
    ],
    [
     "Linkai",
     "Li"
    ],
    [
     "Shan Xiang",
     "Wang"
    ]
   ],
   "title": "No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction",
   "original": "1599",
   "order": 1115,
   "page_count": 5,
   "abstract": [
    "Personalized speech intelligibility prediction is challenging. Previous approaches have mainly relied on audiograms, which are inherently limited in accuracy as they only capture a listener&#x27;s hearing threshold for pure tones. Rather than incorporating additional listener features, we propose a novel approach that leverages an individual&#x27;s existing intelligibility data to predict their performance on new audio. We introduce the Support Sample-Based Intelligibility Prediction Network (SSIPNet), a deep learning model that leverages speech foundation models to build a high-dimensional representation of a listener&#x27;s speech recognition ability from multiple support (audio, score) pairs, enabling accurate predictions for unseen audio. Results on the Clarity Prediction Challenge dataset show that, even with a small number of support (audio, score) pairs, our method outperforms audiogram-based predictions. Our work presents a new paradigm for personalized speech intelligibility prediction."
   ],
   "p1": 5468,
   "pn": 5472,
   "doi": "10.21437/Interspeech.2025-1599",
   "url": "interspeech_2025/zhou25g_interspeech.html"
  },
  "lim25_interspeech": {
   "authors": [
    [
     "Jin Gyo",
     "Lim"
    ],
    [
     "Seong Eun",
     "Kim"
    ]
   ],
   "title": "SIDC-KWS: Efficient Spiking Inception-Dilated Conformer with Self-Attention for Keyword Spotting",
   "original": "1607",
   "order": 544,
   "page_count": 5,
   "abstract": [
    "Recent deep learning advances have improved keyword spotting (KWS). However, as KWS is deployed on edge devices, energy efficiency remains a key challenge. Conventional deep neural networks offer high accuracy but require heavy computation, making them unsuitable for low-power use. To address this, we propose the Spiking Inception-Dilated Conformer for Keyword Spotting (SIDC-KWS), an energy-efficient transformer based on spiking neural networks (SNNs). By integrating an Inception-Dilated (ID) block and spike-based self-attention, SIDC-KWS maintains high accuracy while significantly reducing power consumption. Experiments on the Google Speech Commands V2 (GSC V2) dataset show that SIDC-KWS achieves 96.8% and 94.7% accuracy on 12-class and 35-class tasks, respectively. On the 35-class task, SIDC-KWS consumes 75.59% less energy than its ANN counterpart. These results underscore SNNs as a scalable, low-power alternative for real-time KWS in resource-limited environments."
   ],
   "p1": 2665,
   "pn": 2669,
   "doi": "10.21437/Interspeech.2025-1607",
   "url": "interspeech_2025/lim25_interspeech.html"
  },
  "choi25g_interspeech": {
   "authors": [
    [
     "Muyeol",
     "Choi"
    ],
    [
     "HyunJung",
     "Choi"
    ],
    [
     "Yohan",
     "Lim"
    ],
    [
     "Jeonguk",
     "Bang"
    ],
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Seonhui",
     "Kim"
    ],
    [
     "Seung",
     "Yun"
    ],
    [
     "Donghyun",
     "Kim"
    ],
    [
     "Minsoo",
     "Kim"
    ],
    [
     "SangHun",
     "Kim"
    ]
   ],
   "title": "Bidirectional Spoken-Written Text Conversion with Large Language Models",
   "original": "1610",
   "order": 1038,
   "page_count": 5,
   "abstract": [
    "Traditional ASR systems normalize transcriptions into spoken form for training, leading to spoken form outputs. In contrast, modern Transformer-based models directly map speech-to-text, preserving the written form. However, existing speech databases mainly contain spoken form, causing inconsistencies in recognition results. To address this, Inverse Text Normalization (ITN) is required, and dual transcription data is crucial for effective training. However, constructing such datasets is costly and time-consuming. This study proposes a data augmentation method leveraging LLMs to automatically generate dual transcription data with minimal effort. It employs iterative learning to expand the dataset through supervised and semi-supervised methods. Additionally, the bidirectional text conversion (BTC) model supports both ITN and TN within a unified framework. Experimental results demonstrate that the proposed method achieved an ERR of 13.4% and 4.7%, outperforming prior approaches."
   ],
   "p1": 5088,
   "pn": 5092,
   "doi": "10.21437/Interspeech.2025-1610",
   "url": "interspeech_2025/choi25g_interspeech.html"
  },
  "koudounas25_interspeech": {
   "authors": [
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Moreno",
     "La Quatra"
    ],
    [
     "Eliana",
     "Pastor"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Elena",
     "Baralis"
    ]
   ],
   "title": "“KAN you hear me?” Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding",
   "original": "1612",
   "order": 841,
   "page_count": 5,
   "abstract": [
    "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional neural architectures, yet their application to speech processing remains under explored. This work presents the first investigation of KANs for Spoken Language Understanding (SLU) tasks. We experiment with 2D-CNN models on two datasets, integrating KAN layers in five different configurations within the dense block. The best-performing setup, which places a KAN layer between two linear layers, is directly applied to transformer-based models and evaluated on five SLU datasets with increasing complexity. Our results show that KAN layers can effectively replace the linear layers, achieving comparable or superior performance in most cases. Finally, we provide insights into how KAN and linear layers on top of transformers differently attend to input regions of the raw waveforms."
   ],
   "p1": 4123,
   "pn": 4127,
   "doi": "10.21437/Interspeech.2025-1612",
   "url": "interspeech_2025/koudounas25_interspeech.html"
  },
  "fang25d_interspeech": {
   "authors": [
    [
     "Yulu",
     "Fang"
    ],
    [
     "Mingyue",
     "He"
    ],
    [
     "Qisheng",
     "Xu"
    ],
    [
     "Jianqiao",
     "Zhao"
    ],
    [
     "Cheng",
     "Yang"
    ],
    [
     "Kele",
     "Xu"
    ],
    [
     "Yong",
     "Dou"
    ]
   ],
   "title": "Multi-view Fusion and Parameter Perturbation for Few-Shot Class-Incremental Audio Classification",
   "original": "1613",
   "order": 269,
   "page_count": 5,
   "abstract": [
    "Audio classification tasks typically assume a fixed number of classes, which is often unrealistic in real-world applications where the target class vocabulary is dynamic or unknown in advance. A significant challenge arises when models must adapt to new classes incrementally, as this process is prone to catastrophic forgetting—a sharp decline in performance on previously learned classes, especially in data-scarce scenarios. While dynamic network-based methods and prototype refinementbased methods have been proposed to address these challenges, they overlook two critical issues: (1) inadequate representation of raw audio samples, which limits generalization, and (2) the risk of overfitting, which limits adaptivity. In this paper, we propose Multi-View Fusion and Parameter Perturbation (MVF2P), a novel framework that leverages the complementary learning system to enhance generalizability and adaptivity within a unified incremental learning framework. MVF2P addresses the limitations of existing methods by integrating multi-view learning to enrich feature representation and a parameter perturbation mechanism to reduce overfitting. Extensive evaluations on two widely-used audio datasets, NS-100 and LS-100, demonstrate that MVF2P outperforms state-of-the-art methods in terms of average accuracy and performance drop rate. Notably, MVF2P not only mitigates catastrophic forgetting more effectively but also enhances the model’s adaptability to new classes, making it a robust solution for dynamic audio classification tasks."
   ],
   "p1": 1313,
   "pn": 1317,
   "doi": "10.21437/Interspeech.2025-1613",
   "url": "interspeech_2025/fang25d_interspeech.html"
  },
  "kamo25_interspeech": {
   "authors": [
    [
     "Naoyuki",
     "Kamo"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "MOVER: Combining Multiple Meeting Recognition Systems",
   "original": "1614",
   "order": 696,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose Meeting recognizer Output Voting Error Reduction (MOVER), a novel system combination method for meeting recognition tasks. Although there are methods to combine the output of diarization (e.g., DOVER) or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first approach that can combine the outputs of meeting recognition systems that differ in terms of both diarization and ASR. MOVER combines hypotheses with different time intervals and speaker labels through a five-stage process that includes speaker alignment, segment grouping, word and timing combination, etc. Experimental results on the CHiME-8 DASR task and the multi-channel track of the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple meeting recognition systems with diverse diarization and recognition outputs, achieving relative tcpWER improvements of 9.55 % and 8.51 % over the state-of-the-art systems for both tasks."
   ],
   "p1": 3424,
   "pn": 3428,
   "doi": "10.21437/Interspeech.2025-1614",
   "url": "interspeech_2025/kamo25_interspeech.html"
  },
  "kutsakov25_interspeech": {
   "authors": [
    [
     "Aleksandr",
     "Kutsakov"
    ],
    [
     "Alexandr",
     "Maximenko"
    ],
    [
     "Georgii",
     "Gospodinov"
    ],
    [
     "Pavel",
     "Bogomolov"
    ],
    [
     "Fyodor",
     "Minkin"
    ]
   ],
   "title": "GigaAM: Efficient Self-Supervised Learner for Speech Recognition",
   "original": "1616",
   "order": 249,
   "page_count": 5,
   "abstract": [
    "Self-Supervised Learning (SSL) has demonstrated strong performance in speech processing, particularly in automatic speech recognition. In this paper, we explore an SSL pretraining framework that leverages masked language modeling with targets derived from a speech recognition model. We also present chunkwise attention with dynamic chunk size sampling during pretraining to enable both full-context and streaming fine-tuning. Our experiments examine scaling with respect to model size and the amount of data. Using our method, we train the GigaAM family of models, including a state-of-the-art model for Russian speech recognition that outperforms Whisper-large-v3 by 50%. We have released our foundation and ASR models, along with the inference code, under the MIT license as open-source resources to the research community."
   ],
   "p1": 1213,
   "pn": 1217,
   "doi": "10.21437/Interspeech.2025-1616",
   "url": "interspeech_2025/kutsakov25_interspeech.html"
  },
  "proctor25_interspeech": {
   "authors": [
    [
     "Michael",
     "Proctor"
    ],
    [
     "Tünde",
     "Szalay"
    ],
    [
     "Tharinda",
     "Piyadasa"
    ],
    [
     "Craig",
     "Jin"
    ],
    [
     "Naeim",
     "Sanaei"
    ],
    [
     "Amelia",
     "Gully"
    ],
    [
     "David",
     "Waddington"
    ],
    [
     "Sheryl",
     "Foster"
    ],
    [
     "Kirrie",
     "Ballard"
    ]
   ],
   "title": "Rhotic Articulation in Australian English: Insights from MRI",
   "original": "1619",
   "order": 712,
   "page_count": 5,
   "abstract": [
    "English rhotics are realized with rich allophony across speakers, contexts and varieties, but Australian English /ô/ has not previously been examined in detail. Rhotic approximants produced in three vowel contexts by four speakers of Australian English were captured using real-time and volumetric structural magnetic resonance imaging. /ô/ was articulated with bunched tongue postures by two speakers and more apical configurations by two speakers, but all rhotics were characterized by three coordinated gestures: tongue tip, tongue body and labial constrictions. These data shed new light on the complex goals of production of rhotic approximants beyond the midsagittal plane, and their realization and extent of variation in Australian English."
   ],
   "p1": 3499,
   "pn": 3503,
   "doi": "10.21437/Interspeech.2025-1619",
   "url": "interspeech_2025/proctor25_interspeech.html"
  },
  "zevallos25_interspeech": {
   "authors": [
    [
     "Rodolfo",
     "Zevallos"
    ],
    [
     "Martí",
     "Cortada Garcia"
    ],
    [
     "Sarah",
     "Solito"
    ],
    [
     "Carlos",
     "Mena"
    ],
    [
     "Alex",
     "Peiró-Lilja"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Assessing the Performance and Efficiency of Mamba ASR in Low-Resource Scenarios",
   "original": "1624",
   "order": 1060,
   "page_count": 5,
   "abstract": [
    "Mamba, a state space model-based architecture, is emerging as a strong alternative to Transformer models, showing equal or superior performance in sequence generation, including speech. However, analyses have focused mainly on high-resource scenarios. This paper explores Mamba’s potential in ASR for low-resource scenarios. We compare the Transformer-based Conformer and its state-space counterpart, ConMamba, across nine languages with varying training data. Our results show that ConMamba achieves similar WER to Conformer for short-context inputs but significantly improves performance on long-context inputs, reducing WER by up to 50% on average. Additionally, ConMamba enhances efficiency, requiring 40–45% less training time, using 50% less memory, and accelerating inference by 63–70%, making it a more effective ASR solution across different data availability scenarios."
   ],
   "p1": 5198,
   "pn": 5202,
   "doi": "10.21437/Interspeech.2025-1624",
   "url": "interspeech_2025/zevallos25_interspeech.html"
  },
  "yu25c_interspeech": {
   "authors": [
    [
     "Gwangyeol",
     "Yu"
    ],
    [
     "Junhyeok",
     "Lee"
    ],
    [
     "Seoryeong",
     "Kim"
    ],
    [
     "Jimin",
     "Lee"
    ],
    [
     "Jehyuk",
     "Lee"
    ]
   ],
   "title": "Mimic Blocker: Self-Supervised Adversarial Training for Voice Conversion Defense with Pretrained Feature Extractors",
   "original": "1625",
   "order": 339,
   "page_count": 5,
   "abstract": [
    "Voice conversion (VC) enables natural speech synthesis with minimal data; however, it poses security risks, e.g., identity theft and privacy breaches. To address this, we propose Mimic Blocker, an active defense mechanism that prevents VC models from extracting speaker characteristics while preserving audio quality. Our method employs adversarial training, an audio quality preservation strategy, and an attack strategy. It relies on only publicly available pretrained feature extractors, which ensures model-agnostic protection. Furthermore, it enables self-supervised learning using only the original speaker&#x27;s speech. Experimental results demonstrate that our method achieves robust defense performance in both white-box and black-box scenarios. Notably, the proposed approach maintains audio quality by generating noise imperceptible to human listeners, thereby enabling protection while retaining natural voice characteristics in practical applications."
   ],
   "p1": 1663,
   "pn": 1667,
   "doi": "10.21437/Interspeech.2025-1625",
   "url": "interspeech_2025/yu25c_interspeech.html"
  },
  "gao25f_interspeech": {
   "authors": [
    [
     "Xiyuan",
     "Gao"
    ],
    [
     "Bruce Xiao",
     "Wang"
    ],
    [
     "Meiling",
     "Zhang"
    ],
    [
     "Shuming",
     "Huang"
    ],
    [
     "Zhu",
     "Li"
    ],
    [
     "Shekhar",
     "Nayak"
    ],
    [
     "Matt",
     "Coler"
    ]
   ],
   "title": "A Multimodal Chinese Dataset for Cross-lingual Sarcasm Detection",
   "original": "1632",
   "order": 810,
   "page_count": 5,
   "abstract": [
    "Sarcasm is expressed through subtle cues like pitch, speech rate, and facial expressions, with patterns varying across languages, e.g., English speakers lower the pitch while Cantonese speakers raise it. While humans readily interpret these signals, computational models struggle, creating challenges for Human-Machine Interaction. Most multimodal sarcasm recognition research focuses on English and the lack of high-quality datasets for other languages hinders cross-lingual and cross-cultural studies. We introduce the Multimodal Chinese Sarcasm Dataset (MCSD), containing 10.57 hours of video. We propose a standardized annotation framework that captures annotator certainty to reflect the subjectivity of sarcasm, achieving a Fleiss’ kappa of 0.74 (unweighted) and 0.79 (certainty-weighted). Validation of our dataset using SVM achieves a 76.64% F1-score in sarcasm detection. MCSD lays the foundation for robust cross-lingual sarcasm detection, contributing to advanced, human-centric systems."
   ],
   "p1": 3968,
   "pn": 3972,
   "doi": "10.21437/Interspeech.2025-1632",
   "url": "interspeech_2025/gao25f_interspeech.html"
  },
  "lin25f_interspeech": {
   "authors": [
    [
     "Zheyuan",
     "Lin"
    ],
    [
     "Siqi",
     "Cai"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Decoding Listener's Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer",
   "original": "1637",
   "order": 1131,
   "page_count": 5,
   "abstract": [
    "EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs."
   ],
   "p1": 5548,
   "pn": 5552,
   "doi": "10.21437/Interspeech.2025-1637",
   "url": "interspeech_2025/lin25f_interspeech.html"
  },
  "xing25_interspeech": {
   "authors": [
    [
     "Jingyuan",
     "Xing"
    ],
    [
     "Zhipeng",
     "Li"
    ],
    [
     "Shuaiqi",
     "Chen"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "EATS-Speech: Emotion-Adaptive Transformation and Priority Synthesis for Zero-Shot Text-to-Speech",
   "original": "1638",
   "order": 888,
   "page_count": 5,
   "abstract": [
    "Zero-shot text-to-speech (TTS) supports diverse speech synthesis without speaker-specific data but struggles to accurately transfer emotions from reference to target text. Traditional approaches treat emotion as part of a global style, leading to inconsistent emotional expressiveness. To address this, we propose EATS-Speech, an Emotion-Adaptive Transformation Synthesis framework. EATS-Speech employs Emotion Priority Synthesis through a parallel pipeline that decomposes speech into non-emotion style, emotion, and content. It prioritizes emotion generation to enhance expressiveness. Furthermore, it introduces Emotion-Adaptive Transformation Synthesis, where an LLM-based converter learns text-emotion mapping patterns from the reference speech and transfers them to the target text. Experiments on the LibriTTS dataset demonstrate the improvements in emotional expressiveness and accurate emotion adaptation."
   ],
   "p1": 4358,
   "pn": 4362,
   "doi": "10.21437/Interspeech.2025-1638",
   "url": "interspeech_2025/xing25_interspeech.html"
  },
  "jacquelin25_interspeech": {
   "authors": [
    [
     "Maxime",
     "Jacquelin"
    ],
    [
     "Maëva",
     "Garnier"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Rémy",
     "Vincent"
    ],
    [
     "Olivier",
     "Perrotin"
    ]
   ],
   "title": "LombardTokenizer: Disentanglement and Control of Vocal Effort in a Neural Speech Codec",
   "original": "1639",
   "order": 1177,
   "page_count": 5,
   "abstract": [
    "Disentangling distinct types of information in speech representations is crucial for improving speech synthesis and voice conversion systems. In this work, we introduce LombardTokenizer, a neural speech codec able to separate features related to vocal effort from other acoustic (and semantic) information. This model is built on SpeechTokenizer, a model proposed in the literature based on multi-stage quantisation, which focused on isolating semantic content in its first quantisation layer. We show that the level of vocal effort can be effectively captured in the second quantisation layer by conditioning the quantisation layer with neural encoders trained to represent vocal effort. Experimental results demonstrate that the proposed method significantly outperforms existing methods in speech conversion between neutral and Lombard speech, while maintaining excellent speech synthesis quality, offering improved control over vocal effort and naturalness of synthesised speech."
   ],
   "p1": 5778,
   "pn": 5782,
   "doi": "10.21437/Interspeech.2025-1639",
   "url": "interspeech_2025/jacquelin25_interspeech.html"
  },
  "lu25e_interspeech": {
   "authors": [
    [
     "Chunhui",
     "Lu"
    ],
    [
     "Xue",
     "Wen"
    ],
    [
     "Liming",
     "Song"
    ],
    [
     "Junkwang",
     "Oh"
    ]
   ],
   "title": "Robust Neural Codec Language Modeling with Phoneme Position Prediction for Zero-Shot TTS",
   "original": "1641",
   "order": 506,
   "page_count": 5,
   "abstract": [
    "Though large language models (LLMs) based methods have achieved remarkable progress in zero-shot text-to-speech (TTS) synthesis, they suffer from robustness issues including mispronunciation, word skipping, and word repeating. To address these robustness challenges, we propose incorporating phoneme position prediction into the LLM-based TTS model. More concretely, given an input phoneme sequence as condition, our model autoregressively predicts acoustic codes and their corresponding phoneme positions within the input sequence synchronously. This mechanism ensures accurate and complete alignment between acoustic codes and phonemes. Experimental results demonstrate that our system significantly reduces phoneme skipping/repetition errors compared to strong baselines, achieving a 52.7% relative reduction in character error rate while maintaining comparable performance in zero-shot TTS evaluations."
   ],
   "p1": 2475,
   "pn": 2479,
   "doi": "10.21437/Interspeech.2025-1641",
   "url": "interspeech_2025/lu25e_interspeech.html"
  },
  "yoshinaga25_interspeech": {
   "authors": [
    [
     "Tomoya",
     "Yoshinaga"
    ],
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Keitaro",
     "Tanaka"
    ],
    [
     "Keisuke",
     "Imoto"
    ],
    [
     "Masaki",
     "Onishi"
    ],
    [
     "Shigeo",
     "Morishima"
    ]
   ],
   "title": "Training Onset-and-Offset-Aware Sound Event Detection  on a Heterogeneous Dataset via Probabilistic Sequential Modeling",
   "original": "1642",
   "order": 268,
   "page_count": 5,
   "abstract": [
    "This paper presents a neural method to train onset-and-offset-aware sound event detection (SED) using heterogeneously labeled data. SED models are typically trained to predict frame-wise event activities, which have temporal fluctuations, resulting in unstable event boundaries. An end-to-end (E2E) method based on a hidden semi-Markov model (HSMM) has been proposed to improve performance by converting frame-wise predictions into event boundaries. This method, however, relies on temporal (strong) labels, which are costly to annotate. To overcome this limitation, we propose an E2E method to train an HSMM-based model from clip-level labels and unlabeled data. While the strong supervision was formulated to maximize event-wise posterior probabilities, we derive probabilistic objectives for such incompletely labeled data. Experimental results on the DESED dataset show that our method outperforms standard frame-wise methods."
   ],
   "p1": 1308,
   "pn": 1312,
   "doi": "10.21437/Interspeech.2025-1642",
   "url": "interspeech_2025/yoshinaga25_interspeech.html"
  },
  "gupta25_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Gupta"
    ],
    [
     "MLNS",
     "Karthik"
    ],
    [
     "Yughendaran",
     "P"
    ]
   ],
   "title": "Low Complex IIR Adaptive Hear-Through Ambient Filtering for Overcoming Practical Constraints in Earbuds",
   "original": "1646",
   "order": 634,
   "page_count": 5,
   "abstract": [
    "The materials used in the headphones and earbuds typically attenuate sound passively above frequency of 500-700 Hz. Hear-Through (HT) techniques can compensate for the passive attenuation characteristics of listening devices to achieve a listening experience similar to the open ear for enhancing situational awareness of the users. Most studies have proposed usage of fixed-filter or adaptive finite impulse response (FIR) HT techniques, which can lead to mismatch between the modelled and desired open ear response for different users and fittings or lead to higher computations and larger processing delays. This paper proposes a virtual sensing model with an infinite impulse response (IIR) based adaptive equalization method to model the open ear response at user’s eardrum. The proposed method reduces the computational complexity by 34%, while maintaining stability and providing superior HT performance upto 10 dB compared to existing methods in dynamic acoustic scenarios."
   ],
   "p1": 3115,
   "pn": 3119,
   "doi": "10.21437/Interspeech.2025-1646",
   "url": "interspeech_2025/gupta25_interspeech.html"
  },
  "piyadasa25_interspeech": {
   "authors": [
    [
     "Tharinda",
     "Piyadasa"
    ],
    [
     "Joan",
     "Glaunès"
    ],
    [
     "Amelia",
     "Gully"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Kirrie",
     "Ballard"
    ],
    [
     "Tünde",
     "Szalay"
    ],
    [
     "Naeim",
     "Sanaei"
    ],
    [
     "Sheryl",
     "Foster"
    ],
    [
     "David",
     "Waddington"
    ],
    [
     "Craig",
     "Jin"
    ]
   ],
   "title": "Constrained LDDMM for Dynamic Vocal Tract Morphing: Integrating Volumetric and Real-Time MRI",
   "original": "1650",
   "order": 200,
   "page_count": 5,
   "abstract": [
    "We present a novel framework for analyzing dynamic vocal tract deformations by integrating volumetric Magnetic Resonance Imaging (MRI) data and real-time MRI (rtMRI) boundary constraints within an iterative Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework. More precisely, we apply LDDMM to morph volumetric vocal tract shapes using rtMRI boundary constraints that enable a smooth and anatomically plausible articulatory transformation. We demonstrate the method and discuss the issues involved using a vowel-consonant-vowel sequence. We show the influence of varying the number of rtMRI images on the resulting articulatory transformation."
   ],
   "p1": 968,
   "pn": 972,
   "doi": "10.21437/Interspeech.2025-1650",
   "url": "interspeech_2025/piyadasa25_interspeech.html"
  },
  "xue25_interspeech": {
   "authors": [
    [
     "Hongfei",
     "Xue"
    ],
    [
     "Yufeng",
     "Tang"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Xuelong",
     "Geng"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Selective Invocation for Multilingual ASR: A Cost-effective Approach Adapting to Speech Recognition Difficulty",
   "original": "1652",
   "order": 527,
   "page_count": 5,
   "abstract": [
    "Although multilingual automatic speech recognition (ASR) systems have significantly advanced, enabling a single model to handle multiple languages, inherent linguistic differences and data imbalances challenge SOTA performance across all languages. While language identification (LID) models can route speech to the appropriate ASR model, they incur high costs from invoking SOTA commercial models and suffer from inaccuracies due to misclassification. To overcome these, we propose SIMA, a selective invocation for multilingual ASR that adapts to the difficulty level of the input speech. Built on a spoken large language model (SLLM), SIMA evaluates whether the input is simple enough for direct transcription or requires the invocation of a SOTA ASR model. Our approach reduces word error rates by 18.7% compared to the SLLM and halves invocation costs compared to LID-based methods. Tests on three datasets show that SIMA is a scalable, cost-effective solution for multilingual ASR applications."
   ],
   "p1": 2580,
   "pn": 2584,
   "doi": "10.21437/Interspeech.2025-1652",
   "url": "interspeech_2025/xue25_interspeech.html"
  },
  "a25_interspeech": {
   "authors": [
    [
     "Noumida",
     "A"
    ],
    [
     "Rajeev",
     "Rajan"
    ]
   ],
   "title": "Analysis of Avian Biphonic Vocalization Using Computational Modelling",
   "original": "1658",
   "order": 345,
   "page_count": 5,
   "abstract": [
    "Birds produce complex vocalizations through the coordinated dynamics of neuromuscular control, where the syrinx acts as the sound source, and suprasyringeal vocal tract structures function as acoustic filters. In this study, we developed computational models of the avian syrinx and upper vocal tract, reconstructed from high-resolution micro-CT imaging. Using finite element analysis in COMSOL Multiphysics, we introduce dual sources at the bottom of the trachea to analyze the filtering mechanisms underlying biphonic vocalization. The simulation results are experimentally validated using real bird vocalizations. To further elucidate the biomechanical factors governing resonance modulation, we systematically vary tracheal length, glottal radius, and beak angle. Additionally, we perform an analysis of syllable bandwidth to assess the influence of vocal tract dynamics on the frequency range observed in biphonic vocalizations."
   ],
   "p1": 1693,
   "pn": 1697,
   "doi": "10.21437/Interspeech.2025-1658",
   "url": "interspeech_2025/a25_interspeech.html"
  },
  "liu25m_interspeech": {
   "authors": [
    [
     "Yuyun",
     "Liu"
    ],
    [
     "Yujia",
     "Gu"
    ],
    [
     "Jiahao",
     "Luo"
    ],
    [
     "Wenming",
     "Zheng"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Yuan",
     "Zong"
    ]
   ],
   "title": "Interactive Fusion of Multi-View Speech Embeddings via Pretrained Large-Scale Speech Models for Speech Emotional Attribute Prediction in Naturalistic Conditions ",
   "original": "1662",
   "order": 954,
   "page_count": 5,
   "abstract": [
    "This paper addresses the Task 2 of Speech Emotion Recognition in Naturalistic Conditions Challenge at INTERSPEECH 2025, i.e., Emotional Attribute Prediction, and presents a simple and effective method named the Interactive Fusion of Multi-View Speech Embeddings (IF-MVSE). In this method, pretrained large-scale speech models are first utilized to extract multi-view speech embeddings, allowing for capturing complementary speech representations from multiple perspectives of speech signals. Subsequently, we design an interactive fusion strategy consisting of dual-feature interactive attention and multi-view self-balancing gated operations to integrate and enhance these speech embeddings from multiple views to predict the dimensional emotion attributes. Our IF-MVSE achieved the average CCC of 0.5955 on the official testing set, securing the Third Place in this track."
   ],
   "p1": 4688,
   "pn": 4692,
   "doi": "10.21437/Interspeech.2025-1662",
   "url": "interspeech_2025/liu25m_interspeech.html"
  },
  "cordlandwehr25_interspeech": {
   "authors": [
    [
     "Tobias",
     "Cord-Landwehr"
    ],
    [
     "Tobias",
     "Gburrek"
    ],
    [
     "Marc",
     "Deegen"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Spatio-Spectral Diarization of Meetings by Combining TDOA-based Segmentation and Speaker Embedding-based Clustering",
   "original": "1663",
   "order": 1065,
   "page_count": 5,
   "abstract": [
    "We propose a spatio-spectral, combined model-based and data-driven diarization pipeline consisting of TDOA-based segmentation followed by embedding-based clustering. The proposed system requires neither access to multi-channel training data nor prior knowledge about the number or placement of microphones. It works for both a compact microphone array and distributed microphones, with minor adjustments. Due to its superior handling of overlapping speech during segmentation, the proposed pipeline significantly outperforms the single-channel pyannote approach, both in a scenario with a compact microphone array and in a setup with distributed microphones. Additionally, we show that, unlike fully spatial diarization pipelines, the proposed system can correctly track speakers when they change positions."
   ],
   "p1": 5223,
   "pn": 5227,
   "doi": "10.21437/Interspeech.2025-1663",
   "url": "interspeech_2025/cordlandwehr25_interspeech.html"
  },
  "sofer25_interspeech": {
   "authors": [
    [
     "Amit",
     "Sofer"
    ],
    [
     "Yoav",
     "Goldman"
    ],
    [
     "Shlomo E.",
     "Chazan"
    ]
   ],
   "title": "Pull It Together: Reducing the Modality Gap in Contrastive Learning",
   "original": "1664",
   "order": 41,
   "page_count": 5,
   "abstract": [
    "Contrastive learning has become a powerful strategy for aligning different modalities in a shared embedding space. Contrastive Language–Image Pre-training (CLIP) has achieved remarkable performance across various downstream tasks. This methodology has been extended to the audio-text domain through Contrastive Language–Audio Pre-training (CLAP), demonstrating strong performance in related tasks. However, recent work highlights a modality gap in CLIP’s embedding space, where embeddings from different modalities remain partially separated rather than fully integrated. In this paper, we begin by analyzing the CLAP embedding space and identify a similar modality gap. Furthermore, we propose a novel solution combining a modality classifier with a Gradient Reverse Layer (GRL) to reduce this gap. Our experiments on CLIP and CLAP confirm that our approach reduces the modality gap while improving performance, and even achieving new State Of The Art (SOTA) results in text-audio retrieval."
   ],
   "p1": 196,
   "pn": 200,
   "doi": "10.21437/Interspeech.2025-1664",
   "url": "interspeech_2025/sofer25_interspeech.html"
  },
  "zhao25h_interspeech": {
   "authors": [
    [
     "Siyi",
     "Zhao"
    ],
    [
     "Wei",
     "Wang"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Lightweight Front-end Enhancement for Robust ASR via Frame Resampling and Sub-Band Pruning",
   "original": "1668",
   "order": 693,
   "page_count": 5,
   "abstract": [
    "Recent advancements in automatic speech recognition (ASR) have achieved notable progress, whereas robustness in noisy environments remains challenging. While speech enhancement (SE) front-ends are widely used to mitigate noise as a preprocessing step for ASR, they often introduce computational non-negligible overhead. This paper proposes optimizations to reduce SE computational costs without compromising ASR performance. Our approach integrates layer-wise frame resampling and progressive sub-band pruning. Frame resampling downsamples inputs within layers, utilizing residual connections to mitigate information loss. Simultaneously, sub-band pruning progressively excludes less informative frequency bands, further reducing computational demands. Extensive experiments on synthetic and real-world noisy datasets demonstrate that our system reduces SE computational overhead over 66% compared to the standard BSRNN, while maintaining strong ASR performance."
   ],
   "p1": 3409,
   "pn": 3413,
   "doi": "10.21437/Interspeech.2025-1668",
   "url": "interspeech_2025/zhao25h_interspeech.html"
  },
  "xu25k_interspeech": {
   "authors": [
    [
     "Tianyi",
     "Xu"
    ],
    [
     "Hongjie",
     "Chen"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Lv",
     "Hang"
    ],
    [
     "Jian",
     "Kang"
    ],
    [
     "Jie",
     "Li"
    ],
    [
     "Zhennan",
     "Lin"
    ],
    [
     "Yongxiang",
     "Li"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis",
   "original": "1669",
   "order": 123,
   "page_count": 5,
   "abstract": [
    "Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pretraining, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research."
   ],
   "p1": 584,
   "pn": 588,
   "doi": "10.21437/Interspeech.2025-1669",
   "url": "interspeech_2025/xu25k_interspeech.html"
  },
  "lu25f_interspeech": {
   "authors": [
    [
     "Siqi",
     "Lu"
    ],
    [
     "Hui",
     "Feng"
    ],
    [
     "Ziyu",
     "Xiong"
    ]
   ],
   "title": "The Role of Syntactic Structures in Shaping Directionality in Trisyllabic Tone Sandhi: Evidence from Tianjin Mandarin",
   "original": "1673",
   "order": 146,
   "page_count": 5,
   "abstract": [
    "Prior studies reported inconsistent syntactic influence on trisyllabic tone sandhi directionality in Tianjin Mandarin. This study investigates the role of six syntactic structures and generational differences in the application of sandhi rules, analyzing seven trisyllabic tone patterns across two structural configurations (&quot;2+1&quot;/&quot;1+2&quot;), with data from 26 native speakers. Key findings are: 1) T4-T4-T1 pattern triggers right-edge sandhi, producing a novel T4-T2-T1 pattern. 2) T3-T1-T1 pattern exhibits one-round sandhi in VP+N, A+NP, N+NP, Q+NP, and V+NP structures, but two-round application in NP+N. 3) Diachronic shifts: the absence of T4-T4 sandhi and the replacement of T1-T1→T3-T1 with T1-T1→T2-T1 apply universally across generations, restructuring trisyllabic outcomes without altering directionality. These findings clarify syntax-sandhi interplay and generational variation in Tianjin Mandarin, revealing systematic directional patterns tied to syntactic structures."
   ],
   "p1": 699,
   "pn": 703,
   "doi": "10.21437/Interspeech.2025-1673",
   "url": "interspeech_2025/lu25f_interspeech.html"
  },
  "sun25h_interspeech": {
   "authors": [
    [
     "Jingyi",
     "Sun"
    ],
    [
     "Bowei",
     "Shao"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Apical vs. Regular Vowel Duration: A Corpus-based Analysis of Contextual Influences in Standard Mandarin",
   "original": "1674",
   "order": 967,
   "page_count": 5,
   "abstract": [
    "The syllabic fricatives, traditionally known as apical vowels in Standard Mandarin, share the place of articulation with their sibilant onsets, exhibiting a narrow tongue tip constriction at the dental/alveolar or postalveolar region. We hypothesize that, like voiced fricatives, they face aerodynamic constraints where voicing and frication cannot be fully optimized simultaneously. The narrow constriction may delay intra-oral pressure release, leading to unstable and delayed voicing onset and consequently shorter acoustic durations. To test this hypothesis while minimizing segmentation errors, we applied forced alignment to a large-scale, style-comparable corpus to analyze their durational differences. The results show that in both spontaneous and read speech, the syllabic fricatives are significantly shorter compared to regular vocalic nuclei. The findings join previous articulatory and acoustic evidence and provide further evidence on the fricative nature of these syllabic segments."
   ],
   "p1": 4753,
   "pn": 4757,
   "doi": "10.21437/Interspeech.2025-1674",
   "url": "interspeech_2025/sun25h_interspeech.html"
  },
  "liu25n_interspeech": {
   "authors": [
    [
     "Yizhi",
     "Liu"
    ],
    [
     "Luyuan",
     "Geng"
    ],
    [
     "Yan",
     "Gu"
    ],
    [
     "Mengru",
     "Han"
    ]
   ],
   "title": "Sentence-Final Particles in Mandarin Child-Directed Speech: Frequency and Impact on Speech Rate",
   "original": "1675",
   "order": 777,
   "page_count": 5,
   "abstract": [
    "This study examined the semi-spontaneous speech produced by 40 Mandarin-speaking mothers during a storytelling task, focusing on the frequency of sentence-final particles (SFPs) and their impact on speech rate. Results showed that although the overall usage of SFPs was higher in the child-directed speech (CDS) than in adult-directed speech (ADS), this increase was limited to declaratives; in contrast, interrogatives showed the opposite pattern. Further analysis revealed that the higher frequency of SFPs in CDS was driven by a higher proportion of interrogatives. For speech rate, CDS utterances with SFPs were spoken faster than those without, a pattern not observed in ADS. Moreover, mothers lengthened the duration of SFPs in both CDS and ADS, resulting in a slower rate for SFPs compared to the rest utterance parts. These findings enhanced our understanding of caregivers&#x27; speech strategies when communicating with children and provided new perspectives in the research of Mandarin SFPs."
   ],
   "p1": 3803,
   "pn": 3807,
   "doi": "10.21437/Interspeech.2025-1675",
   "url": "interspeech_2025/liu25n_interspeech.html"
  },
  "xue25b_interspeech": {
   "authors": [
    [
     "Wei",
     "Xue"
    ],
    [
     "Iuliia",
     "Zaitova"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "The Effect of Word Predictability on Spoken Cross-Language Intelligibility",
   "original": "1676",
   "order": 776,
   "page_count": 5,
   "abstract": [
    "Cross-language intelligibility refers to how well speakers of language A understand language B without prior learning. While the impact of linguistic and extra-linguistic factors on cross-language intelligibility has been widely studied, the effect of word predictability, known to impact comprehension and speech perception, remains underexplored. This study examines this effect by comparing German and English native speakers translating Dutch words presented in Dutch spoken sentential utterances with varying word predictability. We also investigate whether additional written context would aid cross-language intelligibility. Our results showed that word predictability significantly influences cross-language intelligibility, with German speakers experiencing even stronger effects, whereas only English speakers benefit from the additional written context. These findings suggest that word predictability dynamically shapes cross-language intelligibility, tending to be language-specific."
   ],
   "p1": 3798,
   "pn": 3802,
   "doi": "10.21437/Interspeech.2025-1676",
   "url": "interspeech_2025/xue25b_interspeech.html"
  },
  "li25t_interspeech": {
   "authors": [
    [
     "Xueru",
     "Li"
    ],
    [
     "Jingyuan",
     "Xing"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Zhipeng",
     "Li"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "SA-RAS: Speaker-Aware Style Retrieval Augmented Generation for Expressive Zero-Shot Text-to-Speech Synthesis",
   "original": "1684",
   "order": 894,
   "page_count": 5,
   "abstract": [
    "Zero-shot text-to-speech (TTS) models can clone an unseen speaker by reference speech. Given multiple reference speeches, Retrieval Augmented Generation (RAG) shows potential in enhancing synthesis quality. The existing method faces two key limitations: semantically-biased retrieval tends to introduce style mismatch, and the deterministic mapping between text and speech fails to fully capture the unique style of the speaker. To solve these, we propose a Speaker-Aware Retrieval Augmented TTS Synthesis (SA-RAS) model. It consists of a two-stage TTS model, Split-TTS and a zero-shot CLAP. The former embeds style and timbre separately to obtain a style space for retrieval. The latter optimizes the correlation between text and style by incorporating speaker information into the retrieval model. Experiments prove that SA-RAS boosts the style expressiveness of synthesized speech."
   ],
   "p1": 4388,
   "pn": 4392,
   "doi": "10.21437/Interspeech.2025-1684",
   "url": "interspeech_2025/li25t_interspeech.html"
  },
  "li25u_interspeech": {
   "authors": [
    [
     "Longhao",
     "Li"
    ],
    [
     "Yangze",
     "Li"
    ],
    [
     "Hongfei",
     "Xue"
    ],
    [
     "Jie",
     "Liu"
    ],
    [
     "Shuai",
     "Fang"
    ],
    [
     "Kai",
     "Wang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Delayed-KD: Delayed Knowledge Distillation based CTC for Low-Latency Streaming ASR",
   "original": "1691",
   "order": 899,
   "page_count": 5,
   "abstract": [
    "CTC-based streaming ASR has gained significant attention in real-world applications but faces two main challenges: accuracy degradation in small chunks and token emission latency. To mitigate these challenges, we propose Delayed-KD, which applies delayed knowledge distillation on CTC posterior probabilities from a non-streaming to a streaming model. Specifically, with a tiny chunk size, we introduce a Temporal Alignment Buffer (TAB) that defines a relative delay range compared to the non-streaming teacher model to align CTC outputs and mitigate non-blank token mismatches. Additionally, TAB enables fine-grained control over token emission delay. Experiments on 178-hour AISHELL-1 and 10,000-hour WenetSpeech Mandarin datasets show consistent superiority of Delayed-KD. Impressively, Delayed-KD at 40 ms latency achieves a lower character error rate (CER) of 5.42% on AISHELL-1, comparable to the competitive U2++ model running at 320 ms latency."
   ],
   "p1": 4413,
   "pn": 4417,
   "doi": "10.21437/Interspeech.2025-1691",
   "url": "interspeech_2025/li25u_interspeech.html"
  },
  "kim25r_interspeech": {
   "authors": [
    [
     "Hyung Kyu",
     "Kim"
    ],
    [
     "Hak Gu",
     "Kim"
    ]
   ],
   "title": "Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation",
   "original": "1692",
   "order": 769,
   "page_count": 5,
   "abstract": [
    "Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation."
   ],
   "p1": 3763,
   "pn": 3767,
   "doi": "10.21437/Interspeech.2025-1692",
   "url": "interspeech_2025/kim25r_interspeech.html"
  },
  "vauquier25_interspeech": {
   "authors": [
    [
     "Nathalie",
     "Vauquier"
    ],
    [
     "Brij Mohan Lal",
     "Srivastava"
    ],
    [
     "Seyed Ahmad",
     "Hosseini"
    ],
    [
     "Emmanuel",
     "Vincent"
    ]
   ],
   "title": "Legally validated evaluation framework for voice anonymization",
   "original": "1699",
   "order": 657,
   "page_count": 5,
   "abstract": [
    "Classical speaker verification metrics used to evaluate voice anonymization systems, such as the equal error rate (EER), fail to properly quantify the residual re-identification risk. This paper introduces a new evaluation framework based on two metrics, Linkability and Singling Out, derived from the legal definitions in the Article 29 Working Party&#x27;s Opinion 05/2014 on Anonymization Techniques endorsed by the European Data Protection Board (EDPB). Our framework translates these legal concepts into quantitative metrics for speech data. The proposed framework has been legally validated by the French Data Protection Authority. Experiments across various attack scenarios reveal that, while the EER remains stable, Linkability and Singling Out vary much more. This demonstrates that the residual privacy risk after anonymization is far more variable than indicated by the EER, underscoring the need for evaluation metrics that align with legal criteria."
   ],
   "p1": 3229,
   "pn": 3233,
   "doi": "10.21437/Interspeech.2025-1699",
   "url": "interspeech_2025/vauquier25_interspeech.html"
  },
  "ugan25_interspeech": {
   "authors": [
    [
     "Enes",
     "Ugan"
    ],
    [
     "Ngoc-Quan",
     "Pham"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Weight Factorization and Centralization for Continual Learning in Speech Recognition",
   "original": "1701",
   "order": 451,
   "page_count": 5,
   "abstract": [
    "Modern neural network based speech recognition models are required to continually absorb new data without re-training the whole system, especially in downstream applications using foundation models, having no access to the original training data. Continually training the models in a rehearsal-free, multilingual, and language agnostic condition, likely leads to catastrophic forgetting, when a seemingly insignificant disruption to the weights can destructively harm the quality of the models. Inspired by the ability of human brains to learn and consolidate knowledge through the waking-sleeping cycle, we propose a continual learning approach with two distinct phases: factorization and centralization, learning and merging knowledge accordingly. Our experiments on a sequence of varied code-switching datasets showed that the centralization stage can effectively prevent catastrophic forgetting by accumulating the knowledge in multiple scattering low-rank adapters."
   ],
   "p1": 2200,
   "pn": 2204,
   "doi": "10.21437/Interspeech.2025-1701",
   "url": "interspeech_2025/ugan25_interspeech.html"
  },
  "tran25b_interspeech": {
   "authors": [
    [
     "Hoan My",
     "Tran"
    ],
    [
     "Damien",
     "Lolive"
    ],
    [
     "David",
     "Guennec"
    ],
    [
     "Aghilas",
     "Sini"
    ],
    [
     "Arnaud",
     "Delhay"
    ],
    [
     "Pierre-François",
     "Marteau"
    ]
   ],
   "title": "Leveraging SSL Speech Features and Mamba for Enhanced DeepFake Detection",
   "original": "1703",
   "order": 1086,
   "page_count": 5,
   "abstract": [
    "Large-scale self-supervised learning models have proven highly effective in extracting robust features for detecting both genuine and spoofed speech. However, leveraging these features remains challenging, particularly in balancing in-domain specialization with out-of-domain generalization. In this work, we propose an effective approach for automatically selecting features using self-gated mechanism, and aggregation of speech representations from a pretrained foundation model to enhance deepfake detection. Our approach integrates a multi-kernel gated convolution module to improve feature learning and facilitate the fusion of features. Additionally, we employ Mamba to effectively capture both short and long-range discriminative patterns in speech. The proposed method achieves strong performance in audio deepfake detection, demonstrating improved generalization across diverse datasets. The source code will be made available on Github."
   ],
   "p1": 5323,
   "pn": 5327,
   "doi": "10.21437/Interspeech.2025-1703",
   "url": "interspeech_2025/tran25b_interspeech.html"
  },
  "hannan25b_interspeech": {
   "authors": [
    [
     "Abdul",
     "Hannan"
    ],
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Shah",
     "Nawaz"
    ],
    [
     "Mubashir",
     "Noman"
    ]
   ],
   "title": "An Effective Training Framework for Light-Weight Automatic Speech Recognition Models",
   "original": "1704",
   "order": 739,
   "page_count": 5,
   "abstract": [
    "Recent advancement in deep learning encouraged developing large automatic speech recognition (ASR) models that achieve promising results while ignoring computational and memory constraints. However, deploying such models on low resource devices is impractical despite of their favorable performance. Existing approaches (pruning, distillation, layer skip etc.) transform the large models into smaller ones at the cost of significant performance degradation or require prolonged training of smaller models for better performance. To address these issues, we introduce an efficacious two-step representation learning based approach capable of producing several small sized models from a single large model ensuring considerably better performance in limited number of epochs. Comprehensive experimentation on ASR benchmarks reveals the efficacy of our approach, achieving three-fold training speed-up and up to 12.54% word error rate improvement."
   ],
   "p1": 3613,
   "pn": 3617,
   "doi": "10.21437/Interspeech.2025-1704",
   "url": "interspeech_2025/hannan25b_interspeech.html"
  },
  "tan25b_interspeech": {
   "authors": [
    [
     "Tianyi",
     "Tan"
    ],
    [
     "Xinan",
     "Chen"
    ],
    [
     "Xiaohuai",
     "Le"
    ],
    [
     "Wenzhi",
     "Fan"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Chuanzeng",
     "Huang"
    ],
    [
     "Jing",
     "Lu"
    ]
   ],
   "title": "CBA-Whisper: Curriculum Learning-Based AdaLoRA Fine-Tuning on Whisper for Low-Resource Dysarthric Speech Recognition",
   "original": "1705",
   "order": 673,
   "page_count": 5,
   "abstract": [
    "Whisper is a powerful automatic speech recognition (ASR) model. However, its zero-shot performance on low-resource speech requires further improvement, especially in dysarthric speech recognition (DSR). This paper addresses the Interspeech 2025 Speech Accessibility Project Challenge (SAPC) by fine-tuning Whisper large-v2 on SAP 2024-04-30, UA-Speech, and TORGO datasets using adaptive low-rank adaptation (AdaLoRA). We incorporate improved WhisperX processing and rule-based postprocessing designed for stuttering and machine hallucination. In addition, we employ curriculum learning (CL) with adaptively optimized data filtering to progressively enhance the performance of our model. Using less than 1/5 of the official training data, our final system ranked 2nd in this challenge, with a WER of 10.51% and a SemScore of 85.50% on Test 2 Split, reducing WER by 41.02% and improving SemScore by 12.72% over the baseline (WER 17.82%, SemScore 75.85%). Code and models are publicly available."
   ],
   "p1": 3309,
   "pn": 3313,
   "doi": "10.21437/Interspeech.2025-1705",
   "url": "interspeech_2025/tan25b_interspeech.html"
  },
  "tadevosyan25_interspeech": {
   "authors": [
    [
     "Nune",
     "Tadevosyan"
    ],
    [
     "Nikolay",
     "Karpov"
    ],
    [
     "Andrei",
     "Andrusenko"
    ],
    [
     "Vitaly",
     "Lavrukhin"
    ],
    [
     "Ante",
     "Jukic"
    ]
   ],
   "title": " Unified Semi-Supervised Pipeline for Automatic Speech Recognition",
   "original": "1706",
   "order": 648,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition has been a longstanding research area, with substantial efforts dedicated to integrating semi-supervised learning due to the scarcity of labeled datasets. However, most prior work has focused on improving learning algorithms using existing datasets, without providing a complete public framework for large-scale semi-supervised training across new datasets or languages. In this work, we introduce a fully open-source semi-supervised training framework encompassing the entire pipeline: from unlabeled data collection to pseudo-labeling and model training. Our approach enables scalable dataset creation for any language using publicly available speech data under Creative Commons licenses. We also propose a novel pseudo-labeling algorithm, TopIPL, and evaluate it in both low-resource (Portuguese, Armenian) and high-resource (Spanish) settings. Notably, TopIPL achieves relative WER improvements of 18-40% for Portuguese, 5-16% for Armenian, and 2-8% for Spanish."
   ],
   "p1": 3184,
   "pn": 3188,
   "doi": "10.21437/Interspeech.2025-1706",
   "url": "interspeech_2025/tadevosyan25_interspeech.html"
  },
  "prakash25_interspeech": {
   "authors": [
    [
     "Jeena",
     "Prakash"
    ],
    [
     "Blessingh",
     "Kumar"
    ],
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Bidisha",
     "Sharma"
    ],
    [
     "Sindhuja",
     "Gopalan"
    ],
    [
     "Malolan",
     "Chetlur"
    ],
    [
     "Shankar",
     "Venkatesan"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM",
   "original": "1707",
   "order": 122,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) models rely on high-quality transcribed data for effective training. Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization. We propose a unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs), replacing voting or other arbitration logic for reconciling the ensemble outputs. We perform a comparative study of multiple architectures with and without LLMs, showing significant improvements in transcription accuracy compared to traditional methods. Furthermore, we use the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets, again showing improved performance with textual and speechLLM transcriptions compared to baselines."
   ],
   "p1": 579,
   "pn": 583,
   "doi": "10.21437/Interspeech.2025-1707",
   "url": "interspeech_2025/prakash25_interspeech.html"
  },
  "wang25q_interspeech": {
   "authors": [
    [
     "Yuxi",
     "Wang"
    ],
    [
     "Yikang",
     "Wang"
    ],
    [
     "Qishan",
     "Zhang"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "VCapAV: A Video-Caption Based Audio-Visual Deepfake Detection Dataset",
   "original": "1713",
   "order": 798,
   "page_count": 5,
   "abstract": [
    "Most existing deepfake datasets focus on speech synthesis or voice cloning, with little attention given to non-speech environmental sounds. Existing audio-focused datasets also lack video content, restricting progress in multimodal detection. To bridge gaps, we introduce VCapAV, a large-scale audio-visual dataset, designed to advance deepfake detection research involving environmental sound manipulations in multimodal scenarios. VCapAV is constructed through an innovative data generation pipeline that synthesizes realistic environmental audio using Text-to-Audio and Video-to-Audio approaches, while deepfake videos are generated through a Text-to-Video model. We establish two baseline detection tasks on this dataset: (i) audio-only deepfake detection, and (ii) visual-only deepfake detection. Experimental results show that existing detection models on the VCapAV dataset compared to standard datasets such as ASVspoof 2019 LA and AV-Deepfake1M. The dataset and baseline codes are released."
   ],
   "p1": 3908,
   "pn": 3912,
   "doi": "10.21437/Interspeech.2025-1713",
   "url": "interspeech_2025/wang25q_interspeech.html"
  },
  "vurma25_interspeech": {
   "authors": [
    [
     "Allan",
     "Vurma"
    ],
    [
     "Einar",
     "Meister"
    ],
    [
     "Lya",
     "Meister"
    ],
    [
     "Jaan",
     "Ross"
    ],
    [
     "Marju",
     "Raju"
    ],
    [
     "Veeda",
     "Kala"
    ],
    [
     "Tuuri",
     "Dede"
    ]
   ],
   "title": "The Role of Voiced Consonant Duration in Sung Vowel-Consonant and Consonant-Vowel Recognition",
   "original": "1716",
   "order": 198,
   "page_count": 5,
   "abstract": [
    "This study explores the impact of consonant duration on the intelligibility of sung consonants (/l/, /m/, /n/, and /v/) across various acoustic settings, pitch levels, and background noise conditions. Forty-two participants (13 male, 29 female; aged 16-69) completed recognition tests involving CV and VC segments containing the vowel /a/, sung and spoken by a mezzo-soprano and a baritone. Consonant durations ranged from 0 ms to 200 ms, with artificial reverberation or brown noise added to some stimuli to simulate performance environments. GLMM revealed that recognition was poorer at high pitches, in reverberant acoustics, and with accompaniment, particularly for VC segments. Extending consonant duration from 20 ms to 200 ms consistently improved recognition by up to 25 percentage points. At low pitches, recognition exceeded chance even when the stationary part of the consonant was absent; 20 ms was sufficient for 95% recognition of spoken CVs, except in the presence of noise."
   ],
   "p1": 958,
   "pn": 962,
   "doi": "10.21437/Interspeech.2025-1716",
   "url": "interspeech_2025/vurma25_interspeech.html"
  },
  "song25c_interspeech": {
   "authors": [
    [
     "Zeyan",
     "Song"
    ],
    [
     "Tianchi",
     "Sun"
    ],
    [
     "Ronghui",
     "Hu"
    ],
    [
     "Kai",
     "Chen"
    ],
    [
     "Jing",
     "Lu"
    ]
   ],
   "title": "Leveraging Self-Supervised Learning Based Speaker Diarization for MISP 2025 AVSD Challenge",
   "original": "1717",
   "order": 388,
   "page_count": 5,
   "abstract": [
    "This paper presents the submission of our team to the audio-visual speaker diarization (AVSD) track of the Multimodal Information Based Speech Processing (MISP) 2025 Challenge. The submitted system is adapted from the DiariZen pipeline, with a primary focus on optimizing it for the challenge dataset. The pipeline consists of a WavLM based local end-to-end neural diarization module followed by two different clustering methods. To further refine the results, DOVER-Lap is employed to integrate results across different input channels and clustering methods. Our final submission system achieves a diarization error rate (DER) of 8.33% on the evaluation set, representing a relative improvement of 46.3% compared to the baseline and ranking 3rd in the AVSD track of this challenge."
   ],
   "p1": 1903,
   "pn": 1907,
   "doi": "10.21437/Interspeech.2025-1717",
   "url": "interspeech_2025/song25c_interspeech.html"
  },
  "zhao25i_interspeech": {
   "authors": [
    [
     "Junhui",
     "Zhao"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Yanhui",
     "Tu"
    ],
    [
     "Feng",
     "Ma"
    ]
   ],
   "title": "TA-RIR: Topology-Aware Neural Modeling of Acoustic Propagation for Room Impulse Response Synthesis",
   "original": "1718",
   "order": 508,
   "page_count": 5,
   "abstract": [
    "Accurate estimation of room impulse responses (RIRs) is crucial for applications like augmented reality and sound field modeling. Current methods either neglect the spatial relationships between the source and receiver or rely on computationally intensive volumetric grids or panoramic images to estimate RIRs. To address these challenges, we introduce TA-RIR, a topology-aware neural network that uses spatial coordinates of sources and receivers, along with reverberant speech, to learn compact embeddings encoding room geometry and acoustics. The topology-aware encoder captures structural relationships between spatial and acoustic features, integrated through a propagation-informed decoder to synthesize RIRs. Experimental results show that TA-RIR generates high-fidelity RIRs, accurately preserving target acoustic parameters such as reverberation time, while significantly reducing computational complexity compared to methods requiring detailed 3D models or room acoustic properties."
   ],
   "p1": 2485,
   "pn": 2489,
   "doi": "10.21437/Interspeech.2025-1718",
   "url": "interspeech_2025/zhao25i_interspeech.html"
  },
  "bodur25_interspeech": {
   "authors": [
    [
     "Kübra",
     "Bodur"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Speech Reduction in French: The Relationship Between Vowel Space and Articulation Dynamics",
   "original": "1720",
   "order": 83,
   "page_count": 5,
   "abstract": [
    "Reduction is an inherent characteristic of conversations, reflecting the dynamic adaptability of language. This study examined the link between vowel space and non-lexicalized reductions (NLR) in spontaneous French speech. The hypothesis posited that speakers with smaller vowel spaces - indicating centralized, less distinct vowels - would produce more NLR, defined as temporally compressed speech zones. Results showed that smaller vowel space (pVSA) predicted greater NLR only when articulation rate was considered, highlighting an interaction between spatial and temporal speech dynamics. Articulation rate was the strongest predictor, supporting theories that link faster speech to reduced articulatory precision. Vowel Distinctiveness Index (VDI) did not significantly predict NLR, suggesting it reflects broader systemic patterns rather than local reductions. These findings emphasized the interplay of temporal dynamics and individual articulation strategies in shaping reduction in speech."
   ],
   "p1": 384,
   "pn": 388,
   "doi": "10.21437/Interspeech.2025-1720",
   "url": "interspeech_2025/bodur25_interspeech.html"
  },
  "jun25_interspeech": {
   "authors": [
    [
     "Yonghyeon",
     "Jun"
    ],
    [
     "Beomjun",
     "Woo"
    ],
    [
     "Myeonghun",
     "Jeong"
    ],
    [
     "Namsoo",
     "Kim"
    ]
   ],
   "title": "SNR-Aligned Consistent Diffusion for Adaptive Speech Enhancement",
   "original": "1721",
   "order": 831,
   "page_count": 5,
   "abstract": [
    "Generative models have shown strong performance in speech enhancement, and consistency models further improve both speed and quality. Building upon these improvements, we propose an SNR adaptation framework that dynamically aligns the diffusion timestep with the SNR of the input signal, enhancing robustness in diverse noise conditions. In our framework, the reverse process is conditioned on a diffusion timestep that is adjusted based on the estimated SNR, while the additive Gaussian noise is modulated according to the same SNR estimate. This design enables a continuous SNR-conditioning mechanism in which the diffusion timestep serves as an SNR control parameter, allowing the model to adjust its enhancement process based on the input SNR. Experimental results demonstrate that our proposed framework consistently improves perceptual quality, with even greater improvements observed under challenging SNR conditions, highlighting its effectiveness."
   ],
   "p1": 4073,
   "pn": 4077,
   "doi": "10.21437/Interspeech.2025-1721",
   "url": "interspeech_2025/jun25_interspeech.html"
  },
  "sharon25_interspeech": {
   "authors": [
    [
     "Rini",
     "Sharon"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Enhancing Syllabic Recognition via Speech-EEG Phase Analysis and Non-Activity State Modeling",
   "original": "1725",
   "order": 593,
   "page_count": 5,
   "abstract": [
    "Decoding inner speech from brain activity holds transformative promise for brain-computer interfaces and speech rehabilitation. This study examines EEG-based neural patterns across speech production, imagination, and perception. Signal-level visualizations reveal distinct, phase-specific cortical signatures that emerge within precise temporal windows in spatially localized regions. These insights motivate the introduction of a non-activity(NA) state to account for resting and transitional EEG periods - akin to pauses in natural speech. Capturing and excluding these &quot;cognitive pauses&quot; de-noises the EEG signal, improving the focus on task-relevant neural activity. Comparative evaluation across continuous and isolated speech datasets demonstrates that NA modeling improves syllabic recognition accuracy by 1.8% and 1.15%, respectively. These results underscore the role and generalizability of cognitive pauses in imagined speech-EEG decoding, enabling robust brain computer interfaces."
   ],
   "p1": 2910,
   "pn": 2914,
   "doi": "10.21437/Interspeech.2025-1725",
   "url": "interspeech_2025/sharon25_interspeech.html"
  },
  "szekely25_interspeech": {
   "authors": [
    [
     "Éva",
     "Székely"
    ],
    [
     "Péter",
     "Mihajlik"
    ],
    [
     "Máté Soma",
     "Kádár"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Voice Reconstruction through Large-Scale TTS Models: Comparing Zero-Shot and Fine-tuning Approaches to Personalise TTS in Assistive Communication",
   "original": "1726",
   "order": 558,
   "page_count": 5,
   "abstract": [
    "Personalised synthetic speech can enhance communication for Augmentative and Alternative Communication (AAC) users, but achieving high-quality, speaker-specific voices depends on various factors such as the condition causing speech loss, and availability of recorded speech. Recent advancements in large-scale zero-shot TTS models may change the data requirements, as they have the potential to adapt to a wider range of inputs. This paper explores the potential of these pretrained models in various data availability scenarios, from extensive spontaneous speech to minimal or no unaffected speech. We evaluate a state-of-the-art TTS system on a case study involving a stroke survivor with dysarthria, leveraging both typical and atypical speech data. Additionally, we introduce a novel interactive approach using dysarthric speech as an audio prompt to enable user-guided prosody adaptation."
   ],
   "p1": 2735,
   "pn": 2739,
   "doi": "10.21437/Interspeech.2025-1726",
   "url": "interspeech_2025/szekely25_interspeech.html"
  },
  "teplansky25_interspeech": {
   "authors": [
    [
     "Kristin",
     "Teplansky"
    ],
    [
     "Emily",
     "Rangel"
    ],
    [
     "Mimi",
     "LaValley"
    ],
    [
     "Jinuk",
     "Kwon"
    ],
    [
     "Beiming",
     "Cao"
    ],
    [
     "Jun",
     "Wang"
    ]
   ],
   "title": "Articulatory Vowel Distinctiveness in Spanish",
   "original": "1727",
   "order": 1140,
   "page_count": 5,
   "abstract": [
    "This study investigated Spanish articulatory patterns in bilingual speakers through a comparative analysis with English, focusing on isolated vowels while controlling for vocal tract differences. A recently developed, wearable electromagnetic articulography device was used to track tongue and lip motion. The y and z-coordinates were used to classify 5 Spanish vowels. The vowel classification results showed medium to high accuracy. While small positional differences indicated distinct articulatory configurations, the overall size of the vowel articulatory distinctiveness space was comparable between the two languages. The similarly high classification accuracy and similar vowel distribution patterns suggest the feasibility of training Silent Speech Interfaces in both Spanish and English for bilingual speakers. To our knowledge, this is the first study on vowel kinematic distinctiveness in Spanish."
   ],
   "p1": 5593,
   "pn": 5597,
   "doi": "10.21437/Interspeech.2025-1727",
   "url": "interspeech_2025/teplansky25_interspeech.html"
  },
  "deoliveira25_interspeech": {
   "authors": [
    [
     "Danilo",
     "de Oliveira"
    ],
    [
     "Julius",
     "Richter"
    ],
    [
     "Jean-Marie",
     "Lemercier"
    ],
    [
     "Simon",
     "Welker"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Non-intrusive Speech Quality Assessment with Diffusion Models Trained on Clean Speech",
   "original": "1728",
   "order": 477,
   "page_count": 5,
   "abstract": [
    "Diffusion models have found great success in generating high quality, natural samples of speech, but their potential for density estimation for speech has so far remained largely unexplored. In this work, we leverage an unconditional diffusion model trained only on clean speech for the assessment of speech quality. We show that the quality of a speech utterance can be assessed by estimating the likelihood of a corresponding sample in the terminating Gaussian distribution, obtained via a deterministic noising process. The resulting method is purely unsupervised, trained only on clean speech, and therefore does not rely on annotations. Our diffusion-based approach leverages clean speech priors to assess quality based on how the input relates to the learned distribution of clean data. Our proposed log-likelihoods show promising results, correlating well with intrusive speech quality metrics and showing the best correlation with human scores in a listening experiment."
   ],
   "p1": 2330,
   "pn": 2334,
   "doi": "10.21437/Interspeech.2025-1728",
   "url": "interspeech_2025/deoliveira25_interspeech.html"
  },
  "mcguire25_interspeech": {
   "authors": [
    [
     "Paul",
     "McGuire"
    ],
    [
     "Kye",
     "Shibata"
    ],
    [
     "Thanh Viet",
     "Cao"
    ],
    [
     "Feng-fan",
     "Hsieh"
    ],
    [
     "Yueh-chin",
     "Chang"
    ]
   ],
   "title": "Supralaryngeal Kinematics of Implosives in Central Vietnamese: An EMA Study",
   "original": "1733",
   "order": 709,
   "page_count": 5,
   "abstract": [
    "This study, using electromagnetic articulography (EMA), examines the supralaryngeal kinematics of bilabial voiced implosives in Central Vietnamese, focusing on lip aperture trajectories and gestural timing during stop formation and release. Results indicate that implosives exhibited a higher peak velocity away from closure, whereas their voiceless plosive counterparts were found to have a longer gestural plateau. To investigate whether these differences are inherently tied to implosivity or simply a consequence of voicing, we analysed voiced and voiceless bilabial plosives in Taiwanese Southern Min. The Southern Min data confirmed that voiced plosives did not exhibit the same rapid lip aperture movement, supporting the conclusion that, in the Central Vietnamese data, higher peak velocity away from the stop closure is a defining supralaryngeal feature of implosives."
   ],
   "p1": 3484,
   "pn": 3488,
   "doi": "10.21437/Interspeech.2025-1733",
   "url": "interspeech_2025/mcguire25_interspeech.html"
  },
  "wang25r_interspeech": {
   "authors": [
    [
     "Yawei",
     "Wang"
    ],
    [
     "Qiaoling",
     "Zhang"
    ],
    [
     "Yi",
     "Zhang"
    ],
    [
     "Junyao",
     "Hu"
    ]
   ],
   "title": "Anomalous Sound Detection Based Feature Fusion and Dual-path Non-linear Independent Components Estimation",
   "original": "1734",
   "order": 534,
   "page_count": 5,
   "abstract": [
    "Anomalous sound detection (ASD) that relies on annotated information during training has achieved excellent performances in benchmarks such as DCASE2020 Task 2. However, annotated data may not always be available in many scenarios. The Non-linear Independent Components Estimation (NICE) has been a promising technique for anomaly detection, as it does not rely on annotated information and is capable of exact likelihood estimation. In this paper, based on the classic NICE, a dual-path NICE model (dpNICE) is proposed, which allows for two input features feeding to two pathways and provides interactive channels for interaction and fusion between the two pathways. Then, a multiple feature fusion and dpNICE-based approach is developed, which leverages information of multiple audio features and learns to detect anomalies without annotated data. Experimental results show that the AUC and pAUC performances of our method are better than other competing annotation-free methods."
   ],
   "p1": 2615,
   "pn": 2619,
   "doi": "10.21437/Interspeech.2025-1734",
   "url": "interspeech_2025/wang25r_interspeech.html"
  },
  "wang25s_interspeech": {
   "authors": [
    [
     "Ziqian",
     "Wang"
    ],
    [
     "Zikai",
     "Liu"
    ],
    [
     "Xinfa",
     "Zhu"
    ],
    [
     "Yike",
     "Zhu"
    ],
    [
     "Mingshuai",
     "Liu"
    ],
    [
     "Jun",
     "Chen"
    ],
    [
     "Longshuai",
     "Xiao"
    ],
    [
     "Chao",
     "Weng"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching",
   "original": "1745",
   "order": 988,
   "page_count": 5,
   "abstract": [
    "Generative models have excelled in audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a condition flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speech’s temporal spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outperforms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available."
   ],
   "p1": 4858,
   "pn": 4862,
   "doi": "10.21437/Interspeech.2025-1745",
   "url": "interspeech_2025/wang25s_interspeech.html"
  },
  "kaneko25_interspeech": {
   "authors": [
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Yuto",
     "Kondo"
    ]
   ],
   "title": "FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation",
   "original": "1747",
   "order": 936,
   "page_count": 5,
   "abstract": [
    "A diffusion-based voice conversion (VC) model (e.g., VoiceGrad) can achieve high speech quality and speaker similarity; however, its conversion process is slow owing to iterative sampling. FastVoiceGrad overcomes this limitation by distilling VoiceGrad into a one-step diffusion model. However, it still requires a computationally intensive content encoder to disentangle the speaker&#x27;s identity and content, which slows conversion. Therefore, we propose FasterVoiceGrad, a novel one-step diffusion-based VC model obtained by simultaneously distilling a diffusion model and content encoder using adversarial diffusion conversion distillation (ADCD), where distillation is performed in the conversion process while leveraging adversarial and score distillation training. Experimental evaluations of one-shot VC demonstrated that FasterVoiceGrad achieves competitive VC performance compared to FastVoiceGrad, with 6.6-6.9 and 1.8 times faster speed on a GPU and CPU, respectively."
   ],
   "p1": 4598,
   "pn": 4602,
   "doi": "10.21437/Interspeech.2025-1747",
   "url": "interspeech_2025/kaneko25_interspeech.html"
  },
  "turnbull25_interspeech": {
   "authors": [
    [
     "Rory",
     "Turnbull"
    ],
    [
     "Elisa",
     "Kiefer"
    ],
    [
     "Sharon",
     "Peperkamp"
    ]
   ],
   "title": "Does English fish sound like French fiche? Perceptual similarity judgments versus acoustic similarity",
   "original": "1748",
   "order": 905,
   "page_count": 5,
   "abstract": [
    "We examine the relationship between phonological word similarity judgments from listeners and acoustic measures of similarity. Native speakers of English or French with varying degrees of proficiency in the other language listened to pairs of words, one in French and the other in English. The words were highly phonologically similar but did not overlap in meaning. Participants judged how similar the items sounded. Each item pair was acoustically analyzed with six different acoustic distance metrics. The results demonstrate a weak correlation between the similarity judgments and five of the six acoustic distance measures. Within the experiment, we found an order of presentation effect: when the first word of a pair was in a participant&#x27;s L1, the pair was rated more similar than if the first word was in the L2. This effect diminished in magnitude with increasing L2 proficiency. We discuss the implications of our results in light of theories of speech representation and processing."
   ],
   "p1": 4443,
   "pn": 4447,
   "doi": "10.21437/Interspeech.2025-1748",
   "url": "interspeech_2025/turnbull25_interspeech.html"
  },
  "zhang25p_interspeech": {
   "authors": [
    [
     "Hongyu",
     "Zhang"
    ],
    [
     "Ming",
     "Cheng"
    ],
    [
     "Jing",
     "Feng"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "Selective Channel Attention based Target Speaker Voice Activity Detection for Speaker Diarization under AD-HOC Microphone Array Settings",
   "original": "1749",
   "order": 1066,
   "page_count": 5,
   "abstract": [
    "Speaker diarization benefits from multi-channel microphone arrays, yet current systems struggle with diverse configurations. We address this by simulating a dataset with various microphone topologies and proposing Selective Channel Attention-based Target Speaker Voice Activity Detection (SCA-TSVAD). We utilize cross-channel self-attention with masking mechanisms to enable selective attention on specific channels, allowing for the effective processing of audio data with variable multi-channel configurations. SCA-TSVAD is built upon the foundation of single-channel TSVAD. It performs superior on our simulated dataset, showcasing its robustness across diverse array configurations. To further validate the effectiveness of a real dataset, we evaluate SCA-TSVAD on the real-world Ali-Meeting database, where it successfully handles multi-channel audio inputs even when some channels were unavailable or malfunctioning, proving its practical applicability."
   ],
   "p1": 5228,
   "pn": 5232,
   "doi": "10.21437/Interspeech.2025-1749",
   "url": "interspeech_2025/zhang25p_interspeech.html"
  },
  "guillaume25_interspeech": {
   "authors": [
    [
     "Mélen",
     "Guillaume"
    ],
    [
     "Anahita",
     "Basirat"
    ],
    [
     "Julien",
     "Diard"
    ]
   ],
   "title": "Theoretical proposal for a unified Bayesian model of adaptation in non-interactive and interactive speech production",
   "original": "1751",
   "order": 212,
   "page_count": 5,
   "abstract": [
    "In the scientific study of speech production, as well as in speech and language therapy, a variety of experimental paradigms and tasks resulting in plasticity and adaptation of the system are used. However, existing computational models accounting for adaptation appear segregated: feedback effects during non-interactive, altered speech are mostly accounted for by speech production models, whereas accommodation effects during interactive speech are mostly accounted for by speech perception models. In this paper, we consider COSMO, a previous unified Bayesian framework of speech perception and speech production, as the basis for a theoretical proposal and Bayesian model of adaptation effects. We show how these effects in both non-interactive and interactive speech production experiments could be described in this framework, thanks for its ability to separate knowledge and representations from their involvement during speech tasks."
   ],
   "p1": 1028,
   "pn": 1032,
   "doi": "10.21437/Interspeech.2025-1751",
   "url": "interspeech_2025/guillaume25_interspeech.html"
  },
  "liu25o_interspeech": {
   "authors": [
    [
     "Chang",
     "Liu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Yu",
     "Gu"
    ]
   ],
   "title": "LIST: Language-Independent Speech Token for Multilingual Speech Synthesis with Language Models",
   "original": "1752",
   "order": 331,
   "page_count": 5,
   "abstract": [
    "Recently, language model (LM)-based speech synthesis models have shown remarkable naturalness and powerful zero-shot capabilities. In this paradigm, discrete speech tokens play a critical role. Prior work has proposed using automatic speech recognition (ASR) tasks to enhance the semantic information and alignment with text in tokens. However, the commonly used byte-pair encoding (BPE) tokenizer in ASR task leads to significant differences in the text token sets of different languages, making it difficult to exploit language-shared information. This paper proposes to use the International Phonetic Alphabet (IPA) as the training target for ASR to learn language-independent speech tokens. In addition, we propose to use a timbre converter for speaker disentanglement in the speech synthesis model. Our proposed approach effectively improves the speaker similarity and expressiveness in both multilingual and cross-lingual zero-shot speech synthesis."
   ],
   "p1": 1623,
   "pn": 1627,
   "doi": "10.21437/Interspeech.2025-1752",
   "url": "interspeech_2025/liu25o_interspeech.html"
  },
  "zezario25_interspeech": {
   "authors": [
    [
     "Ryandhimas E.",
     "Zezario"
    ],
    [
     "Sabato M.",
     "Siniscalchi"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "Feature Importance across Domains for Improving Non-Intrusive Speech Intelligibility Prediction in Hearing Aids",
   "original": "1756",
   "order": 1116,
   "page_count": 5,
   "abstract": [
    "Given the critical role of non-intrusive speech intelligibility assessment in hearing aids (HA), this paper enhances its performance by introducing Feature Importance across Domains (FiDo). We estimate feature importance on spectral and time-domain acoustic features as well as latent representations of Whisper. Importance weights are calculated per frame, and based on these weights, features are projected into new spaces, allowing the model to focus on important areas early. Next, feature concatenation is performed to combine the features before the assessment module processes them. Experimental results show that when FiDo is incorporated into the improved multi-branched speech intelligibility model MBI-Net+, RMSE can be reduced by 7.62% (from 26.10 to 24.11). MBI-Net+ with FiDo also achieves a relative RMSE reduction of 3.98% compared to the best system in the 2023 Clarity Prediction Challenge. These results validate FiDo&#x27;s effectiveness in enhancing neural speech assessment in HA."
   ],
   "p1": 5473,
   "pn": 5477,
   "doi": "10.21437/Interspeech.2025-1756",
   "url": "interspeech_2025/zezario25_interspeech.html"
  },
  "koutsianos25_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Koutsianos"
    ],
    [
     "Stavros",
     "Zacharopoulos"
    ],
    [
     "Yannis",
     "Panagakis"
    ],
    [
     "Themos",
     "Stafylakis"
    ]
   ],
   "title": "Synthetic Speech Source Tracing using Metric Learning",
   "original": "1757",
   "order": 318,
   "page_count": 5,
   "abstract": [
    "This paper addresses source tracing in synthetic speech-identifying generative systems behind manipulated audio via speaker recognition-inspired pipelines. While prior work focuses on spoofing detection, source tracing lacks robust solutions. We evaluate two approaches: classification-based and metric learning. We tested our methods on the MLAADv5 benchmark using ResNet and self-supervised learning (SSL) backbones. The results show that ResNet achieves competitive performance with the metric learning approach, matching and even exceeding SSL-based systems. Our work demonstrates ResNet&#x27;s viability for source tracing while underscoring the need to optimize SSL representations for this task. Our work bridges speaker recognition methodologies with audio forensic challenges, offering new directions for combating synthetic media manipulation."
   ],
   "p1": 1558,
   "pn": 1562,
   "doi": "10.21437/Interspeech.2025-1757",
   "url": "interspeech_2025/koutsianos25_interspeech.html"
  },
  "giraldo25_interspeech": {
   "authors": [
    [
     "Jose",
     "Giraldo"
    ],
    [
     "Alex",
     "Peiró-Lilja"
    ],
    [
     "Carme",
     "Armentano-Oller"
    ],
    [
     "Rodolfo",
     "Zevallos"
    ],
    [
     "Cristina",
     "España-Bonet"
    ]
   ],
   "title": "Evaluating Speech Enhancement Performance Across Demographics and Language",
   "original": "1760",
   "order": 277,
   "page_count": 5,
   "abstract": [
    "Speech enhancement models have traditionally relied on VoiceBank-DEMAND for training and evaluation. However, this dataset presents significant limitations due to its limited diversity and simulated noise conditions. As an alternative, we propose and demonstrate the usefulness of evaluating the generalization capabilities of recent speech enhancement models using CommonPhone, a multilingual and crowdsourced dataset. Since CommonPhone is derived from CommonVoice, it allows to analyze enhancement performance based on demographic variables such as age and gender. Our experiments reveal significant performance variations across these variables. We also introduce a new benchmark dataset designed to challenge enhancement models with difficult and diverse speech samples, facilitating future research in universal speech enhancement."
   ],
   "p1": 1353,
   "pn": 1357,
   "doi": "10.21437/Interspeech.2025-1760",
   "url": "interspeech_2025/giraldo25_interspeech.html"
  },
  "kaneko25b_interspeech": {
   "authors": [
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Yuto",
     "Kondo"
    ]
   ],
   "title": "Vocoder-Projected Feature Discriminator",
   "original": "1763",
   "order": 996,
   "page_count": 5,
   "abstract": [
    "In text-to-speech (TTS) and voice conversion (VC), acoustic features, such as mel spectrograms, are typically used as synthesis or conversion targets owing to their compactness and ease of learning. However, because the ultimate goal is to generate high-quality waveforms, employing a vocoder to convert these features into waveforms and applying adversarial training in the time domain is reasonable. Nevertheless, upsampling the waveform introduces significant time and memory overheads. To address this issue, we propose a vocoder-projected feature discriminator (VPFD), which uses vocoder features for adversarial training. Experiments on diffusion-based VC distillation demonstrated that a pretrained and frozen vocoder feature extractor with a single upsampling step is necessary and sufficient to achieve a VC performance comparable to that of waveform discriminators while reducing the training time and memory consumption by 9.6 and 11.4 times, respectively."
   ],
   "p1": 4898,
   "pn": 4902,
   "doi": "10.21437/Interspeech.2025-1763",
   "url": "interspeech_2025/kaneko25b_interspeech.html"
  },
  "wang25t_interspeech": {
   "authors": [
    [
     "Yuzhu",
     "Wang"
    ],
    [
     "Archontis",
     "Politis"
    ],
    [
     "Konstantinos",
     "Drossos"
    ],
    [
     "Tuomas",
     "Virtanen"
    ]
   ],
   "title": "Attractor-Based Speech Separation of Multiple Utterances  by Unknown Number of Speakers",
   "original": "1764",
   "order": 297,
   "page_count": 5,
   "abstract": [
    "This paper addresses the problem of single-channel speech separation, where the number of speakers is unknown, and each speaker may speak multiple utterances. We propose a speech separation model that simultaneously performs separation, dynamically estimates the number of speakers, and detects individual speaker activities by integrating an attractor module. The proposed system outperforms existing methods by introducing an attractor-based architecture that effectively combines local and global temporal modeling for multi-utterance scenarios. To evaluate the method in reverberant and noisy conditions, a multi-speaker multi-utterance dataset was synthesized by combining Librispeech speech signals with WHAM! noise signals. The results demonstrate that the proposed system accurately estimates the number of sources. The system effectively detects source activities and separates the corresponding utterances into correct outputs in both known and unknown source count scenarios."
   ],
   "p1": 1453,
   "pn": 1457,
   "doi": "10.21437/Interspeech.2025-1764",
   "url": "interspeech_2025/wang25t_interspeech.html"
  },
  "chen25m_interspeech": {
   "authors": [
    [
     "Xueyuan",
     "Chen"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Wenxuan",
     "Wu"
    ],
    [
     "Minglin",
     "Wu"
    ],
    [
     "Jing",
     "Xu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "DiffDSR: Dysarthric Speech Reconstruction Using Latent Diffusion Model",
   "original": "1770",
   "order": 430,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech reconstruction (DSR) aims to convert dysarthric speech into comprehensible speech while maintaining the speaker&#x27;s identity. Despite significant advancements, existing methods often struggle with low speech intelligibility and poor speaker similarity. In this study, we introduce a novel diffusion-based DSR system that leverages a latent diffusion model to enhance the quality of speech reconstruction. Our model comprises: (i) a speech content encoder for phoneme embedding restoration via pre-trained self-supervised learning (SSL) speech foundation models; (ii) a speaker identity encoder for speaker-aware identity preservation by in-context learning mechanism; (iii) a diffusion-based speech generator to reconstruct the speech based on the restored phoneme embedding and preserved speaker identity. Through evaluations on the widely-used UASpeech corpus, our proposed model shows notable enhancements in speech intelligibility and speaker similarity."
   ],
   "p1": 2113,
   "pn": 2117,
   "doi": "10.21437/Interspeech.2025-1770",
   "url": "interspeech_2025/chen25m_interspeech.html"
  },
  "qian25b_interspeech": {
   "authors": [
    [
     "Livia",
     "Qian"
    ],
    [
     "Carol",
     "Figueroa"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "Representation of Perceived Prosodic Similarity of Conversational Feedback",
   "original": "1771",
   "order": 81,
   "page_count": 5,
   "abstract": [
    "Vocal feedback (e.g., &#x27;mhm&#x27;, &#x27;yeah&#x27;, &#x27;okay&#x27;) is an important component of spoken dialogue and is crucial to ensuring common ground in conversational systems. The exact meaning of such feedback is conveyed through both lexical and prosodic form. In this work, we investigate the perceived prosodic similarity of vocal feedback with the same lexical form, and to what extent existing speech representations reflect such similarities. A triadic comparison task with recruited participants is used to measure perceived similarity of feedback responses taken from two different datasets. We find that spectral and self-supervised speech representations encode prosody better than extracted pitch features, especially in the case of feedback from the same speaker. We also find that it is possible to further condense and align the representations to human perception through contrastive learning."
   ],
   "p1": 374,
   "pn": 378,
   "doi": "10.21437/Interspeech.2025-1771",
   "url": "interspeech_2025/qian25b_interspeech.html"
  },
  "park25e_interspeech": {
   "authors": [
    [
     "Kyeongman",
     "Park"
    ],
    [
     "Seongho",
     "Joo"
    ],
    [
     "Kyomin",
     "Jung"
    ]
   ],
   "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of  Multiple Speakers",
   "original": "1773",
   "order": 1030,
   "page_count": 5,
   "abstract": [
    "We introduce MultiActor-Audiobook, a zero-shot approach for generating audiobooks that automatically produces consistent, expressive, and speaker-appropriate prosody, including intonation and emotion. Previous audiobook systems have several limitations: they require users to manually configure the speaker&#x27;s prosody, read each sentence with a monotonic tone compared to voice actors, or rely on costly training. However, our MultiActor-Audiobook addresses these issues by introducing two novel processes: (1) MSP (Multimodal Speaker Persona Generation) and (2) LSI (LLM-based Script Instruction Generation). With these two processes, MultiActor-Audiobook can generate more emotionally expressive audiobooks with a consistent speaker prosody without additional training. We compare our system with commercial products, through human and MLLM evaluations, achieving competitive results. Furthermore, we demonstrate the effectiveness of MSP and LSI through ablation studies."
   ],
   "p1": 5048,
   "pn": 5052,
   "doi": "10.21437/Interspeech.2025-1773",
   "url": "interspeech_2025/park25e_interspeech.html"
  },
  "dewhurst25_interspeech": {
   "authors": [
    [
     "Maya",
     "Dewhurst"
    ],
    [
     "Jack",
     "Collins"
    ],
    [
     "Justin J. H.",
     "Lo"
    ],
    [
     "Roy",
     "Alderton"
    ],
    [
     "Sam",
     "Kirkham"
    ]
   ],
   "title": "Nosey: Open-Source Hardware for Acoustic Nasalance",
   "original": "1775",
   "order": 464,
   "page_count": 5,
   "abstract": [
    "We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost, customizable, 3D-printed system for recording acoustic nasalance data that we have made available as open-source hardware (http://github.com/phoneticslab/nosey). We first outline the motivations and design principles behind our hardware nasalance system, and then present a comparison between Nosey and a commercial nasalance device. Nosey shows consistently higher nasalance scores than the commercial device, but the magnitude of contrast between phonological environments is comparable between systems. We also review ways of customizing the hardware to facilitate testing, such as comparison of microphones and different construction materials. We conclude that Nosey is a flexible and cost-effective alternative to commercial nasometry devices and propose some methodological considerations for its use in data collection."
   ],
   "p1": 2265,
   "pn": 2269,
   "doi": "10.21437/Interspeech.2025-1775",
   "url": "interspeech_2025/dewhurst25_interspeech.html"
  },
  "liang25e_interspeech": {
   "authors": [
    [
     "Liming",
     "Liang"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Xianwei",
     "Zhuang"
    ],
    [
     "Yuxin",
     "Xie"
    ],
    [
     "Luo",
     "Chen"
    ],
    [
     "Yuehan",
     "Jin"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "SpeechSEC: A Unified Multi-Task Framework for Speech Synthesis, Editing, and Continuation",
   "original": "1776",
   "order": 704,
   "page_count": 5,
   "abstract": [
    "Recent advancements in non-autoregressive single-task speech synthesis have garnered significant attention. However,traditional single-task speech synthesis methods focus primarily on mapping semantic tokens to acoustic tokens, which overlooking the internal relationships within acoustic features. Addressing this gap, we propose SpeechSEC, a unified multi-task framework designed for Speech Synthesis, Editing, and Continuation tasks by dynamically adjusting input conditions. SpeechSEC not only surpasses previous state-of-the-art method in audio quality (4.20 vs 4.00), and voice preservation (0.72 vs 0.58) for synthesis task by acquiring shared knowledge, but also efficiently executes editing and continuation tasks with good performance via non-autoregressive techniques. Additionally, SpeechSEC exhibits a strong adaptability to current speech discretization methods, like Hubert, Descript-Audio-Codec and SpeechTokenizer, which showcases robustness of our approach. Audio samples are available."
   ],
   "p1": 3464,
   "pn": 3468,
   "doi": "10.21437/Interspeech.2025-1776",
   "url": "interspeech_2025/liang25e_interspeech.html"
  },
  "li25v_interspeech": {
   "authors": [
    [
     "Zhaoqing",
     "Li"
    ],
    [
     "Haoning",
     "Xu"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Xunying",
     "Liu"
    ]
   ],
   "title": "Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models ",
   "original": "1777",
   "order": 403,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel memory-efficient model compression approach for Conformer ASR and speech foundation systems. Our approach features a unique &quot;small-to-large&quot; design. A compact &quot;seed&quot; model containing a few Conformer or Transformer blocks is trained and unfolded many times to emulate the performance of larger uncompressed models with different logical depths. The seed model and many unfolded paths are jointly trained within a single unfolding cycle. The KL-divergence between the largest unfolded and smallest seed models is used in a self-distillation process to minimize their performance disparity. Experimental results show that our foldable model produces ASR performance comparable to individually constructed Conformer and wav2vec2/HuBERT speech foundation models under various depth configurations, while requiring only minimal memory and storage. Conformer and wav2vec2 models with a reduction of 35% and 30% parameters are obtained without loss of performance, respectively."
   ],
   "p1": 1978,
   "pn": 1982,
   "doi": "10.21437/Interspeech.2025-1777",
   "url": "interspeech_2025/li25v_interspeech.html"
  },
  "sun25i_interspeech": {
   "authors": [
    [
     "Zhaokai",
     "Sun"
    ],
    [
     "Li",
     "Zhang"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Pan",
     "Zhou"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
   "original": "1778",
   "order": 337,
   "page_count": 5,
   "abstract": [
    "Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76% on the AMI test set, demonstrating its robustness and effectiveness in OSD."
   ],
   "p1": 1653,
   "pn": 1657,
   "doi": "10.21437/Interspeech.2025-1778",
   "url": "interspeech_2025/sun25i_interspeech.html"
  },
  "ren25b_interspeech": {
   "authors": [
    [
     "Pengyu",
     "Ren"
    ],
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Kaidi",
     "Wang"
    ],
    [
     "Peijie",
     "Chen"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and Speaker Feature Optimization",
   "original": "1779",
   "order": 284,
   "page_count": 5,
   "abstract": [
    "In recent years, diffusion-based generative models have demonstrated remarkable performance in speech conversion, including Denoising Diffusion Probabilistic Models (DDPM) and others. However, the advantages of these models come at the cost of requiring a large number of sampling steps. This limitation hinders their practical application in real-world scenarios. In this paper, we introduce ReFlow-VC, a novel high-fidelity speech conversion method based on rectified flow. Specifically, ReFlow-VC is an Ordinary Differential Equation (ODE) model that transforms a Gaussian distribution to the true Mel-spectrogram distribution along the most direct path. Furthermore, we propose a modeling approach that optimizes speaker features by utilizing both content and pitch information, allowing speaker features to reflect the properties of the current speech more accurately. Experimental results show that ReFlow-VC performs exceptionally well in small datasets and zero-shot scenarios."
   ],
   "p1": 1388,
   "pn": 1392,
   "doi": "10.21437/Interspeech.2025-1779",
   "url": "interspeech_2025/ren25b_interspeech.html"
  },
  "lin25g_interspeech": {
   "authors": [
    [
     "Zhennan",
     "Lin"
    ],
    [
     "Kaixun",
     "Huang"
    ],
    [
     "Wei",
     "Ren"
    ],
    [
     "Linju",
     "Yang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction and Activation",
   "original": "1785",
   "order": 646,
   "page_count": 5,
   "abstract": [
    "Deep biasing improves automatic speech recognition (ASR) performance by incorporating contextual phrases. However, most existing methods enhance subwords in a contextual phrase as independent units, potentially compromising contextual phrase integrity, leading to accuracy reduction. In this paper, we propose an encoder-based phrase-level contextualized ASR method that leverages dynamic vocabulary prediction and activation. We introduce architectural optimizations and integrate a bias loss to extend phrase-level predictions based on frame-level outputs. We also introduce a confidence-activated decoding method that ensures the complete output of contextual phrases while suppressing incorrect bias. Experiments on Librispeech and Wenetspeech datasets demonstrate that our approach achieves relative WER reductions of 28.31% and 23.49% compared to baseline, with the WER on contextual phrases decreasing relatively by 72.04% and 75.69%."
   ],
   "p1": 3174,
   "pn": 3178,
   "doi": "10.21437/Interspeech.2025-1785",
   "url": "interspeech_2025/lin25g_interspeech.html"
  },
  "alcalapadilla25_interspeech": {
   "authors": [
    [
     "Daniel-José",
     "Alcala Padilla"
    ],
    [
     "Nils L.",
     "Westhausen"
    ],
    [
     "Swati",
     "Vivekananthan"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "Location-Aware Target Speaker Extraction for Hearing Aids",
   "original": "1787",
   "order": 606,
   "page_count": 5,
   "abstract": [
    "Target speaker extraction (TSE) using deep learning offers potential benefits for hearing-impaired listeners. However, their implementation in hearing aids requires low-latency, low-complexity algorithms capable of real-time operation. Existing models that comply with these requirements use multi-channel input from binaural hearing aids to improve speech intelligibility but are limited to extracting speech from a single fixed direction. In this work, we utilize the direction of arrival (DOA) of the target speaker to extract speech at arbitrary angles in complex acoustic environments based on a deep-learning model. We introduce a novel DOA encoding method based on complex exponentials which is compared to one-hot (oh) encoding. We explore three low-complexity methods for integrating DOA information into the model. The evaluation using objective measures demonstrates that our extended model outperforms the baseline system with our novel encoding method achieving superior performance in 16 out of 21 cases."
   ],
   "p1": 2975,
   "pn": 2979,
   "doi": "10.21437/Interspeech.2025-1787",
   "url": "interspeech_2025/alcalapadilla25_interspeech.html"
  },
  "maji25_interspeech": {
   "authors": [
    [
     "Bubai",
     "Maji"
    ],
    [
     "Monorama",
     "Swain"
    ],
    [
     "Shazia",
     "Nasreen"
    ],
    [
     "Debabrata",
     "Majumdar"
    ],
    [
     "Rajlakshmi",
     "Guha"
    ],
    [
     "Aurobinda",
     "Routray"
    ],
    [
     "Anders",
     "Søgaard"
    ]
   ],
   "title": "A Study on The Impact of Foundation Models on Automatic Depression Detection from Speech Signals",
   "original": "1789",
   "order": 1072,
   "page_count": 5,
   "abstract": [
    "An automatic depression detection (ADD) system using spoken language offers the opportunity to develop practical, low-cost tools to detect symptoms early. However, limited data availability, privacy concerns, and transcription efforts pose significant challenges. Recent advancements in foundational models, capable of understanding and processing multimodal inputs, present opportunities for enhancing ADD systems. This study explores various speech foundation models to investigate their impact on ADD. We leverage Whisper and MMS for automatic transcription and integrate speech and text embeddings into a language model optimized with low-rank adaptation (LoRA). In addition, we examine the effects of fine-tuning strategies and prompt formats on model performance. We used English and Bengali datasets to demonstrate the potential of our method in ADD, even with moderate-quality transcriptions. The best speech and language foundation models outperform baseline models on both datasets."
   ],
   "p1": 5258,
   "pn": 5262,
   "doi": "10.21437/Interspeech.2025-1789",
   "url": "interspeech_2025/maji25_interspeech.html"
  },
  "ma25b_interspeech": {
   "authors": [
    [
     "Rao",
     "Ma"
    ],
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Siyuan",
     "Tang"
    ],
    [
     "Stefano",
     "Bannò"
    ],
    [
     "Kate M.",
     "Knill"
    ],
    [
     "Mark J.F.",
     "Gales"
    ]
   ],
   "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models",
   "original": "1793",
   "order": 1036,
   "page_count": 5,
   "abstract": [
    "The growing population of L2 English speakers has increased the demand for developing automatic graders for spoken language assessment (SLA). Historically, statistical models, text encoders, and self-supervised speech models have been utilised for this task. However, cascaded systems suffer from the loss of information, while E2E graders also have limitations. With the recent advancements of multi-modal large language models (LLMs), we aim to explore their potential as L2 oral proficiency graders and overcome these issues. In this work, we compare various training strategies using regression and classification targets. Our results show that speech LLMs outperform all previous competitive baselines, achieving superior performance on two datasets. Furthermore, the trained grader demonstrates strong generalisation capabilities in the cross-part or cross-task evaluation, facilitated by the audio understanding knowledge acquired during LLM pre-training."
   ],
   "p1": 5078,
   "pn": 5082,
   "doi": "10.21437/Interspeech.2025-1793",
   "url": "interspeech_2025/ma25b_interspeech.html"
  },
  "alumae25_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Artem",
     "Fedorchenko"
    ]
   ],
   "title": "TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge",
   "original": "1797",
   "order": 427,
   "page_count": 5,
   "abstract": [
    "This paper describes the language identification and multilingual speech recognition system developed at Tallinn University of Technology for the Interspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification system is used, consisting of a pretrained language embedding model and a lightweight speech recognition model with a shared encoder across languages and language-specific bigram language models. For speech recognition, three models are used, where only a single model is applied for each language, depending on the training data availability and performance on held-out data. The model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with custom language adapters and MMS-zeroshot. The system obtained the top overall score in the challenge."
   ],
   "p1": 2098,
   "pn": 2102,
   "doi": "10.21437/Interspeech.2025-1797",
   "url": "interspeech_2025/alumae25_interspeech.html"
  },
  "li25w_interspeech": {
   "authors": [
    [
     "Lishan",
     "Li"
    ],
    [
     "Yaolin",
     "Zhou"
    ],
    [
     "Xiaoying",
     "Xu"
    ]
   ],
   "title": "Lexical competition in the process of Cantonese tone merging: Diverse Impact Mechanisms Across Different Individuals and Tone Pairs",
   "original": "1798",
   "order": 148,
   "page_count": 5,
   "abstract": [
    "Lexical competition has been shown to exert an inhibitory influence on phonemic mergers in phonetic evolution. This study investigates the merging process of Cantonese tone pairs, examining the lexical competition effect from an individual perspective. Results reveal that lexical competition influences tone production even in speakers without clear mergers, helping maintain tonal contrasts while subtly altering pitch distributions, which may signal future change. For participants merging only one tone pair, the effect of lexical competition varies: it consistently inhibits merging for the T4 (21)-T6 (22) pair, but for the T3 (33)-T6 (22) pair, its influence manifests in three distinct patterns-no effect, inhibition, or promotion of merging. Conversely, in participants merging all three tones, lexical competition shows minimal impact. This research elucidates the diverse mechanisms through which lexical competition shapes tone-merging processes."
   ],
   "p1": 709,
   "pn": 713,
   "doi": "10.21437/Interspeech.2025-1798",
   "url": "interspeech_2025/li25w_interspeech.html"
  },
  "park25f_interspeech": {
   "authors": [
    [
     "Chanho",
     "Park"
    ],
    [
     "Oscar",
     "Saz"
    ]
   ],
   "title": "Character Error Rate Estimation for Semi-Supervised Training of Speech Recognition for Arabic Dialects",
   "original": "1799",
   "order": 647,
   "page_count": 5,
   "abstract": [
    "The use of semi-supervised data for Automatic Speech Recognition (ASR) is nowadays commonplace and is the basis of the most advanced ASR models. For low-resourced languages, where limited labelled data is available, it opens the possibility of using unlimited amounts of data without escalating costs. For this, an initial ASR system is required that can produce a pseudo-transcript of the untranscribed data, but in low-resourced languages, the accuracy of this initial system might not be sufficient to provide accurate pseudo-transcripts, so techniques for data selection become necessary. This paper explores the use of Character Error Rate (CER) estimation for automatically selecting the best segments from a set of nearly 4,000 hours of untranscribed Arabic in different dialects."
   ],
   "p1": 3179,
   "pn": 3183,
   "doi": "10.21437/Interspeech.2025-1799",
   "url": "interspeech_2025/park25f_interspeech.html"
  },
  "kponou25_interspeech": {
   "authors": [
    [
     "D. Fortuné",
     "Kponou"
    ],
    [
     "Salima",
     "Mdhaffar"
    ],
    [
     "Fréjus A. A.",
     "Laleye"
    ],
    [
     "Eugène C.",
     "Ezin"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "Extending the Fongbe to French Speech Translation Corpus:  resources, models and benchmark",
   "original": "1801",
   "order": 923,
   "page_count": 5,
   "abstract": [
    "This paper introduces FFSTC 2, an expanded version of the existing Fongbe-to-French speech translation corpus, addressing the critical need for resources in African dialects for speech recognition and translation tasks. We extended the dataset by adding 36 hours of transcribed audio, bringing the total to 61 hours, thereby enhancing its utility for both automatic speech recognition (ASR) and speech translation (ST) in Fongbe, a low-resource language. Using corpus, we develop both cascade and end-to-end speech translation systems. Our models employ AfriHuBERT and HuBERT147 as encoders, and NLLB and mBART as decoders. We introduce a diacritic-substitution technique for both ASR and Machine Translation (MT) which, yields a BLEU score of 37.23 compared to 39.60 for the fully diacritized configuration. Among the evaluated end-to-end architectures, AfriHuBERT-NLLB with data augmentation attains the highest BLEU score of 26.32."
   ],
   "p1": 4533,
   "pn": 4537,
   "doi": "10.21437/Interspeech.2025-1801",
   "url": "interspeech_2025/kponou25_interspeech.html"
  },
  "zhong25b_interspeech": {
   "authors": [
    [
     "Terry Yi",
     "Zhong"
    ],
    [
     "Esther",
     "Janse"
    ],
    [
     "Cristian",
     "Tejedor-Garcia"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Martha",
     "Larson"
    ]
   ],
   "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers",
   "original": "1805",
   "order": 764,
   "page_count": 5,
   "abstract": [
    "Speech-based Parkinson&#x27;s disease (PD) detection has gained attention for its automated, cost-effective, and non-intrusive nature. As research studies usually rely on data from diagnostic-oriented speech tasks, this work explores the feasibility of diagnosing PD on the basis of speech data not originally intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our findings indicate that TT can be as useful as diagnostic-oriented PD datasets like PC-GITA. We also investigate which specific dataset characteristics impact PD classification performance. The results show that concatenating audio recordings and balancing participants&#x27; gender and status distributions can be beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA generalize poorly to TT, whereas models trained on TT perform better on PC-GITA. Furthermore, we provide insights into the high variability across folds, which is mainly due to large differences in individual speaker performance."
   ],
   "p1": 3738,
   "pn": 3742,
   "doi": "10.21437/Interspeech.2025-1805",
   "url": "interspeech_2025/zhong25b_interspeech.html"
  },
  "kalda25_interspeech": {
   "authors": [
    [
     "Joonas",
     "Kalda"
    ],
    [
     "Clément",
     "Pagés"
    ],
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Hervé",
     "Bredin"
    ]
   ],
   "title": "Diarization-Guided Multi-Speaker Embeddings",
   "original": "1807",
   "order": 1067,
   "page_count": 5,
   "abstract": [
    "Reliable speaker embeddings are critical for multi-speaker speech processing tasks. Traditionally models are trained on single-speaker utterances and suffer from domain mismatch when applied in multi-speaker contexts. Recently proposed guided speaker embeddings (GSE) were shown to improve this by training on synthetic multi-speaker mixtures guided by oracle speaker activity labels. Additionally modeling all speakers present in a chunk is desirable but the performance of such methods has been sub-par up to now. We build on GSE by modeling multiple speakers together and using diarization features for guiding. We also propose a new validation metric for embeddings in multi-speaker context and demonstrate its effectiveness. Results on multiple speaker diarization datasets demonstrate that we improve on speed and performance while reducing the embedding model size."
   ],
   "p1": 5233,
   "pn": 5237,
   "doi": "10.21437/Interspeech.2025-1807",
   "url": "interspeech_2025/kalda25_interspeech.html"
  },
  "abdullah25_interspeech": {
   "authors": [
    [
     "Badr M.",
     "Abdullah"
    ],
    [
     "Matthew",
     "Baas"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Voice Conversion Improves Cross-Domain Robustness  for Spoken Arabic Dialect Identification",
   "original": "1809",
   "order": 569,
   "page_count": 5,
   "abstract": [
    "Arabic dialect identification (ADI) systems are essential for large-scale data collection pipelines that enable the development of inclusive speech technologies for Arabic language varieties. However, the reliability of current ADI systems is limited by poor generalization to out-of-domain speech. In this paper, we present an effective approach based on voice conversion for training ADI models that achieves state-of-the-art performance and significantly improves robustness in cross-domain scenarios. Evaluated on a newly collected real-world test set spanning four different domains, our approach yields consistent improvements of up to +34.1% in accuracy across domains. Furthermore, we present an analysis of our approach and demonstrate that voice conversion helps mitigate the speaker bias in the ADI dataset. We release our robust ADI model and cross-domain evaluation dataset to support the development of inclusive speech technologies for Arabic."
   ],
   "p1": 2790,
   "pn": 2794,
   "doi": "10.21437/Interspeech.2025-1809",
   "url": "interspeech_2025/abdullah25_interspeech.html"
  },
  "huang25f_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Feng-fan",
     "Hsieh"
    ],
    [
     "Yueh-chin",
     "Chang"
    ]
   ],
   "title": "Articulatory variations in Apical Vowels in Southwestern Mandarin",
   "original": "1810",
   "order": 711,
   "page_count": 5,
   "abstract": [
    "This study examines the acoustic and articulatory characteristics of apical vowels in Southwestern Mandarin (SWM) using electromagnetic articulography (EMA). Data were collected from seven speakers in their twenties, focusing apical vowels, the high vowel [i], and their er-suffixed forms. Two distinct patterns emerged: Type A speakers demonstrated differential tongue configurations for apical vowels (si with dental/alveolar position, shi with alveolo-palatal position), while Type B speakers showed similar configurations for both vowels. Type A findings align with previous research on Standard Chinese, confirming that apical vowels share place features with preceding sibilants and showing higher F2 values for shi. However, Type B revealed an intriguing articulatoryacoustic mismatch: despite showing significant differences in horizontal Tongue Tip and Tongue Body positions, these vowels exhibited no significant formant differences. This unexpected pattern suggests a more complex relationship between articulatory configuration and acoustic output in apical vowel production than previously documented."
   ],
   "p1": 3494,
   "pn": 3498,
   "doi": "10.21437/Interspeech.2025-1810",
   "url": "interspeech_2025/huang25f_interspeech.html"
  },
  "yoneyama25_interspeech": {
   "authors": [
    [
     "Reo",
     "Yoneyama"
    ],
    [
     "Masaya",
     "Kawamura"
    ],
    [
     "Ryo",
     "Terashima"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Comparative Analysis of Fast and High-Fidelity Neural Vocoders for Low-Latency Streaming Synthesis in Resource-Constrained Environments",
   "original": "1819",
   "order": 994,
   "page_count": 5,
   "abstract": [
    "In real-time speech synthesis, neural vocoders often require low-latency synthesis through causal processing and streaming. However, streaming introduces inefficiencies absent in batch synthesis, such as limited parallelism, inter-frame dependency management, and parameter loading overhead. This paper proposes multi-stream Wavehax (MS-Wavehax), an efficient neural vocoder for low-latency streaming, by extending the aliasing-free neural vocoder Wavehax with multi-stream decomposition. We analyze the latency-throughput trade-off in a CPU-only environment and identify key bottlenecks in streaming neural vocoders. Our findings provide practical insights for optimizing chunk sizes and designing vocoders tailored to specific application demands and hardware constraints. Furthermore, our subjective evaluations show that MS-Wavehax delivers high speech quality under causal and non-causal conditions while being remarkably compact and easily deployable in resource-constrained environments."
   ],
   "p1": 4888,
   "pn": 4892,
   "doi": "10.21437/Interspeech.2025-1819",
   "url": "interspeech_2025/yoneyama25_interspeech.html"
  },
  "berthommier25_interspeech": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Articulatory modeling of the S-shaped F2 trajectories observed in Öhman's spectrographic analysis of VCV syllables",
   "original": "1820",
   "order": 197,
   "page_count": 5,
   "abstract": [
    "The synthesis of Öhman&#x27;s VCV sequences with intervocalic plosive consonants was first achieved 30 years ago using the DRM model. However, this approach remains primarily acoustic and lacks articulatory constraints. In this study, the same 75 VCVs are analyzed, but generated with the Maeda model, using trajectory planning that differentiates vowel-to-vowel transitions from consonantal influences. Synthetic data exhibit similar characteristics to Öhman&#x27;s sequences, including the presence of S-shaped F2 trajectories. Furthermore, locus equations (LEs) for F2 and F3 are computed from synthetic CV data to investigate their underlying determinism, leading to a reassessment of conventional interpretations. The findings indicate that, although articulatory planning is structured separately for vowel and consonant groups, S-shaped F2 trajectories emerge from a composite mechanism governed by the coordinated synergy of all articulators."
   ],
   "p1": 953,
   "pn": 957,
   "doi": "10.21437/Interspeech.2025-1820",
   "url": "interspeech_2025/berthommier25_interspeech.html"
  },
  "wang25u_interspeech": {
   "authors": [
    [
     "Guitao",
     "Wang"
    ],
    [
     "Jinming",
     "Zhao"
    ],
    [
     "Hao",
     "Yang"
    ],
    [
     "Guilin",
     "Qi"
    ],
    [
     "Tongtong",
     "Wu"
    ],
    [
     "Gholamreza",
     "Haffari"
    ]
   ],
   "title": "Continual Speech Learning with Fused Speech Features",
   "original": "1823",
   "order": 366,
   "page_count": 5,
   "abstract": [
    "Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining."
   ],
   "p1": 1793,
   "pn": 1797,
   "doi": "10.21437/Interspeech.2025-1823",
   "url": "interspeech_2025/wang25u_interspeech.html"
  },
  "linke25_interspeech": {
   "authors": [
    [
     "Julian",
     "Linke"
    ],
    [
     "Jana",
     "Winkler"
    ],
    [
     "Barbara",
     "Schuppler"
    ]
   ],
   "title": "Context is all you need? Low-resource conversational ASR profits from context, coming from the same or from the other speaker",
   "original": "1824",
   "order": 651,
   "page_count": 5,
   "abstract": [
    "Despite the rapid advancement of automatic speech recognition (ASR) systems, spontaneous conversations still pose a major challenge, which is even more of an obstacle for low-resourced languages, dialects or non-dominant varieties. What is more, lively turn-changes in conversational speech cause short utterances that have been found to be error prone for transformer-based ASR systems, requiring larger context. The question thus arises which type of context is useful: rather more from the same speaker, providing acoustically relevant context, or more from the conversation - mixing utterances from both speakers - providing semantically relevant context. Comparing seven ASR systems on conversational Austrian German, we find the best performance with a minimum of 20s of context, independent of whether it was from the same or from the other speaker. Systems fine-tuned with data from the same variety and speaking style require less context and perform overall better than zero-shot systems."
   ],
   "p1": 3199,
   "pn": 3203,
   "doi": "10.21437/Interspeech.2025-1824",
   "url": "interspeech_2025/linke25_interspeech.html"
  },
  "istaiteh25_interspeech": {
   "authors": [
    [
     "Othman",
     "Istaiteh"
    ],
    [
     "Salima",
     "Mdhaffar"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "Beyond Similarity Scoring: Detecting Entailment and Contradiction in Multilingual and Multimodal Contexts",
   "original": "1825",
   "order": 59,
   "page_count": 5,
   "abstract": [
    "Natural Language Inference (NLI) determines whether a hypothesis entails, contradicts, or is neutral with respect to a premise. While text-based NLI is well-studied, its multimodal and multilingual extension remains underexplored. This paper introduces a multilingual, multimodal NLI framework classifying entailment, contradiction, and neutrality across text-text, text-speech, speech-text, and speech-speech pairs in same- and cross-lingual settings. A key motivation is improving translation assessment, where similarity-based approaches may miss contradictions. The framework complements evaluation methods and helps identify inconsistencies by detecting entailment and contradiction alongside semantic similarity. It also extends text-based datasets with speech-text and speech-speech pairs for multilingual multimodal inference. Experiments show the model outperforms BLASER in distinguishing entailment from non-entailment, achieving F1 gains of 0.19 in speech-speech and 0.13 in speech-text."
   ],
   "p1": 286,
   "pn": 290,
   "doi": "10.21437/Interspeech.2025-1825",
   "url": "interspeech_2025/istaiteh25_interspeech.html"
  },
  "magoshi25_interspeech": {
   "authors": [
    [
     "Ryo",
     "Magoshi"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Jaeyoung",
     "Lee"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Multi-lingual and Zero-Shot Speech Recognition by Incorporating Classification of Language-Independent Articulatory Features",
   "original": "1827",
   "order": 20,
   "page_count": 5,
   "abstract": [
    "We address multi-lingual speech recognition including unknown or zero-shot languages based on the International Phonetic Alphabet (IPA) and articulatory features. Articulatory features are language-independent representations for IPA based on phonetic knowledge. In the previous studies, however, they were mostly limited to two dimensions of place of articulation and manner of articulation. Moreover, the classification of articulatory features were not well aligned with phone recognition. In this study, we adopt a comprehensive 24-dimensional vector representation, and propose a training method in which IPA tokens and their corresponding articulatory features are simultaneously predicted based on CTC alignment. Experiments are conducted by fine-tuning the wav2vec 2.0 XLS-R model over 22 languages, and the results demonstrated significant improvements on average as well as in zero-shot language settings."
   ],
   "p1": 91,
   "pn": 95,
   "doi": "10.21437/Interspeech.2025-1827",
   "url": "interspeech_2025/magoshi25_interspeech.html"
  },
  "kanamori25_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Kanamori"
    ],
    [
     "Yuki",
     "Okamoto"
    ],
    [
     "Taisei",
     "Takano"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio ",
   "original": "1830",
   "order": 642,
   "page_count": 5,
   "abstract": [
    "In text-to-audio (TTA) research, the relevance between input text and output audio is an important evaluation aspect. Traditionally, it has been evaluated from both subjective and objective perspectives. However, subjective evaluation is costly in terms of money and time, and objective evaluation is unclear regarding the correlation to subjective evaluation scores. In this study, we construct RELATE, an open-sourced dataset that subjectively evaluates the relevance. Also, we benchmark a model for automatically predicting the subjective evaluation score from synthesized audio. Our model outperforms a conventional CLAPScore model, and that trend extends to many sound categories."
   ],
   "p1": 3155,
   "pn": 3159,
   "doi": "10.21437/Interspeech.2025-1830",
   "url": "interspeech_2025/kanamori25_interspeech.html"
  },
  "li25x_interspeech": {
   "authors": [
    [
     "Zhijie",
     "Li"
    ],
    [
     "Hui",
     "Feng"
    ]
   ],
   "title": "Acoustic Representation and Realization of Weak Elements Subcategories: In the Case of Tianjin Mandarin",
   "original": "1835",
   "order": 147,
   "page_count": 5,
   "abstract": [
    "Weak elements in Tianjin Mandarin are often assumed to have distinct origins and functions across subcategories, yet acoustic differences among them remain underexplored. This study analyzes the first two formants (F1, F2) and fundamental frequency (F0) of weak elements produced by six Tianjin Mandarin speakers. Results reveal: (1) Vowel quality shows minimal variation across subcategories, with F1 reduction and F2 centralization in monophthongs, and F1 centralization and compression in diphthongs. (2) Tone realization varies: affixes, locative prepositions, directional verbs, reduplicated words and optional weak elements exhibit a fixed mid-low pitch target, achieved through target-setting. In contrast, structural particles, aspectual particles, habitual weak elements, and functional weak elements rely on contextual tone spreading. These findings highlight two coexisting patterns of tone realization in Tianjin Mandarin weak elements."
   ],
   "p1": 704,
   "pn": 708,
   "doi": "10.21437/Interspeech.2025-1835",
   "url": "interspeech_2025/li25x_interspeech.html"
  },
  "mehralian25_interspeech": {
   "authors": [
    [
     "Pouya",
     "Mehralian"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Leveraging Geographic Metadata for Dialect-Aware Speech Recognition",
   "original": "1839",
   "order": 237,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems struggle with dialectal variations due to significant phonetic, morphosyntactic, and lexical differences across regions. Traditional approaches rely on dialect-specific models, per-dialect fine-tuning, or unified models that struggle to generalize across diverse dialects. In this work, we propose novel methods for integrating geographical metadata into ASR models to enhance dialect adaptation. By leveraging latitude and longitude as continuous inputs, our model learns smooth transitions across dialectal variations, allowing it to interpolate between known dialects and generalize to unseen regions. Our experiments demonstrate that these approaches significantly outperform both dialect-specific models and state-of-the-art Whisper models while reducing computational and maintenance costs. These findings suggest that location-based adaptation is a promising direction for improving ASR performance in dialect-rich environments."
   ],
   "p1": 1153,
   "pn": 1157,
   "doi": "10.21437/Interspeech.2025-1839",
   "url": "interspeech_2025/mehralian25_interspeech.html"
  },
  "leygue25_interspeech": {
   "authors": [
    [
     "Tahitoa",
     "Leygue"
    ],
    [
     "Astrid",
     "Sabourin"
    ],
    [
     "Christian",
     "Bolzmacher"
    ],
    [
     "Sylvain",
     "Bouchigny"
    ],
    [
     "Margarita",
     "Anastassova"
    ],
    [
     "Quoc-Cuong",
     "Pham"
    ]
   ],
   "title": "Explainable Speech Emotion Recognition Through Attentive Pooling: Insights from Attention-Based Temporal Localization",
   "original": "1841",
   "order": 948,
   "page_count": 5,
   "abstract": [
    "State-of-the-art transformer models for Speech Emotion Recognition (SER) rely on temporal feature aggregation, yet advanced pooling methods remain underexplored. We systematically benchmark pooling strategies, including Multi-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5 percentage point macro F1 gain over average pooling. Attention analysis shows 15 percent of frames capture 80 percent of emotion cues, revealing a localized pattern of emotional information. Analysis of high-attention frames reveals that non-linguistic vocalizations and hyperarticulated phonemes are disproportionately prioritized during pooling, mirroring human perceptual strategies. Our findings position attentive pooling as both a performant SER mechanism and a biologically plausible tool for explainable emotion localization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, our approach obtained a macro F1 score of 0.3649."
   ],
   "p1": 4658,
   "pn": 4662,
   "doi": "10.21437/Interspeech.2025-1841",
   "url": "interspeech_2025/leygue25_interspeech.html"
  },
  "li25y_interspeech": {
   "authors": [
    [
     "Aijun",
     "LI"
    ],
    [
     "Zhiwei",
     "Wang"
    ],
    [
     "Jun",
     "Gao"
    ],
    [
     "Xin",
     "Zhou"
    ]
   ],
   "title": "The Development of Speech Rhythm in Putonghua-Learning Preschool Children in South Xinjiang Uyghur Autonomous Region of China",
   "original": "1847",
   "order": 909,
   "page_count": 5,
   "abstract": [
    "This study investigates Putonghua rhythm development among Uyghur preschoolers in South Xinjiang, China. We examined 40 children (ages 5-7) grouped by Putonghua learning time: mid-class (12 months, mean age 6;1) and senior-class (24 months, mean age 7;1). Speech samples were elicited through picture-guessing tasks and analyzed using rhythmic metrics. Our analyses reveal distinct developmental patterns compared to Putonghua-speaking children, with neither group achieving metrics comparable to age-matched Putonghua-speaking 6-year-olds. Consonant-related metrics emerged as increasingly salient markers in later developmental stages. The findings demonstrate that rhythm acquisition in this bilingual context is modulated by cross-linguistic influence from Uyghur. These results advance our understanding of L2 rhythm acquisition in bilingual settings and have implications for developing automated speech assessment tools and optimizing speech recognition systems for multilingual contexts."
   ],
   "p1": 4463,
   "pn": 4467,
   "doi": "10.21437/Interspeech.2025-1847",
   "url": "interspeech_2025/li25y_interspeech.html"
  },
  "zhang25q_interspeech": {
   "authors": [
    [
     "Suqi",
     "Zhang"
    ],
    [
     "Zheqi",
     "Dai"
    ],
    [
     "Yongyi",
     "Zang"
    ],
    [
     "Yin",
     "Cao"
    ],
    [
     "Qiuqiang",
     "Kong"
    ]
   ],
   "title": "DiffStereo: End-to-End Mono-to-Stereo Audio Generation with Diffusion Transformer",
   "original": "1850",
   "order": 641,
   "page_count": 5,
   "abstract": [
    "Mono-to-stereo audio generation is a task that converts mono audio into two-channel stereo audio. Stereo audio generation plays a crucial role in enhancing spatial perception and auditory immersion. Traditional mono-to-stereo methods include rule-based, simulation-based, and various deep learning-based approaches. These methods require expert knowledge or explicit positional information to generate specific stereo effects, limiting their scalability and generalization. To address these challenges, we propose DiffStereo, an end-to-end diffusion transformer-based model that generates stereo audio conditioned on mono audio. The contributions of DiffStereo are as follows: First, DiffStereo directly synthesizes stereo audio from a mono waveform input in an end-to-end fashion, requiring no human intervention or prior knowledge. Second, DiffStereo achieves competitive objective ratings and consistently better subjective ratings, validating the effectiveness of our end-to-end approach."
   ],
   "p1": 3150,
   "pn": 3154,
   "doi": "10.21437/Interspeech.2025-1850",
   "url": "interspeech_2025/zhang25q_interspeech.html"
  },
  "franz25_interspeech": {
   "authors": [
    [
     "Sven",
     "Franz"
    ],
    [
     "Tanja",
     "Grewe"
    ],
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Jörg",
     "Bitzer"
    ]
   ],
   "title": "Influence of Room Acoustics on Objective Voice Assessment Methods in the Context of Speech and Language Therapy ",
   "original": "1851",
   "order": 1083,
   "page_count": 5,
   "abstract": [
    "Voice and speech disorders can be assessed through subjective perceptual measures or objective indices, such as the Acoustic Voice Quality Index (AVQI) or the Acoustic Breathiness Index (ABI). These objective measures reduce diagnostic variability by eliminating subjective evaluations. However, the impact of room acoustics on their robustness for therapy monitoring remains unclear. In this contribution, reverberation times, impulse responses, and background noise were recorded and analyzed in conjunction with speech samples using statistical models to evaluate their influence. Results indicate that room acoustics significantly affect AVQI and ABI, particularly for non-pathological voices and with distant microphones. For reliable therapeutic use, standardized measurement environments or robust analysis methods are essential. Optimized recording conditions with source-proximate microphones enhance accuracy, advancing objective voice quality assessments and evidence-based speech therapy."
   ],
   "p1": 5308,
   "pn": 5312,
   "doi": "10.21437/Interspeech.2025-1851",
   "url": "interspeech_2025/franz25_interspeech.html"
  },
  "havard25_interspeech": {
   "authors": [
    [
     "William N.",
     "Havard"
    ],
    [
     "Renauld",
     "Govain"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Emmanuel",
     "Schang"
    ]
   ],
   "title": "Self-Supervised Models of Speech Processing for Haitian Creole",
   "original": "1852",
   "order": 820,
   "page_count": 5,
   "abstract": [
    "We develop tailored speech processing models for Haitian Creole positioning it as a high-resource language in terms of SSL models of speech processing. We do so by pretraining monolingual Wav2Vec2-Base, Wav2Vec2-Large and Data2Vec-Audio-Base models from scratch, which are then finetuned on an Automatic Speech Recognition task. We compare the performance of these models with models finetuned from larger multilingual (XLSR-53, XLSR2-300m, MMS-1B) and monolingual French-based models (LeBenchmark 1 to 7K). Our results highlight the effectiveness of pretraining monolingual models from scratch, demonstrating that they can achieve performance comparable to or even surpassing larger models derived from large-scale multilingual pretraining. Our work provides essential resources for robust Haitian Creole speech recognition and offers pretrained models that can be adapted to other French-based Caribbean Creoles, opening new avenues for linguistic research and practical applications in the region."
   ],
   "p1": 4018,
   "pn": 4022,
   "doi": "10.21437/Interspeech.2025-1852",
   "url": "interspeech_2025/havard25_interspeech.html"
  },
  "hopponen25_interspeech": {
   "authors": [
    [
     "Satu",
     "Hopponen"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Alexandre",
     "Nikolaev"
    ],
    [
     "Rosa",
     "González Hautamäki"
    ],
    [
     "Lauri",
     "Tavi"
    ],
    [
     "Einar",
     "Meister"
    ]
   ],
   "title": "FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents",
   "original": "1853",
   "order": 79,
   "page_count": 5,
   "abstract": [
    "We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers, who produced speech in their native language (L1), second language (L2), and imitated L2 (fake foreign accent). The new corpus enables research into language variability from phonetic and technological points of view. Accordingly, we include two preliminary case studies to demonstrate both perspectives. The first case study explores the impact of L2 and imitated L2 on the performance of an automatic speaker verification system, while the second illustrates the articulatory patterns of one speaker in L1, L2, and a fake accent."
   ],
   "p1": 364,
   "pn": 368,
   "doi": "10.21437/Interspeech.2025-1853",
   "url": "interspeech_2025/hopponen25_interspeech.html"
  },
  "lu25g_interspeech": {
   "authors": [
    [
     "Mingxi",
     "LU"
    ],
    [
     "Ran",
     "Tao"
    ],
    [
     "Yujia",
     "Tian"
    ]
   ],
   "title": "Talker Normalization in Chinese Bilinguals: A Comparative Study",
   "original": "1857",
   "order": 474,
   "page_count": 5,
   "abstract": [
    "Talker normalization enables listeners to adapt to speaker-specific acoustic variability, thereby facilitating speech perception across different talkers. While previous research suggests that talker normalization can occur in both Mandarin and Cantonese speech contexts, the extent to which it applies to non-linguistic contexts remains unclear. This study examined talker normalization in Mandarin-Cantonese bilinguals, comparing its effects in speech and nonspeech contexts. We recruited 36 bilingual participants to complete a forced-choice word identification task in one Mandarin paradigm and two Cantonese paradigms. The results revealed that under all three paradigms, tone normalization occurred robustly in the speech condition but was absent in the nonspeech condition. These findings provide novel evidence that talker normalization operates primarily within linguistic boundaries, supporting the view that talker normalization is language-specific rather than domain-general."
   ],
   "p1": 2315,
   "pn": 2319,
   "doi": "10.21437/Interspeech.2025-1857",
   "url": "interspeech_2025/lu25g_interspeech.html"
  },
  "silveira25_interspeech": {
   "authors": [
    [
     "Gustavo",
     "Silveira"
    ],
    [
     "Aviad",
     "Albert"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "Probing Prosodic Differences Between Two Regional Varieties of Brazilian Portuguese",
   "original": "1863",
   "order": 598,
   "page_count": 5,
   "abstract": [
    "This study compares the intonation of two Brazilian Portuguese varieties, Alagoas and São Paulo, focusing on the last three syllables before terminal and non-terminal boundaries of declarative utterances in sociolinguistic interviews from ten male speakers. With the ProPer toolbox, two acoustic measures were extracted from syllables: (i) DeltaF0, measuring F0 change between syllables; and (ii) Synchrony, reflecting F0 trends within syllables. Analyses of 695 boundary tokens reveal continuous differences (F0 excursion size) between varieties for terminal boundaries, and both categorical (edge tone type) and continuous (primarily F0 direction) differences for non-terminal ones. Alagoans produced larger pitch falls for terminal boundaries and favored a high pitch level on post-stressed syllables for non-terminal boundaries, whereas Paulistans preferred a fall on post-stressed syllables. Both regions mark finality vs. continuation primarily with pitch accents, rather than edge tones."
   ],
   "p1": 2935,
   "pn": 2939,
   "doi": "10.21437/Interspeech.2025-1863",
   "url": "interspeech_2025/silveira25_interspeech.html"
  },
  "li25z_interspeech": {
   "authors": [
    [
     "Zhe",
     "Li"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Mert",
     "Pilanci"
    ],
    [
     "Zezhong",
     "Jin"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Disentangling Speaker and Content in Pre-trained Speech Models with Latent Diffusion for Robust Speaker Verification",
   "original": "1865",
   "order": 228,
   "page_count": 5,
   "abstract": [
    "Disentangled speech representation learning for speaker verification aims to separate spoken content and speaker timbre into distinct representations. However, existing variational autoencoder (VAE)--based methods for speech disentanglement rely on latent variables that lack semantic meaning, limiting their effectiveness for speaker verification. To address this limitation, we propose a diffusion-based method that disentangles and separates speaker features and speech content in the latent space. Building upon the VAE framework, we employ a speaker encoder to learn latent variables representing speaker features while using frame-specific latent variables to capture content. Unlike previous sequential VAE approaches, our method utilizes a conditional diffusion model in the latent space to derive speaker-aware representations. Experiments on the VoxCeleb datasets demonstrate that our method effectively isolates speaker features from speech content using pre-trained speech"
   ],
   "p1": 1108,
   "pn": 1112,
   "doi": "10.21437/Interspeech.2025-1865",
   "url": "interspeech_2025/li25z_interspeech.html"
  },
  "koudounas25b_interspeech": {
   "authors": [
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Moreno",
     "La Quatra"
    ],
    [
     "Gabriele",
     "Ciravegna"
    ],
    [
     "Marco",
     "Fantini"
    ],
    [
     "Erika",
     "Crosetti"
    ],
    [
     "Giovanni",
     "Succo"
    ],
    [
     "Tania",
     "Cerquitelli"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Elena",
     "Baralis"
    ]
   ],
   "title": "MVP: Multi-source Voice Pathology detection",
   "original": "1868",
   "order": 726,
   "page_count": 5,
   "abstract": [
    "Voice disorders significantly impact patient quality of life, yet non-invasive automated diagnosis remains under-explored due to both the scarcity of pathological voice data, and the variability in recording sources. This work introduces MVP (Multi-source Voice Pathology detection), a novel approach that leverages transformers operating directly on raw voice signals. We explore three fusion strategies to combine sentence reading and sustained vowel recordings: waveform concatenation, intermediate feature fusion, and decision-level combination. Empirical validation across the German, Portuguese, and Italian languages shows that intermediate feature fusion using transformers best captures the complementary characteristics of both recording types. Our approach achieves up to +13% AUC improvement over single-source methods."
   ],
   "p1": 3548,
   "pn": 3552,
   "doi": "10.21437/Interspeech.2025-1868",
   "url": "interspeech_2025/koudounas25b_interspeech.html"
  },
  "shao25c_interspeech": {
   "authors": [
    [
     "Bowei",
     "Shao"
    ],
    [
     "Philipp",
     "Buech"
    ],
    [
     "Anne",
     "Hermes"
    ],
    [
     "Maria",
     "Giavazzi"
    ]
   ],
   "title": "Lexical stress affects lenition: The case of Italian palato-alveolar affricates",
   "original": "1869",
   "order": 77,
   "page_count": 5,
   "abstract": [
    "The realization of palato-alveolar affricates in Italian varies regionally. While affricates have undergone deaffrication in Tuscan and southern varieties  (/ˈt͡ʃeː.na/ → [ˈʃeː.na]), northern varieties are traditionally described as retaining the affricate. We provide acoustic and articulatory (EMA) evidence of an incipient lenition process in non-deaffricating varieties. This process is stress-conditioned as it is blocked in post-tonic affricates. Five of fifteen speakers deaffricated far-from-stress affricates in nonce words, as indicated by higher RMS amplitude and energy during the closure, while post-tonic affricates were preserved. Ten speakers did not exhibit stress-conditioned deaffrication. Both groups showed a longer acoustic closure duration, caused by delayed articulatory target achievement in post-tonic position. We discuss the phonological implications of these findings and their potential role in sound change."
   ],
   "p1": 354,
   "pn": 358,
   "doi": "10.21437/Interspeech.2025-1869",
   "url": "interspeech_2025/shao25c_interspeech.html"
  },
  "li25aa_interspeech": {
   "authors": [
    [
     "Zexin",
     "Li"
    ],
    [
     "Wenhan",
     "Yao"
    ],
    [
     "Ye",
     "Xiao"
    ],
    [
     "Jinsu",
     "Yang"
    ],
    [
     "Fen",
     "Xiao"
    ],
    [
     "Weiping",
     "Wen"
    ]
   ],
   "title": "LRBA: Stealthy Backdoor Attacks on Speech Classification via Latent Rearrangement in VITS",
   "original": "1872",
   "order": 1152,
   "page_count": 5,
   "abstract": [
    "Speech classification systems based on deep learning are vulnerable to backdoor attacks, causing the model&#x27;s predictions to deviate from normal behavior. Existing speech backdoor methods often produce poisoned samples by perceptible modifications, which reduce the stealthiness of the attack and make it easier to detect. To improve stealthiness, this paper proposed the Latent Rearrangement Backdoor Attack (LRBA), a novel backdoor attack framework utilizing the latent space in a pre-trained VITS model to achieve an imperceptible attack. Explicitly, we manipulate the latent representations by utilizing the normalizing flow of VITS to generate rearranged utterances, where the rearranged semantics can be associated with the attacker&#x27;s specific target label, achieving a backdoor attack. Results show that our method achieves an excellent attack success rate with a very low poisoning rate and maintains a high mean opinion score, outperforming existing methods in effectiveness and stealthiness."
   ],
   "p1": 5653,
   "pn": 5657,
   "doi": "10.21437/Interspeech.2025-1872",
   "url": "interspeech_2025/li25aa_interspeech.html"
  },
  "wang25v_interspeech": {
   "authors": [
    [
     "Zilong",
     "Wang"
    ],
    [
     "Xiaoxue",
     "Zhang"
    ],
    [
     "Xinyang",
     "Jiang"
    ],
    [
     "Kaitao",
     "Song"
    ],
    [
     "Jue",
     "Yu"
    ]
   ],
   "title": "Can AI Understand Mandarin Speech Prosody?  A Framework and Benchmark Showcase",
   "original": "1873",
   "order": 1097,
   "page_count": 5,
   "abstract": [
    "How to model and estimate speech prosody is considered as a challenging task in understanding and generating natural speech. We introduce the Mandarin Speech Prosody Benchmark (MSPB), a linguistically grounded dataset for evaluating Speech Large Language Models (Speech LLMs) in Mandarin. MSPB comprises eight tasks covering crucial prosodic features and their interactions with syntax, semantics, and pragmatics. All MSPB items, designed per Mandarin linguistic principles and validated by experts, were phonetically recorded and verified. We evaluated six Speech LLMs (GPT-4o, Gemini-1.5-Pro, Gemini-2-Flash, Qwen2-Audio-7B-Instruct, GLM-4-Voice, MiniCPM-o 2.6). Although some models perform well with context-rich cues (e.g., irony), they generally struggle with subtle prosodic variations (e.g., focus marking) and underperform humans. MSPB provides a valuable tool to assess and enhance prosodic comprehension, underscoring the need for improved prosodic integration in future research."
   ],
   "p1": 5378,
   "pn": 5382,
   "doi": "10.21437/Interspeech.2025-1873",
   "url": "interspeech_2025/wang25v_interspeech.html"
  },
  "sinha25_interspeech": {
   "authors": [
    [
     "Abhijit",
     "Sinha"
    ],
    [
     "Hemant",
     "Kumar Kathania"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Beyond Traditional Speech Modifications : Utilizing Self Supervised Features for Enhanced Zero-Shot Children ASR",
   "original": "1874",
   "order": 400,
   "page_count": 5,
   "abstract": [
    "Zero-shot automatic speech recognition (ASR) for children is challenging due to pronounced acoustic and linguistic mismatches, speaker variability and limited annotated data. This work utilizes self-supervised learning (SSL) features to address these challenges without requiring child specific data. We perform a layer-wise analysis of SSL models, Wav2Vec2, HuBERT, and Data2Vec to identify optimal representations for zero-shot children ASR. Our results show that features from specific layers (e.g., layer 22 of Wav2Vec2) capture robust, speaker-invariant phonetic information, significantly improving recognition accuracy by reducing the word error rate (WER) from 10.65% to 5.15%, a 51.64% relative improvement over Wav2Vec2 baseline. Additionally, while conventional acoustic modifications (pitch, speaking rate, formant) enhance performance in traditional systems, they yield minimal gains for SSL-based models, highlighting the intrinsic speaker invariance of SSL representations."
   ],
   "p1": 1963,
   "pn": 1967,
   "doi": "10.21437/Interspeech.2025-1874",
   "url": "interspeech_2025/sinha25_interspeech.html"
  },
  "yang25m_interspeech": {
   "authors": [
    [
     "Hongli",
     "Yang"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Ayiduosi",
     "Tuohan"
    ],
    [
     "Yizhou",
     "Peng"
    ]
   ],
   "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR",
   "original": "1875",
   "order": 233,
   "page_count": 5,
   "abstract": [
    "Recent advancements in multilingual automatic speech recognition (ASR) have been driven by large-scale end-to-end models like Whisper. However, challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist. This paper addresses these with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which applies soft prompts to both the encoder and decoder, enhancing feature extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), using cross-lingual similarities to encode shared and language-specific features in lightweight prompt matrices; 3) SPT-Whisper, a toolkit integrating SPT into Whisper, enabling efficient continual learning. Experiments across three languages from FLEURS demonstrate that Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead."
   ],
   "p1": 1133,
   "pn": 1137,
   "doi": "10.21437/Interspeech.2025-1875",
   "url": "interspeech_2025/yang25m_interspeech.html"
  },
  "zhao25j_interspeech": {
   "authors": [
    [
     "Fengyue Lisa",
     "Zhao"
    ],
    [
     "Jennifer",
     "Kuo"
    ]
   ],
   "title": " The Role of Contextual Variation in Learning Cantonese Tones from Naturalistic Speech",
   "original": "1876",
   "order": 907,
   "page_count": 5,
   "abstract": [
    "A central question in speech acquisition is how infants learn contrastive sound categories from variable acoustic cues. The Distributional Learning Across Contexts (DLAC) proposal suggests that infants may identify contrastive sound dimensions by tracking distributions across contexts, with greater contextual variation signaling contrastive categories. Most research on acquisition of sound categories has focused on simple two-way vowel and consonant contrasts, leaving open how infants learn more complex or tonal contrasts. This study addresses these gaps by analyzing naturalistic speech to examine whether the ease of learning Cantonese’s six tonal contrasts is linked to their variation across different contexts. By comparing our findings with existing acquisition data, we show that this proposal could predict which contrasts are easier or harder to learn, suggesting that in absence of invariant acoustic cues, infants may rely on contextual variation to acquire complex sound categories."
   ],
   "p1": 4453,
   "pn": 4457,
   "doi": "10.21437/Interspeech.2025-1876",
   "url": "interspeech_2025/zhao25j_interspeech.html"
  },
  "so25_interspeech": {
   "authors": [
    [
     "Ka Ki",
     "SO"
    ],
    [
     "Chenzi",
     "Xu"
    ],
    [
     "Grace Wenling",
     "Cao"
    ],
    [
     "Peggy",
     "Mok"
    ]
   ],
   "title": "Performance of Montreal Forced Aligner on Cantonese Spontaneous Speech",
   "original": "1882",
   "order": 1101,
   "page_count": 5,
   "abstract": [
    "The study presents a comprehensive evaluation of the Montreal Forced Aligner (MFA) in aligning phone boundaries of Hong Kong Cantonese (HKC) spontaneous speech. We developed two tailored Cantonese MFA models, designed to address distinct Cantonese phonetic features, such as checked syllables. These models were applied to align the same set of recordings from spontaneous interviews, and their performance was compared against human annotations. Our results reveal that the updated Cantonese MFA models achieved decent alignment accuracy on spontaneous speech, with a satisfactory level of agreement with manually adjusted boundaries in vowels. However, Cantonese-specific features and connected speech process remain major challenges for the current models. This observation allows us to propose specific amendments to the models to improve alignment performance, as well as recommendations on manual boundary adjustments."
   ],
   "p1": 5398,
   "pn": 5402,
   "doi": "10.21437/Interspeech.2025-1882",
   "url": "interspeech_2025/so25_interspeech.html"
  },
  "vangysel25_interspeech": {
   "authors": [
    [
     "Christophe",
     "Van Gysel"
    ],
    [
     "Maggie",
     "Wu"
    ],
    [
     "Lyan",
     "Verwimp"
    ],
    [
     "Caglar",
     "Tirkaz"
    ],
    [
     "Marco",
     "Bertola"
    ],
    [
     "Zhihong",
     "Lei"
    ],
    [
     "Youssef",
     "Oualil"
    ]
   ],
   "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction",
   "original": "1885",
   "order": 751,
   "page_count": 5,
   "abstract": [
    "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system&#x27;s training data, and hence, may suffer poor recognition.\n",
    "In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model&#x27;s output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.\n",
    "We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines."
   ],
   "p1": 3673,
   "pn": 3677,
   "doi": "10.21437/Interspeech.2025-1885",
   "url": "interspeech_2025/vangysel25_interspeech.html"
  },
  "dai25c_interspeech": {
   "authors": [
    [
     "Yuhang",
     "Dai"
    ],
    [
     "He",
     "Wang"
    ],
    [
     "Xingchen",
     "Li"
    ],
    [
     "Zihan",
     "Zhang"
    ],
    [
     "Shuiyuan",
     "Wang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Xin",
     "Xu"
    ],
    [
     "Hongxiao",
     "Guo"
    ],
    [
     "Shaoji",
     "Zhang"
    ],
    [
     "Hui",
     "Bu"
    ],
    [
     "Wei",
     "Chen"
    ]
   ],
   "title": "AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker Speech Dataset for Automatic Speech Diarization and Recognition",
   "original": "1886",
   "order": 1120,
   "page_count": 5,
   "abstract": [
    "This paper delineates AISHELL-5, the first open-source in-car multi-channel multi-speaker Mandarin automatic speech recognition (ASR) dataset.  AISHLL-5 includes two parts:  (1) over 100 hours of multi-channel speech data recorded in a hybrid electric vehicle across more than 60 real driving scenarios. This audio data consists of four far-filed speech signals captured by microphones located on each car door, as well as near-field signals obtained from high-fidelity headset microphones worn by each speaker. (2) a collection of 40 hours of real-world environmental noise recordings, which supports the in-car speech data simulation. Moreover, we also provide an open-access, reproducible baseline system based on this dataset. This system features a speech frontend model that employs speech source separation to extract each speaker&#x27;s clean speech from the far-filed signals, along with a speech recognition module that accurately transcribes the content of each individual speaker."
   ],
   "p1": 5493,
   "pn": 5497,
   "doi": "10.21437/Interspeech.2025-1886",
   "url": "interspeech_2025/dai25c_interspeech.html"
  },
  "pahuja25_interspeech": {
   "authors": [
    [
     "Saurav",
     "Pahuja"
    ],
    [
     "Gabriel",
     "Ivucic"
    ],
    [
     "Siqi",
     "Cai"
    ],
    [
     "Dashanka",
     "Da Silva"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "GTAnet: Geometry-Guided Temporal Attention for EEG-Based Sound Source Tracking in Cocktail Party Scenarios",
   "original": "1887",
   "order": 1130,
   "page_count": 5,
   "abstract": [
    "EEG-based sound source tracking in cocktail party scenarios is a challenging task due to the complex acoustic environment. To address this, we propose a novel approach, Geometry-guided Temporal Attention (GTAnet), which integrates the spatial configuration of multi-channel EEG signals with the temporal dynamics of neural activity. GTAnet constructs a geometry-based graph to capture the spatial relationships between electrodes while employing a temporal attention mechanism to highlight key intervals in auditory processing. The results show that GTAnet outperforms baseline models and offers interpretable insights into the neural mechanisms underlying auditory scene analysis."
   ],
   "p1": 5543,
   "pn": 5547,
   "doi": "10.21437/Interspeech.2025-1887",
   "url": "interspeech_2025/pahuja25_interspeech.html"
  },
  "serbest25_interspeech": {
   "authors": [
    [
     "Sanberk",
     "Serbest"
    ],
    [
     "Tijana",
     "Stojkovic"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Andrew",
     "Harper"
    ]
   ],
   "title": "DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration",
   "original": "1889",
   "order": 182,
   "page_count": 5,
   "abstract": [
    "In this work, we propose a full-band real-time speech enhancement system with GAN-based stochastic regeneration. Predictive models focus on estimating the mean of the target distribution, whereas generative models aim to learn the full distribution. This behavior of predictive models may lead to over-suppression, i.e. the removal of speech content. In the literature, it was shown that combining a predictive model with a generative one within the stochastic regeneration framework can reduce the distortion in the output. We use this framework to obtain a real-time speech enhancement system. With 3.58M parameters and a low latency, our system is designed for real-time streaming with a lightweight architecture. Experiments show that our system improves over the first stage in terms of NISQA-MOS metric. Finally, through an ablation study, we show the importance of noisy conditioning in our system. We participated in 2025 Urgent Challenge with our model and later made further improvements."
   ],
   "p1": 878,
   "pn": 882,
   "doi": "10.21437/Interspeech.2025-1889",
   "url": "interspeech_2025/serbest25_interspeech.html"
  },
  "rosero25_interspeech": {
   "authors": [
    [
     "Karen",
     "Rosero"
    ],
    [
     "Ali N",
     "Salman"
    ],
    [
     "Shreeram",
     "Chandra"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Cortney Van’t",
     "Slot"
    ],
    [
     "Alex",
     "Kane"
    ],
    [
     "Rami R",
     "Hallac"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Advancing Pediatric ASR: The Role of Voice Generation in Disordered Speech",
   "original": "1890",
   "order": 589,
   "page_count": 5,
   "abstract": [
    "Zero-shot performance of state-of-the-art automatic speech recognition (ASR) significantly declines on pediatric patients with speech sound disorders (SSDs) due to deviations in phonetic pronunciation. To address this, we train a subject-agnostic ASR system on 77 minutes of pediatric SSD transcribed data, which improved zero-shot ASR by 67.48%. Given the scarcity of data and privacy concerns with children&#x27;s data, we study the suitability of voice conversion (VC) and text-to-speech (TTS) to synthesize disorder-reflective samples. Our ASR system surpassed zero-shot by 71.72% when leveraging TTS and showed potential for privacy preservation when using VC. Notably, pre-training on synthetic samples alone reduces the required real SSD data to 50 minutes (i.e., 65% of the data), while achieving metrics comparable to the model finetuned with all SSD samples. This study enables ASR technologies to assist individuals with SSDs and facilitates automatic transcription of speech therapy sessions."
   ],
   "p1": 2890,
   "pn": 2894,
   "doi": "10.21437/Interspeech.2025-1890",
   "url": "interspeech_2025/rosero25_interspeech.html"
  },
  "ofaolain25_interspeech": {
   "authors": [
    [
     "Cathal",
     "Ó Faoláin"
    ],
    [
     "Andrew",
     "Hines"
    ]
   ],
   "title": "Attention Models and Auditory Transduction Features for Noise Robustness",
   "original": "1892",
   "order": 698,
   "page_count": 5,
   "abstract": [
    "Human abilities surpass current speech processing systems in complex, noisy environments. While popular inputs for Automatic Speech Recognition (ASR) systems, such as raw acoustic signals and Mel spectrograms, perform well in quiet conditions, their effectiveness declines in noise. A recently developed generative WaveNet-based model emulates human auditory transduction in real time, offering alternative input features through its “IHCogram” outputs. We investigate these IHCograms across various Signal-to-Noise ratios (SNRs) using state-of-the-art feature encoders. Our findings show that IHCograms significantly enhance phoneme recognition in noisy conditions with minimal computational overhead, regardless of the model encoder used. Additionally, we introduce our Attention Feature Encoder (AFE) models, which leverage the channel structure of IHCograms and demonstrate superior size and performance compared to existing feature encoders."
   ],
   "p1": 3434,
   "pn": 3438,
   "doi": "10.21437/Interspeech.2025-1892",
   "url": "interspeech_2025/ofaolain25_interspeech.html"
  },
  "do25_interspeech": {
   "authors": [
    [
     "Truong",
     "Do"
    ],
    [
     "Minh-Phuong",
     "Nguyen"
    ],
    [
     "Le -Minh",
     "Nguyen"
    ]
   ],
   "title": "PruneSLU: Efficient On-device Spoken Language Understanding through Vocabulary and Structural Pruning",
   "original": "1893",
   "order": 355,
   "page_count": 5,
   "abstract": [
    "Recent advancements in Spoken Language Understanding (SLU) have been driven by pre-trained speech processing models. However, deploying these models on resource-constrained devices remains challenging due to their large parameter sizes. This paper presents PruneSLU, a new method for compressing pre-trained SLU models while maintaining performance. Our approach combines vocabulary pruning and structural layer-wise pruning to reduce model size while preserving essential knowledge. After pruning, the model undergoes knowledge refinement using integration distillation and contrastive learning. Experiments on the STOP and SLURP datasets demonstrate that PruneSLU compresses a 39M model to 15M while retaining 98\\% of its original performance, outperforming previous compression techniques."
   ],
   "p1": 1738,
   "pn": 1742,
   "doi": "10.21437/Interspeech.2025-1893",
   "url": "interspeech_2025/do25_interspeech.html"
  },
  "urai25_interspeech": {
   "authors": [
    [
     "Ticho",
     "Urai"
    ],
    [
     "Pachara",
     "Boonsarngsuk"
    ],
    [
     "Ekapol",
     "Chuangsuwanich"
    ]
   ],
   "title": "Thai Speech Spoofing Detection Dataset with Variations in Speaking Styles",
   "original": "1895",
   "order": 1150,
   "page_count": 5,
   "abstract": [
    "We develop the Chula Spoofed Speech (CSS) dataset, a spoofing dataset for Thai, which contains 1,332,120 utterances of both bona fide and synthetic speech. Synthetic speech samples were generated using five distinct high-quality text-to-speech (TTS) systems, all based on the same utterances as the bona fide data. The data covers various age ranges and speaking styles. Strong baselines such as AASIST and RawNet2 are trained under different conditions to uncover aspects that affect the performance of the models. Besides unseen attacks, unseen speaking styles also have a big impact on performance, indicating a need for diversity in speaking styles in anti-spoofing datasets. Furthermore, we investigate the models in telephony scenarios against additional TTS systems. The results reveal that the models still face certain challenges in this context."
   ],
   "p1": 5643,
   "pn": 5647,
   "doi": "10.21437/Interspeech.2025-1895",
   "url": "interspeech_2025/urai25_interspeech.html"
  },
  "grossman25_interspeech": {
   "authors": [
    [
     "Raymond",
     "Grossman"
    ],
    [
     "Taejin",
     "Park"
    ],
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "Andrew",
     "Titus"
    ],
    [
     "Sophia",
     "Zhi"
    ],
    [
     "Yulia",
     "Shchadilova"
    ],
    [
     "Weiqing",
     "Wang"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription",
   "original": "1896",
   "order": 826,
   "page_count": 5,
   "abstract": [
    "We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged transcription in the financial domain. SPGISpeech 2.0 improves the diversity of applicable modeling tasks while maintaining the core characteristic of the original SPGISpeech dataset: audio snippets and their corresponding fully formatted text transcriptions, usable for end-to-end automatic speech recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of professionally transcribed earnings calls. Furthermore, the dataset contains call and speaker information for each audio snippet facilitating multi-talker ASR. We validate the utility of SPGISpeech 2.0 through improvements in speaker-tagged ASR performance of popular speech recognition models after fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect SPGISpeech 2.0 to foster advancements in speech recognition technologies and inspire a wide range of research applications."
   ],
   "p1": 4048,
   "pn": 4052,
   "doi": "10.21437/Interspeech.2025-1896",
   "url": "interspeech_2025/grossman25_interspeech.html"
  },
  "mohamedismailyasararafath25_interspeech": {
   "authors": [
    [
     "Karumannil",
     "Mohamed Ismail Yasar Arafath"
    ],
    [
     "Mohammed Abeer",
     "K. C."
    ],
    [
     "Aurobinda",
     "Routray"
    ]
   ],
   "title": "A Naturally Elicited Multimodal Stress Database and Speech Breathing Based Stress Detection",
   "original": "1902",
   "order": 921,
   "page_count": 5,
   "abstract": [
    "Stress is a natural physiological and psychological response to demanding situations, but accurate detection remains challenging in real-world scenarios. Existing stress databases are limited and often recorded under extreme conditions or acted, which makes real-world applications difficult. This study introduces a multi-modal stress database, naturally elicited through student evaluations, incorporating normal and thermal video alongside speech recordings annotated by psychologists. While physiological signals provide robust stress detection, speech and facial expressions are more accessible. We extract physiological parameters from speech to bridge this gap by automatically detecting breath locations. These breath indices are then used to derive speech breathing parameters. Machine learning techniques classify these parameters for stress detection. Our proposed method marginally outperforms conventional speech-based features, demonstrating a promising approach to stress analysis."
   ],
   "p1": 4523,
   "pn": 4527,
   "doi": "10.21437/Interspeech.2025-1902",
   "url": "interspeech_2025/mohamedismailyasararafath25_interspeech.html"
  },
  "buech25_interspeech": {
   "authors": [
    [
     "Philipp",
     "Buech"
    ],
    [
     "Anne",
     "Hermes"
    ],
    [
     "Rachid",
     "Ridouane"
    ]
   ],
   "title": "Equivalence and differences: Formant patterns of labialization and pharyngealization in Tashlhiyt",
   "original": "1905",
   "order": 969,
   "page_count": 5,
   "abstract": [
    "Labialization and pharyngealization are secondary articulations that are produced with different vocal tract modifications. In Tashlhiyt, a language that has both secondary articulations in its consonant system, labialization is produced with lip protrusion and a backing of the tongue, while pharyngealization is articulated by a backward movement of the tongue towards the pharynx. Despite this difference, both secondary articulations are reported to have similar acoustic effects regarding F2 across languages. The current study investigates the impact of labialization and pharyngealization on the formant structure of adjacent vowels in the same language. The analysis of VCV logatomes containing /i, a, u/ produced by 35 Tashlhiyt speakers revealed that both secondary articulations show equivalent patterns related to F2, which are strongest in /i/, followed by /a/, whereas differences between labialization and pharyngealization depend mainly on F1 and vowel quality."
   ],
   "p1": 4763,
   "pn": 4767,
   "doi": "10.21437/Interspeech.2025-1905",
   "url": "interspeech_2025/buech25_interspeech.html"
  },
  "shen25b_interspeech": {
   "authors": [
    [
     "Gaofei",
     "Shen"
    ],
    [
     "Hosein",
     "Mohebbi"
    ],
    [
     "Arianna",
     "Bisazza"
    ],
    [
     "Afra",
     "Alishahi"
    ],
    [
     "Grzegorz",
     "Chrupala"
    ]
   ],
   "title": "On the reliability of feature attribution methods for speech classification",
   "original": "1911",
   "order": 55,
   "page_count": 5,
   "abstract": [
    "As the capabilities of large-scale pre-trained models evolve, understanding the determinants of their outputs becomes more important. Feature attribution aims to reveal which parts of the input elements contribute the most to model outputs. In speech processing, the unique characteristics of the input signal make the application of feature attribution methods challenging. We study how factors such as input type and aggregation and perturbation timespan impact the reliability of standard feature attribution methods, and how these factors interact with characteristics of each classification task.  We find that standard approaches to feature attribution are generally unreliable when applied to the speech domain, with the exception of word-aligned perturbation methods when applied to word-based classification tasks."
   ],
   "p1": 266,
   "pn": 270,
   "doi": "10.21437/Interspeech.2025-1911",
   "url": "interspeech_2025/shen25b_interspeech.html"
  },
  "ick25_interspeech": {
   "authors": [
    [
     "Christopher",
     "Ick"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses",
   "original": "1912",
   "order": 193,
   "page_count": 5,
   "abstract": [
    "The characteristics of a sound field are intrinsically linked to the geometric and spatial properties of the environment surrounding a sound source and a listener. The physics of sound propagation is captured in a time-domain signal known as a room impulse response (RIR). Prior work using neural fields (NFs) has allowed learning spatially-continuous representations of RIRs from finite RIR measurements. However, previous NF-based methods have focused on monaural omnidirectional or at most binaural listeners, which does not precisely capture the directional characteristics of a real sound field at a single point. We propose a direction-aware neural field (DANF) that more explicitly incorporates the directional information by Ambisonic-format RIRs. While DANF inherently captures spatial relations between sources and listeners, we further propose a direction-aware loss. In addition, we investigate the ability of DANF to adapt to new rooms in various ways including low-rank adaptation."
   ],
   "p1": 933,
   "pn": 937,
   "doi": "10.21437/Interspeech.2025-1912",
   "url": "interspeech_2025/ick25_interspeech.html"
  },
  "wang25w_interspeech": {
   "authors": [
    [
     "Yi",
     "Wang"
    ],
    [
     "Oli Danyi",
     "Liu"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models",
   "original": "1913",
   "order": 557,
   "page_count": 5,
   "abstract": [
    "Human speech perception is multimodal. In natural speech, lip movements can precede corresponding voicing by a non-negligible gap of 100-300 ms, especially for specific consonants, affecting the time course of neural phonetic encoding in human listeners. However, it remains unexplored whether self-supervised learning models, which have been used to simulate audio-visual integration in humans, can capture this asynchronicity between audio and visual cues. We compared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using linear classifiers to track their phonetic decodability over time. We found that phoneme information becomes available in AV-HuBERT embeddings only about 20 ms before HuBERT, likely due to AV-HuBERT&#x27;s lower temporal resolution and feature concatenation process. It suggests AV-HuBERT does not adequately capture the temporal dynamics of multimodal speech perception, limiting its suitability for modeling the multimodal speech perception process."
   ],
   "p1": 2730,
   "pn": 2734,
   "doi": "10.21437/Interspeech.2025-1913",
   "url": "interspeech_2025/wang25w_interspeech.html"
  },
  "scheck25_interspeech": {
   "authors": [
    [
     "Kevin",
     "Scheck"
    ],
    [
     "Tom",
     "Dombeck"
    ],
    [
     "Zhao",
     "Ren"
    ],
    [
     "Peter",
     "Wu"
    ],
    [
     "Michael",
     "Wand"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "DiffMV-ETS: Diffusion-based Multi-Voice Electromyography-to-Speech Conversion using Speaker-Independent Speech Training Targets",
   "original": "1914",
   "order": 1136,
   "page_count": 5,
   "abstract": [
    "Electromyography (EMG) signals have been investigated for novel voice prostheses to enable speech communication with silent articulation. In this work, we propose DiffMV-ETS, a multi-voice, diffusion-based EMG-to-speech system that converts EMG signals to speech in selectable voices. We evaluate it for scenarios where no speech of the speaker wearing EMG sensors is used for training. For this purpose, we introduce EMG-VCTK, a dataset containing EMG and audio recordings of sentences from the Voice Conversion Tool Kit corpus. We compare EMG models trained with audio of the same speaker, of auxiliary speakers, and of text-to-speech systems. Experiments indicate that models retain their intelligibility and naturalness when trained with synthetic speech. DiffMV-ETS enhances the speech naturalness and similarity to unseen voices. To the best of our knowledge, this is the first work to train multi-voice EMG-to-speech systems with speaker-independent targets."
   ],
   "p1": 5573,
   "pn": 5577,
   "doi": "10.21437/Interspeech.2025-1914",
   "url": "interspeech_2025/scheck25_interspeech.html"
  },
  "mai25b_interspeech": {
   "authors": [
    [
     "Jialong",
     "Mai"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Weidong",
     "Chen"
    ],
    [
     "Yuanbo",
     "Fang"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "AA-SLLM: An Acoustically Augmented Speech Large Language Model for Speech Emotion Recognition",
   "original": "1915",
   "order": 882,
   "page_count": 5,
   "abstract": [
    "Recently, the rapid advancements in Speech Large Language Models (SpeechLLMs) have greatly accelerated progress in the Speech Emotion Recognition (SER) field. However, SpeechLLMs rely on powerful semantic encoders and acoustically irrelevant pre-training data, granting limited attention to acoustic information, which is closely related to the emotion in speech. In this paper, we leverage acoustic properties correlated with emotions to automatically generate acoustic descriptions. These descriptions are combined with the semantic representations as inputs to the LLM, enhancing emotion recognition capabilities. Accordingly, we propose AA-SLLM, an acoustically augmented SpeechLLM adopting instruction fine-tuning via Low-Rank Adaptation (LoRA). Experimental results indicate that AA-SLLM effectively alleviates the class imbalance problem while improving overall performance. Furthermore, AA-SLLM achieves state-of-the-art results on IEMOCAP, MELD, and LSSED datasets."
   ],
   "p1": 4328,
   "pn": 4332,
   "doi": "10.21437/Interspeech.2025-1915",
   "url": "interspeech_2025/mai25b_interspeech.html"
  },
  "valente25_interspeech": {
   "authors": [
    [
     "Ana",
     "Valente"
    ],
    [
     "Rufael",
     "Marew"
    ],
    [
     "Hawau",
     "Toyin"
    ],
    [
     "Hamdan",
     "Al-Ali"
    ],
    [
     "Anelise",
     "Bohnen"
    ],
    [
     "Inma",
     "Becerra"
    ],
    [
     "Elsa",
     "Soares"
    ],
    [
     "Gonçalo",
     "Leal"
    ],
    [
     "Hanan",
     "Aldarmaki"
    ]
   ],
   "title": "Clinical Annotations for Automatic Stuttering Severity Assessment",
   "original": "1916",
   "order": 880,
   "page_count": 5,
   "abstract": [
    "Stuttering is a complex disorder that requires specialized expertise for effective assessment and treatment. This paper presents an effort to enhance the FluencyBank dataset with a new stuttering annotation scheme based on established clinical standards. To achieve high-quality annotations, we hired expert clinicians to label the data, ensuring that the resulting annotations mirror real-world clinical expertise. The annotations are multi-modal, incorporating audiovisual features for the detection and classification of stuttering moments, secondary behaviors, and tension scores. In addition to individual annotations, we additionally provide a test set with highly reliable annotations based on expert consensus for assessing individual annotators and machine learning models. Our experiments and analysis illustrate the complexity of this task that necessitates extensive clinical expertise for valid training and evaluation of stuttering assessment models."
   ],
   "p1": 4318,
   "pn": 4322,
   "doi": "10.21437/Interspeech.2025-1916",
   "url": "interspeech_2025/valente25_interspeech.html"
  },
  "sasu25_interspeech": {
   "authors": [
    [
     "David",
     "Sasu"
    ],
    [
     "Natalie",
     "Schluter"
    ]
   ],
   "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition",
   "original": "1918",
   "order": 963,
   "page_count": 5,
   "abstract": [
    "We show the performance of Automatic Speech Recognition (ASR) systems that use semi-supervised speech representations can be boosted by a complimentary pitch accent detection module, by introducing a joint ASR and pitch accent detection model. The pitch accent detection component of our model achieves a significant improvement on the state-of-the-art for the task, closing the gap in F1-score by 41%. Additionally, the ASR performance in joint training decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With these results, we show the importance of extending pretrained speech models to retain or re-learn important prosodic cues such as pitch accent."
   ],
   "p1": 4733,
   "pn": 4737,
   "doi": "10.21437/Interspeech.2025-1918",
   "url": "interspeech_2025/sasu25_interspeech.html"
  },
  "alexos25_interspeech": {
   "authors": [
    [
     "Antonios",
     "Alexos"
    ],
    [
     "Raghuveer",
     "Peri"
    ],
    [
     "Sai Muralidhar",
     "Jayanthi"
    ],
    [
     "Metehan",
     "Cekic"
    ],
    [
     "Srikanth",
     "Vishnubhotla"
    ],
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Srikanth",
     "Ronanki"
    ]
   ],
   "title": "Defending Speech-enabled LLMs Against Adversarial Jailbreak Threats",
   "original": "1921",
   "order": 417,
   "page_count": 5,
   "abstract": [
    "The additional modality (such as speech) in multimodal large language models (LLM) increases the vulnerability of these models to adversarial jailbreak attacks. Adversarial training (AT) techniques have shown great promise as defenses in traditional adversarial robustness literature. But they are less explored as countermeasures in speech-enabled LLMs due to the limited availability of training data and computational complexity. In this work, we develop AT techniques tailored to speech LLMs using a combination of synthesized harmful and benign queries. We experiment with different training data configurations, and evaluate the methods on strong white-box adversarial attacks. We demonstrate through extensive ablations that using just 4hrs of harmful speech queries for AT (with 150 hours of benign speech) can provide significant gains compared to vanilla safety fine-tuning, improving safety by 45%-300% relative depending on the model."
   ],
   "p1": 2048,
   "pn": 2052,
   "doi": "10.21437/Interspeech.2025-1921",
   "url": "interspeech_2025/alexos25_interspeech.html"
  },
  "coppietersdegibson25_interspeech": {
   "authors": [
    [
     "Louise",
     "Coppieters de Gibson"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Exploring auditory feedback mechanisms in speech recognition",
   "original": "1924",
   "order": 965,
   "page_count": 5,
   "abstract": [
    "For many years, automatic speech recognition (ASR) has been built on compressed filter-bank features understood to be a rough model of the cochlea.  However, recent understanding, evidenced by oto-acoustic emissions, is that the cochlea is composed of driven oscillators. The Hopf mechanism arising from an oscillator model explains the well known cube-root compression. A bifurcation arises from an inner feedback loop from the outer to inner hair cells. Further, larger feedback loops exist along the efferent path of the auditory nerve.  In the present study, to the extent to which current compute power allows, we investigate how to incorporate the Hopf mechanism and the olivocochlear feedback mechanisms into ASR.  Results show that adding this larger feedback loop appears beneficial for ASR.  The results currently have modest implications for ASR, however, such technology could be used to make inference about the biological mechanism."
   ],
   "p1": 4743,
   "pn": 4747,
   "doi": "10.21437/Interspeech.2025-1924",
   "url": "interspeech_2025/coppietersdegibson25_interspeech.html"
  },
  "pierotti25_interspeech": {
   "authors": [
    [
     "Francesco",
     "Pierotti"
    ],
    [
     "Andrea",
     "Bandini"
    ]
   ],
   "title": "Multimodal Assessment of Speech Impairment in Amyotrophic Lateral Sclerosis Using Audio-Visual and Machine Learning Approaches",
   "original": "1931",
   "order": 765,
   "page_count": 5,
   "abstract": [
    "The analysis of speech in individuals with amyotrophic lateral sclerosis is a powerful tool to support clinicians in the assessment of bulbar dysfunction. However, current methods used in clinical practice consist of subjective evaluations or expensive instrumentation. This study investigates different approaches combining audio-visual analysis and machine learning to predict the speech impairment evaluation performed by clinicians. Using a small dataset of acoustic and kinematic features extracted from audio and video recordings of speech tasks, we trained and tested some regression models. The best performance was achieved using the extreme boosting machine regressor with multimodal features, which resulted in a root mean squared error of 0.93 on a scale ranging from 5 to 25. Results suggest that integrating audio-video analysis enhances speech impairment assessment, providing an objective tool for early detection and monitoring of bulbar dysfunction, also in home settings."
   ],
   "p1": 3743,
   "pn": 3747,
   "doi": "10.21437/Interspeech.2025-1931",
   "url": "interspeech_2025/pierotti25_interspeech.html"
  },
  "meng25b_interspeech": {
   "authors": [
    [
     "Yen",
     "Meng"
    ],
    [
     "Sharon",
     "Goldwater"
    ],
    [
     "Hao",
     "Tang"
    ]
   ],
   "title": "Effective Context in Neural Speech Models",
   "original": "1932",
   "order": 51,
   "page_count": 5,
   "abstract": [
    "Modern neural speech models benefit from having longer context, and many approaches have been proposed to increase the maximum context a model can use. However, few have attempted to measure how much context these models actually use, i.e., the effective context. Here, we propose two approaches to measuring the effective context, and use them to analyze different speech Transformers. For supervised models, we find that the effective context correlates well with the nature of the task, with fundamental frequency tracking, phone classification, and word classification requiring increasing amounts of effective context. For self-supervised models, we find that effective context increases mainly in the early layers, and remains relatively short---similar to the supervised phone model. Given that these models do not use a long context during prediction, we show that HuBERT can be run in streaming mode without modification to the architecture and without further fine-tuning."
   ],
   "p1": 246,
   "pn": 250,
   "doi": "10.21437/Interspeech.2025-1932",
   "url": "interspeech_2025/meng25b_interspeech.html"
  },
  "zheng25c_interspeech": {
   "authors": [
    [
     "Junjie",
     "Zheng"
    ],
    [
     "Zihao",
     "Chen"
    ],
    [
     "Chaofan",
     "Ding"
    ],
    [
     "Yunming",
     "Liang"
    ],
    [
     "Yihan",
     "Fan"
    ],
    [
     "Huan",
     "Yang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Xinhan",
     "Di"
    ]
   ],
   "title": "MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing",
   "original": "1933",
   "order": 768,
   "page_count": 5,
   "abstract": [
    "Current movie dubbing technology can produce the desired speech using a reference voice and input video, maintaining perfect synchronization with the visuals while effectively conveying the intended emotions. However, crucial aspects of movie dubbing, including adaptation to various dubbing styles, effective handling of dialogue, narration, and monologues, as well as consideration of subtle details such as speaker age and gender, remain insufficiently explored. To tackle these challenges, we introduce a multi-modal generative framework. First, it utilizes a multi-modal large vision-language model (VLM) to analyze visual inputs, enabling the recognition of dubbing types and fine-grained attributes. Second, it produces high-quality dubbing using large speech generation models, guided by multimodal inputs. Additionally, a movie dubbing dataset with annotations for dubbing types and subtle details is constructed to enhance movie understanding and improve dubbing quality for the proposed multi-modal framework. Experimental results across multiple benchmark datasets show superior performance compared to state-of-the-art (SOTA) methods. In details, the LSED, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to 1.09%, 8.80%, 19.08%, and 18.74%, respectively."
   ],
   "p1": 3758,
   "pn": 3762,
   "doi": "10.21437/Interspeech.2025-1933",
   "url": "interspeech_2025/zheng25c_interspeech.html"
  },
  "maison25_interspeech": {
   "authors": [
    [
     "Lucas",
     "Maison"
    ],
    [
     "Thomas",
     "Soulas"
    ],
    [
     "Marie-Jean",
     "Meurs"
    ]
   ],
   "title": "CEREALES  : a new dataset of Quebec French accented speech with applications to speech recognition",
   "original": "1934",
   "order": 828,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) systems have become widespread and have broad categories of usage and users. It is therefore crucial for ASR models to be robust to speaker variations, particularly accents. Since ASR models perform best on their domain of training, researchers try to improve them by gathering large quantities of accented speech data. However, the availability of such data is often very limited, especially for languages other than English. In this work, we introduce a new corpus of Quebec-accented French that we call the CEREALES dataset. It is a large, gender-balanced corpus of more than 600 hours of spontaneous speech collected during a commission of inquiry in Quebec. We clean the transcripts and align them with the audio, making the dataset useful for both pre-training and fine-tuning models. We share it for the benefit of the research community. In addition, we run ASR experiments and show how this dataset can help address the challenges of accented ASR."
   ],
   "p1": 4058,
   "pn": 4062,
   "doi": "10.21437/Interspeech.2025-1934",
   "url": "interspeech_2025/maison25_interspeech.html"
  },
  "havras25_interspeech": {
   "authors": [
    [
     "Anna",
     "Havras"
    ],
    [
     "Carlos",
     "Mendes"
    ],
    [
     "Helena",
     "Moniz"
    ],
    [
     "Gueorgui",
     "Hristovsky"
    ],
    [
     "João",
     "Miranda"
    ]
   ],
   "title": "Exploratory Study of Filled Pauses in Ukrainian Language: Phonetic Properties of Filled Pauses",
   "original": "1938",
   "order": 855,
   "page_count": 5,
   "abstract": [
    "This exploratory study addresses a significant gap in research on filled pauses (FPs) in Ukrainian, an understudied language with limited phonetic studies. By analyzing acoustic features such as fundamental frequency, formants, and duration, this research offers a comprehensive typology of Ukrainian FPs. We analyzed four hours of speech across three types of contexts: broadcast news (BN), guest interviews (GI), and political interviews (PI). We identified the most frequent FPs ([a] 54%, [ɪ] 36%) and documented two previously unreported variants ([na] 0.4%, [mna] 0.4%).  Acoustic analysis confirmed gender-based differences in the vocalic triangle, with female speakers exhibiting greater vowel distinction in FPs.  Our findings revealed that FPs followed regular patterns, primarily composed by generally referred as central vowels and/or nasal consonants, and are highly dependent on both the speaker and context."
   ],
   "p1": 4193,
   "pn": 4197,
   "doi": "10.21437/Interspeech.2025-1938",
   "url": "interspeech_2025/havras25_interspeech.html"
  },
  "mayer25_interspeech": {
   "authors": [
    [
     "Paul",
     "Mayer"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Alejandro",
     "Pérez-González-de-Martos"
    ],
    [
     "Angelina",
     "Elizarova"
    ],
    [
     "Lindsey",
     "Vanderlyn"
    ],
    [
     "Dirk",
     "Väth"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis",
   "original": "1940",
   "order": 94,
   "page_count": 5,
   "abstract": [
    "While generative methods have progressed rapidly in recent years, generating expressive prosody for an utterance remains a challenging task in text-to-speech synthesis. This is particularly true for systems that model prosody explicitly through parameters such as pitch, energy, and duration, which is commonly done for the sake of interpretability and controllability. In this work, we investigate the effectiveness of stochastic methods for this task, including Normalizing Flows, Conditional Flow Matching, and Rectified Flows. We compare these methods to a traditional deterministic baseline, as well as to real human realizations. Our extensive subjective and objective evaluations demonstrate that stochastic methods produce natural prosody on par with human speakers by capturing the variability inherent in human speech. Further, they open up additional controllability options by allowing the sampling temperature to be tuned."
   ],
   "p1": 439,
   "pn": 443,
   "doi": "10.21437/Interspeech.2025-1940",
   "url": "interspeech_2025/mayer25_interspeech.html"
  },
  "lalay25_interspeech": {
   "authors": [
    [
     "Louis",
     "Lalay"
    ],
    [
     "Mathieu",
     "Fontaine"
    ],
    [
     "Roland",
     "Badeau"
    ]
   ],
   "title": "Unified Variational and Physics-aware Model for Room Impulse Response Estimation",
   "original": "1942",
   "order": 780,
   "page_count": 5,
   "abstract": [
    "Room impulse response estimation is essential for tasks like speech dereverberation, which improves automatic speech recognition. Most existing methods rely on either statistical signal processing or deep neural networks designed to replicate signal processing principles. However, combining statistical and physical modeling for RIR estimation remains largely unexplored. This paper proposes a novel approach integrating both aspects through a theoretically grounded model. The RIR is decomposed into interpretable parameters: white Gaussian noise filtered by a frequency-dependent exponential decay (e.g. modeling wall absorption) and an autoregressive filter (e.g. modeling microphone response). A variational free-energy cost function enables practical parameter estimation. As a proof of concept, we show that given dry and reverberant speech signals, the proposed method outperforms classical deconvolution in noisy environments, as validated by objective metrics."
   ],
   "p1": 3818,
   "pn": 3822,
   "doi": "10.21437/Interspeech.2025-1942",
   "url": "interspeech_2025/lalay25_interspeech.html"
  },
  "zhang25r_interspeech": {
   "authors": [
    [
     "Theo",
     "Zhang"
    ],
    [
     "Madurya",
     "Suresh"
    ],
    [
     "Anne",
     "Warluamont"
    ],
    [
     "Kasia",
     "Hitczenko"
    ],
    [
     "Alejandrina",
     "Cristia"
    ],
    [
     "Margaret",
     "Cychosz"
    ]
   ],
   "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification",
   "original": "1946",
   "order": 576,
   "page_count": 5,
   "abstract": [
    "Speech technology systems struggle with many downstream tasks for child speech due to small training corpora and the difficulties that child speech pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer models to address a fundamental classification task: identifying child vocalizations. Unlike previous corpora, our dataset captures maximally ecologically-valid child vocalizations across an unprecedented sample, comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu, Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004 labeled vocalizations, magnitudes larger than previous work. Models were trained to distinguish between cry, laughter, mature (consonant+vowel), and immature speech (just consonant or vowel). Models trained on the dataset outperform state-of-the-art models trained on previous datasets, achieved classification accuracy comparable to humans, and were robust across rural and urban settings."
   ],
   "p1": 2825,
   "pn": 2829,
   "doi": "10.21437/Interspeech.2025-1946",
   "url": "interspeech_2025/zhang25r_interspeech.html"
  },
  "joubaud25_interspeech": {
   "authors": [
    [
     "Thomas",
     "Joubaud"
    ],
    [
     "Julien",
     "Hauret"
    ],
    [
     "Véronique",
     "Zimpfer"
    ],
    [
     "Éric",
     "Bavu"
    ]
   ],
   "title": "French Listening Tests for the Assessment of Intelligibility, Quality, and Identity of Body-Conducted Speech Enhancement",
   "original": "1947",
   "order": 1118,
   "page_count": 5,
   "abstract": [
    "This study evaluates the Extreme Bandwidth Extension Network (EBEN) model on body-conduction sensors through listening tests. Using the Vibravox dataset, we assess intelligibility with a French Modified Rhyme Test, speech quality with a MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor) protocol and speaker identity preservation with an A/B identification task. The experiments involved male and female speakers recorded with a forehead accelerometer, rigid in-ear and throat microphones. The results confirm that EBEN enhances both speech quality and intelligibility. It slightly degrades speaker identification performance when applied to female speakers&#x27; throat microphone recordings. The findings also demonstrate a correlation between Short-Time Objective Intelligibility (STOI) and perceived quality in body-conducted speech, while speaker verification using ECAPA2-TDNN aligns well with identification performance. No tested metric reliably predicts EBEN&#x27;s effect on intelligibility."
   ],
   "p1": 5483,
   "pn": 5487,
   "doi": "10.21437/Interspeech.2025-1947",
   "url": "interspeech_2025/joubaud25_interspeech.html"
  },
  "hu25m_interspeech": {
   "authors": [
    [
     "De",
     "Hu"
    ],
    [
     "Shuyao",
     "Liu"
    ],
    [
     "Yanrong",
     "He"
    ]
   ],
   "title": "Joint Reference Microphone Selection and Filter Order Determination in Multi-channel Active Noise Control",
   "original": "1948",
   "order": 511,
   "page_count": 5,
   "abstract": [
    "Multi-channel active noise control (MCANC) systems have been widely investigated for acoustic noise attenuation over a spatial region. In MCANC systems, incorporating more reference microphones (RMs) can boost noise reduction performance but increases computational complexity, which may affect real-time processing capability. To improve computational efficiency, we propose a joint approach for RM selection and filter order (FO) determination in MCANC systems. Under the specified performance constraint, the optimal RM subset is selected by promoting the group sparsity in filter coefficients, and the optimal FO is determined by encouraging the consecutive highest-order filter coefficients to approach zero. By combining the above two criteria, we can simultaneously select the RM subset and  determine the FO. In addition, we present a re-weighting strategy to further reduce both the number of selected RMs and the FO value. Numerical simulations confirm the validity of the proposed method."
   ],
   "p1": 2500,
   "pn": 2504,
   "doi": "10.21437/Interspeech.2025-1948",
   "url": "interspeech_2025/hu25m_interspeech.html"
  },
  "watkins25_interspeech": {
   "authors": [
    [
     "Michaela",
     "Watkins"
    ],
    [
     "Rasmus",
     "Puggaard-Rode"
    ],
    [
     "Paul",
     "Boersma"
    ],
    [
     "Silke",
     "Hamann"
    ]
   ],
   "title": "Robustness of F0 Ratio as a Diagnostic: Comparing Creaky Voice in Danish and Seoul Korean",
   "original": "1949",
   "order": 857,
   "page_count": 5,
   "abstract": [
    "We present an exploratory analysis of F0 ratio, a proposed method to pick up octave jumps in the speech signal. Such jumps, often considered errors, are possibly indicative of the presence of creaky voice. This paper focuses on co-intrinsic voice quality in Seoul Korean fortis stops, building on previous data, and on the Danish contrastive voice quality stød. The results suggest that in Seoul Korean F0 ratio captures a clear jump upwards indicating a modality switch from creaky to modal voice, with a gender difference observed. This suggests that pitch jumps are not necessarily erroneous but may reflect systematic cues to phonological contrasts cued with creak. In Danish, F0 ratio captures a rising intonational contour for non-stød tokens and is able to categorise between stød and non-stød tokens with high accuracy, although this leaves open the question whether F0 ratio captures modality shifts in Danish, or rather a combination of modality shift and pitch contour differences."
   ],
   "p1": 4203,
   "pn": 4207,
   "doi": "10.21437/Interspeech.2025-1949",
   "url": "interspeech_2025/watkins25_interspeech.html"
  },
  "hou25b_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Hou"
    ],
    [
     "Heyang",
     "Liu"
    ],
    [
     "Yuhao",
     "Wang"
    ],
    [
     "Ziyang",
     "Cheng"
    ],
    [
     "Ronghua",
     "Wu"
    ],
    [
     "Qunshan",
     "Gu"
    ],
    [
     "Yanfeng",
     "Wang"
    ],
    [
     "Yu",
     "Wang"
    ]
   ],
   "title": "SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant",
   "original": "1950",
   "order": 1164,
   "page_count": 5,
   "abstract": [
    "Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems."
   ],
   "p1": 5713,
   "pn": 5717,
   "doi": "10.21437/Interspeech.2025-1950",
   "url": "interspeech_2025/hou25b_interspeech.html"
  },
  "burkhardt25_interspeech": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Oliver",
     "Schrüfer"
    ],
    [
     "Uwe",
     "Reichel"
    ],
    [
     "Hagen",
     "Wierstorf"
    ],
    [
     "Anna",
     "Derington"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "EmoDB 2.0: A Database of Emotional Speech in a World that is not Black or White but Grey",
   "original": "1951",
   "order": 914,
   "page_count": 5,
   "abstract": [
    "In 2004, the Berlin Database of Emotional Speech (Berlin EmoDB) has made publicly available, and since then, it has been utilized in numerous studies on emotional speech, with over 3,000 citations. We now extend this database with original material that was previously absent from the distributed versions within the research community. This includes ambiguous samples that were not agreed upon by at least 80 % of the raters, the addition of glottograms for all samples, and a new rater label indicating the perceived naturalness of the samples. The paper provides detailed descriptions and reports on preliminary studies conducted using this extended data. For instance, incorporating glottogram information has been shown to improve the average recall of an SVM classifier by 8.1 % UAR."
   ],
   "p1": 4488,
   "pn": 4492,
   "doi": "10.21437/Interspeech.2025-1951",
   "url": "interspeech_2025/burkhardt25_interspeech.html"
  },
  "gallego25_interspeech": {
   "authors": [
    [
     "Gerard I.",
     "Gállego"
    ],
    [
     "Oriol",
     "Pareras"
    ],
    [
     "Martí",
     "Cortada Garcia"
    ],
    [
     "Lucas",
     "Takanori"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios",
   "original": "1954",
   "order": 8,
   "page_count": 5,
   "abstract": [
    "We propose a Speech-to-Text Translation (S2TT) approach that integrates phoneme representations into a Chain-of-Thought (CoT) framework to improve translation in low-resource and zero-resource settings. By introducing phoneme recognition as an intermediate step, we enhance cross-lingual transfer, enabling translation even for languages with no labeled speech data. Our system builds on a multilingual LLM, which we extend to process speech and phonemes. Training follows a curriculum learning strategy that progressively introduces more complex tasks. Experiments on multilingual S2TT benchmarks show that phoneme-augmented CoT improves translation quality in low-resource conditions and enables zero-resource translation, while slightly impacting high-resource performance. Despite this trade-off, our findings demonstrate that phoneme-based CoT is a promising step toward making S2TT more accessible across diverse languages."
   ],
   "p1": 31,
   "pn": 35,
   "doi": "10.21437/Interspeech.2025-1954",
   "url": "interspeech_2025/gallego25_interspeech.html"
  },
  "do25b_interspeech": {
   "authors": [
    [
     "Huy Ba",
     "Do"
    ],
    [
     "Vy Le-Phuong",
     "Huynh"
    ],
    [
     "Luan Thanh",
     "Nguyen"
    ]
   ],
   "title": "ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances",
   "original": "1958",
   "order": 819,
   "page_count": 5,
   "abstract": [
    "Toxic speech on online platforms is a growing concern, impacting user experience and online safety. While text-based toxicity detection is well-studied, audio-based approaches remain underexplored, especially for low-resource languages like Vietnamese. This paper introduces ViToSA (Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in Vietnamese speech, comprising 11,000 audio samples (25 hours) with accurate human-annotated transcripts. We propose a pipeline that combines ASR and toxic spans detection for fine-grained identification of toxic content. Our experiments show that fine-tuning ASR models on ViToSA significantly reduces WER when transcribing toxic speech, while the text-based toxic spans detection (TSD) models outperform existing baselines. These findings establish a novel benchmark for Vietnamese audio-based toxic spans detection, paving the way for future research in speech content moderation."
   ],
   "p1": 4013,
   "pn": 4017,
   "doi": "10.21437/Interspeech.2025-1958",
   "url": "interspeech_2025/do25b_interspeech.html"
  },
  "shi25f_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Hye-jin",
     "Shim"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
   "original": "1960",
   "order": 367,
   "page_count": 5,
   "abstract": [
    "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment."
   ],
   "p1": 1798,
   "pn": 1802,
   "doi": "10.21437/Interspeech.2025-1960",
   "url": "interspeech_2025/shi25f_interspeech.html"
  },
  "sasu25b_interspeech": {
   "authors": [
    [
     "David",
     "Sasu"
    ],
    [
     "Benedict",
     "Quartey"
    ],
    [
     "Kweku Andoh",
     "Yamoah"
    ],
    [
     "Natalie",
     "Schluter"
    ]
   ],
   "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody",
   "original": "1961",
   "order": 399,
   "page_count": 5,
   "abstract": [
    "Enabling robots to accurately interpret and execute spoken language instructions is essential for effective human-robot collaboration. Traditional methods rely on speech recognition to transcribe speech into text, often discarding crucial prosodic cues needed for disambiguating intent. We propose a novel approach that directly leverages speech prosody to infer and resolve instruction intent. Predicted intents are integrated into large language models via in-context learning to disambiguate and select appropriate task plans. Additionally, we present the first ambiguous speech dataset for robotics, designed to advance research in speech disambiguation. Our method achieves 95.79% accuracy in detecting referent intents within an utterance and determines the intended task plan of ambiguous instructions with 71.96% accuracy, demonstrating its potential to significantly improve human-robot communication."
   ],
   "p1": 1958,
   "pn": 1962,
   "doi": "10.21437/Interspeech.2025-1961",
   "url": "interspeech_2025/sasu25b_interspeech.html"
  },
  "kunze25_interspeech": {
   "authors": [
    [
     "Tarek",
     "Kunze"
    ],
    [
     "Marianne",
     "Métais"
    ],
    [
     "Hadrien",
     "Titeux"
    ],
    [
     "Lucas",
     "Elbert"
    ],
    [
     "Joseph",
     "Coffey"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Alejandrina",
     "Cristia"
    ],
    [
     "Marvin",
     "Lavechin"
    ]
   ],
   "title": "Challenges in Automated Processing of Speech from Child Wearables:  The Case of Voice Type Classifier",
   "original": "1962",
   "order": 580,
   "page_count": 5,
   "abstract": [
    "Recordings gathered with child-worn devices promised to revolutionize both fundamental and applied speech sciences by allowing the effortless capture of children&#x27;s naturalistic speech environment and language production. This promise hinges on speech technologies that can transform the sheer mounds of data thus collected into usable information. This paper demonstrates several obstacles blocking progress by summarizing three years&#x27; worth of experiments aimed at improving one fundamental task: Voice Type Classification. Our experiments suggest that improvements in representation features, architecture, and parameter search contribute to only marginal gains in performance. More progress is made by focusing on data relevance and quantity, which highlights the importance of collecting data with appropriate permissions to allow sharing."
   ],
   "p1": 2845,
   "pn": 2849,
   "doi": "10.21437/Interspeech.2025-1962",
   "url": "interspeech_2025/kunze25_interspeech.html"
  },
  "millot25_interspeech": {
   "authors": [
    [
     "Carole",
     "Millot"
    ],
    [
     "Clara",
     "Ponchard"
    ],
    [
     "Cédric",
     "Gendrot"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Orane",
     "Dufour"
    ]
   ],
   "title": "Using gender, phonation and age to interpret automatically discovered speech attributes for explainable speaker recognition",
   "original": "1963",
   "order": 744,
   "page_count": 5,
   "abstract": [
    "Explainability is particularly necessary for speaker recognition because of its potential forensic applications. A recent approach, BA-LR, offers a new level of explainability. It represents a speech utterance by a binary vector where each coefficient indicates the presence or absence of a given speech attribute. However, the interpretation of these attributes remains unclear, as they are found automatically. The aim of this work is to develop a methodology for interpreting these attributes in terms of comprehensible explicit voice traits such as gender, age and perceived phonation type. Our methodology is based on the assumption that if an attribute is useful for a classification task of an explicit voice trait, this means that this attribute encodes all or part of this trait. Our results show that BA-LR attributes encode, at least partially, gender, age and perceived phonation type. These results pave the way for a comprehensive interpretation of BA-LR speech attributes."
   ],
   "p1": 3638,
   "pn": 3642,
   "doi": "10.21437/Interspeech.2025-1963",
   "url": "interspeech_2025/millot25_interspeech.html"
  },
  "arisoy25_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arisoy"
    ],
    [
     "Merve",
     "Unlu Menevse"
    ],
    [
     "Yusufcan",
     "Manav"
    ],
    [
     "Arzucan",
     "Ozgur"
    ]
   ],
   "title": "Evaluating Large Language Models in Data Generation for Low-Resource Scenarios: A Case Study on Question Answering",
   "original": "1965",
   "order": 362,
   "page_count": 5,
   "abstract": [
    "Large Language Models (LLMs) are powerful tools for generating synthetic data, offering a promising solution to data scarcity in low-resource scenarios. This study evaluates the effectiveness of LLMs in generating question-answer pairs to enhance the performance of question answering (QA) models trained with limited annotated data. While synthetic data generation has been widely explored for text-based QA, its impact on spoken QA remains underexplored. We specifically investigate the role of LLM-generated data in improving spoken QA models, showing performance gains across both text-based and spoken QA tasks. Experimental results on subsets of the SQuAD, Spoken SQuAD, and a Turkish spoken QA dataset demonstrate significant relative F1 score improvements of 7.8%, 7.0%, and 2.7%, respectively, over models trained solely on restricted human-annotated data. Furthermore, our findings highlight the robustness of LLM-generated data in spoken QA settings, even in the presence of noise."
   ],
   "p1": 1773,
   "pn": 1777,
   "doi": "10.21437/Interspeech.2025-1965",
   "url": "interspeech_2025/arisoy25_interspeech.html"
  },
  "pratapsingh25_interspeech": {
   "authors": [
    [
     "Vishwanath",
     "Pratap Singh"
    ],
    [
     "Md",
     "Sahidullah"
    ],
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "Causal Structure Discovery for Error Diagnostics of Children's ASR",
   "original": "1967",
   "order": 591,
   "page_count": 5,
   "abstract": [
    "Children’s automatic speech recognition (ASR) often underperforms compared to that of adults due to a confluence of interdependent factors: physiological (e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation), and extrinsic (e.g., vocabulary limitations, background noise). Existing analysis methods examine the impact of these factors in isolation, neglecting interdependencies—such as age affecting ASR accuracy both directly and indirectly via pronunciation skills. In this paper, we introduce a causal structure discovery to unravel these interdependent relationships among physiology, cognition, extrinsic factors, and ASR errors. Then, we employ causal quantification to measure each factor’s impact on children’s ASR. We extend the analysis to fine-tuned models to identify which factors are mitigated by fine-tuning and which remain largely unaffected. Experiments on Whisper and Wav2Vec2.0 demonstrate the generalizability of our findings across different ASR systems."
   ],
   "p1": 2900,
   "pn": 2904,
   "doi": "10.21437/Interspeech.2025-1967",
   "url": "interspeech_2025/pratapsingh25_interspeech.html"
  },
  "deng25b_interspeech": {
   "authors": [
    [
     "Qingkun",
     "Deng"
    ],
    [
     "Saturnino",
     "Luz"
    ],
    [
     "Sofia",
     "de la Fuente Garcia"
    ]
   ],
   "title": "An interpretable speech foundation model for depression detection by revealing prediction-relevant acoustic features from long speech",
   "original": "1968",
   "order": 1070,
   "page_count": 5,
   "abstract": [
    "Speech-based depression detection tools could aid early screening. Here, we propose an interpretable speech foundation model approach to enhance the clinical applicability of such tools. We introduce a speech-level Audio Spectrogram Transformer (AST) to detect depression using long-duration speech instead of short segments, along with a novel interpretation method that reveals prediction-relevant acoustic features for clinician interpretation. Our experiments show the proposed model outperforms a segment-level AST, highlighting the impact of segment-level labeling noise and the advantage of leveraging longer speech duration for more reliable depression detection. Through interpretation, we observe our model identifies reduced loudness and F0 as relevant depression signals, aligning with documented clinical findings. This interpretability supports a responsible AI approach for speech-based depression detection, rendering such tools more clinically applicable."
   ],
   "p1": 5248,
   "pn": 5252,
   "doi": "10.21437/Interspeech.2025-1968",
   "url": "interspeech_2025/deng25b_interspeech.html"
  },
  "naini25_interspeech": {
   "authors": [
    [
     "Abinay Reddy",
     "Naini"
    ],
    [
     "Lucas",
     "Goncalves"
    ],
    [
     "Ali N.",
     "Salman"
    ],
    [
     "Pravin",
     "Mote"
    ],
    [
     "Ismail R.",
     "Ulgen"
    ],
    [
     "Thomas",
     "Thebaud"
    ],
    [
     "Laureano Moro",
     "Velazquez"
    ],
    [
     "Leibny Paola",
     "Garcia"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "The Interspeech 2025 Challenge on Speech Emotion Recognition in Naturalistic Conditions",
   "original": "1972",
   "order": 950,
   "page_count": 5,
   "abstract": [
    "The Speech Emotion Recognition in Naturalistic Conditions Challenge, part of Interspeech 2025, builds on previous efforts to advance Speech Emotion Recognition (SER) in real-world scenarios. The focus is on recognizing emotions from spontaneous speech, moving beyond controlled datasets. It provides a framework for speaker-independent training, development, and evaluation, with annotations for both categorical and dimensional tasks. The challenge attracted a record number of participants, significantly increasing submissions and benchmarking performances, leading to state-of-the-art results. This paper summarizes key outcomes, analyzing top-performing methods, emerging trends, and innovative directions. We highlight the effectiveness of combining audio and text-based foundational models to achieve robust SER systems. The competition website, with leaderboards, baseline code, and instructions, is available at: \\url{https://lab-msp.com/MSP-Podcast_Competition/IS2025/}."
   ],
   "p1": 4668,
   "pn": 4672,
   "doi": "10.21437/Interspeech.2025-1972",
   "url": "interspeech_2025/naini25_interspeech.html"
  },
  "serditova25_interspeech": {
   "authors": [
    [
     "Dana",
     "Serditova"
    ],
    [
     "Kevin",
     "Tang"
    ],
    [
     "Jochen",
     "Steffens"
    ]
   ],
   "title": "Automatic Speech Recognition Biases in Newcastle English: an Error Analysis",
   "original": "1973",
   "order": 652,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems struggle with regional dialects due to biased training which favours mainstream varieties. While previous research has identified racial, age, and gender biases in ASR, regional bias remains underexamined. This study investigates ASR performance on Newcastle English from 160 speakers, a well-documented regional dialect known to be challenging for ASR. A two-stage analysis was conducted: first, a manual error analysis on a subsample identified key phonological, lexical, and morphosyntactic errors behind ASR misrecognitions; second, a case study focused on the systematic analysis of ASR recognition of the regional pronouns “yous” and “wor”. Results show that ASR errors directly correlate with regional dialectal features, while social factors play a lesser role in ASR mismatches. We advocate for greater dialectal diversity in ASR training data and highlight the value of sociolinguistic analysis in diagnosing and addressing regional biases."
   ],
   "p1": 3204,
   "pn": 3208,
   "doi": "10.21437/Interspeech.2025-1973",
   "url": "interspeech_2025/serditova25_interspeech.html"
  },
  "ducceschi25_interspeech": {
   "authors": [
    [
     "Luca",
     "Ducceschi"
    ],
    [
     "Greta H.",
     "Franzini"
    ]
   ],
   "title": "Speech transcription from South Tyrolean Dialect to Standard German with Whisper",
   "original": "1976",
   "order": 2,
   "page_count": 5,
   "abstract": [
    "This study presents the first fine-tuned Whisper model for the automatic translation of South Tyrolean dialectal speech into Standard German text. To address an unmet need for subtitling and translation, we introduce a small corpus of manually annotated and synthetic speech data compiled for this task. Through fine-tuning and hyperparameter optimisation, our model achieves a BLEU score of 86.18 significantly outperforming baseline error rates. Our findings highlight Whisper&#x27;s effectiveness in handling dialectal speech, contributing to low-resource language research. The model is already being used in a heritage collaboration for large-scale translation of audiovisual archival material and is also being considered for application in news broadcasting and tourism promotion. Future directions include expanding the training data and extending hyperparameter optimisation to improve the model&#x27;s performance and generalisation across South Tyrolean dialectal variations."
   ],
   "p1": 1,
   "pn": 5,
   "doi": "10.21437/Interspeech.2025-1976",
   "url": "interspeech_2025/ducceschi25_interspeech.html"
  },
  "huang25g_interspeech": {
   "authors": [
    [
     "Wen-Chin",
     "Huang"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "SHEET: A Multi-purpose Open-source Speech Human Evaluation Estimation Toolkit",
   "original": "1977",
   "order": 482,
   "page_count": 5,
   "abstract": [
    "We introduce SHEET, a multi-purpose open-source toolkit designed to accelerate subjective speech quality assessment (SSQA) research. SHEET stands for the Speech Human Evaluation Estimation Toolkit, which focuses on data-driven deep neural network-based models trained to predict human-labeled quality scores of speech samples. SHEET provides comprehensive training and evaluation scripts, multi-dataset and multi-model support, as well as pre-trained models accessible via Torch Hub and HuggingFace Spaces. To demonstrate its capabilities, we re-evaluated SSL-MOS, a speech self-supervised learning (SSL)-based SSQA model widely used in recent scientific papers, on an extensive list of speech SSL models. Experiments were conducted on two representative SSQA datasets named BVCC and NISQA, and we identified the optimal speech SSL model, whose performance surpassed the original SSL-MOS implementation and was comparable to state-of-the-art methods."
   ],
   "p1": 2355,
   "pn": 2359,
   "doi": "10.21437/Interspeech.2025-1977",
   "url": "interspeech_2025/huang25g_interspeech.html"
  },
  "mai25c_interspeech": {
   "authors": [
    [
     "Jialong",
     "Mai"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Yangbiao",
     "Li"
    ],
    [
     "Xiangmin",
     "Xu"
    ]
   ],
   "title": "Chain-of-Thought Distillation with Fine-Grained Acoustic Cues for Speech Emotion Recognition",
   "original": "1979",
   "order": 1109,
   "page_count": 5,
   "abstract": [
    "Recent advances in large language models (LLMs) have demonstrated strong reasoning abilities through chain-of-thought (CoT) prompting, yet their application in speech emotion recognition (SER) remains underexplored. Moreover, current SER models lack explainability based on emotion-related acoustic features. We propose AECoTD, a method that transfers reasoning abilities from a large LLM to a domain-specific SER LLM by leveraging fine-grained emotional acoustic features and text transcripts. It uses LoRA to distill the reasoning chain and an emotion-focused loss to preserve correct emotional attention, thereby enhancing the model’s explainability. Ablation experiments highlight the impact of fine-grained acoustic information, emotional CoT reasoning, and emotion-focused loss. Without using pre-trained representations, our method achieves state-of-the-art performance both in-domain and out-of-domain, demonstrating strong generalization ability."
   ],
   "p1": 5438,
   "pn": 5442,
   "doi": "10.21437/Interspeech.2025-1979",
   "url": "interspeech_2025/mai25c_interspeech.html"
  },
  "weilinghoff25_interspeech": {
   "authors": [
    [
     "Andreas",
     "Weilinghoff"
    ]
   ],
   "title": "Transcribing Diverse Voices: Using Whisper for ICE corpora",
   "original": "1980",
   "order": 683,
   "page_count": 5,
   "abstract": [
    "The precise transcription of speech data is crucial yet work-intensive in the field of sociolinguistics. Although recent advancements in end-to-end ASR (e.g. Whisper) offer great potential across various disciplines, these models have rarely been tested for sociolinguistic corpus transcription. This study addresses this gap by harnessing all Whisper models for the re-transcription of classic sociolinguistic reference corpora of non-standard varieties: ICE Nigeria and ICE Scotland. Employing WER metrics, the study utilizes linear mixed-effects modelling to determine significant factors affecting transcription accuracy. The results show that Whisper can manage both varieties, though it is slightly less accurate for Nigerian English. An increased model size reduces WER and boosts robustness, though accuracy varies by sound file. While Whisper proves useful for corpus transcription work overall, challenges such as speaker diarization, hallucinations and idealized transcriptions persist."
   ],
   "p1": 3359,
   "pn": 3363,
   "doi": "10.21437/Interspeech.2025-1980",
   "url": "interspeech_2025/weilinghoff25_interspeech.html"
  },
  "johnson25_interspeech": {
   "authors": [
    [
     "Alexander",
     "Johnson"
    ],
    [
     "Harsh",
     "Deshpande"
    ],
    [
     "Emmy",
     "Phung"
    ],
    [
     "Ahmad",
     "Emami"
    ]
   ],
   "title": "An Exploratory Framework for LLM-assisted Human Annotation of Speech Datasets",
   "original": "1983",
   "order": 867,
   "page_count": 5,
   "abstract": [
    "We introduce a framework for LLM-based human-in-the-loop ASR designed to enhance the quality of ASR transcripts, with a particular focus on accurately capturing named entities. A key contribution of this work is demonstrating that when LLMs are provided with high-quality, human-annotated transcript examples, even a small set can significantly improve WER and entity recall rates. Our framework drastically reduces the costly need for human annotation to just 5% of the entire call. Our framework outperforms all baselines, including out-of-the-box Whisper and Whisper with a zero-shot GPT corrector. In this work, we derive insights on how a chain-of-thought framework can effectively utilize LLM prompts and human input to improve speech data annotation quality. Using our framework, we achieve a 10% or greater relative improvement in WER and entity F1 score over the baseline with a minimal amount of human effort."
   ],
   "p1": 4253,
   "pn": 4257,
   "doi": "10.21437/Interspeech.2025-1983",
   "url": "interspeech_2025/johnson25_interspeech.html"
  },
  "griot25_interspeech": {
   "authors": [
    [
     "Nathan",
     "Griot"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Raphael",
     "Blouet"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Ana",
     "Mantecon"
    ]
   ],
   "title": "Unified Text and Speaker Verification using SSL model for Text-Dependent Speaker Verification",
   "original": "1984",
   "order": 336,
   "page_count": 5,
   "abstract": [
    "The work presented in this article falls within text-dependent speaker recognition. In our framework, each speaker owns and pronounces a secret phrase. It corresponds to two tasks: verification of the spoken text (Text Validation) and  verification of the speaker&#x27;s identity (SV). These tasks are usually carried-out in tandem by two different systems. Maintaining two systems involves a certain amount of complexity and may present shortcomings in terms of reliability. In this paper, we propose to use a Self-Supervised Learning Model (SSL) to develop a unified system capable of performing both tasks simultaneously. The proposed approach combines two models on a common SSL and takes advantage of a teacher-student paradigm to integrate textual constraints into the SV part, without requiring lexical labels during its learning phase. Evaluation on different datasets demonstrates the effectiveness of the approach."
   ],
   "p1": 1648,
   "pn": 1652,
   "doi": "10.21437/Interspeech.2025-1984",
   "url": "interspeech_2025/griot25_interspeech.html"
  },
  "duraisamy25_interspeech": {
   "authors": [
    [
     "Saravanakumar",
     "Duraisamy"
    ],
    [
     "Maurice",
     "Rekrut"
    ],
    [
     "Luis A.",
     "Leiva"
    ]
   ],
   "title": "Functional Connectivity and Hilbert-Based Features for Covert Speech EEG Variability Analysis and Classification",
   "original": "1986",
   "order": 594,
   "page_count": 5,
   "abstract": [
    "We explore inter-trial, inter-class, and inter-subject variability in covert speech imagination using Electroencephalogram (EEG) signals. Two key functional connectivity metrics (Phase Locking Value and Coherence) revealed unique and shared activation patterns across speech commands, influenced by individual word perception and affective states. We also propose a subject-independent classification model using Hilbert envelope and instantaneous phase features across EEG frequency bands with a Bidirectional Long Short-Term Memory (BiLSTM) architecture, achieving 59.14% classification accuracy across five speech categories. Our state-of-the-art results in EEG-based speech decoding contribute a new understanding of the neural dynamics underlying imagined speech and affective processing."
   ],
   "p1": 2915,
   "pn": 2919,
   "doi": "10.21437/Interspeech.2025-1986",
   "url": "interspeech_2025/duraisamy25_interspeech.html"
  },
  "peurey25_interspeech": {
   "authors": [
    [
     "Loann",
     "Peurey"
    ],
    [
     "Marvin",
     "Lavechin"
    ],
    [
     "Tarek",
     "Kunze"
    ],
    [
     "Manel",
     "Khentout"
    ],
    [
     "Lucas",
     "Gautheron"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Alejandrina",
     "Cristia"
    ]
   ],
   "title": "Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity",
   "original": "1987",
   "order": 806,
   "page_count": 5,
   "abstract": [
    "Audio-recordings collected with a child-worn device are a fundamental tool in child language research. Long-form recordings collected over whole days promise to capture children&#x27;s input and production with minimal observer bias, and therefore high validity. The sheer volume of resulting data necessitates automated analysis to extract relevant metrics for researchers and clinicians. This paper summarizes collective knowledge on this technique, providing entry points to existing resources. We also highlight various sources of error that threaten the accuracy of automated annotations and the interpretation of resulting metrics. To address this, we propose potential troubleshooting metrics to help users assess data quality. While a fully automated quality control system is not feasible, we outline practical strategies for researchers to improve data collection and contextualize their analyses."
   ],
   "p1": 3948,
   "pn": 3952,
   "doi": "10.21437/Interspeech.2025-1987",
   "url": "interspeech_2025/peurey25_interspeech.html"
  },
  "parvathala25_interspeech": {
   "authors": [
    [
     "Venkatesh",
     "Parvathala"
    ],
    [
     "Ramesh",
     "Gundluru"
    ],
    [
     "Sreekanth",
     "Sankala"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "Exploiting Bispectral Features for Single-Channel Speech Enhancement",
   "original": "1988",
   "order": 488,
   "page_count": 5,
   "abstract": [
    "Deep neural network-based speech enhancement (SE) algorithms often utilize an encoder - dual-path processing module - decoder architecture. The encoder, consisting of pointwise convolutions and a dilated densenet (DDN), maps time-frequency bins to a higher-dimensional space to better capture signal and noise characteristics but significantly adds to the computational complexity. In this work, we propose a computationally efficient modification by replacing the DDN block within the encoder with bispectral features, which effectively capture frequency correlations and phase information. We demonstrate the effectiveness of the method by incorporating it in two SE models, CMGAN and MP-SENet, and evaluating on two datasets, DNS-2020 and VoiceBank+DEMAND. Our experiments show that the proposed modification achieves similar or better performance while reducing the multiply-accumulate operations by 23.87% in CMGAN and 17.45% in MP-SENet, demonstrating its computational efficiency."
   ],
   "p1": 2385,
   "pn": 2389,
   "doi": "10.21437/Interspeech.2025-1988",
   "url": "interspeech_2025/parvathala25_interspeech.html"
  },
  "kirkland25_interspeech": {
   "authors": [
    [
     "Ambika",
     "Kirkland"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "Who knows best? Effects of speech disfluencies on incentivized decision-making",
   "original": "1990",
   "order": 918,
   "page_count": 5,
   "abstract": [
    "Previous work has shown that speech disfluencies can negatively impact judgments about a speaker&#x27;s competence and confidence. However, these effects have primarily been examined with Likert-type rating scales, which are not informative about how judgments might translate to behavior. Does the presence of disfluencies actually guide decision-making when listeners stand to gain concretely from making the correct choice? We sought to address this question with a web-based decision task in which participants were asked to choose between two conflicting sources of information. Our results suggest that listeners do take speech fluency into account when deciding who or what to believe."
   ],
   "p1": 4508,
   "pn": 4512,
   "doi": "10.21437/Interspeech.2025-1990",
   "url": "interspeech_2025/kirkland25_interspeech.html"
  },
  "ahmed25_interspeech": {
   "authors": [
    [
     "Shafique",
     "Ahmed"
    ],
    [
     "Ryandhimas E.",
     "Zezario"
    ],
    [
     "Nasir",
     "Saleem"
    ],
    [
     "Amir",
     "Hussain"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "A Study on Speech Assessment with Visual Cues",
   "original": "1992",
   "order": 1105,
   "page_count": 5,
   "abstract": [
    "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397→0.9205) for PESQ and 11.47% (0.7403→0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment."
   ],
   "p1": 5418,
   "pn": 5422,
   "doi": "10.21437/Interspeech.2025-1992",
   "url": "interspeech_2025/ahmed25_interspeech.html"
  },
  "zhao25k_interspeech": {
   "authors": [
    [
     "Jiankun",
     "Zhao"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Chengxi",
     "Deng"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Xixin",
     "Wu"
    ]
   ],
   "title": "Defending Unauthorized Voice Cloning with Watermark-Aware Codecs",
   "original": "1993",
   "order": 321,
   "page_count": 5,
   "abstract": [
    "The proliferation of zero-shot TTS models increases the risk of malicious voice cloning using copyrighted speech prompts. Although audio watermarking provides an effective way for encoding copyright information, attackers may still use watermarked speech as prompts to synthesize unwatermarked speech with the same speaker identity. To protect copyrighted voices from being cloned, this study introduces a method to train open-source TTS models to reject watermarked speech prompts for cloning. We observe that mainstream zero-shot TTS models typically rely on pre-trained codec encoders to process speech prompts. By training the codec to ``mute&quot; when encountering watermarked audio, the quality of generated speech will degrade. In this way, we implicitly prevent zero-shot TTS models from cloning watermarked voices. Experiments show that our approach is robust against various attacks while maintaining high-quality TTS ability given unwatermarked speech prompts."
   ],
   "p1": 1573,
   "pn": 1577,
   "doi": "10.21437/Interspeech.2025-1993",
   "url": "interspeech_2025/zhao25k_interspeech.html"
  },
  "aboeitta25_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Aboeitta"
    ],
    [
     "Ahmed",
     "Sharshar"
    ],
    [
     "Youssef",
     "Nafea"
    ],
    [
     "Shady",
     "Shehata"
    ]
   ],
   "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches ",
   "original": "1994",
   "order": 432,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech presents significant challenges for Automatic Speech Recognition (ASR) due to phoneme distortions and high variability. While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown promise, their effectiveness in dysarthric speech remains unclear. This study systematically benchmarks these models with different decoding strategies, including CTC, seq2seq, and LLM-enhanced decoding (BART, GPT-2, Vicuna). Our contributions include (1) benchmarking ASR architectures for dysarthric speech, (2) introducing LLM-based decoding to improve intelligibility, (3) analyzing generalization across datasets, and (4) providing insights into recognition errors across severity levels. Findings highlight that LLM-enhanced decoding improves dysarthric ASR by leveraging linguistic constraints for phoneme restoration and grammatical correction."
   ],
   "p1": 2123,
   "pn": 2127,
   "doi": "10.21437/Interspeech.2025-1994",
   "url": "interspeech_2025/aboeitta25_interspeech.html"
  },
  "marx25_interspeech": {
   "authors": [
    [
     "Darline Monika",
     "Marx"
    ],
    [
     "Marco",
     "Matassoni"
    ],
    [
     "Alessio",
     "Brutti"
    ]
   ],
   "title": "Automatic detection of speech sound disorders in German-speaking children: augmenting the data with typically developed speech",
   "original": "1996",
   "order": 586,
   "page_count": 5,
   "abstract": [
    "Speech Sound Disorders (SSD) are common among children, affecting their academic, social, and emotional development. Traditional diagnostic methods are based on speech-language pathologists, making them resource intensive. Due to the global shortage of experts and increasing demand, exploring deep-learning tools is crucial. Adapting a multi-task framework to fine-tune a pre-trained multilingual Wav2Vec model, this study tackles Automatic Speech Recognition and SSD classification for German children using a custom dataset. We show that incorporating public out-of-domain datasets improves robustness and generalizability. Interestingly,  combining pathological and typical speech data with mis-pronunciations benefits the performance in terms of speech recognition and SSD detection. Finally we investigate a two-step training of the model that further improves the overall performance."
   ],
   "p1": 2875,
   "pn": 2879,
   "doi": "10.21437/Interspeech.2025-1996",
   "url": "interspeech_2025/marx25_interspeech.html"
  },
  "tang25_interspeech": {
   "authors": [
    [
     "Yun",
     "Tang"
    ],
    [
     "Eesung",
     "Kim"
    ],
    [
     "Vijendra",
     "Raj Apsingekar"
    ]
   ],
   "title": "Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data",
   "original": "1997",
   "order": 556,
   "page_count": 5,
   "abstract": [
    "A joint speech and text optimization method is proposed for hybrid transducer and attention-based encoder decoder (TAED) modeling to leverage large amounts of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained with both speech and text input modalities together, while it only takes speech data as input during inference. The trained model can unify the internal representations from different modalities, and be further extended to text based domain adaptation.  It can effectively alleviate data scarcity for mismatch domain tasks since no speech data is required.  Our experiments show J-TAED successfully integrates speech and linguistic information into one model, and reduce the WER by 5.8 ~ 12.8% on the Librispeech dataset. The model is also evaluated on two out-of-domain datasets: one is finance and another is named entity focused. The text based domain adaptation brings 15.3%  and 17.8% WER reduction on those two datasets respectively."
   ],
   "p1": 2725,
   "pn": 2729,
   "doi": "10.21437/Interspeech.2025-1997",
   "url": "interspeech_2025/tang25_interspeech.html"
  },
  "monteroramirez25_interspeech": {
   "authors": [
    [
     "Claudia",
     "Montero-Ramírez"
    ],
    [
     "Alba",
     "Martínez-Serrano"
    ],
    [
     "Jorge",
     "Garcelán-Gómez"
    ],
    [
     "Francisco J.",
     "Valverde-Albacete"
    ],
    [
     "Carmen",
     "Peláez-Moreno"
    ]
   ],
   "title": "Beyond Conventional Metrics: using Entropic Triangles to Explain Balancing Methods in Acoustic Scene Classification",
   "original": "1998",
   "order": 271,
   "page_count": 5,
   "abstract": [
    "Understanding soundscapes is essential for making sense of real-world scenarios in complex environments. However, real-life conditions present significant challenges for AI-based methods, particularly due to the highly imbalanced nature of Audio Tagging problems. In this work, we investigate the impact of data imbalance on the training dynamics of state-of-the-art models for Acoustic Scene Classification. Using the DCASE TAU Urban Acoustic Scenes 2022 dataset and the CP-Mobile model, we introduce controlled imbalance scenarios and analyze their effect through the Entropic Triangle framework. Our findings reveal that the training dynamics are strongly influenced by the chosen balancing approach. It also suggests longer training periods with conventional optimizer for re-balanced classification metrics. In this way, this study provides new insights into the role of entropy-based analysis in developing robust Acoustic Scene Classification systems for real-world applications."
   ],
   "p1": 1323,
   "pn": 1327,
   "doi": "10.21437/Interspeech.2025-1998",
   "url": "interspeech_2025/monteroramirez25_interspeech.html"
  },
  "chandra25b_interspeech": {
   "authors": [
    [
     "Shilpa",
     "Chandra"
    ],
    [
     "Akansha",
     "Tyagi"
    ],
    [
     "Shiven",
     "Patel"
    ],
    [
     "Padmanabhan",
     "Rajan"
    ]
   ],
   "title": "Beyond Attacks: Advancing Fake Speech Detection with Attack-Agnostic Methods",
   "original": "1999",
   "order": 929,
   "page_count": 5,
   "abstract": [
    "Detectors of fake speech are prone to poor performance when presented with data unseen during training. Unseen data can include not only new methods for generating fake speech and various transformations applied to fake speech, but also fake speech in new languages. This study investigates two methods for improving generalization capability of detectors that use a self supervised model as a front-end. The first method uses an encoder-decoder architecture to learn representations robust to different types of fake speech. The second method uses a subspace-based decomposition and learns common information present in various types of fake speech.  The proposed methods are evaluated on the standard ASVspoof 2021 dataset, as well as on a multilingual speech dataset of Indic languages. Experiments reveal that state-of-the-art detectors struggle with speech in unseen languages, and the proposed generalization methods help in reducing the performance gap."
   ],
   "p1": 4563,
   "pn": 4567,
   "doi": "10.21437/Interspeech.2025-1999",
   "url": "interspeech_2025/chandra25b_interspeech.html"
  },
  "tam25_interspeech": {
   "authors": [
    [
     "Johnny",
     "Tam"
    ],
    [
     "Christine",
     "Weaver"
    ],
    [
     "Oliver",
     "Watts"
    ],
    [
     "Siddharthan",
     "Chandran"
    ],
    [
     "Suvankar",
     "Pal"
    ],
    [
     "",
     "Rowling Speech Consortium"
    ]
   ],
   "title": "Anne Rowling Neurological Speech Corpus: clinically annotated longitudinal dataset for developing speech biomarkers in neurodegenerative disorders",
   "original": "2000",
   "order": 1160,
   "page_count": 5,
   "abstract": [
    "There is urgent need for scalable, non-invasive and quantifiable biomarkers in neurodegenerative disorders. Speech is an attractive candidate with potential for remote and cheap assessments. Progress is limited by a lack of high quality clinically annotated speech data. We present a longitudinal speech corpus including speakers with dementia, motor neuron disease, Parkinson’s disease, progressive multiple sclerosis, and healthy individuals. Participants complete standardised recordings on an app co-produced with patients, aligned to contemporaneous phenotyping (clinical rating scales, cognitive tests and blood-based biomarkers). 780 participants have provided 5169 recordings in 1033 assessments. Benchmark classification and regression models show promising performance, and predictions on non-speech segments demonstrate limited bias from recording conditions. We continue to upscale data collection and analysis across larger diverse populations to accelerate clinical translation."
   ],
   "p1": 5693,
   "pn": 5697,
   "doi": "10.21437/Interspeech.2025-2000",
   "url": "interspeech_2025/tam25_interspeech.html"
  },
  "falez25_interspeech": {
   "authors": [
    [
     "Pierre",
     "Falez"
    ],
    [
     "Tony",
     "Marteau"
    ],
    [
     "Damien",
     "Lolive"
    ],
    [
     "Arnaud",
     "Delhay"
    ]
   ],
   "title": "Audio Deepfake Source Tracing using Multi-Attribute Open-Set Identification and Verification",
   "original": "2001",
   "order": 312,
   "page_count": 5,
   "abstract": [
    "Audio deepfake detection has advanced significantly, in particular, thanks to the ASVSpoof challenge.  However, existing approaches primarily rely on binary classification, which does not provide information about the origin of manipulated audio. In this paper, we address the problem of source tracing and propose two protocols to evaluate model performance in an open-set setting: (1) a few-shot identification protocol, where K reference audios are provided, and (2) a verification protocol inspired by speaker verification. We classify either the entire generation system or its components, such as the acoustic model or vocoder. Our models are trained both on an internal dataset and on the MLAAD source tracing dataset. Evaluation is done on five public datasets: three ASVSpoof sets, MLAAD and Blizzard23. Results show promising discrimination of unseen class attributes. Finally, we emphasize the need for a standardized ontology for source tracing in audio deepfake detection."
   ],
   "p1": 1528,
   "pn": 1532,
   "doi": "10.21437/Interspeech.2025-2001",
   "url": "interspeech_2025/falez25_interspeech.html"
  },
  "zhengjie25_interspeech": {
   "authors": [
    [
     "Jie",
     "Zhengjie"
    ],
    [
     "Gaofeng",
     "Cheng"
    ]
   ],
   "title": "Pinyin-Guided Chinese Speech Recognition with Large Language Model",
   "original": "2003",
   "order": 119,
   "page_count": 5,
   "abstract": [
    "While LLM-based automatic speech recognition (LLM-ASR) has demonstrated efficacy through direct acoustic-to-text mapping, its implicit alignment often fails to capture phonetic relationships in Chinese, leading to pronunciation confusion and homophone errors. This paper proposes Pinyin-Guided ASR (PYG-ASR), which innovatively modifies the LLM-ASR to simultaneously map acoustic features to both Pinyin and text tokens, enhancing linguistic representation. PYG-ASR leverages the generated Pinyin alongside text for error correction, prompting text LLM to refine transcriptions without finetuning. Furthermore, error correction phase inherently enables context biasing by filtering bias phrases through Pinyin matching and incorporating them into the prompt. Experiments show that PYG-ASR reduces CER by 25% on the AISHELL-1 test set. Additionally, our approach shows a 49.2% CER reduction relatively for bias phrases on the AISHELL-1 test set after contextual bias."
   ],
   "p1": 564,
   "pn": 568,
   "doi": "10.21437/Interspeech.2025-2003",
   "url": "interspeech_2025/zhengjie25_interspeech.html"
  },
  "tamir25_interspeech": {
   "authors": [
    [
     "Ziv",
     "Tamir"
    ],
    [
     "Thomas",
     "Thebaud"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Oren",
     "Kurland"
    ]
   ],
   "title": "Multimodal Emotion Diarization: Frame-Wise Integration of Text and Audio Representations",
   "original": "2009",
   "order": 884,
   "page_count": 5,
   "abstract": [
    "Speech emotion diarization (SED) is the task of segmenting an audio stream into time-continuous emotional states, akin to speaker diarization but for emotions. While traditional speech emotion recognition (SER) assigns a single emotion label to a given utterance, real-world conversations exhibit dynamic emotional transitions that require a more granular approach. In this work, we propose a novel multimodal SED framework that uses frame-wise integration of text and audio embeddings using temporal synchronization and direct concatenation, followed by a context-aware sliding window smoothing mechanism. Audio representations are extracted using WavLM, and EmoBERTa generates text embeddings aligned to spoken words.  We evaluate our approach using Emotion Diarization Error Rate (EDER), a metric designed for SED. Experimental results show that our proposed method significantly improves diarization performance, with respect to score fusion and cross-attention methods yielding an EDER of 25%."
   ],
   "p1": 4338,
   "pn": 4342,
   "doi": "10.21437/Interspeech.2025-2009",
   "url": "interspeech_2025/tamir25_interspeech.html"
  },
  "alam25_interspeech": {
   "authors": [
    [
     "Firoj",
     "Alam"
    ],
    [
     "Md Arid",
     "Hasan"
    ],
    [
     "Shammur Absar",
     "Chowdhury"
    ]
   ],
   "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs",
   "original": "2011",
   "order": 548,
   "page_count": 5,
   "abstract": [
    "Large Language Models (LLMs) have demonstrated remarkable performance across various disciplines and tasks. However, benchmarking their capabilities with multilingual spoken queries remains largely unexplored. In this study, we introduce SpokenNativQA, the first multilingual and culturally aligned spoken question-answering (SQA) dataset designed to evaluate LLMs in real-world conversational settings. The dataset comprises approximately 33,000 naturally spoken questions and answers in multiple languages, including low-resource and dialect-rich languages, providing a robust benchmark for assessing LLM performance in speech-based interactions. SpokenNativQA addresses the limitations of text-based QA datasets by incorporating speech variability, accents, and linguistic diversity. We benchmark different ASR systems and LLMs for SQA and present our findings. We released the data and the experimental scripts for the research community."
   ],
   "p1": 2685,
   "pn": 2689,
   "doi": "10.21437/Interspeech.2025-2011",
   "url": "interspeech_2025/alam25_interspeech.html"
  },
  "zhao25l_interspeech": {
   "authors": [
    [
     "Yu",
     "Zhao"
    ],
    [
     "Zengqiang",
     "Shang"
    ],
    [
     "Mou",
     "Wang"
    ],
    [
     "Xin",
     "Liu"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "Restoring Harmonics: Enhancing Speech Quality with Deep Mask and Harmonic Restoration Network",
   "original": "2012",
   "order": 1054,
   "page_count": 5,
   "abstract": [
    "While speech enhancement models have achieved remarkable progress in noise suppression, critical limitations persist in speech quality improvement. Current methods tend to over-suppress low-energy speech components and suffer from harmonic structure degradation. To tackle these challenges, we propose a two-stage model, Deep Mask and Harmonic Restoration Network (DMHRN). The Deep Mask (DM) stage utilizes a Noise-Reduced Mask (NRM) to aggressively suppress noise. Subsequently, the Harmonic Restoration (HR) stage leverages estimated fundamental frequency (F0) contours to guide the reconstruction of missing harmonic components. Experimental results on the DNS3 dataset demonstrate that DMHRN effectively restores the complete speech structure with fine-grained detail. It significantly outperforms baseline models, with a 6.01% improvement in P.835 OVRL and a 3.67% gain in P.808, achieving a substantial enhancement in speech quality without compromising noise suppression."
   ],
   "p1": 5168,
   "pn": 5172,
   "doi": "10.21437/Interspeech.2025-2012",
   "url": "interspeech_2025/zhao25l_interspeech.html"
  },
  "wang25x_interspeech": {
   "authors": [
    [
     "Minghan",
     "Wang"
    ],
    [
     "Ye",
     "Bai"
    ],
    [
     "Yuxia",
     "Wang"
    ],
    [
     "Thuy-Trang",
     "Vu"
    ],
    [
     "Ehsan",
     "Shareghi"
    ],
    [
     "Gholamreza",
     "Haffari"
    ]
   ],
   "title": "SpeechDialogueFactory: A Framework for Natural Speech Dialogue Generation",
   "original": "2013",
   "order": 359,
   "page_count": 5,
   "abstract": [
    "High-quality speech conversational datasets are essential for developing and evaluating Speech-LLMs. However, collecting real-world recordings presents significant challenges including high costs, privacy concerns, and inconsistent quality, while existing synthetic approaches often lack authenticity due to limited acoustic variety and insufficient paralinguistic information. We present SpeechDialogueFactory, a framework that addresses these limitations through a three-stage pipeline: generating comprehensive metadata, creating detailed scripts, and producing utterances enriched with paralinguistic features. Our framework retrieves speaker voices from a voice bank and leverages paralinguistic tags for expressive TTS. We also introduce an automated evaluation protocol that shows strong correlation with human assessments. Experimental results demonstrate that our synthesized dialogues achieve quality comparable to human recordings while offering greater flexibility and control."
   ],
   "p1": 1758,
   "pn": 1762,
   "doi": "10.21437/Interspeech.2025-2013",
   "url": "interspeech_2025/wang25x_interspeech.html"
  },
  "ueda25_interspeech": {
   "authors": [
    [
     "Lucas",
     "Ueda"
    ],
    [
     "João",
     "Lima"
    ],
    [
     "Leonardo",
     "Marques"
    ],
    [
     "Paula",
     "Costa"
    ]
   ],
   "title": "Improving Speech Emotion Recognition Through Cross Modal Attention Alignment and Balanced Stacking Model",
   "original": "2014",
   "order": 956,
   "page_count": 5,
   "abstract": [
    "Emotion plays a fundamental role in human interaction, and therefore systems capable of identifying emotions in speech are crucial in the context of human-computer interaction. Speech emotion recognition (SER) is a challenging problem, particularly in natural speech and when the available data is imbalanced across emotions. This paper presents our proposed system in the context of the 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge. Our proposed architecture leverages cross-modality, utilizing cross-modal attention to fuse representations from different modalities. To address class imbalance, we employed two training designs: (i) weighted cross-entropy loss (WCE); and (ii) WCE with an additional neutral-expressive soft margin loss and balancing. We trained a total of 12 multimodal models, which were ensembled using a balanced stacking model. Our proposed system achieves a Macro-F1 score of 0.4094 and an accuracy of 0.4128 on 8-class speech emotion recognition."
   ],
   "p1": 4698,
   "pn": 4702,
   "doi": "10.21437/Interspeech.2025-2014",
   "url": "interspeech_2025/ueda25_interspeech.html"
  },
  "ravi25_interspeech": {
   "authors": [
    [
     "Nagarathna",
     "Ravi"
    ],
    [
     "Thishyan Raj",
     "T"
    ],
    [
     "Ravi Teja",
     "Chaganti"
    ],
    [
     "Vipul",
     "Arora"
    ]
   ],
   "title": "ASR Confidence Estimation using True Class Lexical Similarity Score",
   "original": "2016",
   "order": 748,
   "page_count": 5,
   "abstract": [
    "Deep Neural Networks (DNN) often exhibit overconfidence, leading to poor confidence calibration in Automatic Speech Recognition (ASR) models. State-Of-The-Art (SOTA) approaches to estimate confidence are based on statistical measures or auxiliary models trained in supervised way using binary target scores, which however, fail to capture the degree of errors in substituted outputs. Continuous target score uses temporal alignment between predictions and ground truth, but are prone to inaccurate temporal alignment. To address these limitations, we propose a novel target score, True Class Lexical Similarity (TruCLeS), to train the auxiliary Confidence Estimation Model (CEM). TruCLeS is based on true class probability and lexical similarity between the prediction and ground truth. Experiments with CTC and RNN-Transducer based ASR models support its superiority against SOTA approaches. The codes are available."
   ],
   "p1": 3658,
   "pn": 3662,
   "doi": "10.21437/Interspeech.2025-2016",
   "url": "interspeech_2025/ravi25_interspeech.html"
  },
  "gao25g_interspeech": {
   "authors": [
    [
     "Ming",
     "Gao"
    ],
    [
     "Shilong",
     "Wu"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition",
   "original": "2017",
   "order": 385,
   "page_count": 5,
   "abstract": [
    "Meetings are a valuable yet challenging scenario for speech applications due to complex acoustic conditions. This paper summarizes the outcomes of the MISP 2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal, multi-device meeting transcription by incorporating video modality alongside audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR). We present the challenge’s objectives, tasks, dataset, baseline systems, and solutions proposed by participants. The best-performing systems achieved significant improvements over the baseline: the top AVSD model achieved a Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the best AVDR system achieved a concatenated minimum-permutation Character Error Rate (cpCER) of 11.56%, improving by 72.49%."
   ],
   "p1": 1888,
   "pn": 1892,
   "doi": "10.21437/Interspeech.2025-2017",
   "url": "interspeech_2025/gao25g_interspeech.html"
  },
  "greenberg25_interspeech": {
   "authors": [
    [
     "Craig",
     "Greenberg"
    ],
    [
     "Lukas",
     "Diduch"
    ],
    [
     "Audrey",
     "Tong"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Trang",
     "Nguyen"
    ],
    [
     "Robert",
     "Dunn"
    ],
    [
     "Lisa",
     "Mason"
    ],
    [
     "Beth",
     "Matys"
    ]
   ],
   "title": "The 2024 NIST Speaker Recognition Evaluation",
   "original": "2018",
   "order": 1171,
   "page_count": 5,
   "abstract": [
    "The 2024 U.S. National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) is the latest in a series of SREs conducted by NIST since 1996. The SRE24 evaluation task was automated person detection and had three evaluation tracks: audio, visual, and audio-visual. New SRE24 features included: variable duration enrollment segments, shorter duration test segments, segments containing multiple persons, and updated cost function parameters. 11 teams consisting of 26 sites participated in SRE24. Evaluation results indicate audio-visual fusion produces significant performance improvements over audio or visual systems. Further audio results analyses indicate: use of unrestricted training data produced significant performance gains; some systems performed better using three 10s segments than three 60s segments for enrollment data, with overall differences being less than expected; and, single model systems performed competitively with the fused multi-model systems."
   ],
   "p1": 5748,
   "pn": 5752,
   "doi": "10.21437/Interspeech.2025-2018",
   "url": "interspeech_2025/greenberg25_interspeech.html"
  },
  "siegert25_interspeech": {
   "authors": [
    [
     "Ingo",
     "Siegert"
    ],
    [
     "Jan",
     "Marquenie"
    ],
    [
     "Sven",
     "Grawunder"
    ]
   ],
   "title": "Queer Waves: A German Speech Dataset Capturing Gender and Sexual Diversity from Podcasts and YouTube",
   "original": "2022",
   "order": 142,
   "page_count": 5,
   "abstract": [
    "Developing equitable and inclusive speech technologies requires datasets that represent the full spectrum of human voices, including those of LGBTQIA+ speakers. However, capturing spontaneous, high-quality audio from marginalized gender and sexual identities presents significant ethical, logistical, and representational challenges. This paper introduces Queer Waves, a German speech corpus compiled from podcast and YouTube content featuring self-identified LGBTQIA+ speakers, with a particular focus on diverse gender identities and sexual orientations. We further address the legal and ethical considerations inherent in collecting sensitive personal data. The Queer Waves corpus comprises approximately 335 hours of speech from over 400 self-identified LGBTQIA+ speakers, spanning ages from 18 to 86 years. By expanding representation across a wide range of gender identities and orientations, Queer Waves aims to advance the development of fairer and more accurate speech technologies."
   ],
   "p1": 679,
   "pn": 683,
   "doi": "10.21437/Interspeech.2025-2022",
   "url": "interspeech_2025/siegert25_interspeech.html"
  },
  "carvalho25_interspeech": {
   "authors": [
    [
     "Carlos",
     "Carvalho"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Exploring Linear Variant Transformers and k-NN Memory Inference for Long-Form ASR",
   "original": "2025",
   "order": 733,
   "page_count": 5,
   "abstract": [
    "While transformer-based models excel in short-form (SF) automatic speech recognition, the quadratic complexity of the self-attention mechanism introduces significant challenges in long-form (LF) audio. To address this, we compare strong linear transformer variants—Fastformer, SummaryMixing, BiMamba, and E-Branchformer with Flash Attention (EBranch-FA). For the latter, we also explore rotary positional encodings. Additionally, we propose a new challenging LF benchmark derived from the LibriHeavy corpus, featuring development and test sets with varying average durations to enable comprehensive evaluation across different temporal scales. Furthermore, we propose a memory system, KNN-MAN, for inference, which can be applied to any existing encoder-decoder models, without additional training. For example, with BiMamba, we reduce the word error rate from 18.8% to 17.5% in our LF Test Clean set derived from LibriSpeech."
   ],
   "p1": 3583,
   "pn": 3587,
   "doi": "10.21437/Interspeech.2025-2025",
   "url": "interspeech_2025/carvalho25_interspeech.html"
  },
  "zhang25s_interspeech": {
   "authors": [
    [
     "Miao",
     "Zhang"
    ],
    [
     "Aref",
     "Farhadipour"
    ],
    [
     "Annie",
     "Baker"
    ],
    [
     "Jiachen",
     "Ma"
    ],
    [
     "Bogdan",
     "Pricop"
    ],
    [
     "Eleanor",
     "Chodroff"
    ]
   ],
   "title": "Quantifying and Reducing Speaker Heterogeneity within the Common Voice Corpus for Phonetic Analysis",
   "original": "2027",
   "order": 803,
   "page_count": 5,
   "abstract": [
    "With its crosslinguistic and cross-speaker diversity, the Mozilla Common Voice Corpus (CV) has been a valuable resource for multilingual speech technology and holds tremendous potential for research in crosslinguistic phonetics and speech sciences. Properly accounting for speaker variation is, however, key to the theoretical and statistical bases of speech research. While CV provides a client ID as an approximation to a speaker ID, multiple speakers can contribute under the same ID. This study aims to quantify and reduce heterogeneity in the client ID for a better approximation of a true, though still anonymous speaker ID. Using ResNet-based voice embeddings, we obtained a similarity score among recordings with the same client ID, then implemented a speaker discrimination task to identify an optimal threshold for reducing perceived speaker heterogeneity. These results have major downstream applications for phonetic analysis and the development of speaker-based speech technology."
   ],
   "p1": 3933,
   "pn": 3937,
   "doi": "10.21437/Interspeech.2025-2027",
   "url": "interspeech_2025/zhang25s_interspeech.html"
  },
  "mondal25_interspeech": {
   "authors": [
    [
     "Anindita",
     "Mondal"
    ],
    [
     "Rahul",
     "Biju"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ],
    [
     "Reni K",
     "Cherian"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "ProBiEM: Acoustic and Lexical Correlates of Prosodic Prominence in English-Malayalam Bilingual Speech",
   "original": "2028",
   "order": 1095,
   "page_count": 5,
   "abstract": [
    "Despite advancements in Speech-to-Speech Translation, maintaining expressiveness between the source and target speech, particularly between English and Indian languages, remains challenging. This study investigates prosodic similarities and variations between English and Malayalam, a language spoken in southern India. A set of 22 prompts from the IViE corpus, covering five categories—simple sentences, WH-questions, questions without morphosyntactic markers, inversion questions, and coordinations—was selected. These prompts, originally spoken by UK speakers, were translated into Malayalam and both language prompts were recorded by bilingual Malayalam speakers while preserving expressiveness. Word-level prominence was manually annotated, and comparisons were made across Indian English, Malayalam, and UK English. The analysis reveals that prominence is retained at key points in  Indian English, whereas in Malayalam, it is low due to question specific diacritics and agglutinative nature."
   ],
   "p1": 5368,
   "pn": 5372,
   "doi": "10.21437/Interspeech.2025-2028",
   "url": "interspeech_2025/mondal25_interspeech.html"
  },
  "shashaank25_interspeech": {
   "authors": [
    [
     "N",
     "Shashaank"
    ],
    [
     "Xiao",
     "Quan"
    ],
    [
     "Andrew",
     "Kaluzny"
    ],
    [
     "Leonard",
     "Varghese"
    ],
    [
     "Marko",
     "Stamenovic"
    ],
    [
     "Chuan-Che",
     "Huang"
    ]
   ],
   "title": "Towards Secure User Authentication for Headphones via In-Ear or In-Earcup Microphones",
   "original": "2029",
   "order": 338,
   "page_count": 5,
   "abstract": [
    "As headphones and earbuds with integrated AI assistants become more prevalent, it is important to prevent other people from gaining unauthorized access to these devices and accessing personal, sensitive user information. One solution to this problem is to implement a speaker identification (SID) model that can register authorized users onto the headphones and verify their identity in real-time, but it is challenging for these models to work in loud and/or noisy conditions, and they can be easily hacked using voice cloning, spoofing, and other adversarial attack techniques. In this paper, we propose using speech data collected from in-ear and in-earcup microphones commonly found on noise-cancelling headphones and earbuds to fine-tune SID models to address these issues. We collected inside mic data from 195 speakers across 4 headphones and earbuds and show that fine-tuning the model on multiple devices can improve performance when evaluating on a single device."
   ],
   "p1": 1658,
   "pn": 1662,
   "doi": "10.21437/Interspeech.2025-2029",
   "url": "interspeech_2025/shashaank25_interspeech.html"
  },
  "palzer25_interspeech": {
   "authors": [
    [
     "David",
     "Palzer"
    ],
    [
     "Matthew",
     "Maciejewski"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "End-to-End Diarization utilizing Attractor Deep Clustering",
   "original": "2030",
   "order": 325,
   "page_count": 5,
   "abstract": [
    "Speaker diarization remains challenging due to the need for structured speaker representations, efficient modeling, and robustness to varying conditions. We propose a performant, compact diarization framework that integrates conformer decoders, transformer-updated attractors, and a deep clustering style angle loss. Our approach refines speaker representations with an enhanced conformer structure, incorporating cross-attention to attractors and an additional convolution module. To enforce structured embeddings, we extend deep clustering by constructing label–attractor vectors, aligning their directional structure with audio embeddings. We also impose orthogonality constraints on active attractors for better speaker separation while suppressing non-active attractors to prevent false activations. Finally, a permutation invariant training binary cross-entropy loss refines speaker detection. Experiments show that our method achieves low diarization error while maintaining parameter count."
   ],
   "p1": 1593,
   "pn": 1597,
   "doi": "10.21437/Interspeech.2025-2030",
   "url": "interspeech_2025/palzer25_interspeech.html"
  },
  "pathak25_interspeech": {
   "authors": [
    [
     "Utkarsh",
     "Pathak"
    ],
    [
     "Chandra Sai Krishna",
     "Gunda"
    ],
    [
     "Anusha",
     "Prakash"
    ],
    [
     "Keshav",
     "Agarwal"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages",
   "original": "2031",
   "order": 843,
   "page_count": 5,
   "abstract": [
    "Text-to-speech (TTS) systems typically require high-quality studio data and accurate transcriptions for training. India has 1369 languages, with 22 official using 13 scripts. Training a TTS system for all these languages, most of which have no digital resources, seems a Herculean task. Our work focuses on zero-shot synthesis, particularly for languages whose scripts and phonotactics come from different families. The novelty of our work is in the augmentation of a shared phone representation and modifying the text parsing rules to match the phonotactics of the target language, thus reducing the synthesiser overhead and enabling rapid adaptation. Intelligible and natural speech was generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging linguistic connections across languages with suitable synthesisers. Evaluations confirm the effectiveness of this approach, highlighting its potential to expand speech technology access for under-represented languages."
   ],
   "p1": 4133,
   "pn": 4137,
   "doi": "10.21437/Interspeech.2025-2031",
   "url": "interspeech_2025/pathak25_interspeech.html"
  },
  "mondal25b_interspeech": {
   "authors": [
    [
     "Anindita",
     "Mondal"
    ],
    [
     "Monica",
     "Surtani"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ],
    [
     "Parameswari",
     "Krishnamurthy"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "ExagTTS: An Approach Towards Controllable Word Stress Incorporated TTS for Exaggerated Synthesized Speech Aiding Second Language Learners",
   "original": "2032",
   "order": 96,
   "page_count": 5,
   "abstract": [
    "Computer-Assisted Language Learning systems provide speech exaggerating at mispronounced word locations as a feedback to the L2 learners. Traditionally, expert speakers recordings (which limit the scalability) are considered for this task though there are advancements in text-to-speech (TTS) that can generate native like natural sounding speech. To address these, this work proposes two novel controllable strategies for scalable speech exaggeration. One strategy is direct speech exaggeration that incorporates the proposed label conditioned tokenization in GlowTTS. Another strategy is cascading the state-of-the-art TTS to a WORLD vocoder with proposed energy and duration modifications. A subset of Tatoeba corpus, that we annotated with prominent words, is used for experimentation. Automatic and manual assessment reveals that the exaggerated speech quality from both direct and cascaded strategy with duration modification is closer to the prominent words in the native speaker&#x27;s speech."
   ],
   "p1": 449,
   "pn": 453,
   "doi": "10.21437/Interspeech.2025-2032",
   "url": "interspeech_2025/mondal25b_interspeech.html"
  },
  "zgorzynski25_interspeech": {
   "authors": [
    [
     "Bartłomiej",
     "Zgórzyński"
    ],
    [
     "Juliusz",
     "Wójtowicz-Kruk"
    ],
    [
     "Piotr",
     "Masztalski"
    ],
    [
     "Władysław",
     "Średniawa"
    ]
   ],
   "title": "Multi-task learning for speech emotion recognition in naturalistic conditions",
   "original": "2033",
   "order": 952,
   "page_count": 5,
   "abstract": [
    "This work introduces a multi-encoder joint classification and regression training framework for speech emotion recognition. We present our solution for the Interspeech 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, leveraging a multi-modal, multi-encoder architecture with a fusion module. Our results demonstrate the effectiveness of the multi-task approach for both classification and regression tasks, achieving a top 10 spot in categorical emotion classification and 2nd place in emotional attribute prediction among competing teams. Furthermore, an ablation study shows that employing multi-task learning  outperforms separate task-specific training. These findings highlight the potential of multi-task, multi-encoder systems for speech emotion recognition."
   ],
   "p1": 4678,
   "pn": 4682,
   "doi": "10.21437/Interspeech.2025-2033",
   "url": "interspeech_2025/zgorzynski25_interspeech.html"
  },
  "choi25h_interspeech": {
   "authors": [
    [
     "Anna Seo Gyeong",
     "Choi"
    ],
    [
     "Alexander",
     "Richardson"
    ],
    [
     "Ryan",
     "Partlan"
    ],
    [
     "Sunny X.",
     "Tang"
    ],
    [
     "Sunghye",
     "Cho"
    ]
   ],
   "title": "Comparative Evaluation of Acoustic Feature Extraction Tools for Clinical Speech Analysis",
   "original": "2034",
   "order": 111,
   "page_count": 5,
   "abstract": [
    "This study compares three acoustic feature extraction toolkits—OpenSMILE, Praat, and Librosa—applied to clinical speech data from individuals with schizophrenia spectrum disorders (SSD) and healthy controls (HC). By standardizing extraction parameters across the toolkits, we analyzed speech samples from 77 SSD and 87 HC participants and found significant toolkit-dependent variations. While F0 percentiles showed high cross-toolkit correlation (r=0.962–0.999), measures like F0 standard deviation and formant values often had poor, even negative, agreement. Additionally, correlation patterns differed between SSD and HC groups. Classification analysis identified F0 mean, HNR, and MFCC1 (AUC &gt; 0.70) as promising discriminators. These findings underscore reproducibility concerns and advocate for standardized protocols, multi-toolkit cross-validation, and transparent reporting."
   ],
   "p1": 524,
   "pn": 528,
   "doi": "10.21437/Interspeech.2025-2034",
   "url": "interspeech_2025/choi25h_interspeech.html"
  },
  "huang25h_interspeech": {
   "authors": [
    [
     "Kevin",
     "Huang"
    ],
    [
     "Sean",
     "Foley"
    ],
    [
     "Jihwan",
     "Lee"
    ],
    [
     "Yoonjeong",
     "Lee"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "On the Relationship between Accent Strength and Articulatory Features",
   "original": "2037",
   "order": 924,
   "page_count": 5,
   "abstract": [
    "This paper explores the relationship between accent strength and articulatory features inferred from acoustic speech. To quantify accent strength, we compare phonetic transcriptions with transcriptions based on dictionary-based references, computing phoneme-level difference as a measure of accent strength. The proposed framework leverages recent self-supervised learning articulatory inversion techniques to estimate articulatory features. Analyzing a corpus of read speech from American and British English speakers, this study examines correlations between derived articulatory parameters and accent strength proxies, associating systematic articulatory differences with indexed accent strength. Results indicate that tongue positioning patterns distinguish the two dialects, with notable differences inter-dialects in rhotic and low back vowels. These findings contribute to automated accent analysis and articulatory modeling for speech processing applications."
   ],
   "p1": 4538,
   "pn": 4542,
   "doi": "10.21437/Interspeech.2025-2037",
   "url": "interspeech_2025/huang25h_interspeech.html"
  },
  "slomianka25_interspeech": {
   "authors": [
    [
     "Valeska",
     "Slomianka"
    ],
    [
     "Tobias",
     "May"
    ],
    [
     "Torsten",
     "Dau"
    ]
   ],
   "title": "Impact of Background Noise on Turn-Taking Dynamics in Triadic Conversations",
   "original": "2038",
   "order": 616,
   "page_count": 5,
   "abstract": [
    "Conversations require individuals to comprehend speech while preparing responses, with demands increasing significantly in noise or with more people involved. Here, we explore the impact of noise on turn-taking dynamics in triadic conversations. Ten groups of three young, normal-hearing participants engaged in conversations at two levels of 8-talker babble noise, with each group holding three conversations under noisy conditions and three in quiet. We extracted objective measures of turn-taking dynamics from the recorded speech and assessed subjective ratings of conversational difficulty. Conversations in noise were shorter, with reduced distance between participants, fewer overlaps-within, and fewer, more delayed, and variable floor transfers. The conversational dynamics in noise correlated with the subjective ratings. Overall, changes in turn-taking dynamics in triadic conversations in noise resemble those in dyads, reflecting alterations in conversational flow and engagement."
   ],
   "p1": 3025,
   "pn": 3029,
   "doi": "10.21437/Interspeech.2025-2038",
   "url": "interspeech_2025/slomianka25_interspeech.html"
  },
  "lobashev25_interspeech": {
   "authors": [
    [
     "Alexander",
     "Lobashev"
    ],
    [
     "Assel",
     "Yermekova"
    ],
    [
     "Maria",
     "Larchenko"
    ]
   ],
   "title": "Training-Free Voice Conversion with Factorized Optimal Transport",
   "original": "2043",
   "order": 281,
   "page_count": 5,
   "abstract": [
    "This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline. In contrast with original pipeline, our algorithm performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio. MKL-VC replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution.  Factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets show MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. MKL-VC achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain."
   ],
   "p1": 1373,
   "pn": 1377,
   "doi": "10.21437/Interspeech.2025-2043",
   "url": "interspeech_2025/lobashev25_interspeech.html"
  },
  "tuckute25_interspeech": {
   "authors": [
    [
     "Greta",
     "Tuckute"
    ],
    [
     "Klemen",
     "Kotar"
    ],
    [
     "Evelina",
     "Fedorenko"
    ],
    [
     "Daniel",
     "Yamins"
    ]
   ],
   "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
   "original": "2044",
   "order": 447,
   "page_count": 5,
   "abstract": [
    "We introduce AuriStream, a biologically inspired model for encoding speech via a two-stage framework inspired by the human auditory processing hierarchy. The first stage transforms raw audio into a time-frequency representation based on the human cochlea, from which we extract discrete cochlear tokens. The second stage applies an autoregressive sequence model over the cochlear tokens. AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics. AuriStream shows competitive performance on diverse downstream SUPERB speech tasks. Complementing AuriStream&#x27;s strong representational capabilities, it generates continuations of audio which can be visualized in a spectrogram space and decoded back into audio, providing insights into the model&#x27;s predictions. In summary, we present a two-stage framework for speech representation learning to advance the development of more human-like models that efficiently handle a range of speech-based tasks."
   ],
   "p1": 2180,
   "pn": 2184,
   "doi": "10.21437/Interspeech.2025-2044",
   "url": "interspeech_2025/tuckute25_interspeech.html"
  },
  "polle25_interspeech": {
   "authors": [
    [
     "Roseline",
     "Polle"
    ],
    [
     "Agnes",
     "Norbury"
    ],
    [
     "Alexandra Livia",
     "Georgescu"
    ],
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Stefano",
     "Goria"
    ]
   ],
   "title": "Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models",
   "original": "2055",
   "order": 415,
   "page_count": 5,
   "abstract": [
    "Speaker-dependent modelling can substantially improve performance in speech-based health monitoring applications. While mixed-effect models are commonly used for such speaker adaptation, they require computationally expensive retraining for each new observation, making them impractical in a production environment. We reformulate this task as a meta-learning problem and explore three approaches of increasing complexity: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Using pre-trained speech embeddings, we evaluate these methods on a large longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting time since sleep from speech as a function of fatigue, a symptom commonly associated with ill-health. Our results demonstrate that all meta-learning approaches tested outperformed both cross-sectional and conventional mixed-effects models, with a transformer-based method achieving the strongest performance."
   ],
   "p1": 2038,
   "pn": 2042,
   "doi": "10.21437/Interspeech.2025-2055",
   "url": "interspeech_2025/polle25_interspeech.html"
  },
  "peters25_interspeech": {
   "authors": [
    [
     "Fritz",
     "Peters"
    ],
    [
     "W Richard",
     "Bevan-Jones"
    ],
    [
     "Grace",
     "Threlfall"
    ],
    [
     "Jenny M",
     "Harris"
    ],
    [
     "Julie S",
     "Snowden"
    ],
    [
     "Matthew",
     "Jones"
    ],
    [
     "Jennifer C",
     "Thompson"
    ],
    [
     "Daniel J",
     "Blackburn"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Automatic Detection and Sub-typing of Primary Progressive Aphasia from Speech: Integrating Task-Specific Features and Spatio-Semantic Graphs",
   "original": "2056",
   "order": 1079,
   "page_count": 5,
   "abstract": [
    "Primary progressive aphasia (PPA) describes a group of neurodegenerative diseases that predominantly affect language abilities. Its diagnostic process typically requires experienced clinicians, often available only in specialised hospital departments. Patients with PPA frequently display changes in speech and language early in the disease progression. In this study, we extracted acoustic, linguistic, and task-specific features from audio recordings and evaluated their utility for PPA classification. Using a subset of task-specific features, we detected PPA with 97% accuracy. For sub-typing, models trained on the full feature set achieved 74% accuracy in a three-way classification of PPA variants. Our results highlight the added value of task-specific features, which complement traditional approaches. Additionally, their visualisation offers an intuitive representation of task execution, improving clinical interpretability and potential diagnostic utility."
   ],
   "p1": 5288,
   "pn": 5292,
   "doi": "10.21437/Interspeech.2025-2056",
   "url": "interspeech_2025/peters25_interspeech.html"
  },
  "ratajczak25_interspeech": {
   "authors": [
    [
     "Martin",
     "Ratajczak"
    ],
    [
     "Jean-Philippe",
     "Robichaud"
    ],
    [
     "Jennifer",
     "Drexler Fox"
    ]
   ],
   "title": "Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR",
   "original": "2059",
   "order": 676,
   "page_count": 5,
   "abstract": [
    "Long-form speech recognition is an application area of increasing research focus. ASR models based on multi-head attention (MHA) are ill-suited to long-form ASR because of their quadratic complexity in sequence length. We build on recent work that has investigated linear complexity recurrent attention (RA) layers for ASR. We find that bidirectional RA layers can match the accuracy of MHA for both short-and long-form applications. We present a strong limited-context attention (LCA) baseline, and show that RA layers are just as accurate while being more efficient. We develop a long-form training paradigm which further improves RA performance, leading to better accuracy than LCA with 44% higher throughput. We also present Direction Dropout, a novel regularization method that improves accuracy, provides fine-grained control of the accuracy/throughput trade-off of bidirectional RA, and enables a new alternating directions decoding mode with even higher throughput."
   ],
   "p1": 3324,
   "pn": 3328,
   "doi": "10.21437/Interspeech.2025-2059",
   "url": "interspeech_2025/ratajczak25_interspeech.html"
  },
  "markitantov25_interspeech": {
   "authors": [
    [
     "Maxim",
     "Markitantov"
    ],
    [
     "Elena",
     "Ryumina"
    ],
    [
     "Heysem",
     "Kaya"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "Multi-Modal Multi-Task Affective States Recognition Based on Label Encoder Fusion",
   "original": "2060",
   "order": 613,
   "page_count": 5,
   "abstract": [
    "Despite recent advances in multi-modal approaches, recognizing the full range of human affective states, including emotions and sentiments, remains challenging due to complex interactions between different modalities and the hierarchical nature of affective states. This work presents a novel approach for multi-modal multi-task emotion and sentiment recognition that integrates audio, video, and text data. We introduce a Label Encoder Fusion Strategy, which produces and processes uni-modal emotion and sentiment predictions, which are used alongside modality-specific features during the fusion process to provide additional contextual information. We conduct elaborate multi-corpus experiments on the RAMAS, MELD, and CMU-MOSEI corpora. The proposed approach achieves state-of-the-art performance in both affective tasks. On MELD, we achieve a macro F1 (MF) of 40.9% and 67.02% for emotion and sentiment recognition. On CMU-MOSEI, the mean MF is 62.30% and MF is 62.00% for the same tasks."
   ],
   "p1": 3010,
   "pn": 3014,
   "doi": "10.21437/Interspeech.2025-2060",
   "url": "interspeech_2025/markitantov25_interspeech.html",
   "erratum": "<p>Figure 2 does not accurately represent proposed approach and significantly distorts the key ideas and contributions of the paper. The current illustration does not include Label Encoder with averaging despite the right caption. The corrected figure is shown here:\n\n<figure>\n    <img src=\"./errata/markitantov25_interspeech/figure2.svg\" style=\"width:50%;\" />\n    <figcaption><center>Figure 2: <i>Label Encoder Fusion Strategy with Averaging.</i></center>/figcaption>\n</figure>\n</p>"
  },
  "hussein25_interspeech": {
   "authors": [
    [
     "Amir",
     "Hussein"
    ],
    [
     "Sameer",
     "Khurana"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
   "original": "2063",
   "order": 1100,
   "page_count": 5,
   "abstract": [
    "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced ``hazard&#x27;&#x27;), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD’s encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information."
   ],
   "p1": 5393,
   "pn": 5397,
   "doi": "10.21437/Interspeech.2025-2063",
   "url": "interspeech_2025/hussein25_interspeech.html"
  },
  "firc25_interspeech": {
   "authors": [
    [
     "Anton",
     "Firc"
    ],
    [
     "Manasi",
     "Chhibber"
    ],
    [
     "Jagabandhu",
     "Mishra"
    ],
    [
     "Vishwanath",
     "Pratap Singh"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Kamil",
     "Malinka"
    ]
   ],
   "title": "STOPA: A Dataset of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution",
   "original": "2065",
   "order": 317,
   "page_count": 5,
   "abstract": [
    "A key research area in deepfake speech detection is source tracing — determining the origin of synthesised utterances. The approaches may involve identifying the acoustic model (AM), vocoder model (VM), or other generation-specific parameters. However, progress is limited by the lack of a dedicated, systematically curated dataset. To address this, we introduce STOPA, a systematically varied and metadata-rich dataset for deepfake speech source tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k samples from 13 distinct synthesisers. Unlike existing datasets, which often feature limited variation or sparse metadata, STOPA provides a systematically controlled framework covering a broader range of generative factors, such as the choice of the vocoder model, acoustic model, or pretrained weights, ensuring higher attribution reliability. This control improves attribution accuracy, aiding forensic analysis, deepfake detection, and generative model transparency."
   ],
   "p1": 1553,
   "pn": 1557,
   "doi": "10.21437/Interspeech.2025-2065",
   "url": "interspeech_2025/firc25_interspeech.html"
  },
  "wallbridge25_interspeech": {
   "authors": [
    [
     "Sarenne",
     "Wallbridge"
    ],
    [
     "Christoph",
     "Minixhofer"
    ],
    [
     "Catherine",
     "Lai"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning",
   "original": "2068",
   "order": 961,
   "page_count": 5,
   "abstract": [
    "People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody–e.g., intonation, tempo, and loudness–contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures."
   ],
   "p1": 4723,
   "pn": 4727,
   "doi": "10.21437/Interspeech.2025-2068",
   "url": "interspeech_2025/wallbridge25_interspeech.html"
  },
  "elhajal25_interspeech": {
   "authors": [
    [
     "Karl",
     "El Hajal"
    ],
    [
     "Enno",
     "Hermann"
    ],
    [
     "Sevada",
     "Hovsepyan"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech",
   "original": "2069",
   "order": 563,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: https://github.com/idiap/RnV."
   ],
   "p1": 2760,
   "pn": 2764,
   "doi": "10.21437/Interspeech.2025-2069",
   "url": "interspeech_2025/elhajal25_interspeech.html"
  },
  "hutin25_interspeech": {
   "authors": [
    [
     "Mathilde",
     "Hutin"
    ],
    [
     "Mélanie",
     "Lancien"
    ],
    [
     "Noam",
     "Faust"
    ]
   ],
   "title": "French schwa is not acoustically distinct  from its two lexical neighbors /ø/ and /œ/",
   "original": "2070",
   "order": 966,
   "page_count": 5,
   "abstract": [
    "French schwa, or mute-e, is one of the most studied topics in French phonology, yet its exact phonetic quality remains debated. In particular, whether schwa is acoustically similar to or distinct from its two mid front rounded neighbors has given rise to contradictory results. The main issue is that past studies generally compare schwa and its neighbors without controlling for stress, yet the latter can be stressed while the former can never be, which is bound to impact their respective realizations. In this paper, we present a pilot study to investigate the quality of schwa compared to its neighbors while controlling for stress. Twenty-two French words with schwa or with its neighbors, were embedded in carrier sentences and read aloud among filler sentences. The data from 20 native speakers of Parisian French indicate that, when controlling for stress as well as orthography, schwa and its neighbors are acoustically very similar, in terms of both formants and duration."
   ],
   "p1": 4748,
   "pn": 4752,
   "doi": "10.21437/Interspeech.2025-2070",
   "url": "interspeech_2025/hutin25_interspeech.html"
  },
  "hegde25_interspeech": {
   "authors": [
    [
     "Pradyoth",
     "Hegde"
    ],
    [
     "Santosh",
     "Kesiraju"
    ],
    [
     "Ján",
     "Švec"
    ],
    [
     "Šimon",
     "Sedláček"
    ],
    [
     "Bolaji",
     "Yusuf"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Deepak",
     "K T"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking",
   "original": "2071",
   "order": 980,
   "page_count": 5,
   "abstract": [
    "This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking."
   ],
   "p1": 4818,
   "pn": 4822,
   "doi": "10.21437/Interspeech.2025-2071",
   "url": "interspeech_2025/hegde25_interspeech.html"
  },
  "cheng25c_interspeech": {
   "authors": [
    [
     "Xize",
     "Cheng"
    ],
    [
     "Zehan",
     "Wang"
    ],
    [
     "Rongjie",
     "Huang"
    ],
    [
     "Huadai",
     "Liu"
    ],
    [
     "Tao",
     "Jin"
    ]
   ],
   "title": "Robust Speech-Driven Body Language Generation",
   "original": "2073",
   "order": 1027,
   "page_count": 5,
   "abstract": [
    "With the continuous advancement of video generation technologies, researchers have developed speech-driven body language synthesis, including co-speech gestures and 3D facial animation, and more. However, due to the lack of paired data between visual speech (i.e., lip movements) and body language, existing methods typically rely solely on audio speech. This dependency makes it challenging to accurately synthesize the desired outcomes in noisy environments. To address this limitation, we introduce AV2Body, a robust speech-driven body language synthesis method designed to achieve reliable synthesis even under noisy conditions. Experimental results demonstrate that our method excels in synthesizing a variety of body language modalities in noisy environments, exhibiting strong noise robustness."
   ],
   "p1": 5033,
   "pn": 5037,
   "doi": "10.21437/Interspeech.2025-2073",
   "url": "interspeech_2025/cheng25c_interspeech.html"
  },
  "li25ba_interspeech": {
   "authors": [
    [
     "Zhu",
     "Li"
    ],
    [
     "Yuqing",
     "Zhang"
    ],
    [
     "Xiyuan",
     "Gao"
    ],
    [
     "Shekhar",
     "Nayak"
    ],
    [
     "Matt",
     "Coler"
    ]
   ],
   "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection",
   "original": "2074",
   "order": 811,
   "page_count": 5,
   "abstract": [
    "Sarcasm fundamentally alters meaning through tone and context, yet detecting it in speech remains a challenge due to data scarcity. In addition, existing detection systems often rely on multimodal data, limiting their applicability in contexts where only speech is available. To address this, we propose an annotation pipeline that leverages large language models (LLMs) to generate a sarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ GPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human verification to resolve disagreements. We validate this approach by comparing annotation quality and detection performance on a publicly available sarcasm dataset using a collaborative gating architecture. Finally, we introduce PodSarc, a large-scale sarcastic speech dataset created through this pipeline. The detection model achieves a 73.63% F1 score, demonstrating the dataset’s potential as a benchmark for sarcasm detection research."
   ],
   "p1": 3973,
   "pn": 3977,
   "doi": "10.21437/Interspeech.2025-2074",
   "url": "interspeech_2025/li25ba_interspeech.html"
  },
  "sanders25_interspeech": {
   "authors": [
    [
     "Nicholas",
     "Sanders"
    ],
    [
     "Yuanchao",
     "Li"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information",
   "original": "2075",
   "order": 1102,
   "page_count": 5,
   "abstract": [
    "Quantization in SSL speech models (e.g., HuBERT) improves compression and performance in tasks like language modeling, resynthesis, and text-to-speech but often discards prosodic and paralinguistic information (e.g., emotion, prominence). While increasing codebook size mitigates some loss, it inefficiently raises bitrates. We propose Segmentation-Variant Codebooks (SVCs), which quantize speech at distinct linguistic units (frame, phone, word, utterance), factorizing it into multiple streams of segment-specific discrete features. Our results show that SVCs are significantly more effective at preserving prosodic and paralinguistic information across probing tasks. Additionally, we find that pooling before rather than after discretization better retains segment-level information. Resynthesis experiments further confirm improved style realization and slightly improved quality while preserving intelligibility."
   ],
   "p1": 5403,
   "pn": 5407,
   "doi": "10.21437/Interspeech.2025-2075",
   "url": "interspeech_2025/sanders25_interspeech.html"
  },
  "serre25_interspeech": {
   "authors": [
    [
     "Thomas",
     "Serre"
    ],
    [
     "Mathieu",
     "Fontaine"
    ],
    [
     "Eric",
     "Benhaim"
    ],
    [
     "Slim",
     "Essid"
    ]
   ],
   "title": "MTSE: Multi-Target Speaker Extraction for Conversation Scenarios",
   "original": "2077",
   "order": 605,
   "page_count": 5,
   "abstract": [
    "Target Speaker Extraction (TSE) aims to capture a desired voice among other interfering ones and/or background noise using a reference excerpt acquired during the enrollment phase. While useful in many applications, existing TSE systems cannot handle the scenario where several voices need to be enrolled and/or targeted. In this work, we address this new task, called multi-target speaker extraction (MTSE), which consists of extracting multiple target speakers in a mixture, possibly involving other interfering voices, using multiple speaker embeddings. Such models can thus be used by multiple users without the re-enrollment necessity. We propose a curriculum learning scheme to adapt well-known TSE models to the MTSE task. We prove its effectiveness and obtain, for the first time, successful MTSE results on meeting-type excerpts. We also present single-speaker TSE results with multiple enrolled speakers, proving the robustness and versatility of our solution."
   ],
   "p1": 2970,
   "pn": 2974,
   "doi": "10.21437/Interspeech.2025-2077",
   "url": "interspeech_2025/serre25_interspeech.html"
  },
  "kulkarni25_interspeech": {
   "authors": [
    [
     "Ajinkya",
     "Kulkarni"
    ],
    [
     "Sandipana",
     "Dowerah"
    ],
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion",
   "original": "2079",
   "order": 313,
   "page_count": 5,
   "abstract": [
    "Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system."
   ],
   "p1": 1533,
   "pn": 1537,
   "doi": "10.21437/Interspeech.2025-2079",
   "url": "interspeech_2025/kulkarni25_interspeech.html"
  },
  "roquefort25_interspeech": {
   "authors": [
    [
     "Filomene",
     "Roquefort"
    ],
    [
     "Alexandre",
     "Ducorroy"
    ],
    [
     "Rachid",
     "Riad"
    ]
   ],
   "title": "In-context learning capabilities of Large Language Models to detect suicide risk among adolescents from speech transcripts",
   "original": "2083",
   "order": 89,
   "page_count": 5,
   "abstract": [
    "Early suicide risk detection in adolescents is critical yet hindered by scalability challenges of current assessments. This paper presents our approach to the first SpeechWellness Challenge (SW1), which aims to assess suicide risk in Chinese adolescents through speech analysis. Due to speech anonymization constraints, we focused on linguistic features, leveraging Large Language Models (LLMs) for transcript-based classification. Using DSPy for systematic prompt engineering, we developed a robust in-context learning approach that outperformed traditional fine-tuning on both linguistic and acoustic markers. Our systems achieved third and fourth places among 180+ submissions, with 0.68 accuracy (F1=0.7) using only transcripts. Ablation analyses showed that increasing prompt example improved performance (p=0.003), with varying effects across model types and sizes. These findings advance automated suicide risk assessment and demonstrate LLMs&#x27; value in mental health applications."
   ],
   "p1": 414,
   "pn": 418,
   "doi": "10.21437/Interspeech.2025-2083",
   "url": "interspeech_2025/roquefort25_interspeech.html"
  },
  "yang25n_interspeech": {
   "authors": [
    [
     "Ruochu",
     "Yang"
    ],
    [
     "Milind",
     "Rao"
    ],
    [
     "Harshavardhan",
     "Sundar"
    ],
    [
     "Anirudh",
     "Raju"
    ],
    [
     "Aparna",
     "Khare"
    ],
    [
     "Srinath",
     "Tankasala"
    ],
    [
     "Di",
     "He"
    ],
    [
     "Venkatesh",
     "Ravichandran"
    ]
   ],
   "title": "On Retrieval of Long Audios with Complex Text Queries",
   "original": "2085",
   "order": 543,
   "page_count": 5,
   "abstract": [
    "We consider a challenging problem of LARCQ (Long Audio Retrieval with Complex Queries), where audios to retrieve may be arbitrarily long and queries span multiple events. To solve it, we propose a novel pipeline systematically integrating multi-modal retrieval and ALM/LLM refining. At Steps 1 and 2, we introduce a chunking-aggregation method to retrieve candidate audios by constructing a similarity matrix. At Steps 3 and 4, audio captions are generated for retrieved candidates using ALMs, and the final audio is selected by comparing the query with generated captions through text LLMs/classifiers. Due to lack of benchmarks for LARCQ, we introduce Clotho-LARCQ and SoundDescs-LARCQ featuring long audios and complex queries. Our chunking-aggregation method achieves up to 67% R@1 and 40% R@5 gains. Incorporating ALM/LLM refining, our full pipeline achieves 21% R@1 on the original Clotho benchmark and up to 100% R@1 improvement on new LARCQ benchmarks."
   ],
   "p1": 2660,
   "pn": 2664,
   "doi": "10.21437/Interspeech.2025-2085",
   "url": "interspeech_2025/yang25n_interspeech.html"
  },
  "niculescu25_interspeech": {
   "authors": [
    [
     "Oana",
     "Niculescu"
    ],
    [
     "Monica",
     "Vasileanu"
    ]
   ],
   "title": "Prolongation in Romanian",
   "original": "2087",
   "order": 82,
   "page_count": 5,
   "abstract": [
    "In this study we investigate segmental prolongation (PR) as a form of disfluent hesitation in a corpus of spontaneous Romanian monologues. A total of 3541 PRs were extracted from 216 minutes of speech pertaining to 4 native speakers (2 female, 2 male). In line with the methodology employed by previous corpus-studies on PR, our data reveal that prolonged segments have an average duration of 316ms (sd = 130), surfacing at a frequency of 11.3 per 100 words and following a 17–7–76% position distribution. In Romanian, all segments can undergo PR, with vowels being the preferred target (57.2%), followed by fricatives (12.8%), nasals (11.8%), plosives (10.3%), diphthongs (5.5%), affricates (1.6%) and laterals (0.8%). The vast majority of PRs surface in monosyllabic words (59%). Function words are prolonged in 57% of the cases. By including data from a lesser-studied European language, this paper broadens our understanding of the formal regularities of PR in a cross-linguistic setting."
   ],
   "p1": 379,
   "pn": 383,
   "doi": "10.21437/Interspeech.2025-2087",
   "url": "interspeech_2025/niculescu25_interspeech.html"
  },
  "xiong25b_interspeech": {
   "authors": [
    [
     "Shengyue",
     "Xiong"
    ],
    [
     "Zhe-chen",
     "Guo"
    ],
    [
     "Bharath",
     "Chandrasekaran"
    ]
   ],
   "title": "Language and Accent Familiarity Effects on the Use of Acoustic Cues in Talker Identification",
   "original": "2088",
   "order": 265,
   "page_count": 5,
   "abstract": [
    "Talkers are identified more accurately when the language or accent is familiar to the listener. This is presumably due to access to linguistically relevant cues on top of lower-level acoustic information—an explanation implying that reliance on acoustic cues should decrease as language or accent familiarity increases. We tested this prediction by training Mandarin-speaking listeners to identify talkers while listening to Mandarin-accented English (MAE) and Mandarin (NM) and English (NE) speech produced by their respective native speakers. Using representational similarity analysis, we compared the listeners’ responses with the talkers’ acoustic features (e.g., F0, jitter) to assess acoustic-cue reliance. Results showed greater reliance on acoustic cues in less familiar contexts, supporting the prediction. Notably, in MAE, listeners initially relied more on acoustic cues but later shifted to reduced reliance, highlighting the dynamic nature of talker identification strategies."
   ],
   "p1": 1293,
   "pn": 1297,
   "doi": "10.21437/Interspeech.2025-2088",
   "url": "interspeech_2025/xiong25b_interspeech.html"
  },
  "miodonska25_interspeech": {
   "authors": [
    [
     "Zuzanna",
     "Miodonska"
    ]
   ],
   "title": "Hybrid HMM-SVM classifier using frication-based features for detection of non-normative sibilant articulation patterns in Polish children’s speech",
   "original": "2089",
   "order": 729,
   "page_count": 5,
   "abstract": [
    "This study proposes a method for detecting non-normative articulatory features of the retroflex voiceless fricative, which is among the last sounds to develop in Polish children&#x27;s speech. The approach integrates time-based phone articulation models with SVM in a hybrid binary classification method. The novelty lies in incorporating noise-based acoustic features for improved assessment and employing statistical temporal models to enhance detection accuracy. Results show that the proposed approach achieved over 86% accuracy detection for interdental and over 90% for addental articulation patterns. Dental articulation was most distinguishable, nearing 100% accuracy. Our findings indicate that adding noise features improves classification accuracy. This research contributes to the development of sigmatism diagnostic tools tailored for preschool and early school-age children, a group for whom automated pronunciation assessment remains an open challenge."
   ],
   "p1": 3563,
   "pn": 3567,
   "doi": "10.21437/Interspeech.2025-2089",
   "url": "interspeech_2025/miodonska25_interspeech.html"
  },
  "dindart25_interspeech": {
   "authors": [
    [
     "Juliette",
     "Dindart"
    ],
    [
     "Agnès",
     "Rouxel"
    ],
    [
     "Crystal",
     "Lin"
    ],
    [
     "Trung Kien",
     "Bui"
    ],
    [
     "Muriel",
     "Lefort"
    ],
    [
     "Claire",
     "Pillot-Loiseau"
    ],
    [
     "Christophe",
     "Trésallet"
    ],
    [
     "Frédérique",
     "Frouin"
    ]
   ],
   "title": "Study of vocal fold vibration using M-mode ultrasound: a proof of concept",
   "original": "2091",
   "order": 69,
   "page_count": 5,
   "abstract": [
    "Ultrasound has recently been suggested as an alternative to laryngoscopy for checking vocal fold movement after neck surgery. We propose to use M-mode ultrasound (MUS) to study vocal fold vibration in left and right hemilarynges. MUS is acquired along a 1D line in each hemilarynx for about 5 seconds during vowel phonation. Post-processing estimates spatio-temporal maps of fundamental and second harmonic frequencies. To validate our method, 108 recordings (MUS and voice) from 12 healthy subjects were acquired. We compared the fundamental frequency obtained by MUS with that from voice analysis. The median fundamental frequency was estimated by MUS with high accuracy (y=0.997x+0.293, r=0.999). In our current trial, estimated frequencies are limited to 250 Hz. In future work, frequency will be increased to 1 kHz to avoid aliasing at high pitch, and MUS will be tested on patients with vocal pathologies."
   ],
   "p1": 315,
   "pn": 319,
   "doi": "10.21437/Interspeech.2025-2091",
   "url": "interspeech_2025/dindart25_interspeech.html"
  },
  "morais25_interspeech": {
   "authors": [
    [
     "Edmilson",
     "Morais"
    ],
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Aharon",
     "Satt"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Avihu",
     "Dekel"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "George",
     "Saon"
    ]
   ],
   "title": "Exploring the Limits of Conformer CTC-Encoder for Speech Emotion Recognition using Large Language Models",
   "original": "2093",
   "order": 1110,
   "page_count": 5,
   "abstract": [
    "Conformer CTC-Encoders have consistently delivered state-of-the-art results in the field of Automatic Speech Recognition (ASR); however, their merits for tasks that demand more semantic and paralinguistic information, such as Automatic Speech Understanding (ASRU), Speech Emotion Recognition (SER) and Speech Translation (ST), still need further investigation. In this paper, we introduce a Speech Large Language Model (SLLM) system based on a Conformer CTC-Encoder and on the Granite Large Language Model that allowed us to perform several experiments on ASR, SER and ST tasks. These experiments have not only confirmed the strength of Conformer CTC-encoders for ASR, but also, they have shown that the outputs of intermediate Conformer Blocks, of the Conformer CTC-Encoder, carry important information for SER tasks and that the Conformer CTC-Encoder can be efficiently fine-tuned for SER tasks."
   ],
   "p1": 5443,
   "pn": 5447,
   "doi": "10.21437/Interspeech.2025-2093",
   "url": "interspeech_2025/morais25_interspeech.html"
  },
  "gogoi25_interspeech": {
   "authors": [
    [
     "Parismita",
     "Gogoi"
    ],
    [
     "Vishwanath",
     "Pratap Singh"
    ],
    [
     "Seema",
     "Khadirnaikar"
    ],
    [
     "Soma",
     "Siddhartha"
    ],
    [
     "Sishir",
     "Kalita"
    ],
    [
     "Jagabandhu",
     "Mishra"
    ],
    [
     "Md",
     "Sahidullah"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ],
    [
     "S. R. M.",
     "Prasanna"
    ]
   ],
   "title": "Leveraging AM and FM Rhythm Spectrograms for Dementia Classification and Assessment",
   "original": "2097",
   "order": 114,
   "page_count": 5,
   "abstract": [
    "This study explores the potential of Rhythm Formant Analysis (RFA) to capture long-term temporal modulations in dementia speech. Specifically, we introduce RFA-derived rhythm spectrograms as novel features for dementia classification and regression tasks. We propose two methodologies: (1) handcrafted features derived from rhythm spectrograms, and (2) a data-driven fusion approach, integrating proposed  RFA-derived rhythm spectrograms with vision transformer (ViT) for acoustic representations along with BERT-based linguistic embeddings. We compare these with existing features. Notably, our handcrafted features outperform eGeMAPs with a relative improvement of 14.2% in classification accuracy and comparable performance in the regression task. The fusion approach also shows improvement, with RFA spectrograms surpassing Mel spectrograms in classification by around a relative improvement of 13.1% and a comparable regression score with the baselines. All codes are available in GitHub repo."
   ],
   "p1": 539,
   "pn": 543,
   "doi": "10.21437/Interspeech.2025-2097",
   "url": "interspeech_2025/gogoi25_interspeech.html"
  },
  "akinrintoyo25_interspeech": {
   "authors": [
    [
     "Emmanuel",
     "Akinrintoyo"
    ],
    [
     "Nadine",
     "Abdelhalim"
    ],
    [
     "Nicole",
     "Salomons"
    ]
   ],
   "title": "WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper",
   "original": "2099",
   "order": 289,
   "page_count": 5,
   "abstract": [
    "Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns."
   ],
   "p1": 1413,
   "pn": 1417,
   "doi": "10.21437/Interspeech.2025-2099",
   "url": "interspeech_2025/akinrintoyo25_interspeech.html"
  },
  "grinberg25_interspeech": {
   "authors": [
    [
     "Petr",
     "Grinberg"
    ],
    [
     "Ankur",
     "Kumar"
    ],
    [
     "Surya",
     "Koppisetti"
    ],
    [
     "Gaurav",
     "Bharaj"
    ]
   ],
   "title": "A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations",
   "original": "2105",
   "order": 1091,
   "page_count": 5,
   "abstract": [
    "Evaluating explainability techniques, such as SHAP and LRP, in the context of audio deepfake detection is challenging due to lack of clear ground truth annotations. In the cases when we are able to obtain the ground truth, we find that these methods struggle to provide accurate explanations. In this work, we propose a novel data-driven approach to identify artifact regions in deepfake audio. We consider paired real and vocoded audio, and use the difference in time-frequency representation as the ground-truth explanation. The difference signal then serves as a supervision to train a diffusion model to expose the deepfake artifacts in a given vocoded audio. Experimental results on the VocV4 and LibriSeVoc datasets demonstrate that our method outperforms traditional explainability techniques, both qualitatively and quantitatively."
   ],
   "p1": 5348,
   "pn": 5352,
   "doi": "10.21437/Interspeech.2025-2105",
   "url": "interspeech_2025/grinberg25_interspeech.html"
  },
  "mitrofanov25_interspeech": {
   "authors": [
    [
     "Anton",
     "Mitrofanov"
    ],
    [
     "Sergey",
     "Novoselov"
    ],
    [
     "Tatiana",
     "Prisyach"
    ],
    [
     "Vladislav",
     "Marchevskiy"
    ],
    [
     "Arseniy",
     "Karelin"
    ],
    [
     "Nikita",
     "Khmelev"
    ],
    [
     "Dmitry",
     "Dutov"
    ],
    [
     "Stepan",
     "Malykh"
    ],
    [
     "Igor",
     "Agafonov"
    ],
    [
     "Aleksandr",
     "Nikitin"
    ],
    [
     "Oleg",
     "Petrov"
    ]
   ],
   "title": "Cryfish: On deep audio analysis with Large Language Models",
   "original": "2109",
   "order": 661,
   "page_count": 5,
   "abstract": [
    "The recent revolutionary progress in text-based large language models (LLMs) has contributed to the growth of interest in extending capabilities of such models to multimodal perception and understanding tasks. Hearing is an essential capability that is highly desired to be integrated into LLMs. However, effective integrating listening capabilities into LLMs is a significant challenge lying in generalizing complex auditory tasks across speech and sounds. To address these issues, we introduce Cryfish, our version of auditory-capable LLM. The model integrates WavLM audio-encoder features into Qwen2 model using a transformer-based connector. Cryfish is adapted to various auditory tasks through a specialized training strategy. We evaluate the model on the new Dynamic SUPERB Phase-2 comprehensive multitask benchmark specifically designed for auditory-capable models. The paper presents an in-depth analysis and detailed comparison of Cryfish with the publicly available models."
   ],
   "p1": 3249,
   "pn": 3253,
   "doi": "10.21437/Interspeech.2025-2109",
   "url": "interspeech_2025/mitrofanov25_interspeech.html"
  },
  "chen25n_interspeech": {
   "authors": [
    [
     "Chih-Ning",
     "Chen"
    ],
    [
     "Yu-Lan",
     "Chuang"
    ],
    [
     "Ming-Jhang",
     "Yang"
    ],
    [
     "Wei-Cheng",
     "Hsu"
    ],
    [
     "Yung-An",
     "Tsou"
    ],
    [
     "Yi-Wen",
     "Liu"
    ]
   ],
   "title": "Phonetic Posteriorgram-Based Phoneme Selection for Vocal Cord Disorder Classification in Continuous Mandarin Speech",
   "original": "2110",
   "order": 727,
   "page_count": 5,
   "abstract": [
    "Automatic classification of vocal cord disorders (VCDs) in dysphonia benefits society by enabling home screening when immediate clinical consultation is unavailable. Previous studies focused on VCD classification using single vowels or isolated words. This research advances the field by classifying VCDs from continuous speech, using data from diagnosed patients. By parsing continuous speech into phonemes based on phonetic posteriorgrams (PPGs), we investigated VCD classification using the Mel frequency cepstral coefficients (MFCCs) corresponding to each Mandarin phoneme as the features. Results show a 15% accuracy improvement over a baseline model that ignores phonetic context and a prior study on the same patients using single-word utterances and sustained vowels. Our findings enhance the understanding of phonetic characteristics in VCDs and underscore the significance of continuous speech in automatic classification."
   ],
   "p1": 3553,
   "pn": 3557,
   "doi": "10.21437/Interspeech.2025-2110",
   "url": "interspeech_2025/chen25n_interspeech.html"
  },
  "chiang25_interspeech": {
   "authors": [
    [
     "Hsin-Tien",
     "Chiang"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "A Deformable Convolution GAN Approach for Speech Dereverberation in Cochlear Implant Users",
   "original": "2111",
   "order": 173,
   "page_count": 5,
   "abstract": [
    "Speech dereverberation is crucial for enhancing intelligibility and quality, especially for cochlear implant (CI) users, who are highly susceptible to smearing effects induced by reverberation. While conventional and deep learning-based methods have shown promise for normal-hearing (NH) individuals, their effectiveness for CI users remains limited. To bridge this gap, we propose a deformable convolutional GAN architecture for dereverberation for CI users. The deformable convolution layers introduce kernel offset prediction, adaptively adjusting the receptive field based on distortion in reverberant speech. We first evaluate the effectiveness of the proposed method on REVERB challenge dataset. A listening test is conducted with both NH and CI users. Results show that the proposed method markedly improves speech intelligibility for CI users by preserving a more intact envelope structure, enhancing their ability to perceive key transient speech segments for sentence comprehension."
   ],
   "p1": 833,
   "pn": 837,
   "doi": "10.21437/Interspeech.2025-2111",
   "url": "interspeech_2025/chiang25_interspeech.html"
  },
  "mote25_interspeech": {
   "authors": [
    [
     "Pravin",
     "Mote"
    ],
    [
     "Abinay Reddy",
     "Naini"
    ],
    [
     "Donita",
     "Robinson"
    ],
    [
     "Elizabeth",
     "Richerson"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Analysis of Phonetic Level Similarities Across Languages in Emotional Speech",
   "original": "2112",
   "order": 885,
   "page_count": 5,
   "abstract": [
    "Unsupervised domain adaptation offers significant potential for cross-lingual speech emotion recognition (SER). Most relevant studies have addressed this problem as a domain mismatch without considering phonetical emotional differences across languages. Our study explores universal discrete speech units obtained with vector quantization of wavLM representations from emotional speech in English, Taiwanese Mandarin, and Russian. We estimate cluster-wise distributions of quantized wavLM frames to quantify phonetic commonalities and differences across languages, vowels, and emotions. Our findings indicate that certain emotion-specific phonemes exhibit cross-linguistic similarities. The distribution of vowels varies with emotional content. Certain vowels across languages show close distributional proximity, offering anchor points for cross-lingual domain adaptation. We also propose and validate a method to quantify phoneme distribution similarities across languages."
   ],
   "p1": 4343,
   "pn": 4347,
   "doi": "10.21437/Interspeech.2025-2112",
   "url": "interspeech_2025/mote25_interspeech.html"
  },
  "kulkarni25b_interspeech": {
   "authors": [
    [
     "Ajinkya",
     "Kulkarni"
    ],
    [
     "Francisco",
     "Teixeira"
    ],
    [
     "Enno",
     "Hermann"
    ],
    [
     "Thomas",
     "Rolland"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Children's Voice Privacy: First Steps and Emerging Challenges",
   "original": "2117",
   "order": 573,
   "page_count": 5,
   "abstract": [
    "Children are one of the most under-represented groups in speech technologies, as well as one of the most vulnerable in terms of privacy. Despite this, anonymization techniques targeting this population have received little attention. In this study, we seek to bridge this gap, and establish a baseline for the use of voice anonymization techniques designed for adult speech when applied to children&#x27;s voices. Such an evaluation is essential, as children&#x27;s speech presents a distinct set of challenges when compared to that of adults. This study comprises three children’s datasets, six anonymization methods, and objective and subjective utility metrics for evaluation. Our results show that existing systems for adults are still able to protect children&#x27;s voice privacy, but suffer from much higher utility degradation. In addition, our subjective study displays the challenges of automatic evaluation methods for speech quality in children&#x27;s speech, highlighting the need for further research."
   ],
   "p1": 2810,
   "pn": 2814,
   "doi": "10.21437/Interspeech.2025-2117",
   "url": "interspeech_2025/kulkarni25b_interspeech.html"
  },
  "dey25_interspeech": {
   "authors": [
    [
     "Spandan",
     "Dey"
    ],
    [
     "Hirak",
     "Mondal"
    ],
    [
     "Sanjay Kumar",
     "Kurmi"
    ]
   ],
   "title": "Teacher-Free Knowledge Distillation for Improving Short-Utterance Spoken Language Identification",
   "original": "2120",
   "order": 303,
   "page_count": 5,
   "abstract": [
    "Spoken language identification (LID) systems exhibit performance degradations as the test input duration reduces. To delve deeper, we show that 36.94% of the misclassifications on 2-second (s) LID inputs occur due to out-of-scope elements like non-speech, named entities, filler words, and overlapped speech. To mitigate this, we propose a teacher-free knowledge distillation (TF-KD) using online label smoothing. This method accumulates prediction logits of correctly classified training segments from the preceding epoch and uses them as soft-labels for distillation in the next epoch. We further enhance TF-KD with dynamic weights, conditional label update, and entropy-based soft-label computation. Compared to existing KD-based solutions for 2s inputs, our approach achieves consistent Cavg improvements for both same-corpora and cross-corpora evaluations without training a separate teacher network."
   ],
   "p1": 1483,
   "pn": 1487,
   "doi": "10.21437/Interspeech.2025-2120",
   "url": "interspeech_2025/dey25_interspeech.html"
  },
  "alizadeh25_interspeech": {
   "authors": [
    [
     "Hadi",
     "Alizadeh"
    ],
    [
     "Rahil",
     "Mahdian Toroghi"
    ],
    [
     "Hassan",
     "Zareian"
    ]
   ],
   "title": "ReSepNet: A Unified-Light Model for Recursive Speech Separation with Unknown Speaker Count",
   "original": "2121",
   "order": 298,
   "page_count": 5,
   "abstract": [
    "Single-channel speech separation remains a significant challenge, particularly when the number of concurrent speakers is unknown. Existing methods often rely on prior assumptions about speaker count, limiting their real-world applicability. This paper introduces ReSepNet (Recursive Separation Network), a novel, unified model for speaker-independent speech separation that dynamically adapts to an unknown number of speakers. ReSepNet employs a recursive separation architecture, enabling it to iteratively isolate individual voices without prior knowledge of the speaker count. To enhance efficiency and reduce model complexity, we introduce a novel objective function and workflow. We demonstrate the effectiveness of ReSepNet on the WSJ0 datasets, achieving state-of-the-art separation performance and accurate speaker count estimation. Furthermore, ReSepNet generalizes well to mixtures containing four and five speakers, showcasing its robustness and adaptability to challenging scenarios."
   ],
   "p1": 1458,
   "pn": 1462,
   "doi": "10.21437/Interspeech.2025-2121",
   "url": "interspeech_2025/alizadeh25_interspeech.html"
  },
  "mote25b_interspeech": {
   "authors": [
    [
     "Pravin",
     "Mote"
    ],
    [
     "Donita",
     "Robinson"
    ],
    [
     "Elizabeth",
     "Richerson"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Vector Quantized Cross-lingual Unsupervised Domain Adaptation  for Speech Emotion Recognition",
   "original": "2123",
   "order": 27,
   "page_count": 5,
   "abstract": [
    "Building speech emotion recognition (SER) models for low-resource languages is challenging due to the scarcity of labeled speech data. This limitation mandates the development of cross-lingual unsupervised domain adaptation techniques to effectively utilize labeled data from resource-rich languages. Inspired by the TransVQA framework, we propose a method that leverages a shared quantized feature space to enable knowledge transfer between labeled and unlabeled data across languages. The approach utilizes a quantized codebook to capture shared features, while reducing the domain gap, and aligning class distributions, thereby improving classification accuracy. Additionally, an information loss (InfoLoss) mechanism mitigates critical information loss during quantization. InfoLoss achieves this goal by minimizing the loss within the simplex of posterior class label distributions. The proposed method demonstrates superior performance compared to state-of-the-art baseline approaches."
   ],
   "p1": 126,
   "pn": 130,
   "doi": "10.21437/Interspeech.2025-2123",
   "url": "interspeech_2025/mote25b_interspeech.html"
  },
  "hartmann25_interspeech": {
   "authors": [
    [
     "Carlos",
     "Hartmann"
    ]
   ],
   "title": "Reddit FlairShare: A Human-Annotated Dataset of Gender-Progressive Online Discourse",
   "original": "2128",
   "order": 143,
   "page_count": 5,
   "abstract": [
    "This paper presents a large-scale dataset capturing Reddit comments with pronoun declarations in the respective user flairs, offering a new resource for studying linguistic identity, gender expression, and digital discourse. Totaling 72 million tokens, it contains all comments by pronoun-declaring users to present a broader view of their language use than previous corpora that selected isolated utterances. The dataset enables research across multiple domains, including (online) sociolinguistics, natural language processing (NLP), and other social sciences. It facilitates the study of pronoun-sharing behavior, the distribution and adoption of non-binary pronouns, and the use of mixed pronouns in online discourse. Future work can expand the dataset to capture more rare pronoun declarations; nevertheless, it provides a highly curated, valuable foundation for the study of online gender expression and discourse, innovative language, and identity performance in digital spaces."
   ],
   "p1": 684,
   "pn": 688,
   "doi": "10.21437/Interspeech.2025-2128",
   "url": "interspeech_2025/hartmann25_interspeech.html"
  },
  "gogoi25b_interspeech": {
   "authors": [
    [
     "Parismita",
     "Gogoi"
    ],
    [
     "Sishir",
     "Kalita"
    ],
    [
     "Wendy",
     "Lalhminghlui"
    ],
    [
     "Viyazonuo",
     "Terhiija"
    ],
    [
     "Moakala",
     "Tzudir"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ],
    [
     "S. R. M.",
     "Prasanna"
    ]
   ],
   "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models",
   "original": "2135",
   "order": 851,
   "page_count": 5,
   "abstract": [
    "This study explores the use of self-supervised learning (SSL) models for tone recognition in three low-resource languages from North Eastern India: Angami, Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on both tonal and non-tonal languages. We analyze tone-wise performance across the layers for all three languages and compare the different models. Our results show that tone recognition works best for Mizo and worst for Angami. The middle layers of the SSL models are the most important for tone recognition, regardless of the pre-training language, i.e. tonal or non-tonal. We have also found that the tone inventory, tone types, and dialectal variations affect tone recognition. These findings provide useful insights into the strengths and weaknesses of SSL-based embeddings for tonal languages and highlight the potential for improving tone recognition in low-resource settings. The source code is available at GitHub."
   ],
   "p1": 4173,
   "pn": 4177,
   "doi": "10.21437/Interspeech.2025-2135",
   "url": "interspeech_2025/gogoi25b_interspeech.html"
  },
  "lechler25_interspeech": {
   "authors": [
    [
     "Laura",
     "Lechler"
    ],
    [
     "Chamran",
     "Moradi"
    ],
    [
     "Ivana",
     "Balic"
    ]
   ],
   "title": "Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies: A Comparative Analysis of Subjective and Objective Testing Methods",
   "original": "2138",
   "order": 643,
   "page_count": 5,
   "abstract": [
    "The MUSHRA framework is widely used for detecting subtle audio quality differences but traditionally relies on expert listeners in controlled environments, making it costly and impractical for model development. As a result, objective metrics are often used during development, with expert evaluations conducted later. While effective for traditional DSP codecs, these metrics often fail to reliably evaluate generative models. This paper proposes adaptations for conducting MUSHRA tests with non-expert, crowdsourced listeners, focusing on generative speech codecs. We validate our approach by comparing results from MTurk and Prolific crowdsourcing platforms with expert listener data, assessing test-retest reliability and alignment. Additionally, we evaluate six objective metrics, showing that traditional metrics undervalue generative models. Our findings reveal platform-specific biases and emphasize codec-aware metrics, offering guidance for scalable perceptual testing of speech codecs."
   ],
   "p1": 3160,
   "pn": 3164,
   "doi": "10.21437/Interspeech.2025-2138",
   "url": "interspeech_2025/lechler25_interspeech.html"
  },
  "wang25y_interspeech": {
   "authors": [
    [
     "Weiqing",
     "Wang"
    ],
    [
     "Taejin",
     "Park"
    ],
    [
     "Ivan",
     "Medennikov"
    ],
    [
     "Jinhan",
     "Wang"
    ],
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Nithin",
     "Rao Koluguri"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR",
   "original": "2142",
   "order": 1121,
   "page_count": 5,
   "abstract": [
    "We propose a self-speaker adaptation method for streaming multi-talker automatic speech recognition (ASR) that eliminates the need for explicit speaker queries. Unlike conventional approaches requiring target speaker embeddings or enrollment audio, our technique dynamically adapts individual ASR instances through speaker-wise speech activity prediction. The key innovation involves injecting speaker-specific kernels generated via speaker supervision activations into selected ASR encoder layers. This enables instantaneous speaker adaptation to target speakers while handling fully overlapped speech even in a streaming scenario. Experiments show state-of-the-art performance in both offline and streaming scenarios, demonstrating that our self-adaptive method effectively addresses severe speech overlap through streamlined speaker-focused recognition. The results validate the proposed self-speaker adaptation approach as a robust solution for multi-talker ASR under severe overlapping speech conditions."
   ],
   "p1": 5498,
   "pn": 5502,
   "doi": "10.21437/Interspeech.2025-2142",
   "url": "interspeech_2025/wang25y_interspeech.html"
  },
  "ivucic25_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Ivucic"
    ],
    [
     "Saurav",
     "Pahuja"
    ],
    [
     "Dashanka",
     "Da Silva"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Selective Auditory Attention Decoding in Naturalistic Conversations Using EEG-Based Speech Envelope Tracking in Multi-Speaker Environments",
   "original": "2143",
   "order": 596,
   "page_count": 5,
   "abstract": [
    "In daily life, we effortlessly switch attention between different sound sources while filtering out distractions. Prior research has shown that the target speaker can be decoded from EEG in such settings, however, how the switching of attention influences decoding has not been measured. We investigated EEG-based target speaker decoding with exogenous attention switches in a multi-talker environment, where twenty participants alternated listening to two target speakers taking turns while ignoring two background speakers. Speech envelope reconstruction from EEG showed higher accuracy for the attended speaker than distractors, with single-trial classification at 76%. Our findings showed a brief increase in target speaker reconstruction accuracy after attention switches, suggesting heightened alertness when shifting attention to a new speaker. These findings enhance the understanding of dynamic auditory attention and support the development of attention-based hearing applications."
   ],
   "p1": 2925,
   "pn": 2929,
   "doi": "10.21437/Interspeech.2025-2143",
   "url": "interspeech_2025/ivucic25_interspeech.html"
  },
  "ibrahimov25_interspeech": {
   "authors": [
    [
     "Ibrahim",
     "Ibrahimov"
    ],
    [
     "Csaba",
     "Zainkó"
    ],
    [
     "Gábor",
     "Gosztolya"
    ]
   ],
   "title": "Conformer-based Ultrasound-to-Speech Conversion",
   "original": "2147",
   "order": 1137,
   "page_count": 5,
   "abstract": [
    "Deep neural networks have shown promising potential for ultrasound-to-speech conversion task towards Silent Speech Interfaces. In this work, we applied two Conformer-based DNN architectures (Base and one with bi-LSTM) for this task. Speaker-specific models were trained on the data of four speakers from the Ultrasuite-Tal80 dataset, while the generated mel spectrograms were synthesized to audio waveform using a HiFi-GAN vocoder. Compared to a standard 2D-CNN baseline, objective measurements (MSE and mel cepstral distortion) showed no statistically significant improvement for either model. However, a MUSHRA listening test revealed that Conformer with bi-LSTM provided better perceptual quality, while Conformer Base matched the performance of the baseline along with a 3× faster training time due to its simpler architecture. These findings suggest that Conformer-based models, especially the Conformer with bi-LSTM, offer a promising alternative to CNNs for ultrasound-to-speech conversion."
   ],
   "p1": 5578,
   "pn": 5582,
   "doi": "10.21437/Interspeech.2025-2147",
   "url": "interspeech_2025/ibrahimov25_interspeech.html"
  },
  "serajian25_interspeech": {
   "authors": [
    [
     "Mina",
     "Serajian"
    ],
    [
     "Saeed Najafzadeh",
     "Rahaghi"
    ],
    [
     "Hadi",
     "Veisi"
    ],
    [
     "Saman",
     "Haratizadeh"
    ]
   ],
   "title": "FaVC: A Validated, Transcribed, Parallel Farsi Speech Dataset for Voice Conversion",
   "original": "2151",
   "order": 974,
   "page_count": 5,
   "abstract": [
    "In this paper, we created the first transcribed, parallel, balanced Farsi dataset (FaVC) that can be used for all tasks of speech synthesis, including both parallel and non-parallel voice conversion. FaVC is a balanced dataset that provides all phonemes of the Farsi language in all possible phonetic combinations. A metadata file is provided, including Farsi transcriptions, normal text, and IPA phoneme transcriptions of all audio files. The first Farsi voice conversion results using FaVC are also reported in this paper. The results indicate that by using FaVC, performance is as good as the chosen baseline methods for voice conversion in English. Moreover, objective evaluations of a voice conversion system that requires a parallel speech corpus can be performed using this dataset."
   ],
   "p1": 4788,
   "pn": 4792,
   "doi": "10.21437/Interspeech.2025-2151",
   "url": "interspeech_2025/serajian25_interspeech.html"
  },
  "wagner25_interspeech": {
   "authors": [
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Natalie",
     "Engert"
    ],
    [
     "Seanie",
     "Lee"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Personalized Fine-Tuning with Controllable Synthetic Speech from LLM-Generated Transcripts for Dysarthric Speech Recognition",
   "original": "2155",
   "order": 670,
   "page_count": 5,
   "abstract": [
    "In this work, we present our submission to the Speech Accessibility Project challenge for dysarthric speech recognition. We integrate parameter-efficient fine-tuning with latent audio representations to improve an encoder-decoder ASR system. Synthetic training data is generated by fine-tuning Parler-TTS to mimic dysarthric speech, using LLM-generated prompts for corpus-consistent target transcripts. Personalization with x-vectors consistently reduces word error rates (WERs) over non-personalized fine-tuning. AdaLoRA adapters outperform full fine-tuning and standard low-rank adaptation, achieving relative WER reductions of ~23% and ~22%, respectively. Further improvements (~5% WER reduction) come from incorporating wav2vec 2.0-based audio representations. Training with synthetic dysarthric speech yields up to ~7% relative WER improvement over personalized fine-tuning alone."
   ],
   "p1": 3294,
   "pn": 3298,
   "doi": "10.21437/Interspeech.2025-2155",
   "url": "interspeech_2025/wagner25_interspeech.html"
  },
  "shim25_interspeech": {
   "authors": [
    [
     "Ha Eun",
     "Shim"
    ],
    [
     "Olivia",
     "Yung"
    ],
    [
     "Paige",
     "Tuttösí"
    ],
    [
     "Boey",
     "Kwan"
    ],
    [
     "Angelica",
     "Lim"
    ],
    [
     "Yue",
     "Wang"
    ],
    [
     "H. Henny",
     "Yeung"
    ]
   ],
   "title": "Generating Consistent Prosodic Patterns from Open-Source TTS Systems",
   "original": "2159",
   "order": 1098,
   "page_count": 5,
   "abstract": [
    "Text-to-Speech (TTS) systems now closely approximate human speech prosody. Yet, current deep learning-based TTS systems may struggle to accurately represent some prosodic patterns, like phrase boundaries used to signal syntactic distinctions. Such prosodic parsing can reflect differences in meaning, hence, inconsistencies in synthesis can lead to miscommunications. In this study, we conduct a qualitative assessment of five open-source TTS systems and reveal that they fail to produce acoustic signals that accurately convey distinct prosodic boundaries when given punctuation contrasts (Study 1). To mitigate this gap, we propose a pipeline for improving output using a customized dataset (Study 2), which successfully generates predictable acoustic cues, but only for certain cases. Results suggest that TTS systems require additional training to effectively capture the prosodic subtleties. We conclude by discussing how TTS systems can better generate fine prosodic distinctions."
   ],
   "p1": 5383,
   "pn": 5387,
   "doi": "10.21437/Interspeech.2025-2159",
   "url": "interspeech_2025/shim25_interspeech.html"
  },
  "funfgeld25_interspeech": {
   "authors": [
    [
     "Sophia",
     "Fünfgeld"
    ],
    [
     "Angelika",
     "Braun"
    ],
    [
     "Katharina",
     "Zahner-Ritter"
    ]
   ],
   "title": "Are You Being Sarcastic? Prosodic Cues to Irony Perception in German",
   "original": "2161",
   "order": 1096,
   "page_count": 5,
   "abstract": [
    "This perception study investigated the role of intonation, specifically pitch accent position and pitch accent type, in the interpretation of utterances as sarcastic. Participants from two regions in Germany, Freiburg and Trier, listened to short utterances such as Das sieht ja umwerfend aus (“That looks stunning”) in seven prosodic conditions. The recordings were taken from a production study, and participants classified them as sarcastic or sincere. Results show that in both regions, irony perception is driven by (a) the existence of a prenuclear accent and (b) the type of the nuclear accent, particularly L*+H. Reaction times for ironic responses were shorter for L*+H, as well as when a prenuclear accent was present or when the subject carried the nuclear accent. These findings underscore the importance of intonation in irony perception and have implications for the processing of non-canonical meaning in general and the interpretation of sarcasm across varieties in particular."
   ],
   "p1": 5373,
   "pn": 5377,
   "doi": "10.21437/Interspeech.2025-2161",
   "url": "interspeech_2025/funfgeld25_interspeech.html"
  },
  "talkar25_interspeech": {
   "authors": [
    [
     "Tanya",
     "Talkar"
    ],
    [
     "Kan",
     "Kawabata"
    ],
    [
     "Connor",
     "Higgins"
    ],
    [
     "Sean",
     "Tobyne"
    ]
   ],
   "title": "Development and Validation of a Wav2Vec 2.0-Based Cross-Language Methodology for Measurement of Articulatory Precision",
   "original": "2162",
   "order": 766,
   "page_count": 5,
   "abstract": [
    "Degraded speech intelligibility due to dysarthrias can result from changes inherent to diseases such as Amyotrophic Lateral Sclerosis (ALS). Tracking this degradation longitudinally can highlight treatment effects. Various approaches have been developed to automatically measure speech intelligibility, often focusing on output from automatic speech recognition systems. In this paper, we extract phonetic outputs from the wav2vec 2.0 model and compare them to expected phonetic outputs across 12 languages to derive Articulatory Precision (ArtP). The strongest correlations between ArtP and Azure Pronunciation Assessment were 0.93 for English, 0.85 in German, and 0.66 for Swedish. We additionally find that ArtP has a correlation of 0.77 with the ALSFRS-R speech subscore and is sensitive to perceptual measures of speech intelligibility. The measure shows promise as a reliable and clinically relevant tool that can be used in multiple languages and disorders to assess speech intelligibility."
   ],
   "p1": 3748,
   "pn": 3752,
   "doi": "10.21437/Interspeech.2025-2162",
   "url": "interspeech_2025/talkar25_interspeech.html"
  },
  "kang25c_interspeech": {
   "authors": [
    [
     "Fang",
     "Kang"
    ],
    [
     "Yin",
     "Cao"
    ],
    [
     "Haoyu",
     "Chen"
    ]
   ],
   "title": "Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation",
   "original": "2163",
   "order": 770,
   "page_count": 5,
   "abstract": [
    "Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity &amp; Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU."
   ],
   "p1": 3768,
   "pn": 3772,
   "doi": "10.21437/Interspeech.2025-2163",
   "url": "interspeech_2025/kang25c_interspeech.html"
  },
  "alderete25_interspeech": {
   "authors": [
    [
     "John",
     "Alderete"
    ],
    [
     "Macarious Kin Fung",
     "Hui"
    ],
    [
     "Aanchan",
     "Mohan"
    ]
   ],
   "title": "Evaluating ASR Robustness to Spontaneous Speech Errors: A Study of WhisperX Using a Speech Error Database",
   "original": "2164",
   "order": 368,
   "page_count": 5,
   "abstract": [
    "The Simon Fraser University Speech Error Database (SFUSED) is a public data collection developed for linguistic and psycholinguistic research. Here we demonstrate how its design and annotations can be used to test and evaluate speech recognition models. The database comprises systematically annotated speech errors from spontaneous English speech, with each error tagged for intended and actual error productions. The annotation schema incorporates multiple classificatory dimensions that are of some value to model assessment, including linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and both word-level and syllable-level error positioning. To assess the value of these classificatory variables, we evaluated the transcription accuracy of WhisperX across 5,300 documented word and phonological errors. This analysis demonstrates the database&#x27;s effectiveness as a diagnostic tool for ASR system performance."
   ],
   "p1": 1803,
   "pn": 1807,
   "doi": "10.21437/Interspeech.2025-2164",
   "url": "interspeech_2025/alderete25_interspeech.html"
  },
  "ferrofilho25_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Ferro Filho"
    ],
    [
     "Diogo",
     "Fernandes Costa Silva"
    ],
    [
     "Pedro Elias",
     "Engelberg Silva Borges"
    ],
    [
     "Arlindo Rodrigues",
     "Galvão Filho"
    ]
   ],
   "title": "Evaluating Deep Speaker Embedding Robustness to Domain, Sampling Rate, and Codec Variations",
   "original": "2167",
   "order": 229,
   "page_count": 5,
   "abstract": [
    "Speaker verification systems based on deep speaker embeddings perform well under matched training and evaluation conditions, but performance degrades under domain shifts. This work evaluates the sensitivity of four speaker embedding models—ECAPA-TDNN, TitaNet, ECAPA2, and ReDimNet—to variations in acoustic domains, sampling rates, and audio codecs. Experiments on datasets with far-field speech, noise, and music interference demonstrate that all models degrade under mismatched conditions. ReDimNet demonstrates the smallest performance degradation compared to the other models but is still affected in certain cases. Downsampling and low-bitrate compression further degrade performance, revealing the reliance of models on high-frequency information and their sensitivity to compression artifacts."
   ],
   "p1": 1113,
   "pn": 1117,
   "doi": "10.21437/Interspeech.2025-2167",
   "url": "interspeech_2025/ferrofilho25_interspeech.html"
  },
  "parvathala25b_interspeech": {
   "authors": [
    [
     "Venkatesh",
     "Parvathala"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "MSFNet: A Nested Model for Multi-Sampling-Frequency Speech Enhancement",
   "original": "2168",
   "order": 1048,
   "page_count": 5,
   "abstract": [
    "Recently, speech enhancement research has shifted towards universal models handling various input conditions under multiple degradations. While diverse training data improves robustness, handling data acquisition variations like sampling frequency (SF) requires architectural changes. This study focuses on SF dependency in networks involving fully-connected (FC) layers, as current fixed-resolution STFT-based methods are not directly compatible with FC layers. We propose a multi-SF network (MSFNet) incorporating an FC layer with the nested structure and learnable weight scaling (LWS) mechanism. LWS mitigates the distributional changes at the FC layer&#x27;s output, and the nested structure allows compact submodel extraction. Our experiments demonstrate that the proposed method is computationally efficient and performs comparably to the independently trained models. Notably, an 8 kHz model has 33 times fewer parameters and 6 times fewer multiply-accumulate operations than a 48 kHz model."
   ],
   "p1": 5138,
   "pn": 5142,
   "doi": "10.21437/Interspeech.2025-2168",
   "url": "interspeech_2025/parvathala25b_interspeech.html"
  },
  "malykh25_interspeech": {
   "authors": [
    [
     "Stepan",
     "Malykh"
    ],
    [
     "Alexander",
     "Anikin"
    ],
    [
     "Nikita",
     "Khmelev"
    ],
    [
     "Anastasia",
     "Korenevskaya"
    ],
    [
     "Anastasia",
     "Zorkina"
    ],
    [
     "Sergey",
     "Novoselov"
    ],
    [
     "Vladislav",
     "Marchevskiy"
    ],
    [
     "Vladimir",
     "Volokhov"
    ],
    [
     "Andrey",
     "Shulipa"
    ],
    [
     "Alexander",
     "Kozlov"
    ],
    [
     "Alexander",
     "Melnikov"
    ],
    [
     "Vasiliy",
     "Galyuk"
    ],
    [
     "Timur",
     "Pekhovskiy"
    ]
   ],
   "title": "STCON NIST SRE24 System: Composite Speaker Recognition Solution for Challenging Scenarios",
   "original": "2170",
   "order": 813,
   "page_count": 5,
   "abstract": [
    "This paper addresses the real-world challenges of voice biometrics, specifically those highlighted by the National Institute of Standards and Technology Speaker Recognition (SR) Evaluation 2024 challenge (NIST SRE24). We present multi-module SR systems integrating speaker diarization, robust embedding extraction, and adaptive scoring to handle multi-speaker recordings, variable speech segment durations, and cross-channel/cross-lingual speaker recognition scenarios. We propose methodology for training robust speaker embedding extractors and explore a range of state-of-the-art SR neural network architectures. We focus on the combined optimization of traditionally separate components – diarization and speaker recognition – and present practical observations regarding this integrated approach. The presented findings allowed our systems to secure top leaderboard positions in the NIST SRE24."
   ],
   "p1": 3983,
   "pn": 3987,
   "doi": "10.21437/Interspeech.2025-2170",
   "url": "interspeech_2025/malykh25_interspeech.html"
  },
  "phukan25_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Priyabrata",
     "Mallick"
    ],
    [
     "Pailla Balakrishna",
     "Reddy"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models",
   "original": "2171",
   "order": 341,
   "page_count": 5,
   "abstract": [
    "In this work, we introduce the task of singing voice deepfake source attribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs) such as ImageBind, LanguageBind will be most effective for SVDSA as they are better equipped for capturing subtle source-specific characteristics—such as unique timbre, pitch manipulation, or synthesis artifacts of each singing voice deepfake source due to their crossmodality pre-training. Our experiments with MMFMs, speech foundation models and music foundation models verify the hypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired from related research, we also explore fusion of foundation models (FMs) for improved SVDSA. To this end, we propose a novel framework, COFFE which employs Chernoff Distance as novel loss function for effective fusion of FMs. Through COFFE with the symphony of MMFMs, we attain the topmost performance in comparison to all the individual FMs and baseline fusion methods."
   ],
   "p1": 1673,
   "pn": 1677,
   "doi": "10.21437/Interspeech.2025-2171",
   "url": "interspeech_2025/phukan25_interspeech.html"
  },
  "gebauer25_interspeech": {
   "authors": [
    [
     "Christopher",
     "Gebauer"
    ],
    [
     "Lars",
     "Rumberg"
    ],
    [
     "Lars",
     "Köhn"
    ],
    [
     "Hanna",
     "Ehlert"
    ],
    [
     "Edith",
     "Beaulac"
    ],
    [
     "Jörn",
     "Ostermann"
    ]
   ],
   "title": "Grammatical Error Detection on Spontaneous Children's Speech Using Iterative Pseudo Labeling",
   "original": "2174",
   "order": 584,
   "page_count": 5,
   "abstract": [
    "Language acquisition is fundamental for the development of various skills in the early stage of a children’s life. Unfortunately, developmental language disorder (DLD) is the most common developmental disorder during childhood. A common indicator of DLD is that children with such condition struggle to correctly use grammatical forms. Therefore, we focus in this work on automatic grammatical error detection on spontaneous children’s speech. We extend the state of the art by an iterative pseudo labeling scheme to account for the ambiguity of grammatical error labels. Such ambiguity becomes obvious, when it is unclear which word is incorrect, e. g., for agreement errors. In terms of the F1 gain score (FG1) we significantly improve upon the baseline on sentence- and word-level label. On automatic transcriptions of the kidsTALC corpus we increase the sentence-level FG1 from 0.38 to 0.63. Further, our best performing system achieves a recall of 0.45, while maintaining a precision of 0.36."
   ],
   "p1": 2865,
   "pn": 2869,
   "doi": "10.21437/Interspeech.2025-2174",
   "url": "interspeech_2025/gebauer25_interspeech.html"
  },
  "szmajdzinski25_interspeech": {
   "authors": [
    [
     "Szymon",
     "Szmajdziński"
    ],
    [
     "Juliusz",
     "Wójtowicz-Kruk"
    ],
    [
     "Ivan",
     "Ryzhankow"
    ],
    [
     "Łukasz",
     "Łazarski"
    ],
    [
     "Jakub",
     "Żak"
    ],
    [
     "Władysław",
     "Średniawa"
    ]
   ],
   "title": "Significance of Time-Frequency preprocessing for automatic Ultrasonic Vocalization classification in Autism Spectrum Disorder model detection",
   "original": "2175",
   "order": 351,
   "page_count": 5,
   "abstract": [
    "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition of unclear cause and varying severity, often studied using mice as animal models. To accurately distinguish between wild type and ASD model phenotypes, we present a deep learning approach that uses ultrasonic vocalization as input. The proposed method combines a simple model architecture, of convolutional and fully connected layers, with a high-resolution representation of auditory and ultrasound time-frequency patterns. Our approach surpasses baseline performance, achieving an unweighted average recall score of 0.806 on 30-second ultrasonic vocalization fragments. This work was conducted for the 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization (MAD-UV) Challenge, where it achieved the highest score among all submitted solutions."
   ],
   "p1": 1723,
   "pn": 1727,
   "doi": "10.21437/Interspeech.2025-2175",
   "url": "interspeech_2025/szmajdzinski25_interspeech.html"
  },
  "dinh25_interspeech": {
   "authors": [
    [
     "Quang Minh",
     "Dinh"
    ],
    [
     "Hoda",
     "Rezaee Kaviani"
    ],
    [
     "Mehrdad",
     "Hosseinzadeh"
    ],
    [
     "Yuanhao",
     "Yu"
    ]
   ],
   "title": "Extended Loss: Incorporating Long Context into Training Models when using Short Audio Frames",
   "original": "2177",
   "order": 162,
   "page_count": 5,
   "abstract": [
    "Recently deep learning solutions have been successfully applied to many signal processing tasks including acoustic echo cancellation (AEC). Most existing works focus on architecture design and ignore practical issues such as the effect of frame length on the performance of end-to-end AEC models. In real-time applications, frame length can be as small as 10ms. Since the observed context is very limited during training, it results in boundary discontinuities (glitches) in the final output. While using long frames or post-processing can help, it adds extra delay which may not be desirable depending on the application. In this paper, we investigate the practical issue of handling short frames for AEC, and propose an efficient remedy. By keeping the long context information in each batch and using it during loss calculation, we compensate for the short frames. Our solution is model-agnostic and does not affect the inference time."
   ],
   "p1": 778,
   "pn": 782,
   "doi": "10.21437/Interspeech.2025-2177",
   "url": "interspeech_2025/dinh25_interspeech.html"
  },
  "maciejewski25_interspeech": {
   "authors": [
    [
     "Matthew",
     "Maciejewski"
    ]
   ],
   "title": "Speaker Conditioning of Voice Activity Detection via Implicit Separation",
   "original": "2178",
   "order": 1181,
   "page_count": 5,
   "abstract": [
    "Within the domain of multi-talker recordings, many speech technologies rely on an initial segmentation step of finding when each person was talking. One common approach to this task is Target-Speaker Voice Activity Detection (TS-VAD), in which a model is supplied with a representation corresponding to a particular speaker and then identifies the temporal regions when that person was talking. As in many cases, the increased complexity of this task over regular Voice Activity Detection (VAD) imposes constraints on the data used to train such a model. In this work, we explore conversion of a pre-trained VAD model into a TS-VAD model via an implicitly-trained separation front end—decoupling the need for speaker-discriminative training data from the basic speech/non-speech data used in training VAD models—which can lead to improvements in model robustness and speech recall in the domains present only in the training data of the VAD."
   ],
   "p1": 5798,
   "pn": 5802,
   "doi": "10.21437/Interspeech.2025-2178",
   "url": "interspeech_2025/maciejewski25_interspeech.html"
  },
  "hrabanek25_interspeech": {
   "authors": [
    [
     "Patrik",
     "Hrabánek"
    ],
    [
     "Michaela",
     "Watkins"
    ],
    [
     "Silke",
     "Hamann"
    ]
   ],
   "title": "The function of creaky voice in South Korean: A perception study",
   "original": "2179",
   "order": 473,
   "page_count": 5,
   "abstract": [
    "This study examines whether creaky voice serves as a perceptual cue for the fortis-lenis word-initial stop distinction in Standard Seoul Korean. Twenty-nine native speakers completed an online forced-choice ABX task, where they determined whether a manipulated, artificially creaky lenis token (Sound X) with ambiguous values of VOT and F0 resembled more closely a natural fortis (Sound A) or a natural lenis token (Sound B). Results showed considerable inter-speaker variation: some rarely categorized creaky lenis tokens as fortis, while others did so up to 15% of the time. Statistical analysis showed that presence of creak in Sound X did not change the perceptual classification significantly, suggesting creak is not a primary cue alongside VOT and F0. Word-specific effects also emerged, highlighting the complexity of perceptual cues in Korean stop contrasts."
   ],
   "p1": 2310,
   "pn": 2314,
   "doi": "10.21437/Interspeech.2025-2179",
   "url": "interspeech_2025/hrabanek25_interspeech.html"
  },
  "ersoy25_interspeech": {
   "authors": [
    [
     "Asim",
     "Ersoy"
    ],
    [
     "Basel Ahmad",
     "Mousi"
    ],
    [
     "Shammur Absar",
     "Chowdhury"
    ],
    [
     "Firoj",
     "Alam"
    ],
    [
     "Fahim I",
     "Dalvi"
    ],
    [
     "Nadir",
     "Durrani"
    ]
   ],
   "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models",
   "original": "2180",
   "order": 50,
   "page_count": 5,
   "abstract": [
    "The emergence of large language models has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts - showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. To support reproducibility, we have released our code along with a curated audio version of the SST-2 dataset for public access."
   ],
   "p1": 241,
   "pn": 245,
   "doi": "10.21437/Interspeech.2025-2180",
   "url": "interspeech_2025/ersoy25_interspeech.html"
  },
  "eren25_interspeech": {
   "authors": [
    [
     "Eray",
     "Eren"
    ],
    [
     "Qingju",
     "Liu"
    ],
    [
     "Hyeongwoo",
     "Kim"
    ],
    [
     "Pablo",
     "Garrido"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs",
   "original": "2189",
   "order": 92,
   "page_count": 5,
   "abstract": [
    "Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding. The decoder predicts acoustics in the masked region using both the encoded prosody input and unmasked textual content. Trained on the GigaSpeech dataset, we compare our method with state-of-the-art style encoders. For F0 and energy predictions, we show consistent improvements for our model at different levels of granularity. We also integrate these predicted prosodic features into a TTS system and conduct perceptual tests, which show higher prosody preference compared to the baselines, demonstrating the model’s potential in tasks where prosody modeling is important."
   ],
   "p1": 429,
   "pn": 433,
   "doi": "10.21437/Interspeech.2025-2189",
   "url": "interspeech_2025/eren25_interspeech.html"
  },
  "kuhlmann25_interspeech": {
   "authors": [
    [
     "Michael",
     "Kuhlmann"
    ],
    [
     "Fritz",
     "Seebauer"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Towards Frame-level Quality Predictions of Synthetic Speech ",
   "original": "2190",
   "order": 471,
   "page_count": 5,
   "abstract": [
    "While automatic subjective speech quality assessment has witnessed much progress, an open question is whether an automatic quality assessment at frame resolution is possible. This would be highly desirable, as it adds explainability to the assessment of speech synthesis systems. Here, we take first steps towards this goal by identifying issues of existing quality predictors that prevent sensible frame-level prediction. Further, we define criteria that a frame-level predictor should fulfill. We also suggest a chunk-based processing that avoids the impact of a localized distortion on the score of neighboring frames. Finally, we measure in experiments with localized artificial distortions the localization performance of a set of frame-level quality predictors and show that they can outperform detection performance of human annotations obtained from a crowd-sourced perception experiment."
   ],
   "p1": 2300,
   "pn": 2304,
   "doi": "10.21437/Interspeech.2025-2190",
   "url": "interspeech_2025/kuhlmann25_interspeech.html"
  },
  "phukan25b_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Pailla Balakrishna",
     "Reddy"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition",
   "original": "2191",
   "order": 28,
   "page_count": 5,
   "abstract": [
    "Compression-based representations (CBRs) from neural audio codecs such as EnCodec capture intricate acoustic features like pitch and timbre, while representation-learning-based representations (RLRs) from pre-trained models trained for speech representation learning such as WavLM encode high-level semantic and prosodic information. Previous research on Speech Emotion Recognition (SER) has explored both, however, fusion of CBRs and RLRs haven’t been explored yet. In this study, we solve this gap and investigate the fusion of RLRs and CBRs and hypothesize they will be more effective by providing complementary information. To this end, we propose, HYFuse, a novel framework that fuses the representations by transforming them to hyperbolic space. With HYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the top performance in comparison to individual representations as well as the homogeneous fusion of RLRs and CBRs and report SOTA."
   ],
   "p1": 131,
   "pn": 135,
   "doi": "10.21437/Interspeech.2025-2191",
   "url": "interspeech_2025/phukan25b_interspeech.html"
  },
  "mcgahay25_interspeech": {
   "authors": [
    [
     "John",
     "McGahay"
    ]
   ],
   "title": "Modeling Vowel System Typology Using Iterated Confusion Minimization",
   "original": "2192",
   "order": 602,
   "page_count": 5,
   "abstract": [
    "This work presents a unified theory of vowel system typology based on a novel algorithm called Iterated Confusion Minimization, which simulates the interaction of optimal listeners and speakers over time. Compatible with existing sound change models that involve confusion-reducing adjustments to phonetic distributions, simulations over random initial vowel systems resemble observed sound change (e.g., chain shifts) and most often result in typologically dominant systems with minimal confusion, suggesting a diachronic explanation for the typological dominance of perceptually optimal vowel systems. In contrast to past vowel dispersion models, Iterated Confusion Minimization easily accounts for interior vowels like /ə/, allowing for prediction of the most common systems up to 11 vowels. Finally, variation in simulation output mirrors typological variation, with a tight correlation between system emergence rate and typological frequency up to 6 vowels that outperforms past baselines."
   ],
   "p1": 2955,
   "pn": 2959,
   "doi": "10.21437/Interspeech.2025-2192",
   "url": "interspeech_2025/mcgahay25_interspeech.html"
  },
  "ho25b_interspeech": {
   "authors": [
    [
     "Phuoc Hoang",
     "Ho"
    ],
    [
     "Dragoș Alexandru",
     "Bălan"
    ],
    [
     "Dirk K. J.",
     "Heylen"
    ],
    [
     "Khiet P.",
     "Truong"
    ]
   ],
   "title": "Enhancing Transcripts of Open-Source Automatic Speech Recognition Models Through Fine-Tuning with Laughter and Speech-Laugh",
   "original": "2193",
   "order": 919,
   "page_count": 5,
   "abstract": [
    "Non-lexical sounds such as laughter are considered important discourse markers in conversation as these sounds complement the lexical information in shaping interpersonal relations, managing the conversation, and in expressing attitudes and affect. However, general purpose open-source automatic speech recognition (ASR) systems are typically not focusing on these sounds. Laughter transcription is an understudied task in ASR: laughter is often not modelled as a token and is discarded in the evaluation of ASR. In our study, we investigate how current open-source ASR models are handling laughter and speech-laugh (speech interspersed with laughter). Using Switchboard and BuckEye as conversational speech corpora, we fine-tuned and evaluated Whisper and Wav2Vec2. Our results show that laughter can be integrated in ASR transcriptions without substantially degrading word error rate."
   ],
   "p1": 4513,
   "pn": 4517,
   "doi": "10.21437/Interspeech.2025-2193",
   "url": "interspeech_2025/ho25b_interspeech.html"
  },
  "papadimitriou25_interspeech": {
   "authors": [
    [
     "Katerina",
     "Papadimitriou"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ]
   ],
   "title": "A Multi-Stream Framework Utilizing 3D Human Reconstruction for Cued Speech Recognition",
   "original": "2195",
   "order": 925,
   "page_count": 5,
   "abstract": [
    "In this paper, we propose a novel multi-stream framework for automatic cued speech recognition (ACSR) that directly processes the upper-body region, addressing hand-lip asynchrony without requiring explicit segmentation or synchronization. Our model integrates two distinct modalities: (i) an appearance-based stream leveraging the ResNet18 for feature extraction and (ii) a skeletal stream based on a modulated graph convolutional network (GCN). For graph construction, we incorporate, for the first time in ACSR, 3D pose parameters inferred from the PIXIE model. Both modalities are coupled with temporal convolution for short-range dynamics learning and a BiGRU encoder for long-term sequence modeling. In addition, we introduce an alignment module that combines CTC with two auxiliary losses, improving each modality performance and enabling effective late fusion during inference. Our model achieves state-of-the-art performance across three benchmark datasets, demonstrating its effectiveness."
   ],
   "p1": 4543,
   "pn": 4547,
   "doi": "10.21437/Interspeech.2025-2195",
   "url": "interspeech_2025/papadimitriou25_interspeech.html"
  },
  "balasubramanian25_interspeech": {
   "authors": [
    [
     "Sivakumar",
     "Balasubramanian"
    ],
    [
     "Jose Antonio",
     "Jimenez Amador"
    ],
    [
     "Kaustubh",
     "Kalgaonkar"
    ],
    [
     "King Wei",
     "Hor"
    ],
    [
     "Sriram",
     "Srinivasan"
    ]
   ],
   "title": "SMARTMOS: Modeling Subjective Audio Quality Evaluation for Real-Time Applications",
   "original": "2198",
   "order": 644,
   "page_count": 5,
   "abstract": [
    "Evaluating audio quality is a crucial task, with subjective listening tests being the gold standard. However, these tests are time-consuming intrusive and expensive, making them impractical for real-time applications like telecommunications. Despite efforts to develop automatic methods that match human listener fidelity, current approaches have limitations that hinder their use in real-world scenarios. In this paper, we introduce SMARTMOS, a novel approach that addresses some of these gaps and enables accurate, real-time, non-intrusive, privacy aware audio quality assessment. We demonstrate the effectiveness of SMARTMOS through a case study on Noise Suppression (NS) and Packet Loss Concealment (PLC) modules. Furthermore, we show how semi-supervised learning techniques can be leveraged to build a joint MOS model that seamlessly covers both PLC and NS scenarios."
   ],
   "p1": 3165,
   "pn": 3169,
   "doi": "10.21437/Interspeech.2025-2198",
   "url": "interspeech_2025/balasubramanian25_interspeech.html"
  },
  "gaudrain25_interspeech": {
   "authors": [
    [
     "Etienne",
     "Gaudrain"
    ],
    [
     "Sarah",
     "Verhulst"
    ],
    [
     "Deniz",
     "Başkent"
    ]
   ],
   "title": "Speech stimulus design to study the neural coding of speech and the impact of cochlear synaptopathy",
   "original": "2199",
   "order": 263,
   "page_count": 5,
   "abstract": [
    "Since auditory neural coding mechanisms have been uncovered using simple artificial stimuli, the nature of their involvement in speech perception has remained unclear. However, using analysis-resynthesis techniques, it is possible to design naturalistic speech stimuli that present the same appeal as artificial tones in terms of precise control and parametrisation. Here, speech stimuli were designed to assess the involvement of a specific coding mechanism: the coding of temporal fine structure through phase-locking. This mechanism is supposed to be impaired in cochlear synaptopathy, a form of hearing loss that remains undetected through pure tone audiometry. The design process described here seeks to accommodate all the constraints that can arise from large multi-centric studies covering different species, methodologies, and languages."
   ],
   "p1": 1283,
   "pn": 1287,
   "doi": "10.21437/Interspeech.2025-2199",
   "url": "interspeech_2025/gaudrain25_interspeech.html"
  },
  "parvathala25c_interspeech": {
   "authors": [
    [
     "Venkatesh",
     "Parvathala"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "Dynamic Layer Gating for Speech Enhancement",
   "original": "2200",
   "order": 484,
   "page_count": 5,
   "abstract": [
    "Conventional deep learning-based speech enhancement models perform inference with fixed network structure. However, due to the time-varying spectral characteristics of speech and noise, the noisy signal exhibits varying subband and segmental signal-to-noise ratios. Adapting the network structure to these variations may offer computational efficiency and interpretability. This work introduces two gating mechanisms, sample-level gating (SG) and time-frequency level gating (TFG), to dynamically skip layers within the two-stage conformer blocks of the conformer-based metric GAN network. Our experiments demonstrate competitive performance while reducing multiply-accumulate operations by 36% with SG and 48% with TFG. Furthermore, we provide insights into the model behavior by analyzing the gate activations and intermediate layer outputs. Our analysis shows that initial layers prioritize low-frequency speech regions, while later layers focus on nonspeech regions and higher frequencies."
   ],
   "p1": 2365,
   "pn": 2369,
   "doi": "10.21437/Interspeech.2025-2200",
   "url": "interspeech_2025/parvathala25c_interspeech.html"
  },
  "batchelderschwab25_interspeech": {
   "authors": [
    [
     "Andre",
     "Batchelder-Schwab"
    ],
    [
     "Vasileios",
     "Michos"
    ],
    [
     "Jonathan",
     "Barnes"
    ]
   ],
   "title": "Stress in Spoken and Whistled Greek",
   "original": "2203",
   "order": 84,
   "page_count": 5,
   "abstract": [
    "This paper presents experimental results testing vowels of a register of whistled Greek called Sfyria. We used a frame sentence and minimal pairs. Each participant performed the experiment in whistled Greek, then again in spoken Greek. The results suggest that all five vowel qualities of Greek remain distinct in the whistled register (/i/ /e/ /o/ through F0 alone; /a/ /u/ through intensity). While acoustic differences were found between stressed and unstressed versions of all spoken vowel qualities (namely intensity), correlates of a stress distinction were not found for /i/. This might be a ceiling effect from general high intensity of front vowels in the whistled register, and we tentatively suggest that stressed /i/ in Sfyria is not produced differently from its unstressed counterpart."
   ],
   "p1": 389,
   "pn": 393,
   "doi": "10.21437/Interspeech.2025-2203",
   "url": "interspeech_2025/batchelderschwab25_interspeech.html"
  },
  "phukan25c_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Priyabrata",
     "Mallick"
    ],
    [
     "Santanu",
     "Roy"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism",
   "original": "2206",
   "order": 412,
   "page_count": 5,
   "abstract": [
    "In this study, we focus on heart murmur classification (HMC) and hypothesize that combining neural audio codec representations (NACRs) such as EnCodec with spectral features (SFs), such as MFCC, will yield superior performance. We believe such fusion will trigger their complementary behavior as NACRs excel at capturing fine-grained acoustic patterns such as rhythm changes, spectral features focus on frequency-domain properties such as harmonic structure, spectral energy distribution crucial for analyzing the complex of heart sounds. To this end, we propose, BAOMI, a novel framework banking on novel bandit-based cross-attention mechanism for effective fusion. Here, a agent provides more weightage to most important heads in multi-head cross-attention mechanism and helps in mitigating the noise. With BAOMI, we report the topmost performance in comparison to individual NACRs, SFs, and baseline fusion techniques and setting new state-of-the-art."
   ],
   "p1": 2023,
   "pn": 2027,
   "doi": "10.21437/Interspeech.2025-2206",
   "url": "interspeech_2025/phukan25c_interspeech.html"
  },
  "zhang25t_interspeech": {
   "authors": [
    [
     "Jinda",
     "Zhang"
    ],
    [
     "Aanchan",
     "Mohan"
    ]
   ],
   "title": "Towards atypical speech transcription using LLM-based ASR",
   "original": "2214",
   "order": 121,
   "page_count": 5,
   "abstract": [
    "Training a linear transformation between speech encoders and LLMs enable LLMs to transcribe speech. SLAM-ASR is one such recently proposed architecture. This paper examines its adaptability across three domains of varying difficulty: read speech (Librispeech, easiest), meeting speech (AMI, medium) and post-stroke aphasic speech (AphasiaBank, most difficult) for both word- and phoneme-level transcription. After studying cross-domain adaptability, our work explores the use of transfer learning to seed model fine-tuning for the target domain from a source domain. Results show that transferring from an easier to a harder domain offers little benefit, while the reverse seems to improve model robustness in the easier target domain. Our work also looks at the impact of a phoneme encoder at the input and multiple single-task instruction fine tuning on phoneme and word transcription tasks. This work advances the adaptation of LLM-based ASR for atypical speech transcription."
   ],
   "p1": 574,
   "pn": 578,
   "doi": "10.21437/Interspeech.2025-2214",
   "url": "interspeech_2025/zhang25t_interspeech.html"
  },
  "mousavi25_interspeech": {
   "authors": [
    [
     "Pooneh",
     "Mousavi"
    ],
    [
     "Shubham",
     "Gupta"
    ],
    [
     "Cem",
     "Subakan"
    ],
    [
     "Mirco",
     "Ravanelli"
    ]
   ],
   "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs",
   "original": "2218",
   "order": 660,
   "page_count": 5,
   "abstract": [
    "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN (Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks."
   ],
   "p1": 3244,
   "pn": 3248,
   "doi": "10.21437/Interspeech.2025-2218",
   "url": "interspeech_2025/mousavi25_interspeech.html"
  },
  "ma25c_interspeech": {
   "authors": [
    [
     "Teng",
     "Ma"
    ],
    [
     "Sile",
     "Yin"
    ],
    [
     "Li-Chia",
     "Yang"
    ],
    [
     "Shuo",
     "Zhang"
    ]
   ],
   "title": "Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations",
   "original": "2223",
   "order": 14,
   "page_count": 5,
   "abstract": [
    "Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system."
   ],
   "p1": 61,
   "pn": 65,
   "doi": "10.21437/Interspeech.2025-2223",
   "url": "interspeech_2025/ma25c_interspeech.html"
  },
  "leschly25_interspeech": {
   "authors": [
    [
     "Emma Cathrine Liisborg",
     "Leschly"
    ],
    [
     "Oliver",
     "Roesler"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Abhishek",
     "Hosamath"
    ],
    [
     "Lakshmi",
     "Arbatti"
    ],
    [
     "Line H.",
     "Clemmensen"
    ],
    [
     "Melanie",
     "Ganz"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "An Exploration of Interpretable Deep Learning Models for the Assessment of Mild Cognitive Impairment",
   "original": "2225",
   "order": 56,
   "page_count": 5,
   "abstract": [
    "Early diagnosis and intervention are crucial for mild cognitive impairment (MCI), as MCI often progresses to more severe neurodegenerative conditions. In this study, we explore utilizing deep learning for MCI detection without loosing the interpretability provided by feature-based approaches. We used a dataset consisting of 90 MCI patients and 91 controls collected via a remote assessment platform and analyzed the participants’ spontaneous speech responses to the Patient Report of Problems (PROP) which asks patients to report their most bothersome general health problems. The proposed deep neural network, which features a bottleneck layer including 13 interpretable symptom domains, achieved an AUC of 0.62, thereby outperforming a set of feature-based classifiers while ensuring interpretability due to the bottleneck layer. We further illustrated the model’s interpretability by examining how the predicted PROP domains influence final predictions using Shapley values."
   ],
   "p1": 271,
   "pn": 275,
   "doi": "10.21437/Interspeech.2025-2225",
   "url": "interspeech_2025/leschly25_interspeech.html"
  },
  "phukan25d_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Pailla Balakrishna",
     "Reddy"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction",
   "original": "2227",
   "order": 629,
   "page_count": 5,
   "abstract": [
    "In this study, we focus on Singing Voice Mean Opinion Score (SingMOS) prediction. Previous research have shown the performance benefit with the use of state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven’t explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we hypothesize that it will be the most effective for SingMOS prediction. We believe that due to their speaker recognition pretraining, it equips them to capture fine-grained vocal features (e.g., pitch, tone, intensity) from synthesized singing voices in a much more better way than other PTMs. Our experiments with SOTA PTMs including SPTMs and music PTMs validates the hypothesis. Additionally, we introduce a novel fusion framework, BATCH that uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of speaker recognition SPTMs, we report the topmost performance comparison to all the individual PTMs and baseline fusion techniques as well as setting SOTA."
   ],
   "p1": 3090,
   "pn": 3094,
   "doi": "10.21437/Interspeech.2025-2227",
   "url": "interspeech_2025/phukan25d_interspeech.html"
  },
  "hope25_interspeech": {
   "authors": [
    [
     "Maxwell",
     "Hope"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "Voices of `cyborg awesomeness': Posthuman embodiment of nonbinary gender expression in AI speech technologies",
   "original": "2229",
   "order": 144,
   "page_count": 5,
   "abstract": [
    "Speech-generating devices (SGDs) provide users with text-to-speech (TTS) voices that shape identity and self-expression. Current TTS voices enable self-expression but often lack customizable features for authentic voice embodiment, particularly for nonbinary SGD users seeking gender affirmation as existing TTS voices largely reproduce binary, cisgender speech patterns. This study examines how nonbinary SGD users embody, or disembody, synthetic voices and the factors influencing voice affirmation. Through a survey, we analyze experiences of nonbinary SGD users and their impressions of generated speech samples, investigating the role of technological possibilities in gender affirmation and voice embodiment. Findings inform the creation of more user-centered TTS technologies, and challenge dominant paradigms in speech technology, gesturing toward a posthumanist rethinking of voice as co-constructed between human and machine."
   ],
   "p1": 689,
   "pn": 693,
   "doi": "10.21437/Interspeech.2025-2229",
   "url": "interspeech_2025/hope25_interspeech.html"
  },
  "phukan25e_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Jaya Sai Kiran",
     "Patibandla"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "PARROT: Synergizing Mamba and Attention-based SSL Pre-Trained Models via Parallel Branch Hadamard Optimal Transport for Speech Emotion Recognition",
   "original": "2233",
   "order": 910,
   "page_count": 5,
   "abstract": [
    "The emergence of Mamba as an alternative to attention-based architectures has led to the development of Mamba-based self-supervised learning (SSL) pre-trained models (PTMs) for speech and audio processing. Recent studies suggest that these models achieve comparable or superior performance to state-of-the-art (SOTA) attention-based PTMs for speech emotion recognition (SER). Motivated by prior work demonstrating the benefits of PTM fusion across different speech processing tasks, we hypothesize that leveraging the complementary strengths of Mamba-based and attention-based PTMs will enhance SER performance beyond the fusion of homogenous attention-based PTMs. To this end, we introduce a novel framework, PARROT that integrates parallel branch fusion with Optimal Transport and Hadamard Product. Our approach achieves SOTA results against individual PTMs, homogeneous PTMs fusion, and baseline fusion techniques, thus, highlighting the potential of heterogeneous PTM fusion for SER."
   ],
   "p1": 4468,
   "pn": 4472,
   "doi": "10.21437/Interspeech.2025-2233",
   "url": "interspeech_2025/phukan25e_interspeech.html"
  },
  "medennikov25_interspeech": {
   "authors": [
    [
     "Ivan",
     "Medennikov"
    ],
    [
     "Taejin",
     "Park"
    ],
    [
     "Weiqing",
     "Wang"
    ],
    [
     "He",
     "Huang"
    ],
    [
     "Kunal",
     "Dhawan"
    ],
    [
     "Jinhan",
     "Wang"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering",
   "original": "2244",
   "order": 1068,
   "page_count": 5,
   "abstract": [
    "This paper presents a streaming extension for the Sortformer speaker diarization framework, whose key property is the arrival-time ordering of output speakers. The proposed approach employs an Arrival-Order Speaker Cache (AOSC) to store frame-level acoustic embeddings of previously observed speakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings by speaker index corresponding to their arrival time order, and is dynamically updated by selecting frames with the highest scores based on the model&#x27;s past predictions. Notably, the number of stored embeddings per speaker is determined dynamically by the update mechanism, ensuring efficient cache utilization and precise speaker tracking. Experiments on benchmark datasets confirm the effectiveness and flexibility of our approach, even in low-latency setups. These results establish Streaming Sortformer as a robust solution for real-time multi-speaker tracking and a foundation for streaming multi-talker speech processing."
   ],
   "p1": 5238,
   "pn": 5242,
   "doi": "10.21437/Interspeech.2025-2244",
   "url": "interspeech_2025/medennikov25_interspeech.html"
  },
  "verdini25_interspeech": {
   "authors": [
    [
     "Francesco",
     "Verdini"
    ],
    [
     "Pierfrancesco",
     "Melucci"
    ],
    [
     "Stefano",
     "Perna"
    ],
    [
     "Francesco",
     "Cariaggi"
    ],
    [
     "Marco",
     "Gaido"
    ],
    [
     "Sara",
     "Papi"
    ],
    [
     "Szymon",
     "Mazurek"
    ],
    [
     "Marek",
     "Kasztelnik"
    ],
    [
     "Luisa",
     "Bentivogli"
    ],
    [
     "Sebastien",
     "Bratières"
    ],
    [
     "Paolo",
     "Merialdo"
    ],
    [
     "Simone",
     "Scardapane"
    ]
   ],
   "title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not",
   "original": "2245",
   "order": 370,
   "page_count": 5,
   "abstract": [
    "The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM."
   ],
   "p1": 1813,
   "pn": 1817,
   "doi": "10.21437/Interspeech.2025-2245",
   "url": "interspeech_2025/verdini25_interspeech.html"
  },
  "stein25_interspeech": {
   "authors": [
    [
     "Anna",
     "Stein"
    ],
    [
     "Kevin",
     "Tang"
    ]
   ],
   "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning",
   "original": "2246",
   "order": 72,
   "page_count": 5,
   "abstract": [
    "This study compares probabilistic predictors based on information theory with Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, focusing on probabilistic reduction. We examine three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors. Results show that the N-gram model outperforms both NDL models, challenging the assumption that NDL is more effective due to its cognitive motivation. However, incorporating information-theoretic formulas into NDL improves model performance over the traditional model. This research highlights a) the need to incorporate not only frequency and contextual predictability but also average contextual predictability, and b) the importance of combining information-theoretic metrics of predictability and information derived from discriminative learning in modeling acoustic reduction."
   ],
   "p1": 330,
   "pn": 334,
   "doi": "10.21437/Interspeech.2025-2246",
   "url": "interspeech_2025/stein25_interspeech.html"
  },
  "yan25c_interspeech": {
   "authors": [
    [
     "Brian",
     "Yan"
    ],
    [
     "Injy",
     "Hamed"
    ],
    [
     "Shuichiro",
     "Shimizu"
    ],
    [
     "Vasista Sai",
     "Lodagala"
    ],
    [
     "William",
     "Chen"
    ],
    [
     "Olga",
     "Iakovenko"
    ],
    [
     "Bashar",
     "Talafha"
    ],
    [
     "Amir",
     "Hussein"
    ],
    [
     "Alexander",
     "Polok"
    ],
    [
     "Kalvin",
     "Chang"
    ],
    [
     "Dominik",
     "Klement"
    ],
    [
     "Sara",
     "Althubaiti"
    ],
    [
     "Puyuan",
     "Peng"
    ],
    [
     "Matthew",
     "Wiesner"
    ],
    [
     "Thamar",
     "Solorio"
    ],
    [
     "Ahmed",
     "Ali"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset",
   "original": "2247",
   "order": 155,
   "page_count": 5,
   "abstract": [
    "We present CS-FLEURS, a new dataset for developing and evaluating code-switched speech recognition and translation systems beyond high-resourced languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique code-switched language pairs across 52 languages: 1) a 14 X-English language pair set with real voices reading synthetically generated code-switched sentences, 2) a 16 X-English language pair set with generative text-to-speech 3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the generative text-to-speech, and 4) a 45 X-English lower-resourced language pair test set with concatenative text-to-speech. Besides the three test sets, CS-FLEURS also provides a training set with 128 hours of generative text-to-speech data across 16 X-English language pairs. Our hope is that CS-FLEURS helps to broaden the scope of future code-switched speech research."
   ],
   "p1": 743,
   "pn": 747,
   "doi": "10.21437/Interspeech.2025-2247",
   "url": "interspeech_2025/yan25c_interspeech.html"
  },
  "phukan25f_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Abu Osama",
     "Siddiqui"
    ],
    [
     "Sarthak",
     "Jain"
    ],
    [
     "Priyabrata",
     "Mallick"
    ],
    [
     "Jaya Sai Kiran",
     "Patibandla"
    ],
    [
     "Pailla Balakrishna",
     "Reddy"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer",
   "original": "2251",
   "order": 551,
   "page_count": 5,
   "abstract": [
    "As video-sharing platforms have grown over the past decade, child viewership has surged, increasing the need for precise detection of harmful content like violence or explicit scenes. Malicious users exploit moderation systems by embedding unsafe content in minimal frames to evade detection. While prior research has focused on visual cues and advanced such fine-grained detection, audio features remain underexplored. In this study, we embed audio cues with visual for fine-grained child harmful content detection and introduce SNIFR, a novel framework for effective alignment. SNIFR employs a transformer encoder for intra-modality interaction, followed by a cascaded cross-transformer for inter-modality alignment. Our approach achieves superior performance over unimodal and baseline fusion methods, setting a new state-of-the-art."
   ],
   "p1": 2700,
   "pn": 2704,
   "doi": "10.21437/Interspeech.2025-2251",
   "url": "interspeech_2025/phukan25f_interspeech.html"
  },
  "malisz25_interspeech": {
   "authors": [
    [
     "Zofia",
     "Malisz"
    ],
    [
     "Jan",
     "Foremski"
    ],
    [
     "Magłorzata",
     "Kul"
    ]
   ],
   "title": "Contextual predictability effects on acoustic distinctiveness in read Polish speech",
   "original": "2256",
   "order": 73,
   "page_count": 5,
   "abstract": [
    "This paper examines how contextual predictability (surprisal) influences acoustic features in Polish speech using the PRODIS dataset. The study analyzes connected read speech from Wikipedia texts on history, politics, culture, and science, extracting surprisal values from a phoneme-based language model. Results show that high surprisal increases acoustic distinctiveness, such as longer segment duration and larger vowel space, while low surprisal reduces distinctiveness. We also find effects of text topic, lexical frequency, and lexical stress on surprisal. These findings highlight the complex interplay between predictability, discourse, and prosody in speech. The work contributes to understudied analyses on predictability and acoustics in Slavic languages."
   ],
   "p1": 335,
   "pn": 339,
   "doi": "10.21437/Interspeech.2025-2256",
   "url": "interspeech_2025/malisz25_interspeech.html"
  },
  "chan25_interspeech": {
   "authors": [
    [
     "Le Xuan",
     "Chan"
    ],
    [
     "Annika",
     "Heuser"
    ]
   ],
   "title": "Relative cue weighting in multilingual stop voicing production",
   "original": "2259",
   "order": 25,
   "page_count": 5,
   "abstract": [
    "How does a multilingual speaker produce similar phonological contrasts across the different languages that they speak? Some theories predict crosslinguistic influence while others predict that multilinguals keep separate sound inventories for each language. In this paper, we present crosslinguistic data from early multilingual speakers in Malaysia. We investigate the interaction of a true voicing language (Malay), a variable voicing language (English), and an aspiration language (Mandarin). Using a random forest classification of nine acoustic correlates of stop voicing, we show that 1) all early multilinguals show language-specific productions of stop voicing, and 2) variation driven by dominance can still be observed despite this language-specificity. In addition, we present evidence that closure voicing is a salient correlate alongside aspiration in Malaysian English, and that English is more reliant on secondary correlates than Malay and Mandarin."
   ],
   "p1": 116,
   "pn": 120,
   "doi": "10.21437/Interspeech.2025-2259",
   "url": "interspeech_2025/chan25_interspeech.html"
  },
  "ozyilmaz25_interspeech": {
   "authors": [
    [
     "Ömer Tarik",
     "Özyilmaz"
    ],
    [
     "Matt",
     "Coler"
    ],
    [
     "Matias",
     "Valdenegro-Toro"
    ]
   ],
   "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning",
   "original": "2260",
   "order": 238,
   "page_count": 5,
   "abstract": [
    "Although commercial Arabic automatic speech recognition (ASR) systems support Modern Standard Arabic (MSA), they struggle with dialectal speech. We investigate the effect of fine-tuning OpenAI&#x27;s Whisper on five major Arabic dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA training size effects, benefits of pre-training on MSA data, and dialect-specific versus dialect-pooled models. We find that small amounts of MSA fine-tuning data yield substantial improvements for smaller models, matching larger non-fine-tuned models. While MSA pre-training shows minimal benefit, suggesting limited shared features between MSA and dialects, our dialect-pooled models perform comparably to dialect-specific ones. This indicates that pooling dialectal data, when properly balanced, can help address data scarcity in low-resource ASR without significant performance loss."
   ],
   "p1": 1158,
   "pn": 1162,
   "doi": "10.21437/Interspeech.2025-2260",
   "url": "interspeech_2025/ozyilmaz25_interspeech.html"
  },
  "salihs25_interspeech": {
   "authors": [
    [
     "Sumaya Ahmed",
     "Salihs"
    ],
    [
     "Isaac",
     "Wiafe"
    ],
    [
     "Jamal-Deen",
     "Abdulai"
    ],
    [
     "Elikem Doe",
     "Atsakpo"
    ],
    [
     "Gifty",
     "Ayoka"
    ],
    [
     "Richard",
     "Cave"
    ],
    [
     "Akon Obu",
     "Ekpezu"
    ],
    [
     "Catherine",
     "Holloway"
    ],
    [
     "Katrin",
     "Tomanek"
    ],
    [
     "Fiifi Baffoe Payin",
     "Winful"
    ]
   ],
   "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages",
   "original": "2261",
   "order": 941,
   "page_count": 5,
   "abstract": [
    "This study presents an approach for collecting speech samples to build Automatic Speech Recognition (ASR) models for impaired speech, particularly, low-resource languages. The project aims to democratize ASR technology and data collection by developing a &quot;cookbook&quot; of best practices and training for community-driven data collection and ASR model building. As a proof-of-concept, this project created the first open-source dataset of impaired speech in Akan: a widely spoken indigenous language in Ghana. The project involved participants from diverse backgrounds with speech impairments. The resulting dataset, along with the cookbook and open-source tools, are publicly available to enable researchers and practitioners to create inclusive ASR technologies tailored to the unique needs of speech impaired individuals. In addition, this study presents the initial results of tuning open-source ASR models to better recognize impaired speech in Akan."
   ],
   "p1": 4623,
   "pn": 4627,
   "doi": "10.21437/Interspeech.2025-2261",
   "url": "interspeech_2025/salihs25_interspeech.html"
  },
  "nilsson25_interspeech": {
   "authors": [
    [
     "Mattias",
     "Nilsson"
    ],
    [
     "Riccardo",
     "Miccini"
    ],
    [
     "Julian",
     "Rossbroich"
    ],
    [
     "Clément",
     "Laroche"
    ],
    [
     "Tobias",
     "Piechowiak"
    ],
    [
     "Friedemann",
     "Zenke"
    ]
   ],
   "title": "Efficient Streaming Speech Quality Prediction with Spiking Neural Networks",
   "original": "2269",
   "order": 1106,
   "page_count": 5,
   "abstract": [
    "As speech processing systems become more ubiquitous, the need for real-time, efficient speech quality prediction (SQP) is growing. Conventional artificial neural networks (ANNs) offer strong prediction performance but can be computationally demanding, which limits their deployment on mobile and edge devices. Spiking neural networks (SNNs) present a promising alternative for ultra-low-power, streaming inference due to their sparse activity and event-driven processing. However, their potential for SQP remains largely unexplored. This article introduces deep convolutional SNNs for SQP and evaluates their performance against state-of-the-art ANN models. Our results show that SNNs achieve comparable accuracy while significantly reducing computational cost. These findings highlight the potential of SNNs to enable real-time, energy-efficient SQP in resource-constrained settings."
   ],
   "p1": 5423,
   "pn": 5427,
   "doi": "10.21437/Interspeech.2025-2269",
   "url": "interspeech_2025/nilsson25_interspeech.html"
  },
  "rommel25_interspeech": {
   "authors": [
    [
     "Meike",
     "Rommel"
    ],
    [
     "Míša",
     "Hejná"
    ],
    [
     "Nicole",
     "Dehé"
    ]
   ],
   "title": "Pre-aspiration in Iceland Is Conditioned by Gender/Sex",
   "original": "2271",
   "order": 682,
   "page_count": 5,
   "abstract": [
    "This paper focuses on pre-aspiration, defined here as a period of (primarily) glottal friction found in sequences of vocalic and consonantal sonorants and phonetically voiceless obstruents. Previous pre-aspiration studies revealed sensitivity of the phenomenon to (bio)social factors. However, there is little work that actually focuses on sociophonetic effects on pre-aspiration. One of the most common findings in the sociophonetics of pre-aspiration is that female speakers are more likely to pre-aspirate more often and with longer durations. Our study asks whether pre-aspiration is conditioned by gender/sex in Icelandic, assumed to be one of the prototypical pre-aspirating languages. We analyse 20 speakers of Modern Icelandic participating in a picture naming task. Our results show that female speakers produce longer durations of local breathiness and voiceless pre-aspiration, as well as noisier tokens."
   ],
   "p1": 3354,
   "pn": 3358,
   "doi": "10.21437/Interspeech.2025-2271",
   "url": "interspeech_2025/rommel25_interspeech.html"
  },
  "neumann25_interspeech": {
   "authors": [
    [
     "Michael",
     "Neumann"
    ],
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Beverly",
     "Insel"
    ],
    [
     "Anzalee",
     "Khan"
    ],
    [
     "Danyah",
     "Nadim"
    ],
    [
     "Jean-Pierre",
     "Lindenmayer"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Multimodal Speech, Language and Orofacial Analysis for Remote Assessment of Positive, Negative and Cognitive Symptoms in Schizophrenia",
   "original": "2272",
   "order": 1162,
   "page_count": 5,
   "abstract": [
    "This study provides a comprehensive analysis of digital speech, orofacial and linguistic features for the assessment of schizophrenia. We recorded audio and video from 94 people with schizophrenia (pSCZ) and 100 healthy controls (HC) and extracted features automatically. Clinical rating scales were administered to assess positive, negative, and cognitive symptoms. We show that pSCZ exhibit significant alterations in speech timing, orofacial dynamics, and lexical richness, as compared to HC. A multimodal classification approach achieved high accuracy (96% AUC, 87% UAR), with speech features contributing most to discrimination. Correlation analysis revealed that speech timing and lip velocity measures are correlated with blunted affect and alogia. Linguistic features correlate well with positive symptoms, particularly conceptual disorganization and excitement. Cognitive abilities are most strongly associated with speech timing and specific linguistic features."
   ],
   "p1": 5703,
   "pn": 5707,
   "doi": "10.21437/Interspeech.2025-2272",
   "url": "interspeech_2025/neumann25_interspeech.html"
  },
  "braun25_interspeech": {
   "authors": [
    [
     "Franziska",
     "Braun"
    ],
    [
     "Christopher",
     "Witzl"
    ],
    [
     "Andreas",
     "Erzigkeit"
    ],
    [
     "Hartmut",
     "Lehfeld"
    ],
    [
     "Thomas",
     "Hillemacher"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Pitfalls and Limits in Automatic Dementia Assessment",
   "original": "2280",
   "order": 1154,
   "page_count": 5,
   "abstract": [
    "Current work on speech-based dementia assessment focuses on either feature extraction to predict assessment scales, or on the automation of existing test procedures. Most research uses public data unquestioningly and rarely performs a detailed error analysis, focusing primarily on numerical performance. We perform an in-depth analysis of an automated standardized dementia assessment, the Syndrom-Kurz-Test. We find that while there is a high overall correlation with human annotators, due to certain artifacts, we observe high correlations for the severely impaired individuals, which is less true for the healthy or mildly impaired ones. Speech production decreases with cognitive decline, leading to overoptimistic correlations when test scoring relies on word naming. Depending on the test design, fallback handling introduces further biases that favor certain groups. These pitfalls remain independent of group distributions in datasets and require differentiated analysis of target groups."
   ],
   "p1": 5663,
   "pn": 5667,
   "doi": "10.21437/Interspeech.2025-2280",
   "url": "interspeech_2025/braun25_interspeech.html"
  },
  "zhong25c_interspeech": {
   "authors": [
    [
     "Jinzuomu",
     "Zhong"
    ],
    [
     "Suyuan",
     "Liu"
    ],
    [
     "Dan",
     "Wells"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis",
   "original": "2283",
   "order": 469,
   "page_count": 5,
   "abstract": [
    "Despite growing interest in generating high-fidelity accents, evaluating accent similarity in speech synthesis has been underexplored. We aim to enhance both subjective and objective evaluation methods for accent similarity. Subjectively, we refine the XAB listening test by adding components that achieve higher statistical significance with fewer listeners and lower costs. Our method involves providing listeners with transcriptions, having them highlight perceived accent differences, and implementing meticulous screening for reliability. Objectively, we utilise pronunciation-related metrics, based on distances between vowel formants and phonetic posteriorgrams, to evaluate accent generation. Comparative experiments reveal that these metrics, alongside accent similarity, speaker similarity, and Mel Cepstral Distortion, can be used. Moreover, our findings underscore significant limitations of common metrics like Word Error Rate in assessing underrepresented accents."
   ],
   "p1": 2290,
   "pn": 2294,
   "doi": "10.21437/Interspeech.2025-2283",
   "url": "interspeech_2025/zhong25c_interspeech.html"
  },
  "karimov25_interspeech": {
   "authors": [
    [
     "Elvir",
     "Karimov"
    ],
    [
     "Alexander",
     "Varlamov"
    ],
    [
     "Danil",
     "Ivanov"
    ],
    [
     "Dmitrii",
     "Korzh"
    ],
    [
     "Oleg",
     "Rogov"
    ]
   ],
   "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
   "original": "2290",
   "order": 309,
   "page_count": 5,
   "abstract": [
    "Deep learning voice models are commonly used nowadays, but the safety of processing of personal data, such as human identity and speech content, remains suspicious. To prevent malicious user identification, speaker obfuscation methods were proposed. Current methods, particularly based on universal adversarial patch (UAP) applications, have drawbacks such as significant degradation of audio quality, decreased speech recognition quality, low transferability across different voice biometrics models, and performance dependence on the input audio length. To mitigate these drawbacks, in this work, we introduce and leverage the novel Exponential Total Variance (TV) loss function and provide experimental evidence that it positively affects UAP strength and imperceptibility. Moreover, we present a novel scalable UAP insertion procedure and demonstrate its uniformly high performance for various audio lengths."
   ],
   "p1": 1513,
   "pn": 1517,
   "doi": "10.21437/Interspeech.2025-2290",
   "url": "interspeech_2025/karimov25_interspeech.html"
  },
  "hameed25_interspeech": {
   "authors": [
    [
     "Razhan",
     "Hameed"
    ],
    [
     "Sina",
     "Ahmadi"
    ],
    [
     "Hanah",
     "Hadi"
    ],
    [
     "Rico",
     "Sennrich"
    ]
   ],
   "title": "Automatic Speech Recognition for Low-Resourced Middle Eastern Languages",
   "original": "2296",
   "order": 153,
   "page_count": 5,
   "abstract": [
    "Despite significant advancements in language and speech technologies, many languages in the Middle East remain under-served, leading to a technological disparity that negatively impacts these languages. This paper presents a pioneering effort to address this issue by focusing on speech technologies for low-resourced languages in the Middle East. We introduce a community-driven volunteer-based initiative to collect audio recordings for six languages spoken by an estimated population of 30 million speakers. Through this initiative, we collect over 40 hours of speech data, with 75% of utterances based on multilingual parallel corpora. In our experiments, we demonstrate the impact of data collection and fine-tuning models on the performance of speech technologies for these languages. This research serves as a crucial step towards preserving and promoting linguistic diversity in the Middle East while ensuring equal access to speech technologies for all language communities."
   ],
   "p1": 733,
   "pn": 737,
   "doi": "10.21437/Interspeech.2025-2296",
   "url": "interspeech_2025/hameed25_interspeech.html"
  },
  "febrinanto25_interspeech": {
   "authors": [
    [
     "Falih Gozi",
     "Febrinanto"
    ],
    [
     "Kristen",
     "Moore"
    ],
    [
     "Chandra",
     "Thapa"
    ],
    [
     "Jiangang",
     "Ma"
    ],
    [
     "Vidya",
     "Saikrishna"
    ],
    [
     "Feng",
     "Xia"
    ]
   ],
   "title": "Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection",
   "original": "2298",
   "order": 1093,
   "page_count": 5,
   "abstract": [
    "The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don&#x27;t effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953% across five experiences. The code is available."
   ],
   "p1": 5358,
   "pn": 5362,
   "doi": "10.21437/Interspeech.2025-2298",
   "url": "interspeech_2025/febrinanto25_interspeech.html"
  },
  "bafna25_interspeech": {
   "authors": [
    [
     "Niyati",
     "Bafna"
    ],
    [
     "Matthew",
     "Wiesner"
    ]
   ],
   "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech",
   "original": "2300",
   "order": 304,
   "page_count": 5,
   "abstract": [
    "Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speaker&#x27;s native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID."
   ],
   "p1": 1488,
   "pn": 1492,
   "doi": "10.21437/Interspeech.2025-2300",
   "url": "interspeech_2025/bafna25_interspeech.html"
  },
  "rachman25_interspeech": {
   "authors": [
    [
     "Laura",
     "Rachman"
    ],
    [
     "Deniz",
     "Başkent"
    ]
   ],
   "title": "Characterization of voice cue sensitivity and vocal emotion recognition across the adult lifespan",
   "original": "2303",
   "order": 266,
   "page_count": 5,
   "abstract": [
    "Studies show that vocal emotion recognition declines with age, but whether this is due to reduced sensitivity to acoustic cues (e.g., pitch-based cues) or cognitive changes is unclear. We assessed sensitivity to two important voice cues, fundamental frequency (F0) and vocal-tract length (VTL), which may also be relevant for emotion perception in speech. Using an adaptive 3I-3AFC paradigm, we measured just-noticeable differences to evaluate sensitivity to F0 and VTL. In a second task, we assessed vocal emotion recognition for happy, angry, and sad utterances in non-language-specific pseudo-speech. Participants ranged in age from 24 to 70 years, allowing us to examine how both lower-level sensory processing and higher-level emotion processing are affected by age, and the predictive value of F0 and VTL sensitivity in emotion recognition tasks."
   ],
   "p1": 1298,
   "pn": 1302,
   "doi": "10.21437/Interspeech.2025-2303",
   "url": "interspeech_2025/rachman25_interspeech.html"
  },
  "kothare25_interspeech": {
   "authors": [
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Multimodal Speech-Based Biomarkers Outperform the ALS Functional Rating Scale in Predicting Individual Disease Progression in ALS",
   "original": "2307",
   "order": 1084,
   "page_count": 5,
   "abstract": [
    "Disease progression in ALS is heterogeneous due to the varying presentation of clinical symptoms. This heterogeneity makes it difficult to accurately quantify longitudinal disease severity in people with ALS (pALS), making it difficult to determine the efficacy of therapeutic interventions. In this work, we explore a Bayesian Logistic Mixed-Effects model that can help predict individual trajectories in pALS. We used metrics extracted from 143 pALS who interacted with a cloud-based multimodal assessment platform comprising standard speaking exercises. We found that multimodal biomarkers can be predicted more accurately than the ALSFRS-R, the clinical gold standard to measure disease state, with dense and sparse training data. Such non-linear models have the potential to help with stratification of pALS into fast and slow progressors and thus inform treatment approaches. Patient stratification is also a key factor in designing clinical trials to test drug efficacy in slowing progression."
   ],
   "p1": 5313,
   "pn": 5317,
   "doi": "10.21437/Interspeech.2025-2307",
   "url": "interspeech_2025/kothare25_interspeech.html"
  },
  "letellier25_interspeech": {
   "authors": [
    [
     "Quentin",
     "Le Tellier"
    ],
    [
     "Marc",
     "Evrard"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Jean-Sylvain",
     "Liénard"
    ]
   ],
   "title": "Robust Vocal Intensity Prediction: Overcoming Dataset Bias with Pretrained Deep Models",
   "original": "2311",
   "order": 352,
   "page_count": 5,
   "abstract": [
    "Vocal intensity prediction has been investigated in previous studies, where machine-learning (ML) models (e.g., linear regression, SVM) trained on basic speech features (such as spectrograms and Mel-spectrograms) have demonstrated good prediction accuracy at the utterance level. In this work, we revisit these methods by evaluating them on two calibrated datasets, including a bilingual corpus containing both French and English speech. Our findings show that prior approaches struggle to generalize across datasets. To address this limitation, we leverage embeddings from the Wav2Vec 2.0 model as input features to classical ML regressors. While previous work used such embeddings for vocal intensity classification, our study demonstrates their effectiveness in addressing cross-dataset generalization issues for the regression task. The results confirm that pretrained embeddings significantly improve generalization and mitigate overfitting issues linked to traditional acoustic features."
   ],
   "p1": 1728,
   "pn": 1732,
   "doi": "10.21437/Interspeech.2025-2311",
   "url": "interspeech_2025/letellier25_interspeech.html"
  },
  "sanguedolce25_interspeech": {
   "authors": [
    [
     "Giulia",
     "Sanguedolce"
    ],
    [
     "Jón",
     "Guðnason"
    ],
    [
     "Dragos C.",
     "Gruia"
    ],
    [
     "Emilie",
     "d'Olne"
    ],
    [
     "Fatemeh",
     "Geranmayeh"
    ],
    [
     "Patrick A.",
     "Naylor"
    ]
   ],
   "title": "Physiologically-Informed Feature Analysis of Acquired  Speech Disorders for Stroke Assessment",
   "original": "2313",
   "order": 169,
   "page_count": 5,
   "abstract": [
    "Post-stroke speech disorders impair communication and rehabilitation outcomes, often requiring prolonged, intensive therapy sessions. The diversity of symptoms, coupled with the high cost and logistical burden of traditional speech therapy, underscores the need for accurate, automatic assessment to support tailored interventions. Leveraging a purpose-built database of stroke patients, this study introduces a feature-driven framework integrating traditional acoustic features with physiologically informed glottal parameters for classifying impaired speech after stroke. Evaluating unimodal, combined, and SHAP-derived feature configurations, our approach achieved a 97% F1-score in distinguishing pathological from healthy speech. These results highlight the potential of combining clinically meaningful glottal and acoustic information to support early speech deterioration detection, enhancing accessibility and personalised rehabilitation strategies for improved patient outcomes."
   ],
   "p1": 813,
   "pn": 817,
   "doi": "10.21437/Interspeech.2025-2313",
   "url": "interspeech_2025/sanguedolce25_interspeech.html"
  },
  "kibria25_interspeech": {
   "authors": [
    [
     "Imran E",
     "Kibria"
    ],
    [
     "Donald S.",
     "Williamson"
    ]
   ],
   "title": "AttentiveMOS: A Lightweight Attention-Only Model forSpeech Quality Prediction",
   "original": "2315",
   "order": 479,
   "page_count": 5,
   "abstract": [
    "Research in modeling subjective metrics for quality assessment has led to the development of no-reference speech models that directly operate on utterance waveforms to predict the Mean Opinion Score (MOS). These models often rely on convolutional layers for local feature extraction and embeddings from impractically large pretrained networks to enhance generalization. We propose an attention-only model based on Swin transformer and standard transformer layers to extract local context features and global utterance features, respectively. The self-attention operator excels at processing sequences, and our lightweight design enhances generalization on limited MOS datasets while improving real-world applicability. We train our network using a sequential self-teaching strategy to improve generalization on MOS labels affected by noise in listener ratings. Experiments on three datasets confirm the effectiveness of our design and demonstrate improvement over baseline models."
   ],
   "p1": 2340,
   "pn": 2344,
   "doi": "10.21437/Interspeech.2025-2315",
   "url": "interspeech_2025/kibria25_interspeech.html"
  },
  "tomashenko25_interspeech": {
   "authors": [
    [
     "Natalia",
     "Tomashenko"
    ],
    [
     "Emmanuel",
     "Vincent"
    ],
    [
     "Marc",
     "Tommasi"
    ]
   ],
   "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems",
   "original": "2317",
   "order": 1046,
   "page_count": 5,
   "abstract": [
    "The temporal dynamics of speech, encompassing variations in rhythm, intonation, and speaking rate, contain important and unique information about speaker identity. This paper proposes a new method for representing speaker characteristics by extracting context-dependent duration embeddings from speech temporal dynamics. We develop novel attack models using these representations and analyze the potential vulnerabilities in speaker verification and voice anonymization systems. The experimental results show that the developed attack models provide a significant improvement in speaker verification performance for both original and anonymized data in comparison with more simple representations of speech temporal dynamics reported in the literature."
   ],
   "p1": 5128,
   "pn": 5132,
   "doi": "10.21437/Interspeech.2025-2317",
   "url": "interspeech_2025/tomashenko25_interspeech.html"
  },
  "lu25h_interspeech": {
   "authors": [
    [
     "Yijing",
     "Lu"
    ],
    [
     "Khalil",
     "Iskarous"
    ],
    [
     "Louis",
     "Goldstein"
    ]
   ],
   "title": "Towards a dynamical model of transitions between fluent and stuttered speech",
   "original": "2320",
   "order": 68,
   "page_count": 5,
   "abstract": [
    "This paper introduces a dynamical systems framework for understanding stuttering, conceptualizing it as a qualitative shift in speech articulation driven by a single control parameter. Using a forced Duffing oscillator model, we demonstrate how variations in the excitation frequency can account for transitions between fluent and stuttered speech states. The model generates specific predictions about articulatory behaviors during stuttering, which we test using real-time MRI data of stuttered speech. Analysis of articulatory movements provides empirical support for the model’s predictions, suggesting that stuttering can be understood as a dynamical disease—an intact system operating outside its typical parameter range. This framework offers new insights into the nature of stuttering and potential approaches to intervention."
   ],
   "p1": 310,
   "pn": 314,
   "doi": "10.21437/Interspeech.2025-2320",
   "url": "interspeech_2025/lu25h_interspeech.html"
  },
  "jalal25_interspeech": {
   "authors": [
    [
     "Md Asif",
     "Jalal"
    ],
    [
     "Luca",
     "Remaggi"
    ],
    [
     "Vasileios",
     "Moschopoulos"
    ],
    [
     "Thanasis",
     "Kotsiopoulos"
    ],
    [
     "Vandana",
     "Rajan"
    ],
    [
     "Karthikeyan",
     "Saravanan"
    ],
    [
     "Anastasis",
     "Drosou"
    ],
    [
     "Junho",
     "Heo"
    ],
    [
     "Hyuk",
     "Oh"
    ],
    [
     "Seokyeong",
     "Jeong"
    ]
   ],
   "title": "Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling",
   "original": "2321",
   "order": 390,
   "page_count": 5,
   "abstract": [
    "Traditional speech separation and speaker diarization approaches rely on prior knowledge of target speakers or a predetermined number of participants in audio signals. To address these limitations, recent advances focus on developing enrollment-free methods capable of identifying targets without explicit speaker labelling. This work introduces a new approach to train simultaneous speech separation and diarization using automatic identification of target speaker embeddings within mixtures. Our proposed model employs a dual-stage training pipeline designed to learn robust speaker representation features that are resilient to background noise interference. Furthermore, we present an overlapping spectral loss function specifically tailored for enhancing diarization accuracy during overlapped speech frames. Experimental results show significant performance gains compared to the current SOTA baseline, achieving 71% relative reduction in DER and 69% in cpWER."
   ],
   "p1": 1913,
   "pn": 1917,
   "doi": "10.21437/Interspeech.2025-2321",
   "url": "interspeech_2025/jalal25_interspeech.html"
  },
  "tisdale25_interspeech": {
   "authors": [
    [
     "Daniel",
     "Tisdale"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "David",
     "Paulter"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Accessible Real-time Eye-gaze Tracking for Neurocognitive Health Assessment: A Multimodal Web-based Approach",
   "original": "2322",
   "order": 623,
   "page_count": 5,
   "abstract": [
    "We introduce a novel integration of real-time predictive eye-gaze tracking models into a scalable cloud-based multimodal dialogue system tailored for remote health assessments. A virtual human guide interacted with 10 participants with Mild Cognitive Impairment (MCI) and 29 healthy controls during a crowdsourced pilot study. She engaged participants in an approximately 10-minute interview with 11 interactive eye-gaze tasks, ranging from simple free gaze exploration to more specialized directed gaze tasks. We found that metrics of eye-gaze dynamics and reaction times extracted from these tasks, combined with an Adaboost classifier effectively differentiates MCI participants from healthy adults, with an average accuracy of 0.94. Furthermore, over 80% of participants reported high engagement and liked the user experience with the platform, across demographics and internet connections, demonstrating its robustness and scalability for real world use."
   ],
   "p1": 3060,
   "pn": 3064,
   "doi": "10.21437/Interspeech.2025-2322",
   "url": "interspeech_2025/tisdale25_interspeech.html"
  },
  "phukan25g_interspeech": {
   "authors": [
    [
     "Orchid Chetia",
     "Phukan"
    ],
    [
     "",
     "Girish"
    ],
    [
     "Mohd Mujtaba",
     "Akhtar"
    ],
    [
     "Shubham",
     "Singh"
    ],
    [
     "Swarup Ranjan",
     "Behera"
    ],
    [
     "Vandana",
     "Rajan"
    ],
    [
     "Muskaan",
     "Singh"
    ],
    [
     "Arun Balaji",
     "Buduru"
    ],
    [
     "Rajesh",
     "Sharma"
    ]
   ],
   "title": "Towards Machine Unlearning for Paralinguistic Speech Processing",
   "original": "2326",
   "order": 911,
   "page_count": 5,
   "abstract": [
    "In this work, we pioneer the study of Machine Unlearning (MU) for Paralinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech Emotion Recognition (SER) and Depression Detection (DD). To this end, we propose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU method, SISA by merging models trained on different shards with weight-averaging. With such modifications, we show that SISA++ preserves performance more in comparison to SISA after unlearning in benchmark SER (CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier adoption of MU for PSP, we present “cookbook recipes” - actionable recommendations for selecting optimal feature representations and downstream architectures that can mitigate performance degradation after the unlearning process."
   ],
   "p1": 4473,
   "pn": 4477,
   "doi": "10.21437/Interspeech.2025-2326",
   "url": "interspeech_2025/phukan25g_interspeech.html"
  },
  "wu25k_interspeech": {
   "authors": [
    [
     "Yihan",
     "Wu"
    ],
    [
     "Ruibo",
     "Chen"
    ],
    [
     "Georgios",
     "Milis"
    ],
    [
     "Junfeng",
     "Guo"
    ],
    [
     "Heng",
     "Huang"
    ]
   ],
   "title": "A Watermark for Auto-Regressive Speech Generation Models",
   "original": "2328",
   "order": 706,
   "page_count": 5,
   "abstract": [
    "As auto-regressive speech generation models have recently demonstrated exceptional ability in producing realistic and contextually appropriate spoken language, concerns about their potential misuse also grow. To mitigate these risks, our research introduces a pioneering statistical watermarking framework tailored for auto-regressive speech generation models. This framework integrates the statistical algorithms from existing watermark techniques to ensure that the audio outputs remain traceable and accountable without compromising audio quality. Besides, we identify the re-encoded mismatch, a significant hurdle in maintaining detection accuracy when audio outputs are re-encoded for verification. Through comprehensive experiments, we valid the detectability of our watermark and provide a detailed examination of how the re-encoded mismatch impacts watermark detection efficiency."
   ],
   "p1": 3474,
   "pn": 3478,
   "doi": "10.21437/Interspeech.2025-2328",
   "url": "interspeech_2025/wu25k_interspeech.html"
  },
  "premananth25_interspeech": {
   "authors": [
    [
     "Gowtham",
     "Premananth"
    ],
    [
     "Philip",
     "Resnik"
    ],
    [
     "Sonia",
     "Bansal"
    ],
    [
     "Deanna L.",
     "Kelly"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation",
   "original": "2332",
   "order": 624,
   "page_count": 5,
   "abstract": [
    "Studies on schizophrenia assessments using deep learning typically treat it as a classification task to detect the presence or absence of the disorder, oversimplifying the condition and reducing its clinical applicability. This traditional approach overlooks the complexity of schizophrenia, limiting its practical value in healthcare settings. This study shifts the focus to individual symptom severity estimation using a multimodal approach that integrates speech, video, and text inputs. We develop unimodal models for each modality and a multimodal framework to improve accuracy and robustness. By capturing a more detailed symptom profile, this approach can help in enhancing diagnostic precision and supports personalized treatment, offering a scalable and objective tool for mental health assessment."
   ],
   "p1": 3065,
   "pn": 3069,
   "doi": "10.21437/Interspeech.2025-2332",
   "url": "interspeech_2025/premananth25_interspeech.html"
  },
  "titeux25_interspeech": {
   "authors": [
    [
     "Hadrien",
     "Titeux"
    ],
    [
     "Quang Tuan Rémy",
     "Nguyen"
    ],
    [
     "Andres",
     "Gil-Salcedo"
    ],
    [
     "Anne-Catherine",
     "Bachoud-Levi"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ]
   ],
   "title": "A simple method for predicting Clinical Scores in Huntington’s Disease by leveraging ASR's uncertainty on spontaneous speech",
   "original": "2333",
   "order": 381,
   "page_count": 5,
   "abstract": [
    "Recent automatic speech recognition (ASR) models struggle to correctly transcribe pathological speech but implicitly capture phonetic, syntactic, and semantic properties. Unlike state-of-the-art methods that rely on speech features based on time-consuming handcrafted speech transcription, we propose a simple and fully automated approach using ASR log-probabilities to quantify intelligibility in spontaneous speech of patients with Huntington’s disease. By linking this measure to clinical scores, we explore its potential as a scalable, lightweight biomarker for disease progression. Our findings suggest that ASR-derived uncertainty offers a novel, efficient, and noninvasive alternative for clinical assessment."
   ],
   "p1": 1868,
   "pn": 1872,
   "doi": "10.21437/Interspeech.2025-2333",
   "url": "interspeech_2025/titeux25_interspeech.html"
  },
  "yang25o_interspeech": {
   "authors": [
    [
     "Haici",
     "Yang"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Ryo",
     "Aihara"
    ],
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "Sameer",
     "Khurana"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Investigating continuous autoregressive generative speech enhancement",
   "original": "2335",
   "order": 483,
   "page_count": 5,
   "abstract": [
    "Following the success of autoregressive (AR) language models in predicting discrete tokens, it has become common practice for autoregressive audio and speech models to use discrete tokens generated by a neural audio codec. However, recent work has demonstrated that replacing discrete token probability modeling in an AR model with a continuous diffusion procedure can improve both model performance and efficiency for image generation. In this paper, we explore applying such a diffusion loss to replace discrete token modeling in an AR generative speech enhancement model. We explore several important design choices, including comparing standard AR models with masked AR models, and mel spectrograms with learned latents as the continuous feature representation. Our results demonstrate the potential of continuous AR speech enhancement, particularly in cases of severe noise."
   ],
   "p1": 2360,
   "pn": 2364,
   "doi": "10.21437/Interspeech.2025-2335",
   "url": "interspeech_2025/yang25o_interspeech.html"
  },
  "arora25_interspeech": {
   "authors": [
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Hayato",
     "Futami"
    ],
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems",
   "original": "2339",
   "order": 983,
   "page_count": 5,
   "abstract": [
    "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems maintain full differentiability and effectively capture non-phonemic information, making them well-suited for spoken dialogue modeling. However, existing E2E approaches often require large-scale training data and struggle to produce semantically coherent responses. In this work, we propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)&#x27;s pre-training on speech recognition (ASR), text-to-speech synthesis (TTS), and text LM tasks. Our results demonstrate that our approach is highly compute-efficient, enabling the successful training of E2E spoken dialogue systems on publicly available human-human conversation datasets—even with as little as 300 hours of data, such as Switchboard. To support future research, we will publicly release our models and training code."
   ],
   "p1": 4833,
   "pn": 4837,
   "doi": "10.21437/Interspeech.2025-2339",
   "url": "interspeech_2025/arora25_interspeech.html"
  },
  "pu25_interspeech": {
   "authors": [
    [
     "Yu",
     "Pu"
    ],
    [
     "Xiaoqian",
     "Liu"
    ],
    [
     "Guangyu",
     "Zhang"
    ],
    [
     "Zheng",
     "Yan"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "Empowering Large Language Models for End-to-End Speech Translation Leveraging Synthetic Data",
   "original": "2341",
   "order": 7,
   "page_count": 5,
   "abstract": [
    "Speech-to-speech translation (S2ST) is a key technology for seamless cross-lingual communication. Traditional cascaded systems, which involve speech recognition, text translation, and speech synthesis, are prone to error propagation and latency. In this work, we present SLAM-TR, an end-to-end speech translation model which directly map input speech to output speech, eliminating the need for intermediate text representations. By fine-tuning from the large language model Qwen2-0.5B, SLAM-TR achieves superior performance over the cascaded baseline and state-of-the-art open-source models with minimal training time. Additionally, SLAM-TR demonstrates strong generalization, achieving an ASR-BLEU score of 8.20 on the FLEURS benchmark, outperforming both cascaded and open-source systems. In addition, addressing the challenge of limited natural speech translation data, we propose SynStard-1000, a 1,000-hour synthetic speech translation dataset."
   ],
   "p1": 26,
   "pn": 30,
   "doi": "10.21437/Interspeech.2025-2341",
   "url": "interspeech_2025/pu25_interspeech.html"
  },
  "premananth25b_interspeech": {
   "authors": [
    [
     "Gowtham",
     "Premananth"
    ],
    [
     "Vinith",
     "Kugathasan"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives",
   "original": "2342",
   "order": 305,
   "page_count": 5,
   "abstract": [
    "Advancements in AI-driven speech-based applications have transformed diverse industries ranging from healthcare to customer service. However, the increasing prevalence of non-native accented speech in global interactions poses significant challenges for speech-processing systems, which are often trained on datasets dominated by native speech. This study investigates accented English speech through articulatory and acoustic analysis, identifying simpler coordination patterns and higher average pitch than native speech. Using eigenspectra and Vocal Tract Variable-based coordination features, we establish an efficient method for quantifying accent strength without relying on resource-intensive phonetic transcriptions. Our findings provide a new avenue for research on the impacts of accents on speech intelligibility and offer insights for developing inclusive, robust speech processing systems that accommodate diverse linguistic communities."
   ],
   "p1": 1493,
   "pn": 1497,
   "doi": "10.21437/Interspeech.2025-2342",
   "url": "interspeech_2025/premananth25b_interspeech.html"
  },
  "jones25_interspeech": {
   "authors": [
    [
     "Karen",
     "Jones"
    ],
    [
     "Kevin",
     "Walker"
    ],
    [
     "Christopher",
     "Caruso"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Trang",
     "Nguyen"
    ],
    [
     "Robert",
     "Dunn"
    ],
    [
     "Stephanie",
     "Strassel"
    ]
   ],
   "title": "TELVID: A Multilingual Multi-modal Corpus for Speaker Recognition",
   "original": "2345",
   "order": 1169,
   "page_count": 5,
   "abstract": [
    "The TELVID corpus is a new multi-language, multi-modal resource for speaker recognition, comprising multiple conversational telephone speech and video recordings from each of 300 multilingual speakers. Consented subjects contributed recordings in a wide variety of recording conditions, with a minimum of 11 calls, 10 videos and one selfie image per person. Every speaker made recordings in Tunisian Arabic, North African French and/or English, along with two “freestyle” recordings that utilize the speaker’s choice of any language, dialect or mix of varieties. Recordings were audited to verify quality and speaker identity and portions of the data were selected for test data for the NIST 2024 Speaker Recognition Evaluation. We developed audio and visual baseline systems and measured baseline system performance. The TELVID corpus will be published in the Linguistic Data Consortium Catalog, making it broadly available for language-related research, education and technology development."
   ],
   "p1": 5738,
   "pn": 5742,
   "doi": "10.21437/Interspeech.2025-2345",
   "url": "interspeech_2025/jones25_interspeech.html"
  },
  "yong25_interspeech": {
   "authors": [
    [
     "Zheng Xin",
     "Yong"
    ],
    [
     "Vineel",
     "Pratap"
    ],
    [
     "Michael",
     "Auli"
    ],
    [
     "Jean",
     "Maillard"
    ]
   ],
   "title": "Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR",
   "original": "2351",
   "order": 236,
   "page_count": 5,
   "abstract": [
    "To build an automatic speech recognition (ASR) system that can serve everyone in the world, the ASR needs to be robust to a wide range of accents including unseen accents. We systematically study how three different variables in training data - the number of speakers, the audio duration per each individual speaker, and the diversity of accents - affect ASR robustness towards unseen accents in a low-resource training regime. We observe that for a fixed number of ASR training hours, it is more beneficial to increase the number of speakers (which means each speaker contributes less) than the number of hours contributed per speaker. We also observe that more speakers enables ASR performance gains from scaling number of hours. Surprisingly, we observe minimal benefits to prioritizing speakers with different accents when the number of speakers is controlled. Our work suggests that practitioners should prioritize increasing the speaker count in ASR training data composition for new languages."
   ],
   "p1": 1148,
   "pn": 1152,
   "doi": "10.21437/Interspeech.2025-2351",
   "url": "interspeech_2025/yong25_interspeech.html"
  },
  "sirigiaju25_interspeech": {
   "authors": [
    [
     "Meenakshi",
     "Sirigiaju"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "GoP2Vec: A few shot learning for pronunciation assessment with goodness of pronunciation (GoP) based representations from an i-vector framework and augmentation",
   "original": "2359",
   "order": 1033,
   "page_count": 5,
   "abstract": [
    "Automatic pronunciation assessment is a critical component in computer assisted language learning. Typically, modeling pronunciation assessment tasks need labels, which are difficult to obtain as it requires expert annotators. Thus, it is essential to build an accurate model with less annotated data. In this work, an approach is proposed that considers a few speech samples using the i-vector framework. Each sample, first, is lengthened by T factor by concatenating the augmented samples of the same speech. The augmentation is obtained using time-scale modification (TSM), pitch-scale modification (PSM) and both. Next, phoneme-level goodness-of-pronunciation scores of concatenated speech are converted to a vector (GoP2Vec) with the i-vector framework. Experiments on two datasets revealed that the proposed GoP2Vec outperforms the state-of-the-art (SOTA) unsupervised methods and is on par with the SOTA supervised methods when it is used to train a simple neural model with a few samples."
   ],
   "p1": 5063,
   "pn": 5067,
   "doi": "10.21437/Interspeech.2025-2359",
   "url": "interspeech_2025/sirigiaju25_interspeech.html"
  },
  "fathan25_interspeech": {
   "authors": [
    [
     "Abderrahim",
     "Fathan"
    ],
    [
     "Jahangir",
     "Alam"
    ],
    [
     "Xiaolin",
     "Zhu"
    ]
   ],
   "title": "An Investigative Study on Recent Sharpness- and Flatness-Based Optimizers for Enhanced Self-Supervised Speaker Verification",
   "original": "2361",
   "order": 307,
   "page_count": 5,
   "abstract": [
    "Flat minima have recently proven effective for improving generalization and robustness against label noise. While Adam and Stochastic gradient descent (SGD) remain the most popular optimizers in deep learning, other optimizers such as Sharpness-Aware Minimization (SAM), simultaneously minimizing loss value and minimizing loss sharpness and/or improving flatness, have proven to be more effective across a variety of tasks. Motivated by this, we present the first comprehensive study of general-purpose optimizers (ADOPT, AdEMAMix) and sharpness-aware variants (SAM, ASAM, GAM, GSAM) for the supervised and self-supervised speaker verification tasks. We also explore different regularization methods such as weight decay, EMA, and SWITCH EMA to enhance robustness and learn flat optima without extra optimization costs. Our experimental results highlight the significant impact of the optimizer choice on generalization, achieving state-of-the-art performance in self-supervised speaker verification."
   ],
   "p1": 1503,
   "pn": 1507,
   "doi": "10.21437/Interspeech.2025-2361",
   "url": "interspeech_2025/fathan25_interspeech.html"
  },
  "sheikh25_interspeech": {
   "authors": [
    [
     "Zaid",
     "Sheikh"
    ],
    [
     "Shuichiro",
     "Shimizu"
    ],
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Samuele",
     "Cornell"
    ],
    [
     "Xinjian",
     "Li"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Scalable Spontaneous Speech Dataset (SSSD): Crowdsourcing Data Collection to Promote Dialogue Research",
   "original": "2362",
   "order": 809,
   "page_count": 5,
   "abstract": [
    "This paper introduces the Scalable Spontaneous Speech Dataset (SSSD) project, comprising 727 hours of spontaneous English conversations between two randomly-matched, anonymous participants on Amazon Mechanical Turk (MTurk) crowd-sourcing platform. The dataset features conversations averaging 25-30 minutes, covering a wide range of everyday topics. A key innovation of this work is our approach to maximizing the number of MTurk workers concurrently participating in our task, enabling more effective randomized matching and live two-person conversations. Data quality is ensured through a two-tiered task structure: a qualification round to select reliable workers, followed by the main recording sessions. We detail our methodology for collecting and recording spontaneous voice conversations, present analyses of the conversational content and speech quality of the dataset in comparison to other datasets, and discuss potential usage."
   ],
   "p1": 3963,
   "pn": 3967,
   "doi": "10.21437/Interspeech.2025-2362",
   "url": "interspeech_2025/sheikh25_interspeech.html"
  },
  "schouwenaars25_interspeech": {
   "authors": [
    [
     "Atty",
     "Schouwenaars"
    ],
    [
     "Esther",
     "Ruigendijk"
    ]
   ],
   "title": "Processing of grammatical information in cochlear implant simulated speech by German adult listeners",
   "original": "2369",
   "order": 774,
   "page_count": 5,
   "abstract": [
    "This study investigates how German which-questions are processed in cochlear implant (CI) simulated speech compared to normal speech. In addition, it explores the role of working memory capacity (WM). Using eye-tracking, accuracy and gaze patterns for subject, object and passive questions were studied. Questions were disambiguated with grammatical information, specifically morphosyntactic cues: case and/or subject-verb agreement. Results show that only object question accuracy was affected by CI simulation. Similarly, eye movements showed weaker interpretation preferences for object questions in CI simulation. Participants with high WM had higher accuracy and their eye gaze showed quicker revisions of garden path effects compared to those with medium or low WM capacity. These findings show that morphosyntactic cues support correct interpretation of object questions in CI simulation, but processing these questions is slower and influenced by working memory capacity."
   ],
   "p1": 3788,
   "pn": 3792,
   "doi": "10.21437/Interspeech.2025-2369",
   "url": "interspeech_2025/schouwenaars25_interspeech.html"
  },
  "netzorg25_interspeech": {
   "authors": [
    [
     "Robin",
     "Netzorg"
    ],
    [
     "Naomi",
     "Carvalho"
    ],
    [
     "Andrea",
     "Guzman"
    ],
    [
     "Lydia",
     "Wang"
    ],
    [
     "Juliana",
     "Francis"
    ],
    [
     "Klo Vivienne",
     "Garoute"
    ],
    [
     "Keith",
     "Johnson"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "On the Production and Perception of a Single Speaker's Gender",
   "original": "2372",
   "order": 140,
   "page_count": 5,
   "abstract": [
    "A voice&#x27;s gender is considered to be dictated by one&#x27;s biology and cultural situation. Without modification, this determinism results in colinearity between acoustic metrics, making disentangling a metric&#x27;s contribution to gender perception difficult. To study disentanglement on natural speech, we collaborate with a gender-affirming voice teacher to collect the Disentangled Source-Filter Dataset (DSFD): 45-minutes of audio along 25 Pitch, Resonance, and Weight voice configurations, coupled with Electroglottograph (EGG) measurements. Our analysis demonstrates certain acoustic and physical metrics, namely avg. $F_0$, $\\Delta F$, Contact Quotient (CQ), and Loudness correlate with Pitch, Resonance, and Weight. Going on to perform perceptual studies of gender, naturalness, and realness, we see that $\\Delta F$ is the strongest predictor of perceived gender. Perceived naturalness and realness of a voice, however, prove to be unpredictable by these acoustic metrics."
   ],
   "p1": 669,
   "pn": 673,
   "doi": "10.21437/Interspeech.2025-2372",
   "url": "interspeech_2025/netzorg25_interspeech.html"
  },
  "mujtaba25_interspeech": {
   "authors": [
    [
     "Dena",
     "Mujtaba"
    ],
    [
     "Nihar R.",
     "Mahapatra"
    ]
   ],
   "title": "Fine-Tuning ASR for Stuttered Speech: Personalized vs. Generalized Approaches",
   "original": "2373",
   "order": 730,
   "page_count": 5,
   "abstract": [
    "Stuttering - characterized by involuntary disfluencies such as blocks, prolongations, and repetitions - is often misinterpreted by automatic speech recognition (ASR) systems, resulting in elevated word error rates and making voice-driven technologies inaccessible to people who stutter. The variability of disfluencies across speakers and contexts further complicates ASR training, compounded by limited annotated stuttered speech data. In this paper, we investigate fine-tuning ASRs for stuttered speech, comparing generalized models (trained across multiple speakers) to personalized models tailored to individual speech characteristics. Using a diverse range of voice-AI scenarios, including virtual assistants and video interviews, we evaluate how personalization affects transcription accuracy. Our findings show that personalized ASRs significantly reduce word error rates, especially in spontaneous speech, highlighting the potential of tailored models for more inclusive voice technologies."
   ],
   "p1": 3568,
   "pn": 3572,
   "doi": "10.21437/Interspeech.2025-2373",
   "url": "interspeech_2025/mujtaba25_interspeech.html"
  },
  "wang25z_interspeech": {
   "authors": [
    [
     "Qingzheng",
     "Wang"
    ],
    [
     "Jiancheng",
     "Sun"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC",
   "original": "2377",
   "order": 425,
   "page_count": 5,
   "abstract": [
    "Multilingual speech processing with self-supervised or supervised pre-trained Speech Foundation Models (SFM) has achieved strong performance on tasks like Language Identification (LID) and Automatic Speech Recognition (ASR). However, these models struggle with limited resources during fine-tuning. This paper enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple strategies for adapting SFMs, including frozen upstream training, partial fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation to mitigate performance gaps in few-shot settings and introduce LID Connectionist Temporal Classification (CTC) loss for regularization. Our approach achieves a 14% relative improvement in LID accuracy and a 30% relative reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place in the Interspeech 2025 ML-SUPERB 2.0 Challenge."
   ],
   "p1": 2088,
   "pn": 2092,
   "doi": "10.21437/Interspeech.2025-2377",
   "url": "interspeech_2025/wang25z_interspeech.html"
  },
  "dumpala25_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "Chandramouli S.",
     "Sastry"
    ],
    [
     "Rudolf",
     "Uher"
    ],
    [
     "Sageev",
     "Oore"
    ]
   ],
   "title": "Test-Time Training for Speech-based Depression Detection",
   "original": "2378",
   "order": 102,
   "page_count": 5,
   "abstract": [
    "Previous works on speech-based depression detection typically use datasets collected in similar environments for both training and testing the models. However, in practice, the training and testing distributions often differ. Distributional shifts in speech can result from various factors, such as differences in recording environments (e.g., background noise) and demographic attributes (e.g., gender, age). These shifts can significantly degrade the performance of depression detection models. In this paper, we analyze the application of test-time training (TTT) to improve the robustness of depression detection models against such shifts. Our results demonstrate that TTT can substantially enhance model performance under various distributional shifts, including those caused by (a) background noise, (b) gender bias, and (c) differences in data collection and curation procedures, where training and testing samples originate from different datasets."
   ],
   "p1": 479,
   "pn": 483,
   "doi": "10.21437/Interspeech.2025-2378",
   "url": "interspeech_2025/dumpala25_interspeech.html"
  },
  "tabatabaee25b_interspeech": {
   "authors": [
    [
     "Saba",
     "Tabatabaee"
    ],
    [
     "Suzanne",
     "Boyce"
    ],
    [
     "Liran",
     "Oren"
    ],
    [
     "Mark",
     "Tiede"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality",
   "original": "2387",
   "order": 71,
   "page_count": 5,
   "abstract": [
    "Speech is produced through the coordination of vocal tract constricting organs: lips, tongue, velum, and glottis. Previous works developed a Speech Inversion (SI) systems to recover acoustic-to-articulatory mappings for lip and tongue constrictions, called oral tract variables (TVs), which were later enhanced by including source information (periodic and aperiodic energies, and F0 frequency) as proxies for glottal control. Comparison of the nasometric measures with high-speed nasopharyngoscopy showed that nasalance can serve as ground truth, and that an SI system trained with it reliably recovers velum movement patterns for American English speakers. Here, two SI training approaches are compared: baseline models that estimate oral TVs and nasalance independently, and a synergistic model that combines oral TVs and source features with nasalance. The synergistic model shows relative improvements of 5% in oral TVs estimation and 9% in nasalance estimation compared to the baseline models."
   ],
   "p1": 325,
   "pn": 329,
   "doi": "10.21437/Interspeech.2025-2387",
   "url": "interspeech_2025/tabatabaee25b_interspeech.html"
  },
  "xie25_interspeech": {
   "authors": [
    [
     "Jiamin",
     "Xie"
    ],
    [
     "Ju",
     "Lin"
    ],
    [
     "Yiteng",
     "Huang"
    ],
    [
     "Tyler",
     "Vuong"
    ],
    [
     "Zhaojiang",
     "Lin"
    ],
    [
     "Zhaojun",
     "Yang"
    ],
    [
     "Peng",
     "Su"
    ],
    [
     "Prashant",
     "Rawat"
    ],
    [
     "Sangeeta",
     "Srivastava"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition",
   "original": "2388",
   "order": 796,
   "page_count": 5,
   "abstract": [
    "Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities. However, the ability of Speech LLMs to comprehend and process multi-channel audio with spatial cues remains a relatively uninvestigated area of research. In this work, we present directional-SpeechLlama, a novel approach that leverages the microphone array of smart glasses to achieve directional speech recognition, source localization, and bystander cross-talk suppression. To enhance the model&#x27;s ability to understand directivity, we propose two key techniques: serialized directional output training (S-DOT) and contrastive direction data augmentation (CDDA). Experimental results show that our proposed directional-SpeechLlama effectively captures the relationship between textual cues and spatial audio, yielding strong performance in both speech recognition and source localization tasks."
   ],
   "p1": 3898,
   "pn": 3902,
   "doi": "10.21437/Interspeech.2025-2388",
   "url": "interspeech_2025/xie25_interspeech.html"
  },
  "ahadzi25_interspeech": {
   "authors": [
    [
     "Edem",
     "Ahadzi"
    ],
    [
     "Vishwanath",
     "Pratap Singh"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Ville",
     "Hautamaki"
    ]
   ],
   "title": "Continuous Learning for Children's ASR: Overcoming Catastrophic Forgetting with Elastic Weight Consolidation and Synaptic Intelligence",
   "original": "2393",
   "order": 587,
   "page_count": 5,
   "abstract": [
    "In this work, we present the first study addressing automatic speech recognition (ASR) for children in an online learning setting. This is particularly important for both child-centric applications and the privacy protection of minors, where training models with sequentially arriving data is critical. The conventional approach of model fine-tuning often suffers from catastrophic forgetting. To tackle this issue, we explore two established techniques: elastic weight consolidation (EWC) and synaptic intelligence (SI). Using a custom protocol on the MyST corpus, tailored to the online learning setting, we achieve relative word error rate (WER) reductions of 5.21 % with EWC and 4.36 % with SI, compared to the fine-tuning baseline."
   ],
   "p1": 2880,
   "pn": 2884,
   "doi": "10.21437/Interspeech.2025-2393",
   "url": "interspeech_2025/ahadzi25_interspeech.html"
  },
  "shi25g_interspeech": {
   "authors": [
    [
     "Xuan",
     "Shi"
    ],
    [
     "Yubin",
     "Zhang"
    ],
    [
     "Yijing",
     "Lu"
    ],
    [
     "Marcus",
     "Ma"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Haley",
     "Hsu"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "75-Speaker Annot-16: A benchmark dataset for speech articulatory rt-MRI annotation with articulator contours and phonetic alignment",
   "original": "2394",
   "order": 446,
   "page_count": 5,
   "abstract": [
    "High-quality speech articulatory databases are essential for advancing speech science and technology research. However, the lack of standardized annotations limits their full potential use and broad accessibility. In this context, we introduce 75-Speaker Annot-16, a comprehensive annotation dataset derived from the 75-Speaker vocal tract MRI database. Annot-16 provides phonetic alignments, articulator contour annotations, and handmade ground-truth articulator contours. Our annotation process integrates automated algorithms with expert verification to ensure accuracy and efficiency. To demonstrate its utility, we establish three benchmark tasks: speech phoneme recognition, articulatory contour segmentation, and articulatory phoneme recognition. Annot-16 can serve as a valuable resource for speech modeling, computer vision, and cross-modal learning, bridging engineering applications, speech science, and linguistic research."
   ],
   "p1": 2175,
   "pn": 2179,
   "doi": "10.21437/Interspeech.2025-2394",
   "url": "interspeech_2025/shi25g_interspeech.html"
  },
  "fort25_interspeech": {
   "authors": [
    [
     "Alexandra",
     "Fort"
    ],
    [
     "Francis",
     "Tyers"
    ]
   ],
   "title": "Evaluating Wav2Vec2-Bert for Computer-Assisted Pronunciation Training for isiZulu",
   "original": "2400",
   "order": 443,
   "page_count": 5,
   "abstract": [
    "Pronunciation training is essential for learning a second language, but tools for computer-assisted pronunciation training have only been developed for a small subset of languages. Through creating and evaluating three fine-tuned versions of Wav2Vec2-Bert, this paper investigates the performance of Wav2Vec2-Bert for detecting language learner errors in low-resource settings. The results provide insight into how the data used for fine-tuning can impact performance and a thorough analysis of erroneous predictions. The evaluation of Wav2Vec2-Bert for this task offers a case study of an under-resourced language and suggestions for how a large-language model can be used to develop pronunciation training tools in under-resourced settings."
   ],
   "p1": 2160,
   "pn": 2164,
   "doi": "10.21437/Interspeech.2025-2400",
   "url": "interspeech_2025/fort25_interspeech.html"
  },
  "wu25l_interspeech": {
   "authors": [
    [
     "Hongchen",
     "Wu"
    ],
    [
     "Yixin",
     "Gu"
    ]
   ],
   "title": "CrossPhon: An Auto Phone Mapping Tool to Streamline Cross-language Modeling for Phone Alignment of Low-resource Languages",
   "original": "2401",
   "order": 19,
   "page_count": 5,
   "abstract": [
    "Phone alignment matches spoken sounds with text, streamlining speech dataset creation and analysis. However, most trained aligners focus on Indo-European languages, leaving under-resourced languages unsupported. Developing new aligners for these languages requires expertise and large datasets, which are often scarce. Cross-language phone alignment offers a solution using aligners trained in one language to align speech in another, but it traditionally relies on expert-crafted phone mappings. Our tool, CrossPhon, automates this process, making cross-language phone alignment more efficient. In tests on 14 languages from 7 families, CrossPhon achieved agreement rates of 78.95% to 97.77% compared to human expert mappings and delivered competitive performance in cross-language phone alignment. CrossPhon provides an efficient, reliable solution for generating cross-language phone alignment in under-resourced languages, helping bridge the digital divide and efficiently study these languages."
   ],
   "p1": 86,
   "pn": 90,
   "doi": "10.21437/Interspeech.2025-2401",
   "url": "interspeech_2025/wu25l_interspeech.html"
  },
  "eads25_interspeech": {
   "authors": [
    [
     "Amanda",
     "Eads"
    ],
    [
     "Heather",
     "Kabakoff"
    ],
    [
     "Nina",
     "Benway"
    ],
    [
     "Elaine",
     "Hitchcock"
    ],
    [
     "Jonathan",
     "Preston"
    ],
    [
     "Tara",
     "McAllister"
    ]
   ],
   "title": "PERCEPT-US: A Multimodal American English Child Speech Corpus Specialized for Articulatory Feedback",
   "original": "2407",
   "order": 572,
   "page_count": 5,
   "abstract": [
    "We present PERCEPT-US, a multimodal corpus of audio and ultrasound data from 126 American English-speaking children ages 8 to 17. Collected during clinical trials investigating biofeedback in speech therapy for residual speech sound disorder (RSSD), it includes 80 children with no history of speech-language-hearing challenges and 46 children with no challenges other than RSSD. The corpus is balanced by sex (58 females, 68 males) and stratified by age and speech therapy history. Participants completed syllabic, word, and sentence level speech production tasks, totaling 24,699 utterances, with a focus on American English rhotics. The corpus demonstration uses mixed-effects linear regression on segmented acoustic and labeled ultrasound data from 69 children to show that rhotic tongue shape categories significantly predict the acoustics of higher formant frequency values - the first study to demonstrate this acoustic-articulatory relationship in a large pediatric sample."
   ],
   "p1": 2805,
   "pn": 2809,
   "doi": "10.21437/Interspeech.2025-2407",
   "url": "interspeech_2025/eads25_interspeech.html"
  },
  "xie25b_interspeech": {
   "authors": [
    [
     "Jingran",
     "Xie"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Hui",
     "Wang"
    ],
    [
     "Yue",
     "Yu"
    ],
    [
     "Yang",
     "Xiang"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving",
   "original": "2409",
   "order": 497,
   "page_count": 5,
   "abstract": [
    "Large language models (LLMs) have shown remarkable generalization across tasks, leading to increased interest in integrating speech with LLMs. These speech LLMs (SLLMs) typically use supervised fine-tuning to align speech with text-based LLMs. However, the lack of annotated speech data across a wide range of tasks hinders alignment efficiency, resulting in poor generalization. To address these issues, we propose a novel multi-task &#x27;behavior imitation&#x27; method with speech-text interleaving, called MTBI, which relies solely on paired speech and transcripts. By ensuring the LLM decoder generates equivalent responses to paired speech and text, we achieve a more generalized SLLM. Interleaving is used to further enhance alignment efficiency. We introduce a simple benchmark to evaluate prompt and task generalization across different models. Experimental results demonstrate that our MTBI outperforms state-of-the-art SLLMs on both prompt and task generalization, while requiring less supervised speech data."
   ],
   "p1": 2430,
   "pn": 2434,
   "doi": "10.21437/Interspeech.2025-2409",
   "url": "interspeech_2025/xie25b_interspeech.html"
  },
  "cheng25d_interspeech": {
   "authors": [
    [
     "Jiali",
     "Cheng"
    ],
    [
     "Hadi",
     "Amiri"
    ]
   ],
   "title": "Speech Unlearning",
   "original": "2412",
   "order": 653,
   "page_count": 5,
   "abstract": [
    "We introduce a challenging and under-explored task -- Machine Unlearning for Speech Tasks. Machine unlearning aims to remove influence of a subset of data from an already trained model. It has practical applications of removing private or sensitive information, and updating outdated knowledge. We propose two unlearning tasks: sample unlearning and class unlearning, and discuss their challenges. In addition, we propose to use subset performances and membership inference attack as evaluation metrics. Experiments on two different speech tasks -- keyword spotting and speaker identification -- demonstrate that unlearning on speech data is more challenging than unlearning on image and text data, which receives more attention. We discuss future steps to advance machine unlearning on speech data, including curriculum learning."
   ],
   "p1": 3209,
   "pn": 3213,
   "doi": "10.21437/Interspeech.2025-2412",
   "url": "interspeech_2025/cheng25d_interspeech.html"
  },
  "subramanian25b_interspeech": {
   "authors": [
    [
     "Aswin Shanmugam",
     "Subramanian"
    ],
    [
     "Amit",
     "Das"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios",
   "original": "2414",
   "order": 1123,
   "page_count": 5,
   "abstract": [
    "We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models—Conformer Transducer for streaming and Sequence-to-Sequence for offline—or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions."
   ],
   "p1": 5508,
   "pn": 5512,
   "doi": "10.21437/Interspeech.2025-2414",
   "url": "interspeech_2025/subramanian25b_interspeech.html"
  },
  "zhou25h_interspeech": {
   "authors": [
    [
     "Xuanru",
     "Zhou"
    ],
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Cheol Jun",
     "Cho"
    ],
    [
     "Tejas",
     "Prabhune"
    ],
    [
     "Shuhe",
     "Li"
    ],
    [
     "William",
     "Li"
    ],
    [
     "Rodrigo",
     "Ortiz"
    ],
    [
     "Zoe",
     "Ezzes"
    ],
    [
     "Jet",
     "Vonk"
    ],
    [
     "Brittany",
     "Morin"
    ],
    [
     "Rian",
     "Bogley"
    ],
    [
     "Lisa",
     "Wauters"
    ],
    [
     "Zachary",
     "Miller"
    ],
    [
     "Maria",
     "Gorno-Tempini"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "Towards Accurate Phonetic Error Detection Through Phoneme Similarity Modeling",
   "original": "2417",
   "order": 964,
   "page_count": 5,
   "abstract": [
    "Phonetic error detection, as a core subtask of automatic pronunciation assessment, aims to identify pronunciation deviations at the fine-grained phoneme level. However, variability in both speech production and perception, including accents, and dysfluencies, presents a significant challenge for phoneme recognition. Current models are unable to capture these discrepancies effectively. In this work, we propose a framework for verbatim phoneme recognition, employing multi-task training with a novel phoneme similarity modeling. Unlike most previous studies that focus on transcribing what the person is supposed to say, our method aims to transcribe what the person actually said. We develop a simulated dataset VCTK-accent contains phonetic errors, which is open-sourced, and propose two novel metrics for assessing pronunciation differences. Our work provides a new benchmark for the phonetic error detection task."
   ],
   "p1": 4738,
   "pn": 4742,
   "doi": "10.21437/Interspeech.2025-2417",
   "url": "interspeech_2025/zhou25h_interspeech.html"
  },
  "ali25_interspeech": {
   "authors": [
    [
     "Hashim",
     "Ali"
    ],
    [
     "Surya",
     "Subramani"
    ],
    [
     "Raksha",
     "Varahamurthy"
    ],
    [
     "Nithin",
     "Adupa"
    ],
    [
     "Lekha",
     "Bollinani"
    ],
    [
     "Hafiz",
     "Malik"
    ]
   ],
   "title": "Collecting, Curating, and Annotating Good Quality Speech deepfake dataset for Famous Figures: Process and Challenges",
   "original": "2418",
   "order": 802,
   "page_count": 5,
   "abstract": [
    "Recent advances in speech synthesis have introduced unprecedented challenges in maintaining voice authenticity, particularly concerning public figures who are frequent targets of impersonation attacks. This paper presents a comprehensive methodology for collecting, curating, and generating synthetic speech data for political figures and a detailed analysis of challenges encountered. We introduce a systematic approach incorporating an automated pipeline for collecting high-quality bonafide speech samples, featuring transcription-based segmentation that significantly improves synthetic speech quality. We experimented with various synthesis approaches; from single-speaker to zero-shot synthesis, and documented the evolution of our methodology. The resulting dataset comprises bonafide and synthetic speech samples from ten public figures, demonstrating superior quality with a NISQA-TTS naturalness score of 3.69 and the highest human misclassification rate of 61.9%."
   ],
   "p1": 3928,
   "pn": 3932,
   "doi": "10.21437/Interspeech.2025-2418",
   "url": "interspeech_2025/ali25_interspeech.html"
  },
  "chen25o_interspeech": {
   "authors": [
    [
     "Xi",
     "Chen"
    ],
    [
     "Renzhe",
     "Yu"
    ],
    [
     "Yanshen",
     "Tan"
    ],
    [
     "Yiyi",
     "Li"
    ],
    [
     "Quan",
     "Qian"
    ],
    [
     "Ying",
     "Lin"
    ]
   ],
   "title": "Predicting Adolescent Suicidal Risk from Multi-task-based Speech: An Ensemble Learning Approach",
   "original": "2419",
   "order": 88,
   "page_count": 5,
   "abstract": [
    "Adolescent suicide is a pressing global public health issue. Timely identification of suicide risk is crucial. Traditional methods of assessing suicide risk are often limited by their reliance on subjective input and resource requirements. This paper aims to address these limitations by detecting suicide risk from multi-task-based speech, utilizing a dataset of 600 Chinese adolescents (age: 10-18 yr) provided by the 1st SpeechWellness Challenge. Our approach involved both acoustic and semantic features extracted by OpenSmile, Emotion2Vec, and a fine-tuned BERT-Chinese model. The base models were trained using XGBoost and SVM, etc., with hyperparameters tuned by Bayesian optimization. Then we implemented a multi-model and multi-task nested voting ensemble framework to integrate the base models, achieving a final test set accuracy of 0.63 (recall=0.74, F1≈0.67). This work highlights the potential of voice-based biomarkers in mental health assessment."
   ],
   "p1": 409,
   "pn": 413,
   "doi": "10.21437/Interspeech.2025-2419",
   "url": "interspeech_2025/chen25o_interspeech.html"
  },
  "huang25i_interspeech": {
   "authors": [
    [
     "Shangkun",
     "Huang"
    ],
    [
     "Jing",
     "Deng"
    ],
    [
     "Jintao",
     "Kang"
    ],
    [
     "Rong",
     "Zheng"
    ]
   ],
   "title": "Leveraging LLM for Stuttering Speech: A Unified Architecture Bridging Recognition and Event Detection",
   "original": "2425",
   "order": 376,
   "page_count": 5,
   "abstract": [
    "The performance bottleneck of Automatic Speech Recognition (ASR) in stuttering speech scenarios has limited its applicability in domains such as speech rehabilitation. This paper proposed an LLM-driven ASR-SED multi-task learning framework that jointly optimized the ASR and Stuttering Event Detection (SED) tasks. We proposed a dynamic interaction mechanism where the ASR branch leveraged CTC-generated soft prompts to assist LLM context modeling, while the SED branch output stutter embeddings to enhance LLM comprehension of stuttered speech. We incorporated contrastive learning to strengthen the discriminative power of stuttering acoustic features and applied Focal Loss to mitigate the long-tailed distribution in stuttering event categories. Evaluations on the AS-70 Mandarin stuttering dataset demonstrated that our framework reduced the ASR character error rate (CER) to 5.45% (-37.71% relative reduction) and achieved an average SED F1-score of 73.63% (+46.58% relative improvement)."
   ],
   "p1": 1843,
   "pn": 1847,
   "doi": "10.21437/Interspeech.2025-2425",
   "url": "interspeech_2025/huang25i_interspeech.html"
  },
  "xiao25e_interspeech": {
   "authors": [
    [
     "Yao",
     "Xiao"
    ],
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Stefan",
     "Goetze"
    ]
   ],
   "title": "Alzheimer’s Dementia Detection Using Perplexity from Paired Large Language Models",
   "original": "2428",
   "order": 291,
   "page_count": 5,
   "abstract": [
    "Alzheimer’s dementia (AD) is a neurodegenerative disorder with cognitive decline that commonly impacts language ability. This work extends the paired perplexity approach to detecting AD by using a recent large language model (LLM), the instruction-following version of Mistral-7B. We improve accuracy by an average of 3.33% over the best current paired perplexity method and by 6.35% over the top-ranked method from the ADReSS 2020 challenge benchmark. Our further analysis demonstrates that the proposed approach can effectively detect AD with a clear and interpretable decision boundary in contrast to other methods that suffer from opaque decision-making processes. Finally, by prompting the fine-tuned LLMs and comparing the model-generated responses to human responses, we illustrate that the LLMs have learned the special language patterns of AD speakers, which opens up possibilities for novel methods of model interpretation and data augmentation."
   ],
   "p1": 1423,
   "pn": 1427,
   "doi": "10.21437/Interspeech.2025-2428",
   "url": "interspeech_2025/xiao25e_interspeech.html"
  },
  "wu25m_interspeech": {
   "authors": [
    [
     "Hongchen",
     "Wu"
    ],
    [
     "Yao",
     "Du"
    ],
    [
     "Zirong",
     "Li"
    ],
    [
     "Yixin",
     "Gu"
    ],
    [
     "Disha Thotappala",
     "Jayaprakash"
    ],
    [
     "Li",
     "Sheng"
    ]
   ],
   "title": "Evaluating Automatic Speech Recognition Pipelines for Mandarin-English Bilingual Child Language Assessment in Telehealth",
   "original": "2430",
   "order": 626,
   "page_count": 5,
   "abstract": [
    "Bilingualism is rising worldwide, yet bilingual child assessments face major challenges. A shortage of bilingual clinicians and the labor-intensive nature of speech data annotation often cause misdiagnoses, delaying care and research. Using a Mandarin-English adult-child speech dataset (53 telehealth sessions), we explore how speech models can automate the annotation of clinical data involving multi-languages, multi-speakers, children&#x27;s speech, and code-switching utterances. Findings indicated that simple pre-processing improves automatic speech recognition (ASR) accuracy. Specifically, integrating speaker diarization with OpenAI’s Whisper medium model reduces word error rates to 35% for child speech and 30% for code-switching, rivaling fine-tuned transformer models. As the first ASR pipeline evaluation for a Mandarin-English clinical dataset, our study highlights model limitations, establishes a benchmark for bilingual speech technology, and improves clinical services."
   ],
   "p1": 3075,
   "pn": 3079,
   "doi": "10.21437/Interspeech.2025-2430",
   "url": "interspeech_2025/wu25m_interspeech.html"
  },
  "gohider25_interspeech": {
   "authors": [
    [
     "Nada",
     "Gohider"
    ],
    [
     "Otman",
     "Basir"
    ]
   ],
   "title": "Towards Inclusive and Fair ASR: Insights from the SAPC Challenge for Optimizing Disordered Speech Recognition",
   "original": "2431",
   "order": 666,
   "page_count": 5,
   "abstract": [
    "ASR has advanced significantly, yet remains limited for impaired speakers due to data scarcity. In response to this gap, the Speech Accessibility Project (SAP) represents a significant initiative in data collection on impaired speech. This paper reports our participation in the SAPC challenge, where we leveraged SAP data to improve ASR performance for disordered speech. Our system ranked fourth in terms of Word Error Rate, recording values of 10.06% and 11.8% WER for Test1 and Test2 subsets, respectively, on the challenge leaderboard. In particular, our research examines the power of SOTA ASR models to capture contextual information in the presence of disordered speech disfluencies. We focused on two ASR architectures, ContextNet and Parakeet, based on their documented ability to efficiently and effectively handle contextual information for typical speech, utilizing distinct mechanisms. Our experiments demonstrated that Parakeet slightly outperformed ContextNet, as evidenced by WER."
   ],
   "p1": 3274,
   "pn": 3278,
   "doi": "10.21437/Interspeech.2025-2431",
   "url": "interspeech_2025/gohider25_interspeech.html"
  },
  "fathan25b_interspeech": {
   "authors": [
    [
     "Abderrahim",
     "Fathan"
    ],
    [
     "Jahangir",
     "Alam"
    ]
   ],
   "title": "Automatic Labeling and Correction of Noisy Labels for Robust Self-Supervised Speaker Verification",
   "original": "2433",
   "order": 868,
   "page_count": 5,
   "abstract": [
    "Supervised speaker verification relies on large labeled datasets, which are costly and labor-intensive to create. However, both manual and clustering-based labeling methods introduce label noise, degrading model generalization. To leverage unlabeled speech data, we propose a framework that automatically generates and refines pseudo speaker labels. It first generates pseudo-labels using a clustering algorithm, then trains a speaker verification system to boost the quality of pseudo-labeled data using self-supervised learning and a neural embedding extractor optimized with refined loss function. This function integrates a dynamic and adaptive label noise cleansing method, termed AdaptiveDropSC, which tracks dominant sub-centers via a dictionary table for better label correction. Experiments on VoxCeleb corpus show that our method improves pseudo-labeling accuracy across different clustering techniques, achieving state-of-the-art performance in self-supervised speaker verification."
   ],
   "p1": 4258,
   "pn": 4262,
   "doi": "10.21437/Interspeech.2025-2433",
   "url": "interspeech_2025/fathan25b_interspeech.html"
  },
  "lee25i_interspeech": {
   "authors": [
    [
     "Seonggyu",
     "Lee"
    ],
    [
     "Sein",
     "Cheong"
    ],
    [
     "Sangwook",
     "Han"
    ],
    [
     "Kihyuk",
     "Kim"
    ],
    [
     "Jong Won",
     "Shin"
    ]
   ],
   "title": "Speech Enhancement based on cascaded two flows ",
   "original": "2436",
   "order": 989,
   "page_count": 5,
   "abstract": [
    "Speech enhancement (SE) based on diffusion probabilistic models has exhibited impressive performance, while requiring a relatively high number of function evaluations (NFE). Recently, SE based on flow matching has been proposed, which showed competitive performance with a small NFE. Early approaches adopted the noisy speech as the only conditioning variable. There have been other approaches which utilize speech enhanced with a predictive model as another conditioning variable and to sample an initial value, but they require a separate predictive model on top of the generative SE model. In this work, we propose to employ an identical model based on flow matching for both SE and generating enhanced speech used as an initial starting point and a conditioning variable. Experimental results showed that the proposed method required the same or fewer NFEs even with two cascaded generative methods while achieving equivalent or better performances to the previous baselines."
   ],
   "p1": 4863,
   "pn": 4867,
   "doi": "10.21437/Interspeech.2025-2436",
   "url": "interspeech_2025/lee25i_interspeech.html"
  },
  "liu25p_interspeech": {
   "authors": [
    [
     "Xi",
     "Liu"
    ],
    [
     "Mu",
     "Yang"
    ],
    [
     "Szu-Jui",
     "Chen"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "A Neural Codec Approach for Noise-Robust Bandwidth Expansion",
   "original": "2438",
   "order": 833,
   "page_count": 5,
   "abstract": [
    "Neural audio codec technology has recently attracted significant attention in various speech processing tasks due to its efficient quantized latent features. In this work, we introduce a novel approach that leverages a pre-trained neural codec network to perform both speech denoising and bandwidth expansion simultaneously. Specifically, we design a conformer-based deep neural network to predict clean codebook indices, which are then used by the pre-trained audio codec model to generate enhanced and bandwidth-expanded audio. We investigated several strategies for generating the clean indices and compared our approach with state-of-the-art methods on the Valentini-Botinhao noisy test set. Experimental results demonstrate that our method achieves performance comparable to leading approaches in noise-robust bandwidth expansion tasks while offering promising improvements in the quality and intelligibility of narrow-band signals. Audio samples are available."
   ],
   "p1": 4083,
   "pn": 4087,
   "doi": "10.21437/Interspeech.2025-2438",
   "url": "interspeech_2025/liu25p_interspeech.html"
  },
  "smith25b_interspeech": {
   "authors": [
    [
     "Carey",
     "Smith"
    ],
    [
     "Hu",
     "Cheng"
    ],
    [
     "Pertti",
     "Palo"
    ],
    [
     "Daniel",
     "Aalto"
    ],
    [
     "Steven M.",
     "Lulich"
    ]
   ],
   "title": "Exploratory Analysis of Brainstem fMRI Data During Sustained Phonation",
   "original": "2444",
   "order": 219,
   "page_count": 5,
   "abstract": [
    "Very little work has been done on functional imaging of the cranial nerve nuclei in the brainstem in the context of motor control. This study is a preliminary step in understanding the functional organization and involvement of the brainstem during speech motor control. We aim to identify the impact of using contrastive sustained phonation tasks in isolating activation patterns. We provide preliminary observations of activation patterns in the cranial nerve nuclei. In the study 4 participants performed sustained phonation of [a] and [i] for 15 seconds, repeated over 12 trials. Phonation of [a] resulted in greater activation in the upper pons, in the vicinity of the trigeminal nucleus, compared to [i]. Phonation of [i] elicited activation more anterior and inferior, consistent with expected locations of the facial nucleus. Both phonemes resulted in activation in the medulla, consistent with the expected location of the hypoglossal nucleus."
   ],
   "p1": 1063,
   "pn": 1067,
   "doi": "10.21437/Interspeech.2025-2444",
   "url": "interspeech_2025/smith25b_interspeech.html"
  },
  "guo25d_interspeech": {
   "authors": [
    [
     "Chenxu",
     "Guo"
    ],
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Xuanru",
     "Zhou"
    ],
    [
     "Jinming",
     "Zhang"
    ],
    [
     "Shuhe",
     "Li"
    ],
    [
     "Zongli",
     "Ye"
    ],
    [
     "Peter",
     "Park"
    ],
    [
     "Anaisha",
     "Das"
    ],
    [
     "Zoe",
     "Ezzes"
    ],
    [
     "Jet",
     "Vonk"
    ],
    [
     "Brittany",
     "Morin"
    ],
    [
     "Rian",
     "Bogley"
    ],
    [
     "Lisa",
     "Wauters"
    ],
    [
     "Zachary",
     "Miller"
    ],
    [
     "Maria",
     "Gorno-Tempini"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection",
   "original": "2446",
   "order": 452,
   "page_count": 5,
   "abstract": [
    "Automatic detection of speech dysfluency aids speech-language pathologists in efficient transcription of disordered speech, enhancing diagnostics and treatment planning. Traditional methods, often limited to classification, provide insufficient clinical insight, and text-independent models misclassify dysfluency, especially in context-dependent cases. This work introduces Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with upstream encoders like WavLM and requires no additional training. It achieves state-of-the-art performance in both phonetic error rate and dysfluency detection on simulated and real speech data. Our approach is lightweight, interpretable, and effective, demonstrating that explicit modeling of pronunciation behavior in decoding, rather than complex architectures, is key to improving dysfluency processing systems."
   ],
   "p1": 2205,
   "pn": 2209,
   "doi": "10.21437/Interspeech.2025-2446",
   "url": "interspeech_2025/guo25d_interspeech.html"
  },
  "lin25h_interspeech": {
   "authors": [
    [
     "Zijian",
     "Lin"
    ],
    [
     "Yang",
     "Zhang"
    ],
    [
     "Yougen",
     "Yuan"
    ],
    [
     "Yuming",
     "Yan"
    ],
    [
     "Jinjiang",
     "Liu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Pengfei",
     "Hu"
    ],
    [
     "Qun",
     "Yu"
    ]
   ],
   "title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding",
   "original": "2447",
   "order": 1128,
   "page_count": 5,
   "abstract": [
    "Modern autoregressive speech synthesis models leveraging language models have demonstrated remarkable performance. However, the sequential nature of next token prediction in these models leads to significant latency, hindering their deployment in scenarios where inference speed is critical. In this work, we propose Speech Speculative Decoding (SSD), a novel framework for autoregressive speech synthesis acceleration. Specifically, our method employs a lightweight draft model to generate candidate token sequences, which are subsequently verified in parallel by the target model using the proposed SSD framework. Experimental results demonstrate that SSD achieves a significant speedup of 1.4x compared with conventional autoregressive decoding, while maintaining high fidelity and naturalness. Subjective evaluations further validate the effectiveness of SSD in preserving the perceptual quality of the target model while accelerating inference."
   ],
   "p1": 5533,
   "pn": 5537,
   "doi": "10.21437/Interspeech.2025-2447",
   "url": "interspeech_2025/lin25h_interspeech.html"
  },
  "araizaillan25_interspeech": {
   "authors": [
    [
     "Gloria",
     "Araiza-Illan"
    ],
    [
     "Luke",
     "Meyer"
    ],
    [
     "Bert",
     "Maat"
    ],
    [
     "Deniz",
     "Başkent"
    ]
   ],
   "title": "Robot-assisted Recognition of Vocal Emotions in Pseudospeech for Cochlear Implanted Adolescents",
   "original": "2448",
   "order": 170,
   "page_count": 5,
   "abstract": [
    "Recognising vocal emotions in speech is difficult for children with hearing loss and who use a cochlear implant (CI). As regular monitoring could be burdensome, we propose a NAO robot as a test interface. Adolescents with CIs (10-17yr) performed the EmoHI test for recognising vocal emotions in pseudospeech (no linguistic emotion information), once with a computer and once with a NAO. Interfaces are compared via test results and durations, and participants’ perception of the interfaces. Test results (sensitivity index, d’) were similar (0.36 ± 0.36 vs. 0.37 ± 0.43), but durations were significantly longer on the NAO (4.18 min ± 39 sec vs. 5.13 min ± 51 sec). The computer had a higher perceived usability, but the NAO was rated more enjoyable, engaging and preferable. Overall, the NAO sound quality seems sufficient for conducting the EmoHI test, even with a CI. The higher ratings of enjoyability for NAO may be especially useful in conducting such tests in populations with hearing devices."
   ],
   "p1": 818,
   "pn": 822,
   "doi": "10.21437/Interspeech.2025-2448",
   "url": "interspeech_2025/araizaillan25_interspeech.html"
  },
  "zheng25d_interspeech": {
   "authors": [
    [
     "Qixi",
     "Zheng"
    ],
    [
     "Yushen",
     "Chen"
    ],
    [
     "Zhikang",
     "Niu"
    ],
    [
     "Ziyang",
     "Ma"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Kai",
     "Yu"
    ],
    [
     "Xie",
     "Chen"
    ]
   ],
   "title": "Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling",
   "original": "2449",
   "order": 500,
   "page_count": 5,
   "abstract": [
    "Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS, and F5-TTS, have attracted significant attention in recent years. These models require multiple sampling steps to reconstruct speech from noise, making inference speed a key challenge. Reducing the number of sampling steps can greatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a training-free approach to accelerate the inference of flow-matching-based TTS models. By inspecting the sampling trajectory of F5-TTS, we identify redundant steps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform time-step sampling strategy that effectively reduces the number of sampling steps. Our approach achieves a 7-step generation with an inference RTF of 0.030 on an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS while maintaining comparable performance. Furthermore, EPSS performs well on E2 TTS models, demonstrating its strong generalization ability."
   ],
   "p1": 2445,
   "pn": 2449,
   "doi": "10.21437/Interspeech.2025-2449",
   "url": "interspeech_2025/zheng25d_interspeech.html"
  },
  "chang25c_interspeech": {
   "authors": [
    [
     "Andrew",
     "Chang"
    ],
    [
     "Chenkai",
     "Hu"
    ],
    [
     "Ji",
     "Qi"
    ],
    [
     "Zhuojian",
     "Wei"
    ],
    [
     "Kexin",
     "Zhang"
    ],
    [
     "Viswadruth",
     "Akkaraju"
    ],
    [
     "David",
     "Poeppel"
    ],
    [
     "Dustin",
     "Freeman"
    ]
   ],
   "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience",
   "original": "2451",
   "order": 879,
   "page_count": 5,
   "abstract": [
    "Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model&#x27;s full-data performance. This shows an annotation-efficient framework for modeling videoconference experience."
   ],
   "p1": 4313,
   "pn": 4317,
   "doi": "10.21437/Interspeech.2025-2451",
   "url": "interspeech_2025/chang25c_interspeech.html"
  },
  "zwilling25_interspeech": {
   "authors": [
    [
     "Chris",
     "Zwilling"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Heather",
     "Hodges"
    ],
    [
     "Lorraine",
     "Ramig"
    ],
    [
     "Adina",
     "Bradshaw"
    ],
    [
     "Clarion",
     "Mendes"
    ],
    [
     "Heejin",
     "Kim"
    ],
    [
     "Alexandria",
     "Barkhimer"
    ],
    [
     "Laura",
     "Mattie"
    ],
    [
     "Meg",
     "Dickinson"
    ],
    [
     "Shawnise",
     "Carter"
    ],
    [
     "Marie Moore",
     "Channell"
    ]
   ],
   "title": "The Speech Accessibility Project: Best Practices for Collection and Curation of Disordered Speech",
   "original": "2454",
   "order": 804,
   "page_count": 5,
   "abstract": [
    "Drawing lessons from the collection and curation of disordered speech samples from a large open-source database that includes dysarthria, apraxia, dysphonia, or otherwise atypical speech—the Speech Accessibility Project—this paper provides best practices to guide the collection of diverse speech samples and enhance reproducibility. This paper describes data collection partnerships; the information technology architecture required to collect a massively distributed dataset spanning countries; the data collection process from the participant point of view; and the speech prompts and annotation process used for diverse speech. The open-source data set resulting from this set of best practices will provide a high-quality testbed for researchers around the world to train automatic speech recognition algorithms for disordered speech."
   ],
   "p1": 3938,
   "pn": 3942,
   "doi": "10.21437/Interspeech.2025-2454",
   "url": "interspeech_2025/zwilling25_interspeech.html"
  },
  "cheema25_interspeech": {
   "authors": [
    [
     "Ahsan",
     "Cheema"
    ],
    [
     "Sunil",
     "Puria"
    ]
   ],
   "title": "Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration",
   "original": "2458",
   "order": 171,
   "page_count": 5,
   "abstract": [
    "Trouble hearing in noisy situations remains a common complaint for both individuals with hearing loss and individuals with normal hearing. This is hypothesized to arise due to condition called: cochlear neural degeneration (CND) which can also result in significant variabilities in hearing aids outcomes. This paper uses computational models of auditory periphery to simulate various hearing tasks. We present an objective method to quantify hearing loss and CND by comparing auditory nerve fiber responses using a Neurogram Similarity Index Measure (NSIM). Specifically study 1, shows that NSIM can be used to map performance of individuals with hearing loss on phoneme recognition task with reasonable accuracy. In the study 2, we show that NSIM is a sensitive measure that can also be used to capture the deficits resulting from CND and can be a candidate for noninvasive biomarker of auditory synaptopathy."
   ],
   "p1": 823,
   "pn": 827,
   "doi": "10.21437/Interspeech.2025-2458",
   "url": "interspeech_2025/cheema25_interspeech.html"
  },
  "kwon25b_interspeech": {
   "authors": [
    [
     "Chloe D.",
     "Kwon"
    ]
   ],
   "title": "Speaker-specific Patterns of Phonetic Covariation in Korean Word-medial Stops and the Role of Phonological and Morphological Contexts",
   "original": "2465",
   "order": 971,
   "page_count": 5,
   "abstract": [
    "Individual speaker differences in speech have often been regarded as random, but growing evidence suggests that variation across speakers follows systematic patterns. Yet, it remains an open question whether such structured variation is universal across languages, spans multiple phonetic dimensions, and remains consistent across contexts. Korean word-medial stops provide a compelling test case, as these stops are subject to extensive contextual variability. This study examines patterns in individual speaker variation by analyzing the phonetic distributions and covariation patterns between word-medial stops in Seoul Korean in varying phonological and morphological contexts. The results show that while individual differences arise in phonetic realization of these stops, speakers maintain structured covariation that allows stop categories to remain distinct despite variability, providing further support for the phonetic uniformity account."
   ],
   "p1": 4773,
   "pn": 4777,
   "doi": "10.21437/Interspeech.2025-2465",
   "url": "interspeech_2025/kwon25b_interspeech.html"
  },
  "tripathi25_interspeech": {
   "authors": [
    [
     "Kumud",
     "Tripathi"
    ],
    [
     "Chowdam Venkata",
     "Kumar"
    ],
    [
     "Pankaj",
     "Wasnik"
    ]
   ],
   "title": "Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion",
   "original": "2466",
   "order": 108,
   "page_count": 5,
   "abstract": [
    "Voice Activity Detection (VAD) plays a vital role in speech processing, often relying on hand-crafted or neural features. This study examines the effectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained model (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and Whisper. We propose FusionVAD, a unified framework that combines both feature types using three fusion strategies: concatenation, addition, and cross-attention (CA). Experimental results reveal that simple fusion techniques, particularly addition, outperform CA in both accuracy and efficiency. Fusion-based models consistently surpass single-feature models, highlighting the complementary nature of MFCCs and PTM features. Notably, our best fusion model outperforms state-of-the-art Pyannote VAD model across multiple datasets, achieving an absolute average improvement of 2.04%. These results confirm that simple feature fusion enhances VAD robustness while maintaining computational efficiency."
   ],
   "p1": 509,
   "pn": 513,
   "doi": "10.21437/Interspeech.2025-2466",
   "url": "interspeech_2025/tripathi25_interspeech.html"
  },
  "uehara25_interspeech": {
   "authors": [
    [
     "Kohei",
     "Uehara"
    ],
    [
     "Ryoichi",
     "Takashima"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ]
   ],
   "title": "Zero-Shot Learning for Acoustic Event Classification Using an Attribute Vector and Conditional GAN",
   "original": "2469",
   "order": 529,
   "page_count": 5,
   "abstract": [
    "This paper presents a zero-shot acoustic event classification (ZS-AEC) method to classify acoustic events for which there is no training data. A previous study proposed a method to classify unseen events by estimating attribute information instead of acoustic event labels, where each acoustic event is associated with attribute information such as sound source material and the pitch (high or low) of the sound. However, this method often leads to the misclassification of unseen acoustic events as seen events. In this paper, we propose a generative-based ZS-AEC method to reduce the bias of prediction toward seen acoustic events. The proposed method generates latent features of unseen acoustic events from their attribute information using a conditional GAN, and a classifier is trained using the generated latent features. Experimental results demonstrated that the proposed method achieved higher classification accuracies than the conventional method based on attribute information."
   ],
   "p1": 2590,
   "pn": 2594,
   "doi": "10.21437/Interspeech.2025-2469",
   "url": "interspeech_2025/uehara25_interspeech.html"
  },
  "cm25_interspeech": {
   "authors": [
    [
     "Vikram",
     "C M"
    ],
    [
     "Sanjoy",
     "Pal"
    ],
    [
     "Nidhi",
     "Mantri"
    ],
    [
     "Gopal Kumar",
     "Agrawal"
    ]
   ],
   "title": "Effect of Loudspeaker Emitted Speech on ASR performance",
   "original": "2470",
   "order": 645,
   "page_count": 4,
   "abstract": [
    "Speech signal played out from the loudspeaker is referred as loudspeaker emitted speech or loud speaker speech. Most of the automatic speech recognition (ASR) systems are trained on the natural speech signals, recorded directly from the human speakers and gives higher word error rate (WER) for the loudspeaker speech. In this paper, first, we analyzed the whisper-medium ASR performance on the loudspeaker emitted speech. Five different equalizer modes, i.e., normal, pop, rock, jazz, and classic along with the distances 0m, 3m, and 5m are considered for the study. Further, based on the spectral differences between natural and loudspeaker speech, an algorithm is proposed to generate the loudspeaker quality speech from natural speech recordings. This algorithm is used to augment the Librispeech data and used to fine-tune the whisper-medium. The fine-tuned ASR on simulated loudspeaker quality speech showed significant improvement when compared to baseline system."
   ],
   "p1": 3170,
   "pn": 3173,
   "doi": "10.21437/Interspeech.2025-2470",
   "url": "interspeech_2025/cm25_interspeech.html"
  },
  "chowdhury25_interspeech": {
   "authors": [
    [
     "Tahiya",
     "Chowdhury"
    ],
    [
     "Veronica",
     "Romero"
    ]
   ],
   "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling",
   "original": "2472",
   "order": 112,
   "page_count": 5,
   "abstract": [
    "Machine learning-based behavioral models rely on features extracted from audio-visual recordings. The recordings are processed using open-source tools to extract speech features for classification models. These tools often lack validation to ensure reliability in capturing behaviorally relevant information. This gap raises concerns about reproducibility and fairness across diverse populations and contexts. Speech processing tools, when used outside of their design context, can fail to capture behavioral variations equitably and can then contribute to bias. We evaluate speech features extracted from two widely used speech analysis tools, OpenSMILE and Praat, to assess their reliability when considering adolescents with autism. We observed considerable variation in features across tools, which influenced model performance across context and demographic groups. We encourage domain-relevant verification to enhance the reliability of machine learning models in clinical applications."
   ],
   "p1": 529,
   "pn": 533,
   "doi": "10.21437/Interspeech.2025-2472",
   "url": "interspeech_2025/chowdhury25_interspeech.html"
  },
  "fu25_interspeech": {
   "authors": [
    [
     "Linya",
     "Fu"
    ],
    [
     "Yu",
     "Liu"
    ],
    [
     "Zhijie",
     "Liu"
    ],
    [
     "Zedong",
     "Yang"
    ],
    [
     "Zhong-Qiu",
     "Wang"
    ],
    [
     "Youfu",
     "Li"
    ],
    [
     "He",
     "Kong"
    ]
   ],
   "title": "AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers",
   "original": "2478",
   "order": 194,
   "page_count": 5,
   "abstract": [
    "We propose AuralNet, a novel 3D multi-source binaural sound source localization approach that localizes overlapping sources in both azimuth and elevation without prior knowledge of the number of sources. AuralNet employs a gated coarse-to-fine architecture, combining a coarse classification stage with a fine-grained regression stage, allowing for flexible spatial resolution through sector partitioning. The model incorporates a multi-head self-attention mechanism to capture spatial cues in binaural signals, enhancing robustness in noisy-reverberant environments. A masked multi-task loss function is designed to jointly optimize sound detection, azimuth, and elevation estimation. Extensive experiments in noisy-reverberant conditions demonstrate the superiority of AuralNet over recent methods."
   ],
   "p1": 938,
   "pn": 942,
   "doi": "10.21437/Interspeech.2025-2478",
   "url": "interspeech_2025/fu25_interspeech.html"
  },
  "gan25b_interspeech": {
   "authors": [
    [
     "Shanhui",
     "Gan"
    ],
    [
     "Zijian",
     "Liang"
    ],
    [
     "Kai",
     "Niu"
    ],
    [
     "Ping",
     "Zhang"
    ]
   ],
   "title": "Synonymity-Based Semantic Coding for Efficient Speech Compression",
   "original": "2483",
   "order": 124,
   "page_count": 5,
   "abstract": [
    "Recent neural speech coding methods optimize the coding rates for perceptual performance in an end-to-end manner. In this paper, we build the relationship between perceptual-oriented compression and the concept of &quot;semantic&quot; compression. We propose a synonymity-based semantic speech coding framework, in which synonymous representations corresponding to the extracted latent features serve as the input of the semantic compression. This framework is designed to approach the compression limits established by recent semantic information theory while preserving perceptual qualities. We provide an implementation of our proposed framework using a K-means algorithm to determine the synonymous representations and a nonlinear transform coding model as the semantic compression method to approach the compression limits. Experimental results show that our method outperforms both traditional and neural speech coding schemes, achieving superior compression efficiency and better perceptual quality."
   ],
   "p1": 589,
   "pn": 593,
   "doi": "10.21437/Interspeech.2025-2483",
   "url": "interspeech_2025/gan25b_interspeech.html"
  },
  "seong25_interspeech": {
   "authors": [
    [
     "Ju-Seok",
     "Seong"
    ],
    [
     "Jeong-Hwan",
     "Choi"
    ],
    [
     "Ye-Rin",
     "Jeoung"
    ],
    [
     "Ilseok",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Enhancing Target-speaker Automatic Speech Recognition Using Multiple Speaker Embedding Extractors with Virtual Speaker Embedding",
   "original": "2486",
   "order": 1000,
   "page_count": 5,
   "abstract": [
    "Target-speaker automatic speech recognition (TS-ASR) utilizes speaker embeddings to identify a target speaker in multi-talker environments. While high-performance speaker embedding extractors provide discriminative embeddings, their computational demands limit practical deployment. In this study, we present two novel methods that effectively utilize lightweight extractors to enhance TS-ASR performance. First, we propose a multiple embeddings modulation that effectively transfers comprehensive speaker information to the ASR module, thereby improving overall performance and robustness against embedding variations. Second, we present a virtual speaker embedding augmentation technique that synthesizes embeddings of unseen speakers, reducing dependence on specific extractors while enhancing independent contributions from each extractor. Experimental results on the Libri2Mix dataset demonstrate that our proposed methods achieve significant WER reductions compared to the baseline model."
   ],
   "p1": 4918,
   "pn": 4922,
   "doi": "10.21437/Interspeech.2025-2486",
   "url": "interspeech_2025/seong25_interspeech.html"
  },
  "pulikodan25_interspeech": {
   "authors": [
    [
     "Sujith",
     "Pulikodan"
    ],
    [
     "Sahapthan",
     "K"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ],
    [
     "Visruth",
     "Sanka"
    ],
    [
     "Nihar",
     "Desai"
    ]
   ],
   "title": "An approach to measuring the performance of Automatic Speech Recognition(ASR) models in the context of Large Language Model(LLM) powered applications",
   "original": "2488",
   "order": 1165,
   "page_count": 5,
   "abstract": [
    "Automatic Speech Recognition (ASR) plays a crucial role in human-machine interaction and serves as an interface for a wide range of applications. Traditionally, ASR performance has been evaluated using Word Error Rate (WER), a metric that quantifies the number of insertions, deletions, and substitutions in the generated transcriptions. However, with the increasing adoption of large and powerful Large Language Models (LLMs) as the core processing component in various applications, the significance of different types of ASR errors in downstream tasks warrants further exploration. In this work, we analyze the capabilities of LLMs to correct errors introduced by ASRs and propose a new measure to evaluate ASR performance for LLM-powered applications."
   ],
   "p1": 5718,
   "pn": 5722,
   "doi": "10.21437/Interspeech.2025-2488",
   "url": "interspeech_2025/pulikodan25_interspeech.html"
  },
  "serrand25_interspeech": {
   "authors": [
    [
     "Coralie",
     "Serrand"
    ],
    [
     "Amira",
     "Morsli"
    ],
    [
     "Gilles",
     "Boulianne"
    ]
   ],
   "title": "CommissionsQC: a Québec French Speech Corpus for Automatic Speech Recognition",
   "original": "2490",
   "order": 800,
   "page_count": 5,
   "abstract": [
    "We introduce CommissionsQC, a novel Québec French speech corpus designed for training and evaluation of Automatic Speech Recognition (ASR) systems. The dataset addresses the underrepresentation of Québec French in existing multilingual speech corpora, which mostly include European French. CommissionsQC includes over 1,000 hours of high-quality, spontaneous Québec French speech derived from two public inquiries commissioned by Québec&#x27;s provincial government. The official pdf transcripts, produced by stenographers, were processed to standardize text and align it with audio recordings. The corpus underwent extensive normalization and filtering to ensure accurate alignment and segmentation. Quality metrics and ASR training results confirm that CommissionsQC is suitable for evaluating or enhancing of speech recognition performance in Québec French."
   ],
   "p1": 3918,
   "pn": 3922,
   "doi": "10.21437/Interspeech.2025-2490",
   "url": "interspeech_2025/serrand25_interspeech.html"
  },
  "ye25b_interspeech": {
   "authors": [
    [
     "Zongli",
     "Ye"
    ],
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Xuanru",
     "Zhou"
    ],
    [
     "Jinming",
     "Zhang"
    ],
    [
     "Haodong",
     "Li"
    ],
    [
     "Shuhe",
     "Li"
    ],
    [
     "Chenxu",
     "Guo"
    ],
    [
     "Anaisha",
     "Das"
    ],
    [
     "Peter",
     "Park"
    ],
    [
     "Zoe",
     "Ezzes"
    ],
    [
     "Jet",
     "Vonk"
    ],
    [
     "Brittany",
     "Morin"
    ],
    [
     "Rian",
     "Bogley"
    ],
    [
     "Lisa",
     "Wauters"
    ],
    [
     "Zachary",
     "Miller"
    ],
    [
     "Maria",
     "Gorno-Tempini"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis",
   "original": "2496",
   "order": 377,
   "page_count": 5,
   "abstract": [
    "Accurate alignment of dysfluent speech with intended text is crucial for automating the diagnosis of neurodegenerative speech disorders. Traditional methods often fail to model phoneme similarities effectively, limiting their performance. In this work, we propose Neural LCS, a novel approach for dysfluent text-text and speech-text alignment. Neural LCS addresses key challenges, including partial alignment and context-aware similarity mapping, by leveraging robust phoneme-level modeling. We evaluate our method on a large-scale simulated dataset, generated using advanced data simulation techniques, and real PPA data. Neural LCS significantly outperforms state-of-the-art models in both alignment accuracy and dysfluent speech segmentation. Our results demonstrate the potential of Neural LCS to enhance automated systems for diagnosing and analyzing speech disorders, offering a more accurate and linguistically grounded solution for dysfluent speech alignment."
   ],
   "p1": 1848,
   "pn": 1852,
   "doi": "10.21437/Interspeech.2025-2496",
   "url": "interspeech_2025/ye25b_interspeech.html"
  },
  "kumar25d_interspeech": {
   "authors": [
    [
     "Chowdam Venkata Thirumala",
     "Kumar"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "SGED-Probe: Probing E2E ASR decoder and aligner for spoken grammar error detection under three speaking practice conditions",
   "original": "2500",
   "order": 491,
   "page_count": 5,
   "abstract": [
    "Grammar error detection in one&#x27;s speech, referred to as spoken grammar error detection (SGED), is a critical component for building computer-assisted language learning systems. Traditionally, ASR followed by text-based GED has been used, but ASR errors can limit SGED performance. In this work, we analyse SGED under three speaking conditions: spontaneous, semispontaneous, and memorized, using five state-of-the-art ASRs. We examine the decoded text and ASR cues such as confidence scores and aligner probabilities, out of which, for the latter one, the ground-truth grammatically correct (GGC) text is used. Experiments revealed that autoregressive ASRs show bias toward GGC text, leading to suboptimal performance. It is observed that there is an increase in performance in response to the decreased freedom of speech, with semi-spontaneous speech outperforming memorized speech. Aligner probabilities outperform confidence scores despite the alignment and overconfidence issues."
   ],
   "p1": 2400,
   "pn": 2404,
   "doi": "10.21437/Interspeech.2025-2500",
   "url": "interspeech_2025/kumar25d_interspeech.html"
  },
  "jiang25c_interspeech": {
   "authors": [
    [
     "Nan",
     "Jiang"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Qing",
     "Gu"
    ],
    [
     "Haoyu",
     "Song"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Ian",
     "McLoughlin"
    ]
   ],
   "title": "Finetune Large Pre-Trained Model Based on Frequency-Wise Multi-Query Attention Pooling for Anomalous Sound Detection",
   "original": "2503",
   "order": 689,
   "page_count": 5,
   "abstract": [
    "Existing anomalous sound detection (ASD) methods generally rely on the learned patterns of normal sounds to identify deviations as anomalies in generative and discriminative ways. However, partly due to the diverse and sparse distribution of anomalies, they may suffer from poor generalization capability to distinguish unseen normal sounds and anomalies. In this paper, we propose to finetune the large pre-trained model based on frequency-wise multi-query attention pooling (FMQAP) for ASD. Specifically, FMQAP first dynamically fuses multiple output embeddings from the pre-trained model, and then exploits a novel frequency-wise attention pooling scheme to improve the generalization capability for various machine types. Evaluations on DCASE 2023 Task2 benchmark demonstrate the superiority of FMQAP, achieving the state-of-the-art 77.62% in terms of official score."
   ],
   "p1": 3389,
   "pn": 3393,
   "doi": "10.21437/Interspeech.2025-2503",
   "url": "interspeech_2025/jiang25c_interspeech.html"
  },
  "durmus25_interspeech": {
   "authors": [
    [
     "Berkin",
     "Durmus"
    ],
    [
     "Blaise",
     "Munyampirwa"
    ],
    [
     "Eduardo",
     "Pacheco"
    ],
    [
     "Atila",
     "Orhon"
    ],
    [
     "Andrey",
     "Leonov"
    ]
   ],
   "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization",
   "original": "2505",
   "order": 326,
   "page_count": 5,
   "abstract": [
    "Even state-of-the-art speaker diarization systems exhibit high variance in error rates across different datasets, representing numerous use cases and domains. Furthermore, comparing across systems requires careful application of best practices such as dataset splits and metric definitions to allow for apples-to-apples comparison. We propose SDBench (Speaker Diarization Benchmark), an open-source benchmark suite that integrates 13 diverse datasets with built-in tooling for consistent and fine-grained analysis of speaker diarization performance for various on-device and server-side systems. SDBench enables reproducible evaluation and easy integration of new systems over time. To demonstrate the efficacy of SDBench, we built SpeakerKit, an inference efficiency-focused system built on top of Pyannote v3. SDBench enabled rapid execution of ablation studies that led to SpeakerKit being 9.6x faster than Pyannote v3 while achieving comparable error rates."
   ],
   "p1": 1598,
   "pn": 1602,
   "doi": "10.21437/Interspeech.2025-2505",
   "url": "interspeech_2025/durmus25_interspeech.html"
  },
  "teleki25_interspeech": {
   "authors": [
    [
     "Maria",
     "Teleki"
    ],
    [
     "Lingfeng",
     "Shi"
    ],
    [
     "Chengkai",
     "Liu"
    ],
    [
     "James",
     "Caverlee"
    ]
   ],
   "title": "I want a horror – comedy – movie: Slips-of-the-Tongue Impact Conversational Recommender System Performance",
   "original": "2509",
   "order": 363,
   "page_count": 5,
   "abstract": [
    "Disfluencies are a characteristic of speech. We focus on the impact of a specific class of disfluency - whole-word speech substitution errors (WSSE) - on LLM-based conversational recommender system performance. We develop Syn-WSSE, a psycholinguistically-grounded framework for synthetically creating genre-based WSSE at varying ratios to study their impact on conversational recommender system performance. We find that LLMs are impacted differently: llama and mixtral have improved performance in the presence of these errors, while gemini, gpt-4o, and gpt-4o-mini have deteriorated performance. We hypothesize that this difference in model resiliency is due to differences in the pre- and post-training methods and data, and that the increased performance is due to the introduced genre diversity. Our findings indicate the importance of a careful choice of LLM for these systems, and more broadly, that disfluencies must be carefully designed for as they can have unforeseen impacts."
   ],
   "p1": 1778,
   "pn": 1782,
   "doi": "10.21437/Interspeech.2025-2509",
   "url": "interspeech_2025/teleki25_interspeech.html"
  },
  "srinivasmenon25_interspeech": {
   "authors": [
    [
     "Aditya",
     "Srinivas Menon"
    ],
    [
     "Raj Prakash",
     "Gohil"
    ],
    [
     "Kumud",
     "Tripathi"
    ],
    [
     "Pankaj",
     "Wasnik"
    ]
   ],
   "title": "LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention",
   "original": "2512",
   "order": 741,
   "page_count": 5,
   "abstract": [
    "Speaker recognition models face challenges in multilingual settings due to the entanglement of linguistic information within speaker embeddings. The overlap between vocal traits such as accent, vocal anatomy, and a language’s phonetic structure complicates separating linguistic and speaker information. Disentangling these components can significantly improve speaker recognition accuracy. To this end, we propose a novel disentanglement learning strategy that integrates joint learning through prefix-tuned cross-attention. This approach is particularly effective when speakers switch between languages. Experimental results show the model generalizes across monolingual and multi-lingual settings, including unseen languages. Notably, the proposed model improves the equal error rate across multiple datasets, highlighting its ability to separate language information from speaker embeddings and enhance recognition in diverse linguistic conditions."
   ],
   "p1": 3623,
   "pn": 3627,
   "doi": "10.21437/Interspeech.2025-2512",
   "url": "interspeech_2025/srinivasmenon25_interspeech.html"
  },
  "shi25h_interspeech": {
   "authors": [
    [
     "Zhonghao",
     "Shi"
    ],
    [
     "Xuan",
     "Shi"
    ],
    [
     "Anfeng",
     "Xu"
    ],
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Harshvardhan",
     "Srivastava"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Maja",
     "Mataric"
    ]
   ],
   "title": "Examining Test-Time Adaptation for Personalized Child Speech Recognition",
   "original": "2513",
   "order": 575,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) models often experience performance degradation due to data domain shifts introduced at test time, a challenge that is further amplified for child speakers. Test-time adaptation (TTA) methods have shown great potential in bridging this domain gap. However, the use of TTA to adapt ASR models to the individual differences in each child’s speech has not yet been systematically studied. In this work, we investigate the effectiveness of two widely used TTA methods-SUTA, SGEM-in adapting off-the-shelf ASR models and their fine-tuned versions for child speech recognition, with the goal of enabling continuous, unsupervised adaptation at test time. Our findings show that TTA significantly improves the performance of both off-the-shelf and fine-tuned ASR models, both on average and across individual child speakers, compared to unadapted baselines. However, while TTA helps adapt to individual variability, it may still be limited with non-linguistic child speech."
   ],
   "p1": 2820,
   "pn": 2824,
   "doi": "10.21437/Interspeech.2025-2513",
   "url": "interspeech_2025/shi25h_interspeech.html"
  },
  "rasendiranr25_interspeech": {
   "authors": [
    [
     "Ezhini",
     "Rasendiran R"
    ],
    [
     "Chandresh Kumar",
     "Maurya"
    ]
   ],
   "title": "Improving Bird Classification with Primary Color Additives",
   "original": "2516",
   "order": 347,
   "page_count": 5,
   "abstract": [
    "We address the problem of classifying bird species using their song recordings, a challenging task due to environmental noise, overlapping vocalizations, and missing labels. Existing models struggle with low-SNR or multi-species recordings. We hypothesize that birds can be classified by visualizing their pitch pattern, speed, and repetition—collectively called motifs. Deep learning models applied to spectrogram images help, but similar motifs across species cause confusion. To mitigate this, we embed frequency information into spectrograms using primary color additives. This enhances species distinction, improving classification accuracy. Our experiments show that the proposed approach achieves statistically significant gains over models without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by 7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results show the effectiveness of incorporating frequency information via colorization."
   ],
   "p1": 1703,
   "pn": 1707,
   "doi": "10.21437/Interspeech.2025-2516",
   "url": "interspeech_2025/rasendiranr25_interspeech.html"
  },
  "mallela25_interspeech": {
   "authors": [
    [
     "Jhansi",
     "Mallela"
    ],
    [
     "Upendra Vishwanath",
     "Y. S."
    ],
    [
     "Sankara Bharadwaj",
     "Rangavajjala"
    ],
    [
     "Bhaskar",
     "Bhatt"
    ],
    [
     "Chiranjeevi",
     "Yarra"
    ]
   ],
   "title": "SupraDoRAL: Automatic Word Prominence Detection Using Suprasegmental Dependencies of Representations with Acoustic and Linguistic Context",
   "original": "2519",
   "order": 1176,
   "page_count": 5,
   "abstract": [
    "Word-level prominence plays a crucial role in spoken language understanding, as it helps to interpret the speaker’s intent. However, automatic word-level prominence detection remains underexplored, particularly in non-native speech, where prominence patterns are more variable due to native language influence. While prominence is determined by acoustic features such as energy, duration, and pitch, capturing statistics on these acoustics at the word level provides only a global representation, missing finer suprasegmental variations. In this study, we consider syllables as the suprasegmental unit and propose a methodology that jointly models syllable prominence variations and their contribution to word prominence using sequential neural networks. This enables learning word-level prominence representations with minimal labeled data. Our method outperforms an unsupervised n-gram-based baseline by 24.01% and a supervised SVM by 5.73%, demonstrating its effectiveness over both approaches."
   ],
   "p1": 5773,
   "pn": 5777,
   "doi": "10.21437/Interspeech.2025-2519",
   "url": "interspeech_2025/mallela25_interspeech.html"
  },
  "bai25_interspeech": {
   "authors": [
    [
     "Qibing",
     "Bai"
    ],
    [
     "Sho",
     "Inoue"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Zhongjie",
     "Jiang"
    ],
    [
     "Yannan",
     "Wang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Accent Normalization Using Self-Supervised Discrete Tokens with Non-Parallel Data",
   "original": "2520",
   "order": 330,
   "page_count": 5,
   "abstract": [
    "Accent normalization converts foreign-accented speech into native-like speech while preserving speaker identity. We propose a novel pipeline using self-supervised discrete tokens and non-parallel training data. The system extracts tokens from source speech, converts them through a dedicated model, and synthesizes the output using flow matching. Our method demonstrates superior performance over a frame-to-frame baseline in naturalness, accentedness reduction, and timbre preservation across multiple English accents. Through token-level phonetic analysis, we validate the effectiveness of our token-based approach. We also develop two duration preservation methods, suitable for applications such as dubbing."
   ],
   "p1": 1618,
   "pn": 1622,
   "doi": "10.21437/Interspeech.2025-2520",
   "url": "interspeech_2025/bai25_interspeech.html"
  },
  "pothula25_interspeech": {
   "authors": [
    [
     "Aishwarya",
     "Pothula"
    ],
    [
     "Bhavana",
     "Akkiraju"
    ],
    [
     "Srihari",
     "Bandarupalli"
    ],
    [
     "Charan",
     "D"
    ],
    [
     "Santosh",
     "Kesiraju"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data",
   "original": "2525",
   "order": 10,
   "page_count": 5,
   "abstract": [
    "The scarcity of high-quality annotated data presents a significant challenge in developing effective end-to-end speech-to-text translation (ST) systems, particularly for low-resource languages. This paper explores the hypothesis that weakly labeled data can be used to build ST models for low-resource language pairs. We constructed speech-to-text translation datasets with the help of bitext mining using state-of-the-art sentence encoders. We mined the multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi, Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data with varying degrees of quality and quantity to investigate the effect of quality versus quantity of weakly labeled data on ST model performance. Results demonstrate that ST systems can be built using weakly labeled data, with performance comparable to massive multi-modal multilingual baselines such as SONAR and SeamlessM4T."
   ],
   "p1": 41,
   "pn": 45,
   "doi": "10.21437/Interspeech.2025-2525",
   "url": "interspeech_2025/pothula25_interspeech.html"
  },
  "baser25_interspeech": {
   "authors": [
    [
     "Oguzhan",
     "Baser"
    ],
    [
     "Ahmet Ege",
     "Tanriverdi"
    ],
    [
     "Kaan",
     "Kale"
    ],
    [
     "Sandeep",
     "Chinchali"
    ],
    [
     "Sriram",
     "Vishwanath"
    ]
   ],
   "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing",
   "original": "2528",
   "order": 991,
   "page_count": 5,
   "abstract": [
    "Speech embeddings often retain sensitive attributes such as speaker identity, accent, or demographic information, posing risks in biased model training and privacy leakage. We propose WavShape, an information-theoretic speech representation learning framework that optimizes embeddings for fairness and privacy while preserving task-relevant information. We leverage mutual information (MI) estimation using the Donsker-Varadhan formulation to guide an MI-based encoder that systematically filters sensitive attributes while maintaining speech content essential for downstream tasks. Experimental results on three known datasets show that WavShape reduces MI between embeddings and sensitive attributes by up to 81% while retaining 97% of task-relevant information. By integrating information theory with self-supervised speech models, this work advances the development of fair, privacy-aware, and resource-efficient speech systems."
   ],
   "p1": 4873,
   "pn": 4877,
   "doi": "10.21437/Interspeech.2025-2528",
   "url": "interspeech_2025/baser25_interspeech.html"
  },
  "hoq25_interspeech": {
   "authors": [
    [
     "Enjamamul",
     "Hoq"
    ],
    [
     "Nikhil",
     "Gupta"
    ],
    [
     "Danielle",
     "Omondi"
    ],
    [
     "Ifeoma",
     "Nwogu"
    ]
   ],
   "title": "FUSE-MOS: Fusion of Speech Embeddings for MOS Prediction with Uncertainty Quantification",
   "original": "2532",
   "order": 481,
   "page_count": 5,
   "abstract": [
    "The rapid advancements in text-to-speech (TTS) and voice conversion (VC) technologies necessitate evaluating the quality of synthesized speech. In this paper, we propose a novel network, FUSE-MOS, which combines the learned latent representations from raw audio waveforms and their corresponding Log-Mel spectrograms, to estimate the posterior distribution of Mean Opinion Score (MOS). Our method thus learns a broader and more nuanced representation of the speech signal. At inference, it predicts MOS value (point estimate) and also provides a measure of uncertainty of that prediction. By leveraging the combined latent representation, FUSE-MOS achieves significant improvements in performance metrics when compared to other existing approaches on benchmark datasets. We also explore an intelligent form of uncertainty filtering strategy to filter out low-confidence (high-uncertainty) samples. It shows FUSE-MOS&#x27;s capability to maintain strong performance even with reduced data."
   ],
   "p1": 2350,
   "pn": 2354,
   "doi": "10.21437/Interspeech.2025-2532",
   "url": "interspeech_2025/hoq25_interspeech.html"
  },
  "gupta25b_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Gupta"
    ],
    [
     "MLNS",
     "Karthik"
    ],
    [
     "Chelamkuri",
     "Omsrinath"
    ]
   ],
   "title": "Sub-band based Adaptive IIR Algorithm with Biquad Filter Stability Constraints for Feedforward Hear-Through Equalization",
   "original": "2533",
   "order": 635,
   "page_count": 5,
   "abstract": [
    "Hear through (HT) filter is designed to provide a listening experience similar to open ear listening. The key challenge in the design of the HT filter is to ensure low processing delay to closely match the open ear response. Existing adaptive HT approaches such as adaptive finite impulse response (FIR) and neural network-based approaches can lead to higher processing delays. This paper proposes a sub-band based infinite impulse response (IIR) adaptive feedforward equalization method to improve the HT and convergence performance for sound sources incident from different directions. To ensure IIR filter stability, we propose inclusion of stability constraint based on least mean square fourth criterion. The proposed method improves performance over existing methods up to 13 dB for simulated outdoor and indoor scenarios, while maintaining stability and similar complexity as existing adaptive IIR and FIR techniques in simulated complex indoor and outdoor real-world acoustic environments."
   ],
   "p1": 3120,
   "pn": 3124,
   "doi": "10.21437/Interspeech.2025-2533",
   "url": "interspeech_2025/gupta25b_interspeech.html"
  },
  "park25g_interspeech": {
   "authors": [
    [
     "Yeseul",
     "Park"
    ],
    [
     "Bowon",
     "Lee"
    ]
   ],
   "title": "Fine-tuning Strategies for Automatic Speech Recognition of Low-Resource Speech with Autism Spectrum Disorder",
   "original": "2535",
   "order": 379,
   "page_count": 5,
   "abstract": [
    "Individuals with autism spectrum disorder (ASD) exhibit unique speech patterns that challenge conventional automatic speech recognition (ASR) systems. However, research on ASD-adapted ASR models remains limited. This study explores fine-tuning strategies for ASD-specific ASR models using Whisper, comparing full fine-tuning, selective fine-tuning, adapter tuning, and LoRA-based fine-tuning. Experiments using a small-scale Korean ASD speech dataset demonstrate that adapter tuning and LoRA significantly reduce the character error rate (CER) while reducing trainable parameters. In case of Whisper-small, adapter tuning and LoRA improve the CER by 7.22% and 10.14% over full fine-tuning, respectively. Furthermore, LoRA improved CER by 10.35% and 10.15% with Whisper-base and Whisper-large-v2 models compared to full fine-tuning. These results demonstrate the adaptation efficiency and effectiveness of LoRA for low resource ASD speech dataset."
   ],
   "p1": 1858,
   "pn": 1862,
   "doi": "10.21437/Interspeech.2025-2535",
   "url": "interspeech_2025/park25g_interspeech.html"
  },
  "jung25c_interspeech": {
   "authors": [
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Soumi",
     "Maiti"
    ],
    [
     "Yihan",
     "Wu"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Yuta",
     "Matsunaga"
    ],
    [
     "Seyun",
     "Um"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Hye-jin",
     "Shim"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Joon Son",
     "Chung"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "The Text-to-speech in the Wild (TITW) Database",
   "original": "2536",
   "order": 976,
   "page_count": 5,
   "abstract": [
    "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings. Recently, an effort known as &quot;noisy-TTS training&quot; has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8. Beyond TTS, TITW’s unique design, leveraging a automatic speaker recognition dataset, strengthens ethical efforts to counteract malicious use of TTS models by supporting tasks such as speech deepfake detection."
   ],
   "p1": 4798,
   "pn": 4802,
   "doi": "10.21437/Interspeech.2025-2536",
   "url": "interspeech_2025/jung25c_interspeech.html"
  },
  "zhong25d_interspeech": {
   "authors": [
    [
     "Wenjie",
     "Zhong"
    ],
    [
     "Jason",
     "Naradowsky"
    ],
    [
     "Yusuke",
     "Miyao"
    ]
   ],
   "title": "A Simple-Yet-Effective Data Augmentation Method for Speaker Identification in Novels",
   "original": "2545",
   "order": 1172,
   "page_count": 5,
   "abstract": [
    "Speaker identification in novels is crucial for speech synthesis systems to assign appropriate voices in audiobook production. It attributes a speaker to an utterance through context analysis. Traditional approaches heavily rely on human-annotated datasets, which are costly and scarce, limiting model performance. To overcome this, we propose a simple-yet-effective data augmentation method using large language models (LLMs) to generate synthetic dialogues and post-process the dialogues into augmented training instances. Our experiments show that this method achieves a state-of-the-art accuracy of 82.6%, surpassing the previous baseline by 2.4%. Performance gains are especially notable in the Implicit (hard) category, where our method exceeds the previous baseline by 3.5%. Our analysis suggests that it enhances the ability to capture long-term dependencies and there is a mutually reinforce effect between the Implicit and Anaphoric (middle) categories."
   ],
   "p1": 5753,
   "pn": 5757,
   "doi": "10.21437/Interspeech.2025-2545",
   "url": "interspeech_2025/zhong25d_interspeech.html"
  },
  "yang25p_interspeech": {
   "authors": [
    [
     "Hongli",
     "Yang"
    ],
    [
     "Yizhou",
     "Peng"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Sheng",
     "Li"
    ]
   ],
   "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning",
   "original": "2549",
   "order": 1061,
   "page_count": 5,
   "abstract": [
    "Large-scale multilingual ASR models like Whisper excel in high-resource settings but face challenges in low-resource scenarios, such as rare languages and code-switching (CS), due to computational costs and catastrophic forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method to enhance CS ASR while preserving prior knowledge. We evaluate two strategies: 1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model, demonstrating improved cross-lingual capabilities compared to traditional methods, and 2) adhering to SPT’s original design by freezing model parameters and only training soft prompts. Additionally, we introduce SPT4ASR, a combination of different SPT variants. Experiments on the SEAME and ASRU2019 datasets show that deep prompt tuning is the most effective SPT approach, and our SPT4ASR methods achieve  further error reductions in CS ASR, maintaining parameter efficiency similar to LoRA, without degrading performance on existing languages."
   ],
   "p1": 5203,
   "pn": 5207,
   "doi": "10.21437/Interspeech.2025-2549",
   "url": "interspeech_2025/yang25p_interspeech.html"
  },
  "huang25j_interspeech": {
   "authors": [
    [
     "Jingya",
     "Huang"
    ],
    [
     "Aashish N.",
     "Patel"
    ],
    [
     "Sowmya Manojna",
     "Narasimha"
    ],
    [
     "Gal",
     "Mishne"
    ],
    [
     "Vikash",
     "Gilja"
    ]
   ],
   "title": "Word-Level Error Analysis in Decoding Systems: From Speech Recognition to Brain-Computer Interfaces",
   "original": "2550",
   "order": 1134,
   "page_count": 5,
   "abstract": [
    "Brain-to-text (BTT) systems that decode attempted speech from neural activity have achieved 4.2% word error rate (WER). These systems demonstrate potential for daily use similar to automatic speech recognition (ASR) systems, enabling communication for individuals with profound speech loss. To examine fine-grained error patterns in both BTT and ASR, we introduce a refined alignment algorithm to detect word edits, along with four word-level metrics to assess exact correctness and semantic distance of these edits. Analyzing errors by word frequency reveals a significant performance disparity among frequent, infrequent, and rare words across all models. Although transformer-based architectures and self-supervised pre-training achieve lower error rates, the gap between frequent and infrequent words remains substantial. Our analysis indicates that misclassifying infrequent words incurs higher semantic costs, suggesting that addressing this word-level performance gap could enhance overall system usability across ASR and BTT. Our implementation is available."
   ],
   "p1": 5563,
   "pn": 5567,
   "doi": "10.21437/Interspeech.2025-2550",
   "url": "interspeech_2025/huang25j_interspeech.html"
  },
  "lewis25_interspeech": {
   "authors": [
    [
     "Robert",
     "Lewis"
    ],
    [
     "Szymon",
     "Fedor"
    ],
    [
     "Nelson",
     "Hidalgo Julia"
    ],
    [
     "Joshua",
     "Curtiss"
    ],
    [
     "Jiyeon",
     "Kim"
    ],
    [
     "Noah",
     "Jones"
    ],
    [
     "David",
     "Mischoulon"
    ],
    [
     "Thomas F",
     "Quatieri"
    ],
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Paola",
     "Pedrelli"
    ],
    [
     "Rosalind",
     "Picard"
    ]
   ],
   "title": "Towards the Objective Characterisation of Major Depressive Disorder Using Speech Data from a 12-week Observational Study with Daily Measurements",
   "original": "2556",
   "order": 105,
   "page_count": 5,
   "abstract": [
    "Our analysis focuses on identifying relations between properties of the voice and depression symptom severity. On a novel corpus of 3,374 longitudinal speech recordings from 71 patients clinically diagnosed with major depressive disorder (MDD), we use a statistical modelling approach to identify associations between depression symptom severity and 38 acoustic and cognitive features. Significant negative associations with daily within-individual fluctuations of depression include speaking rate and articulation rate. Furthermore, when analysing how the changes in speech-derived features covary over time with the change in depression severity, we find that the standard deviation of the pitch has a significant negative association, as well as the speaking and articulation rate. We also discover that several performance metrics derived from the cognitive tasks (digit-span and Stroop) have significant associations with fluctuations or changes in depression symptom severity."
   ],
   "p1": 494,
   "pn": 498,
   "doi": "10.21437/Interspeech.2025-2556",
   "url": "interspeech_2025/lewis25_interspeech.html"
  },
  "hidalgojulia25_interspeech": {
   "authors": [
    [
     "Nelson",
     "Hidalgo Julia"
    ],
    [
     "Robert",
     "Lewis"
    ],
    [
     "Craig",
     "Ferguson"
    ],
    [
     "Simon",
     "Goldberg"
    ],
    [
     "Wendy",
     "Lau"
    ],
    [
     "Caroline",
     "Swords"
    ],
    [
     "Gabriela",
     "Valdivia"
    ],
    [
     "Christine",
     "Wilson-Mendenhall"
    ],
    [
     "Raquel",
     "Tartar"
    ],
    [
     "Rosalind",
     "Picard"
    ],
    [
     "Richard",
     "Davidson"
    ]
   ],
   "title": "Identifying Vocal and Facial Biomarkers of Depression in Large-Scale Remote Recordings: A Multimodal Study Using Mixed-Effects Modeling",
   "original": "2560",
   "order": 1073,
   "page_count": 5,
   "abstract": [
    "We examine vocal and facial data from a new study with n=954 depressed participants, each characterized by six time points of the eight-item Patient Health Questionnaire survey (PHQ-8). Patients interacted with a smartphone app over four weeks, with a 3-month follow-up. The app&#x27;s animated character asked participants to describe, for 90 seconds, an emotional experience from the past 24 hours. We obtained 4,875 audio-video recordings, and applied linear mixed-effects models to examine associations between depression severity and 30 acoustic, linguistic and facial action unit features. Significant associations were found with speech timing and prosody, voice quality, linguistic sentiment, the use of self-referential pronouns, and facial action units related to smiling. We also show that these features allow accurate estimation of depression severity in multimodal mixed-effects machine learning models."
   ],
   "p1": 5263,
   "pn": 5267,
   "doi": "10.21437/Interspeech.2025-2560",
   "url": "interspeech_2025/hidalgojulia25_interspeech.html"
  },
  "ohashi25_interspeech": {
   "authors": [
    [
     "Atsumoto",
     "Ohashi"
    ],
    [
     "Shinya",
     "Iizuka"
    ],
    [
     "Jingjing",
     "Jiang"
    ],
    [
     "Ryuichiro",
     "Higashinaka"
    ]
   ],
   "title": "Towards a Japanese Full-duplex Spoken Dialogue System",
   "original": "2564",
   "order": 364,
   "page_count": 5,
   "abstract": [
    "Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model&#x27;s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness."
   ],
   "p1": 1783,
   "pn": 1787,
   "doi": "10.21437/Interspeech.2025-2564",
   "url": "interspeech_2025/ohashi25_interspeech.html"
  },
  "zhou25i_interspeech": {
   "authors": [
    [
     "ZhaoHui",
     "Zhou"
    ],
    [
     "Hui",
     "Luo"
    ]
   ],
   "title": "Cross-corpus open-set Speech Emotion Recognition Method Based on Spatiotemporal Features with Inverse-Entropy Regularization",
   "original": "2571",
   "order": 915,
   "page_count": 5,
   "abstract": [
    "We propose a method to address the performance degradation due to the distribution shift and unknown emotion categories in cross-corpus open-set speech emotion recognition. The method combines spatiotemporal feature extraction and inverse-entropy regularization. First, the long-range spatiotemporal dependencies are extracted from emotional audio sequences using a deep fusion network. To further align distributions from the source and target corpora, the MMD regularization is applied to minimize the distance between their joint distributions. Moreover, we propose an inverse-entropy regularization to learn the discriminative information used to reject known classes, which can balance the classification confidence of samples from the known or unknown categories in the open-set setting, allowing the model to predict unknown classes while preventing over-prediction. Experimental results show that our method outperforms baseline models across four cross-corpus speech emotion datasets."
   ],
   "p1": 4493,
   "pn": 4497,
   "doi": "10.21437/Interspeech.2025-2571",
   "url": "interspeech_2025/zhou25i_interspeech.html"
  },
  "sakuma25_interspeech": {
   "authors": [
    [
     "Asahi",
     "Sakuma"
    ],
    [
     "Hiroaki",
     "Sato"
    ],
    [
     "Ryuga",
     "Sugano"
    ],
    [
     "Tadashi",
     "Kumano"
    ],
    [
     "Yoshihiko",
     "Kawai"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ]
   ],
   "title": "Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition",
   "original": "2572",
   "order": 1122,
   "page_count": 5,
   "abstract": [
    "This paper presents a novel framework for multi-talker automatic speech recognition without the need for auxiliary information. Serialized Output Training (SOT), a widely used approach, suffers from recognition errors due to speaker assignment failures. Although incorporating auxiliary information, such as token-level timestamps, can improve recognition accuracy, extracting such information from natural conversational speech remains challenging. To address this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension of CTC that jointly assigns a token and its corresponding speaker label to each frame. We further integrate SD-CTC into the SOT framework, enabling the SOT model to learn speaker distinction using only overlapping speech and transcriptions. Experimental comparisons show that multi-task learning with SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves performance comparable to state-of-the-art methods relying on auxiliary information."
   ],
   "p1": 5503,
   "pn": 5507,
   "doi": "10.21437/Interspeech.2025-2572",
   "url": "interspeech_2025/sakuma25_interspeech.html"
  },
  "lodagala25_interspeech": {
   "authors": [
    [
     "Vasista Sai",
     "Lodagala"
    ],
    [
     "Lamya",
     "Alkanhal"
    ],
    [
     "Daniel",
     "Izham"
    ],
    [
     "Shivam",
     "Mehta"
    ],
    [
     "Shammur Absar",
     "Chowdhury"
    ],
    [
     "Aqeelah",
     "Makki"
    ],
    [
     "Hamdy S.",
     "Hussein"
    ],
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "Ahmed",
     "Ali"
    ]
   ],
   "title": "SawtArabi: A Benchmark Corpus for Arabic TTS.  Standard, Dialectal and Code-Switching",
   "original": "2573",
   "order": 975,
   "page_count": 5,
   "abstract": [
    "Curating Text-to-Speech (TTS) datasets is a strenuous task given the quality considerations. While it is hard to find high-quality TTS datasets in languages other than English, it is rare to come across code-switching (CS) datasets. As a part of this work, we curate a 4-hour Arabic-English TTS corpus consisting of code-switched Egyptian-English, monolingual Modern Standard Arabic (MSA), Egyptian, and English, all recorded by the same voice talent. We demonstrate the importance of vowelization and the need for better phonemization of Arabic text. To this effect, we present the modified espeak-ng phonemizer that handles various irregularities of espeak-ng over Arabic text. Upon training baseline TTS systems over this benchmark, we demonstrate its efficacy through extensive subjective evaluations."
   ],
   "p1": 4793,
   "pn": 4797,
   "doi": "10.21437/Interspeech.2025-2573",
   "url": "interspeech_2025/lodagala25_interspeech.html"
  },
  "patakis25_interspeech": {
   "authors": [
    [
     "Andreas",
     "Patakis"
    ],
    [
     "Vassilis",
     "Lyberatos"
    ],
    [
     "Spyridon",
     "Kantarelis"
    ],
    [
     "Edmund",
     "Dervakos"
    ],
    [
     "Giorgos",
     "Stamou"
    ]
   ],
   "title": "Semantic-Aware Interpretable Multimodal Music Auto-Tagging",
   "original": "2574",
   "order": 49,
   "page_count": 5,
   "abstract": [
    "Music auto-tagging is essential for organizing and discovering music in extensive digital libraries. While foundation models achieve exceptional performance in this domain, their outputs often lack interpretability, limiting trust and usability for researchers and end-users alike. In this work, we present an interpretable framework for music auto-tagging that leverages groups of musically meaningful multimodal features, derived from signal processing, deep learning, ontology engineering, and natural language processing. To enhance interpretability, we cluster features semantically and employ an expectation maximization algorithm, assigning distinct weights to each group based on its contribution to the tagging process. Our method achieves competitive tagging performance while offering a deeper understanding of the decision-making process, paving the way for more transparent and user-centric music tagging systems."
   ],
   "p1": 236,
   "pn": 240,
   "doi": "10.21437/Interspeech.2025-2574",
   "url": "interspeech_2025/patakis25_interspeech.html"
  },
  "rangappa25_interspeech": {
   "authors": [
    [
     "Pradeep",
     "Rangappa"
    ],
    [
     "Andrés",
     "Carofilis"
    ],
    [
     "Jeena",
     "Prakash"
    ],
    [
     "Shashi",
     "Kumar"
    ],
    [
     "Sergio",
     "Burdisso"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "Esaú",
     "Villatoro-Tello"
    ],
    [
     "Bidisha",
     "Sharma"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Shankar",
     "Venkatesan"
    ],
    [
     "Saurabh",
     "Vyas"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering",
   "original": "2580",
   "order": 1002,
   "page_count": 5,
   "abstract": [
    "Fine-tuning pretrained ASR models for specific domains is challenging for small organizations with limited labeled data and computational resources. Here we explore different data selection pipelines and propose a robust approach that improves ASR adaptation by filtering pseudo-labels generated using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach integrates multiple selection strategies-including word error rate (WER) prediction, named entity recognition (NER), and character error rate (CER) analysis-to extract high-quality training segments. We evaluate our method on Whisper and Zipformer using a 7500-hour baseline, comparing it to a CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on 7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our filtering reduces the dataset to 100 hours (1.4%) with similar performance; a similar trend is observed on Fisher English."
   ],
   "p1": 4928,
   "pn": 4932,
   "doi": "10.21437/Interspeech.2025-2580",
   "url": "interspeech_2025/rangappa25_interspeech.html"
  },
  "kim25s_interspeech": {
   "authors": [
    [
     "Yunsik",
     "Kim"
    ],
    [
     "Yoonyoung",
     "Chung"
    ]
   ],
   "title": "Modality-Specific Speech Enhancement and Noise-Adaptive Fusion for Acoustic and Body-Conduction Microphone Framework",
   "original": "2581",
   "order": 783,
   "page_count": 5,
   "abstract": [
    "Body-conduction microphone signals (BMS) bypass airborne sound, providing strong noise resistance. However, a complementary modality is required to compensate for the inherent loss of high-frequency information. In this study, we propose a novel multi-modal framework that combines BMS and acoustic microphone signals (AMS) to achieve both noise suppression and high-frequency reconstruction. Unlike conventional multi-modal approaches that simply merge features, our method employs two specialized networks: a mapping-based model to enhance BMS and a masking-based model to denoise AMS. These networks are integrated through a dynamic fusion mechanism that adapts to local noise conditions, ensuring the optimal use of each modality’s strengths. We performed evaluations on the TAPS dataset, augmented with DNS-2023 noise clips, using objective speech quality metrics. The results clearly demonstrate that our approach outperforms single-modal solutions in a wide range of noisy environments."
   ],
   "p1": 3833,
   "pn": 3837,
   "doi": "10.21437/Interspeech.2025-2581",
   "url": "interspeech_2025/kim25s_interspeech.html"
  },
  "oh25c_interspeech": {
   "authors": [
    [
     "Changhan",
     "Oh"
    ],
    [
     "Kiyoung",
     "Park"
    ],
    [
     "Jeomja",
     "Kang"
    ],
    [
     "Woo Yong",
     "Choi"
    ],
    [
     "Hwa Jeon",
     "Song"
    ]
   ],
   "title": "Improving Cross-Attention based on Positional Alignment during Inference for Robust Long-form Speech Recognition",
   "original": "2582",
   "order": 677,
   "page_count": 5,
   "abstract": [
    "End-to-end (E2E) models have significantly advanced automatic speech recognition (ASR), with hybrid architectures that combine Connectionist Temporal Classification (CTC) and attention-based encoder-decoder (AED) models demonstrating superior performance. However, AED architectures, particularly Conformer, face notable challenges with long-form speech, with performance degradation becoming evident for audio exceeding 25 second. In this study, we propose improving the Conformer’s robustness for long-form ASR by applying Gaussian masking to the cross-attention mechanism of the Transformer decoder during inference, using the aligned positions obtained from the CTC prefix score. The proposed method achieves an error reduction rate (ERR) of 88.41% (from 26.41% to 3.06%) for audio longer than 20 seconds on a LibriSpeech evaluation set constructed by concatenating three utterances. Moreover, the method remains effective with either a Transformer or an E-Branchformer encoder."
   ],
   "p1": 3329,
   "pn": 3333,
   "doi": "10.21437/Interspeech.2025-2582",
   "url": "interspeech_2025/oh25c_interspeech.html"
  },
  "baser25b_interspeech": {
   "authors": [
    [
     "Oguzhan",
     "Baser"
    ],
    [
     "Ahmet Ege",
     "Tanriverdi"
    ],
    [
     "Sriram",
     "Vishwanath"
    ],
    [
     "Sandeep",
     "Chinchali"
    ]
   ],
   "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection",
   "original": "2583",
   "order": 1088,
   "page_count": 5,
   "abstract": [
    "Deepfake (DF) attacks pose a growing threat as generative models become increasingly advanced. However, our study reveals that existing DF datasets fail to deceive human perception, unlike real DF attacks that influence public discourse. It highlights the need for more realistic DF attack vectors. We introduce PhonemeFake (PF), a DF attack that manipulates critical speech segments using language reasoning, significantly reducing human perception by up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF dataset on HuggingFace and open-source bilevel DF segment detection model that adaptively prioritizes compute on manipulated regions. Our extensive experiments across three known DF datasets reveal that our detection model reduces EER by 91% while achieving up to 90% speed-up, with minimal compute overhead and precise localization beyond existing models as a scalable solution."
   ],
   "p1": 5333,
   "pn": 5337,
   "doi": "10.21437/Interspeech.2025-2583",
   "url": "interspeech_2025/baser25b_interspeech.html"
  },
  "kim25t_interspeech": {
   "authors": [
    [
     "Nam-Gyu",
     "Kim"
    ],
    [
     "Deok-Hyeon",
     "Cho"
    ],
    [
     "Seung-Bin",
     "Kim"
    ],
    [
     "Seong-Whan",
     "Lee"
    ]
   ],
   "title": "Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech",
   "original": "2586",
   "order": 892,
   "page_count": 5,
   "abstract": [
    "Recent advances in expressive text-to-speech (TTS) have introduced diverse methods based on style embedding extracted from reference speech. However, synthesizing high-quality expressive speech remains challenging. We propose Spotlight-TTS, which exclusively emphasizes style via voiced-aware style extraction and style direction adjustment. Voiced-aware style extraction focuses on voiced regions highly related to style while maintaining continuity across different speech regions to improve expressiveness. We adjust the direction of the extracted style for optimal integration into the TTS model, which improves speech quality. Experimental results demonstrate that Spotlight-TTS achieves superior performance compared to baseline models in terms of expressiveness, overall speech quality, and style transfer capability. Our audio samples are publicly available."
   ],
   "p1": 4378,
   "pn": 4382,
   "doi": "10.21437/Interspeech.2025-2586",
   "url": "interspeech_2025/kim25t_interspeech.html"
  },
  "kim25u_interspeech": {
   "authors": [
    [
     "Jongsuk",
     "Kim"
    ],
    [
     "Jaemyung",
     "Yu"
    ],
    [
     "Minchan",
     "Kwon"
    ],
    [
     "Junmo",
     "Kim"
    ]
   ],
   "title": "FairASR: Fair Audio Contrastive Learning for Automatic Speech Recognition",
   "original": "2590",
   "order": 792,
   "page_count": 5,
   "abstract": [
    "Large-scale ASR models have achieved remarkable gains in accuracy and robustness. However, fairness issues remain largely unaddressed despite their critical importance in real-world applications.  In this work, we introduce FairASR, a system that mitigates demographic bias by learning representations that are uninformative of group membership, enabling fair generalization across demographic groups. Leveraging a multi-demographic dataset, our approach employs a gradient reversal layer to suppress demographic-discriminative features while maintaining the ability to capture generalizable speech patterns through an unsupervised contrastive loss. Experimental results show that FairASR delivers competitive overall ASR performance while significantly reducing performance disparities across different demographic groups."
   ],
   "p1": 3878,
   "pn": 3882,
   "doi": "10.21437/Interspeech.2025-2590",
   "url": "interspeech_2025/kim25u_interspeech.html"
  },
  "shahidi25_interspeech": {
   "authors": [
    [
     "Lidea",
     "Shahidi"
    ],
    [
     "Erdem Baha",
     "Topbas"
    ],
    [
     "Thu Ngan",
     "Dang"
    ],
    [
     "Tobias",
     "Goehring"
    ]
   ],
   "title": "Harnessing Text-to-Speech Voice Cloning Models for Improved Audiological Speech Assessment",
   "original": "2595",
   "order": 445,
   "page_count": 5,
   "abstract": [
    "Conventional audiological speech assessments are limited in their predictive utility, due to the small number of available stimuli and the restricted communications experiences that they represent. To enhance the capabilities of audiological speech assessments, this work evaluates the ability of several text-to-speech voice cloning models on the task of replicating a standard UK open-set speech test used clinically. Models are evaluated using complementary measures of speech perception: speech intelligibility in background noise, speech quality, and speaker discrimination, in a large-scale online study (N = 73). To ensure speech intelligibility measurements are comparable, the psychometric functions are characterized for each model. Results indicate models which accurately and consistently replicate speaker characteristics and produce speech that is similarly intelligible and natural for audiological speech assessment."
   ],
   "p1": 2170,
   "pn": 2174,
   "doi": "10.21437/Interspeech.2025-2595",
   "url": "interspeech_2025/shahidi25_interspeech.html"
  },
  "wang25aa_interspeech": {
   "authors": [
    [
     "Ning",
     "Wang"
    ],
    [
     "Bingyang",
     "Wen"
    ],
    [
     "Minghui",
     "Wu"
    ],
    [
     "Yang",
     "Sun"
    ],
    [
     "Zongru",
     "Shao"
    ],
    [
     "Haojie",
     "Zhou"
    ],
    [
     "K.P.",
     "Subbalakshmi"
    ]
   ],
   "title": "Decoding Alzheimer’s: Interpretable Visual and Logical Attention in Picture Description Tasks",
   "original": "2596",
   "order": 416,
   "page_count": 5,
   "abstract": [
    "Recent studies have started to incorporate imagery information from picture-description tasks in clinical interviews to automate Alzheimer’s disease detection in the elderly. However, the high-level logical flow of visual-attention cognition mechanisms has not yet been investigated for enhanced interpretability. In this study, we systematically analyze the elements of picture-description tasks and propose a set of top-to-bottom human-interpretable features to describe the cognitive behaviors of patients, focusing on visual attention patterns, description quality, and repetition characteristics. These features achieve 85% accuracy in AD detection without specialized equipment, offering valuable insights for clinical practices and non-expert caregivers. Our results demonstrate that these high-level descriptive features, particularly those related to visual attention and the logical flow of speech, serve as effective biomarkers for AD detection."
   ],
   "p1": 2043,
   "pn": 2047,
   "doi": "10.21437/Interspeech.2025-2596",
   "url": "interspeech_2025/wang25aa_interspeech.html"
  },
  "sharma25b_interspeech": {
   "authors": [
    [
     "Chetan",
     "Sharma"
    ],
    [
     "Vaishnavi",
     "Chandwanshi"
    ],
    [
     "Shreya Shrikant",
     "Karkun"
    ],
    [
     "Aditya Anand",
     "Gupta"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "A real-time MRI study on asymmetry in velum dynamics during VCV production with nasal sounds",
   "original": "2597",
   "order": 218,
   "page_count": 5,
   "abstract": [
    "Velum movement controls airflow through nasal passage enabling production of nasal sound. The asymmetry in velum dynamics during velum lowering vs raising is not well understood, although several studies on asymmetry of other articulators exist. In this study of asymmetry in velum dynamics, we use real-time MRI videos of 68 speakers speaking symmetric vowel-consonant-vowel (VCV) sequence with V being vowels (/a/, /i/, /u/) and C being nasals (/m/, /n/). The asymmetry is analyzed in terms of the extent and speed of velum movement. The study reveals that the extent of velum displacement is significantly higher (by a factor of ~2) during V-C transition (velum lowering) compared to that during C-V transition (velum raising) for all six vowel and nasal combinations chosen. It is also found that the speed with which velum lowers is higher (~1.5 times) than that of velum raising during production of all VCVs except /ana/ and /unu/, for which no significant difference is observed."
   ],
   "p1": 1058,
   "pn": 1062,
   "doi": "10.21437/Interspeech.2025-2597",
   "url": "interspeech_2025/sharma25b_interspeech.html"
  },
  "carofilis25_interspeech": {
   "authors": [
    [
     "Andrés",
     "Carofilis"
    ],
    [
     "Pradeep",
     "Rangappa"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "Shashi",
     "Kumar"
    ],
    [
     "Sergio",
     "Burdisso"
    ],
    [
     "Jeena",
     "Prakash"
    ],
    [
     "Esaú",
     "Villatoro-Tello"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Bidisha",
     "Sharma"
    ],
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Shankar",
     "Venkatesan"
    ],
    [
     "Saurabh",
     "Vyas"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering",
   "original": "2601",
   "order": 740,
   "page_count": 5,
   "abstract": [
    "Fine-tuning pretrained ASR models for specific domains is challenging when labeled data is scarce. But unlabeled audio and labeled data from related domains are often available. We propose an incremental semi-supervised learning pipeline that first integrates a small in-domain labeled set and an auxiliary dataset from a closely related domain, achieving a relative improvement of 4% over no auxiliary data. Filtering based on multi-model consensus or named entity recognition (NER) is then applied to select and iteratively refine pseudo-labels, showing slower performance saturation compared to random selection. Evaluated on the multi-domain Wow call center and Fisher English corpora, it outperforms single-step fine-tuning. Consensus-based filtering outperforms other methods, providing up to 22.3% relative improvement on Wow and 24.8% on Fisher over single-step fine-tuning with random selection. NER is the second-best filter, providing competitive performance at a lower computational cost."
   ],
   "p1": 3618,
   "pn": 3622,
   "doi": "10.21437/Interspeech.2025-2601",
   "url": "interspeech_2025/carofilis25_interspeech.html"
  },
  "kuparinen25_interspeech": {
   "authors": [
    [
     "Olli",
     "Kuparinen"
    ]
   ],
   "title": "Automatic Dialectal Transcription: An Evaluation on Finnish and Norwegian",
   "original": "2602",
   "order": 489,
   "page_count": 5,
   "abstract": [
    "The fields of dialectology and sociolinguistics are highly reliant on phonetically transcribed spoken language data, which are often written in language-specific styles and alphabets. Transcribing dialectal speech phonetically is time-consuming and thus a major bottleneck for data collection in variational linguistics. Meanwhile, the field of automatic speech recognition (ASR) has taken leaps forward in recent years. In this work, we introduce automatic dialectal transcription as a distinct ASR task and investigate solutions using data from two unrelated languages (Finnish and Norwegian) with two levels of transcription precision. We find a large performance gap between the languages, as Finnish models are much more efficient than those for Norwegian. We further evaluate the character-level errors of both languages and dialectal difficulties of the best Finnish model. We find that the most geographically central dialects tend to be easier to transcribe than the more distant ones."
   ],
   "p1": 2390,
   "pn": 2394,
   "doi": "10.21437/Interspeech.2025-2602",
   "url": "interspeech_2025/kuparinen25_interspeech.html"
  },
  "koudounas25c_interspeech": {
   "authors": [
    [
     "Alkis",
     "Koudounas"
    ],
    [
     "Claudio",
     "Savelli"
    ],
    [
     "Flavio",
     "Giobergia"
    ],
    [
     "Elena",
     "Baralis"
    ]
   ],
   "title": "``Alexa, can you forget me?'' Machine Unlearning Benchmark in Spoken Language Understanding",
   "original": "2607",
   "order": 361,
   "page_count": 5,
   "abstract": [
    "Machine unlearning, the process of efficiently removing specific information from machine learning models, is a growing area of interest for responsible AI. However, few studies have explored the effectiveness of unlearning methods on complex tasks, particularly speech-related ones. This paper introduces UnSLU-BENCH, the first benchmark for machine unlearning in spoken language understanding (SLU), focusing on four datasets spanning four languages. We address the unlearning of data from specific speakers as a way to evaluate the quality of potential &quot;right to be forgotten&quot; requests. We assess eight unlearning techniques and propose a novel metric to simultaneously better capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation for unlearning in SLU and reveals significant differences in the effectiveness and computational feasibility of various techniques."
   ],
   "p1": 1768,
   "pn": 1772,
   "doi": "10.21437/Interspeech.2025-2607",
   "url": "interspeech_2025/koudounas25c_interspeech.html"
  },
  "khurana25_interspeech": {
   "authors": [
    [
     "Sameer",
     "Khurana"
    ],
    [
     "Dominik",
     "Klement"
    ],
    [
     "Antoine",
     "Laurent"
    ],
    [
     "Dominik",
     "Boboš"
    ],
    [
     "Juraj",
     "Novosad"
    ],
    [
     "Peter",
     "Gazdik"
    ],
    [
     "Ellen",
     "Zhang"
    ],
    [
     "Zili",
     "Huang"
    ],
    [
     "Amir",
     "Hussein"
    ],
    [
     "Ricard",
     "Marxer"
    ],
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "Ryo",
     "Aihara"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "François G.",
     "Germain"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization",
   "original": "2612",
   "order": 715,
   "page_count": 5,
   "abstract": [
    "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels—acoustic, phonetic, and lexical—within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC’s factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC’s potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks."
   ],
   "p1": 3514,
   "pn": 3518,
   "doi": "10.21437/Interspeech.2025-2612",
   "url": "interspeech_2025/khurana25_interspeech.html"
  },
  "carta25_interspeech": {
   "authors": [
    [
     "Salvatore",
     "Carta"
    ],
    [
     "Alessandro",
     "Giuliani"
    ],
    [
     "Marco Manolo",
     "Manca"
    ],
    [
     "Mirko",
     "Marras"
    ],
    [
     "Leonardo",
     "Piano"
    ]
   ],
   "title": "SardinianVoxes: A Speech Recognition Dataset for the Sardinian Languages",
   "original": "2615",
   "order": 395,
   "page_count": 5,
   "abstract": [
    "Low-resource languages inherently lack the extensive data typically required to train effective intelligent systems, leading to a substantial gap in technological support. In this paper, we address this challenge by focusing on the language varieties spoken in Sardinia, which are marked by limited and fragmented linguistic resources. First, we present the design and implementation of a reproducible pipeline that led to the preparation of SardinianVoxes, an audio-text dataset comprising approximately 170 hours of transcribed speech, carefully annotated to reflect the internal linguistic diversity of Sardinian. Second, we introduce a tailored evaluation protocol and benchmarking suite aimed at assessing the performance of state-of-the-art speech-to-text models, and their fine-tuned counterparts, across Sardinian varieties. We expect that our contributions will support the development of intelligent speech technologies for Sardinian."
   ],
   "p1": 1938,
   "pn": 1942,
   "doi": "10.21437/Interspeech.2025-2615",
   "url": "interspeech_2025/carta25_interspeech.html"
  },
  "das25b_interspeech": {
   "authors": [
    [
     "Shoutrik",
     "Das"
    ],
    [
     "Nishant",
     "Singh"
    ],
    [
     "Arjun",
     "Gangwar"
    ],
    [
     "S",
     "Umesh"
    ]
   ],
   "title": "Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching",
   "original": "2617",
   "order": 431,
   "page_count": 5,
   "abstract": [
    "Dysarthria is a neurological disorder that significantly impairs speech intelligibility, often rendering affected individuals unable to communicate effectively. This necessitates the development of robust dysarthric-to-regular speech conversion techniques. In this work, we investigate the utility and limitations of self-supervised learning (SSL) features and their quantized representations as an alternative to mel-spectrograms for speech generation. Additionally, we explore methods to mitigate speaker variability by generating clean speech in a single-speaker voice using features extracted from WavLM. To this end, we propose a fully non-autoregressive approach that leverages Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct mapping from dysarthric to clean speech. Our findings highlight the effectiveness of discrete acoustic units in improving intelligibility while achieving faster convergence compared to traditional mel-spectrogram-based approaches."
   ],
   "p1": 2118,
   "pn": 2122,
   "doi": "10.21437/Interspeech.2025-2617",
   "url": "interspeech_2025/das25b_interspeech.html"
  },
  "buker25_interspeech": {
   "authors": [
    [
     "Aykut",
     "Büker"
    ],
    [
     "Oğuzhan",
     "Kurnaz"
    ],
    [
     "Şule",
     "Bekiryazıcı"
    ],
    [
     "Selim Can",
     "Demirtaş"
    ],
    [
     "Cemal",
     "Hanilçi"
    ]
   ],
   "title": "Evaluating Parameter Sharing for Spoofing-Aware Speaker Verification: A Case Study on the ASVspoof 5 Dataset",
   "original": "2618",
   "order": 931,
   "page_count": 5,
   "abstract": [
    "Spoofing-aware speaker verification (SASV) is an important but challenging task and has been a primary focus of the recently organized ASVspoof 5 challenge. As SASV integrates automatic speaker verification (ASV) and countermeasure (CM) systems, its performance depends on the effectiveness of each system. This study systematically examines the impact of different parameter-sharing (PS) strategies, which facilitate joint optimization, on SASV performance using the ASVspoof 5 dataset. Experimental results indicate that PS enhances performance for specific attack types and codec conditions. For example, the baseline system achieves a min a-DCF of 0.329 on the A26 attack, which improves to 0.233 with PS. Similarly, for AMR-compressed signals, PS yields a 14.09% performance gain. These observations show that PS techniques are effective in mitigating certain spoofing attacks and improving robustness to degraded audio conditions in SASV systems."
   ],
   "p1": 4573,
   "pn": 4577,
   "doi": "10.21437/Interspeech.2025-2618",
   "url": "interspeech_2025/buker25_interspeech.html"
  },
  "sudo25b_interspeech": {
   "authors": [
    [
     "Yui",
     "Sudo"
    ],
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Atsushi",
     "Kojima"
    ],
    [
     "Tomoya",
     "Mizumoto"
    ],
    [
     "Lianbo",
     "Liu"
    ]
   ],
   "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary",
   "original": "2621",
   "order": 1058,
   "page_count": 5,
   "abstract": [
    "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models (OWSM), are trained on massive datasets to achieve accurate automatic speech recognition. However, even SFMs struggle to accurately recognize rare and unseen words. While contextual biasing (CB) is a promising approach to improve recognition of such words, most CB methods are trained from scratch, resulting in lower performance than SFMs due to the lack of pre-trained knowledge. This paper integrates an existing CB method with OWSM v3.1 while freezing its pre-trained parameters. By leveraging the knowledge embedded in SFMs, the proposed method enables effective CB while preserving the advantages of SFMs, even with a small dataset. Experimental results show that the proposed method improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9 point improvement in the overall WER while reducing the real-time factor by 7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean set."
   ],
   "p1": 5188,
   "pn": 5192,
   "doi": "10.21437/Interspeech.2025-2621",
   "url": "interspeech_2025/sudo25b_interspeech.html"
  },
  "li25ca_interspeech": {
   "authors": [
    [
     "Zhaolin",
     "Li"
    ],
    [
     "Jan",
     "Niehues"
    ]
   ],
   "title": "In-context Language Learning for Endangered Languages in Speech Recognition",
   "original": "2626",
   "order": 154,
   "page_count": 5,
   "abstract": [
    "With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs."
   ],
   "p1": 738,
   "pn": 742,
   "doi": "10.21437/Interspeech.2025-2626",
   "url": "interspeech_2025/li25ca_interspeech.html"
  },
  "martinek25_interspeech": {
   "authors": [
    [
     "Alicja",
     "Martinek"
    ],
    [
     "Joanna",
     "Gajewska"
    ],
    [
     "Ewelina",
     "Bartuzi-Trokielewicz"
    ]
   ],
   "title": "Do you read me? - flow of speech effect on speaker recognition systems",
   "original": "2629",
   "order": 745,
   "page_count": 5,
   "abstract": [
    "Comparing two types of speech – read and spontaneous – poses a significant challenge for speaker verification models. This study examines the impact of these differences on the performance of advanced biometric systems. We conducted tests using two baseline speaker verification models and two spoof-aware approaches to assess their ability to handle variations between read and spontaneous speech. Additionally, we generated synthetic speech using two state-of-the-art Text-to-Speech methods, training the models either on spontaneous or read speech. The results indicate that mixing spontaneous and read speech compared with uniform type of speech yields higher error rates in biometric verification. The situation is similar in both testing scenarios, comparing genuines with impostors and genuines with synthetic speech, regardless of the type of speaker recognition model — base or spoof-aware."
   ],
   "p1": 3643,
   "pn": 3647,
   "doi": "10.21437/Interspeech.2025-2629",
   "url": "interspeech_2025/martinek25_interspeech.html"
  },
  "santamariajorda25_interspeech": {
   "authors": [
    [
     "Jaume",
     "Santamaría-Jordà"
    ],
    [
     "Pablo",
     "Segovia-Martínez"
    ],
    [
     "Gonçal V.",
     "Garcés Díaz-Munío"
    ],
    [
     "Joan Albert",
     "Silvestre-Cerdà"
    ],
    [
     "Adrià",
     "Giménez"
    ],
    [
     "Rubén",
     "Gaspar Aparicio"
    ],
    [
     "René",
     "Fernández Sánchez"
    ],
    [
     "Jorge",
     "Civera"
    ],
    [
     "Albert",
     "Sanchis"
    ],
    [
     "Alfons",
     "Juan"
    ]
   ],
   "title": "LHCP-ASR: An English Speech Corpus of High-Energy Particle Physics Talks for Narrow-Domain ASR Benchmarking",
   "original": "2630",
   "order": 823,
   "page_count": 5,
   "abstract": [
    "We present LHCP-ASR, an English speech corpus of high-energy particle physics talks, with 235 hours of transcribed speeches extracted from the 2020-2022 Large Hadron Collider Physics (LHCP) conferences, plus 1.5G tokens of in-domain text extracted from scientific documents. About 30 hours of conference talks were manually transcribed to build two reliable tasks for narrow-domain ASR benchmarking. The remaining conference talks (205 hours) were pseudo-labelled using a very competitive in-domain ASR system, in order to build a dataset for training or adaptation purposes. This paper describes the creation of this dataset, and provides first reference WER% figures using OpenAI&#x27;s Whisper models and our in-domain ASR system, achieving 13.6% and 15.0% WER points on the two test sets. This corpus is publicly released under an open licence. We believe it will fulfil the need in the area of having new open, reliable, real-life and challenging ASR benchmarks."
   ],
   "p1": 4033,
   "pn": 4037,
   "doi": "10.21437/Interspeech.2025-2630",
   "url": "interspeech_2025/santamariajorda25_interspeech.html"
  },
  "singh25c_interspeech": {
   "authors": [
    [
     "Akanksha",
     "Singh"
    ],
    [
     "Yi-Ping Phoebe",
     "Chen"
    ],
    [
     "Vipul",
     "Arora"
    ]
   ],
   "title": "H-QuEST: Accelerating Query-by-Example Spoken Term Detection with Hierarchical Indexing",
   "original": "2631",
   "order": 538,
   "page_count": 5,
   "abstract": [
    "Query-by-example spoken term detection (QbE-STD) searches for matching words or phrases in an audio dataset using a sample spoken query. When annotated data is limited or unavailable, QbE-STD is often done using template matching methods like dynamic time warping (DTW), which are computationally expensive and can’t scale well. To address this, we propose H-QuEST (Hierarchical Query-by-Example Spoken Term Detection), a novel framework that accelerates spoken term retrieval by utilizing Term Frequency and Inverse Document Frequency (TF-IDF)-based sparse representations obtained through advanced audio representation learning techniques and Hierarchical Navigable Small World (HNSW) indexing with further refinement to perform search. Experimental results show that H-QuEST delivers substantial improvements in retrieval speed, without sacrificing accuracy compared to existing methods."
   ],
   "p1": 2635,
   "pn": 2639,
   "doi": "10.21437/Interspeech.2025-2631",
   "url": "interspeech_2025/singh25c_interspeech.html"
  },
  "chatzichristodoulou25_interspeech": {
   "authors": [
    [
     "Georgios",
     "Chatzichristodoulou"
    ],
    [
     "Despoina",
     "Kosmopoulou"
    ],
    [
     "Antonios",
     "Kritikos"
    ],
    [
     "Anastasia",
     "Poulopoulou"
    ],
    [
     "Efthymios",
     "Georgiou"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Vassilis",
     "Katsouros"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Medusa: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions",
   "original": "2636",
   "order": 953,
   "page_count": 5,
   "abstract": [
    "SER is a challenging task due to the subjective nature of human emotions and their uneven representation under naturalistic conditions. We propose MEDUSA, a multimodal framework with a four-stage training pipeline, which effectively handles class imbalance and emotion ambiguity. The first two stages train an ensemble of classifiers that utilize DeepSER, a novel extension of a deep cross-modal transformer fusion mechanism from pretrained self-supervised acoustic and linguistic representations. Manifold MixUp is employed for further regularization. The last two stages optimize a trainable meta-classifier that combines the ensemble predictions. Our training approach incorporates human annotation scores as soft targets, coupled with balanced data sampling and multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic Conditions Challenge."
   ],
   "p1": 4683,
   "pn": 4687,
   "doi": "10.21437/Interspeech.2025-2636",
   "url": "interspeech_2025/chatzichristodoulou25_interspeech.html"
  },
  "song25d_interspeech": {
   "authors": [
    [
     "Yonghun",
     "Song"
    ],
    [
     "Yeeun",
     "Kim"
    ],
    [
     "Yoonyoung",
     "Chung"
    ]
   ],
   "title": "Lightweight Speech Enhancement Model Based on Harmonic Attention and Phase Estimation with Skin-Attachable Accelerometer",
   "original": "2642",
   "order": 15,
   "page_count": 5,
   "abstract": [
    "Skin-attachable accelerometers (ACCs) capture speech vibrations through the skin, providing a noise-robust complement to microphone (MIC) signals. However, prior multi-modal models combining these signals face trade-offs between processing overhead and performance. This study proposes a lightweight ACC-assisted U-Net (LAU-Net) for real-time speech enhancement. The LAU-Net employs a harmonic attention module to enhance spectral clarity by emphasizing speech harmonics predicted from ACC signals while only increasing the parameter count from 92.29k to 92.98k. The phase estimation block of LAU-Net adaptively integrates ACC and MIC phases based on noise levels, eliminating the need for phase data training. The LAU-Net achieves a PESQ of 2.92 with 39M MACs/s on the TAPS dataset, demonstrating a balance between speech quality and computational efficiency. These results highlight the LAU-Net as a practical solution for robust and efficient speech processing with real-time edge deployment."
   ],
   "p1": 66,
   "pn": 70,
   "doi": "10.21437/Interspeech.2025-2642",
   "url": "interspeech_2025/song25d_interspeech.html"
  },
  "horii25_interspeech": {
   "authors": [
    [
     "Koharu",
     "Horii"
    ],
    [
     "Naohiro",
     "Tawara"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Shoko",
     "Araki"
    ]
   ],
   "title": "Why is children's ASR so difficult? Analyzing children's phonological error patterns using SSL-based phoneme recognizers",
   "original": "2645",
   "order": 585,
   "page_count": 5,
   "abstract": [
    "Children&#x27;s automatic speech recognition (child ASR) is generally more challenging than adult ASR, as children&#x27;s speech differs from that of adults and continuously evolves with a child&#x27;s development. To investigate the causes of this difficulty and leverage the findings to improve the child ASR performance in the future, this study examines phoneme recognition error patterns using self-supervised learning (SSL)-based phoneme recognizers (PRs). First, we apply several SSL-based child PRs to English children&#x27;s speech from kindergarten to grade 10, confirming that the WavLM-based model performs best. Then, using this model, we examine whether misrecognitions are primarily due to child mispronunciations or model errors. Furthermore, analyzing the results obtained by the WavLM-based model, we investigate how error patterns change along with age. The results show that mispronunciations persist longer than previously reported, particularly in certain manners of articulation."
   ],
   "p1": 2870,
   "pn": 2874,
   "doi": "10.21437/Interspeech.2025-2645",
   "url": "interspeech_2025/horii25_interspeech.html"
  },
  "vesterbacka25_interspeech": {
   "authors": [
    [
     "Leonora",
     "Vesterbacka"
    ],
    [
     "Faton",
     "Rekathati"
    ],
    [
     "Robin",
     "Kurtz"
    ],
    [
     "Justyna",
     "Sikora"
    ],
    [
     "Agnes",
     "Toftgård"
    ]
   ],
   "title": "Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition",
   "original": "2646",
   "order": 158,
   "page_count": 5,
   "abstract": [
    "This work presents a suite of fine-tuned Whisper models for Swedish, trained on a dataset of unprecedented size and variability for this mid-resourced language. As languages of smaller sizes are often underrepresented in multilingual training datasets, substantial improvements in performance can be achieved by fine-tuning existing multilingual models, as shown in this work. This work reports an overall improvement across model sizes compared to OpenAI&#x27;s Whisper evaluated on Swedish. Most notably, we report an average 47% reduction in WER comparing our best performing model to OpenAI&#x27;s Whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST."
   ],
   "p1": 758,
   "pn": 762,
   "doi": "10.21437/Interspeech.2025-2646",
   "url": "interspeech_2025/vesterbacka25_interspeech.html"
  },
  "huang25k_interspeech": {
   "authors": [
    [
     "Shangkun",
     "Huang"
    ],
    [
     "Yuxuan",
     "Du"
    ],
    [
     "Jingwen",
     "Yang"
    ],
    [
     "Dejun",
     "Zhang"
    ],
    [
     "Xupeng",
     "Jia"
    ],
    [
     "Jing",
     "Deng"
    ],
    [
     "Jintao",
     "Kang"
    ],
    [
     "Rong",
     "Zheng"
    ]
   ],
   "title": "Overlap-Adaptive Hybrid Speaker Diarization and ASR-Aware Observation Addition for MISP 2025 Challenge",
   "original": "2648",
   "order": 389,
   "page_count": 5,
   "abstract": [
    "This paper presents the system developed to address the MISP 2025 Challenge. For the diarization system, we proposed a hybrid approach combining a WavLM end-to-end segmentation method with a traditional multi-module clustering technique to adaptively select the appropriate model for handling varying degrees of overlapping speech. For the automatic speech recognition (ASR) system, we proposed an ASR-aware observation addition method that compensates for the performance limitations of Guided Source Separation (GSS) under low signal-to-noise ratio conditions. Finally, we integrated the speaker diarization and ASR systems in a cascaded architecture to address Track 3. Our system achieved character error rates (CER) of 9.48% on Track 2 and concatenated minimum permutation character error rate (cpCER) of 11.56% on Track 3, ultimately securing first place in both tracks and thereby demonstrating the effectiveness of the proposed methods in real-world meeting scenarios."
   ],
   "p1": 1908,
   "pn": 1912,
   "doi": "10.21437/Interspeech.2025-2648",
   "url": "interspeech_2025/huang25k_interspeech.html"
  },
  "zhang25u_interspeech": {
   "authors": [
    [
     "Jinming",
     "Zhang"
    ],
    [
     "Xuanru",
     "Zhou"
    ],
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Shuhe",
     "Li"
    ],
    [
     "William",
     "Li"
    ],
    [
     "Zoe",
     "Ezzes"
    ],
    [
     "Rian",
     "Bogley"
    ],
    [
     "Lisa",
     "Wauters"
    ],
    [
     "Zachary",
     "Miller"
    ],
    [
     "Jet",
     "Vonk"
    ],
    [
     "Brittany",
     "Morin"
    ],
    [
     "Maria",
     "Gorno-Tempini"
    ],
    [
     "Gopala",
     "Anumanchipalli"
    ]
   ],
   "title": "Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection",
   "original": "2658",
   "order": 378,
   "page_count": 5,
   "abstract": [
    "Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys - the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance."
   ],
   "p1": 1853,
   "pn": 1857,
   "doi": "10.21437/Interspeech.2025-2658",
   "url": "interspeech_2025/zhang25u_interspeech.html"
  },
  "ranjan25_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Ranjan"
    ],
    [
     "Kishan",
     "Pipariya"
    ],
    [
     "Mayank",
     "Vatsa"
    ],
    [
     "Richa",
     "Singh"
    ]
   ],
   "title": "SynHate: Detecting Hate Speech in Synthetic Deepfake Audio",
   "original": "2659",
   "order": 1146,
   "page_count": 5,
   "abstract": [
    "The rise of deepfake audio and hate speech, powered by advanced text-to-speech, threatens online safety. We present SynHate, the first multilingual dataset for detecting hate speech in synthetic audio, spanning 37 languages. SynHate uses a novel four-class scheme: Real-normal, Real-hate, Fake-normal, and Fake-hate. Built from MuTox and ADIMA datasets, it captures diverse hate speech patterns globally and in India. We evaluate five leading self-supervised models (Whisper-small/medium, XLS-R, AST, mHuBERT), finding notable performance differences by language, with Whisper-small performing best overall. Cross-dataset generalization remains a challenge. By releasing SynHate and baseline code, we aim to advance robust, culturally sensitive, and multilingual solutions against synthetic hate speech. The dataset is available."
   ],
   "p1": 5623,
   "pn": 5627,
   "doi": "10.21437/Interspeech.2025-2659",
   "url": "interspeech_2025/ranjan25_interspeech.html"
  },
  "elmers25_interspeech": {
   "authors": [
    [
     "Mikey",
     "Elmers"
    ],
    [
     "Koji",
     "Inoue"
    ],
    [
     "Divesh",
     "Lala"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems",
   "original": "2660",
   "order": 614,
   "page_count": 5,
   "abstract": [
    "Turn-taking is a fundamental component of spoken dialogue, however conventional studies mostly involve dyadic settings. This work focuses on applying voice activity projection (VAP) to predict upcoming turn-taking in triadic multi-party scenarios. The goal of VAP models is to predict the future voice activity for each speaker utilizing only acoustic data. This is the first study to extend VAP into triadic conversation. We trained multiple models on a Japanese triadic dataset where participants discussed a variety of topics. We found that the VAP trained on triadic conversation outperformed the baseline for all models but that the type of conversation affected the accuracy. This study establishes that VAP can be used for turn-taking in triadic dialogue scenarios. Future work will incorporate this triadic VAP turn-taking model into spoken dialogue systems."
   ],
   "p1": 3015,
   "pn": 3019,
   "doi": "10.21437/Interspeech.2025-2660",
   "url": "interspeech_2025/elmers25_interspeech.html"
  },
  "li25da_interspeech": {
   "authors": [
    [
     "Shaole",
     "Li"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Jiangyu",
     "Han"
    ],
    [
     "Ke",
     "Zhang"
    ],
    [
     "Wupeng",
     "Wang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "REAL-T: Real Conversational Mixtures for Target Speaker Extraction",
   "original": "2662",
   "order": 392,
   "page_count": 5,
   "abstract": [
    "Current target speaker extraction (TSE) systems achieve remarkable performance on synthetic datasets like LibriMix and WSJMix. However, their effectiveness in real conversational scenarios, where the cocktail party problem is most prevalent, remains largely unexplored. In this paper, we conduct a comprehensive analysis of several speaker diarization datasets and introduce REAL-T, the first conversation-centric dataset specifically designed for TSE in real-world conditions. Our evaluations reveal significant performance degradation of existing TSE models on this dataset, highlighting the unaddressed complexity of real-world speech extraction. To facilitate controlled benchmarking, we define two subsets: BASE and PRIMARY, ensuring more manageable yet challenging evaluation settings."
   ],
   "p1": 1923,
   "pn": 1927,
   "doi": "10.21437/Interspeech.2025-2662",
   "url": "interspeech_2025/li25da_interspeech.html"
  },
  "kim25v_interspeech": {
   "authors": [
    [
     "Hyun-Soo",
     "Kim"
    ],
    [
     "Da-Hee",
     "Yang"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Spatially Weighted Contrastive Learning for Robust Sound Source Localization",
   "original": "2666",
   "order": 509,
   "page_count": 5,
   "abstract": [
    "We propose a spatially weighted contrastive loss (SWeC loss) for sound source localization in real-world scenarios using multi-channel speech data. In multi-channel localization, phase differences between microphone channels provide critical cues for estimating the azimuth angle of incoming speech. To effectively extract azimuth information, we leverage contrastive learning and introduce a novel loss function that incorporates spatial relationships between azimuth classes. Specifically, our loss assigns weights to negative pairs based on their angular distance, penalizing high similarity between embeddings corresponding to distant angles. Furthermore, we propose a contrastive data generation method tailored to multi-channel localization, enhancing the effectiveness of contrastive learning. Experimental results demonstrate that the proposed loss function and data generation strategy significantly improve localization performance."
   ],
   "p1": 2490,
   "pn": 2494,
   "doi": "10.21437/Interspeech.2025-2666",
   "url": "interspeech_2025/kim25v_interspeech.html"
  },
  "ranjan25b_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Ranjan"
    ],
    [
     "Likhith",
     "Ayinala"
    ],
    [
     "Mayank",
     "Vatsa"
    ],
    [
     "Richa",
     "Singh"
    ]
   ],
   "title": "Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages",
   "original": "2668",
   "order": 342,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel multimodal framework for hate speech detection in deepfake audio, excelling even in zero-shot scenarios. Unlike previous approaches, our method uses contrastive learning to jointly align audio and text representations across languages. We present the first benchmark dataset with 127,290 paired text and synthesized speech samples in six languages: English and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil, Telugu). Our model learns a shared semantic embedding space, enabling robust cross-lingual and cross-modal classification. Experiments on two multilingual test sets show our approach outperforms baselines, achieving accuracies of 0.819 and 0.701, and generalizes well to unseen languages. This demonstrates the advantage of combining modalities for hate speech detection in synthetic media, especially in low-resource settings where unimodal models falter. The Dataset is available."
   ],
   "p1": 1678,
   "pn": 1682,
   "doi": "10.21437/Interspeech.2025-2668",
   "url": "interspeech_2025/ranjan25b_interspeech.html"
  },
  "mittal25_interspeech": {
   "authors": [
    [
     "Ashish",
     "Mittal"
    ],
    [
     "Darshan",
     "Prabhu"
    ],
    [
     "Sunita",
     "Sarawagi"
    ],
    [
     "Preethi",
     "Jyothi"
    ]
   ],
   "title": "Skip-Salsa: Skip Synchronous Fusion of ASR LLM Decoders",
   "original": "2669",
   "order": 137,
   "page_count": 5,
   "abstract": [
    "The integration of large language models (LLMs) with ASR is increasingly explored, but remains challenging for low-resource languages. Loose coupling via N-best lists fails due to high ASR errors, while tight coupling using audio tokens requires too much data. A promising middle ground SALSA enables synchronous decoding by cascading ASR and LLM decoders via projection layers, overcoming differing tokenizations. In this work, we show that SALSA fails when the ASR and LLM tokenizations have a large token fertility gap. This problem particularly plagues low-resource languages; the ASR decoder overtokenizes LLM tokens starving the LLM decoder of sufficient audio context. To address this, we propose SKIP-SALSA, that adaptively skips ahead and advances the ASR decoder states to synchronize with the LLM. The skip size is learned via a lightweight skip predictor. SKIP-SALSA significantly improves ASR performance on multiple low-resource languages yielding up to 20% over a strong baseline."
   ],
   "p1": 654,
   "pn": 658,
   "doi": "10.21437/Interspeech.2025-2669",
   "url": "interspeech_2025/mittal25_interspeech.html"
  },
  "mun25c_interspeech": {
   "authors": [
    [
     "Seongkyu",
     "Mun"
    ],
    [
     "Jubum",
     "Han"
    ]
   ],
   "title": "Boundary-Conscious Pruning: Hard Set-Aware Model Compression for Efficient Speaker Recognition",
   "original": "2675",
   "order": 753,
   "page_count": 5,
   "abstract": [
    "While modern CNN-based speaker recognition systems have achieved impressive accuracy, their computational and memory demands continue to pose challenges for on-device deployments, such as in smart home devices and consumer electronics. We propose an iterative channel pruning approach that is Hard set-aware, ensuring that channels essential for discriminating near-boundary cases are preserved. Our framework employs an angular medoid algorithm to dynamically partition samples into a hard set and a normal set. Channel importance is computed by integrating gradient-based measures, SE attention, and embedding perturbation, with the contributions of these metrics experimentally optimized. Coupled with iterative fine-tuning and pruning ratio adjustments, our method efficiently reduces model parameters while maintaining robust performance on critical hard set samples, thereby mitigating the severe degradation typically observed in conventional one-shot pruning approaches."
   ],
   "p1": 3683,
   "pn": 3687,
   "doi": "10.21437/Interspeech.2025-2675",
   "url": "interspeech_2025/mun25c_interspeech.html"
  },
  "gorthi25_interspeech": {
   "authors": [
    [
     "Nidheesh",
     "Gorthi"
    ],
    [
     "Kartik",
     "Thakral"
    ],
    [
     "Rishabh",
     "Ranjan"
    ],
    [
     "Richa",
     "Singh"
    ],
    [
     "Mayank",
     "Vatsa"
    ]
   ],
   "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security",
   "original": "2677",
   "order": 1153,
   "page_count": 5,
   "abstract": [
    "Biometric authentication systems are increasingly being deployed in critical applications, but they remain susceptible to spoofing. Since most of the research efforts focus on modality-specific anti-spoofing techniques, building a unified, resource-efficient solution across multiple biometric modalities remains a challenge. To address this, we propose LitMAS, a Lightweight and generalizable Multi-modal Anti-Spoofing framework designed to detect spoofing attacks in speech, face, iris, and fingerprint-based biometric systems. At the core of LitMAS is a Modality-Aligned Concentration Loss, which enhances interclass separability while preserving cross-modal consistency and enabling robust spoof detection across diverse biometric traits. With just 6M parameters, LitMAS surpasses state-of-the-art methods by 1.36% in average EER across seven datasets, demonstrating high efficiency, strong generalizability, and suitability for edge deployment. Code and trained models are available."
   ],
   "p1": 5658,
   "pn": 5662,
   "doi": "10.21437/Interspeech.2025-2677",
   "url": "interspeech_2025/gorthi25_interspeech.html"
  },
  "sanchez25_interspeech": {
   "authors": [
    [
     "Ariadna",
     "Sanchez"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Can We Reconstruct a Dysarthric Voice with the Large Speech Model Parler TTS?",
   "original": "2679",
   "order": 844,
   "page_count": 5,
   "abstract": [
    "Speech disorders can make communication hard or even impossible for those who develop them. Personalised Text-to-Speech is an attractive option as a communication aid. We attempt voice reconstruction using a large speech model, with which we generate an approximation of a dysarthric speaker&#x27;s voice prior to the onset of their condition. In particular, we investigate whether a state-of-the-art large speech model, Parler TTS, can generate intelligible speech while maintaining speaker identity. We curate a dataset and annotate it with relevant speaker and intelligibility information, and use this to fine-tune the model. Our results show that the model can indeed learn to generate from the distribution of this challenging data, but struggles to control intelligibility and to maintain consistent speaker identity. We propose future directions to improve controllability of this class of model, for the voice reconstruction task."
   ],
   "p1": 4138,
   "pn": 4142,
   "doi": "10.21437/Interspeech.2025-2679",
   "url": "interspeech_2025/sanchez25_interspeech.html"
  },
  "gulzar25_interspeech": {
   "authors": [
    [
     "Haris",
     "Gulzar"
    ],
    [
     "Monikka Roslianna",
     "Busto"
    ],
    [
     "Akiko",
     "Masaki"
    ],
    [
     "Takeharu",
     "Eda"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Leveraging LLMs for Written to Spoken Style Data Transformation to Enhance Spoken Dialog State Tracking",
   "original": "2681",
   "order": 356,
   "page_count": 5,
   "abstract": [
    "Dialog State Tracking (DST) is an important part of Task-Oriented Dialog (TOD) systems, as it needs to navigate the complex human conversational flow to accomplish a task. Most TOD systems are trained on written-style text data, and their performance plunges when deployed in spoken scenarios due to natural disfluencies and human-speech recognition errors. Labeled spoken-style TOD data is limited because of the high data collection cost and privacy concerns. As Large Language Models (LLMs) emerge as a tool for synthetic text data generation, we explored their capability to generate spoken-style text-based TOD data. Through meticulously crafting LLM prompts, our generated labeled spoken style TOD data improved the absolute Joint Goal Accuracy (JGA) by 3.39% and relative JGA by 11.6%, for dedicated DST models. In this work, we showcase our divide-and-conquer-based data generation strategies and DST training to improve the performance of task-specific dialog models."
   ],
   "p1": 1743,
   "pn": 1747,
   "doi": "10.21437/Interspeech.2025-2681",
   "url": "interspeech_2025/gulzar25_interspeech.html"
  },
  "wardah25_interspeech": {
   "authors": [
    [
     "Wafaa",
     "Wardah"
    ],
    [
     "Robert P.",
     "Spang"
    ],
    [
     "Vincent",
     "Barriac"
    ],
    [
     "Jan",
     "Reimes"
    ],
    [
     "Anna",
     "Llagostera"
    ],
    [
     "Jens",
     "Berger"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "SQ-AST: A Transformer-Based Model for Speech Quality Prediction",
   "original": "2683",
   "order": 478,
   "page_count": 5,
   "abstract": [
    "We present SQ-AST, a transformer-based model for non-intrusive speech quality prediction. The model predicts overall speech quality and four perceptual dimensions—noisiness, discontinuity, coloration, and loudness—using only the degraded signal. SQ-AST leverages Audio Spectrogram Transformers (AST), pretrained on large-scale audio datasets and fine-tuned on diverse speech quality corpora. It operates on short speech clips (4–12 seconds) without requiring a reference signal. Training was conducted on 106 databases comprising 165,791 samples. Independent evaluations confirm strong generalization to real-world conditions. The model is currently under consideration for ITU-T standardization, highlighting its potential for benchmarking, quality assessment, and industry adoption."
   ],
   "p1": 2335,
   "pn": 2339,
   "doi": "10.21437/Interspeech.2025-2683",
   "url": "interspeech_2025/wardah25_interspeech.html"
  },
  "wang25ba_interspeech": {
   "authors": [
    [
     "Kaidi",
     "Wang"
    ],
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Ziyue",
     "Jiang"
    ],
    [
     "Hukai",
     "Huang"
    ],
    [
     "Peijie",
     "Chen"
    ],
    [
     "Weijie",
     "Wu"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
   "original": "2684",
   "order": 283,
   "page_count": 5,
   "abstract": [
    "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker&#x27;s voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech."
   ],
   "p1": 1383,
   "pn": 1387,
   "doi": "10.21437/Interspeech.2025-2684",
   "url": "interspeech_2025/wang25ba_interspeech.html"
  },
  "yong25b_interspeech": {
   "authors": [
    [
     "Vi Jun Sean",
     "Yong"
    ],
    [
     "Serkan",
     "Kumyol"
    ],
    [
     "Pau Le Lisa",
     "Low"
    ],
    [
     "Suk Wai Winnie",
     "Leung"
    ],
    [
     "Tristan",
     "Braud"
    ]
   ],
   "title": "HK-GenSpeech: A Generative AI Scene Creation Framework for Speech Based Cognitive Assessment",
   "original": "2685",
   "order": 113,
   "page_count": 5,
   "abstract": [
    "Current methods of automated speech-based cognitive assessment often rely on fixed-picture descriptions in major languages, limiting repeatability, engagement, and locality. This paper introduces HK-GenSpeech (HKGS), a framework using generative AI to create pictures that present similar features to those used in cognitive assessment, augmented with descriptors reflecting the local context. We demonstrate HKGS through a dataset of 423 Cantonese speech samples collected in Hong Kong from 141 participants, with HK-MoCA scores ranging from 11 to 30. Each participant described the cookie theft picture, an HKGS fixed image, and an HKGS dynamic image. Regression experiments show comparable accuracy for all image types, indicating HKGS&#x27; adequacy in generating relevant assessment images. Lexical analysis further suggests that HKGS images elicit richer speech. By mitigating learning effects and improving engagement, HKGS supports broader data collection, particularly in low-resource settings."
   ],
   "p1": 534,
   "pn": 538,
   "doi": "10.21437/Interspeech.2025-2685",
   "url": "interspeech_2025/yong25b_interspeech.html"
  },
  "agrawal25b_interspeech": {
   "authors": [
    [
     "Jatin",
     "Agrawal"
    ],
    [
     "Bramhendra",
     "Koilakuntla"
    ],
    [
     "Srikanth",
     "Konjeti"
    ]
   ],
   "title": "Spot and Merge: A Hybrid Context Biasing Approach for Rare Word and Out of Vocabulary Recognition",
   "original": "2692",
   "order": 675,
   "page_count": 5,
   "abstract": [
    "Deep context biasing improves rare word recognition in automatic speech recognition (ASR) but often struggles with larger biasing lists. Performance on Out-of-Vocabulary (OOV) words remains limited as the ASR model must learn to generate unseen token sequences. These limitations become more pronounced in contact center applications with prevalent business-specific terminologies. Additionally, using an existing ASR model is challenging, as deep biasing requires full joint training of the ASR model and the biasing module. To address these issues, we introduce &#x27;spot and merge&#x27; (SAM), a novel Low-Rank Adapter (LoRA) based system that spots bias words in the cross-attention weights of the biasing module and merges them with ASR output. Unlike existing methods, our approach maintains strong performance even with larger biasing lists, achieving a 1.0% absolute word error rate (WER) reduction on LibriSpeech. It also demonstrates robust OOV recognition on an in-house contact center dataset."
   ],
   "p1": 3319,
   "pn": 3323,
   "doi": "10.21437/Interspeech.2025-2692",
   "url": "interspeech_2025/agrawal25b_interspeech.html"
  },
  "mizumoto25_interspeech": {
   "authors": [
    [
     "Tomoya",
     "Mizumoto"
    ],
    [
     "Atsushi",
     "Kojima"
    ],
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Lianbo",
     "Liu"
    ],
    [
     "Yui",
     "Sudo"
    ]
   ],
   "title": "Is Synthetic Data Truly Effective for Training Speech Language Models?",
   "original": "2693",
   "order": 369,
   "page_count": 5,
   "abstract": [
    "The development of Large Language Models (LLMs) has expanded beyond text-based tasks to speech applications such as Automatic Speech Recognition (ASR) and Automated Speech Translation (AST). However, training speech language models based on LLMs requires large-scale datasets, which are challenging to construct. To address data scarcity, previous studies have explored synthetic data generation using ASR for transcribing unlabeled speech and Text-to-Speech (TTS) for generating speech from text. While synthetic data enables large-scale dataset construction without human intervention, concerns persist regarding quality degradation and its impact on model performance. This study investigates the effects of synthetic data on ASR and AST tasks. Experimental results indicate that synthetic data alone may degrade performance, whereas combining it with real data can enhance performance, demonstrating its potential when integrated with other data sources."
   ],
   "p1": 1808,
   "pn": 1812,
   "doi": "10.21437/Interspeech.2025-2693",
   "url": "interspeech_2025/mizumoto25_interspeech.html"
  },
  "he25c_interspeech": {
   "authors": [
    [
     "Jiajun",
     "He"
    ],
    [
     "Jinyi",
     "Mi"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints",
   "original": "2696",
   "order": 550,
   "page_count": 5,
   "abstract": [
    "Multimodal emotion recognition (MER) extracts emotions from multimodal data, including visual, speech, and text inputs, playing a key role in human-computer interaction. Attention-based fusion methods dominate MER research, achieving strong classification performance. However, two key challenges remain: effectively extracting modality-specific features and capturing cross-modal similarities despite distribution differences caused by modality heterogeneity. To address these, we propose a gated interactive attention mechanism to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions. Additionally, we introduce a modality-invariant generator to learn modality-invariant representations and constrain domain shifts by aligning cross-modal similarities. Experiments on IEMOCAP demonstrate that our method outperforms state-of-the-art MER approaches, achieving WA 80.7% and UA 81.3%."
   ],
   "p1": 2695,
   "pn": 2699,
   "doi": "10.21437/Interspeech.2025-2696",
   "url": "interspeech_2025/he25c_interspeech.html"
  },
  "jin25d_interspeech": {
   "authors": [
    [
     "Jiawei",
     "Jin"
    ],
    [
     "Zhihan",
     "Yang"
    ],
    [
     "Yixuan",
     "Zhou"
    ],
    [
     "Zhiyong",
     "Wu"
    ]
   ],
   "title": "In This Environment, As That Speaker: A Text-Driven Framework for Multi-Attribute Speech Conversion",
   "original": "2697",
   "order": 285,
   "page_count": 5,
   "abstract": [
    "We propose TES-VC (Text-driven Environment and Speaker controllable Voice Conversion), a text-driven voice conversion framework with independent control of speaker timbre and environmental acoustics. TES-VC processes simultaneous text inputs for target voice and environment, accurately generating speech matching described timbre/environment while preserving source content. Trained on synthetic data with decoupled vocal/environment features via latent diffusion modeling, our method eliminates interference between attributes. The Retrieval-Based Timbre Control (RBTC) module enables precise manipulation using abstract descriptions without paired data. Experiments confirm TES-VC effectively generates contextually appropriate speech in both timbre and environment with high content retention and superior controllability which demonstrates its potential for widespread applications."
   ],
   "p1": 1393,
   "pn": 1397,
   "doi": "10.21437/Interspeech.2025-2697",
   "url": "interspeech_2025/jin25d_interspeech.html"
  },
  "sridhar25_interspeech": {
   "authors": [
    [
     "Charan",
     "Sridhar"
    ],
    [
     "Shaomei",
     "Wu"
    ]
   ],
   "title": "J-j-j-just Stutter: Benchmarking Whisper's Performance Disparities on Different Stuttering Patterns",
   "original": "2700",
   "order": 767,
   "page_count": 5,
   "abstract": [
    "Despite their prevalence in everyday technologies, automated speech recognition (ASR) systems often struggle with disfluent speech. To diagnose and address these technical challenges, we evaluate OpenAI&#x27;s Whisper, a state-of-the-art ASR model, using speech samples from podcasts with people who stutter. Our results show significant disparities in Whisper&#x27;s performance between fluent and stuttered speech. Within disfluent speech, Whisper performs significantly worse on speech with sound repetitions - a disfluency more unique to stuttering. Notably, sound repetitions not only lead to transcription mistakes but also trigger Whisper to hallucinate over 20% of the time. Conducted by researchers who stutter, this study brings new insights on ASR biases against disfluent speech and highlights the value of disability-led research in addressing technological inequities affecting people with disabilities."
   ],
   "p1": 3753,
   "pn": 3757,
   "doi": "10.21437/Interspeech.2025-2700",
   "url": "interspeech_2025/sridhar25_interspeech.html"
  },
  "dutta25b_interspeech": {
   "authors": [
    [
     "Bikash",
     "Dutta"
    ],
    [
     "Rishabh",
     "Ranjan"
    ],
    [
     "Shyam",
     "Sathvik"
    ],
    [
     "Mayank",
     "Vatsa"
    ],
    [
     "Richa",
     "Singh"
    ]
   ],
   "title": "Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?",
   "original": "2701",
   "order": 932,
   "page_count": 5,
   "abstract": [
    "Quantization is essential for deploying large audio language models (LALMs) efficiently in resource-constrained environments. However, its impact on complex tasks, such as zero-shot audio spoofing detection, remains underexplored. This study evaluates the zero-shot capabilities of five LALMs, GAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct datasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their robustness to quantization (FP32, FP16, INT8). Despite high initial spoof detection accuracy, our analysis demonstrates severe predictive biases toward spoof classification across all models, rendering their practical performance equivalent to random classification. Interestingly, quantization to FP16 precision resulted in negligible performance degradation compared to FP32, effectively halving memory and computational requirements without materially impacting accuracy. However, INT8 quantization intensified model biases, significantly degrading balanced accuracy. These findings highlight critical architectural limitations and emphasize FP16 quantization as an optimal trade-off, providing guidelines for practical deployment and future model refinement."
   ],
   "p1": 4578,
   "pn": 4582,
   "doi": "10.21437/Interspeech.2025-2701",
   "url": "interspeech_2025/dutta25b_interspeech.html"
  },
  "kim25w_interspeech": {
   "authors": [
    [
     "Minseop",
     "Kim"
    ],
    [
     "Minsu",
     "Han"
    ],
    [
     "Seokyoung",
     "Hong"
    ],
    [
     "Myoung-wan",
     "Koo"
    ]
   ],
   "title": "Data Augmentation using Speech Synthesis for Speaker-Independent Dysarthria Severity Classification",
   "original": "2711",
   "order": 560,
   "page_count": 5,
   "abstract": [
    "Accurate dysarthria severity classification is essential for assessing motor speech disorders, and automation can improve efficiency and accessibility in clinical settings. While deep learning has significantly advanced this field, recent studies have increasingly leveraged large foundation ASR models. However, most studies focus on speaker-dependent (SD) classification, leaving speaker-independent (SI) classification as a major challenge due to limited datasets. SI classification is crucial in real-world scenarios where patient-specific information is unavailable. To address this, we applied two types of speech synthesis models for the first time in this task. We explore various strategies for integrating zero-shot text-to-speech (ZS-TTS) and voice conversion (VC) models to enhance SI classification and propose the most effective utilization settings. Our approach significantly improves the SI severity classification performance, paving the way for further research in this area."
   ],
   "p1": 2745,
   "pn": 2749,
   "doi": "10.21437/Interspeech.2025-2711",
   "url": "interspeech_2025/kim25w_interspeech.html"
  },
  "javed25_interspeech": {
   "authors": [
    [
     "Tahir",
     "Javed"
    ],
    [
     "Kaushal",
     "Bhogale"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data",
   "original": "2714",
   "order": 190,
   "page_count": 5,
   "abstract": [
    "We introduce Nirantar, a comprehensive framework for evaluating continual learning (CL) in multilingual and multi-domain ASR. Designed to reflect real-world CL challenges, Nirantar leverages data collected incrementally across 22 languages and 208 districts in India through natural episodes. This enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL), and the novel Language-Incremental Domain-Incremental Learning (LIDIL) scenarios. Unlike prior work that relies on simulated episodes, Nirantar presents dynamic, non-uniform language and domain shifts, making it an ideal testbed for CL research. With 3250 hours of human-transcribed speech, including 1720 hours newly introduced in this work, our framework enables systematic benchmarking of CL methods. We evaluate existing approaches and demonstrate that no single method performs consistently well, underscoring the need for more robust CL strategies."
   ],
   "p1": 918,
   "pn": 922,
   "doi": "10.21437/Interspeech.2025-2714",
   "url": "interspeech_2025/javed25_interspeech.html"
  },
  "singh25d_interspeech": {
   "authors": [
    [
     "Anup",
     "Singh"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Vipul",
     "Arora"
    ]
   ],
   "title": "Language-Agnostic Speech Tokenizer for Spoken Term Detection with Efficient Retrieval",
   "original": "2722",
   "order": 537,
   "page_count": 5,
   "abstract": [
    "The surge in multilingual and code-switched spoken content demands efficient Query-by-Example Spoken Term Detection (STD) systems capable of handling diverse languages. Existing STD systems are monolingual; they typically require large labeled datasets for training and use costly DTW-based matching during inference, limiting their practicality. This paper proposes a novel speech tokenizer that converts speech into language-agnostic tokens. Furthermore, a multi-stage search algorithm enables fast and efficient retrieval from large datasets. In experimental evaluations, the tokens from the proposed tokenizer demonstrate strong speaker invariance, consistent performance across languages, and a capability to generalize effectively to unseen languages, outperforming the baselines significantly."
   ],
   "p1": 2630,
   "pn": 2634,
   "doi": "10.21437/Interspeech.2025-2722",
   "url": "interspeech_2025/singh25d_interspeech.html"
  },
  "baumann25_interspeech": {
   "authors": [
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Pathology-Aware Speech Encoding and Data Augmentation for Dysarthric Speech Recognition",
   "original": "2724",
   "order": 669,
   "page_count": 5,
   "abstract": [
    "Automatic speech recognition (ASR) for pathologic speech remains a major challenge due to high variability in articulation, phonation, and prosody distortions. In this work, we propose a pathology-aware speech encoder based on BEST-RQ pre-training, which incorporates 46k hours of speech, including pathologic and atypical speech. We continue pre-training for domain adaptation and experiment with etiology-specific codebooks. We achieve a 13.2% relative word error rate (WER) improvement using the pathology-aware speech encoder with etiology-specific continued pre-training. Additionally, we examine the impact of incorporating synthetic and out-of-domain (OOD) data to further enhance ASR performance. Synthetic data reduces WER by up to 8.7%, while OOD data improves WER by 12.2%. Finally, we introduce a semantic similarity-based data augmentation technique to optimize data selection, achieving a WER improvement of up to 9.7% while minimizing the need for additional training data."
   ],
   "p1": 3289,
   "pn": 3293,
   "doi": "10.21437/Interspeech.2025-2724",
   "url": "interspeech_2025/baumann25_interspeech.html"
  },
  "behera25_interspeech": {
   "authors": [
    [
     "Avishkar",
     "Behera"
    ],
    [
     "Riya Ann",
     "Easow"
    ],
    [
     "Venkatesh",
     "Parvathala"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "Test-Time Training for Speech Enhancement",
   "original": "2725",
   "order": 486,
   "page_count": 5,
   "abstract": [
    "This paper introduces a novel application of Test-Time Training (TTT) for Speech Enhancement, addressing the challenges posed by unpredictable noise conditions and domain shifts. This method combines a main speech enhancement task with a self-supervised auxiliary task in a Y-shaped architecture. The model dynamically adapts to new domains during inference time by optimizing the proposed self-supervised tasks like noise-augmented signal reconstruction or masked spectrogram prediction, bypassing the need for labeled data. We further introduce various TTT strategies offering a trade-off between adaptation and efficiency. Evaluations across synthetic and real-world datasets show consistent improvements across speech quality metrics, outperforming the baseline model. This work highlights the effectiveness of TTT in speech enhancement, providing insights for future research in adaptive and robust speech processing."
   ],
   "p1": 2375,
   "pn": 2379,
   "doi": "10.21437/Interspeech.2025-2725",
   "url": "interspeech_2025/behera25_interspeech.html"
  },
  "chen25p_interspeech": {
   "authors": [
    [
     "Peijie",
     "Chen"
    ],
    [
     "Wenhao",
     "Guan"
    ],
    [
     "Kaidi",
     "Wang"
    ],
    [
     "Weijie",
     "Wu"
    ],
    [
     "Hukai",
     "Huang"
    ],
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ]
   ],
   "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
   "original": "2726",
   "order": 998,
   "page_count": 5,
   "abstract": [
    "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction."
   ],
   "p1": 4908,
   "pn": 4912,
   "doi": "10.21437/Interspeech.2025-2726",
   "url": "interspeech_2025/chen25p_interspeech.html"
  },
  "rolland25_interspeech": {
   "authors": [
    [
     "Thomas",
     "Rolland"
    ],
    [
     "Alberto",
     "Abad"
    ]
   ],
   "title": "Exploring Shared-Weight Mechanisms in Transformer and Conformer Architectures for Automatic Speech Recognition",
   "original": "2733",
   "order": 588,
   "page_count": 5,
   "abstract": [
    "In recent years, the increasing demand for parameter-efficient automatic speech recognition (ASR) systems has driven researchers to explore innovative architectures and techniques designed for minimising model size while maintaining recognition performance. Thus, weight-sharing mechanisms stand out as a promising approach. In this study, we present a comprehensive evaluation of weight-sharing applied to the different components of both Transformer and Conformer architectures in the context of ASR. Furthermore, we investigate the behaviour of these weight-sharing configurations when fine-tuned for a low-resource task, specifically children&#x27;s ASR. Additionally, we introduce Shared-Conformer, a novel architecture that achieves a 63% parameter reduction with only a minimal increase in word error rate. Our findings demonstrate that weight-sharing significantly reduces the number of parameters while preserving competitive performance in both well-resourced and low-resourced scenarios."
   ],
   "p1": 2885,
   "pn": 2889,
   "doi": "10.21437/Interspeech.2025-2733",
   "url": "interspeech_2025/rolland25_interspeech.html"
  },
  "joshi25_interspeech": {
   "authors": [
    [
     "Sakshi",
     "Joshi"
    ],
    [
     "Eldho",
     "Ittan George"
    ],
    [
     "Tahir",
     "Javed"
    ],
    [
     "Kaushal",
     "Bhogale"
    ],
    [
     "Nikhil",
     "Narasimhan"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women",
   "original": "2734",
   "order": 865,
   "page_count": 5,
   "abstract": [
    "Digital inclusion remains a challenge for marginalized communities, especially rural women in low-resource language regions like Bhojpuri. Voice-based access to agricultural services, financial transactions, government schemes, and healthcare is vital for their empowerment, yet existing ASR systems for this group remain largely untested. To address this gap, we create SRUTI, a benchmark consisting of rural Bhojpuri women speakers. Evaluation of current ASR models on SRUTI shows poor performance due to data scarcity, which is difficult to overcome due to social and cultural barriers that hinder large-scale data collection. To overcome this, we propose generating synthetic speech using just 25–30 seconds of audio per speaker from approximately 100 rural women. Augmenting existing datasets with this synthetic data achieves an improvement of 4.7 WER, providing a scalable, minimally intrusive solution to enhance ASR and promote digital inclusion in low-resource language."
   ],
   "p1": 4243,
   "pn": 4247,
   "doi": "10.21437/Interspeech.2025-2734",
   "url": "interspeech_2025/joshi25_interspeech.html"
  },
  "barahona25_interspeech": {
   "authors": [
    [
     "Sara",
     "Barahona"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Ladislav",
     "Mošner"
    ],
    [
     "Junyi",
     "Peng"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Johan",
     "Rohdin"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Jiangyu",
     "Han"
    ],
    [
     "Petr",
     "Palka"
    ],
    [
     "Federico",
     "Landini"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Themos",
     "Stafylakis"
    ],
    [
     "Sandro",
     "Cumani"
    ],
    [
     "Dominik",
     "Boboš"
    ],
    [
     "Miroslav",
     "Hlavaček"
    ],
    [
     "Martin",
     "Kodovsky"
    ],
    [
     "Tomaš",
     "Pavliček"
    ]
   ],
   "title": "Analysis of ABC Frontend Audio Systems for the NIST-SRE24",
   "original": "2737",
   "order": 1174,
   "page_count": 5,
   "abstract": [
    "We present a comprehensive analysis of the embedding extractors (frontends) developed by the ABC team for the audio track of NIST SRE 2024. We follow the two scenarios imposed by NIST: using only a provided set of telephone recordings for training (fixed) or adding publicly available data (open condition). Under these constraints, we develop the best possible speaker embedding extractors for the pre-dominant conversational telephone speech (CTS) domain. We explored architectures based on ResNet with different pooling mechanisms, recently introduced ReDimNet architecture, as well as a system based on the XLS-R model, which represents the family of large pre-trained self-supervised models. In open condition, we train on VoxBlink2 dataset, containing 110 thousand speakers across multiple languages. We observed a good performance and robustness of VoxBlink-trained models, and our experiments show practical recipes for developing state-of-the-art frontends for speaker recognition."
   ],
   "p1": 5763,
   "pn": 5767,
   "doi": "10.21437/Interspeech.2025-2737",
   "url": "interspeech_2025/barahona25_interspeech.html"
  },
  "chen25q_interspeech": {
   "authors": [
    [
     "Zhuangqi",
     "Chen"
    ],
    [
     "Xianjun",
     "Xia"
    ],
    [
     "Xiaohuai",
     "Le"
    ],
    [
     "Siyu",
     "Sun"
    ],
    [
     "Chuanzeng",
     "Huang"
    ]
   ],
   "title": "AF-Vocoder: Artifact-Free Neural Vocoder with Global Artifact Filter",
   "original": "2739",
   "order": 997,
   "page_count": 5,
   "abstract": [
    "Recent studies have demonstrated the advantage of generative adversarial network (GAN)-based vocoders in high-fidelity speech synthesis and fast inference speed. However, they often suffer from audible artifacts such as aliasing and blurring. In this paper, we propose AF-Vocoder, a novel GAN-based vocoder that can synthesize high-fidelity speech with fewer artifacts. Specifically, we introduce a frequency-domain artifacts filter named GAFilter to achieve artifact removal. GAFilter incorporates a learnable frequency filter, which enforces a desired inductive bias of frequency control for artifact-free speech synthesis. Experimental results show that the proposed AF-Vocoder outperforms other GAN-based vocoders in speech reconstruction quality and artifact suppression on various datasets including out-of-domain speakers."
   ],
   "p1": 4903,
   "pn": 4907,
   "doi": "10.21437/Interspeech.2025-2739",
   "url": "interspeech_2025/chen25q_interspeech.html"
  },
  "hoffner25_interspeech": {
   "authors": [
    [
     "Dirk",
     "Hoffner"
    ],
    [
     "Simon",
     "Weihe"
    ],
    [
     "Thomas",
     "Brand"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "Hearing deficits of transformer-based ASR for anechoic and spatial signals",
   "original": "2741",
   "order": 1168,
   "page_count": 5,
   "abstract": [
    "We compare automatic speech recognition (ASR) with human speech recognition (HSR) based on speech material that is traditionally used for diagnosing hearing deficits. Specifically, we quantify the human-machine gap with sentences in noise for different model sizes and two languages supported by Whisper, e.g., German and English. For German speech, we also determine the gap in different rooms and the presence of reverberation and localized noise. Results are put in context of audiological diagnosis using the speech reception threshold (SRT) (i.e., the SNR with 50% word recognition rate). We find that the largest ASR system is mildly hearing impaired when exposed to non-spatial, unpredictable US-English sentences and that using German speech degrades the SRT by 4.9 dB. In reverberant rooms, the gap reaches at least 5.9 dB. Based on the language bias, we estimate that same model can achieve better performance than normal-hearing listeners in anechoic conditions for US-English."
   ],
   "p1": 5733,
   "pn": 5737,
   "doi": "10.21437/Interspeech.2025-2741",
   "url": "interspeech_2025/hoffner25_interspeech.html"
  },
  "eom25_interspeech": {
   "authors": [
    [
     "SooHwan",
     "Eom"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "SiamCTC:  Learning Speech Representations through Monotonic Temporal Alignment ",
   "original": "2746",
   "order": 731,
   "page_count": 5,
   "abstract": [
    "Self-supervised speech representation learning has made significant progress through Siamese networks, which leverage different views of the same input. However, existing methods often require frame-wise alignment between these views, overlooking the broader linguistic context invariance across different speaking styles. We introduce SiamCTC, a framework that integrates Siamese networks with Connectionist Temporal Classification (CTC) to learn speech representations without strict frame-level correspondence. By employing CTC loss to establish flexible, monotonic alignments between differing temporal realizations of the same content, SiamCTC accommodates speed perturbations and other temporal augmentations. This design relaxes frame-wise constraints while preserving temporal coherence and enhancing robustness to speaking-rate variations in downstream tasks. Our experiments demonstrate that SiamCTC leads to more adaptable speech representations, particularly at diverse speaking rates."
   ],
   "p1": 3573,
   "pn": 3577,
   "doi": "10.21437/Interspeech.2025-2746",
   "url": "interspeech_2025/eom25_interspeech.html"
  },
  "minixhofer25_interspeech": {
   "authors": [
    [
     "Christoph",
     "Minixhofer"
    ],
    [
     "Ondřej",
     "Klejch"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Scaling Laws for Synthetic Speech for Model Training",
   "original": "2750",
   "order": 649,
   "page_count": 5,
   "abstract": [
    "We investigate how the scale of Text-to-Speech (TTS) models&#x27; training data influences Automatic Speech Recognition (ASR) performance when real training data is replaced entirely by synthetic speech. We propose an extension to established data scaling laws that incorporates an additional term capturing the mismatch between real and synthetic distributions in low-data regimes. We compare Mean Squared Error (MSE) and Denoising Diffusion Probabilistic Models (DDPMs) for TTS: MSE-based speech, though oversmoothed, provides stronger ASR results with smaller TTS datasets, while DDPM-based speech surpasses MSE once trained on enough data to better approximate the real distribution. Our findings also show that synthetic speech can only approximate or match real data performance if the TTS model itself is trained on a sufficiently large corpus, emphasizing that distribution coverage is crucial for fully synthetic ASR training."
   ],
   "p1": 3189,
   "pn": 3193,
   "doi": "10.21437/Interspeech.2025-2750",
   "url": "interspeech_2025/minixhofer25_interspeech.html"
  },
  "kang25d_interspeech": {
   "authors": [
    [
     "Jiawen",
     "Kang"
    ],
    [
     "Dongrui",
     "Han"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Jingyan",
     "Zhou"
    ],
    [
     "Jinchao",
     "Li"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "On the Within-class Variation Issue in Alzheimer's Disease Detection",
   "original": "2751",
   "order": 1155,
   "page_count": 5,
   "abstract": [
    "Alzheimer&#x27;s Disease (AD) detection employs machine learning classification models to distinguish between individuals with AD and those without. Different from conventional classification tasks, we identify within-class variation as a critical challenge in AD detection: individuals with AD exhibit a spectrum of cognitive impairments. Therefore, simplistic binary AD classification may overlook two crucial aspects: within-class heterogeneity and instance-level imbalance. In this work, we found using a sample score estimator can generate sample-specific soft scores aligning with cognitive scores. We subsequently propose two simple yet effective methods: Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two problems respectively. Based on the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the advantages of the proposed approaches in detection performance. These findings provide insights for developing robust and reliable AD detection models."
   ],
   "p1": 5668,
   "pn": 5672,
   "doi": "10.21437/Interspeech.2025-2751",
   "url": "interspeech_2025/kang25d_interspeech.html"
  },
  "rastogi25_interspeech": {
   "authors": [
    [
     "Sparsh",
     "Rastogi"
    ],
    [
     "Harsh",
     "Dadwal"
    ],
    [
     "Khushboo",
     "Modi"
    ],
    [
     "Jatin",
     "Bedi"
    ],
    [
     "Jasmeet",
     "Singh"
    ]
   ],
   "title": "Towards Sentence Level Imagined Speech Generation from EEG signals",
   "original": "2752",
   "order": 1133,
   "page_count": 5,
   "abstract": [
    "Brain-Computer Interfaces (BCIs) have emerged as alternative means of communication for individuals with speech &amp; motor impairments. These systems enable patients to express themselves without any articulation, by decoding speech from neural activity. However, most of the existing studies rely on invasive surgical procedures, with limited studies using non-invasive signals for phoneme or word level classification, thus covering a short vocabulary. To the best of our knowledge, this study presents the first demonstration of a framework for sentence level imagined speech synthesis from non-invasive electroencephalography (EEG) signals. Our model uses an Efficient-Net based masked auto-encoder approach for learning feature embeddings from EEG signals which are then used for fine-tuning BERT for next token generation. For this study, Large Spanish Speech EEG Dataset has been used with a mixed subject approach for both training &amp; evaluation purposes, resulting into a 48.92% accuracy."
   ],
   "p1": 5558,
   "pn": 5562,
   "doi": "10.21437/Interspeech.2025-2752",
   "url": "interspeech_2025/rastogi25_interspeech.html"
  },
  "krzywdziak25_interspeech": {
   "authors": [
    [
     "Justyna",
     "Krzywdziak"
    ],
    [
     "Bartłomiej",
     "Eljasiak"
    ],
    [
     "Joanna",
     "Stępień"
    ],
    [
     "Michał",
     "Świątek"
    ],
    [
     "Agnieszka",
     "Pruszek"
    ]
   ],
   "title": "Leveraging Text and Speech Processing for Suicide Risk Classification in Chinese Adolescents",
   "original": "2755",
   "order": 85,
   "page_count": 5,
   "abstract": [
    "The increasing prevalence of depression among young people is a growing global concern. Early detection and intervention are crucial, making the development of effective diagnostic tools essential. This work explores the use of advanced text and speech processing techniques to classify suicide risk within the context of the SpeechWellness Challenge (SW1) and verifies if speech can be used as a non-invasive and readily available mental health indicator. The analysis incorporated both linguistic features and audio-based methods for spontaneous speech and passage reading. For text classification, Large Language Models like Qwen2.5 and BERT were evaluated. For audio-based prediction, state-of-the-art speech processing models, including Whisper, Wav2Vec2 and HuBERT were employed. Furthermore, a multimodal approach combining both vocal and textual features was investigated. The results obtained in this research ranked among the highest in the challenge."
   ],
   "p1": 394,
   "pn": 398,
   "doi": "10.21437/Interspeech.2025-2755",
   "url": "interspeech_2025/krzywdziak25_interspeech.html"
  },
  "sankar25_interspeech": {
   "authors": [
    [
     "Ashwin",
     "Sankar"
    ],
    [
     "Yoach",
     "Lacombe"
    ],
    [
     "Sherry",
     "Thomas"
    ],
    [
     "Praveen",
     "Srinivasa Varadhan"
    ],
    [
     "Sanchit",
     "Gandhi"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "Rasmalai : Resources for Adaptive Speech Modeling in IndiAn Languages with Accents and Intonations",
   "original": "2758",
   "order": 842,
   "page_count": 5,
   "abstract": [
    "We introduce RASMALAI, a large-scale speech dataset with rich text descriptions, designed to advance controllable and expressive text-to-speech (TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours of speech and 24 million text-description annotations with fine-grained attributes like speaker identity, accent, emotion, style, and background conditions. Using RASMALAI, we develop INDICPARLERTTS, the first open-source, text-description-guided TTS for Indian languages. Systematic evaluation demonstrates its ability to generate high-quality speech for named speakers, reliably follow text descriptions and accurately synthesize specified attributes. Additionally, it effectively transfers expressive characteristics both within and across languages. INDICPARLERTTS consistently achieves strong performance across these evaluations, setting a new standard for controllable multilingual expressive speech synthesis in Indian languages."
   ],
   "p1": 4128,
   "pn": 4132,
   "doi": "10.21437/Interspeech.2025-2758",
   "url": "interspeech_2025/sankar25_interspeech.html"
  },
  "sedlacek25_interspeech": {
   "authors": [
    [
     "Šimon",
     "Sedláček"
    ],
    [
     "Bolaji",
     "Yusuf"
    ],
    [
     "Ján",
     "Švec"
    ],
    [
     "Pradyoth",
     "Hegde"
    ],
    [
     "Santosh",
     "Kesiraju"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs",
   "original": "2764",
   "order": 357,
   "page_count": 5,
   "abstract": [
    "In this work, we approach spoken Dialogue State Tracking (DST) by bridging the representation spaces of speech encoders and LLMs via a small connector module, with a focus on fully open-sourced and open-data components (WavLM-large, OLMo). We focus on ablating different aspects of such systems including full/LoRA adapter fine-tuning, the effect of agent turns in the dialogue history, as well as fuzzy matching-based output post-processing, which greatly improves performance of our systems on named entities in the dialogue slot values. We conduct our experiments on the SpokenWOZ dataset, and additionally utilize the Speech-Aware MultiWOZ dataset to augment our training data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17% JGA on SpokenWOZ test."
   ],
   "p1": 1748,
   "pn": 1752,
   "doi": "10.21437/Interspeech.2025-2764",
   "url": "interspeech_2025/sedlacek25_interspeech.html"
  },
  "srinivasavaradhan25_interspeech": {
   "authors": [
    [
     "Praveen",
     "Srinivasa Varadhan"
    ],
    [
     "Sherry",
     "Thomas"
    ],
    [
     "Sai",
     "Teja M S"
    ],
    [
     "Suvrat",
     "Bhooshan"
    ],
    [
     "Mitesh M.",
     "Khapra"
    ]
   ],
   "title": "The State Of TTS: A Case Study with Human Fooling Rates",
   "original": "2765",
   "order": 468,
   "page_count": 5,
   "abstract": [
    "While subjective evaluations in recent years indicate rapid progress in TTS, can current TTS systems truly pass a human deception test in a Turing-like evaluation? We introduce Human Fooling Rate (HFR), a metric that directly measures how often machine-generated speech is mistaken for human. Our large-scale evaluation of open-source and commercial TTS models reveals critical insights: (i) CMOS-based claims of human parity often fail under deception testing, (ii) TTS progress should be benchmarked on datasets where human speech achieves high HFRs, as evaluating against monotonous or less expressive reference samples sets a low bar, (iii) Commercial models approach human deception in zero-shot settings, while open-source systems still struggle with natural conversational speech; (iv) Fine-tuning on high-quality data improves realism but does not fully bridge the gap. Our findings underscore the need for more realistic, human-centric evaluations alongside existing subjective tests."
   ],
   "p1": 2285,
   "pn": 2289,
   "doi": "10.21437/Interspeech.2025-2765",
   "url": "interspeech_2025/srinivasavaradhan25_interspeech.html"
  },
  "ys25_interspeech": {
   "authors": [
    [
     "Upendra Vishwanath",
     "Y. S."
    ],
    [
     "Tanuka",
     "Bhattacharjee"
    ],
    [
     "Deekshitha",
     "G"
    ],
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Kumar",
     "Chowdam Venkata Thirumala"
    ],
    [
     "Madassu",
     "Keerthipriya"
    ],
    [
     "Darshan",
     "Chikktimmegowda"
    ],
    [
     "Dipti",
     "Baskar"
    ],
    [
     "Yamini",
     "Belur"
    ],
    [
     "Seena",
     "Vengalil"
    ],
    [
     "Atchayaram",
     "Nalini"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Comparison of Acoustic and Textual Features for Dysarthria Severity Classification in Amyotrophic Lateral Sclerosis",
   "original": "2767",
   "order": 167,
   "page_count": 5,
   "abstract": [
    "We explore language-agnostic deep text embeddings for severity classification of dysarthria in Amyotrophic Lateral Sclerosis (ALS). Speech recordings are transcribed by human and ASR and embeddings of the transcripts are considered. Though speech recognition accuracy has been studied for grading dysarthria severity, no effort has yet been made to utilize text embeddings of the transcripts. We perform severity classification at different granularity (2, 3, and 5-class) using data obtained from 47 ALS subjects. Experiments with dense neural network based classifiers suggest that, though text features achieve nearly equal performances as baseline speech features, like statistics of mel frequency cepstral coefficients (MFCC), for 2-class classification, speech features outperform for higher number of classes. Concatenation of text embeddings and MFCC statistics attains the best performances with mean F1 scores of 88%, 68%, and 53%, respectively, in 2, 3, and 5-class classification."
   ],
   "p1": 803,
   "pn": 807,
   "doi": "10.21437/Interspeech.2025-2767",
   "url": "interspeech_2025/ys25_interspeech.html"
  },
  "degroot25_interspeech": {
   "authors": [
    [
     "Dimme",
     "de Groot"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Devendra",
     "Kayande"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Zhengjun",
     "Yue"
    ]
   ],
   "title": "Objective and Subjective Evaluation of Diffusion-Based Speech  Enhancement for Dysarthric Speech",
   "original": "2768",
   "order": 559,
   "page_count": 5,
   "abstract": [
    "Dysarthric speech poses significant challenges for automatic speech recognition (ASR) systems due to its high variability and reduced intelligibility. In this work we explore the use of diffusion models for dysarthric speech enhancement, which is based on the hypothesis that using diffusion-based speech enhancement moves the distribution of dysarthric speech closer to that of typical speech, which could potentially improve dysarthric speech recognition performance. We assess the effect of two diffusion-based and one signal-processing-based speech enhancement algorithms on intelligibility and speech quality of two English dysarthric speech corpora. We applied speech enhancement to both typical and dysarthric speech and evaluate the ASR performance using Whisper-Turbo, and the subjective and objective speech quality of the original and enhanced dysarthric speech. We also fine-tuned Whisper-Turbo on the enhanced speech to assess its impact on recognition performance."
   ],
   "p1": 2740,
   "pn": 2744,
   "doi": "10.21437/Interspeech.2025-2768",
   "url": "interspeech_2025/degroot25_interspeech.html"
  },
  "gong25c_interspeech": {
   "authors": [
    [
     "Ziwei",
     "Gong"
    ],
    [
     "Lin",
     "Ai"
    ],
    [
     "Harsh",
     "Deshpande"
    ],
    [
     "Alexander",
     "Johnson"
    ],
    [
     "Emmy",
     "Phung"
    ],
    [
     "Zehui",
     "Wu"
    ],
    [
     "Ahmad",
     "Emami"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Comparison-Based Automatic Evaluation for Meeting Summarization",
   "original": "2771",
   "order": 60,
   "page_count": 5,
   "abstract": [
    "Large Language Models (LLMs) have spurred interest in automatic evaluation methods for summarization, offering a faster, more cost-effective alternative to human evaluation. However, existing methods often fall short when applied to complex tasks like long-context summarizations and dialogue-based meeting summarizations. In this paper, we introduce CREAM (Comparison-based Reference-free Elo-ranked Automatic evaluation for Meeting summarization), a novel framework that addresses the unique challenges of evaluating meeting summaries. CREAM leverages a combination of chain-of-thought reasoning and key facts alignment to assess conciseness and completeness of model-generated summaries without requiring reference. By employing an ELO ranking system, our approach provides a robust mechanism for comparing the quality of different models or prompt configurations."
   ],
   "p1": 291,
   "pn": 295,
   "doi": "10.21437/Interspeech.2025-2771",
   "url": "interspeech_2025/gong25c_interspeech.html"
  },
  "yue25_interspeech": {
   "authors": [
    [
     "Zhengjun",
     "Yue"
    ],
    [
     "Mara",
     "Barberis"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Judith",
     "Dineley"
    ],
    [
     "Willemijn",
     "Doedens"
    ],
    [
     "Lottie",
     "Stipdonk"
    ],
    [
     "YuanYuan",
     "Zhang"
    ],
    [
     "Elke de",
     "Witte"
    ],
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Djaina",
     "Satoer"
    ],
    [
     "Marina",
     "Ruiter"
    ],
    [
     "Laureano Moro",
     "Velazquez"
    ],
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Challenges and practical guidelines for atypical speech data collection, annotation, usage and sharing: A multi-project perspective",
   "original": "2774",
   "order": 805,
   "page_count": 5,
   "abstract": [
    "Speech technologies have advanced significantly, yet they remain largely trained on typical speech, limiting their applicability to individuals with speech and language impairments. A key obstacle is the lack of well-annotated and representative atypical speech corpora. This paper conducts a multi-project survey and shares the first-hand experience on the challenges of collecting, annotating, using, and sharing atypical speech data. Experiences from seven research projects on collecting atypical speech data, involving both academic and clinical perspectives, are reported and potential issues are discussed. Furthermore, the paper provides practical guidelines that allow for standardisation and harmonisation of data collection practices, which are crucial to allow studies to be compared, replicated, and validated, which is essential for developing more inclusive and effective speech technologies."
   ],
   "p1": 3943,
   "pn": 3947,
   "doi": "10.21437/Interspeech.2025-2774",
   "url": "interspeech_2025/yue25_interspeech.html"
  },
  "moussa25_interspeech": {
   "authors": [
    [
     "Omer",
     "Moussa"
    ],
    [
     "Mariya",
     "Toneva"
    ]
   ],
   "title": " Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain",
   "original": "2776",
   "order": 592,
   "page_count": 5,
   "abstract": [
    "Pretrained self-supervised speech models excel in speech tasks but do not reflect the hierarchy of human speech processing, as they encode rich semantics in middle layers and poor semantics in late layers. Recent work showed that brain-tuning (fine-tuning models using human brain recordings) improves speech models&#x27; semantic understanding. Here, we examine how well brain-tuned models further reflect the brain’s intermediate stages of speech processing. We find that late layers of brain-tuned models substantially improve over pretrained models in their alignment with semantic language regions. Further layer-wise probing reveals that early layers remain dedicated to low-level acoustic features, while late layers become the best at complex high-level tasks. These findings show that brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing going from acoustic to semantic representations, making them better model organisms for human speech processing."
   ],
   "p1": 2905,
   "pn": 2909,
   "doi": "10.21437/Interspeech.2025-2776",
   "url": "interspeech_2025/moussa25_interspeech.html"
  },
  "chang25d_interspeech": {
   "authors": [
    [
     "Yi",
     "Chang"
    ],
    [
     "Zhao",
     "Ren"
    ],
    [
     "Zhonghao",
     "Zhao"
    ],
    [
     "Thanh Tam",
     "Nguyen"
    ],
    [
     "Kun",
     "Qian"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Breaking Resource Barriers in Speech Emotion Recognition via Data Distillation",
   "original": "2778",
   "order": 30,
   "page_count": 5,
   "abstract": [
    "Speech emotion recognition (SER) plays a crucial role in human-computer interaction. The emergence of edge devices in the Internet of Things (IoT) presents challenges in constructing intricate deep learning models due to constraints in memory and computational resources. Moreover, emotional speech data often contains private information, raising concerns about privacy leakage during the deployment of SER models. To address these challenges, we propose a data distillation framework to facilitate efficient development of SER models in IoT applications using a synthesised, smaller, and distilled dataset. Our experiments demonstrate that the distilled dataset can be effectively utilised to train SER models with fixed initialisation, achieving performances comparable to those developed using the original full emotional speech dataset."
   ],
   "p1": 141,
   "pn": 145,
   "doi": "10.21437/Interspeech.2025-2778",
   "url": "interspeech_2025/chang25d_interspeech.html"
  },
  "yang25q_interspeech": {
   "authors": [
    [
     "Yiyuan",
     "Yang"
    ],
    [
     "Shitong",
     "Xu"
    ],
    [
     "Niki",
     "Trigoni"
    ],
    [
     "Andrew",
     "Markham"
    ]
   ],
   "title": "Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization",
   "original": "2779",
   "order": 510,
   "page_count": 5,
   "abstract": [
    "Sound source localization (SSL) is a critical technology for determining the position of sound sources in complex environments. However, existing methods face challenges such as high computational costs and precise calibration requirements, limiting their deployment in dynamic or resource-constrained environments. This paper introduces a novel 3D SSL framework, which uses sparse cross-attention, pretraining, and adaptive signal coherence metrics, to achieve accurate and computationally efficient localization with fewer input microphones. The framework is also fault-tolerant to unreliable or even unknown microphone position inputs, ensuring its applicability in real-world scenarios. Preliminary experiments demonstrate its scalability for multi-source localization without requiring additional hardware. This work advances SSL by balancing the model&#x27;s performance and efficiency and improving its robustness for real-world scenarios."
   ],
   "p1": 2495,
   "pn": 2499,
   "doi": "10.21437/Interspeech.2025-2779",
   "url": "interspeech_2025/yang25q_interspeech.html"
  },
  "hartanto25_interspeech": {
   "authors": [
    [
     "Roland",
     "Hartanto"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "SepVAC: Multitask Learning of Speaker Separation, Speaker Localization, Microphone Array Localization, and Room Acoustic Parameter Estimation in Various Acoustic Conditions ",
   "original": "2784",
   "order": 507,
   "page_count": 5,
   "abstract": [
    "This paper proposes a multitask learning method for speech separation, that Separates speech and estimates the recording conditions in Various Acoustic Conditions (SepVAC) jointly. Unlike the previous methods that aim to achieve robustness against the uncertainty caused by noise and reverberation, this method explicitly estimates speaker &amp; microphone location and room acoustic parameters to disambiguate them from speech features. We introduce curriculum learning to learn the model parameters stably. In our evaluation using SMS-WSJ-Plus dataset, it outperforms the state-of-the-art SpatialNet baseline by 0.67 points in word error rate (WER)."
   ],
   "p1": 2480,
   "pn": 2484,
   "doi": "10.21437/Interspeech.2025-2784",
   "url": "interspeech_2025/hartanto25_interspeech.html"
  },
  "marcinek25_interspeech": {
   "authors": [
    [
     "Lubos",
     "Marcinek"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Joakim",
     "Gustafsson"
    ]
   ],
   "title": "Towards Adaptable and Intelligible Speech Synthesis in Noisy Environments",
   "original": "2787",
   "order": 444,
   "page_count": 5,
   "abstract": [
    "We present an investigation into adaptable speech synthesis for noisy environments. Leveraging a zero-shot TTS we synthesized a corpus of 1,200 speech samples from 100 sentences of varying complexity, each generated at six distinct levels of vocal effort. To simulate realistic listening conditions, the synthesized speech is merged with environmental noise recordings from a diverse range of indoor and transportation settings at nine different signal-to-noise ratios. We assess the intelligibility of the resulting noisy speech using the ASR word error rates across conditions. Additionally, the input text was evaluated using four metrics on sentence complexity and word predictability. A number of regression models that used noise type, SNR, vocal effort and text as input were trained to predict ASR WER. Results show that increased vocal effort improves intelligibility, with benefits up to 30% in adverse conditions, most most pronounced in environments with competing speech at low SNRs."
   ],
   "p1": 2165,
   "pn": 2169,
   "doi": "10.21437/Interspeech.2025-2787",
   "url": "interspeech_2025/marcinek25_interspeech.html"
  },
  "freisinger25_interspeech": {
   "authors": [
    [
     "Steffen",
     "Freisinger"
    ],
    [
     "Philipp",
     "Seeberger"
    ],
    [
     "Thomas",
     "Ranzenberger"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation",
   "original": "2792",
   "order": 57,
   "page_count": 5,
   "abstract": [
    "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric."
   ],
   "p1": 276,
   "pn": 280,
   "doi": "10.21437/Interspeech.2025-2792",
   "url": "interspeech_2025/freisinger25_interspeech.html"
  },
  "wakayama25_interspeech": {
   "authors": [
    [
     "Keigo",
     "Wakayama"
    ],
    [
     "Tomoko",
     "Kawase"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Masahiro",
     "Yasuda"
    ],
    [
     "Shoko",
     "Araki"
    ]
   ],
   "title": "Real-time TSE demonstration via SoundBeam with KD",
   "original": "2806",
   "order": 718,
   "page_count": 2,
   "abstract": [
    "The objective of target sound extraction (TSE) is to extract sound sources of a specified class from mixed signals. Research into TSE has been actively conducted with the aim of applying it to immersive systems and auditory devices. We propose to demonstrate a real-time TSE system, which can isolate the signal from a desired sound class from sound mixtures recorded on the fly. This demonstration is based on the recently proposed causal SoundBeam model, which is trained using knowledge distillation (KD) from a non-causal TSE system. Experiments have demonstrated that SoundBeam with KD exhibits superior extraction accuracy compared to a state-of-the-art (SOTA) TSE, i.e., Waveformer. This paper explains the implementation of the proposed real-time TSE demonstration system. It is noteworthy that this demonstration will show for the first time at Interspeech, the ability to extract sound signals of a selected sound event (SE) class in real-time on a laptop."
   ],
   "p1": 3529,
   "pn": 3530
  },
  "gourav25_interspeech": {
   "authors": [
    [
     "Vishal",
     "Gourav"
    ],
    [
     "Phanindra",
     "Mankale"
    ]
   ],
   "title": "Code Mix TTS: An Approach to Infer Human Like Speech for Multi-Lingual Input Texts",
   "original": "2807",
   "order": 436,
   "page_count": 2,
   "abstract": [
    "We have come a far way in terms of producing high quality, human like audios for given input texts using TTS or Text to Speech Systems. We simply provide the TTS system with an input text and it generates an audio output. But, if we look at it closely, we provide the input text in a particular language only. However, as humans, while interacting with each other, do we actually speak in just one particular language? The answer is No, in most cases. Our speech in general, specially in multiple language countries, is a mix of native language of the specific country and a popular language(mostly English) and sometimes the other way round where we mix a bit of the native language words in our English conversation. In this paper, we talk about an approach to achieve the similar kind of &quot;Code Mix&quot; inference output for &quot;Multi-Lingual&quot; input texts from our TTS system without the need of any additional training data or fine tuning."
   ],
   "p1": 2143,
   "pn": 2144
  },
  "nguyen25e_interspeech": {
   "authors": [
    [
     "Binh",
     "Nguyen"
    ],
    [
     "Thai",
     "Le"
    ]
   ],
   "title": "Turing's Echo: Investigating Linguistic Sensitivity of Deepfake Voice Detection via Gamification",
   "original": "2808",
   "order": 437,
   "page_count": 2,
   "abstract": [
    "Recent advancements in text-to-speech technology have made it easier than ever to synthesize high-fidelity, human-like audio, commonly coined as deepfake speech. While this technology is beneficial to many applications, it has introduced significant risks, especially in enabling hyper-realistic impersonation threats. Existing research has developed robust algorithms capable of detecting deepfake speech under various acoustic conditions, such as pitch shift and background noise. However, the indirect impact of linguistic factors, such as word choice, grammar, and sentence structure of a deepfake speech&#x27;s transcript, on the performance of deepfake detectors remains unexplored. As the first step to bridge this gap, this paper introduces a gamified research prototype, called TURING&#x27;S ECHO, to evaluate (1) how humans perceive such linguistic sensitivity in comparison with machine and (2) how robust is state-of-the-art deepfake speech detection under different linguistic nuances."
   ],
   "p1": 2145,
   "pn": 2146
  },
  "lay25b_interspeech": {
   "authors": [
    [
     "Bunlong",
     "Lay"
    ],
    [
     "Rostilav",
     "Makarov"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Real-Time Diffusion Buffer for Speech Enhancement On A Laptop",
   "original": "2809",
   "order": 719,
   "page_count": 2,
   "abstract": [
    "We demonstrate a real-time, diffusion-based speech enhancement system capable of removing background noise in a reverberant environment using a single microphone. Building on recent advances in diffusion models for speech enhancement, the proposed system achieves sub-second latency while preserving speech quality in challenging conference scenarios, e.g. loud babble noise. Attendees will personally experience the difference between turning on/off the speech enhancement system and regulating the quality/latency trade off. This demonstration showcases, for the first time, an online, diffusion-based enhancement framework suitable for live events."
   ],
   "p1": 3531,
   "pn": 3532
  },
  "cho25c_interspeech": {
   "authors": [
    [
     "Namhyun",
     "Cho"
    ],
    [
     "Sunmin",
     "Kim"
    ],
    [
     "Minsu",
     "Kang"
    ],
    [
     "Seolhee",
     "Lee"
    ],
    [
     "Choonghyeon",
     "Lee"
    ],
    [
     "Yangsun",
     "Lee"
    ]
   ],
   "title": "Unleashing   the  Inner Monster: Demonstrating High-Fidelity Human to Non-Human  Voice Conversion",
   "original": "2810",
   "order": 438,
   "page_count": 2,
   "abstract": [
    "Non-human monsters in massive multiplayer online role-playing games (MMORPGs) are essential for immersive game environments and significantly enhance player engagement. However, producing high-quality monster sounds demands considerable time and financial resources. AI-driven voice conversion offers a potential solution, but existing models rely on standard human voice training and human-centric audio features, limiting their ability to generate realistic non-human voices. This study presents a human-to-non-human (H2NH) voice conversion model designed to address these challenges. The model effectively generated high-quality non-human sounds by recording the voices of participants and converting them in real-time into selected monster vocalizations."
   ],
   "p1": 2147,
   "pn": 2148
  },
  "roman25_interspeech": {
   "authors": [
    [
     "Javier",
     "Román"
    ],
    [
     "Pol",
     "Pastells"
    ],
    [
     "Mauro",
     "Vázquez"
    ],
    [
     "Clara",
     "Puigventós"
    ],
    [
     "Montserrat",
     "Nofre"
    ],
    [
     "Mariona",
     "Taulé"
    ],
    [
     "Mireia",
     "Farrús"
    ]
   ],
   "title": "SCRIBAL: A Digital Transcription Tool in Higher Education",
   "original": "2812",
   "order": 1008,
   "page_count": 2,
   "abstract": [
    "SCRIBAL is a digital transcription and translation tool for university teaching, covering Catalan transcription and translation to the main foreign languages in class. Based on Whisper, SCRIBAL is fine-tuned for specific Catalan dialectal varieties and specialized academic terminology. It becomes an essential tool for accessibility, as well as for breaking language barriers and preserving the national languages in higher education."
   ],
   "p1": 4958,
   "pn": 4959
  },
  "shepardson25_interspeech": {
   "authors": [
    [
     "Victor",
     "Shepardson"
    ],
    [
     "Jonathan",
     "Reus"
    ],
    [
     "Thor",
     "Magnusson"
    ]
   ],
   "title": "Tungnaá In Live Performance: An Implementation Of Interactive Artistic Text-To-Voice",
   "original": "2813",
   "order": 439,
   "page_count": 2,
   "abstract": [
    "We demonstrate Tungnaa, a software musical instrument for controlling interactive artistic text-to-voice (IATV) models. IATV is a proposed low-resource text-conditional voice generation task for musical performance applications. Tungnaá is a Python package implementing IATV training, a real-time inference engine and graphical interface for performance with IATV models. We also describe an application of Tungnaa in collaboration with sound poet Jaap Blonk."
   ],
   "p1": 2149,
   "pn": 2150
  },
  "bokkahallisatish25_interspeech": {
   "authors": [
    [
     "Shree Harsha",
     "Bokkahalli Satish"
    ],
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "Hear Me Out: Interactive evaluation and bias discovery platform for speech-to-speech conversational AI",
   "original": "2814",
   "order": 440,
   "page_count": 2,
   "abstract": [
    "A new wave of speech foundation models is emerging, capable of processing spoken language directly from audio. These models promise more expressive and emotionally aware interactions by retaining prosodic information throughout conversations. ‘Hear Me Out’ evaluates their ability to preserve crucial vocal cues, enabling users to explore how variations in speaker characteristics and paralinguistic features influence AI responses. Through real-time voice conversion, users can ask a question and then re-ask it with a modified one, immediately observing differences in response tone, phrasing, and behavior. The system presents paired responses side by side, offering direct comparisons of AI interpretations of both the original and transformed voices, thereby highlighting potential biases. By creating inquiry into speaker modeling, contextual understanding, and fairness, this immersive experience encourages users to reflect on identity, voice, and also promote inclusive future research."
   ],
   "p1": 2151,
   "pn": 2152
  },
  "francis25_interspeech": {
   "authors": [
    [
     "Juliana",
     "Francis"
    ],
    [
     "Joakim",
     "Gustafsson"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "From Static to Dynamic: Enhancing AAC with Generative Imagery and Zero-Shot TTS",
   "original": "2815",
   "order": 1009,
   "page_count": 3,
   "abstract": [
    "This paper presents an Augmentative and Alternative Communication (AAC) approach for minimally verbal children with Autism Spectrum Disorder. Traditional AAC systems use fixed symbol sets and pre-defined Text-to-Speech (TTS) voices, this proposed method leverages text-to-image generation and zero-shot TTS to expand expressive capabilities. Users can create visual symbols for concepts and interests, enabling richer communication. Further, zero-shot TTS allows users to upload or record personalized voices, enabling users to have individualized output. By minimizing reliance on static symbols and voices, this approach aims to increase communicative agency, personal relevance, and social validity, areas often neglected in traditional interventions. Future research will explore long-term effects on communicative skills, user satisfaction, social engagement, and adaptability across various cultural and linguistic settings, aiming to develop more dynamic and personalized AAC solutions."
   ],
   "p1": 4960,
   "pn": 4962,
   "doi": "10.21437/Interspeech.2025-2815",
   "url": "interspeech_2025/francis25_interspeech.html"
  },
  "deluca25_interspeech": {
   "authors": [
    [
     "Alessandro",
     "De Luca"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "Volker",
     "Dellwo"
    ]
   ],
   "title": "Voxplorer: Voice data exploration and projection in an interactive dashboard",
   "original": "2816",
   "order": 61,
   "page_count": 2,
   "abstract": [
    "The acoustic signal of voice is inherently multi-dimensional. Early speech and voice research often focused on isolated or limited acoustic features. However, past research has demonstrated that a comprehensive understanding of voice requires analysing multiple dimensions simultaneously. The Voxplorer interactive dashboard addresses this issue by making state-of-the-art feature extraction and dimensionality reduction methods more accessible. It allows users to interactively explore, subset, and visualise pre-computed high-dimensional data, or extract features from recordings directly in the dashboard. The Voxplorer aims to provide modern technical resources for researchers, broadening the ideas and scope of future researcher in the field of voice communication sciences."
   ],
   "p1": 296,
   "pn": 297
  },
  "baihaqi25b_interspeech": {
   "authors": [
    [
     "Muhammad Yeza",
     "Baihaqi"
    ],
    [
     "Angel García",
     "Contreras"
    ],
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "Co-Speech Motion for Virtual Agents in Dialogue Using LLM-Driven Primitive Action Selection",
   "original": "2817",
   "order": 720,
   "page_count": 2,
   "abstract": [
    "Non-verbal behaviors, such as co-speech motion, are essential for making artificial agents more lifelike and engaging. However, existing approaches to generating co-speech motions still face significant challenges. Rule-based systems, while capable of producing natural and engaging motions, struggle with generalizability. In contrast, the common data-driven methods can generate a wide variety of gestures but come with high costs and require substantial adaptation to be applied to agents. Leveraging the power of Large Language Models (LLMs) for contextual planning and understanding, we propose an LLM-based motion control model that uses a primitive action selection strategy. This approach is expected to provide a more flexible and scalable solution for generating contextually appropriate co-speech motions across various embodied systems, including virtual agents and robots."
   ],
   "p1": 3533,
   "pn": 3534
  },
  "arai25_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Vocal-tract model with two directions: Static design for a dummy head and dynamic design for a speaking machine",
   "original": "2818",
   "order": 441,
   "page_count": 2,
   "abstract": [
    "Physical models of the human vocal tract have been developed for many purposes, including education in acoustics and phonetics, speech and language pathology, and speech science/technology. To cover different application areas, variations are needed. When organizing the types of vocal-tract models developed for such a wide range, one of the major axes is static vs. dynamic aspects. In this study, we demonstrate two models that are the current extremities of the two directions of this axis. The first model is an ultimate static one in which the vocal-tract configuration is one vowel. This model is embedded in a dummy head so that it can be applied to produce exactly the same vowel repeatedly or for a very long duration with a realistic directional radiation pattern. The second model is a recently developed dynamic one with several blocks simulating articulatory movements. The cam mechanisms help to change the vocal-tract configurations in real time."
   ],
   "p1": 2153,
   "pn": 2154
  },
  "merzougui25_interspeech": {
   "authors": [
    [
     "Dhia Eddine",
     "Merzougui"
    ],
    [
     "Nilesh",
     "Tete"
    ],
    [
     "Fabrice",
     "Maurel"
    ],
    [
     "Gaël",
     "Dias"
    ],
    [
     "Mohammed",
     "Hasanuzzaman"
    ],
    [
     "Aurélien",
     "Bournonville"
    ],
    [
     "Edgar",
     "Madelaine"
    ],
    [
     "Thomas",
     "Berthelin Le Tellier"
    ],
    [
     "François",
     "Ledoyen"
    ],
    [
     "Laure",
     "Poutrain-Lejeune"
    ],
    [
     "François",
     "Rioult"
    ],
    [
     "Jérémie",
     "Pantin"
    ]
   ],
   "title": "Concurrent Speech and Auditory Tag Clouds for Non-Visual Web Interaction",
   "original": "2819",
   "order": 1010,
   "page_count": 2,
   "abstract": [
    "Millions of visually impaired individuals face significant barriers to independent information access, limiting their autonomy and self-determination. Despite advancements in assistive technologies, challenges remain, particularly in handling web document structure and multi-channel information transposition. This paper addresses these issues by introducing the TagThunder experimental framework, which applies the &quot;cocktail party effect&quot; metaphor to transpose the morpho-dispositional semantics of web documents into an auditory tag cloud, enabling rapid non-visual skimming. Additionally, it explores interactive stimuli for structured information scanning through discrete and continuous guiding exploration strategies."
   ],
   "p1": 4963,
   "pn": 4964
  },
  "rai25_interspeech": {
   "authors": [
    [
     "Anand",
     "Rai"
    ],
    [
     "Satyam",
     "Rahangdale"
    ],
    [
     "Utkarsh",
     "Anand"
    ],
    [
     "Animesh",
     "Mukherjee"
    ]
   ],
   "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems",
   "original": "2820",
   "order": 62,
   "page_count": 2,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday applications, yet significant disparities in performance across diverse demographic groups persist. In this work, we introduce the ASR-FAIRBENCH leaderboard which is designed to assess both the accuracy and equity of ASR models in real-time. Leveraging the Meta&#x27;s Fair-Speech dataset, which captures diverse demographic characteristics, we employ a mixed-effects Poisson regression model to derive an overall fairness Score. This score is integrated with traditional metrics like Word Error Rate (WER) to compute the Fairness Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our approach reveals significant performance disparities in SOTA ASR models across demographic groups and offers a benchmark to drive the development of more inclusive ASR technologies."
   ],
   "p1": 298,
   "pn": 299
  },
  "draxler25_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Julian",
     "Pömp"
    ],
    [
     "Henk",
     "van den Heuvel"
    ],
    [
     "Fabio",
     "Ardolino"
    ],
    [
     "Arjan",
     "van Hessen"
    ]
   ],
   "title": "Transcribing Oral History Recordings Using the Transcription Portal",
   "original": "2821",
   "order": 63,
   "page_count": 2,
   "abstract": [
    "The Transcription Portal is a web-based service for multilingual orthographic transcription of speech, notably Oral History interviews. It is targeted at non-technical users: it provides a simple and intuitive GUI, supports several languages, and the workflow is pre-configured. Currently, the workflow consists of three steps: 1) automatic speech recognition, 2) manual correction of the transcript, 3) data export. Summarization and translation are planned. We demonstrate the portal on a set of historical Italian interviews on the Ravensbrück concentration camps."
   ],
   "p1": 300,
   "pn": 301
  },
  "vukovic25_interspeech": {
   "authors": [
    [
     "Teodora",
     "Vuković"
    ],
    [
     "Jeremy",
     "Zehr"
    ],
    [
     "Jonathan",
     "Schaber"
    ],
    [
     "Igor",
     "Mustač"
    ],
    [
     "Nikolina",
     "Rajović"
    ],
    [
     "Daniel",
     "McDonald"
    ],
    [
     "Johannes",
     "Graën"
    ],
    [
     "Noah",
     "Bubenhofer"
    ]
   ],
   "title": "LiRI Corpus Platform: Demonstration of a Web-Based Infrastructure for Multimodal Corpus Analysis",
   "original": "2822",
   "order": 64,
   "page_count": 2,
   "abstract": [
    "We present the LiRI Corpus Platform (LCP), a web-based infrastructure for storing, exploring, and analyzing diverse linguistic corpora with a focus on multimodal and audiovisual data. The platform supports synchronized querying of text, speech, and video through unified interfaces and a custom query language (Descriptive Query Definition - DQD). Dedicated frontends enable time-aligned exploration of gesture, audio, and spoken transcripts. LCP is designed to support researchers working with multimodal and annotated datasets, enabling cross-modal queries and layered annotation."
   ],
   "p1": 302,
   "pn": 303
  },
  "peirolilja25_interspeech": {
   "authors": [
    [
     "Alex",
     "Peiró-Lilja"
    ],
    [
     "Rodolfo",
     "Zevallos"
    ],
    [
     "Carme",
     "Armentano-Oller"
    ],
    [
     "Jose",
     "Giraldo"
    ],
    [
     "Cristina",
     "España-Bonet"
    ],
    [
     "Mireia",
     "Farrús"
    ]
   ],
   "title": "Towards Domain-Specific Spoken Language Understanding for a Catalan Voice-Controlled Video Game",
   "original": "2823",
   "order": 1011,
   "page_count": 2,
   "abstract": [
    "We design a voice-controlled video game to integrate Catalan into gaming using speech technologies developed under the Aina project. The game is designed to elicit natural speech commands from players. However, a significant challenge in this endeavor is the limited availability of Catalan-language Spoken Language Understanding (SLU) datasets, especially those covering specialized linguistic domains relevant to interactive gaming environments. To address this, we implement a cascading SLU system that combines automatic speech recognition (ASR) with roBERTa-based models previously trained in Catalan. The latter was finetuned as a multi-task classifier by generating synthetic transcriptions from a small set of human-written examples. With acceptable accuracy and time inference, our goal is to evaluate its performance in-game and gather feedback from users."
   ],
   "p1": 4965,
   "pn": 4966
  },
  "pallala25_interspeech": {
   "authors": [
    [
     "Arun Kumar",
     "Pallala"
    ],
    [
     "Nivedita",
     "Chennupati"
    ],
    [
     "Balaji",
     "Padmanaban"
    ],
    [
     "Rakesh",
     "Pogula"
    ],
    [
     "Uma Subhashini",
     "Ravuri"
    ],
    [
     "Naveen",
     "Ellanki"
    ],
    [
     "Harish",
     "Rajamani"
    ],
    [
     "Naveen",
     "Ambati"
    ]
   ],
   "title": "TargetVoice: Single Channel Low-Latency Target Speaker Extraction",
   "original": "2826",
   "order": 721,
   "page_count": 2,
   "abstract": [
    "We present TargetVoice, a lightweight, low-latency target speaker extraction (TSE) model optimized for edge devices. It isolates a target speaker’s voice from multi-speaker and noisy environments, making it ideal for use in call centers, conference calls, hands-free communication, and smart speakers. By streaming only the enrolled speaker’s voice, TargetVoice also improves speech recognition accuracy in real-world conditions. Unlike existing models that struggle with similar-gender speakers or varying acoustic environments, TargetVoice leverages a robust in-house data strategy and a specialized speaker embedding extraction system. The model uses a compact 10MB speaker encoder to generate a reliable embedding from a single 3 second enrollment. This embedding is fused with the input mixture in a 12MB extraction block with 6G MACs to isolate the target voice efficiently, enabling real-time performance on resource-constrained platforms."
   ],
   "p1": 3535,
   "pn": 3536
  },
  "mcallister25b_interspeech": {
   "authors": [
    [
     "Tara",
     "McAllister"
    ],
    [
     "Peter",
     "Traver"
    ],
    [
     "Amanda",
     "Eads"
    ],
    [
     "William",
     "Haack"
    ],
    [
     "Helen",
     "Carey"
    ],
    [
     "Yi",
     "Shan"
    ],
    [
     "Wendy",
     "Liang"
    ],
    [
     "Tae Hong",
     "Park"
    ]
   ],
   "title": "Accessible Delivery of Visual-Acoustic Biofeedback for Speech Sound Disorder",
   "original": "2827",
   "order": 1012,
   "page_count": 2,
   "abstract": [
    "This paper presents staRt, an iOS and web application for visual-acoustic biofeedback aiming to facilitate widespread uptake of technology-enhanced speech training. staRt provides a real-time linear predictive coding (LPC) spectrum that can be compared to an adjustable slider representing a target frequency for vocal tract resonance. The software currently presents exercises for children with speech sound disorder who have difficulty pronouncing &quot;r&quot; sounds in English, but it shows promise for adaptation to a number of other contexts."
   ],
   "p1": 4967,
   "pn": 4968
  },
  "quinterovillalobos25_interspeech": {
   "authors": [
    [
     "Yuni Amaloa",
     "Quintero Villalobos"
    ],
    [
     "Wafaa",
     "Wardah"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Robert P.",
     "Spang"
    ]
   ],
   "title": "Rollback Speech: Smart Feedback Prompts for Lost Utterances in Unstable Online Calls",
   "original": "2828",
   "order": 722,
   "page_count": 2,
   "abstract": [
    "We present a prototype system designed to enhance the user experience of online conferencing in the presence of poor network conditions. When connection issues cause parts of a speaker’s audio to be lost, the system uses dual automatic speech recognition (ASR) outputs—one local and one remote—to detect which utterances were not received. Upon reconnection, the speaker is provided with extracted keywords representing the information that did not reach the listener, enabling efficient and targeted repetition without full redundancy. This helps maintain conversational flow and reduces frustration during virtual communication. The system comprises real-time transcription, network degradation simulation, discrepancy detection, and lightweight keyword extraction in a modular architecture. Our “Show and Tell” demonstration allows participants to experience how the system augments communication continuity, even under conditions of packet loss or delayed transmission."
   ],
   "p1": 3537,
   "pn": 3538
  },
  "li25ea_interspeech": {
   "authors": [
    [
     "Zirong",
     "Li"
    ],
    [
     "Hongchen",
     "Wu"
    ],
    [
     "Yixin",
     "Gu"
    ],
    [
     "Yao",
     "Du"
    ],
    [
     "Yang",
     "Yue"
    ]
   ],
   "title": "Speech Annotation for A: Accuracy, Access, and Application",
   "original": "2831",
   "order": 65,
   "page_count": 2,
   "abstract": [
    "Accurate and efficient annotation of bilingual clinical recordings remains a persistent challenge, as existing solutions often require high demand for manual work by bilingual clinicians and their assistants and significant training related to annotation tools. To address this issue, we introduce Speech Annotation for A (SAFA)—an end-to-end, user-friendly “lazy mode” annotation workflow. By pairing annotation drafts generated from large language models with chunk-based editing, real-time difference highlighting, and speaker &amp; language tagging - even in multi-speaker code-switching scenarios - SAFA delivers high-quality audio annotations ready for research with minimal setup and minimal human check. It further provides standardized CSV/TXT exports, bridging the gap between fully automated approaches and the meticulous accuracy demanded by multilingual clinical research, while facilitating the creation and expansion of high-quality labeled datasets for downstream studies."
   ],
   "p1": 304,
   "pn": 305
  },
  "okamoto25_interspeech": {
   "authors": [
    [
     "Takuma",
     "Okamoto"
    ],
    [
     "Michiyo",
     "Kono"
    ]
   ],
   "title": "Simultaneous Speech Translation Integrated Compact Multiple Sound Spot Synthesis System On A Laptop Carried Out With A Backpack",
   "original": "2832",
   "order": 723,
   "page_count": 2,
   "abstract": [
    "Multiple sound spot synthesis, which can present different sounds in different zones simultaneously using a loudspeaker array, is an important spatial sound presentation technology for speech and audio applications. We have previously implemented a portable multiple sound spot synthesis system with a compact circular array of 16 loudspeakers, which can be carried out with a suitcase. To further improve the mobility, we implement a very small 16-channel amplifier directly mounted under the compact circular array. Additionally, we implement a system integrating multiple sound spot synthesis and multilingual simultaneous speech-to-speech translation on-premise on a laptop without network connection. Finally, the complete demo system can be carried out with a backpack. In the Show &amp; Tell, we demonstrate four-language sound spot synthesis combined with multilingual simultaneous speech-to-speech translation using the compact demo system."
   ],
   "p1": 3539,
   "pn": 3540
  },
  "znotins25_interspeech": {
   "authors": [
    [
     "Arturs",
     "Znotins"
    ],
    [
     "Didzis",
     "Gosko"
    ],
    [
     "Normunds",
     "Gruzitis"
    ]
   ],
   "title": "LATE: Open Source Toolkit for Latvian and Latgalian Speech Transcription",
   "original": "2833",
   "order": 66,
   "page_count": 2,
   "abstract": [
    "We present a unique software toolkit LATE for automatic speech recognition (ASR) and formatted transcription. It has a lightweight front-end and a completely statically compiled and linked stateless back-end for running Whisper-based ASR models. Thus, LATE can be relatively easily deployed and scaled in a cloud infrastructure, as well as run privately on a local PC. The LATE toolkit is available as both open source and precompiled binaries. By default, it comes with fine-tuned Latvian and Latgalian ASR models, as well as the multilingual Whisper model, while compatible models for other languages can be added."
   ],
   "p1": 306,
   "pn": 307
  },
  "patapati25_interspeech": {
   "authors": [
    [
     "Santosh",
     "Patapati"
    ],
    [
     "Aashrith",
     "Tatineni"
    ],
    [
     "Trisanth",
     "Srinivasan"
    ]
   ],
   "title": "GenECA: A General-Purpose Framework for Real-Time Adaptive Multimodal Embodied Conversational Agents",
   "original": "2834",
   "order": 724,
   "page_count": 2,
   "abstract": [
    "We present GenECA, a general-purpose framework for real-time multimodal interaction with embodied conversational agents. GenECA captures audio and visual signals from standard devices to analyze nonverbal features such as facial expressions, vocal tone, gaze, and posture. This information is used to generate context-aware dialogue and synchronize the agent&#x27;s speech with dynamic gestures and backchannel facial animations in real time. GenECA provides the first ECA system able to deliver context-aware speech and well-timed animations in real-time without reliance on human operators. Through modular design, it can support a wide variety of applications, such as education, customer service, and therapy. Our research enables rapid prototyping and deployment of interactive virtual agents."
   ],
   "p1": 3541,
   "pn": 3542
  },
  "raju25_interspeech": {
   "authors": [
    [
     "Giri",
     "Raju"
    ],
    [
     "Sandeep",
     "Konam"
    ]
   ],
   "title": "End-to-End Indian Language Dubbing with Zero-Shot Speaker Preservation",
   "original": "2835",
   "order": 1013,
   "page_count": 2,
   "abstract": [
    "This paper presents an end-to-end AI-driven dubbing platform designed specifically for Indian languages. The system leverages state-of-the-art speech models (ASR, MT, TTS) to streamline the dubbing process, minimizing manual effort while enabling precise output control. It generates high-quality, natural-sounding speech, preserving speaker characteristics through zero-shot synthesis irrespective of accent, age, or gender. Already successfully deployed with creators and educators, the platform enhances content accessibility and cross-lingual adaptation in education and entertainment for India&#x27;s diverse linguistic communities."
   ],
   "p1": 4969,
   "pn": 4970
  },
  "nethil25_interspeech": {
   "authors": [
    [
     "Kumarmanas",
     "Nethil"
    ],
    [
     "Vaibhav",
     "Mishra"
    ],
    [
     "Kriti",
     "Anandan"
    ],
    [
     "Kavya",
     "Manohar"
    ]
   ],
   "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms",
   "original": "2838",
   "order": 67,
   "page_count": 2,
   "abstract": [
    "We propose an open-source framework for Command-style dictation that addresses the gap between resource-intensive Online systems and high-latency Batch processing. Our approach uses Voice Activity Detection (VAD) to segment audio and transcribes these segments in parallel using Whisper models, enabling efficient multiplexing across audios. Unlike proprietary systems like SuperWhisper, this framework is also compatible with most ASR architectures, including widely used CTC-based models. Our multiplexing technique maximizes compute utilization in real-world settings, as demonstrated by its deployment in around 15% of India&#x27;s courtrooms. Evaluations on live data show consistent latency reduction as user concurrency increases, compared to sequential batch processing. The live demonstration will showcase our open-sourced implementation and allow attendees to interact with it in real-time."
   ],
   "p1": 308,
   "pn": 309
  },
  "blouir25_interspeech": {
   "authors": [
    [
     "Sam",
     "Blouir"
    ],
    [
     "Celeste",
     "Watkins"
    ],
    [
     "Milind",
     "Agarwal"
    ],
    [
     "Poorvi",
     "Acharya"
    ],
    [
     "Belu",
     "Ticona"
    ],
    [
     "Defne",
     "Circi"
    ],
    [
     "Flavia",
     "Negrete"
    ],
    [
     "Staci",
     "Chan"
    ],
    [
     "Jonathan",
     "Mbuya Kbala"
    ],
    [
     "Syeda Sabrina",
     "Akter"
    ],
    [
     "Joshua",
     "Otten"
    ],
    [
     "Ruoyu",
     "Xie"
    ],
    [
     "Kourosh",
     "Baghaei"
    ],
    [
     "Christina B",
     "Downing"
    ],
    [
     "Antonis",
     "Anastasopoulos"
    ],
    [
     "Amarda",
     "Shehu"
    ]
   ],
   "title": "SLP Sidekick: An Open-Source, Multilingual Speech Therapy Platform",
   "original": "2839",
   "order": 1014,
   "page_count": 2,
   "abstract": [
    "Speech-language impairments affect millions globally, yet access to treatment is limited by clinician shortages, high costs, and linguistic barriers. We present SLP Sidekick, an open-source AI system that enhances speech-language therapy accessibility through multilingual, culturally adapted exercises powered by automatic speech recognition (ASR) and large language models (LLMs). The system offers adaptive feedback, automated session documentation, and voice-to-voice interactions suitable for diverse linguistic and cultural contexts. We detail the system’s architecture, workflow, theoretical foundations, and potential impact, complemented by illustrative images and a demonstration video."
   ],
   "p1": 4971,
   "pn": 4972
  },
  "moore25_interspeech": {
   "authors": [
    [
     "Roger",
     "Moore"
    ]
   ],
   "title": "From Talking and Listening Devices to Intelligent Communicative Machines",
   "original": "Keynote1",
   "order": 1,
   "page_count": 0,
   "abstract": [
    "<h4>Abstract</h4>\n<p>\nHaving been 'in the business' of speech technology for over 50 years, I've had the pleasure of witnessing (and being involved first-hand) in many of the astounding developments that have led to the incredible solutions we have today. Indeed, my involvement in the field of spoken language has been somewhat of a love affair, and it's been a huge honour and privilege to have been working with so many excellent researchers on \"the most sophisticated behaviour of the most complex organism in the known universe\"! Although I've always been heavily committed to the establishment of machine learning approaches to spoken language processing - including publishing one of the first papers on the application of artificial neural networks to automatic speech recognition - my approach has always been one of attempting to uncover the underlying mechanisms of 'intelligent' (speech-based) interaction, on the basis that living systems are remarkably data-efficient in their learning. This talk will both look back (rather a long way) and look forward, asking the question how did we get here and where are we going? I hope that some of my insights may inspire others to follow a similar path.</p>\n<h4>Biography</h4>\n<p>\nProf. Moore has over 50 years’ experience in Speech Technology R&D and, although an engineer by training, much of his research has been based on insights from human speech perception and production. He studied Computer & Communications Engineering at the University of Essex and was awarded the B.A. (Hons.) degree in 1973. He subsequently received the M.Sc.(Res.) and Ph.D. degrees from the same university in 1975 and 1977 respectively, both theses being on the topic of automatic speech recognition. After a period of post-doctoral research in the Phonetics Department at University College London, Prof. Moore was recruited in 1980 to establish a speech recognition research team at the Royal Signals and Radar Establishment (RSRE) in Malvern. As Head of the UK Government's Speech Research Unit from 1985 to 1999, he was responsible for the development of the Aurix range of speech technology products and the subsequent formation of 20/20 Speech Ltd. Since 2004 he has been Professor of Spoken Language Processing at the University of Sheffield, and also holds Visiting Chairs at Bristol Robotics Laboratory and University College London Psychology & Language Sciences. Since joining Sheffield, his research has focused on understanding the fundamental principles of speech-based interaction, and in 2017 he initiated the first in the series of international workshops on ‘Vocal Interactivity in-and-between Humans, Animals and Robots' (VIHAR).\nAs President of both the European Speech Communication Association (ESCA) and Permanent Council of the International Conference on Spoken Language Processing (PC-ICSLP) from 1997, Prof. Moore pioneered their integration to form the International Speech Communication Association (ISCA). He was subsequently General Chair for INTERSPEECH-2009 and ISCA Distinguished Lecturer during 2014-15. He has received several awards, including the UK Institute of Acoustics Tyndall Medal for “distinguished work in the field of speech research and technology“, the NATO RTO Scientific Achievement Award for “repeated contribution in scientific and technological cooperation”, the LREC Antonio Zampoli Prize for \"Outstanding Contributions to the Advancement of Language Resources & Language Technology Evaluation within Human Language Technologies\", and the ISCA Special Service Medal for \"Service in the establishment, leadership and international growth of ISCA\". Prof. Moore is the current Editor-in-Chief of Computer Speech & Language, and Associate Editor for Speech Communication, Languages, the Journal of Future Robot Life, and Frontiers in Robotics and AI (Computational Intelligence in Robotics).</p>\n"
   ],
   "p1": "",
   "pn": ""
  },
  "waibel25_interspeech": {
   "authors": [
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "From Speech Science to Language Transparence",
   "original": "Keynote2",
   "order": 354,
   "page_count": 0,
   "abstract": [
    "<h4>Abstract</h4>\n<p>\nBreaking down language barriers has been a dream of centuries. Seemingly unsolvable, we are now lucky to live in the one generation that makes global communication a common reality. Such global transformation was not thought to be possible, and has only become possible through revolutionary advances in AI, language and speech processing. Indeed, the challenges of processing spoken language have required, caused, guided and motivated the most impactful advances in AI. During a time of knowledge-based speech and language processing, I became convinced that only data-driven machine learning can reasonably be expected to handle the complexities, the uncertainty, and variability of communication, and that only latent learned representations would be able to abstract and fuse new and complementary knowledge. It turned out to work beyond our wildest expectations. Starting with small shift-invariant time-delay neural networks (TDNN’s) for phonemes, we would eventually scale neural systems to massive speech, language and interpretating systems. From small vocabulary recognition, we could advance to simultaneous interpretation, summarization, interactive dialog, multimodal systems and now automatic lip-synchronous dubbing. Despite the data-driven machine learning, however, speech science was necessary to inspire the models, and observing human communication continues to motivate our ongoing work in AI. In the first part of my talk, I will revisit some of our earliest prototypes, demonstrators, and their transition into start-up companies and products in the real world. I will highlight the research advances that took us there from poorly performing early attempts to human parity on popular performance benchmarks and the lessons learned. In the second part I will discuss current research and a roadmap for the future: the dream of a language-barrier free world between all the peoples on the planet has not yet been reached. What is the missing science and how can we approach the remaining challenges? What do we learn from human speech interaction, and what would future machine learning models have to look like to better emulate and engage in human interaction? What are the opportunities, and lessons learned for students, scientists, and entrepreneurs? The talk will include demos and examples of SOTA speech translation and dubbing systems.</p>\n<h4>Biography</h4>\n<p>\nAlexander Waibel is Professor of Computer Science at Carnegie Mellon University (USA) and at the Karlsruhe Institute of Technology (Germany). He is director of the International Center for Advanced Communication Technologies. Waibel is known for his work on AI, Machine Learning, Multimodal Interfaces and Speech Translation Systems. He proposed early Neural Network based Speech and Language systems, including in 1987 the TDNN, the first shift-invariant (“Convolutional”) Neural Network, and early Neural Speech and Language systems. Based on advances in ML, he and his team developed early (’93-’98) multimodal interfaces including the first emotion recognizer, face tracker, lipreader, error repair system, a meeting browser, support for smart rooms and human-robot collaboration. Waibel pioneered many cross-lingual communication systems that now overcome language barriers via speech and image interpretation: first consecutive (1992) and simultaneous (2005) speech translation systems, road sign translator, heads-up display translation goggles, face/lip and EMG translators. Waibel founded & co-founded more than 10 companies and various non-profit services to transition results from academic work to practical deployment. This included “Jibbigo LLC” (2009), the first speech translator on a phone (acquired by Facebook 2013), “M*Modal” medical transcription and reporting (acquired by Medquist and 3M), “Kites” interpreting services for subtitling and video conferencing (acquired by Zoom in 2021), “Lecture Translator”, the first automatic simultaneous translation service (2012) at Universities and European Parliament, and STS services for medical missions/disaster relief.\nWaibel published ~1,000 articles, books, and patents. He is a member of the National Academy of Sciences of Germany, Life-Fellow of IEEE, Fellow of ISCA, Fellow of the Explorers Club, and Research Fellow at Zoom. Waibel received many awards, including the IEEE Flanagan award, the ICMI sustained achievement award, the Meta prize, the A. Zampolli award, and the Alcatel-SEL award. He received his BS from MIT, and MS and PhD degrees from CMU.</p>\n"
   ],
   "p1": "",
   "pn": ""
  },
  "espywilson25_interspeech": {
   "authors": [
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Speech Kinematic Analysis from Acoustics: Scientific, Clinical and Practical Applications",
   "original": "Keynote3",
   "order": 707,
   "page_count": 0,
   "abstract": [
    "<h4>Abstract</h4>\n<p>\nMuch of my research has involved studying how small changes in the spatiotemporal coordination of speech articulators affect variability in the acoustic characteristics of the speech signal. This interest in speech variability ultimately led me to develop a speech inversion (SI) system that recovers articulatory movements of the lips, tongue tip, and tongue body from the speech signal. Recently, we were able to extend the SI system to provide information about the velopharyngeal port opening (nasality) and will soon investigate a methodology to uncover information about the tongue root and the size of the glottal opening. Our SI system has proven to be speaker independent and generalizes well across acoustic databases. In this talk, I will explain how we developed the SI system, and ways in which we have used it to date: for clinical purposes in mental health and speech disorder assessment, in scientific analysis of cross-linguistic speech patterns, and for improving automatic speech recognition.</p>\n<h4>Biography</h4>\n<p>\nCarol Espy-Wilson is a full professor in the Electrical and Computer Engineering Department and the Institute for Systems Research at the University of Maryland College Park. She received her BS in electrical engineering from Stanford University and her MS, EE and PhD degrees in electrical engineering from the Massachusetts Institute of Technology. Dr. Espy-Wilson is a Fellow of the Acoustical Society of America (ASA), the International Speech Communication Association (ISCA) and the IEEE. She was recently elected VP-elect of ASA, and to the ISCA Advisory Board. She is currently serving on the Editorial Board of Computer, Speech and Language. She has been Chair of the Speech Communication Technical Committee of ASA, elected member of the Speech and Language Technical Committee of IEEE and Associate Editor of the Journal of the Acoustical Society of America. Finally, at the National Institutes of Health, she has served on the Advisory Councils for the National Institute on Deafness and Communication Disorders and the National Institutes of Biomedical Imaging and Bioengineering, on the Medical Rehabilitation Advisory Board of the National Institute of Child Health and Human Development, and she has been a member of the Language and Communication Study Section.\nCarol directs the Speech Communication Lab where they combine digital signal processing, speech science, linguistics and machine learning to conduct research in speech communication. Current research projects include speech inversion, mental health assessment based on speech, video and text, speech recognition for elementary school classrooms, entrainment based on articulatory and facial gestures in unstructured conversations between neurotypical and neurodiverse participants, and speech enhancement. Her laboratory has received federal funding (NSF, NIH and DoD) and industry grants and she has 13 patents.</p>\n"
   ],
   "p1": "",
   "pn": ""
  },
  "holler25_interspeech": {
   "authors": [
    [
     "Judith",
     "Holler"
    ]
   ],
   "title": "Using and comprehending language in face-to-face conversation",
   "original": "Keynote4",
   "order": 1075,
   "page_count": 0,
   "abstract": [
    "<h4>Abstract</h4>\n<p>\nFace-to-face conversational interaction is at the very heart of human sociality and the natural ecological niche in which language has evolved and is acquired. Yet, we still know rather little about how utterances are produced and comprehended in this environment. In this talk, I will focus on how hand gestures, facial and head movements are organised to convey semantic and pragmatic meaning in conversation, as well as on how the presence and timing of these signals impacts utterance comprehension and responding. Specifically, I will present studies based on complementary approaches, which feed into and inform one another. This includes qualitative and quantitative multimodal corpus studies showing that visual signals indeed often occur early, and experimental comprehension studies, which are based on and inspired by the corpus results, implementing controlled manipulations to test for causal effects between visual bodily signals and comprehension processes and mechanisms. These experiments include behavioural and EEG studies, most of them using multimodally animated virtual characters. Together, the findings provide evidence for the hypothesis that visual bodily signals form an integral part of semantic and pragmatic meaning communication in conversational interaction, and that they facilitate language processing, especially due to their timing and the predictive potential they gain through their temporal orchestration.</p>\n<h4>Biography</h4>\n<p>\nJudith Holler is Associate Professor at the Donders Institute for Brain, Cognition & Behaviour, Radboud University where she leads the research group Communication in Social Interaction, and senior investigator at the Max Planck Institute for Psycholinguistics. Her research program investigates human language in the very environment in which it has evolved, is acquired, and used most: face-to-face interaction. Within this context, Judith focuses on the semantics and pragmatics of human communication from a multimodal perspective considering spoken language within the rich, visual infrastructure that embeds it, such as manual gestures, head movements, facial signals, and gaze. She uses a combination of methods from different fields to investigate human multimodal communication, including quantitative conversational corpus analyses, in-situ eyetracking, behavioural and neurocognitive experimentation using multimodal language stimuli involving virtual animations. Her research has been supported by a range of prestigious research grants from funders including the European Research Council (EU), The Dutch Science Foundation (NWO), Marie Curie Fellowships (EU), Economic & Social Research Council (UK), Parkinson UK, The Leverhulme Trust (UK), the British Academy (UK), Volkswagen Stiftung (Germany) and the German Science Foundation (DFG, Mercator Fellowships).</p>\n"
   ],
   "p1": "",
   "pn": ""
  }
 },
 "sessions": [
  {
   "title": "Keynote1 - Roger Moore: From Talking and Listening Devices to Intelligent Communicative Machines",
   "papers": [
    "moore25_interspeech"
   ]
  },
  {
   "title": "Spoken Machine Translation 1",
   "papers": [
    "ducceschi25_interspeech",
    "subramanian25_interspeech",
    "kumar25c_interspeech",
    "ou25_interspeech",
    "higuchi25_interspeech",
    "pu25_interspeech",
    "gallego25_interspeech",
    "futami25_interspeech",
    "pothula25_interspeech",
    "wang25i_interspeech"
   ]
  },
  {
   "title": "Real-time Speech Enhancement",
   "papers": [
    "pei25_interspeech",
    "lu25_interspeech",
    "ma25c_interspeech",
    "song25d_interspeech",
    "gao25_interspeech",
    "b25_interspeech"
   ]
  },
  {
   "title": "Multilinguality, Cross-linguistic Studies, L2 Speech",
   "papers": [
    "zhou25b_interspeech",
    "wu25l_interspeech",
    "magoshi25_interspeech",
    "hsu25_interspeech",
    "bakkouche25b_interspeech",
    "yazawa25_interspeech",
    "hamann25_interspeech",
    "chan25_interspeech",
    "white25_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition 1",
   "papers": [
    "mote25b_interspeech",
    "phukan25b_interspeech",
    "fang25b_interspeech",
    "chang25d_interspeech",
    "bijoy25_interspeech",
    "gong25b_interspeech"
   ]
  },
  {
   "title": "Multimodal Resources",
   "papers": [
    "ren25_interspeech",
    "xu25j_interspeech",
    "nguyen25d_interspeech",
    "wu25e_interspeech",
    "peng25b_interspeech",
    "inoue25_interspeech",
    "dong25c_interspeech",
    "xu25_interspeech",
    "sofer25_interspeech"
   ]
  },
  {
   "title": "Interpretability in Audio and Speech Technology",
   "papers": [
    "yin25_interspeech",
    "fucci25_interspeech",
    "bolanos25_interspeech",
    "chang25b_interspeech",
    "onda25_interspeech",
    "ashihara25_interspeech",
    "getman25_interspeech",
    "patakis25_interspeech",
    "ersoy25_interspeech",
    "meng25b_interspeech",
    "bentum25_interspeech",
    "deheerkloots25_interspeech",
    "huo25_interspeech",
    "shen25b_interspeech",
    "leschly25_interspeech"
   ]
  },
  {
   "title": "Summarization",
   "papers": [
    "freisinger25_interspeech",
    "kano25_interspeech",
    "istaiteh25_interspeech",
    "gong25c_interspeech"
   ]
  },
  {
   "title": "Show and Tell 1: ASR / Tools",
   "papers": [
    "deluca25_interspeech",
    "rai25_interspeech",
    "draxler25_interspeech",
    "vukovic25_interspeech",
    "li25ea_interspeech",
    "znotins25_interspeech",
    "nethil25_interspeech"
   ]
  },
  {
   "title": "Models of Speech Production",
   "papers": [
    "lu25h_interspeech",
    "dindart25_interspeech",
    "lee25d_interspeech",
    "tabatabaee25b_interspeech"
   ]
  },
  {
   "title": "Speech and Grammar/Articulatory Analyses",
   "papers": [
    "stein25_interspeech",
    "malisz25_interspeech",
    "yuen25_interspeech",
    "xu25h_interspeech",
    "kakouros25_interspeech",
    "shao25c_interspeech",
    "birkholz25_interspeech",
    "hopponen25_interspeech"
   ]
  },
  {
   "title": "Speaking Styles, Register and Conversational Speech",
   "papers": [
    "xiang25b_interspeech",
    "qian25b_interspeech",
    "niculescu25_interspeech",
    "bodur25_interspeech",
    "batchelderschwab25_interspeech"
   ]
  },
  {
   "title": "Emotional Distress in Speech",
   "papers": [
    "krzywdziak25_interspeech",
    "wu25_interspeech",
    "gao25e_interspeech",
    "chen25o_interspeech",
    "roquefort25_interspeech",
    "kim25p_interspeech",
    "martin25_interspeech"
   ]
  },
  {
   "title": "Prosody in Speech Synthesis",
   "papers": [
    "eren25_interspeech",
    "lee25f_interspeech",
    "mayer25_interspeech",
    "ogura25_interspeech",
    "mondal25b_interspeech",
    "lee25c_interspeech"
   ]
  },
  {
   "title": "Depression Detection and Assessment 1",
   "papers": [
    "white25b_interspeech",
    "liang25_interspeech",
    "zhou25f_interspeech",
    "prakrankamanant25_interspeech",
    "dumpala25_interspeech",
    "zuo25_interspeech",
    "loweimi25_interspeech",
    "lewis25_interspeech",
    "young25_interspeech"
   ]
  },
  {
   "title": "Speech Analysis, Detection and Classification 1",
   "papers": [
    "li25c_interspeech",
    "tripathi25_interspeech",
    "kim25c_interspeech",
    "kim25d_interspeech",
    "choi25h_interspeech",
    "chowdhury25_interspeech"
   ]
  },
  {
   "title": "Speech-based Cognitive Assessment 1",
   "papers": [
    "yong25b_interspeech",
    "gogoi25_interspeech",
    "liu25k_interspeech",
    "jia25_interspeech",
    "sun25b_interspeech"
   ]
  },
  {
   "title": "Large Language Models in Speech Recognition",
   "papers": [
    "ma25_interspeech",
    "zhengjie25_interspeech",
    "su25b_interspeech",
    "zhang25t_interspeech",
    "prakash25_interspeech",
    "xu25k_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Echo Cancellation",
   "papers": [
    "gan25b_interspeech",
    "kim25o_interspeech",
    "wan25_interspeech",
    "wu25f_interspeech",
    "chae25b_interspeech",
    "zhang25l_interspeech",
    "guo25c_interspeech",
    "choi25d_interspeech",
    "zhao25e_interspeech"
   ]
  },
  {
   "title": "Decoding Algorithms",
   "papers": [
    "okabe25_interspeech",
    "xu25c_interspeech",
    "bataev25_interspeech",
    "grigoryan25_interspeech",
    "mittal25_interspeech",
    "kwok25c_interspeech"
   ]
  },
  {
   "title": "Queer and Trans Speech Science and Technology",
   "papers": [
    "mcallister25_interspeech",
    "netzorg25_interspeech",
    "ross25_interspeech",
    "siegert25_interspeech",
    "hartmann25_interspeech",
    "hope25_interspeech"
   ]
  },
  {
   "title": "Tone",
   "papers": [
    "dong25d_interspeech",
    "lu25f_interspeech",
    "li25x_interspeech",
    "li25w_interspeech",
    "zhang25b_interspeech",
    "du25_interspeech"
   ]
  },
  {
   "title": "Cross-Lingual and Multilingual Processing",
   "papers": [
    "marmor25_interspeech",
    "klejch25_interspeech",
    "hameed25_interspeech",
    "li25ca_interspeech",
    "yan25c_interspeech",
    "shao25b_interspeech",
    "nguyen25_interspeech",
    "vesterbacka25_interspeech"
   ]
  },
  {
   "title": "Echo Cancellation, Feedback Control, and Near-end Enhancement",
   "papers": [
    "zhao25b_interspeech",
    "wang25d_interspeech",
    "wang25o_interspeech",
    "dinh25_interspeech",
    "villani25_interspeech",
    "wu25d_interspeech",
    "lay25_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 1",
   "papers": [
    "liu25f_interspeech",
    "ys25_interspeech",
    "ghosh25_interspeech",
    "sanguedolce25_interspeech"
   ]
  },
  {
   "title": "Hearing Disorders",
   "papers": [
    "araizaillan25_interspeech",
    "cheema25_interspeech",
    "jin25c_interspeech",
    "chiang25_interspeech",
    "hao25_interspeech",
    "wang25l_interspeech",
    "ding25_interspeech"
   ]
  },
  {
   "title": "Interspeech 2025 URGENT Challenge",
   "papers": [
    "zhang25j_interspeech",
    "saijo25_interspeech",
    "rong25_interspeech",
    "le25b_interspeech",
    "sun25d_interspeech",
    "serbest25_interspeech",
    "goswami25_interspeech",
    "chao25b_interspeech"
   ]
  },
  {
   "title": "Spoken Machine Translation 2",
   "papers": [
    "rouas25_interspeech",
    "mohammadamini25_interspeech",
    "r25_interspeech",
    "le25_interspeech",
    "blaschke25_interspeech",
    "javed25_interspeech"
   ]
  },
  {
   "title": "Spatial Audio and Acoustics 1",
   "papers": [
    "lyu25_interspeech",
    "zhang25e_interspeech",
    "ick25_interspeech",
    "fu25_interspeech",
    "chen25l_interspeech",
    "xiao25_interspeech"
   ]
  },
  {
   "title": "Articulatory and Vocal Tract Modelling",
   "papers": [
    "berthommier25_interspeech",
    "vurma25_interspeech",
    "funk25_interspeech",
    "piyadasa25_interspeech",
    "wu25j_interspeech",
    "azzouz25_interspeech",
    "zhang25i_interspeech"
   ]
  },
  {
   "title": "Acoustic Assessment of Respiratory Health",
   "papers": [
    "vanbemmel25_interspeech",
    "yan25b_interspeech",
    "niizumi25_interspeech",
    "reinders25_interspeech",
    "dong25e_interspeech",
    "wei25_interspeech",
    "jeong25_interspeech",
    "toikkanen25_interspeech"
   ]
  },
  {
   "title": "Advances in Modelling and Imaging",
   "papers": [
    "guillaume25_interspeech",
    "simko25_interspeech",
    "guo25b_interspeech",
    "chen25e_interspeech",
    "zhu25c_interspeech",
    "yan25_interspeech",
    "sharma25b_interspeech",
    "smith25b_interspeech"
   ]
  },
  {
   "title": "Conversation, Communication and Interaction 1",
   "papers": [
    "heo25_interspeech",
    "oconnorrussell25_interspeech",
    "fukunaga25_interspeech",
    "baihaqi25_interspeech",
    "huttner25_interspeech",
    "cavalcanti25_interspeech"
   ]
  },
  {
   "title": "Robust Speaker Verification",
   "papers": [
    "lepage25_interspeech",
    "kim25n_interspeech",
    "li25z_interspeech",
    "ferrofilho25_interspeech",
    "chen25_interspeech",
    "li25h_interspeech"
   ]
  },
  {
   "title": "Multilingual ASR",
   "papers": [
    "mimura25_interspeech",
    "yang25m_interspeech",
    "li25p_interspeech",
    "bagat25_interspeech",
    "yong25_interspeech",
    "mehralian25_interspeech",
    "ozyilmaz25_interspeech",
    "zhuo25_interspeech",
    "wang25_interspeech"
   ]
  },
  {
   "title": "Multi-channel Speech Enhancement",
   "papers": [
    "yang25k_interspeech",
    "wang25h_interspeech",
    "shen25_interspeech",
    "han25c_interspeech",
    "alip25_interspeech",
    "qin25_interspeech"
   ]
  },
  {
   "title": "Self-supervised Learning",
   "papers": [
    "cui25_interspeech",
    "vaessen25_interspeech",
    "kutsakov25_interspeech",
    "chi25_interspeech",
    "onda25c_interspeech",
    "whetten25_interspeech"
   ]
  },
  {
   "title": "Singing Voice and Audio Synthesis",
   "papers": [
    "choi25e_interspeech",
    "yang25f_interspeech",
    "yang25h_interspeech",
    "liu25h_interspeech",
    "gu25_interspeech",
    "zhao25g_interspeech",
    "chen25d_interspeech",
    "zhou25e_interspeech",
    "chae25_interspeech"
   ]
  },
  {
   "title": "Acoustic and Articulatory Cues in Speech Perception",
   "papers": [
    "dong25_interspeech",
    "gaudrain25_interspeech",
    "janse25_interspeech",
    "xiong25b_interspeech",
    "rachman25_interspeech",
    "fan25b_interspeech"
   ]
  },
  {
   "title": "Audio Event Detection and Classification",
   "papers": [
    "yoshinaga25_interspeech",
    "fang25d_interspeech",
    "si25_interspeech",
    "monteroramirez25_interspeech",
    "acevedo25_interspeech",
    "ryu25b_interspeech"
   ]
  },
  {
   "title": "Inclusivity",
   "papers": [
    "emezue25_interspeech",
    "jahan25_interspeech",
    "altwlkany25_interspeech",
    "giraldo25_interspeech"
   ]
  },
  {
   "title": "Voice Conversion 1",
   "papers": [
    "akti25_interspeech",
    "suda25_interspeech",
    "biyani25_interspeech",
    "lobashev25_interspeech",
    "liu25j_interspeech",
    "wang25ba_interspeech",
    "ren25b_interspeech",
    "jin25d_interspeech",
    "kamper25_interspeech",
    "hu25i_interspeech"
   ]
  },
  {
   "title": "Speech-based Cognitive Assessment 2",
   "papers": [
    "ke25_interspeech",
    "akinrintoyo25_interspeech",
    "botelho25_interspeech",
    "xiao25e_interspeech",
    "mansi25_interspeech",
    "woszczyk25_interspeech"
   ]
  },
  {
   "title": "Source Separation 1",
   "papers": [
    "kim25h_interspeech",
    "jing25b_interspeech",
    "yang25_interspeech",
    "wang25t_interspeech",
    "alizadeh25_interspeech",
    "avidan25_interspeech",
    "wang25j_interspeech",
    "tao25b_interspeech",
    "jiang25_interspeech"
   ]
  },
  {
   "title": "Language and Accent Identification and Speaker Privacy",
   "papers": [
    "dey25_interspeech",
    "bafna25_interspeech",
    "premananth25b_interspeech",
    "ambikairajah25_interspeech",
    "fathan25_interspeech",
    "hu25j_interspeech",
    "karimov25_interspeech",
    "meng25_interspeech",
    "liu25i_interspeech"
   ]
  },
  {
   "title": "Source Tracing: The Origins of Synthetic or Manipulated Speech",
   "papers": [
    "falez25_interspeech",
    "kulkarni25_interspeech",
    "chen25j_interspeech",
    "stan25_interspeech",
    "negroni25_interspeech",
    "firc25_interspeech",
    "koutsianos25_interspeech",
    "xiao25c_interspeech",
    "doan25_interspeech",
    "zhao25k_interspeech",
    "klein25_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization 1",
   "papers": [
    "han25_interspeech",
    "singh25_interspeech",
    "palzer25_interspeech",
    "durmus25_interspeech",
    "tan25_interspeech",
    "horiguchi25b_interspeech"
   ]
  },
  {
   "title": "Multilingual Speech Synthesis and Special Applications 1",
   "papers": [
    "kwon25_interspeech",
    "bai25_interspeech",
    "liu25o_interspeech",
    "naseem25_interspeech",
    "rautenberg25_interspeech",
    "tannander25_interspeech"
   ]
  },
  {
   "title": "Characterization and Multimodal Approaches for Speaker Recognition",
   "papers": [
    "peng25_interspeech",
    "griot25_interspeech",
    "sun25i_interspeech",
    "shashaank25_interspeech",
    "yu25c_interspeech",
    "kc25_interspeech",
    "phukan25_interspeech",
    "ranjan25b_interspeech",
    "maeda25_interspeech",
    "kim25l_interspeech"
   ]
  },
  {
   "title": "Acoustic Analysis and Bioacoustics",
   "papers": [
    "a25_interspeech",
    "li25o_interspeech",
    "rasendiranr25_interspeech",
    "wu25c_interspeech",
    "song25_interspeech",
    "yang25b_interspeech",
    "szmajdzinski25_interspeech",
    "letellier25_interspeech",
    "terashima25_interspeech"
   ]
  },
  {
   "title": "Keynote2 - Alexander Waibel: From Speech Science to Language Transparence",
   "papers": [
    "waibel25_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems 1",
   "papers": [
    "do25_interspeech",
    "gulzar25_interspeech",
    "sedlacek25_interspeech",
    "mori25_interspeech",
    "wang25x_interspeech",
    "shi25c_interspeech",
    "koudounas25c_interspeech",
    "arisoy25_interspeech",
    "teleki25_interspeech",
    "ohashi25_interspeech"
   ]
  },
  {
   "title": "Speech Assessment",
   "papers": [
    "yang25g_interspeech",
    "wang25u_interspeech",
    "shi25f_interspeech",
    "alderete25_interspeech",
    "mizumoto25_interspeech",
    "verdini25_interspeech"
   ]
  },
  {
   "title": "Audio-Visual ASR and Multimodal System",
   "papers": [
    "zapata25_interspeech",
    "cappellazzo25_interspeech",
    "nguyen25b_interspeech",
    "li25q_interspeech",
    "makishima25b_interspeech"
   ]
  },
  {
   "title": "Speech and Voice Disorders 1",
   "papers": [
    "huang25i_interspeech",
    "ye25b_interspeech",
    "zhang25u_interspeech",
    "park25g_interspeech",
    "masson25_interspeech",
    "titeux25_interspeech",
    "bendom25_interspeech",
    "hojo25_interspeech"
   ]
  },
  {
   "title": "Multimodal Information Based Speech Processing (MISP) 2025 Challenge",
   "papers": [
    "luo25b_interspeech",
    "gao25g_interspeech",
    "li25l_interspeech",
    "cheng25b_interspeech",
    "song25c_interspeech",
    "huang25k_interspeech"
   ]
  },
  {
   "title": "Speaker Extraction 1",
   "papers": [
    "jalal25_interspeech",
    "dai25b_interspeech",
    "li25da_interspeech",
    "pan25_interspeech",
    "pan25d_interspeech"
   ]
  },
  {
   "title": "Low Resource Speech Recognition",
   "papers": [
    "carta25_interspeech",
    "smith25_interspeech",
    "zhang25m_interspeech",
    "suen25_interspeech",
    "sasu25b_interspeech",
    "sinha25_interspeech",
    "visser25_interspeech"
   ]
  },
  {
   "title": "Computational Resource Constrained ASR",
   "papers": [
    "li25_interspeech",
    "li25v_interspeech",
    "xu25e_interspeech",
    "gu25b_interspeech",
    "someki25_interspeech",
    "hilmes25_interspeech",
    "fong25_interspeech"
   ]
  },
  {
   "title": "Speech and Language Technology for Health Applications",
   "papers": [
    "pan25c_interspeech",
    "battula25_interspeech",
    "nie25_interspeech",
    "phukan25c_interspeech",
    "chen25c_interspeech",
    "sage25_interspeech",
    "polle25_interspeech",
    "wang25aa_interspeech"
   ]
  },
  {
   "title": "Responsible Speech Foundation Models + SUPERB Challenge",
   "papers": [
    "alexos25_interspeech",
    "lin25c_interspeech",
    "puhach25_interspeech",
    "geng25c_interspeech",
    "bhattacharya25b_interspeech",
    "kuan25_interspeech",
    "lu25c_interspeech",
    "manakul25_interspeech",
    "wang25z_interspeech",
    "chen25h_interspeech",
    "alumae25_interspeech"
   ]
  },
  {
   "title": "Dysarthric Speech Assessment 1",
   "papers": [
    "zhong25_interspeech",
    "jeon25_interspeech",
    "chen25m_interspeech",
    "das25b_interspeech",
    "aboeitta25_interspeech",
    "li25f_interspeech",
    "xiong25_interspeech",
    "park25c_interspeech"
   ]
  },
  {
   "title": "Show and Tell 2: Speech Synthesis",
   "papers": [
    "gourav25_interspeech",
    "nguyen25e_interspeech",
    "cho25c_interspeech",
    "shepardson25_interspeech",
    "bokkahallisatish25_interspeech",
    "arai25_interspeech"
   ]
  },
  {
   "title": "Databases and Progress in Methodology",
   "papers": [
    "rustagi25_interspeech",
    "fort25_interspeech",
    "marcinek25_interspeech",
    "shahidi25_interspeech",
    "shi25g_interspeech",
    "tuckute25_interspeech",
    "park25d_interspeech",
    "bakkouche25_interspeech",
    "onda25b_interspeech"
   ]
  },
  {
   "title": "Novel Architectures for ASR",
   "papers": [
    "ugan25_interspeech",
    "guo25d_interspeech",
    "hsieh25_interspeech",
    "sudo25_interspeech",
    "jin25_interspeech",
    "peng25c_interspeech"
   ]
  },
  {
   "title": "Deepfake Detection",
   "papers": [
    "kwok25_interspeech",
    "elkheir25_interspeech",
    "glazer25_interspeech",
    "muller25_interspeech",
    "kim25g_interspeech",
    "yang25l_interspeech"
   ]
  },
  {
   "title": "Tools for Speech Analysis",
   "papers": [
    "jin25b_interspeech",
    "dewhurst25_interspeech",
    "tanner25_interspeech"
   ]
  },
  {
   "title": "Text Processing and Evaluation for Speech Synthesis 1",
   "papers": [
    "sun25_interspeech",
    "roychowdhury25_interspeech",
    "srinivasavaradhan25_interspeech",
    "zhong25c_interspeech",
    "lameris25_interspeech",
    "kuhlmann25_interspeech"
   ]
  },
  {
   "title": "Segmental and Tonal Units",
   "papers": [
    "hui25_interspeech",
    "hrabanek25_interspeech",
    "lu25g_interspeech",
    "ariga25_interspeech",
    "wepner25_interspeech"
   ]
  },
  {
   "title": "Speech Quality Assessment",
   "papers": [
    "deoliveira25_interspeech",
    "wardah25_interspeech",
    "kibria25_interspeech",
    "shi25b_interspeech",
    "hoq25_interspeech",
    "huang25g_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "yang25o_interspeech",
    "parvathala25c_interspeech",
    "phaye25_interspeech",
    "behera25_interspeech",
    "han25b_interspeech",
    "parvathala25_interspeech"
   ]
  },
  {
   "title": "Language Learning and Assessment",
   "papers": [
    "kuparinen25_interspeech",
    "harmsen25_interspeech",
    "kumar25d_interspeech",
    "parikh25b_interspeech",
    "elkheir25b_interspeech",
    "hsieh25b_interspeech",
    "geng25_interspeech",
    "oh25_interspeech",
    "xie25b_interspeech",
    "phan25_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Paradigms and Methods 1",
   "papers": [
    "park25b_interspeech",
    "zheng25d_interspeech",
    "gao25d_interspeech",
    "li25b_interspeech",
    "zhang25c_interspeech",
    "lu25b_interspeech",
    "zalkow25_interspeech",
    "lu25e_interspeech"
   ]
  },
  {
   "title": "Spatial Audio and Acoustics 2",
   "papers": [
    "hartanto25_interspeech",
    "zhao25i_interspeech",
    "kim25v_interspeech",
    "yang25q_interspeech",
    "hu25m_interspeech",
    "tao25_interspeech",
    "hu25h_interspeech",
    "guzik25_interspeech",
    "zhang25f_interspeech"
   ]
  },
  {
   "title": "Text Processing and Evaluation for Speech Synthesis 2",
   "papers": [
    "ohnaka25_interspeech",
    "berger25_interspeech",
    "mak25_interspeech",
    "hu25_interspeech",
    "lemaguer25_interspeech",
    "miniconi25_interspeech",
    "setoguchi25_interspeech"
   ]
  },
  {
   "title": "General Topics in ASR",
   "papers": [
    "rossenbach25_interspeech",
    "hu25e_interspeech",
    "lin25b_interspeech",
    "he25_interspeech",
    "xue25_interspeech"
   ]
  },
  {
   "title": "Acoustic Event Detection and Classification",
   "papers": [
    "taylor25_interspeech",
    "uehara25_interspeech",
    "dai25_interspeech",
    "komatsu25_interspeech",
    "ishikawa25_interspeech",
    "fujita25b_interspeech",
    "wang25r_interspeech",
    "jiang25b_interspeech",
    "hoang25_interspeech"
   ]
  },
  {
   "title": "Keyword Spotting and Retrieval",
   "papers": [
    "singh25d_interspeech",
    "singh25c_interspeech",
    "hu25b_interspeech",
    "jung25b_interspeech",
    "fang25c_interspeech",
    "choi25f_interspeech",
    "yang25n_interspeech",
    "lim25_interspeech",
    "saladukha25_interspeech",
    "zhu25b_interspeech",
    "zhang25h_interspeech",
    "alam25_interspeech"
   ]
  },
  {
   "title": "Multimodal Systems",
   "papers": [
    "lee25_interspeech",
    "he25c_interspeech",
    "phukan25f_interspeech",
    "liu25g_interspeech",
    "hannan25_interspeech",
    "hu25f_interspeech",
    "wang25m_interspeech",
    "tang25_interspeech",
    "wang25w_interspeech"
   ]
  },
  {
   "title": "Dysarthric Speech Assessment 2",
   "papers": [
    "szekely25_interspeech",
    "degroot25_interspeech",
    "kim25w_interspeech",
    "m25_interspeech",
    "li25n_interspeech",
    "elhajal25_interspeech"
   ]
  },
  {
   "title": "Dialect Identification in Different Languages",
   "papers": [
    "gutscher25_interspeech",
    "kumar25_interspeech",
    "elleuch25_interspeech",
    "fischbach25_interspeech",
    "parsons25_interspeech",
    "abdullah25_interspeech"
   ]
  },
  {
   "title": "Connecting Speech Science and Speech Technology for Children’s Speech",
   "papers": [
    "fan25_interspeech",
    "benway25_interspeech",
    "eads25_interspeech",
    "kulkarni25b_interspeech",
    "tabatabaee25_interspeech",
    "shi25h_interspeech",
    "zhang25r_interspeech",
    "ankita25_interspeech",
    "feng25b_interspeech",
    "xu25g_interspeech",
    "kunze25_interspeech",
    "gao25c_interspeech",
    "vidal25_interspeech",
    "raman25_interspeech",
    "gebauer25_interspeech",
    "horii25_interspeech",
    "marx25_interspeech",
    "ahadzi25_interspeech",
    "rolland25_interspeech",
    "rosero25_interspeech",
    "balajishankar25_interspeech",
    "pratapsingh25_interspeech"
   ]
  },
  {
   "title": "Brain and Cognition",
   "papers": [
    "moussa25_interspeech",
    "sharon25_interspeech",
    "duraisamy25_interspeech",
    "shams25_interspeech",
    "ivucic25_interspeech",
    "alradhi25_interspeech"
   ]
  },
  {
   "title": "Regional, Social and Diachronic Variation",
   "papers": [
    "silveira25_interspeech",
    "marchini25_interspeech",
    "popescu25_interspeech",
    "bressensdorf25_interspeech",
    "mcgahay25_interspeech",
    "zhao25_interspeech"
   ]
  },
  {
   "title": "Speaker Extraction 2",
   "papers": [
    "navon25_interspeech",
    "serre25_interspeech",
    "alcalapadilla25_interspeech",
    "zhao25f_interspeech",
    "yu25b_interspeech",
    "kienegger25_interspeech"
   ]
  },
  {
   "title": "Multimodal Emotion Recognition",
   "papers": [
    "sun25c_interspeech",
    "chandra25_interspeech",
    "chochlakis25_interspeech",
    "markitantov25_interspeech"
   ]
  },
  {
   "title": "Conversation, Communication and Interaction 2",
   "papers": [
    "elmers25_interspeech",
    "paierl25_interspeech",
    "slomianka25_interspeech",
    "charuau25_interspeech",
    "pawlowski25_interspeech",
    "kawanishi25_interspeech",
    "mori25b_interspeech"
   ]
  },
  {
   "title": "Multimodal Speech and Language Processing in Healthcare Settings",
   "papers": [
    "kommineni25_interspeech",
    "mun25b_interspeech",
    "tisdale25_interspeech",
    "premananth25_interspeech",
    "bn25_interspeech",
    "wu25m_interspeech"
   ]
  },
  {
   "title": "Music and Audio Analysis",
   "papers": [
    "huang25_interspeech",
    "hsieh25c_interspeech",
    "phukan25d_interspeech",
    "cr25_interspeech",
    "jing25_interspeech",
    "xu25b_interspeech",
    "liang25c_interspeech",
    "gupta25_interspeech",
    "gupta25b_interspeech"
   ]
  },
  {
   "title": "Audio Analysis, Generation and Assessment",
   "papers": [
    "tian25_interspeech",
    "takeuchi25_interspeech",
    "choi25_interspeech",
    "ahn25b_interspeech",
    "lee25b_interspeech",
    "zhang25q_interspeech",
    "kanamori25_interspeech",
    "lechler25_interspeech",
    "balasubramanian25_interspeech"
   ]
  },
  {
   "title": "Other Topics in Speech Recognition",
   "papers": [
    "cm25_interspeech",
    "lin25g_interspeech",
    "park25f_interspeech",
    "tadevosyan25_interspeech",
    "minixhofer25_interspeech",
    "tran25_interspeech",
    "linke25_interspeech",
    "serditova25_interspeech"
   ]
  },
  {
   "title": "Privacy and Anonymization",
   "papers": [
    "cheng25d_interspeech",
    "liu25b_interspeech",
    "yao25_interspeech",
    "franzreb25_interspeech",
    "vauquier25_interspeech"
   ]
  },
  {
   "title": "Language Modeling for Conversational Systems",
   "papers": [
    "hsiao25_interspeech",
    "dao25_interspeech",
    "mousavi25_interspeech",
    "mitrofanov25_interspeech",
    "mai25_interspeech",
    "tian25b_interspeech",
    "chien25_interspeech"
   ]
  },
  {
   "title": "Speech Accessibility Project Challenge",
   "papers": [
    "zheng25_interspeech",
    "gohider25_interspeech",
    "ducorroy25_interspeech",
    "laquatra25_interspeech",
    "baumann25_interspeech",
    "wagner25_interspeech",
    "wang25g_interspeech",
    "takahashi25_interspeech",
    "tan25b_interspeech"
   ]
  },
  {
   "title": "Neural Network Training Methods 1",
   "papers": [
    "mitsumori25_interspeech",
    "agrawal25b_interspeech",
    "ratajczak25_interspeech",
    "oh25c_interspeech",
    "novitasari25b_interspeech",
    "lu25d_interspeech"
   ]
  },
  {
   "title": "Diversity: Age, Sex, Gender, Ethnicity, and More",
   "papers": [
    "lin25e_interspeech",
    "weirich25_interspeech",
    "rommel25_interspeech",
    "weilinghoff25_interspeech",
    "amoniyan25_interspeech",
    "mohsin25_interspeech"
   ]
  },
  {
   "title": "Anomalous Sound Detection",
   "papers": [
    "wang25n_interspeech",
    "wang25p_interspeech",
    "wu25b_interspeech",
    "jiang25c_interspeech",
    "zhou25d_interspeech",
    "niu25_interspeech"
   ]
  },
  {
   "title": "Far-field and Robust Speech Recognition",
   "papers": [
    "luo25c_interspeech",
    "zhao25h_interspeech",
    "wang25b_interspeech",
    "ahn25_interspeech",
    "kamo25_interspeech",
    "panda25_interspeech",
    "ofaolain25_interspeech",
    "kong25b_interspeech",
    "zhao25c_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Paradigms and Methods 2",
   "papers": [
    "fernandez25_interspeech",
    "chen25b_interspeech",
    "choi25c_interspeech",
    "liang25e_interspeech",
    "huang25c_interspeech",
    "wu25k_interspeech"
   ]
  },
  {
   "title": "Keynote3 - Carol Espy-Wilson: Speech Kinematic Analysis from Acoustics: Scientific, Clinical and Practical Applications",
   "papers": [
    "espywilson25_interspeech"
   ]
  },
  {
   "title": "Articulatory Analyses",
   "papers": [
    "birkholz25b_interspeech",
    "mcguire25_interspeech",
    "szalay25b_interspeech",
    "huang25f_interspeech",
    "proctor25_interspeech",
    "lo25_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Analysis and Representation",
   "papers": [
    "hartuv25_interspeech",
    "khurana25_interspeech",
    "pepino25_interspeech",
    "yadav25_interspeech"
   ]
  },
  {
   "title": "Show and Tell 3: Signal Processing / Multimodal processing",
   "papers": [
    "wakayama25_interspeech",
    "lay25b_interspeech",
    "baihaqi25b_interspeech",
    "pallala25_interspeech",
    "quinterovillalobos25_interspeech",
    "okamoto25_interspeech",
    "patapati25_interspeech"
   ]
  },
  {
   "title": "Speech and Voice Disorders 2",
   "papers": [
    "du25c_interspeech",
    "koudounas25b_interspeech",
    "chen25n_interspeech",
    "tienkamp25_interspeech",
    "miodonska25_interspeech",
    "mujtaba25_interspeech"
   ]
  },
  {
   "title": "Neural Network Training Methods 2",
   "papers": [
    "eom25_interspeech",
    "ko25_interspeech",
    "carvalho25_interspeech",
    "moriya25_interspeech",
    "ng25_interspeech",
    "zhuang25_interspeech",
    "xu25i_interspeech",
    "zhang25n_interspeech",
    "hannan25b_interspeech",
    "carofilis25_interspeech"
   ]
  },
  {
   "title": "Disentanglement of Information for Speaker Recognition",
   "papers": [
    "srinivasmenon25_interspeech",
    "yao25b_interspeech",
    "turavecino25_interspeech",
    "millot25_interspeech",
    "martinek25_interspeech",
    "ai25_interspeech"
   ]
  },
  {
   "title": "Error Correction and Confidence Estimation",
   "papers": [
    "yamashita25_interspeech",
    "ravi25_interspeech",
    "park25_interspeech",
    "novitasari25_interspeech",
    "vangysel25_interspeech",
    "attia25_interspeech"
   ]
  },
  {
   "title": "Training and Scoring Methods for Speaker Recognition",
   "papers": [
    "mun25c_interspeech",
    "chen25f_interspeech",
    "gu25c_interspeech",
    "masztalski25_interspeech",
    "liu25_interspeech",
    "asali25_interspeech",
    "kim25i_interspeech",
    "nam25b_interspeech",
    "cumani25b_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 2",
   "papers": [
    "hermes25_interspeech",
    "halpern25_interspeech",
    "zhong25b_interspeech",
    "pierotti25_interspeech",
    "talkar25_interspeech",
    "sridhar25_interspeech"
   ]
  },
  {
   "title": "Multimodal and Visual Speech Synthesis",
   "papers": [
    "zheng25c_interspeech",
    "kim25r_interspeech",
    "kang25c_interspeech",
    "kim25b_interspeech",
    "que25_interspeech",
    "liang25d_interspeech"
   ]
  },
  {
   "title": "Lexicon and Grammar",
   "papers": [
    "schouwenaars25_interspeech",
    "ye25_interspeech",
    "xue25b_interspeech",
    "liu25n_interspeech",
    "ram25_interspeech"
   ]
  },
  {
   "title": "Noise Reduction and Dereverberation",
   "papers": [
    "sharma25_interspeech",
    "lalay25_interspeech",
    "luan25_interspeech",
    "li25s_interspeech",
    "kim25s_interspeech",
    "hu25g_interspeech",
    "wen25b_interspeech",
    "dang25_interspeech",
    "zhu25_interspeech"
   ]
  },
  {
   "title": "Neural Network Training Methods and Architectures",
   "papers": [
    "rittergutierrez25_interspeech",
    "damianos25_interspeech",
    "song25b_interspeech",
    "schuster25_interspeech",
    "kim25u_interspeech",
    "mojarad25_interspeech",
    "kwok25b_interspeech",
    "makishima25_interspeech",
    "xie25_interspeech"
   ]
  },
  {
   "title": "Challenges in Speech Data Collection, Curation and Annotation - Part 1",
   "papers": [
    "chen25k_interspeech",
    "wang25q_interspeech",
    "shiota25_interspeech",
    "serrand25_interspeech",
    "raokoluguri25_interspeech",
    "ali25_interspeech",
    "zhang25s_interspeech",
    "zwilling25_interspeech",
    "yue25_interspeech",
    "peurey25_interspeech",
    "wang25e_interspeech",
    "lahtinen25_interspeech",
    "sheikh25_interspeech",
    "gao25f_interspeech",
    "li25ba_interspeech"
   ]
  },
  {
   "title": "Evaluation and Forensic Applications of Speaker Recognition",
   "papers": [
    "cumani25_interspeech",
    "malykh25_interspeech",
    "lee25e_interspeech",
    "harrington25_interspeech",
    "reuter25_interspeech",
    "zuo25b_interspeech"
   ]
  },
  {
   "title": "Language Resources",
   "papers": [
    "luo25_interspeech",
    "do25b_interspeech",
    "havard25_interspeech",
    "alabi25_interspeech",
    "ong25_interspeech",
    "santamariajorda25_interspeech",
    "ormaechea25_interspeech",
    "gong25_interspeech",
    "grossman25_interspeech",
    "parcollet25_interspeech",
    "maison25_interspeech"
   ]
  },
  {
   "title": "Bandwidth Expansion and Diffusion-based Speech Enhancement",
   "papers": [
    "xiang25_interspeech",
    "byun25_interspeech",
    "jun25_interspeech",
    "xu25d_interspeech",
    "liu25p_interspeech",
    "liu25d_interspeech",
    "bao25_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "biswas25b_interspeech",
    "agrawal25_interspeech",
    "lepagnol25_interspeech",
    "huang25d_interspeech",
    "kumar25b_interspeech",
    "koudounas25_interspeech"
   ]
  },
  {
   "title": "Multilingual Speech Synthesis and Special Applications 2",
   "papers": [
    "sankar25_interspeech",
    "pathak25_interspeech",
    "sanchez25_interspeech",
    "stucki25_interspeech",
    "lobato25_interspeech",
    "kang25b_interspeech",
    "geng25b_interspeech",
    "nguyen25c_interspeech",
    "levkovitch25_interspeech"
   ]
  },
  {
   "title": "Prosody and Voice Quality",
   "papers": [
    "gogoi25b_interspeech",
    "sun25e_interspeech",
    "chuang25_interspeech",
    "kakouros25b_interspeech",
    "havras25_interspeech",
    "patman25_interspeech",
    "watkins25_interspeech"
   ]
  },
  {
   "title": "Generative Models for Audio",
   "papers": [
    "cohen25_interspeech",
    "jung25_interspeech",
    "liang25b_interspeech",
    "hu25k_interspeech",
    "he25b_interspeech",
    "hai25_interspeech"
   ]
  },
  {
   "title": "Challenges in Speech Data Collection, Curation and Annotation - Part 2",
   "papers": [
    "gaznepoglu25_interspeech",
    "joshi25_interspeech",
    "liu25e_interspeech",
    "johnson25_interspeech",
    "fathan25b_interspeech",
    "zhang25_interspeech",
    "szalay25_interspeech",
    "cai25_interspeech",
    "christodoulidou25_interspeech",
    "hiruta25_interspeech",
    "ravenscroft25_interspeech",
    "biswas25_interspeech",
    "mena25_interspeech",
    "karpov25_interspeech",
    "cheng25_interspeech",
    "chang25c_interspeech",
    "valente25_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition 3",
   "papers": [
    "kang25_interspeech",
    "mai25b_interspeech",
    "shi25d_interspeech",
    "tamir25_interspeech",
    "mote25_interspeech",
    "hu25c_interspeech",
    "ryu25_interspeech"
   ]
  },
  {
   "title": "Emotion and Expressivity in Speech Synthesis and Voice Conversion",
   "papers": [
    "xing25_interspeech",
    "fujita25_interspeech",
    "li25i_interspeech",
    "cho25b_interspeech",
    "kim25t_interspeech",
    "murata25_interspeech",
    "li25t_interspeech",
    "su25c_interspeech",
    "chou25_interspeech",
    "wu25g_interspeech"
   ]
  },
  {
   "title": "Streaming ASR",
   "papers": [
    "xia25_interspeech",
    "li25u_interspeech",
    "nam25_interspeech",
    "choi25b_interspeech",
    "zhou25_interspeech",
    "ho25_interspeech"
   ]
  },
  {
   "title": "L1 and L2 Acquisition, Perception and Processing",
   "papers": [
    "dong25b_interspeech",
    "turnbull25_interspeech",
    "ji25_interspeech",
    "zhao25j_interspeech",
    "cao25_interspeech",
    "li25y_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition 2",
   "papers": [
    "phukan25e_interspeech",
    "phukan25g_interspeech",
    "zhou25c_interspeech",
    "xiang25c_interspeech",
    "burkhardt25_interspeech",
    "zhou25i_interspeech",
    "shi25_interspeech",
    "pendyala25_interspeech"
   ]
  },
  {
   "title": "Speaker Traits Recognition",
   "papers": [
    "kirkland25_interspeech",
    "ho25b_interspeech",
    "aggarwal25_interspeech",
    "mohamedismailyasararafath25_interspeech",
    "bhattacharya25_interspeech",
    "kponou25_interspeech",
    "huang25h_interspeech",
    "papadimitriou25_interspeech",
    "niebuhr25_interspeech"
   ]
  },
  {
   "title": "Spoofing and Adversarial Attacks",
   "papers": [
    "das25_interspeech",
    "sankala25_interspeech",
    "chandra25b_interspeech",
    "weizman25_interspeech",
    "buker25_interspeech",
    "dutta25b_interspeech"
   ]
  },
  {
   "title": "Voice Conversion 2",
   "papers": [
    "pan25b_interspeech",
    "qi25_interspeech",
    "li25r_interspeech",
    "kaneko25_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 3",
   "papers": [
    "postma25_interspeech",
    "hu25d_interspeech",
    "su25_interspeech",
    "kim25_interspeech",
    "salihs25_interspeech",
    "narain25_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition in Naturalistic Conditions Challenge",
   "papers": [
    "chen25i_interspeech",
    "uniyal25_interspeech",
    "feng25_interspeech",
    "lertpetchpun25_interspeech",
    "cho25_interspeech",
    "leygue25_interspeech",
    "dutta25_interspeech",
    "naini25_interspeech",
    "jon25_interspeech",
    "zgorzynski25_interspeech",
    "chatzichristodoulou25_interspeech",
    "liu25m_interspeech",
    "shi25e_interspeech",
    "ueda25_interspeech",
    "singh25b_interspeech",
    "tzeng25_interspeech",
    "wang25k_interspeech"
   ]
  },
  {
   "title": "Prosody, Phoneme and Stress Modeling in ASR",
   "papers": [
    "yosha25_interspeech",
    "wallbridge25_interspeech",
    "portes25_interspeech",
    "sasu25_interspeech",
    "zhou25h_interspeech",
    "coppietersdegibson25_interspeech"
   ]
  },
  {
   "title": "Segments",
   "papers": [
    "hutin25_interspeech",
    "sun25h_interspeech",
    "wang25c_interspeech",
    "buech25_interspeech",
    "yang25i_interspeech",
    "kwon25b_interspeech"
   ]
  },
  {
   "title": "Datasets and Tools for Speech Synthesis",
   "papers": [
    "langman25_interspeech",
    "kondo25_interspeech",
    "serajian25_interspeech",
    "lodagala25_interspeech",
    "jung25c_interspeech",
    "liu25c_interspeech",
    "toyin25_interspeech",
    "sosawelford25_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems 2",
   "papers": [
    "hegde25_interspeech",
    "shabtay25_interspeech",
    "kim25m_interspeech",
    "arora25_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement and Representation Learning",
   "papers": [
    "yuan25_interspeech",
    "chung25_interspeech",
    "sun25g_interspeech",
    "gogate25_interspeech",
    "wang25s_interspeech",
    "lee25i_interspeech",
    "zhang25d_interspeech",
    "baser25_interspeech"
   ]
  },
  {
   "title": "Neural Codecs and Vocoders",
   "papers": [
    "zheng25b_interspeech",
    "li25e_interspeech",
    "yoneyama25_interspeech",
    "zhao25d_interspeech",
    "kaneko25b_interspeech",
    "chen25q_interspeech",
    "chen25p_interspeech",
    "takagi25_interspeech"
   ]
  },
  {
   "title": "Adaptation and Target-speaker ASR",
   "papers": [
    "seong25_interspeech",
    "hirano25_interspeech",
    "rangappa25_interspeech",
    "deng25_interspeech",
    "yang25e_interspeech",
    "vieting25_interspeech",
    "fang25_interspeech",
    "vandalen25_interspeech"
   ]
  },
  {
   "title": "Show and Tell 4: Education / Assistive Technology",
   "papers": [
    "roman25_interspeech",
    "francis25_interspeech",
    "merzougui25_interspeech",
    "peirolilja25_interspeech",
    "mcallister25b_interspeech",
    "raju25_interspeech",
    "blouir25_interspeech"
   ]
  },
  {
   "title": "Source Separation 2",
   "papers": [
    "yang25d_interspeech",
    "han25d_interspeech",
    "lee25g_interspeech",
    "yang25c_interspeech",
    "hasumi25_interspeech",
    "itani25_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "zhang25k_interspeech",
    "wen25_interspeech",
    "tseng25_interspeech",
    "guo25_interspeech",
    "sadok25_interspeech",
    "casanova25_interspeech"
   ]
  },
  {
   "title": "Multimodality",
   "papers": [
    "cheng25c_interspeech",
    "maran25_interspeech",
    "oneata25_interspeech",
    "park25e_interspeech",
    "wu25h_interspeech",
    "menezes25_interspeech"
   ]
  },
  {
   "title": "Speech Assessment and Language Learning",
   "papers": [
    "sirigiaju25_interspeech",
    "parikh25_interspeech",
    "oh25b_interspeech",
    "ma25b_interspeech",
    "qian25_interspeech",
    "choi25g_interspeech"
   ]
  },
  {
   "title": "Watermarking and Anonymization",
   "papers": [
    "xu25f_interspeech",
    "lin25d_interspeech",
    "kim25e_interspeech",
    "li25g_interspeech",
    "ozer25_interspeech",
    "zeng25_interspeech",
    "meyer25_interspeech",
    "tomashenko25_interspeech",
    "zhang25o_interspeech"
   ]
  },
  {
   "title": "Single-channel Speech Enhancement",
   "papers": [
    "parvathala25b_interspeech",
    "li25d_interspeech",
    "kuhne25_interspeech",
    "li25m_interspeech",
    "kim25f_interspeech",
    "kim25q_interspeech",
    "zhao25l_interspeech"
   ]
  },
  {
   "title": "Contextual Biasing and Adaptation",
   "papers": [
    "kong25_interspeech",
    "nakagome25_interspeech",
    "hou25_interspeech",
    "sudo25b_interspeech",
    "yang25j_interspeech",
    "zevallos25_interspeech",
    "yang25p_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization 2",
   "papers": [
    "horiguchi25_interspeech",
    "li25k_interspeech",
    "broughton25_interspeech",
    "cordlandwehr25_interspeech",
    "zhang25p_interspeech",
    "kalda25_interspeech",
    "medennikov25_interspeech",
    "kim25k_interspeech"
   ]
  },
  {
   "title": "Depression Detection and Assessment 2",
   "papers": [
    "deng25b_interspeech",
    "gomezzaragoza25_interspeech",
    "maji25_interspeech",
    "hidalgojulia25_interspeech",
    "you25_interspeech"
   ]
  },
  {
   "title": "Keynote4 - Judith Holler: Using and comprehending language in face-to-face conversation",
   "papers": [
    "holler25_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 4",
   "papers": [
    "gimenogomez25_interspeech",
    "hovsepyan25_interspeech",
    "escobargrisales25_interspeech",
    "peters25_interspeech",
    "kommagouni25_interspeech",
    "miyahara25_interspeech",
    "mun25_interspeech",
    "franz25_interspeech",
    "kothare25_interspeech"
   ]
  },
  {
   "title": "Speech Deepfakes",
   "papers": [
    "kim25j_interspeech",
    "tran25b_interspeech",
    "wu25i_interspeech",
    "baser25b_interspeech",
    "huang25e_interspeech",
    "combei25_interspeech",
    "grinberg25_interspeech",
    "zhang25g_interspeech",
    "febrinanto25_interspeech"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "chen25g_interspeech",
    "mondal25_interspeech",
    "funfgeld25_interspeech",
    "wang25v_interspeech",
    "shim25_interspeech",
    "vlasenko25_interspeech"
   ]
  },
  {
   "title": "Speech Analysis and Quality Assessment",
   "papers": [
    "hussein25_interspeech",
    "so25_interspeech",
    "sanders25_interspeech",
    "xiao25b_interspeech",
    "cumlin25_interspeech",
    "ahmed25_interspeech",
    "nilsson25_interspeech",
    "hu25l_interspeech"
   ]
  },
  {
   "title": "Emotions and Foundational Models",
   "papers": [
    "du25b_interspeech",
    "mai25c_interspeech",
    "morais25_interspeech",
    "halim25_interspeech",
    "raut25_interspeech",
    "chao25_interspeech"
   ]
  },
  {
   "title": "Prediction and Evaluation of Speech Quality and Intelligibility",
   "papers": [
    "yamamoto25_interspeech",
    "zhou25g_interspeech",
    "zezario25_interspeech",
    "bashir25_interspeech",
    "joubaud25_interspeech",
    "leschanowsky25_interspeech"
   ]
  },
  {
   "title": "Multi-Talker ASR",
   "papers": [
    "dai25c_interspeech",
    "wang25y_interspeech",
    "sakuma25_interspeech",
    "subramanian25b_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Paradigms and Methods 3",
   "papers": [
    "lee25h_interspeech",
    "yoon25_interspeech",
    "murata25b_interspeech",
    "sun25f_interspeech",
    "lin25h_interspeech",
    "kawamura25_interspeech"
   ]
  },
  {
   "title": "Biosignal-enabled Spoken Communication",
   "papers": [
    "pahuja25_interspeech",
    "lin25f_interspeech",
    "khanday25_interspeech",
    "rastogi25_interspeech",
    "huang25j_interspeech",
    "dasilva25_interspeech",
    "scheck25_interspeech",
    "ibrahimov25_interspeech",
    "mcghee25_interspeech",
    "bandekar25_interspeech",
    "teplansky25_interspeech",
    "li25j_interspeech",
    "inoue25b_interspeech",
    "shah25_interspeech",
    "pham25_interspeech"
   ]
  },
  {
   "title": "Speech Deepfakes, Antispoofing and Backdoor Attacks",
   "papers": [
    "xiao25d_interspeech",
    "ranjan25_interspeech",
    "mahapatra25_interspeech",
    "phuong25_interspeech",
    "trachu25_interspeech",
    "urai25_interspeech",
    "huang25b_interspeech",
    "li25aa_interspeech",
    "gorthi25_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis 5",
   "papers": [
    "braun25_interspeech",
    "kang25d_interspeech",
    "shao25_interspeech",
    "liu25l_interspeech",
    "hwang25_interspeech",
    "gao25b_interspeech",
    "tam25_interspeech",
    "sung25_interspeech",
    "neumann25_interspeech"
   ]
  },
  {
   "title": "ASR Assessment and Foundational Models",
   "papers": [
    "phukon25_interspeech",
    "hou25b_interspeech",
    "pulikodan25_interspeech",
    "chang25_interspeech",
    "kando25_interspeech",
    "hoffner25_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "jones25_interspeech",
    "goebiowska25_interspeech",
    "greenberg25_interspeech",
    "zhong25d_interspeech",
    "gan25_interspeech",
    "barahona25_interspeech"
   ]
  },
  {
   "title": "Speech Analysis, Detection and Classification 2",
   "papers": [
    "ljubesic25_interspeech",
    "mallela25_interspeech",
    "jacquelin25_interspeech",
    "lin25_interspeech",
    "chi25b_interspeech",
    "yu25_interspeech",
    "maciejewski25_interspeech",
    "wang25f_interspeech",
    "male25_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2025"
}
