{
 "title": "7th International Conference on Spoken Language Processing (ICSLP 2002)",
 "location": "Denver, Colorado, USA",
 "startDate": "16/9/2002",
 "endDate": "20/9/2002",
 "chair": "General Chair: John Hansen",
 "conf": "ICSLP",
 "year": "2002",
 "name": "icslp_2002",
 "series": "ICSLP",
 "SIG": "",
 "title1": "7th International Conference on Spoken Language Processing",
 "title2": "(ICSLP 2002)",
 "date": "16-20 September 2002",
 "booklet": "icslp_2002.pdf",
 "papers": {
  "fitch02_icslp": {
   "authors": [
    [
     "W. Tecumseh",
     "Fitch"
    ]
   ],
   "title": "The evolution of spoken language: a comparative approach",
   "original": "i02_0001",
   "page_count": 8,
   "order": 1,
   "p1": "1",
   "pn": "8",
   "abstract": [
    "We are entering a new era in the study of the evolution of spoken language, where poorly-grounded speculation based mainly on fossil evidence is being superseded by comparative, empirical study of living animals. This is particularly true for the evolution of speech, where a variety of methodologies and theoretical constructs developed by speech scientists have been applied to animal vocalization, greatly deepening our understanding of the evolutionary precursors to speech. Animals both produce and perceive formants, and the descent of the larynx has both homologous and analogous equivalents in the animal world. Vocal imitation, which is key to spoken language, does not exist in other primates but is found in a far-flung group including birds, seals and dolphins. Despite some illuminating results, a vast menagerie of fascinating vocal adaptations exist in vertebrates that are just beginning to be explored. These provide a fertile field for research by speech scientists, who have the skills and theoretical background to lead bioacoustic research into the next millennium.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-1"
  },
  "young02_icslp": {
   "authors": [
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Talking to machines (statistically speaking)",
   "original": "i02_0009",
   "page_count": 8,
   "order": 2,
   "p1": "9",
   "pn": "16",
   "abstract": [
    "Statistical methods have long been the dominant approach in speech recognition and probabilistic modelling in ASR is now a mature technology. The use of statistical methods in other areas of spoken dialogue is however more recent and rather less mature. This paper reviews spoken dialogue systems from a statistical modelling perspective. The complete system is first presented as a partially observable Markov decision process. The various sub-components are then exposed by introducing appropriate intermediate variables. Samples of existing work are reviewed within this framework, including dialogue control and optimisation, semantic interpretation, goal detection, natural language generation and synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-2"
  },
  "macho02_icslp": {
   "authors": [
    [
     "Duncan",
     "Macho"
    ],
    [
     "Laurent",
     "Mauuary"
    ],
    [
     "Bernhard",
     "Noé"
    ],
    [
     "Yan Ming",
     "Cheng"
    ],
    [
     "Doug",
     "Ealey"
    ],
    [
     "Denis",
     "Jouvet"
    ],
    [
     "Holly",
     "Kelleher"
    ],
    [
     "David",
     "Pearce"
    ],
    [
     "Fabien",
     "Saadoun"
    ]
   ],
   "title": "Evaluation of a noise-robust DSR front-end on Aurora databases",
   "original": "i02_0017",
   "page_count": 4,
   "order": 3,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "This paper describes a noise-robust front-end designed within a collaboration of Motorola, France Télécom and Alcatel for the ETSI standardization of the advanced front-end for distributed speech recognition (DSR). The proposed algorithm is based on the cumulative knowledge in the three companies history in the areas of noise reduction, speech enhancement as well as other related fields. The major components of this algorithm are noise reduction, waveform processing, cepstrum calculation, blind equalization, and voice-activity detection. In the evaluation of the proposed front-end on Aurora 2 and Aurora 3 databases we obtained an average error rate reduction of 52.75% and 51.51%, respectively, when compared to the WI007 ETSI MFCC-based DSR front-end performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-3"
  },
  "adami02_icslp": {
   "authors": [
    [
     "Andre",
     "Adami"
    ],
    [
     "Lukás",
     "Burget"
    ],
    [
     "Stephane",
     "Dupont"
    ],
    [
     "Hari",
     "Garudadri"
    ],
    [
     "Frantisek",
     "Grezl"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Sachin",
     "Kajarekar"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Sunil",
     "Sivadas"
    ]
   ],
   "title": "Qualcomm-ICSI-OGI features for ASR",
   "original": "i02_0021",
   "page_count": 4,
   "order": 4,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "Our feature extraction module for the Aurora task is based on a combination of a conventional noise suppression technique (Wiener filtering) with our temporal processing techniques (linear discriminant RASTA filtering and nonlinear TempoRAl Pattern (TRAP) classifier). We observe better than 58% relative error improvement on the prescribed Aurora Digit Task, a performance level that is somewhat better than the new ETSI Advanced Feature standard. Furthermore, to test generalization of our approach to an independent test set not available during development, we evaluate performance on American English SpeechDatCar digits and show 10.54% relative improvement over the new ETSI standard.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-4"
  },
  "kleinschmidt02_icslp": {
   "authors": [
    [
     "Michael",
     "Kleinschmidt"
    ],
    [
     "David",
     "Gelbart"
    ]
   ],
   "title": "Improving word accuracy with Gabor feature extraction",
   "original": "i02_0025",
   "page_count": 4,
   "order": 5,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "A novel type of feature extraction for automatic speech recognition is investigated. Two-dimensional Gabor functions, with varying extents and tuned to different rates and directions of spectro-temporal modulation, are applied as filters to a spectro-temporal representation provided by mel spectra. The use of these functions is motivated by findings in neurophysiology and psychoacoustics. Data-driven parameter selection was used to obtain Gabor feature sets, the performance of which is evaluated on the Aurora 2 and 3 datasets both on their own and in combination with the Qualcomm-OGI-ICSI Aurora proposal. The Gabor features consistently provide performance improvements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-5"
  },
  "droppo02_icslp": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Evaluation of SPLICE on the Aurora 2 and 3 tasks",
   "original": "i02_0029",
   "page_count": 4,
   "order": 6,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "Stereo-based Piecewise Linear Compensation for Environments (SPLICE) is a general framework for removing distortions from noisy speech cepstra. It contains a non-parametric model for cepstral corruption, which is learned from two channels of training data. We evaluate SPLICE on both the Aurora 2 and 3 tasks. These tasks consist of digit sequences in five European languages. Noise corruption is both synthetic (Aurora 2) and realistic (Aurora 3). For both the Aurora 2 and 3 tasks, we use the same training and testing procedure provided with the corpora. By holding the back-end constant, we ensure that any increase in word accuracy is due to our front-end processing techniques. In the Aurora 2 task, we achieve a 76.86% average decrease in word error rate with clean acoustic models, and an overall improvement of 62.63%. For the Aurora 3 task, we achieve a 75.06% average decrease in word error rate for the high-mismatch experiment, and an overall improvement of 47.19%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-6"
  },
  "mak02_icslp": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Yik-Cheung",
     "Tam"
    ]
   ],
   "title": "Performance of discriminatively trained auditory features on Aurora2 and Aurora3",
   "original": "i02_0033",
   "page_count": 4,
   "order": 7,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "The design of acoustic models involves two main tasks: feature extraction and data modeling; and hidden Markov modeling (HMM) is commonly used in contemporary automatic speech recognition. In the past, discriminative training has been applied successfully to re- fine HMM parameters that are initially trained by EM algorithm. Recently, we applied discriminative training in the feature extraction process. We proposed a novel Discriminative Auditory Feature extraction method (DAF) in which filters are discriminatively trained from data. In DAF, we do not make any assumptions on the functional form of the auditory filters except that they have to be smooth and triangular-like. On the method of discriminative training, we also proposed an alternative approach to finding the competing hypotheses which we call N-nearest hypotheses (as opposed to the traditional N-best hypotheses). By applying the two new ideas and the new robust auditory features proposed by Li et al. of Bell Labs, we reduce the overall word error rate (WER) by 30.27% over ICSLP2002 Aurora2 baseline on multi-condition training. Similarly, we obtain a relative WER reduction of 38.42% over ICSLP2002 Aurora3 baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-7"
  },
  "segura02_icslp": {
   "authors": [
    [
     "José C.",
     "Segura"
    ],
    [
     "M.C.",
     "Benítez"
    ],
    [
     "Ángel de la",
     "Torre"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "Feature extraction combining spectral noise reduction and cepstral histogram equalization for robust ASR",
   "original": "i02_0225",
   "page_count": 4,
   "order": 8,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "This work is mainly focused on showing experimental results using a combination of two methods for noise compensation which are shown to be complementary: classical spectral subtraction algorithm and histogram equalization. While spectral subtraction is focused on the reduction of the additive noise in the spectral domain, histogram equalization is applied in the cepstral domain to compensate the remaining non-linear effects associated to channel distortion and additive noise. The estimation of the noise spectrum for the spectral subtraction method relies on a new algorithm for speech / non-speech detection (SND) based on order statistics. This SND classification is also used for dropping long speech pauses. Results on Aurora 2 and Aurora 3 are reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-8"
  },
  "chen02_icslp": {
   "authors": [
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Dimitris",
     "Dimitriadis"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Qi",
     "Li"
    ],
    [
     "Tor André",
     "Myrvoll"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Bell labs approach to Aurora evaluation on connected digit recognition",
   "original": "i02_0229",
   "page_count": 4,
   "order": 9,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "In this paper we study various front-end features, modeling and adaptation algorithms on the Aurora 3 databases, including auditory, moment, and AM-FM modulation features, context-dependent digit models, segmental K-means training, discriminative training, and model adaptations. The evaluation results on Aurora 3 are presented with a brief summary of our Aurora 2 results. Evaluation of Robust Speech Recognition\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-9"
  },
  "kim02_icslp": {
   "authors": [
    [
     "Hong Kook",
     "Kim"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Algorithms for distributed speech recognition in a noisy automobile environment",
   "original": "i02_0233",
   "page_count": 4,
   "order": 10,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "In this paper, we evaluate the performance of several robust speech recognition algorithms in a noisy automobile environment as characterized by the Finnish SpeechDat-Car ASR task [1]. By applying acoustic feature compensation, model compensation, and speech detection algorithms to this task, a 51% reduction in word error rate (WER) was obtained relative to the ETSI standard ASR front-end. In addition, these same techniques achieved an average 35% WER reduction for clean condition training and multiple condition training on a simulated speech-in-noise task as characterized by the Aurora 2 ASR task [2]. The paper also presents alternatives for how these algorithms can be implemented in a distributed speech recognition framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-10"
  },
  "hilger02_icslp": {
   "authors": [
    [
     "Florian",
     "Hilger"
    ],
    [
     "Sirko",
     "Molau"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Quantile based histogram equalization for online applications",
   "original": "i02_0237",
   "page_count": 4,
   "order": 11,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "The noise robustness of automatic speech recognition systems can be increased by transforming the signal to make the cumulative density functions of the signals values in recognition match the ones that where estimated on the training data. This paper describes a real-time online algorithm to approximate the cumulative density functions, after Mel scaled filtering, using a small number of quantiles. Recognition tests where carried out on the Aurora noisy TI digit strings and SpeechDat-Car databases. The average relative reduction of the word error rates was 32% on the noisy TI digit strings and 29% on SpeechDat-Car.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-11"
  },
  "chen02b_icslp": {
   "authors": [
    [
     "Chia-Ping",
     "Chen"
    ],
    [
     "Karim",
     "Filali"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Frontend post-processing and backend model enhancement on the Aurora 2.0/3.0 databases",
   "original": "i02_0241",
   "page_count": 4,
   "order": 12,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "We investigate a highly effective and extremely simple noise-robust front end based on novel post-processing of standard MFCC features on the Aurora databases. It performs remarkably well on both the Aurora 2.0 and Aurora 3.0 databases without requiring any increase in model complexity. Our experiments on Aurora 2.0 have been reported in [1]. In this paper, we evaluate this technique on the Aurora 3.0 corpus, and present updated results on Aurora 2.0. Results in the past have shown that endpointing (i.e., pre-segmentation) on Aurora 3.0 can yield significant improvements. Our experiments reported herein show that our approach integrates well with this endpointing, namely we obtain additional significant improvements. Overall, on Aurora 3.0 we obtain a 47.17% improvement over the segmented baseline. Also, our most recent Aurora 2.0 results show an overall improvement of 41.09% over the baseline for the matched training conditions, and 65.07% for the mis-matched conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-12"
  },
  "ida02_icslp": {
   "authors": [
    [
     "Masaki",
     "Ida"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "HMM COmposition-based rapid model adaptation using a priori noise GMM adaptation evaluation on Aurora2 corpus",
   "original": "i02_0437",
   "page_count": 4,
   "order": 13,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "When a speech recognition system is used in a real environment, its recognition performance is affected by the surrounding noise. Most types of additional noise as well as SNRs are difficult to predict, so there is a mismatch between the training and test data. We need a method to deal with this problem. In this paper, we propose an HMM composition-based model adaptation method with a priori noise GMM adaptation against the mismatch between different types of noise in noisy data. We also prepare multiple HMMs for several SNRs and select the one that can most effectively, based on the acoustic likelihood, deal with unknown SNRs. We carried out speech recognition experiments in noisy environments by using an AURORA2 task test set B. The results show 53% improvement in word accuracy from the baseline system with one-second real noise data used for adaptation. The performance is equivalent that of conventional HMM composition methods using ten-second real data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-13"
  },
  "hung02_icslp": {
   "authors": [
    [
     "Jeih-weih",
     "Hung"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Data-driven temporal filters obtained via different optimization criteria evaluated on Aurora2 database",
   "original": "i02_0441",
   "page_count": 4,
   "order": 14,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "In deriving the data-driven temporal filters for speech features, the Linear Discriminant Analysis (LDA) has been shown to be successful in improving the feature robustness [1,2,3]. In our previous works [4,5] it was shown that the criteria of Principal Component Analysis (PCA) and Minimum Classification Error (MCE) can also be used to obtain the data-driven temporal filters in improving the speech recognition performance. In this paper, we proposed to perform Cepstral Normalization before applying these temporal filters, and evaluated the effectiveness of these different data-driven temporal filters on the AURORA2 database. Test results showed very signifi- cant improvements for almost all different cases, specially when the training and testing environments are highly mismatched. Robust MFCC Feature Extraction Algorithm Using\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-14"
  },
  "kotnik02_icslp": {
   "authors": [
    [
     "Bojan",
     "Kotnik"
    ],
    [
     "Damjan",
     "Vlaj"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Efficient additive and convolutional noise reduction procedures",
   "original": "i02_0445",
   "page_count": 4,
   "order": 15,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "In this paper a robust mel frequency cepstral coefficient feature extraction procedure using noise reduction, frame attenuation and RASTA processing is presented. In the preprocessing stage a hybrid Hamming-Cosine window is applied. To minimize the effect of additive environmental noise on speech signal a spectral subtraction based on spectral smoothing is used. A general mel filtering approach is performed on noise reduced signal. To detect speech frames, a voice activity detection based on log filter-bank energies is performed. The log filter-bank magnitudes of noise-only frames are attenuated. To reduce the level of convolutional distortion, a RASTA filtering of log filter-bank energy trajectories is applied. At final stage, a noise robust feature vector, which consists of 12 mel cepstrum coefficients and the log energy is created. For evaluation of improvement of speech recognition with the proposed front-end, the Aurora (2), 3 databases together with the HTK speech recognition toolkit have been chosen. The total improvement of 41.14% (Aurora 2) and 45.06% (Aurora 3) relative to the baseline MFCC front-end is achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-15"
  },
  "lieb02_icslp": {
   "authors": [
    [
     "Markus",
     "Lieb"
    ],
    [
     "Alexander",
     "Fischer"
    ]
   ],
   "title": "Progress with the philips continuous ASR system on the Aurora 2 noisy digits database",
   "original": "i02_0449",
   "page_count": 4,
   "order": 16,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "With this paper we report various experiments we conducted on the noisy digits recognition task of AURORA 2. We are aiming not only for optimized feature extraction algorithms, but also for improved strategies throughout the entire ASR system. Starting from a detailed examination of the noise-estimation procedure in spectral subtraction and its influence on ASR performance, we introduce an alternative speech enhancement approach that is based on singular value decomposition. Discriminative techniques during HMM training, namely rival training, are shown to improve system performance significantly, especially for small-footprint recognizer set-ups. In our full resource system we obtain slight additional improvements over previous results by applying LDA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-16"
  },
  "wu02_icslp": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "An environment compensated minimum classification error training approach and its evaluation on Aurora2 database",
   "original": "i02_0453",
   "page_count": 4,
   "order": 17,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "A conventional feature compensation module for robust automatic speech recognition is usually designed separately from the training of HMM parameters of the recognizer, albeit a maximum likelihood criterion might be used in both designs. In this paper, we present an environment compensated minimum classification error training approach for the joint design of the feature compensation module and the recognizer itself. By evaluating the proposed approach on Aurora2 connected digits database, a digit recognition error rate, averaged on all three test sets, of 9.15% and 13.98% is achieved for multi- and clean-condition training respectively. In comparison with the performance achieved by the baseline system without any environment compensation provided by the organizer of the ICSLP-2002 special session on Aurora tasks, our approach achieves an overall error rate reduction of 54.60%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-17"
  },
  "yao02_icslp": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Dong-Lai",
     "Zhu"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Evaluation of a noise adaptive speech recognition system on the Aurora 3 database",
   "original": "i02_0457",
   "page_count": 4,
   "order": 18,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "In this paper, we present evaluation results of a noise adaptive speech recognition system with combination of several techniques for robust speech recognition. The evaluation was on AURORA 3 database which contains noisy digit utterances collected in real car environments through close-talking and hands-free microphones. The techniques in the system include segmentation, maximum likelihood linear regression (MLLR) and non-stationary environment compensation by noise adaptive speech recognition. Through experiments, it is observed that the system has competitive performance improvement in all evaluations over the baseline results provided for the evaluation. As a whole, the system achieved 28% of relative performance improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-18"
  },
  "docioferandez02_icslp": {
   "authors": [
    [
     "Laura",
     "Docío-Ferández"
    ],
    [
     "Carmen",
     "García-Mateo"
    ]
   ],
   "title": "Distributed speech recognition over IP networks on the Aurora 3 database",
   "original": "i02_0461",
   "page_count": 4,
   "order": 19,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "In this paper we present the performance obtained by a Distributed Speech Recognition System (DSR) operating over simulated Internet networks. The front-end and recognizer are the standards proposed by ETSI STQ-AURORA Project; and the Aurora 3 databases were considered for training and testing. One characteristic of the IP network is the loss of packets. Here the packet transmission over IP networks was modeled by (1) random losses, (2) losses generated by a 2-state Gilbert model and (3) Bottleneck network simulations. The recognition results show that solitary losses and short burst losses do not severely affect the recognition performance. However, strongly bursty packet losses, as those generated by real Web traffic over a Internet network, can have a harmful impact on recognition performance. Therefore, so that DSR over Internet would be successful it is necessary high levels of Quality of Service.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-19"
  },
  "fujimoto02_icslp": {
   "authors": [
    [
     "M.",
     "Fujimoto"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Evaluation of noisy speech recognition based on noise reduction and acoustic model adaptation on the Aurora2 tasks",
   "original": "i02_0465",
   "page_count": 4,
   "order": 20,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "In this paper, we have evaluated a noisy speech recognition method based on noise reduction and acoustic model adaptation, on the AURORA2 tasks.\n",
    "For noise reduction method, we employed two noise reduction methods. One is an Adaptive Sub-Band Spectral Subtraction (ASBSS) method which can optimize the noise subtraction rate according to the SNR in frequency bands at each frame. The other is a Kalman filtering estimation method which re-estimates the accurate speech spectra from those estimated by ASBSS. The accurate speech spectra was estimated by combining these methods. Usually, a noise reduction method has a problem that it degrades the recognition rate because of spectral distortion caused by residual noise occurred through noise reduction and over estimation. To solve the problem in noise reduction method, adaptation of the acoustic models is employed by using an unsupervised MLLR adaptation to the spectral distortion.\n",
    "In evaluation on the AURORA2 tasks, our method showed the significant improvement in recognition accuracy for both clean training condition and multi training condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-20"
  },
  "saon02_icslp": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Juan M.",
     "Huerta"
    ]
   ],
   "title": "Improvements to the IBM Aurora 2 multi-condition system",
   "original": "i02_0469",
   "page_count": 4,
   "order": 21,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "In this paper we describe some recent improvements to the performance of the Aurora 2 noisy digits speech recognition system for the matched training and test condition. The algorithms that we used pertain to discriminant acoustic modeling based on the Maximum Mutual Information (MMI) criterion, non-linear speaker/channel adaptation through probability distribution function matching. In addition, we revisited our last years baseline system and improved its performance through cross-word context dependent modeling and Gaussian mixture components selection using the Bayesian Information Criterion (BIC). The aggregated result is 93.3% word accuracy for the multi-condition training data scenario.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-21"
  },
  "jain02_icslp": {
   "authors": [
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Distributed speech recognition using noise-robust MFCC and traps-estimated manner features",
   "original": "i02_0473",
   "page_count": 4,
   "order": 22,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "In this paper, we investigate the use of TemPoRal PatternS (TRAPS) classifiers for estimating manner of articulation features on the smallvocabulary Aurora-2002 database. By combining a stream of TRAPSestimated manner features with a stream of noise-robust MFCC features (earlier proposed in the Aurora-2002 evaluation by OGI, ICSI and Qualcomm), we obtain an average absolute improvement of 0.4% to 1.0% in word recognition accuracy over noise-robust MFCC baseline features on Aurora tasks. This yields an average relative improvement of 54% over the reference end-pointed MFCC baseline. Estimation of the manner features can be performed on the server without increasing the terminal-side computational complexity in a distributed speech recognition (DSR) system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-22"
  },
  "kitaoka02_icslp": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Evaluation of spectral subtraction with smoothing of time direction on the Aurora 2 task",
   "original": "i02_0477",
   "page_count": 4,
   "order": 23,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "To reduce the effects of additive noises, spectral subtraction (SS) is often used. We discuss SS on the power spectral domain. This method has two problems to make the estimation of clean speech dif- ficult:(1) There exists the estimation error between true power spectrum of noise and estimated one (2) The correlation between speech and noise also exists because of the phase difference. To overcome these problems, we proposed a spectral subtraction using a smoothing method of time direction. We consider the average of estimated speech power spectra over some frames as the estimated speech power spectrum. This operation makes the estimation of noise more accurate. We can reduce the effect of correlation between speech and noise.\n",
    "In this paper, we tested this method on the AURORA 2 database, which consists of English connected digit added with various realistic noises. We achieved 47.26% relative improvement of word accuracy with acoustic models trained under clean condition and 11.95% with models trained under multi-condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-23"
  },
  "cui02_icslp": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Markus",
     "Iseli"
    ],
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Evaluation of noise robust features on the Aurora databases",
   "original": "i02_0481",
   "page_count": 4,
   "order": 24,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "In this paper, we evaluate our noise robust feature extraction algorithms on the Aurora 2 and the German part of Aurora 3. Several algorithms are introduced and evaluated to deal with the noisy speech signals including our previous noise robust techniques used with Aurora (2), and new approaches evaluated with Aurora 3. Since there exist some differences between the two databases, modifications of front-end modules are needed. For Aurora (2), the average error rate reduction is 47% for clean training and 12% for multicondition training compared with the new baseline with endpoint detection. In Aurora (3), we obtain 17%, 27% and 53% error rate reduction for the well-matched, medium-mismatched and high-mismatched cases, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-24"
  },
  "evans02_icslp": {
   "authors": [
    [
     "Nicholas W. D.",
     "Evans"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Computationally efficient noise compensation for robust automatic speech recognition assessed under the Aurora 2/3 framework",
   "original": "i02_0485",
   "page_count": 4,
   "order": 25,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "In the context of mobile telephony there is a need for low resource, computationally efficient noise compensation and speech enhancement approaches. This paper assesses the performance of efficient quantile-based noise estimation integrated into a nonlinear spectral subtraction framework. The approach has been implemented in realtime with minimal latency on a 500Mhz processor and is well within the processing capabilities. Experiments are reported on the AURORA 2 and AURORA 3 corpa. Results show an average relative improvement of 15% on the clean and multi-condition training sets of the AURORA 2 database and an overall average relative improvement of 20% across the four AURORA 3 databases. It is acknowledged that these are not state-of-the-art results and further optimisation is anticipated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-25"
  },
  "farooq02_icslp": {
   "authors": [
    [
     "O.",
     "Farooq"
    ],
    [
     "S.",
     "Datta"
    ]
   ],
   "title": "Mel-scaled wavelet filter based features for noisy unvoiced phoneme recognition",
   "original": "i02_1017",
   "page_count": 4,
   "order": 26,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "In this paper we propose a filter bank structure derived by using admissible wavelet packet transform. These filters have Mel scale spacing and have an advantage of easy implementation with higher resolution in time-frequency domain because of wavelet transform. The features are obtained by first calculating the energy in each filter band and then applying the Discrete Cosine Transform (DCT) to the energy vector. We evaluate the recognition performance of the features derived from the Mel-Scaled Wavelet Filter (MSWF) bank structure and compare it with that derived from Mel Frequency Cepstral Coefficients (MFCC). Experimental results on the phoneme recognition from the TIMIT database show that, features derived by using MSWF performs better as compared to MFCC features for unvoiced stops and unvoiced fricatives. Further the noise performance of these features are also found to be better as compared to MFCC features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-26"
  },
  "onoe02_icslp": {
   "authors": [
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Hiroyuki",
     "Segi"
    ],
    [
     "Takeshi",
     "Kobayakawa"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Filter bank subtraction for robust speech recognition",
   "original": "i02_1021",
   "page_count": 4,
   "order": 27,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "In this paper, we propose a new technique of filter bank subtraction for robust speech recognition under various acoustic conditions. Spectral subtraction is a simple and useful technique for reducing the influence of additive noise. Conventional spectral subtraction assumes accurate estimation of the noise spectrum and no correlation between speech and noise. Those assumptions, however, are rarely satisfied in reality, leading to the degradation of speech recognition accuracy. Moreover, the recognition improvement attained by conventional methods is slight when the input SNR changes sharply. We propose a new method in which the output values of filter banks are used for noise estimation and subtraction. By estimating noise at each filter bank, instead of at each frequency point, the method alleviates the necessity for precise estimation of noise. We also take into consideration phase differences between the spectra of speech and noise in the subtraction. Recognition experiments on test sets at several SNRs showed that the filter bank subtraction technique improved the word accuracy significantly and got better results than conventional spectral subtraction on all the test sets. In other experiments, on recognizing speech from TV news field reports with environmental noise, the proposed subtraction method yielded better results than the conventional method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-27"
  },
  "morris02_icslp": {
   "authors": [
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "Simon",
     "Payne"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Low cost duration modelling for noise robust speech recognition",
   "original": "i02_1025",
   "page_count": 4,
   "order": 28,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "State transition matrices as used in standard HMM decoders have two widely perceived limitations. One is that the implicit Geometric state duration distributions which they model do not accurately reflect true duration distributions. The other is that they impose no hard limit on maximum duration with the result that state transition probabilities often have little influence when combined with acoustic probabilities, which are of a different order of magnitude. Explicit duration models were developed in the past to address the first problem. These were not widely taken up because their performance advantage in clean speech recognition was often not suf- ficiently great to offset the extra complexity which they introduced. However, duration models have much greater potential when applied to noisy speech recognition. In this paper we present a simple and generic form of explicit duration model and show that this leads to strong performance improvements when applied to connected digit recognition in noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-28"
  },
  "gong02_icslp": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "A comparative study of approximations for parallel model combination of static and dynamic parameters",
   "original": "i02_1029",
   "page_count": 4,
   "order": 29,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "Reducing mismatch between HMMs trained with clean speech and speech signals corrupted with background noise can be approached by speech distribution adaptation using parallel model combination (PMC). Accurate PMC has no closed-form expression, therefore simplification assumptions must be made in implementation. Under three assumptions, i.e. log-normal, log-add and log-max, adaptation formula for log-spectral parameters are presented, both for static and dynamic parameters.\n",
    "Experimental evaluation uses TI-DIGITS speech database corrupted with car noise at 0dB signal-to-noise ratio. The recognition performance of the above three types of simplification is established. It is shown that, the adaptation of both static and dynamic parameters gives as much as 30% lower WER compared to adapting only static parameters.\n",
    "The findings and results presented in the paper provide a basis for trading-offs between recognition accuracy and computation requirement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-29"
  },
  "moticek02_icslp": {
   "authors": [
    [
     "Petr",
     "Motícek"
    ],
    [
     "Lukás",
     "Burget"
    ]
   ],
   "title": "Noise estimation for efficient speech enhancement and robust speech recognition",
   "original": "i02_1033",
   "page_count": 4,
   "order": 30,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "Different approaches of minima tracking based noise estimation algorithms are compared and modifications increasing their efficiency are proposed. Estimated noise is used by noise suppression algorithm that is a part of speech recognition system. Moreover, the algorithms are developed to be applied in feature extraction of Distributed Speech Recognition (DSR). Therefore we propose such modifications to the noise estimation techniques that are quickly adaptable on varying noise and do not need so much information from past segments. We also minimized the algorithmic delay. The robustness of proposed algorithms were tested under several noisy conditions on five Speech-Dat Car (SDC) and Aurora 2 evaluation databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-30"
  },
  "cetin02_icslp": {
   "authors": [
    [
     "Özgür",
     "Çetin"
    ],
    [
     "Harriet J.",
     "Nock"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "The 2001 GMTK-based SPINE ASR system",
   "original": "i02_1037",
   "page_count": 4,
   "order": 31,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "This paper provides a detailed description of the University of Washington automatic speech recognition (ASR) system for the 2001 DARPA SPeech In Noisy Environments (SPINE) task. Our system makes heavy use of the graphical modeling toolkit (GMTK), a general purpose graphical modeling-based ASR system that allows arbitrary parameter tying, flexible deterministic and stochastic dependencies between variables, and a generalized maximum likelihood parameter estimation algorithm. In our SPINE system, GMTK was used for acoustic model training whereas feature extraction, speaker adaptation, and first-pass decoding were performed by HTK. Our integrated GMTK/HTK system demonstrates the relative merits provided by each tool. Novel aspects of our SPINE system include the capturing of correlations among feature vectors via a globally-shared factored sparse inverse covariance matrix and generalized EM training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-31"
  },
  "hung02b_icslp": {
   "authors": [
    [
     "Wei-Wen",
     "Hung"
    ]
   ],
   "title": "Using adaptive signal limiter together with weighting techniques for noisy speech recognition",
   "original": "i02_1041",
   "page_count": 4,
   "order": 32,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "In an automatic speech recognition (ASR) system, environmental mismatch between speech models and testing speech utterances causes serious performance degradation. To alleviate this environmental mismatch problem, smoothing process and weighting technique are two of the most widely used methods. In this paper, an investigation is presented into the feasibility of combining an adaptive signal limiter with a weighting technique for seeking further performance improvement in noisy speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-32"
  },
  "yamade02_icslp": {
   "authors": [
    [
     "Shingo",
     "Yamade"
    ],
    [
     "Kanako",
     "Matsunami"
    ],
    [
     "Akira",
     "Baba"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Spectral subtraction in noisy environments applied to speaker adaptation based on HMM sufficient statistics",
   "original": "i02_1045",
   "page_count": 4,
   "order": 33,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "Noise and speaker adaptation techniques are essential to realize robust speech recognition in real noisy environments . In this paper, we applied spectral subtraction to an unsupervised speaker adaptation algorithm in noisy environments. The adaptation algorithm consists of the following five steps. (1) Spectral subtraction is carried out for noise added database. (2) Noise matched acoustic models are trained by using noise added speech database. (3) HMM sufficient statistics for each speaker are calculated from noise added speech database, and stored. (4) According to one arbitrary utterance, speakers close to a test speaker are selected by using speaker GMMs. (5) Speaker adapted acoustic models are constructed from HMM sufficient statistics of the selected speakers. We evaluated our unsupervised speaker adaptation algorithm in noisy environments in the 20k dictation task. The recognition experiments show that our speaker adapted acoustic model can achieve 82% word accuracy in 20dB SNR, which is about 6% higher than that of the noise matched models trained by Forward-Backward algorithm.\n",
    "We also investigated the robustness of the adapted models in various SNR conditions. Integration with the supervised MLLR is also examined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-33"
  },
  "siu02_icslp": {
   "authors": [
    [
     "Manhung",
     "Siu"
    ],
    [
     "Yu-Chung",
     "Chan"
    ]
   ],
   "title": "Robust speech recognition against short-time noise",
   "original": "i02_1049",
   "page_count": 4,
   "order": 34,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "This paper develops a robust speech recognition algorithm against short-time noise, of which no prior knowledge of their spectral characteristics is known. However, we assume that these noises only affects certain part of the speech and are also known as partially temporal corruption in [1]. Examples of these short-time noises include door slam, click sound of keyboard or packet loss in network transmission of voice. These noises are found to degrade the performance of an automatic speech recognizer (ASR) significantly. In our previous work [2], we proposed a robust algorithm, called frameskipping Viterbi algorithm (FSVA), which ignores the likelihood contributions of the K worst performing frames during Viterbi decoding. We showed that FSVA algorithm is robust against random replacement of speech frames by Gaussian noise. One important issue that we have not addressed is the determination of the number of skips. This paper extends our work by applying the FSVA to additive shorttime noise with unknown spectral characteristic and unexpected occurrence under different SNR, rate of corruption and the length of corruption. We also propose a solution for determining the appropriate number of skips on log likelihood ratio.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-34"
  },
  "toma02_icslp": {
   "authors": [
    [
     "M.",
     "Toma"
    ],
    [
     "A.",
     "Lodi"
    ],
    [
     "R.",
     "Guerrieri"
    ]
   ],
   "title": "Word endpoints detection in the presence of non-stationary noise",
   "original": "i02_1053",
   "page_count": 4,
   "order": 35,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "Most of isolated-word speech recognition systems need to detect boundaries of utterances. In this paper, we present a new approach to accomplish this task of endpoints identification. The proposed algorithm has very low computational complexity, since the parameters used for discrimination are obtained through a re-use of calculations already made by a mel-cepstrum frontend. This endpointer works well in most real-life environments, even the most challenging ones, like non-stationary background noise. In the presence of a babble background noise with an SNR of 0dB the rate of undetected words is about 3.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-35"
  },
  "pujolmarsal02_icslp": {
   "authors": [
    [
     "Pere",
     "Pujol Marsal"
    ],
    [
     "Susagna",
     "Pol Font"
    ],
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Comparison and combination of RASTA-PLP and FF features in a hybrid HMM/MLP speech recognition system",
   "original": "i02_1057",
   "page_count": 4,
   "order": 36,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "Recently, the advantages of the spectral parameters obtained by frequency filtering (FF) of the logarithmic filter bank energies (logFBEs) have been reported. These parameters, which are frequency derivatives of the logFBEs, lie in the frequency domain, and have shown good recognition performance with respect to the conventional melfrequency cepstral coefficients (MFCC) for HMM systems. In this paper, the FF features are compared with the MFCCs and the Rasta-PLP features in the framework of a hybrid HMM/MLP recognition system, for both clean and noisy speech.\n",
    "Taking advantage of the ability of the hybrid system to deal with correlated features, the inclusion of the second frequency derivatives and the raw logFBEs as additional features is proposed. Furthermore, in order to enhance the robustness of these features in noisy conditions, they are combined with the Rasta temporal filtering approach. Finally, a study of the FF in the framework of multistream processing is presented. From the experimental tests, it appears that the new spectral parameters and the tested combinations yield an enhanced recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-36"
  },
  "xu02_icslp": {
   "authors": [
    [
     "Tao",
     "Xu"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "Robust MMSE-FW-LAASR scheme at low SNRs",
   "original": "i02_1061",
   "page_count": 4,
   "order": 37,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "In this paper, a novel feature weight (FW) algorithm for robust automatic speech recognition (ASR) is proposed. In this algorithm every feature will be weighted according to their credible probability, especially, the weight factors are formulated and obtained from the gain coefficients generated as by-products of speech enhancement based on minimum mean square error (MMSE) estimation. Moreover a new robust ASR scheme is presented. In this scheme the MMSEbased speech enhancement, the FW algorithm and the Log-Add (LA) model compensation will be integrated together. Experimental evaluations show that this MMSE-FW-LA scheme can achieve significant improvement in recognition across a wide range of signal-to-noise ratios (SNR), especially in very low SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-37"
  },
  "zolnay02_icslp": {
   "authors": [
    [
     "András",
     "Zolnay"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Robust speech recognition using a voiced-unvoiced feature",
   "original": "i02_1065",
   "page_count": 4,
   "order": 38,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "In this paper, a voiced-unvoiced measure is used as acoustic feature for continuous speech recognition. The voiced-unvoiced measure was combined with the standard Mel Frequency Cepstral Coefficients (MFCC) using linear discriminant analysis (LDA) to choose the most relevant features. Experiments were performed on the SieTill (German digit strings recorded over telephone line) and on the SPINE (English spontaneous speech under different simulated noisy environments) corpus. The additional voiced-unvoiced measure results in improvements in word error rate (WER) of up to 11% relative to using MFCC alone with the same overall number of parameters in the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-38"
  },
  "wet02_icslp": {
   "authors": [
    [
     "Febe de",
     "Wet"
    ],
    [
     "Johan de",
     "Veth"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Accumulated kullback divergence for analysis of ASR performance in the presence of noise",
   "original": "i02_1069",
   "page_count": 4,
   "order": 39,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "In this paper, the accumulated Kullback divergence (AKD) is used to analyze ASR performance deterioration due to the presence of background noise. The AKD represents a distance between the feature value distribution observed during training and the distribution of the observations in the noisy test condition for each individual feature vector component. In our experiments the AKD summed over all feature vector components shows a high correlation with word error rate and AKD computed per component can be used to pinpoint those feature vector components that substantially contribute to recognition errors. It is argued that the distance measure could be a useful evaluation tool for analyzing the strengths and weaknesses of existing noise robustness approaches and might help to suggest research strategies that focus on those elements of the acoustic feature vector that are most severely affected by the noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-39"
  },
  "kingsbury02_icslp": {
   "authors": [
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Andre",
     "Adami"
    ]
   ],
   "title": "A hybrid HMM/traps model for robust voice activity detection",
   "original": "i02_1073",
   "page_count": 4,
   "order": 40,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "We present three voice activity detection (VAD) algorithms that are suitable for the off-line processing of noisy speech and compare their performance on SPINE-2 evaluation data using speech recognition error rate as the quality metric. One VAD system is a simple HMM- based segmenter that uses normalized log-energy and a degree of voicing measure as raw features. The other two VAD systems focus on frequency-localized temporal information in the speech signal using a TempoRAl PatternS (TRAPS) classifier. They differ only in the processing of the TRAPS output. One VAD system uses median filtering to generate segment hypotheses, while the other is a hybrid system that uses a Viterbi search identical to that used in the HMM segmenter. Recognition on the hybrid HMM/TRAPS segmentation is more accurate than recognition on the other two segmentations by 1% absolute. This difference is statistically significant at a 99% con- fidence level according to a matched pairs sentence-segment word error test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-40"
  },
  "zheng02_icslp": {
   "authors": [
    [
     "Chengyi",
     "Zheng"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Run time information fusion in speech recognition",
   "original": "i02_1077",
   "page_count": 4,
   "order": 41,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "Approaches have been studied to utilize the complementary information from different knowledge sources in Automatic Speech Recognition (ASR). These approaches can be classified into two categories, the pre-recognition fusion and the post-recognition fusion. A common problem of those approaches is that complementary information is exploited either before or after recognition. To avoid unrecoverable information loss due to pruning in decoding stage, and to better utilize the complementary information, we propose a duringrecognition information fusion scheme. Experimental result based on the proposed run-time fusion is reported. A significant improvement was observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-41"
  },
  "arrowood02_icslp": {
   "authors": [
    [
     "Jon A.",
     "Arrowood"
    ],
    [
     "Mark A.",
     "Clements"
    ]
   ],
   "title": "Using observation uncertainty in HMM decoding",
   "original": "i02_1561",
   "page_count": 4,
   "order": 42,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "This paper proposes a new technique for adapting Hidden Markov Model (HMM) speech recognition systems to additive environmental noise by incorporating information about the uncertainty of observations. Current techniques, such as the Parallel Model Combination (PMC) algorithm [1], are successful in steady state noise environments. However, the computational requirements both in processing time and memory prevent its widespread use in continuously adaptive systems, necessary for environments with changing background noise. This paper presents an approach that reformulates the model combination technique to update the each observation instead of the model. As in PMC, a model of the background is generated during nonspeech times. This is used for each input frame to generate a pdf describing the original clean signal, given the noise model. The HMM decoding algorithm is extended as in [2, 3] to allow pdf inputs, and recognition results are presented that show this technique compares favorably with PMC in unchanging noise environments, but has significant benefits in changing noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-42"
  },
  "stuttle02_icslp": {
   "authors": [
    [
     "M. N.",
     "Stuttle"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Combining a Gaussian mixture model front end with MFCC parameters",
   "original": "i02_1565",
   "page_count": 4,
   "order": 43,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "Fitting a Gaussian mixture model (GMM) to the smoothed speech spectrum allows an alternative set of features to be extracted from the speech signal. These features have been shown to possess information complementary to the standard MFCC parameterisation. This paper further investigates the use of theseGMMfeatures in combination with MFCCs. The extraction and use of a confidence metric to combine GMM features with MFCCs is described. Results using the confidence metric on the WSJ task are presented. Also, GMM features for speech corrupted with additive noise are extracted from data corrupted with coloured additive noise. Techniques for noise robustness and compensation are investigated for GMM features and the performance is examined on the RM task with additive noise. A Nonlinear Observation Model for Removing\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-43"
  },
  "droppo02b_icslp": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Noise from corrupted speech log mel-spectral energies",
   "original": "i02_1569",
   "page_count": 4,
   "order": 44,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "In this paper we present a new statistical model, which describes the corruption to speech recognition Mel-frequency spectral features caused by additive noise. This model explicitly represents the effect of unknown phase together with the unobserved clean speech and noise as three hidden variables. We use this model to produce noise robust features for automatic speech recognition. The model is constructed in the log Mel-frequency feature domain. In addition to being linearly related to MFCC recognition parameters, we gain the advantage of low dimensionality and independence of the corruption across feature dimensions. We illustrate the surprising result that, even when the true noise Mel-frequency spectral feature is known, the traditional spectral subtraction formula is flawed. We show the new model can be used to derive a spectral subtraction formula which produces superior error rate results, and is less sensitive to tuning parameters. Finally, we present results demonstrating that the new model is more general than spectral subtraction, and can take advantage of a prior noise estimate to produce robust features, rather than relying on point estimates of noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-44"
  },
  "lima02_icslp": {
   "authors": [
    [
     "Carlos",
     "Lima"
    ],
    [
     "Luís B.",
     "Almeida"
    ],
    [
     "João L.",
     "Monteiro"
    ]
   ],
   "title": "Improving the role of unvoiced speech segments by spectral normalisation in robust speech recognition",
   "original": "i02_1573",
   "page_count": 4,
   "order": 45,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "This paper presents a spectral normalisation based method for extraction of speech robust features in additive noise. The method has two main goals: The \"peaked\" spectral zones, where the most speech energy is concentrated must be preserved (from clean to noisy speech features) as much as possible by the feature extraction process. Usually, these spectral regions are the most reliable due to the higher speech energy, and the frequently assumption of independence between speech and noise. 2. The speech regions with less energy need more robustness, since in these regions the noise is more dominant, thus the speech is more corrupted. Usually these speech regions correspond to unvoiced speech where are included nearly half of the consonants. The proposed normalisation will be optimal if the corrupted and the noise process have both white noise characteristics. Optimal normalisation means that the corrupting noise does not change at all the means of the observed vectors of the corrupted process. For Signal to Noise Ratio greater than 5 dB the results show that for stationary white noise, the proposed normalisation process where the noise characteristics are ignored, outperforms the conventional Markov models composition where the noise must be known. Additionally, if the noise is known, a reasonable approximation of the inverted system can easily be obtained by performing noise compensation and still increasing the recogniser performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-45"
  },
  "gadde02_icslp": {
   "authors": [
    [
     "Venkata Ramana Rao",
     "Gadde"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Kemal",
     "Sönmez"
    ],
    [
     "Anand",
     "Venkataraman"
    ]
   ],
   "title": "Building an ASR system for noisy environments: SRIs 2001 SPINE evaluation system",
   "original": "i02_1577",
   "page_count": 4,
   "order": 46,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "We describe SRIs recognition system as used in the 2001 DARPA Speech in Noisy Environments (SPINE) evaluation. The SPINE task involves recognition of speech in simulated military environments. The task had some unique challenges, including segmentation of foreground speech from noisy background, the need for robust acoustic models to handle noisy speech, and development of language models from limited training data. In developing the SRI evaluation system for this task, we addressed each of these challenges using a combination of state-of-the-art techniques, including several types of feature normalization, model adaptation, class-based language modeling, multi-pass segmentation and recognition, and word posterior-based decoding and system combination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-46"
  },
  "son02_icslp": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Evidence for efficiency in vowel production",
   "original": "i02_0037",
   "page_count": 4,
   "order": 47,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "Speaking is generally considered efficient in that less effort is spent articulating more redundant items. With efficient speech production, less reduction is expected in the pronunciation of phonemes that are more important (distinctive) for word identification. The importance of a single phoneme in word recognition can be quanti- fied as the information (in bits) it adds to the preceding word onset to narrow down the lexical search. In our study, segmental information showed to correlate consistently with two measures of reduction: vowel duration and formant reduction. This correlation was found after accounting for speaker and vowel identity, speaking style, lexical stress, modeled prominence, and position of the syllable in the word. However, consistent correlations are only found in high-frequency words. Furthermore, the correlation is strongest in normal reading and weaker in spontaneous and anomalous read speech. Combined, these facts suggest that this type of efficiency in production might rely on retrieving stored words from memory. Ef- ficiency in vowel production seems to be less or absent when words have to be assembled on-line.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-47"
  },
  "aylett02_icslp": {
   "authors": [
    [
     "Matthew P.",
     "Aylett"
    ]
   ],
   "title": "Stochastic suprasegmentals: relationship between the spectral characteristics of vowels, redundancy and prosodic structure",
   "original": "i02_0041",
   "page_count": 4,
   "order": 48,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "Previous work has shown a relationship between syllabic duration, redundancy within speech, and prosodic structure [1]. In addition, a spectral care of articulation measure of vowels in spontaneous speech has supported these duration results and suggest that care of articulation varies inversely with redundancy and conversely with prosodic prominence. However, these spectral measures remain inconclusive due to measurement difficulties. In this paper a simpler spectral measurement is presented as a metric of care of articulation and applied to three vowels from each corner of the vowel triangle. Prosodic and redundancy factors can predict up to 5% of the variance of this new measurement, supporting the inverse redundancy result. However, in contrast to syllabic duration, whether prosodic boundaries are controlled for or not, the predictive power of prosodic factors and redundancy factors remain relatively independent. This suggests that 1. phrase final syllables, despite lengthening, do not show increased care of articulation if measured spectrally, and 2. unlike duration, redundancy factors affect the spectral characteristics of vowels independently of prosodic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-48"
  },
  "serkhane02_icslp": {
   "authors": [
    [
     "J.",
     "Serkhane"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Louis Jean",
     "Boë"
    ],
    [
     "B.",
     "Davis"
    ],
    [
     "C.",
     "Matyear"
    ]
   ],
   "title": "Motor specifications of a baby robot via the analysis of infants² vocalizations",
   "original": "i02_0045",
   "page_count": 4,
   "order": 49,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "In order to assess infants motor skills during speech development, we used a statistical model of the vocal tract that integrates growth of the effector system. This model allowed us to infer, from actual vocalizations, the likeliest explored acoustic regions, articulatory degrees of freedom and vocal tract shapes, and to test MacNeilage and Davis co-occurrence predictions. Our results will support the building of a virtual robot, modeling speech development.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-49"
  },
  "koenig02_icslp": {
   "authors": [
    [
     "Laura L.",
     "Koenig"
    ],
    [
     "Jorge C.",
     "Lucero"
    ]
   ],
   "title": "Oral-laryngeal control patterns for fricatives in 5-year-olds and adults",
   "original": "i02_0049",
   "page_count": 4,
   "order": 50,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "Fricative production in children and adults has been widely studied using acoustic measures, but little information exists on how children learn to control both laryngeal and oral gestures for (voiceless) fricative production. Here, we report oral airflow data from four 5-year-old children and four women producing laryngeal and oral fricatives /h s z/. Functional data analysis was used to normalize the signals, obtain average productions for each subject, and determine how temporal and amplitude variability were distributed over the consonant. Higher temporal variability in the 5-year-olds than the adults was observed, but at localized regions in the signals. The locations of maximum phasing variability and amplitude variability differed among subjects. For an individual subject, regions of high phasing variability were not necessarily regions of high amplitude variability. Finally, whereas adult women evidence different airflow patterns for /s/ and /z/, some of the children do not.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-50"
  },
  "delvaux02_icslp": {
   "authors": [
    [
     "Véronique",
     "Delvaux"
    ],
    [
     "Thierry",
     "Metens"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "French nasal vowels: acoustic and articulatory properties",
   "original": "i02_0053",
   "page_count": 4,
   "order": 51,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "This paper presents data about the articulatory and acoustic properties of French nasal vowels. Data show that many covarying articulations support the phonological contrast between nasal and oral vowels, in addition to the lowering of the velum. The majority of the articulatory adjustments occuring in the oral cavity lead to a lowering of F2. We relate the F2 lowering with the effects of nasal coupling, i.e. the changes in spectral balance due to the loss of energy at higher frequencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-51"
  },
  "kenny02_icslp": {
   "authors": [
    [
     "P.",
     "Kenny"
    ],
    [
     "G.",
     "Boulianne"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Maximum likelihood estimation of eigenvoices and residual variances for large vocabulary speech recognition tasks",
   "original": "i02_0057",
   "page_count": 4,
   "order": 52,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "We describe a new algorithm for estimating eigenvoices (or, equivalently, EMAP correlation matrices) for large vocabulary speech recognition tasks. The algorithm is an EM procedure based on a novel maximum likelihood formulation of the estimation problem which is similar to the mathematical model underlying probabilistic principal components analysis. It enables us to extend eigenvoice/EMAP adaptation in a natural way to adapt variances as well as mean vectors. It differs from other approaches in that it does not require that speaker dependent or speaker adapted models for the training speakers be given in advance (these are derived as a byproduct of the estimation procedure). Accordingly our algorithm can be applied directly to large vocabulary tasks even if the training data is sparse in the sense that only a small fraction of the total number of Gaussians is observed for each training speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-52"
  },
  "pusateri02_icslp": {
   "authors": [
    [
     "Ernest J.",
     "Pusateri"
    ],
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "Rapid speaker adaptation using speaker clustering",
   "original": "i02_0061",
   "page_count": 4,
   "order": 53,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "This paper examines an approach to speaker adaptation called speaker cluster weighting (SCW) for rapid adaptation in the Jupiter weather information system. SCW extends the ideas of previous speaker cluster techniques by allowing the speaker cluster models (learned from training data) to be adaptively weighted to match the current speaker. We explore strategies for automatic speaker clustering as well as cluster model training procedures for use with this algorithm. As part of this exploration, we develop a novel algorithm called least squares linear regression (LSLR) clustering for the clustering of speakers for whom only a small amount of data is available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-53"
  },
  "huang02_icslp": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Tao",
     "Chen"
    ],
    [
     "Eric",
     "Chang"
    ]
   ],
   "title": "Adaptive model combination for dynamic speaker selection training",
   "original": "i02_0065",
   "page_count": 4,
   "order": 54,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "Acoustic variability across speakers is one of the challenges of speaker independent speech recognition systems. In this paper we propose a two-stage speaker selection training method for speaker adaptation. After cohort speakers are selected for test speaker, an adaptive model combination method is developed to replace the formerly used retraining process. In addition, impacts of number of selected cohort speakers and number of utterances from test speaker are investigated. Preliminary experiments on dynamic speaker selection are shown. Relative error rate reduction of 12.27% is achieved when only 10 utterances are available. Finally, further extensions of model combination scheme and dynamic selection are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-54"
  },
  "kwan02_icslp": {
   "authors": [
    [
     "Ka-Yan",
     "Kwan"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Chen",
     "Yang"
    ]
   ],
   "title": "Unsupervised n-best based model adaptation using model-level confidence measures",
   "original": "i02_0069",
   "page_count": 4,
   "order": 55,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "This paper presents a study on using confidence measures for unsupervised N-Best based adaptation of hidden Markov model (HMM) parameters. Confidence measures have been widely used for the detection of speech recognition errors. They are also useful in selecting and/or screening data for unsupervised adaptation of HMM. In this paper, a model-level confidence measure is proposed for model adaptation with the Maximum Likelihood Linear Regression (MLLR) technique. The model-level confidence measure provides a finer selection of adaptation data than the word or utterance level measures. The proposed confidence measure is derived from the N-best hypotheses. The computation involves not only the recognized models but also other models that are easily confused with them. Experimental results show the proposed confidence measure improves the effectiveness of unsupervised model adaptation. The relative improvement in word error rate is up to 9.75%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-55"
  },
  "nguyen02_icslp": {
   "authors": [
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Luca",
     "Rigazio"
    ],
    [
     "Christian",
     "Wellekens"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "LU factorization for feature transformation",
   "original": "i02_0073",
   "page_count": 4,
   "order": 56,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "Linear feature space transformations are often used for speaker or environment adaptation. Usually, numerical methods are sought to obtain solutions. In this paper, we derive a closed-form solution to ML estimation of full feature transformations. Closed-form solutions are desirable because the problem is quadratic and thus blind numerical analysis may converge to poor local optima. We decompose the transformation into upper and lower triangular matrices, which are estimated alternatively using the EM algorithm. Furthermore, we extend the theory to Bayesian adaptation. On the Switchboard task, we obtain 1.6% WER improvement by combining the method with MLLR, or 4% absolute using adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-56"
  },
  "ding02_icslp": {
   "authors": [
    [
     "Guo-Hong",
     "Ding"
    ],
    [
     "Yi-Fei",
     "Zhu"
    ],
    [
     "Chengrong",
     "Li"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Implementing vocal tract length normalization in the MLLR framework",
   "original": "i02_1389",
   "page_count": 4,
   "order": 57,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "Vocal Tract Length Normalization (VTLN) and Maximum Likelihood Linear Regression (MLLR) are two approaches to reduce the degradation in speech recognition performance caused by variation of speakers. This paper derives a novel efficient adaptation algorithm from the two techniques. Based on prior knowledge of usual VTLN, an approximate constrained-form linear transformation is obtained. The transformation is learned using EM algorithm and then applied in the MLLR setting. Experiments of three tasks are performed on an isolated word recognition system. Experimental results shows that with several adaptation words, WER is decreased greatly. Online Adaptation of Continuous Density Hidden\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-57"
  },
  "kim02b_icslp": {
   "authors": [
    [
     "Dong Kook",
     "Kim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Markov models based on speaker space model evolution",
   "original": "i02_1393",
   "page_count": 4,
   "order": 58,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "In this paper, we propose a new approach to online adaptation of continuous density hidden Markov model (CDHMM) based on speaker space model evolution. The speaker space model which characterizes the a priori knowledge of the training speakers is effectively described in terms of the latent variable model such as the factor analysis (FA) or probabilistic principal component analysis (PPCA). The latent variable models are employed to provide not only the speaker space model but also a joint prior distribution of CDHMM parameters, which can be directly applied to the maximum a posteriori (MAP) adaptation framework. We establish an online adaptation scheme based on the quasi-Bayes (QB) estimation technique which incrementally updates the hyperparameters of the speaker space model and the CDHMM parameters simultaneously. In a series of speaker adaptation experiments on the task of continuous digit recognition, we demonstrate that the proposed approach not only achieves a good performance for a small amount of adaptation data but also maintains a good asymptotic convergence property as the data size increases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-58"
  },
  "li02_icslp": {
   "authors": [
    [
     "Baojie",
     "Li"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Robust speech recognition using inter-speaker and intra-speaker adaptation",
   "original": "i02_1397",
   "page_count": 4,
   "order": 59,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "Inter-speaker variation can be coped rather well in speech recognition by speaker adaptation techniques such as MLLR and MAP. However, when dealing with speech other than reading style, such as conversational speech, emotional speech and so on, current recognition systems cannot achieve a satisfactory performance even after speaker adaptation. In view of this situation, two-level adaptation method was newly proposed, where adaptation technique was applied in two levels to handle inter-speaker and intra-speaker variations. A speaker independent model is first adapted to a specific speaker to generate a speaker dependent model. Then, after classifying the training data into several categories, the speaker dependent model is further adapted to each category using data classified to it (category dependent model). The recognition is done in parallel using the speaker dependent model and each category dependent model, and the result with highest likelihood is selected as the fi- nal recognition result. Recognition experiments were conducted for speech with various emotions (emotion of input speech is unknown), and the results showed that the proposed method outperformed the conventional MLLR-based speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-59"
  },
  "lima02b_icslp": {
   "authors": [
    [
     "Carlos",
     "Lima"
    ],
    [
     "Luís B.",
     "Almeida"
    ],
    [
     "João L.",
     "Monteiro"
    ]
   ],
   "title": "Continuous environmental adaptation of a speech recogniser in telephone line conditions",
   "original": "i02_1401",
   "page_count": 4,
   "order": 60,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "This paper presents a maximum likelihood (ML) approach, relative to the background model estimation, in noisy acoustic non-stationary environments. The external noise source is characterised by a time constant convolutional and a time varying additive components, which is consistent with the telephone channel. The HMM composition technique, provides a mechanism for integrating parametric models of acoustic background with the signal model, so that noise compensation is tightly coupled with the background model estimation. However, the existing continuous adaptation algorithms usually do not take advantage of this approach, being essentially based on the MLLR algorithm. Consequently, a model for environmental mismatch is not available and, even under constrained conditions a significant number of model parameters have to be updated. From a theoretical point of view only the noise model parameters need to be updated, being the clean speech ones unchanged by the environment. So, it can be advantageous to have a model for environmental mismatch. This approach was followed in the development of the algorithm proposed in this paper. One drawback sometimes attributed to the continuous adaptation approach is that recognition failures originate poor background estimates. This paper also proposes a MAP-like method to deal with this situation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-60"
  },
  "illina02_icslp": {
   "authors": [
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Tree-structured maximum a posteriori adaptation for a segment-based speech recognition system",
   "original": "i02_1405",
   "page_count": 4,
   "order": 61,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "In this paper, the problem of the adaptation of a speech recognition system to a new environment is addressed. Recently, a Structural Maximum a Posteriori adaptation (SMAP) for a frame-based HMMmodel adaptation has been developed. In this method, acoustic model pdfs are organised in a tree and the means and variances of the pdfs are adapted using the linear transformations estimated under MAP criteria. In this paper, we extend the SMAP adaptation to a segmentbased model: the Mixture Stochastic Trajectory Model (MSTM). SMAP approach is completed by the tree construction driven by adaptation data, a Minimum Description Length (MDL) structure definition of this tree and trajectory and state adaptations. On the Resource Management task, the speaker adaptation and noise adaptation experiments show that the proposed SMAP approach gives a significant improvement compared to unadapted system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-61"
  },
  "plotz02_icslp": {
   "authors": [
    [
     "Thomas",
     "Plötz"
    ],
    [
     "Gernot A.",
     "Fink"
    ]
   ],
   "title": "Robust time-synchronous environmental adaptation for continuous speech recognition systems",
   "original": "i02_1409",
   "page_count": 4,
   "order": 62,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "In this paper we describe system architectures for robust MLLR based environmental adaptation of continuous speech recognition systems. Inspired by an existing broadcast news transcription system [1] we refined the identification of acoustic scenarios by using a combined GMM/HMM method. Thus environmental adaptation regarding arbitrary acoustic scenarios beyond speaker changes becomes possible. For deploying acoustic adaptation in interactive applications, such as human machine interaction, a time-synchronous adaptation approach is proposed. For different corpora the evaluation of our approaches shows significant improvements in recognition accuracy while satisfying the constraint of time-synchronous processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-62"
  },
  "niesler02_icslp": {
   "authors": [
    [
     "Thomas",
     "Niesler"
    ],
    [
     "Daniel",
     "Willett"
    ]
   ],
   "title": "Unsupervised language model adaptation for lecture speech transcription",
   "original": "i02_1413",
   "page_count": 4,
   "order": 63,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "Unsupervised adaptation methods have been applied successfully to the acoustic models of speech recognition systems for some time. Relatively little work has been carried out in the area of unsupervised language model adaptation however. The work presented here uses the output of a speech recogniser to adapt the backoff n-gram language model used in the decoding process. We report results for two different methods of language model adaptation, and find that best results are obtained when these two are used in conjunction with one another. The adaptation methods are applied to a Japanese large vocabulary transcription task, for which improvements both in perplexity and word error-rate are achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-63"
  },
  "li02b_icslp": {
   "authors": [
    [
     "Yongxin",
     "Li"
    ],
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Etienne",
     "Marcheret"
    ]
   ],
   "title": "Incremental on-line feature space MLLR adaptation for telephony speech recognition",
   "original": "i02_1417",
   "page_count": 4,
   "order": 64,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "In this paper, we present a method for incremental on-line adaptation based on feature space Maximum Likelihood Linear Regression (FMLLR) for telephony speech recognition applications. We explain how to incorporate a feature space MLLR transform into a stack decoder and perform on-line adaptation. The issues discussed are as follows: collecting adaptation data on-line and in real time; mapping adaptation data from previous feature space to the present feature space; and smoothing adaptation statistics with initial statistics based on original acoustical model to achieve stability. Testing results on various systems demonstrate that on-line incremental FMLLR adaptation could be an effective and stable method when the adaptation statistics are mapped and smoothed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-64"
  },
  "molau02_icslp": {
   "authors": [
    [
     "Sirko",
     "Molau"
    ],
    [
     "Florian",
     "Hilger"
    ],
    [
     "Daniel",
     "Keysers"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Enhanced histogram normalization in the acoustic feature space",
   "original": "i02_1421",
   "page_count": 4,
   "order": 65,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "We describe two methods that aim at normalizing acoustic vectors at the filterbank level such that the test data distribution matches the training data distribution. They enhance the histogram normalization technique proposed earlier by taking care of the variable silence fraction for each speaker, and by rotating the feature space. We report a number of recognition tests under minor (different microphones in training and test, telephone data) and major (office vs. car recordings) mismatch conditions. Both methods give superior performance to the basic histogram normalization approach. The overall improvements in word error rate (WER) range between 6% and 85% relative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-65"
  },
  "levin02_icslp": {
   "authors": [
    [
     "David N.",
     "Levin"
    ]
   ],
   "title": "Blind normalization of speech from different channels and speakers",
   "original": "i02_1425",
   "page_count": 4,
   "order": 66,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "This paper describes representations of time-dependent signals that are invariant under any invertible time-independent transformation of the signal time series. Such a representation is created by rescaling the signal in a non-linear dynamic manner that is determined by recently encountered signal levels. This technique may make it possible to normalize signals that are related by channel-dependent and speaker-dependent transformations, without having to characterize the form of the signal transformations, which remain unknown. The technique is illustrated by applying it to the time-dependent spectra of speech that has been filtered to simulate the effects of different channels. The experimental results show that the rescaled speech representations are largely normalized (i.e., channel-independent), despite the channel-dependence of the raw (unrescaled) speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-66"
  },
  "ogata02_icslp": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Unsupervised acoustic model adaptation based on phoneme error minimization",
   "original": "i02_1429",
   "page_count": 4,
   "order": 67,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "In this paper, a new decoding method for unsupervised acoustic model adaptation is presented. In unsupervised adaptation framework, the effectiveness of adaptation process is greatly affected by the mis-recognized labels. Therefore, selection of the adaptation data guided by the confidence measures is effective in unsupervised adaptation. We propose phoneme error minimization framework for exact phoneme labels and use of phoneme-level confidence measures for improved unsupervised adaptation. Experimental results showed that the proposed method could reduce the mis-recognized labels in the adaptation process, and consequently improved the adaptation accuracy. Furthermore, it was confirmed that the proposed method is effective in an iterative unsupervised adaptation framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-67"
  },
  "zhou02_icslp": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Improved structural maximum likelihood eigenspace mapping for rapid speaker adaptation",
   "original": "i02_1433",
   "page_count": 4,
   "order": 68,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "In this paper, we expand on a previously proposed algorithm entitled Structural Maximum Likelihood Eigenspace Mapping (SMLEM) [5, 6] for rapid speaker adaptation by exploring a variety of model clustering methods and incorporating a multi-stream approach. The SMLEM algorithm directly adapts speaker independent acoustic models to a test speaker by mapping the mixture Gaussian components from a speaker independent eigenspace to speaker dependent eigenspaces in a maximum likelihood manner, with very limited amounts of adaptation data. Evaluations are performed using the WSJ Spoke3 corpus. Employing the improved proposed methods, SMLEM consistently outperforms both standard MLLR and block diagonal MLLR for small amounts of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-68"
  },
  "torre02_icslp": {
   "authors": [
    [
     "Ángel de la",
     "Torre"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Statistical adaptation of acoustic models to noise conditions for robust speech recognition",
   "original": "i02_1437",
   "page_count": 4,
   "order": 69,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "Noise degrades the performance of Automatic Speech Recognition (ASR) systems working in real condition. The mismatch between the training and recognition conditions is considered the main factor involved in this degradation, and most methods for robust ASR are focussed on its minimization. In this work, we compare robust methods for ASR based on (a) the compensation of the noise effects and (b) the adaptation of the acoustic models to noise conditions. We propose a method for the adaptation of the acoustic models to the noise conditions based on a statistical formulation. In this method, each Gaussian is adapted to the noisy environment according to the estimated noise conditions. Recognition experiments have been carried out using speech acquired in real car environments. The results show the statistical formulation for adaptation provides an accurate method for robust ASR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-69"
  },
  "brugnara02_icslp": {
   "authors": [
    [
     "F.",
     "Brugnara"
    ],
    [
     "M.",
     "Cettolo"
    ],
    [
     "M.",
     "Federico"
    ],
    [
     "D.",
     "Giuliani"
    ]
   ],
   "title": "Issues in automatic transcription of historical audio data",
   "original": "i02_1441",
   "page_count": 4,
   "order": 70,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "This work deals with some interesting issues that arose when the ITCirst broadcast news transcription system was applied to transcribe the audio track of historical documentary films. Due to an evident acoustic and linguistic mismatch between the broadcast news and the new application domain, the initial word error rate was of 46.4%. By exploiting a limited amount of manually annotated training data, adaptation of all components of the transcription system was performed, namely the audio partitioner, the acoustic model, and the language model. This permitted to achieve a word error rate of 30%, which makes automatic transcription of documentary films effective for information retrieval applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-70"
  },
  "stockmal02_icslp": {
   "authors": [
    [
     "Verna",
     "Stockmal"
    ],
    [
     "Zinny S.",
     "Bond"
    ]
   ],
   "title": "Same talker, different language: a replication",
   "original": "i02_0077",
   "page_count": 4,
   "order": 71,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "When discriminating between spoken samples of unknown foreign languages, infants, young children and adult listeners are able to make same-language/different-language judgments at better than chance levels. Adults can even discriminate between languages when they are produced by the same bilingual talkers. That is, listeners are able to separate talker from language characteristics. One question raised by this investigation had to do with the familiarity of languages. The bilingual talkers who provided spoken language samples spoke a home language and a language they had learned as part of formal education such as French, German or Russian. It is possible that the listeners, American college students, had some familiarity with the school language and could distinguish it from the home language on this basis. In the current study, four bilingual talkers provided spoken samples of languages spoken in Africa which would be expected to be equally unfamiliar to American listeners. One of the languages spoken by all the talkers was Swahili; the other languages were Akan, Haya, Kikuyu, and Luhya. American listeners were asked to judge whether spoken paired samples were produced in the same language or in two different languages. Overall, listeners were able to identify languages as same or different at better than chance expectation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-71"
  },
  "jayram02_icslp": {
   "authors": [
    [
     "A. K. V. Sai",
     "Jayram"
    ],
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Automatic language identification using acoustic sub-word units",
   "original": "i02_0081",
   "page_count": 4,
   "order": 72,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "We propose a parallel sub-word recognition system (PSWR) as an alternative to the parallel phone recognition (PPR) system conventionally reported for language identification (LID) task. The sub-word recognizer (SWR) used in the PSWR system can be obtained from training data without phonetic transcription in any of the languages in the task. It is based on automatic segmentation followed by segment clustering and segment HMM modeling. The SWR can replace the front-end phone recognizer (PR) in the PPR system as well as in the PRLM and P-PRLM systems which constitute two other well accepted frameworks in LID system design. This allows easy expansion of these systems to a large number of languages without requiring tedious manually labeled training speech data in any of the languages in the task. On a 6 language LID task, using the OGI-TS database, we show that the PSWR system performs comparably to the PPR system, thus providing an efficient automatic alternative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-72"
  },
  "maddieson02_icslp": {
   "authors": [
    [
     "Ian",
     "Maddieson"
    ],
    [
     "Ioana",
     "Vasilescu"
    ]
   ],
   "title": "Factors in human language identification",
   "original": "i02_0085",
   "page_count": 4,
   "order": 73,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Listeners skill at identifying five target languages from short samples and discriminating the targets among a set of other languages was tested. Subjects in two groups, US and French, were generally quite successful in identification. Individual variation is poorly explained by degree of prior casual exposure to the target languages or academic linguistic training. Linguistic training does predict some greater success in discrimination. The implied language groupings which emerge from identification errors and in the discrimination phase are more informative from the largely linguist US subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-73"
  },
  "torrescarrasquillo02_icslp": {
   "authors": [
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Mary A.",
     "Kohler"
    ],
    [
     "Richard J.",
     "Greene"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "J. R.",
     "Deller Jr."
    ]
   ],
   "title": "Approaches to language identification using Gaussian mixture models and shifted delta cepstral features",
   "original": "i02_0089",
   "page_count": 4,
   "order": 74,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "Published results indicate that automatic language identification (LID) systems that rely on multiple-language phone recognition and ngram language modeling produce the best performance in formal LID evaluations. By contrast, Gaussian mixture model (GMM) systems, which measure acoustic characteristics, are far more efficient computationally but have tended to provide inferior levels of performance. This paper describes two GMM-based approaches to language identi- fication that use shifted delta cepstra (SDC) feature vectors to achieve LID performance comparable to that of the best phone-based systems. The approaches include both acoustic scoring and a recently developed GMM tokenization system that is based on a variation of phonetic recognition and language modeling. System performance is evaluated on both the CallFriend and OGI corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-74"
  },
  "wong02_icslp": {
   "authors": [
    [
     "Eddie",
     "Wong"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Methods to improve Gaussian mixture model based language identification system",
   "original": "i02_0093",
   "page_count": 4,
   "order": 75,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "This paper investigates the use of Vocal Tract Length Normalisation (VTLN) and Output Score Fusion techniques to improve the performance of a Gaussian Mixture Model (GMM) based Language Identi- fication (LID) system. The Universal Background Model (UBM) technique, which has been successfully employed in Speaker Verification, is incorporated into the GMM LID system to reduce the time requirement for both training and testing. The paper also presents a fast approach for selecting the normalisation factor for VTLN during the testing stage of LID which is based on the UBM technique. The output scores generated by the GMM system have been fused with a phonetic based LID system to improve the overall scores. Experimental results show that a reduction in the relative error rate by over 50% is possible for the 45-second test case in the NIST 1994 Evaluation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-75"
  },
  "jing02_icslp": {
   "authors": [
    [
     "Hongyan",
     "Jing"
    ],
    [
     "Evelyne",
     "Tzoukermann"
    ]
   ],
   "title": "Part-of-speech tagging in French text-to-speech synthesis: experiments in tagset selection",
   "original": "i02_0097",
   "page_count": 4,
   "order": 76,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "Part-of-speech tagging is needed for French Text-to-Speech (TTS) synthesis to disambiguate the pronunciation of homograph heterophones, liaison instances, and eventually to model intonational contours. A core problem in the part-of-speech tagging in French TTS is to decide on the tagset used for the tagger and the tagset needed by TTS. We carried out a number of experiments on several sizes of tagsets as well as on several algorithms to investigate this problem. Our experiment results suggest that there may be an optimal tagset to be used for the part-of-speech disambiguation in French TTS. This optimal tagset contains a slightly larger number of tags than the tagset that is needed by TTS for pronunciation disambiguation and intonational modeling purposes. In our experiments, the optimal tagset gives a 98.4% tagging accuracy for TTS, when a trigram Hidden Markov Model tagger is used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-76"
  },
  "uebler02_icslp": {
   "authors": [
    [
     "Ulla",
     "Uebler"
    ]
   ],
   "title": "Grapheme-to-phoneme conversion using pseudo-morphological units",
   "original": "i02_0101",
   "page_count": 4,
   "order": 77,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "This paper presents a new approach for grapheme-to-phoneme conversion based on morphology. With this approach, a high accuracy can be obtained, although not for all words a transcription is achieved. The principle of this approach is to automatically decompose an existing pronunciation lexicon into morpheme-similar units called pseudo-morphological units. The pronunciation of the pseudo-morphological units is also generated from the pronunciation lexicon. The pronunciation of unknown words is composed from the pronunciation of its morphemes. With this approach, it is possible to transcribe also abbreviations and words, where a differentiation between spelling and pronunciation-as-a-word is difficult for other approaches. Another advantage of this approach is that it is performed automatically. It can further be used for any language as long as a word based pronunciation lexicon is available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-77"
  },
  "bisani02_icslp": {
   "authors": [
    [
     "M.",
     "Bisani"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Investigations on joint-multigram models for grapheme-to-phoneme conversion",
   "original": "i02_0105",
   "page_count": 4,
   "order": 78,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "We present a fully data-driven, language independent way of building a grapheme-to-phoneme converter. We apply the joint-multigram approach to the alignment problem and use standard language modelling techniques to model transcription probabilities. We study model parameters, training procedures and effects of corpus size in detail. Experiments were conducted on English and German pronunciation lexica. Our proposed training scheme performs better than previously published ones. Phoneme error rates as low as 3.98%for English and 0.51% for German were achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-78"
  },
  "galescu02_icslp": {
   "authors": [
    [
     "Lucian",
     "Galescu"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "Pronunciation of proper names with a joint n-gram model for bi-directional grapheme-to-phoneme conversion",
   "original": "i02_0109",
   "page_count": 4,
   "order": 79,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "Pronunciation of proper names is known to be a difficult problem, but one of great practical importance for both speech synthesis and speech recognition. Recently a few data-driven grapheme-to-phoneme conversion techniques have been proposed to tackle this problem. In this paper we apply the joint n-gram model for bi-directional grapheme- to-phoneme conversion, which has already been shown to achieve excellent results on general tasks, to the more specific task of converting between name pronunciations and spellings. The performance of our technique on generating name pronunciations exceeds that of other techniques even when they use additional information. We find the reverse task, of generating orthographic transcriptions from phonemic input, to be a much more difficult task for names than for common words. However, we derive valuable information from our results about the potential of sub-lexical recognition of novel proper names.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-79"
  },
  "jilka02_icslp": {
   "authors": [
    [
     "Matthias",
     "Jilka"
    ],
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "The AT&t German text-to-speech system: realistic linguistic description",
   "original": "i02_0113",
   "page_count": 4,
   "order": 80,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "Like many current TTS systems the AT&T German text-to-speech system is based on the methods of unit selection and concatenative synthesis [1]. This paper highlights efforts to improve TTS quality by closely matching the speakers original productions with linguistic descriptions. On the segmental level this is achieved by adjusting the speakers individual productions to an established, general norm via strict monitoring and correspondingly by having the linguistic representations that control automatic alignment and TTS output, i.e. the recognition dictionary and letter-to-sound rules, reflect those original productions. The chosen standard represents a realistic form of spoken German, avoiding overly formal pronunciations. A perceptual comparison with a more traditional interpretation of German pronunciation demonstrates the positive effect of these measures on overall synthesis quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-80"
  },
  "li02c_icslp": {
   "authors": [
    [
     "Haiping",
     "Li"
    ],
    [
     "Fangxin",
     "Chen"
    ],
    [
     "Liqin",
     "Shen"
    ]
   ],
   "title": "Generating script using statistical information of the context variation unit vector",
   "original": "i02_0117",
   "page_count": 4,
   "order": 81,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "A statistical selection method is proposed for generating an optimized recording script for Concatenative Speech Synthesizer. This method starts with traveling a large text corpus to collect the statistical information of the Context Variation Unit Vectors (CVUV), which represent the multi-dimension phonetic contexts and properties of the synthesis unit. Each CVUV descriptor is organized as a node in a sorted tree of the CVUV forest to record the dimension values and the index to its position in the corpus. Then it selects sentences according to the pre-defined criteria relating to the CVUV distribution in the corpus. This selection algorithm has been implemented to generate syllable-based Chinese script and yielded satisfactory results. The context dimension definition concept is described in this paper, and the coverage analysis and computing time estimation are reported also.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-81"
  },
  "kuo02_icslp": {
   "authors": [
    [
     "Chih-Chung",
     "Kuo"
    ],
    [
     "Jing-Yi",
     "Huang"
    ]
   ],
   "title": "Efficient and scalable methods for text script generation in corpus-based TTS design",
   "original": "i02_0121",
   "page_count": 4,
   "order": 82,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "This paper proposes performance indices and search criteria for the text script generation in the design of corpus-based TTS systems. Based on the criteria a new search method is presented to solve the text selection problem more systematically and efficiently. Experiment results have shown that with the same hit rate of unit types the new method can reduce up to 40% of text script size in some cases. Furthermore, by control the weighting factor the covering rate of unit types can be increased to improve the robustness of the TTS system. Finally, the scalable and controllable design of the multi-stage search can produce various kinds of text scripts ideally suitable for the requirement of various kinds of corpus-based TTS systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-82"
  },
  "rutten02_icslp": {
   "authors": [
    [
     "Peter",
     "Rutten"
    ],
    [
     "Matthew P.",
     "Aylett"
    ],
    [
     "Justin",
     "Fackrell"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "A statistically motivated database pruning technique for unit selection synthesis",
   "original": "i02_0125",
   "page_count": 4,
   "order": 83,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "An important topic in unit selection based speech synthesis is the scalability of such systems. Related to this problem is the question regarding the optimal size of a unit selection database. An ideal system should produce ever better synthesis results when more data is added to the system, but for a practical system this might not be the case. The unit selection criteria are generally not sufficiently developed to ensure that a system makes an optimal use of the data that it has available.\n",
    "In this paper we propose a database reduction technique based on the statistical behaviour of unit selection. We investigate the effect of scaling down the database by objective and subjective criteria. We compare the proposed reduction technique with a technique that simply limits the size of unit lists to a fraction of their original size (random removal).\n",
    "The results show that the proposed technique is far better than random removal, and that we can remove a significant portion of our database without causing any severe quality loss.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-83"
  },
  "wu02b_icslp": {
   "authors": [
    [
     "Yi-Jian",
     "Wu"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Xiaoru",
     "Wu"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "A new method of building decision tree based on target information",
   "original": "i02_0129",
   "page_count": 4,
   "order": 84,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "The algorithm of unit pre-selection based decision tree has been adopted in the corpus-based TTS system, but the effect of the decision tree, which is built by conventional methods, is not satisfactory. This paper proposed a new method of building a decision tree based on a mass of target information obtained in the synthesizing course. By synthesizing a large quantity of text and logging the information of target units, we used these target information as training data to classify the corpus unit and build the decision tree. The splitting criterion is to ensure that the best units of target units correctly classified, which guarantee that the synthesis quality would not be reduced. And the decision tree built by this method has an advantage that it has equal units among the different classes. By evaluating the synthesis quality in objective and subjective measure, this method is better than the conventional ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-84"
  },
  "yamagishi02_icslp": {
   "authors": [
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A context clustering technique for average voice model in HMM-based speech synthesis",
   "original": "i02_0133",
   "page_count": 4,
   "order": 85,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "This paper describes a new technique for constructing a decision tree used for clustering average voice model, i.e., speaker independent speech units. In the technique, we first train speaker dependent models using multi-speaker speech database, and then construct a speaker independent decision tree for context clustering common to these speaker dependent models. When a node of the decision tree is split, only the context related questions which can split the node for all speaker dependent models is adopted. Consequently, all nodes of the decision tree have all speakers training data. From the result of the paired comparison test, we show that the average voice model trained using the proposed technique can synthesize more natural sounding speech than the conventional average voice model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-85"
  },
  "tsuzaki02_icslp": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Feature extraction for unit selection in concatenative speech synthesis: comparison between AIM, LPC, and MFCC",
   "original": "i02_0137",
   "page_count": 4,
   "order": 86,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "A comprehensive computational model of the human auditory peripherals (AIM) was applied to extract basic features of speech sounds aiming at optimal unit selection in concatenative speech synthesis. The performance of AIM was compared to that of a purely physical model (LPC) as well as that of an approximate auditory model (MFCC) by basic perceptual experiments. While a significant advantage of AIM over LPC was observed, the performance based on AIM selection and MFCC selection did not differ significantly. However, a phoneme space based on the AIM features did not completely match one based on the MFCC features, demonstrating that the selection was not perfect yet. A detailed investigation conducted on the case of poor concatenation indicates that acoustic discontinuity at comparatively steady phonemic boundaries, especially those between vowel-like sounds, spoils perceptual impression. Sensitivity to such discontinuity will be required in order to further improve acoustic measures for unit selection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-86"
  },
  "campillodiaz02_icslp": {
   "authors": [
    [
     "Francisco",
     "Campillo-Díaz"
    ],
    [
     "Eduardo R.",
     "Banga"
    ]
   ],
   "title": "Combined prosody and candidate unit selections for corpus-based text-to-speech systems",
   "original": "i02_0141",
   "page_count": 4,
   "order": 87,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "Traditionally, corpus-based text-to-speech systems generate the speech signal as the result of a two-staged process. First, the target prosody is determined and, after that, a set of speech units that minimize a cost function is selected. Once the target prosody is selected, no alternative prosodic information is generally considered, even when appropriated speech units are not found. In this paper we propose an alternative technique that takes into account several possible intonation contours, selecting the one that minimizes the cost function. In this method, both the candidate pitch contours and the candidate speech units are obtained by means of a unit selection process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-87"
  },
  "kim02c_icslp": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Automatic segmentation combining an HMM-based approach and spectral boundary correction",
   "original": "i02_0145",
   "page_count": 4,
   "order": 88,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "Currently, AT&T Labs Natural Voices multilingual TTS system produces high-quality synthetic speech with a largescale speech corpus [1]. In the development of such systems, automatic segmentation constitutes a major component technology. The prevalent approach for automatic segmentation in speech synthesis is Hidden Markov Model (HMM) - based. Even though an HMM- based approach is the most automatic and reliable, there are still several limitations, such as mismatches between hand-labeled transcriptions and HMM alignment labels which can lead to discontinuities in the synthetic speech, or the need for hand-labeled bootstrap data in HMM initialization. This paper introduces a new approach to automatic segmentation which aims both to minimize human intervention and to achieve a higher segmental quality of synthetic speech in unit-concatenative speech synthesis, by combining a conventional HMM-based approach and spectral boundary correction. A preference test demonstrates the proposed method is effective in reducing discontinuities in synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-88"
  },
  "sethy02_icslp": {
   "authors": [
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Refined speech segmentation for concatenative speech synthesis",
   "original": "i02_0149",
   "page_count": 4,
   "order": 89,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "High accuracy phonetic segmentation is critical for achieving good quality in concatenative text to speech synthesis. Due to the shortcomings of current automated techniques based on HMM-based alignment or Dynamic Time Warping (DTW), manual verification and labeling are often required. In this paper we present a novel technique for automatic placement of phoneme boundaries in a speech waveform using explicit statistical models for phoneme boundaries. Thus we are able to cut down substantially on the labor and time intensive manual labeling process required to build a new voice. The phonetic speech segmentation is carried out using a two-step process, similar to the way a human expert would label the waveform. In the first step an initial estimate of the labeling is generated using an HMMbased phoneme recognizer. The second step refines the boundary placements by searching for the best match in a region near the estimated boundaries with predefined boundary models generated from existing labeled speech corpora. The proposed method can be used in conjunction with any of the segmentation schemes used in practice. In the performance evaluations carried out the system is able to give time marks which are 30-40% better than the schemes currently used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-89"
  },
  "breen02_icslp": {
   "authors": [
    [
     "Andrew",
     "Breen"
    ],
    [
     "Barry",
     "Eggleton"
    ],
    [
     "Peter",
     "Dion"
    ],
    [
     "Steve",
     "Minnis"
    ]
   ],
   "title": "Refocussing on the text normalisation process in text-to-speech systems",
   "original": "i02_0153",
   "page_count": 4,
   "order": 90,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Many Natural Language Processing applications depend crucially on the front end processes that handle the input text and transform it into a form usable by the more \"sophisticated\" linguistic component of the applications. Despite this crucial role, often these front end processes are considered uninteresting, yet it is not unusual for the perception of the complete application to be affected by this weakest link in the processing chain.\n",
    "With the recent productisation of many text to speech (TTS) systems, the performance of the TTS front end process, typically called the text normalization (TN) process, has been highlighted. This component performs sentence recognition, symbol and term expansions and word tokenisation - but these tasks are not independent. For this reason, enhancing TN coverage often has adverse side-effects, especially when dealing with unrestricted text, so a crucial part of our Nuance Vocalizer 2.0 TTS system development concerns itself with comprehensive regression testing of coverage.\n",
    "As TTS systems are increasingly employed as part of general application suites, the TN component becomes the main interface with the controlling applications. Detailed specification of this interface is required, which lends itself to testing. Preprocessors, such as SSML transducers and email filers should ensure that no information is lost in subsuming some of the tasks that TN would normally undertake. Refocusing attention on the TN process and its testing is timely and can have important dividends.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-90"
  },
  "vepa02_icslp": {
   "authors": [
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Jahnavi",
     "Ayachitam"
    ],
    [
     "K. V. K. Kalpana",
     "Reddy"
    ]
   ],
   "title": "A text-to-speech synthesis system for telugu",
   "original": "i02_0157",
   "page_count": 4,
   "order": 91,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "In this paper, a diphone based Text-to-Speech (TTS) system for the Telugu language is presented. Telugu is one of the main south-Indian languages spoken by more than 100 million people. Speech output is generated using the Festival Speech Synthesis System and the MBROLA synthesis engine. The design and collection of diphones and voice building process are described. Our text analysis module, the methods used for segment duration and generation of pitch contours are briefly discussed. Also, we present waveform generation techniques used in both MBROLA and Festival synthesis systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-91"
  },
  "freitas02_icslp": {
   "authors": [
    [
     "Diamantino",
     "Freitas"
    ],
    [
     "Daniela",
     "Braga"
    ]
   ],
   "title": "Towards an intonation module for a portuguese TTS system",
   "original": "i02_0161",
   "page_count": 4,
   "order": 92,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "In this paper, a correlation between the linguistic structure of the written text and the real intonation behavior of the read speech in European Portuguese language (EP) is presented. It is our belief that intonation behavior in EP can be strongly predicted from two main coordinates: the syntactic structure of the sentence and its pragmatic communicative function, in one way, combined with the phonological and syntactic nature of the words, in the other way. The purpose of our work is to identify in real speech the main intonation elements, which are relevant to speech naturalness as well as to analyze the factors that determine them. This work addresses the cases of declarative/imperative, interrogative and enumerative phrases. Basic categorizations of the intonation elements, in correlation with the underlying factors are presented. General regularities and correlations as well as the resulting rules, that may be a starting point for practical implementation of an intonation module, are presented and demonstrated, under a Fujisakis phonetic/physiological approach.\n",
    "The methodology was based on the observation and modeling of a significant prosodic corpus where different intonation patterns occur in a diversity of text structures. It is our goal to contribute with practical techniques and experience in order to perform a more accurate intonation modeling of Text-to-Speech (TTS) applications, using a rule-based approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-92"
  },
  "saito02_icslp": {
   "authors": [
    [
     "Takashi",
     "Saito"
    ],
    [
     "Masaharu",
     "Sakamoto"
    ]
   ],
   "title": "Applying a hybrid intonation model to a seamless speech synthesizer",
   "original": "i02_0165",
   "page_count": 4,
   "order": 93,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "We present a speech synthesizer to seamlessly concatenate recorded and synthetic phrases to produce natural sounding and highly expressive speech. Not only the acoustic units, but also the F0 contours are seamlessly concatenated together from recorded and synthetic phrases. When mixed with recorded phrases, the F0 contours of synthetic phrases are generated adaptively relative to the actual surrounding F0 shapes of the recorded phrases. Although the intonation generation scheme was originally developed for unlimited speech synthesis, it is quite naturally extended to a hybrid intonation generation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-93"
  },
  "hirai02_icslp": {
   "authors": [
    [
     "Toshio",
     "Hirai"
    ],
    [
     "Seiichi",
     "Tenpaku"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Using start/end timings of spectral transitions between phonemes in concatenative speech synthesis",
   "original": "i02_2357",
   "page_count": 4,
   "order": 94,
   "p1": "2357",
   "pn": "2360",
   "abstract": [
    "The definition of \"phoneme boundary timing\" in a speech corpus affects the quality of concatenative speech synthesis systems. For example, if the selected speech unit is not appropriately match to the speech unit of the required phoneme environment, the quality may be degraded. In this paper, a dynamic segment boundary defi- nition is proposed. In the definition, the concatenation point is chosen from the start or end timings of spectral transition depending on the phoneme environment at the boundaries. For a listening test to compare the naturalness of conventional/proposed methods, 100 Japanese place names were selected randomly and synthesized. The ratio of naturalness was 1 to 3.3 (conventional v.s. proposed) by four subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-94"
  },
  "ni02_icslp": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Design of a Mandarin sentence set for corpus-based speech synthesis by use of a multi-tier algorithm taking account of the varied prosodic and spectral characteristics",
   "original": "i02_2361",
   "page_count": 4,
   "order": 95,
   "p1": "2361",
   "pn": "2364",
   "abstract": [
    "This paper presents a multi-tier algorithm to extract a sentence set from a large raw text corpus for synthesis of Mandarin speech, taking account of varied prosodic and spectral characteristics. The prosodic and spectral characteristics are statistically analyzed from the text corpus and transcribed as syllable-sized unit candidates in a multi-tier way. The unit candidates cover all of the syllables, typical phonetic and tone contexts for each syllable, and effects of the phrase construction and sentence intonation on the syllable. The algorithm seeks to maximize the coverage of the unit candidates involved in the extracted sentence set. Experiments were run on a 580k-sentence corpus including dialog and news text. A (9),479 sentence set was selected out. It covers 87.7% of the primary prosodic and spectral characteristics in statements and 61.0% of those in questions. Also, this paper discusses the raw text corpus selection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-95"
  },
  "mori02_icslp": {
   "authors": [
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Takahiro",
     "Ohtsuka"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "A data-driven approach to source-formant type text-to-speech system",
   "original": "i02_2365",
   "page_count": 4,
   "order": 96,
   "p1": "2365",
   "pn": "2368",
   "abstract": [
    "A data-driven formant-type TTS system is proposed. The formanttype speech synthesizer is one of the most promising architectures to enable flexible control of various voice qualities. By applying the ARX-based speech analysis method, source and formant parameters are automatically obtained. It is shown that a TTS system can be built by using the parameters, without requiring any heuristic rules to control vocal tract characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-96"
  },
  "shi02_icslp": {
   "authors": [
    [
     "Yu",
     "Shi"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Hu",
     "Peng"
    ],
    [
     "Min",
     "Chu"
    ]
   ],
   "title": "Power spectral density based channel equalization of large speech database for concatenative TTS system",
   "original": "i02_2369",
   "page_count": 4,
   "order": 97,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "This paper proposes a channel equalization algorithm for a large speech database with application in concatenative TTS systems. The convolutional channel distortion is equalized by comparing the power spectral densities (PSDs) of utterances of different recording sessions. Autoregressive linear filters are designed on a corpus level and are used offline to filter the corresponding sentences to compensate for the relative distortions caused by the channel effects. Two experiments are carried out to evaluate the benefit of the channel equalization approach. First, this method is used to reduce the distance of their PSDs between two recording sessions to verify the effectiveness of the method. Secondly, it is applied practically in the TTS system. The whole TTS speech database is processed to reduce the PSDs variance over all sessions. Moreover, a subjective listening test is carried out to obtain human evaluation of the new TTS system. Almost all listeners prefer the synthetic speech generated by the new TTS system. Furthermore, an analysis of variance (ANOVA) on this subjective listening test demonstrates that the channel equalization process has significant effect on increasing the perceived voice-quality consistency of the TTS system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-97"
  },
  "meng02_icslp": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Chi Kin",
     "Keung"
    ],
    [
     "Kai Chung",
     "Siu"
    ],
    [
     "Tien Ying",
     "Fung"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "CU VOCAL: corpus-based syllable concatenation for Chinese speech synthesis across domains and dialects",
   "original": "i02_2373",
   "page_count": 4,
   "order": 98,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "This paper describes CU VOCAL, a Chinese text-to-speech synthesis system that adopts the approach of corpus-based syllable concatenation. We have demonstrated the applicability of the approach primarily for Cantonese, a major dialect of Chinese predominant in Hong Kong, South China and many overseas Chinese communities. This work extends our previous work as described in [1]. Our approach is able to synthesize speech from free-form text, and it can also be optimized for response generation in specific application domains. We have also demonstrated the portability of the approach to Putonghua, the official Chinese dialect, in a domain-optimized setting. Coarticulatory context is expressed in terms of distinctive features. Tonal context is also included. We conducted a series of listening tests using CU VOCAL, which gave favorable performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-98"
  },
  "lu02_icslp": {
   "authors": [
    [
     "Jinlin",
     "Lu"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Perceptual evaluation of naturalness due to substitution of Chinese syllable for concatenative speech synthesis",
   "original": "i02_2377",
   "page_count": 4,
   "order": 99,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "In order to estimate the degradation of naturalness in concatenative speech synthesis due to the mismatch of syllables between unit selection and its use, perceptual experiments were conducted using speech stimuli synthesized by concatenating the preceding Final (vowel) of the first syllable and the succeeding Initial (consonant) of the second syllable, and by combining the tone positions. The results for substitution of the succeeding final of one syllable and the preceding initial of the next syllable showed that naturalness was low when the speech segmentation was difficult, such as [s, m, y, w]. For the substitution of the tone, the results showed that the naturalness was (1) high for a combination of high tone (1st) and rising tone (2nd), (2) low for a combination of low tone (3rd) and rising tone (2nd). From these results, the searching time can be reduce by 43% for tone selection, with the same effect on the selection of the succeeding final of one syllable and the preceding initial of the next syllable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-99"
  },
  "chazan02_icslp": {
   "authors": [
    [
     "Dan",
     "Chazan"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Zvi",
     "Kons"
    ],
    [
     "Dorel",
     "Silberstein"
    ],
    [
     "Alexander",
     "Sorin"
    ]
   ],
   "title": "Reducing the footprint of the IBM trainable speech synthesis system",
   "original": "i02_2381",
   "page_count": 4,
   "order": 100,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "This paper presents a novel approach for concatenative speech synthesis. This approach enables reduction of the dataset size of a concatenative text-to-speech system, namely the IBM trainable speech synthesis system, by more than an order of magnitude. A spectral acoustic feature based speech representation is used for computing a cost function during segment selection as well as for speech generation. Initial results indicate that even with a dataset size of a few megabytes it is possible to achieve quality which is significantly higher than existing small footprint formant based synthesizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-100"
  },
  "lee02_icslp": {
   "authors": [
    [
     "Sung-Joo",
     "Lee"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Computationally efficient time-scale modification of speech using 3 level clipping",
   "original": "i02_2385",
   "page_count": 4,
   "order": 101,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "Among the conventional time-scale modification methods [1]- [6], the synchronized overlap and add (SOLA) method [4] is used widely because of its good performance with relatively low computational complexity. But the SOLA method still requires much computation in evaluating the normalized cross-correlation function for synchronization procedure [9]. In this paper, we employ 3 level center clipping method in order to reduce the computational complexity of SOLA method. The result of subjective preference test indicates that the proposed method can reduce computational complexity by over 80% comparing with the conventional SOLA method without considerable performance degradation. We also apply the variable time-scale modification method using transient information [7] to the proposed algorithm. By doing so, we can maintain the intelligibility of time-scale modified speech in the case of very fast playback.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-101"
  },
  "shuang02_icslp": {
   "authors": [
    [
     "Zhi-Wei",
     "Shuang"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "A miniature Chinese TTS system based on tailored corpus",
   "original": "i02_2389",
   "page_count": 4,
   "order": 102,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "Miniature Text to Speech (TTS) systems are broadly applied to embedded system and speech chip, where limited resource requires the corpus to be relatively small and the computing complexity to be low. In general, speech synthesized by conventional miniature TTS systems lacks naturalness due to the limitation of corpus size. In this paper, a method of automatic building a small corpus from a large speech database is described. A new way of distance measurement among candidate instances is also proposed. Based on the tailored corpus, a miniature Chinese TTS system is built, which can produce speech with high naturalness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-102"
  },
  "song02_icslp": {
   "authors": [
    [
     "Hoeun",
     "Song"
    ],
    [
     "Jaein",
     "Kim"
    ],
    [
     "Kyongrok",
     "Lee"
    ],
    [
     "Jinyoung",
     "Kim"
    ]
   ],
   "title": "Phonetic normalization using z-score in segmental prosody estimation for corpus-based TTS system",
   "original": "i02_2393",
   "page_count": 4,
   "order": 103,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "Recently, corpus-based text-to-speech (CB-TTS) has been actively studied through the world. Statistical training methods are generally ap- plied for prosodic rules in CB-TTS, and classification and regression tree (CART) is one of the mostly used methods. In this paper, we present an efficient CART training approach of z-score based phonetic normalization. The idea of ours comes from the fact that the most important three parameters of CART training for segmental prosody are phone and its right and left phones, especially in Korean language. Our approach reduces the number of CART terminal nodes effectively. The reduction ratios are approximately 14-94% for estimation of segmental duration and 45-70% for intensity estimation. Also, the experimental results show that phonetic normalization slightly lessens the estimation errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-103"
  },
  "kawahara02_icslp": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Alain de",
     "Cheveigné"
    ]
   ],
   "title": "On F0 trajectory optimization for very high-quality speech manipulation",
   "original": "i02_2397",
   "page_count": 4,
   "order": 104,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "An optimized fundamental frequency (F0) trajectory extraction method, which alleviates systematic F0 glitches at vowel-nasal boundaries and in the vicinity of consonants, is introduced. The proposed method employs minimum phase group delay compensation for apparent F0 modulations due to variations in their corresponding vocal tract transfer functions. This method can also be considered as an implementation of a generalized version of analysis by synthesis. Evaluation using EGG reference signals revealed that the proposed method reduces the systematic biases by 50%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-104"
  },
  "lee02b_icslp": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "Greg",
     "Kochanski"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Yujia",
     "Li"
    ]
   ],
   "title": "Modeling tones in continuous Cantonese speech",
   "original": "i02_2401",
   "page_count": 4,
   "order": 105,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "Cantonese is a major Chinese dialect with a complicated tone system. This research focuses on quantitative modeling of Cantonese tones. It uses Stem-ML, a language-independent framework for quantitative intonation modeling and generation. A set of F0 prediction models are built, and trained on acoustic data. The prediction error is about 11 Hz or 1 semitone. The resulting optimal model parameters are analyzed in accordance with linguistic knowledge. Key observations include: (1) There is no obvious advantage to model the entering tones separately. They can be considered as simply truncated versions of the non-entering tones; (2) Cantonese appears to have a declining phrase intonation; (3) Tones at initial positions of a phrase or a sentence tend to have a greater prosodic strength than those at the final positions; (4) Content words are stronger than function words; (5) Long words are stronger than short words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-105"
  },
  "dong02_icslp": {
   "authors": [
    [
     "Minghui",
     "Dong"
    ],
    [
     "Kim-Teng",
     "Lua"
    ]
   ],
   "title": "Pitch contour model for Chinese text-to-speech using CART and statistical model",
   "original": "i02_2405",
   "page_count": 4,
   "order": 106,
   "p1": "2405",
   "pn": "2408",
   "abstract": [
    "This paper describes an approach to generating prosody parameters for Mandarin Chinese text-to-speech system. The Chinese fundamental frequency contour is decomposed into two parts, a global intonation contour and a syllable level tone contour. The global intonation contour is converted to pitch target labels in corpus. It is predicted by first predicting pitch target labels using statistical model and classification tree, and then the labels are converted into real pitch values. The local syllable level tone contour is classified into a definite number of contour types using clustering approach. The prediction of local syllable pitch contour is done by classification tree approach. Experiment shows that this approach achieves an accurate prediction result.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-106"
  },
  "navas02_icslp": {
   "authors": [
    [
     "Eva",
     "Navas"
    ],
    [
     "Inmaculada",
     "Hernáez"
    ],
    [
     "Juan María",
     "Sánchez"
    ]
   ],
   "title": "Basque intonation modelling for text to speech conversion",
   "original": "i02_2409",
   "page_count": 4,
   "order": 107,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "The present paper presents the modeling of standard Basque intonation to be used in text to speech conversion systems. The parameterization process of the Basque f0 curves made according to Fujisakis intonation model is explained: experiments made in the placing of the phrase commands of the model are described and the results of these experiments are analyzed. The statistical analysis of the obtained parameters using classification and regression trees is described and the results obtained in this study are also explained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-107"
  },
  "low02_icslp": {
   "authors": [
    [
     "Phuay Hui",
     "Low"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "Application of microprosody models in text to speech synthesis",
   "original": "i02_2413",
   "page_count": 4,
   "order": 108,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "This paper presents a Markovian model of the sequential dependency of the acoustic correlates of speech, namely the pitch trajectory, formants trajectories, the power trajectory, the expected duration of successive phonetic speech segments and the speaking rate for text to speech synthesis. Using a first-order Markov model, microprosody is modeled within biphone segments. Since there is a substantial degree of overlap between biphones and syllables, the method can be considered as an effective model for microprosody within syllables. The model is used to improve the quality of the output of a text to speech synthesis system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-108"
  },
  "zhao02_icslp": {
   "authors": [
    [
     "Sheng",
     "Zhao"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Prosodic phrasing with inductive learning",
   "original": "i02_2417",
   "page_count": 4,
   "order": 109,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "Prosodic phrasing is an important component in modern TTS systems, which inserts natural and reasonable breaks into long utterance. This paper reports the study of applying several inductive machine-learning algorithms to prosodic phrasing in unrestricted Chinese texts. Two feature sets are carefully selected considering the effectiveness and reliability of them in practice. Then features and target boundary labels are extracted from a prepared speech corpus and used as training examples for inductive learning algorithms such as decision tree (C4.5), memory-based learning (MBL) and support vector machines (SVMs). The paper places emphasis on the comparison of the performance and speed of different learning techniques by training and testing them on the same corpus. The experiments show that all the algorithms achieve comparable results for both prosodic word and phrase prediction. It seems that prosodic word can be predicted from Chinese texts more accurately than prosodic phrase when using the same features and learning technique. Inductive learning is a promising way to prosodic phrasing, but its more important to find out good features than to apply different learning algorithms in order to improve the prediction accuracy dramatically.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-109"
  },
  "milner02_icslp": {
   "authors": [
    [
     "Ben",
     "Milner"
    ],
    [
     "Xu",
     "Shao"
    ]
   ],
   "title": "Speech reconstruction from mel-frequency cepstral coefficients using a source-filter model",
   "original": "i02_2421",
   "page_count": 4,
   "order": 110,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "This work presents a method of reconstructing a speech signal from a stream of MFCC vectors using a source-filter model of speech production. The MFCC vectors are used to provide an estimate of the vocal tract filter. This is achieved by inverting the MFCC vector back to a smoothed estimate of the magnitude spectrum. The Wiener- Khintchine theorem and linear predictive analysis transform this into an estimate of the vocal tract filter coefficients. The excitation signal is produced from a series of pitch pulses or white noise, depending on whether the speech is voiced or unvoiced. This pitch estimate forms an extra element of the feature vector. Listening tests reveal that the reconstructed speech is intelligible and of similar quality to a system based on LPC analysis of the original speech. Spectrograms of the MFCC-derived speech and the real speech are included which confirm the similarity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-110"
  },
  "kawanami02_icslp": {
   "authors": [
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Tsuyoshi",
     "Masuda"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Designing Japanese speech database covering wide range in prosody for hybrid speech synthesizer",
   "original": "i02_2425",
   "page_count": 4,
   "order": 111,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "For the purpose of building Text-to-Speech (TTS) system that can generate high-quality and wide range speech in prosody, we conducted speech database construction. As a speech synthesizer, we use a hybrid system which consists of a unit selection module and prosody modification by STRAIGHT (vocoder type high quality analysis- synthesis method). Our viewpoint is to reduce an amount of prosody modification which causes quality deterioration. In other words, it is to generate any prosody at will within permissible prosody modification. Based on the aspect, we designed 9 sub-databases those consist of same phonetic balanced texts with different prosody. In this paper, we describe the designing policy and general features of the obtained database and the results of listening tests focused on the effectiveness about durational feature. They shows the advantage of the proposed database. and but it is also observed the necessity to change unit selection cost function according to output speech rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-111"
  },
  "buhler02_icslp": {
   "authors": [
    [
     "Dirk",
     "Bühler"
    ],
    [
     "Wolfgang",
     "Minker"
    ],
    [
     "Jochen",
     "Häußler"
    ],
    [
     "Sven",
     "Krüger"
    ]
   ],
   "title": "Flexible multimodal human-machine interaction in mobile environments",
   "original": "i02_0169",
   "page_count": 4,
   "order": 112,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This article describes a prototype system for multimodal humanmachine interaction in mobile environments that is being developed within the German national SmartKom research project. The system is novel in that it a) aims at combining the flexibility of state-of-theart handheld digital devices with the computing power of standard PC machinery, and b) connects two substantially different mobile environments, namely pedestrian and car driver.\n",
    "Motivated by the specific safety and privacy considerations and requirements in both environments, we present a framework for flexible modality control. A characteristic feature of our framework is the insight that both user and system may independently and asynchronously initiate a modality transition. We conclude with a brief discussion of further issues and research questions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-112"
  },
  "kaiser02_icslp": {
   "authors": [
    [
     "Edward C.",
     "Kaiser"
    ],
    [
     "Philip R.",
     "Cohen"
    ]
   ],
   "title": "Implementation testing of a hybrid symbolic/statistical multimodal architecture",
   "original": "i02_0173",
   "page_count": 4,
   "order": 113,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "The design and implementation of hybrid symbolic/statistical architectures is a major area of interest in current multimodal system development. Such an architecture attempts to improve multimodal recognition and disambiguation rates by using corpus-based statistics to weight the contributions from various input streams. This is in contrast to current architectures that assume independence between input streams, and combine un-weighted posterior probabilities simply by taking their cross product.\n",
    "Recently a Members, Teams, Committee (MTC) approach for statistically hybridizing the Quickset multimodal system has been put forward on the basis of strong empirical results in an offline analysis. MTC uses small-dimensional input streams as Members, which in turn are input into various Teams where their conditional weights are trained. The Committee then extracts a decision from the output of the Teams. This paper discusses a fully implemented regression test of MTC within Quickset, and our modification of the approach to use more specific training features. We report a relative decrease in multimodal error rate of 30%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-113"
  },
  "yamakata02_icslp": {
   "authors": [
    [
     "Yoko",
     "Yamakata"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Belief network based disambiguation of object reference in spoken dialogue system for robot",
   "original": "i02_0177",
   "page_count": 4,
   "order": 114,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "We are studying joint activity in which a remote robot finds an object by communicating with the user over a voice-only channel. We focus on how the robot disambiguates the reference of the uttered word or phrase to the target object. For example, by \"cup\", one may refer to a \"teacup\", a \"coffee cup\", or even a \"glass\" under some situations. This reference (hereafter, \"object reference\") is user-dependent. We confirm that a user model of object references is significant by conducting a survey of 12 subjects. In addition to ambiguity of object reference, actual systems should cope with two other sources of uncertainty in speech and image recognition. We present a Belief Network based probabilistic reasoning system to determine the object reference. The resulting system demonstrates that the number of interactions needed to find a common reference is reduced as the user model is refined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-114"
  },
  "beskow02_icslp": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "Magnus",
     "Nordstrand"
    ]
   ],
   "title": "Specification and realisation of multimodal output in dialogue systems",
   "original": "i02_0181",
   "page_count": 4,
   "order": 115,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "We present a high level formalism for specifying verbal and nonverbal output from a multimodal dialogue system. The output speci- fication is XML-based and provides information about communicative functions of the output without detailing the realisation of these functions. The specification can be used to control an animated character that uses speech and gestures. We give examples from an implementation in a multimodal spoken dialogue system, and describe how facial gestures are implemented in a (3) D-animated talking agent within this system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-115"
  },
  "quek02_icslp": {
   "authors": [
    [
     "Francis",
     "Quek"
    ],
    [
     "Yingen",
     "Xiong"
    ],
    [
     "David",
     "McNeill"
    ]
   ],
   "title": "Gestural trajectory symmetries and discourse segmentation",
   "original": "i02_0185",
   "page_count": 4,
   "order": 116,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "Our approach is motivated by the conviction that gesture and speech are coexpressive of the underlying dynamic ideation that drives human communication. As such, transitions and cohesions is gestural behavior would inform us as to the discourse conceptualization. In this paper, we examine the role of motion symmetries of twohanded gestures in the structuring of speech. We employ a set of hand motion traces extracted from video and compute the correlation of these traces. The signs and magnitudes of the correlation coefficients computed in the cardinal directions of the subjects torso (lateral and vertical in this work) characterize the symmetries. We employ a windowed computation approach that permits a balance between temporal resolution and robustness to noise. The resulting correlation profiles are merged according to a temporal proximity rule. We apply this analysis to two conversational video sequences. A detailed analysis of the first sequence reveals the persistence of gestural imagery between semantically-similar discourse pieces. A symmetry transition analysis is applied to the second dataset and compared against a manually generated discourse segmentation to demonstrate the potential of cross-modal discourse segmentation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-116"
  },
  "quek02b_icslp": {
   "authors": [
    [
     "Francis",
     "Quek"
    ],
    [
     "David",
     "McNeill"
    ],
    [
     "Robert",
     "Bryll"
    ],
    [
     "Mary",
     "Harper"
    ]
   ],
   "title": "Gestural spatialization in natural discourse segmentation",
   "original": "i02_0189",
   "page_count": 4,
   "order": 117,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "Human multimodal communicative behaviors form a tightly integrated whole. By matching up gestural features with a carefully time-tagged transcription of the speech, we can observe how gesture features and discourse unit transitions cohere. Space usage SU is a key gestural component. We summarize the theory of SU. In our experiments where subjects make action plans around a terrain map, such SU become key organizational loci around which the discourse may be built. Our vision-based approach extracts SU histograms from stereo video describing the locus of motion of a speakers dominant hand. An N ×N fuzzy correlation of these histograms yields a correlation space in which similar SU is clustered. By locating the cluster transitions we can locate topical shifts in the discourse. We show results by comparing the transitions extracted from a sentential coding with a psycholinguistic semantic coding. We do the same with a uniform distributed time units and demonstrate the ability to recover discourse transitions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-117"
  },
  "nakadai02_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Hiroaki",
     "Kitano"
    ]
   ],
   "title": "Real-time sound source localization and separation for robot audition",
   "original": "i02_0193",
   "page_count": 4,
   "order": 118,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "Robot audition in the real world should cope with environment noises and reverberation and motor noises caused by the robots own movements. This paper presents the active direction-pass filter (ADPF) to separate sounds originating from the specified direction with a pair of microphones. The ADPF is implemented by hierarchical integration of visual and auditory processing with hypothetical reasoning on interaural phase difference (IPD) and interaural intensity difference (IID) for each subband. In creating hypotheses, the reference data of IPD and IID is calculated by the auditory epipolar geometry on demand. Since the performance of the ADPF depends on the direction, the ADPF controls the direction by motor movement. The human tracking and sound source separation based on the ADPF is implemented on an upper-torso humanoid and runs in real-time with 4 PCs connected over Gigabit ethernet. The signal-to-noise ratio (SNR) of each sound separated by the ADPF from a mixture of two speeches with the same loudness is improved to about 10 dB from 0 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-118"
  },
  "ma02_icslp": {
   "authors": [
    [
     "Jiyong",
     "Ma"
    ],
    [
     "Jie",
     "Yan"
    ],
    [
     "Ronald",
     "Cole"
    ]
   ],
   "title": "CU animate tools for enabling conversations with animated characters",
   "original": "i02_0197",
   "page_count": 4,
   "order": 119,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "In this paper, we describe CU Animate: a set of software tools for researching full-bodied three-dimensional animated characters, and for controlling and rendering them in real time. Presently, eight complete characters are included with the system. Each character has a fully articulated skeleton, a set of viseme targets for the phonemes of English, a tongue that moves to target states for each phoneme, and a coarticulation model that controls the movements of the articulators between phonemes during speech production. A set of authoring tools enables designers to create arbitrary animation sequences. A text markup language enables authors to control facial expressions and gestures during dialogue interaction and narration of text. CU Animate has been integrated into the Galaxy architecture within the CU Communicator system, enabling mixed-initiative conversational interaction with animated characters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-119"
  },
  "cohen02_icslp": {
   "authors": [
    [
     "Philip R.",
     "Cohen"
    ],
    [
     "Rachel",
     "Coulston"
    ],
    [
     "Kelly",
     "Krout"
    ]
   ],
   "title": "Multiparty multimodal interaction: a preliminary analysis",
   "original": "i02_0201",
   "page_count": 4,
   "order": 120,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "When people work together, they often talk about the objects in their environment. Not surprisingly, their dialogues are multimodal, incorporating speech, gesture, gaze, haptics, and perhaps other modalities. However, proponents of technology may be troubled to learn that despite the current state and future promise of spoken and multimodal research, many of these workers do not particularly want to talk to machines - they want to converse with their colleagues. Still, if there were unobtrusive computer support for their multimodal dialogues, these same individuals would be pleased to benefit from digital technology. This paper offers a first step towards building such multimodal systems for supporting face-to-face collaborative work by providing both qualitative and quantitative analyses of multiparty multimodal dialogues in a field setting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-120"
  },
  "poller02_icslp": {
   "authors": [
    [
     "Peter",
     "Poller"
    ],
    [
     "Jochen",
     "Müller"
    ]
   ],
   "title": "Distributed audio-visual speech synchronization",
   "original": "i02_0205",
   "page_count": 4,
   "order": 121,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "The main scientific goal of the SmartKom project is to develop a new human-machine interaction metaphor for multimodal dialog systems. It combines speech, gesture, and facial expression input with speech, gesture and graphics output. The system is realized as a distributed collection of communicating and cooperating autonomous modules based on a multi-blackboard architecture. Multimodal output generation is consequently separated in two steps. First, the modality-specific output data are generated. Second, an inter-media synchronization of these data is realized on independent media devices to perform the multimodal presentation to the user. This paper describes the generation of appropriate lip animations that are based on a phonetic representation of the speech output signal and as a second computational step the timestamp based realization of audio-visual speech output on distributed media devices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-121"
  },
  "daubias02_icslp": {
   "authors": [
    [
     "Philippe",
     "Daubias"
    ],
    [
     "Paul",
     "Deléglise"
    ]
   ],
   "title": "Lip-reading based on a fully automatic statistical model",
   "original": "i02_0209",
   "page_count": 4,
   "order": 122,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "In this paper, we describe audiovisual automatic speech recognition experiments carried using visual parameters extracted from \"natural\" images. Unlike many other experiments in the AV ASR field, these visual parameters are obtained without any hand-labeling phase and are naturally noisy, due to the extraction process. We evaluate our models with different strategies among which : use of a shape model combined with or after an appearance model. For audiovisual parameters integration, we use a basic DI architecture with a fixed weight. We use a new evaluation criterion to measure the quality of parameters which proves to be efficient, and aim to use it in the near future, for an adaptive weighting scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-122"
  },
  "liu02_icslp": {
   "authors": [
    [
     "Xiaoxing",
     "Liu"
    ],
    [
     "Yibao",
     "Zhao"
    ],
    [
     "Xiaobo",
     "Pi"
    ],
    [
     "Luhong",
     "Liang"
    ],
    [
     "Ara V.",
     "Nefian"
    ]
   ],
   "title": "Audio-visual continuous speech recognition using a coupled hidden Markov model",
   "original": "i02_0213",
   "page_count": 4,
   "order": 123,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "With the increase in the computational complexity of recent computers, audio-visual speech recognition (AVSR) became an attractive research topic that can lead to a robust solution for speech recognition in noisy environments. In the audio visual continuous speech recognition system presented in this paper, the audio and visual observation sequences are integrated using a coupled hidden Markov model (CHMM). The statistical properties of the CHMM can describe the asyncrony of the audio and visual features while preserving their natural correlation over time. The experimental results show that the current system tested on the XM2VTS database reduces the error rate of the audio only speech recognition system at SNR of 0db by over 55%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-123"
  },
  "dybkjr02_icslp": {
   "authors": [
    [
     "Laila",
     "Dybkjær"
    ],
    [
     "Niels Ole",
     "Bernsen"
    ]
   ],
   "title": "Data, annotation schemes and coding tools for natural interactivity",
   "original": "i02_0217",
   "page_count": 4,
   "order": 124,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "This paper briefly presents results of three surveys of natural interactivity and multimodal resources carried out by a Working Group in the ISLE project on International Standards for Language Engineering. Information has been collected on a large number of corpora, coding schemes and coding tools world-wide.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-124"
  },
  "quek02c_icslp": {
   "authors": [
    [
     "Francis",
     "Quek"
    ],
    [
     "Yang",
     "Shi"
    ],
    [
     "Cemil",
     "Kirbas"
    ],
    [
     "Shunguang",
     "Wu"
    ]
   ],
   "title": "VisSTA: a tool for analyzing multimodal discourse data",
   "original": "i02_0221",
   "page_count": 4,
   "order": 125,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "Human communication, seen in the broader sense, is multimodal involving the words spoken, prosody, hand gestures, head and eye gestures, body posture variation and facial expression. We present the multimedia Visualization for Situated Temporal Analysis (VisSTA) system for the analysis of multimodal human communication video, audio, speech transcriptions, and gesture and head orientation data. VisSTA is based on the Multiple Linked Representation, MLR strategy and keeps the user temporally situated by ensuring tight linkage among all interface components. Each component serves both as a system controller and display keeping every data element being visualized synchronized with the current time focus. VisSTA maintains multiple representations that include a hierarchical video-shot organization, a variety of animated graphs, animated time synchronized multi-tier text transcriptions, and an avatar representation. All data is synchronized with the underlying video.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-125"
  },
  "lambacher02_icslp": {
   "authors": [
    [
     "Stephen",
     "Lambacher"
    ],
    [
     "William",
     "Martens"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ]
   ],
   "title": "The influence of identification training on identification and production of the american English mid and low vowels by native speakers of Japanese",
   "original": "i02_0245",
   "page_count": 4,
   "order": 126,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "Vowel identification and production performance for an experimental group of 35 Japanese subjects was measured before and after a 6-week period within which subjects received identification training with feedback. A control group of 9 Japanese subjects was administered the same identification and pre- and post-test, separated by the same 6-week interval during which they did not receive the experimental vowel identification training that provided feedback regarding which vowel sound had been produced by the native American English talkers. The results showed that the experimental groups identification performance (as measured by d) for each of the five American English (AE) vowels /æ/, /A/, /2/, /O/, /Ç/ improved more than the control groups did. Both before and after the 6-week identification- training period, recordings were made of the experimental groups productions of a list of words in a varied [CVC] context, each containing one of the five target AE vowels, which were evaluated by 3 AE native listeners using a 2-interval, forced-choice identification task. The results showed the AE listeners preferred the post-test to pretest productions for four out of the five target AE vowels. Overall, the results indicate the feedback-based identification training had a positive effect on both the experimental groups identification and production performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-126"
  },
  "tajima02_icslp": {
   "authors": [
    [
     "Keiichi",
     "Tajima"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Tsuneo",
     "Yamada"
    ]
   ],
   "title": "Perceptual learning of second-language syllable rhythm by elderly listeners",
   "original": "i02_0249",
   "page_count": 4,
   "order": 127,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "Past studies attempting to train second-language (L2) learners to accurately perceive L2 speech have focused primarily on non-native segmental contrasts, using young, college-aged listeners as trainees. To examine whether similar training methods improve perception of prosodic properties of L2, and whether older listeners can benefit from such training, the present study investigated the effect of auditory training using feedback on elderly native Japanese listeners perception of relatively complex syllables in spoken English words. Monolingual Japanese listeners aged 60-69 were trained to identify the number of syllables in spoken English words. Before training, listeners correctly counted syllables 56% of the time. After roughly three weeks of training, however, listeners correctly responded 91% of the time. Furthermore, listeners responses were less affected by syllable complexity of the stimuli after training than before. Results suggest that auditory training is effective for improving L2 syllable perception, even for elderly listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-127"
  },
  "clarke02_icslp": {
   "authors": [
    [
     "Constance M.",
     "Clarke"
    ]
   ],
   "title": "Perceptual adjustment to foreign-accented English with short term exposure",
   "original": "i02_0253",
   "page_count": 4,
   "order": 128,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "Non-native, or foreign-accented, speech deviates from native pronunciation norms, and these deviations can cause perceptual dif- ficulty for listeners. However, there is reason to believe that nonnative speech can be learned by the adult perceptual system. The present study investigated perceptual learning of foreign-accented speech with very limited experience. Sixteen auditory sentences were presented to two groups whose task was to indicate whether an orthographic word presented immediately after each sentence matched the final word of the sentence. The experimental group heard 16 Spanish-accented sentences. For the control group, only the last four sentences were accented; the rest were non-accented. The experimental groups response time decreased across the sixteen sentences, suggesting they rapidly adjusted to the accented speech. The experimental group was also faster for the last four accented sentences compared to the control group, indicating learning was not completely due to practice with the task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-128"
  },
  "burnham02_icslp": {
   "authors": [
    [
     "Denis K.",
     "Burnham"
    ],
    [
     "Ron",
     "Brooker"
    ]
   ],
   "title": "Absolute pitch and lexical tones: tone perception by non-musician, musician, and absolute pitch non-tonal language speakers",
   "original": "i02_0257",
   "page_count": 4,
   "order": 129,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In this paper we investigate whether musically trained non-tonal language speakers perceive lexical tone better than their non-musician counterparts. Three groups of English language speakers, non-musicians, musicians, and musicians with absolute pitch (n=24, N=72), were tested for same/different discrimination of Central Thai tone pairs. These were presented in three separate conditions: as speech (on the syllable [ba]), as filtered speech, or as violin sounds. Non-musicians discriminated tones better in music than in filtered speech, and better in each of these than in speech. Musicians without absolute pitch showed the same pattern of results but were better in all three contexts compared with the non-musicians. On the other hand absolute pitch musicians were equally good in all three contexts, and better overall than the other musicians and the non-musicians. It is concluded that speech and music perception are not independent: musical training and absolute pitch ability may affect speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-129"
  },
  "broersma02_icslp": {
   "authors": [
    [
     "Mirjam",
     "Broersma"
    ]
   ],
   "title": "Comprehension of non-native speech: inaccurate phoneme processing and activation of lexical competitors",
   "original": "i02_0261",
   "page_count": 4,
   "order": 130,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "Native speakers of Dutch with English as a second language and native speakers of English participated in an English lexical decision experiment. Phonemes in real words were replaced by others from which they are hard to distinguish for Dutch listeners. Non-native listeners judged the resulting near-words more often as a word than native listeners. This not only happened when the phonemes that were exchanged did not exist as separate phonemes in the native language Dutch, but also when phoneme pairs that do exist in Dutch were used in word-final position, where they are not distinctive in Dutch. In an English bimodal priming experiment with similar groups of participants, word pairs were used which differed in one phoneme. These phonemes were hard to distinguish for the non-native listeners. Whereas in native listening both words inhibited each other, in non-native listening presentation of one word led to unresolved competition between both words. The results suggest that inaccurate phoneme processing by non-native listeners leads to the activation of spurious lexical competitors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-130"
  },
  "minker02_icslp": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Overview on recent activities in speech understanding and dialogue systems evaluation",
   "original": "i02_0265",
   "page_count": 4,
   "order": 131,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "This paper discusses issues in evaluating spoken language dialogue systems in terms of technical performance and end-user acceptance. Recent efforts in this domain have been carried out in the framework of two major research initiatives: the European Esprit longterm project Spoken Language Dialogue Systems and Components - Best practice in development and evaluation (DISC) and the US American DARPA COMMUNICATOR project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-131"
  },
  "walker02_icslp": {
   "authors": [
    [
     "Marilyn A.",
     "Walker"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Rashmi",
     "Prasad"
    ],
    [
     "John",
     "Aberdeen"
    ],
    [
     "Elizabeth Owen",
     "Bratt"
    ],
    [
     "John S.",
     "Garofolo"
    ],
    [
     "Helen",
     "Hastie"
    ],
    [
     "Audrey N.",
     "Le"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Alex",
     "Potamianos"
    ],
    [
     "Rebecca",
     "Passonneau"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "Gregory A.",
     "Sanders"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "David",
     "Stallard"
    ]
   ],
   "title": "DARPA communicator: cross-system results for the 2001 evaluation",
   "original": "i02_0269",
   "page_count": 4,
   "order": 132,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper describes the evaluation methodology and results of the 2001 DARPA Communicator evaluation. The experiment spanned 6 months of 2001 and involved eight DARPA Communicator systems in the travel planning domain. It resulted in a corpus of 1242 dialogs which include many more dialogues for complex tasks than the 2000 evaluation. We describe the experimental design, the approach to data collection, and the results. We compare the results by the type of travel plan and by system. The results demonstrate some large differences across sites and show that the complex trips are clearly more difficult.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-132"
  },
  "walker02b_icslp": {
   "authors": [
    [
     "Marilyn A.",
     "Walker"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "John",
     "Aberdeen"
    ],
    [
     "Elizabeth Owen",
     "Bratt"
    ],
    [
     "John S.",
     "Garofolo"
    ],
    [
     "Helen",
     "Hastie"
    ],
    [
     "Audrey N.",
     "Le"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Alex",
     "Potamianos"
    ],
    [
     "Rebecca",
     "Passonneau"
    ],
    [
     "Rashmi",
     "Prasad"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "Gregory A.",
     "Sanders"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "David",
     "Stallard"
    ]
   ],
   "title": "DARPA communicator evaluation: progress from 2000 to 2001",
   "original": "i02_0273",
   "page_count": 4,
   "order": 133,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "This paper describes the evaluation methodology and results of the DARPA Communicator spoken dialog system evaluation experiments in 2000 and 2001. Nine spoken dialog systems in the travel planning domain participated in the experiments resulting in a total corpus of 1904 dialogs. We describe and compare the experimental design of the 2000 and 2001 DARPA evaluations. We describe how we established a performance baseline in 2001 for complex tasks. We present our overall approach to data collection, the metrics collected, and the application of PARADISE to these data sets. We compare the results we achieved in 2000 for a number of core metrics with those for 2001. These results demonstrate large performance improvements from 2000 to 2001 and show that the Communicator program goal of conversational interaction for complex tasks has been achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-133"
  },
  "sanders02_icslp": {
   "authors": [
    [
     "Gregory A.",
     "Sanders"
    ],
    [
     "Audrey N.",
     "Le"
    ],
    [
     "John S.",
     "Garofolo"
    ]
   ],
   "title": "Effects of word error rate in the DARPA communicator data during 2000 and 2001",
   "original": "i02_0277",
   "page_count": 4,
   "order": 134,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "During 2000 and 2001 two large data collections were performed, with paid users. We analyze the effects of speech recognition accuracy, as measured by Word Error Rate (WER), on other metrics. Analysis shows a linear correlation between WER and the Task Completion metrics, and (unexpectedly) this relationship remains more or less linear even for quite high values of WER. The picture for User Satisfaction metrics is more complex, and a linear model derived from the data by using the PARADISE framework [1] is given by Walker et al. [2]. We present evidence suggesting a somewhat linear relationship between WER and User Satisfaction for WER less than 35% or 40% in 2001, compared to stronger correlations in 2000. Finally, we note that the size of effect of increasing WER on Task Completion (slope of the least-squares regression line) appears to be about half as large in 2001 as in 2000, which we attribute to improved strategies for accomplishing tasks despite speech recognition errors. We consider this to be an important accomplishment of the research groups who built the Communicator implementations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-134"
  },
  "sidner02_icslp": {
   "authors": [
    [
     "Candace L.",
     "Sidner"
    ],
    [
     "Clifton",
     "Forlines"
    ]
   ],
   "title": "Subset languages for conversing with collaborative interface agents",
   "original": "i02_0281",
   "page_count": 4,
   "order": 135,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "This paper reports on experiments with subjects who must learn to use a small artificially constructed subset language (of English) to interact with a conversational spoken language system. The subjects converse with a collaborative interface agent about tasks involving TV recording and schedule navigation. The subjects perform their tasks in two conditions, one with contextually appropriate help on what to say always available on the screen and those with only a help sheet that they request to see. Our experiments indicate that users can perform their tasks in either condition, but demonstrate limits in remembering the language in a subsequent session.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-135"
  },
  "watanabe02_icslp": {
   "authors": [
    [
     "Tomomi",
     "Watanabe"
    ],
    [
     "Takahiro",
     "Murakami"
    ],
    [
     "Munehiro",
     "Namba"
    ],
    [
     "Tetsuya",
     "Hoya"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Transformation of spectral envelope for voice conversion based on radial basis function networks",
   "original": "i02_0285",
   "page_count": 4,
   "order": 136,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "This paper presents a novel algorithm that modifies the speech uttered by a source speaker to sound as if produced by a target speaker. In particular, we address the issue of transformation of the vocal tract characteristics from one speaker to another. The approach is based on estimating spectral envelopes using radial basis function (RBF) networks, which is one of the well-known models of artificial neural networks. The simulation results show that the proposed method achieves nearly optimal spectral conversion performance. Moreover, average cepstrum distance to the target speech is reduced by 87%, and in the listening tests, around 84% of mean opinion score (MOS) is obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-136"
  },
  "turk02_icslp": {
   "authors": [
    [
     "Oytun",
     "Turk"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "Subband based voice conversion",
   "original": "i02_0289",
   "page_count": 4,
   "order": 137,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "A new voice conversion method that improves the quality of the voice conversion output at higher sampling rates is proposed. Speaker Transformation Algorithm Using Segmental Codebooks (STASC) is modified to process source and target speech spectra in different subbands. The new method ensures better conversion at sampling rates above 16KHz. Discrete Wavelet Transform (DWT) is employed for subband decomposition to estimate the speech spectrum better with higher resolution. Faster voice conversion is achieved since the computational complexity decreases at a lower sampling rate. A Voice Conversion System (VCS) is implemented using the proposed algorithm with necessary tools. The performance of the proposed method is demonstrated by both subjective listening tests and applications to film dubbing and looping. In ABX listening tests, the listeners preferred the subband based output by 92.1% as compared to the full-band based output.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-137"
  },
  "mashimo02_icslp": {
   "authors": [
    [
     "Mikiko",
     "Mashimo"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Evaluation of cross-language voice conversion using bilingual and non-bilingual databases",
   "original": "i02_0293",
   "page_count": 4,
   "order": 138,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "Cross-language voice conversion is useful for many applications, and we are trying to apply the technique to a language training system for reducing voice individuality differences. In this paper, we describe experiments that test effectiveness of an extension of singlelanguage voice conversion, to include cross-language utterances. The performance was investigated by objective and perceptual evaluation using bilingual-speakers data for training. Then, the correlations between a computed distance measure and a human perceptual pronunciation evaluation score were compared before and after applying conversion. From these results, it was found that the crosslanguage voice conversion reduces speakers voice differences between the pairs, and the phoneme based measures show somewhat clearer correspondences to the human perceptual score in vowels test after applying voice conversion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-138"
  },
  "gustafson02_icslp": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Kåre",
     "Sjölander"
    ]
   ],
   "title": "Voice transformations for improving children²s speech recognition in a publicly available dialogue system",
   "original": "i02_0297",
   "page_count": 4,
   "order": 139,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "To be able to build acoustic models for children, that can be used in spoken dialogue systems, speech data has to be collected. Commercial recognizers available for Swedish are trained on adult speech, which makes them less suitable for childrens computer-directed speech. This paper describes some experiments with on-the-fly voice transformation of childrens speech. Two transformation methods were tested, one inspired by the Phase Vocoder algorithm and another by the Time-Domain Pitch-Synchronous Overlap-Add (TD-PSOLA) algorithm. The speech signal is transformed before being sent to the speech recognizer for adult speech. Our results show that this method reduces the error rates in the order of thirty to forty-five percent for children users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-139"
  },
  "burger02_icslp": {
   "authors": [
    [
     "Susanne",
     "Burger"
    ],
    [
     "Victoria",
     "MacLaren"
    ],
    [
     "Hua",
     "Yu"
    ]
   ],
   "title": "The ISL meeting corpus: the impact of meeting type on speech style",
   "original": "i02_0301",
   "page_count": 4,
   "order": 140,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "Speech research is becoming very interested in new application domains, such as meeting summarization and automatic transcription, and has thus begun to work with recorded meeting data. The following paper gives an overview of the meeting data collection at Interactive Systems Laboratories. There are currently over 100 meetings of different types recorded. An experiment is described which aimed at testing the possibility of controlling issues of speaking style by meeting type. Results show that depending on the meeting type, speaking style varies in terms of turn length, speed and disfluencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-140"
  },
  "lopezcozar02_icslp": {
   "authors": [
    [
     "R.",
     "López-Cózar"
    ],
    [
     "Ángel de la",
     "Torre"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "J. M.",
     "López-Soler"
    ]
   ],
   "title": "A new method for testing dialogue systems based on simulations of real-world conditions",
   "original": "i02_0305",
   "page_count": 4,
   "order": 141,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This paper presents a new method for testing dialogue systems using a variety of real-world conditions simulated in lab. The method is based on the use of an additional dialogue system, called simulator, designed to behave as users interacting with the dialogue system to test. The behavior of the simulator is decided from diverse scenarios that represent user goals. The simulator tries to achieve the goals in the scenarios during the interaction with the dialogue system. We applied the method to test in noise conditions a dialogue system under development in our lab, considering white and babble noise. The method allowed to find out errors in the recognizer and in the strategy the system uses to handle user confirmations. Using the method, we tested the behavior of the system in terms of task completion, taking into account a VTS noise compensation technique. The experiments show that, if the VTS technique is not used, the average task completion is 31,43% for the white noise and 29,64% for the babble noise. If the VTS technique is used, the average task completion increases to 57,35% for the white noise and to 54,44% for the babble noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-141"
  },
  "ludwig02_icslp": {
   "authors": [
    [
     "Thorsten",
     "Ludwig"
    ]
   ],
   "title": "Comfort noise detection and GSM-FR-codec detection for speech-quality evaluations in telephone networks",
   "original": "i02_0309",
   "page_count": 4,
   "order": 142,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "This paper proposes two algorithms to measure special quality- relevant characteristics of telephone links. The first algorithm presented here allows to detect the GSM-FR codec in transmission systems. For this purpose, the spectral region of the decoded signal around 2700 Hz is evaluated. The GSM-FR coding principle inserts a spectral attenuation in this frequency region that can be detected. The error rate is below 5%. The purpose of the second algorithm is to detect comfort noise in telephone connections. Therefore, frequency points of the background-noise spectrum throughout the duration of speech utterances are sampled, by making use of minimum statistics in frequency-tracks of speech segments. These frequency points are compared to the noise in speech pauses in a statistical manner to evaluate differences and decide about the occurrence of comfort noise. The error rate for the used data base is below 5%, but further investigations are necessary to verify this algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-142"
  },
  "cucchiarini02_icslp": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ]
   ],
   "title": "Validation and improvement of automatic phonetic transcriptions",
   "original": "i02_0313",
   "page_count": 4,
   "order": 143,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "The ultimate aim of our research is to show that good-quality phonetic transcriptions of large speech corpora can be obtained by employing automatic techniques initially developed for ASR. The experiment presented in this paper has two aims. The first is to show how the quality of an automatic transcription that is easily obtained through lexicon lookup can be measured in a way that is methodologically sound. The second is to show how, while measuring the quality of an automatic transcription, it is possible to obtain information that can subsequently be used to improve the automatic transcription where necessary. As a result, correction by human transcribers should become more efficient or even superfluous.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-143"
  },
  "aman02_icslp": {
   "authors": [
    [
     "Shigeaki",
     "Aman"
    ],
    [
     "Kazumi",
     "Kato"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "Development of Japanese infant speech database and speaking rate analysis",
   "original": "i02_0317",
   "page_count": 4,
   "order": 144,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "Utterances of five infants with their parents were recorded every month from their birth until 30 to 60 months old to investigate spoken language development from the viewpoint of acoustic characteristics. Recording time was at least one hour per month. An infant speech database is now being developed from the recordings. Each entry of the database contains a speech file of an utterance and its transcription with some property tags such as speaker, background noise level, and utterance characteristics. A search program with WEB browsing interface was developed for the contents of the database. Using the database, preliminary analysis was conducted on speaking rate. It was observed that infant speaking rate is much slower than parent speaking rate and parent speaking rate is almost equal to the speaking rate of adult conversation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-144"
  },
  "dong02b_icslp": {
   "authors": [
    [
     "Minghui",
     "Dong"
    ],
    [
     "Kim-Teng",
     "Lua"
    ]
   ],
   "title": "Automatic prosodic break labeling for Mandarin Chinese speech data",
   "original": "i02_0321",
   "page_count": 4,
   "order": 145,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "For corpus-based speech synthesis, large quantities of labeled speech are required. Manually labeling speech data is quite labor-intensive. Therefore, automatic speech labeling is highly desired. Prosodic break detection is one of the tasks for automatic speech labeling. In the paper, we propose an automatic break detection algorithm for mandarin Chinese speech. In this approach, we use energy contour to normalize duration of syllables and use the concept of normalized transition time to represent the time interval between two syllables. A recursive algorithm is then used to select locally longer intervals as pauses. Language specific constraint rules are also used to produce a better judgment. The automatic break labeling results have been proved to be good.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-145"
  },
  "zitouni02_icslp": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Joseph",
     "Olive"
    ],
    [
     "Dorota",
     "Iskra"
    ],
    [
     "Khalid",
     "Choukri"
    ],
    [
     "Ossama",
     "Emam"
    ],
    [
     "Oren",
     "Gedge"
    ],
    [
     "Emmanuel",
     "Maragoudakis"
    ],
    [
     "Herbert",
     "Tropf"
    ],
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Albino Nogueiras",
     "Rodriguez"
    ],
    [
     "Barbara",
     "Heuft"
    ],
    [
     "Rainer",
     "Siemund"
    ]
   ],
   "title": "Orientel: speech-based interactive communication applications for the mediterranean and the middle east",
   "original": "i02_0325",
   "page_count": 4,
   "order": 146,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "In this paper, we introduce a new European project named Orien- Tel. The aim of OrienTel is to enable the projects participants to design and develop multilingual interactive communication services for the Mediterranean and the Middle East, ranging from Morocco in the West to the Gulf states in the East, including Turkey and Cyprus. These multilingual applications will be largely speech-based and will typically be implemented on mobile and multi-modal platforms such as cellular GSM or UMTS phones, personal digital assistants (PDAs) or combinations of the two. Applications of the kind targeted are uni- fied messaging, information retrieval, customer care, banking, WAP and service portals. To achieve this aim, the consortium will produce various surveys of the OrienTel region, compile a set of 22 linguistic databases, conduct research into ASR-related problems the OrienTel languages hold and develop demonstrator applications bearing evidence of OrienTels multilingual orientation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-146"
  },
  "alvarez02_icslp": {
   "authors": [
    [
     "Yolanda Vazquez",
     "Alvarez"
    ],
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "The reliability of the ITU-t p.85 standard for the evaluation of text-to-speech systems",
   "original": "i02_0329",
   "page_count": 4,
   "order": 147,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "An evaluation of the reliability of the ITU-T P.85 recommended standard for the evaluation of voice output systems was conducted using six English TTS systems. The P.85 standard is based on meanopinion- score judgements of a listening panel on a number of rating scales. The study looked at how the ranking of the six systems on the scales varied across four different text genres and across two listening sessions. Rankings were also compared with a much simpler pair-comparison test across genres and listening sessions. For the ITU test a large degree of correlation was found across scales, implying that these were not really testing different aspects of the systems. There were surprisingly similar results across sessions, implying that listeners were indeed making real judgements. In comparison, the pair comparison test gave (almost) identical rankings for systems with far less variability, making statistically significant comparisons between systems possible, even across genres.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-147"
  },
  "demuynck02_icslp": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Tom",
     "Laureys"
    ],
    [
     "Steven",
     "Gillis"
    ]
   ],
   "title": "Automatic generation of phonetic transcriptions for large speech corpora",
   "original": "i02_0333",
   "page_count": 4,
   "order": 148,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "We describe a method for the automatic production of phonetic transcriptions in large speech corpora. First, we focus on the application of different techniques for the generation of pronunciation variants. Then, we explain the application of a speech recognition system for selecting the acoustically best matching phonetic transcription. The system is evaluated on different test sets selected from the Spoken Dutch Corpus, ranging from read-aloud text to spontaneous speech, and achieves promising first results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-148"
  },
  "minker02b_icslp": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Overview on recent activities in speech understanding and dialogue systems evaluation",
   "original": "i02_0337",
   "page_count": 4,
   "order": 149,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "This paper discusses issues in evaluating spoken language dialogue systems in terms of technical performance and end-user acceptance. Recent efforts in this domain have been carried out in the framework of two major research initiatives: the European Esprit longterm project Spoken Language Dialogue Systems and Components - Best practice in development and evaluation (DISC) and the US American DARPA COMMUNICATOR project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-149"
  },
  "bennett02_icslp": {
   "authors": [
    [
     "Christina",
     "Bennett"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "The carnegie mellon communicator corpus",
   "original": "i02_0341",
   "page_count": 4,
   "order": 150,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "As part of the DARPA Communicator program, Carnegie Mellon has, over the past three years, collected a large corpus of speech produced by callers to its Travel Planning system. To date, a total of 180,605 utterances (90.9 hours) have been collected. The data were used for a number of purposes, including acoustic and language modeling and the development of a spoken dialog system. The collection, transcription and annotation of these data prompted us to develop a number of procedures for managing the transcription process and for ensuring accuracy. We describe these, as well as some results based on these data. A portion of this corpus, covering the years 1999-2001, is being published for research purposes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-150"
  },
  "schultz02_icslp": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Globalphone: a multilingual speech and text database developed at karlsruhe university",
   "original": "i02_0345",
   "page_count": 4,
   "order": 151,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "This paper describes the design, collection, and current status of the multilingual database GlobalPhone, an ongoing project since 1995 at Karlsruhe University. GlobalPhone is a high quality read speech and text database in a large variety of languages which is suitable for the development of large vocabulary speech recognition systems in many languages. It has already been successfully applied to language independent and language adaptive speech recognition. GlobalPhone currently covers 15 languages Arabic, Chinese (Mandarin and Shanghai), Croatian, Czech, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. The corpus contains more than 300 hours of transcribed speech spoken by more than 1500 native, adult speakers and will soon be available from ELRA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-151"
  },
  "salor02_icslp": {
   "authors": [
    [
     "Özgül",
     "Salor"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Tolga",
     "Çiloglu"
    ],
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Mübeccel",
     "Demirekler"
    ]
   ],
   "title": "On developing new text and audio corpora and speech recognition tools for the turkish language",
   "original": "i02_0349",
   "page_count": 4,
   "order": 152,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "This paper describes recent work towards development of new corpora and tools for Turkish speech research. This effort represents an on-going collaboration between the Center for Spoken Language Research (CSLR) at the University of Colorado and the Department of Electrical Engineering at the Middle East Technical University (METU). A new text corpus developed from Turkish newspapers text is described. In addition, a 193-speaker audio corpus and pronunciation lexicon for the Turkish language is developed. We then describe our initial work towards porting Sonic, the CSLR speech recognition system, to the Turkish language. Results are shown for phonetic alignment and phoneme recognition accuracy using the newly constructed corpus and speech tools. It is shown that 91.2% of the automatically labeled phoneme boundaries are placed within 20 msec of hand-labeled locations for the Turkish audio corpus. Finally, a phoneme recognition error rate of 29.3% is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-152"
  },
  "martell02_icslp": {
   "authors": [
    [
     "Craig",
     "Martell"
    ]
   ],
   "title": "FORM: an extensible, kinematically-based gesture annotation scheme",
   "original": "i02_0353",
   "page_count": 4,
   "order": 153,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "Annotated corpora have played a critical role in speech and natural language research; and, there is an increasing interest in corporabased research in sign language and gesture as well. We present a non-semantic, geometrically-based annotation scheme, FORM, which allows an annotator to capture the kinematic information in a gesture just from videos of speakers. In addition, FORM stores this gestural information in Annotation Graph format-allowing for easy integration of gesture information with other types of communication information, e.g., discourse structure, parts of speech, intonation information, etc.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-153"
  },
  "hosom02_icslp": {
   "authors": [
    [
     "John-Paul",
     "Hosom"
    ]
   ],
   "title": "Automatic phoneme alignment based on acoustic-phonetic modeling",
   "original": "i02_0357",
   "page_count": 4,
   "order": 154,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "This paper presents a method for speaker-independent automatic phonetic alignment that is distinguished from standard HMM-based \"forced alignment\" in three respects: (1) specific acoustic-phonetic features are used, in addition to PLP features, by the phonetic classifier; (2) the units of classification consist of distinctive phonetic features instead of phonemes; and (3) observation probabilities depend not only on the current state, but also on the state transition information. This proposed method is compared with a state-of-the-art baseline forced-alignment system on a number of corpora, including telephone speech, microphone speech, and childrens speech. The new method has agreement of 92.57% within 20 msec on the TIMIT corpus, which is a 26% reduction in error over the baseline method (with 89.95% agreement on TIMIT). Average reduction in error over all corpora is 28%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-154"
  },
  "gupta02_icslp": {
   "authors": [
    [
     "Narendra K.",
     "Gupta"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Extracting clauses for spoken language understanding in conversational systems",
   "original": "i02_0361",
   "page_count": 4,
   "order": 155,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "Spontaneous human utterances in the context of human-human and human-machine dialogs are rampant with dysfluencies, and speech repairs. Furthermore, when recognized using a speech recognizer, these utterances produce a sequence of words with no identification of clausal units. Such long strings of words combined with speech errors pose a difficult problem for spoken language parsing and understanding. In this paper, we address the issue of editing speech repairs as well as segmenting user utterances into clause units with a view of parsing and understanding spoken language utterances. We present generative and discriminative models for this task and present evaluation results on the human-human conversations obtained from the Switchboard corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-155"
  },
  "lefevre02_icslp": {
   "authors": [
    [
     "F.",
     "Lefèvre"
    ],
    [
     "H.",
     "Bonneau-Maynard"
    ]
   ],
   "title": "Issues in the development of a stochastic speech understanding system",
   "original": "i02_0365",
   "page_count": 4,
   "order": 156,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "In the development of a speech understanding system, the recourse to stochastic techniques can greatly reduce the need for human expertise. A known disadvantage is that stochastic models require large annotated training corpora in order to reliably estimate model parameters. Manual semantic annotation of such corpora is tedious, expensive, and subject to inconsistencies. In order to decrease the development cost, this work investigates the performance of stochastic understanding models with two parameters: the use of automatically segmented data and the use of automatically learned lexical normalisation rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-156"
  },
  "pfitzinger02_icslp": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "10 years of phondat-II: a reassessment",
   "original": "i02_0369",
   "page_count": 4,
   "order": 157,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "In this paper we conduct an evaluation as well as a reassessment of the PhonDatII spoken language resource. 10 years after the record of PhonDatII it is time to summarize and to look into its future. At present, the corpus comprises 39612 manually labelled phone tokens and 15083 syllable tokens of read German utterances. We describe the corpus in detail, and then we present a new method to evaluate segmentation boundaries. Finally, we ask the question as to how we can refine the PhonDatII database for the future. The mean phone duration results of this study, which are based on a corrected and extended version of the PhonDatII corpus, are in correspondence with earlier research. Consequently, the actual size of this spoken language resource seems to be sufficient for generalization of results on the segmental level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-157"
  },
  "kumar02_icslp": {
   "authors": [
    [
     "Shankar",
     "Kumar"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Risk based lattice cutting for segmental minimum Bayes-risk decoding",
   "original": "i02_0373",
   "page_count": 4,
   "order": 158,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "Minimum Bayes-Risk (MBR) speech recognizers have been shown to give improvements over the conventional maximum a-posteriori probability (MAP) decoders through N-best list rescoring and A* search over word lattices. Segmental MBR (SMBR) decoders simplify the implementation of MBR recognizers by segmenting the N-best lists or lattices over which the recognition is performed. We present a lattice cutting procedure that attempts to minimize the total Bayes-Risk of all word strings in the segmented lattice. We provide experimental results on the Switchboard conversational speech corpus showing that this segmentation procedure, in conjunction with SMBR decoding, gives modest but significant improvements over MAP decoders as well as MBR decoders on unsegmented lattices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-158"
  },
  "wendt02_icslp": {
   "authors": [
    [
     "Sascha",
     "Wendt"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Dynamic search-space pruning for time-constrained speech recognition",
   "original": "i02_0377",
   "page_count": 4,
   "order": 159,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "In automatic speech recognition complex state spaces are searched during the recognition process. By limiting these search spaces the computation time can be reduced, but unfortunately the recognition rate mostly decreases, too. However, especially for time-critical recognition tasks a search-space pruning is necessary. Therefore, we developed a dynamic mechanism to optimize the pruning parameters for time-constrained recognition tasks, e.g. speech recognition for robotic systems, in respect to word accuracy and computation time. With this mechanism an automatic speech recognition system can process speech signals with an approximately constant processing rate. Compared to a system without such a dynamic mechanism and the same time available for computation, the variance of the processing rate is decreased greatly without a significant loss of word accuracy. Furthermore, the extended system can be sped up to realtime processing, if desired or necessary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-159"
  },
  "lee02c_icslp": {
   "authors": [
    [
     "Raymond H.",
     "Lee"
    ],
    [
     "Eric H. C.",
     "Choi"
    ]
   ],
   "title": "A Gaussian selection method for multi-mixture HMM based continuous speech recognition",
   "original": "i02_0381",
   "page_count": 4,
   "order": 160,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "This paper concerns improving Gaussian selection for reducing output probability computation. We investigate the use of principal component analysis (PCA) to generate questions for a decision tree which is then used to cluster a set of Gaussians for selection purpose. By dividing a feature vector into several subspaces and generating a decision tree for each subspace, we are able to generate a smaller shortlist and hence reduce computation further. Moreover we investigate different voting strategies to combine the shortlists selected from individual decision trees. Experiments on a Mandarin Chinese base syllable recognition task have revealed that our proposed method virtually does not degrade recognition accuracy, even though there is more than 50% reduction in output probability computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-160"
  },
  "dong02c_icslp": {
   "authors": [
    [
     "Rong",
     "Dong"
    ],
    [
     "Jie",
     "Zhu"
    ]
   ],
   "title": "On use of duration modeling for continuous digits speech recognition",
   "original": "i02_0385",
   "page_count": 4,
   "order": 161,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "In this paper, we describe our duration model techniques in HMM based speech recognizer. With this approach, a large amount of deletion and insertion errors can be reduced in Mandarin continuous digits recognizer. We address a simple duration penalty function, which can be explicitly combined into Viterbi-Beam search with negligible incremental computation overload. Different parametric distributions are investigated to accurately approximate the syllable-level duration information. A relative Rate of Speech (ROS) based duration normalization scheme is proposed to eliminate variation caused by different speaking rate. In order to directly incorporate this normalization strategy, an online dynamic ROS estimation method is introduced into real-time recognition application. Experimental results demonstrated significant performance improvement has been achieved. The word error rate (WER) was reduced 52.1%, compared with our baseline recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-161"
  },
  "zweig02_icslp": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "F.",
     "Yvon"
    ]
   ],
   "title": "Arc minimization in finite state decoding graphs with cross-word acoustic context",
   "original": "i02_0389",
   "page_count": 4,
   "order": 162,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "Recent approaches to large vocabulary decoding with finite state graphs have focused on the use of state minimization algorithms to produce relatively compact graphs. This paper extends the fi- nite state approach by developing complementary arc-minimization techniques. The use of these techniques in concert with state minimization allows us to statically compile decoding graphs in which the acoustic models utilize a full word of cross-word context. This is in significant contrast to typical systems which use only a single phone. We show that the particular arc-minimization problem that arises is in fact an NP-complete combinatorial optimization problem, and describe the reduction from 3-SAT. We present experimental results that illustrate the moderate sizes and runtimes of graphs for the Switchboard task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-162"
  },
  "zheng02b_icslp": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Fast hierarchical grammar optimization algorithm toward time and space efficiency",
   "original": "i02_0393",
   "page_count": 4,
   "order": 163,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "We present an algorithm for hierarchical grammar optimization achieving time and space efficiency in Automatic Speech Recognition (ASR). The algorithm includes two parts: a selective grammar expansion algorithm and a graph reduction algorithm. The latter algorithm includes a node-merging procedure and an arc-sharing procedure. The algorithm is general so as to handle all types of grammar used in ASR; it is efficient so as to process dynamically generated grammars online; it is flexible and accepts several parameters controlling tradeoff between time and space complexity in favor of various different application requirements. We used this algorithm to optimize a grammar representing a class-based trigram language model, and obtained significant grammar size reduction and a 140% recognition speedup.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-163"
  },
  "abdou02_icslp": {
   "authors": [
    [
     "Sherif",
     "Abdou"
    ],
    [
     "Michael",
     "Scordilis"
    ]
   ],
   "title": "Dynamic tuning of language model score in speech recognition using a confidence measure",
   "original": "i02_0397",
   "page_count": 4,
   "order": 164,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "Speech recognition errors limit the capability of language models to predict subsequent words correctly. An effective way to enhance the functions of the language model is by using confidence measures. Most of current efforts for developing confidence measures for speech recognition focus on applying these measures to the fi- nal recognition result. However, using these measures early in the search process may guide the search to more promising paths. In this work we propose to use a word-based acoustic confidence metric estimated from word posterior probability to dynamically tune the contribution of the language model score. The performance of this approach was tested on a conversational telephone speech corpus and results show significant reductions in recognition error rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-164"
  },
  "zhang02_icslp": {
   "authors": [
    [
     "Xiao",
     "Zhang"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Minimum perfect hashing for fast n-gram language model lookup",
   "original": "i02_0401",
   "page_count": 4,
   "order": 165,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "A new technique is proposed for N-gram language model (LM) retrieval based on minimum perfect hashing (MPH). A hierarchical data structure is used to store N-gram scores in hash tables according to the order of N-grams, and a LM score is retrieved by probing the appropriate hash table slot without collision. Both integer key and character-string key based MPH functions are studied. The proposed MPH-based technique for N-gram LM lookup was evaluated on the Switchboard database and compared with the hierarchical binary search method of ISIP and the combined hash and linear search method of HTK. The proposed MPH-based technique outperformed the ISIP and HTK methods in significantly reduced LM retrieval time for bigram and trigram LMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-165"
  },
  "li02d_icslp": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Combining search spaces of heterogeneous recognizers for improved speech recogniton",
   "original": "i02_0405",
   "page_count": 4,
   "order": 166,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "In speech recognition systems, information from multiple sources such as different feature streams or acoustic models can be combined in many different ways to yield better recognition performance. It is theoretically expected that the best performance is obtainable through the simultaneous use of all sources of information, in a system capable of using these in parallel. Such systems, however, are extremely complex and difficult to construct. In this paper we propose a simple alternative criterion for combination which can factorize the complex recognizer into several simple recognizers, each of which is based on a single source of information. We use this criterion in simple experiments which combine lattices from recognizers built with different feature streams. Experimental results obtained on five different corpora show that the proposed method is effective in improving recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-166"
  },
  "pellant02_icslp": {
   "authors": [
    [
     "Karel",
     "Pellant"
    ],
    [
     "Jan",
     "Mejzlík"
    ],
    [
     "Karel",
     "Prikryl"
    ],
    [
     "Zdenek",
     "Skvor"
    ]
   ],
   "title": "Transmission characteristics of outer ear canal",
   "original": "i02_0409",
   "page_count": 4,
   "order": 167,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "The eigenvalue problem is solved on the finite element model of the external outer ear canal. The absorption of the canal walls and the interaction between external ear cavity subsystem and the elastic tympanic membrane is considered. The results of the mathematical modeling are compared with experimental measurements on human dissections. The calculations support hypothesis of possible influence of external ear canal on the enhancement of hearing sensitivity in 2 - 4 kHz frequency range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-167"
  },
  "kates02_icslp": {
   "authors": [
    [
     "James M.",
     "Kates"
    ]
   ],
   "title": "Hearing-aid benefits and limitations: predictions from a cochlear model",
   "original": "i02_0413",
   "page_count": 4,
   "order": 168,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "A cochlear model has been developed [1,2] which can model aspects of auditory impairment, including changes in the auditory threshold and filter bandwidth. This model is used to simulate the cochlear response to tones and speech in normal and impaired ears. The model illustrates some of the difficulties that can be encountered in developing signal processing for hearing aids. The model results suggest that complete compensation for hearing loss may not be possible in an acoustic signal processing system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-168"
  },
  "nelson02_icslp": {
   "authors": [
    [
     "Peggy B.",
     "Nelson"
    ],
    [
     "Jeffrey J.",
     "DiGiovanni"
    ],
    [
     "Robert S.",
     "Schlauch"
    ]
   ],
   "title": "A psychoacoustic basis for spectral sharpening",
   "original": "i02_0417",
   "page_count": 4,
   "order": 169,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "Spectral sharpening of speech has been proposed as a possible method for improving speech understanding by listeners with hearing loss. Animal neural physiological data and cochlear models differ in their predictions as to the potential success of spectral sharpening for improved speech recognition. Data from early implementations of spectral sharpening are also ambiguous. Our recent investigation tested the theoretical viability of spectral sharpening for the detection and discrimination of spectral peaks in broadband noise. Results suggested that spectral sharpening (decrements) surrounding spectral peaks (increments) made those peaks more easily detected and discriminated than were spectral peaks without sharpening. All participants with moderate hearing loss demonstrated benefit from the spectral sharpening for peak detection and discrimination. Implications for speech processing algorithms will be discussed. (Work supported by the NIDCD grant R03 DC 04135).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-169"
  },
  "huettel02_icslp": {
   "authors": [
    [
     "Lisa G.",
     "Huettel"
    ],
    [
     "Leslie M.",
     "Collins"
    ]
   ],
   "title": "Model-based predictions of intensity discrimination for normal- and impaired-hearing listeners",
   "original": "i02_0421",
   "page_count": 4,
   "order": 170,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "Interpretation of psychophysical data from impaired-hearing individuals on intensity discrimination tasks has been confounded by the fact that some impaired individuals performance is near-normal in quiet, whereas for others, the difference limen is elevated. This paper presents a theoretical analysis of the effects of cochlear impairments on intensity discrimination using a combination of signal detection theory and a computational auditory model. Not only were we able to replicate the trends observed in experimental data, but, by using a model, we could also establish a link between the psychophysical predictions and the underlying physiology. In doing so, we are able to support the hypothesis that the observed behavior is due to the spread of excitation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-170"
  },
  "assmann02_icslp": {
   "authors": [
    [
     "Peter F.",
     "Assmann"
    ],
    [
     "Terrance M.",
     "Nearey"
    ],
    [
     "Jack M.",
     "Scott"
    ]
   ],
   "title": "Modeling the perception of frequency-shifted vowels",
   "original": "i02_0425",
   "page_count": 4,
   "order": 171,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "A significant fact about speech perception is that intelligibility is preserved when the spectrum is shifted up or down along the frequency scale, across a fairly wide range. To study the relationship between fundamental frequency (F0) and spectrum envelope shifts in vowel perception, we used a high-quality vocoder (STRAIGHT) to process a set of vowels spoken by 3 adult males in /hVd/ context. Identification accuracy dropped by about 30% when the spectrum envelope was scaled upwards by a factor of 2.0, and in a separate condition, by about 50% when F0 was raised by 2 octaves. However, when spectrum envelope and F0 were both increased at the same time, identification accuracy showed a marked improvement, compared to conditions where each cue was manipulated separately. The synergy between formant frequency and F0 was predicted by a model which accounts for the intelligibility of frequency-shifted vowels in terms of learned relationships between measured values of F0 and formant frequencies. A second model, based on auditory excitation patterns, predicted the main effects of F0 and spectrum envelope, but did not predict the pattern of interaction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-171"
  },
  "mackersie02_icslp": {
   "authors": [
    [
     "Carol L.",
     "Mackersie"
    ]
   ],
   "title": "The relationship between pure-tone sequential stream segregation and perceptual separation of male and female talkers by listeners with hearing loss",
   "original": "i02_0429",
   "page_count": 4,
   "order": 172,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "The purpose of this study was to describe the relationship between sequential stream segregation abilities of listeners with hearing loss and the ability to recognize pairs of sentences spoken simultaneously by a man and a woman. In the streaming task, the fusion threshold was measured as the frequency separation at which listeners could no longer perceptually separate a series of fixed and varying-frequency pure-tones. The varying frequency tones started at frequencies either below (ascending) or above (descending) the frequency of the fixed tone. Seven of 11 subjects showed significant differences between the ascending and descending fusion thresholds. Ascending fusion thresholds were found to predict the intelligibility of the male talker, but not the female talker. Conversely, descending fusion thresholds were found to predict the intelligibility of the female talker, but not the male talker. In both cases, higher speech perception scores were associated with lower (better) fusion thresholds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-172"
  },
  "johansson02_icslp": {
   "authors": [
    [
     "Mathias",
     "Johansson"
    ],
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Lars-Erik",
     "Hoffsten"
    ],
    [
     "Anders",
     "Torberger"
    ]
   ],
   "title": "A phoneme recognizer for the hearing impaired",
   "original": "i02_0433",
   "page_count": 4,
   "order": 173,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "This paper describes an automatic speech recognition system designed to investigate the use of phoneme recognition as a hearing aid in telephone communication. The system was tested in two experiments. The first involved 19 normal hearing subjects with a simulated severe hearing impairment. The second involved 5 hearing impaired subjects. In both studies we used a procedure called Speech Tracking, which measures the effective communication speed between two persons. A substantial improvement was found in both cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-173"
  },
  "fischer02_icslp": {
   "authors": [
    [
     "V.",
     "Fischer"
    ],
    [
     "E.",
     "Janke"
    ],
    [
     "S.",
     "Kunzmann"
    ]
   ],
   "title": "Likelihood combination and recognition output voting for the decoding of non-native speech with multilingual HMMs",
   "original": "i02_0489",
   "page_count": 4,
   "order": 174,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "In this paper we report on the combination of multilingual Hidden Markov Models for the recognition of non-native speech. Using a digit recognition task as an example, we first demonstrate the bene- fits of bilingual acoustic models that incorporate training data from both the target language and the speakers native language, and then compare two different recognizer combination methods, namely voting on recognition output (ROVER) and frame based, time synchronous likelihood combination. Finally, we demonstrate the usefulness of the proposed methods for speakers whose native language is not in the training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-174"
  },
  "angkititrakul02_icslp": {
   "authors": [
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Stochastic trajectory model analysis for accent classification",
   "original": "i02_0493",
   "page_count": 4,
   "order": 175,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "It is believed that knowledge gained from reliable accent classification could be employed to improve the performance of speech recognition and speaker recognition algorithms. In this paper, we investigate the use of articulatory movement in the spectral domain to classify accented speech. The trajectories are modelled by a mixture of probability density functions of a random sequence of states. The approach is based on a Stochastic Trajectory Model (STM) which has been considered for speech recognition[1] and speech synthesis[2]. The CU-Accent database is collected over a telephone channel using speakers with foreign language accents. Experiments are performed at a context-independent phoneme-class level. Accent classification evaluations using English produced by native speakers of Mandarin Chinese, Thai, Turkish, and native American English showed classifi- cation rate in the range of 64.2-67.4% for STM versus 62.4-66.6% for GMM, using single phonemes. One of the key results is the formulation of an accent sensitive phoneme tree across the four English accents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-175"
  },
  "tian02_icslp": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Juha",
     "Häkkinen"
    ],
    [
     "Olli",
     "Viikki"
    ]
   ],
   "title": "Multilingual pronunciation modeling for improving multilingual speech recognition",
   "original": "i02_0497",
   "page_count": 4,
   "order": 176,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "Multilinguality aspects are becoming increasingly important in the Automatic Speech Recognition (ASR) systems. It is apparent that coping with large variability of the speech signal is an even bigger challenge in multilingual ASR systems than it has been in conventional monolingual systems. In this paper, we address the importance of combining multilingual pronunciation modeling and acoustic model adaptation. To compensate the pronunciation variability across various speakers, multilingual pronunciation modeling method is proposed. Due to the limited processing power and memory resources available in many systems, we also propose a pruning scheme that removes pronunciation variants from the vocabulary based on the statistical scores obtained during the deployment of the system. To further compensate the mismatches between the multilingual acoustic models and the speakers pronunciation, online MAP acoustic model adaptation is applied. Experimental results with 25 languages indicate the usefulness and efficiency of the joint use of these techniques both in clean and noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-176"
  },
  "tian02b_icslp": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Juha",
     "Häkkinen"
    ],
    [
     "Søren",
     "Riis"
    ],
    [
     "Kåre Jean",
     "Jensen"
    ]
   ],
   "title": "On text-based language identification for multilingual speech recognition systems",
   "original": "i02_0501",
   "page_count": 4,
   "order": 177,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "The demand for multilingual speech recognition systems is growing rapidly. Automatic language identification is an integral part of multilingual systems that use dynamic vocabularies. Most state-of-theart automatic language identification approaches identify the language based on probabilities of phoneme sequences extracted from the acoustic signal. Such methods can, however, not be applied to language identification based on text alone. This paper compares three text-based language identification methods aimed particularly at very short segments of text as encountered in, e.g., name dialling or command word control applications. The first method is based on artificial neural networks, the second on decision trees and the third on n-gram letter statistics. We conducted a series of experiments and the neural network approach is clearly better in terms of generalization performance and complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-177"
  },
  "ma02b_icslp": {
   "authors": [
    [
     "Bin",
     "Ma"
    ],
    [
     "Cuntai",
     "Guan"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Multilingual speech recognition with language identification",
   "original": "i02_0505",
   "page_count": 4,
   "order": 178,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "This paper presents a new approach to multilingual speech recognition. The proposed algorithm combines both language identification (LID) and speech recognition into a single process. It is shown to be effective for multilingual grammar-based speech recognition where the language information is not available prior to recognition. The idea is to make use of acoustic-phonetic and lexical information in each language to reduce possible mismatch caused by potential difference in acoustic and recording conditions when the training utterances for each language were collected. By doing so, it is shown that, with the help of LID information, the word error rate of a mixed Mandarin and English speech recognition system is greatly reduced. The same formulation can also be used to enhance language identi- fication accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-178"
  },
  "chengalvarayan02_icslp": {
   "authors": [
    [
     "Rathi",
     "Chengalvarayan"
    ]
   ],
   "title": "Robust HMM training for unified dutch and German speech recognition",
   "original": "i02_0509",
   "page_count": 4,
   "order": 179,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "This paper describes our recent work in developing an unified Dutch and German speech recognition system in the SpeechDat domain. The acoustic component of the multilingual system is accomplished through sharing common phonemes without preserving any information about the languages. We propose a more robust MCE-based training algorithm, where only the language dependent phoneme models are allowed to be adjusted, according to the type of training data. Experimental results on Dutch and German subword recognition tasks clearly show an overall string error rate reduction of about 7% and 13% obtained by the newly trained unified recognizer in comparison with the conventional MCE-trained multilingual system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-179"
  },
  "khudanpur02_icslp": {
   "authors": [
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Woosung",
     "Kim"
    ]
   ],
   "title": "Using cross-language cues for story-specific language modeling",
   "original": "i02_0513",
   "page_count": 4,
   "order": 180,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "We propose methods to exploit contemporary news articles in a resource rich language, together with cross-language information retrieval and machine translation, to sharpen language models for a news story in a language with fewer linguistic resources. We report experimental results on story-specific Chinese language models that use cues from a parallel corpus of English news stories. We demonstrate that even with fairly crude cross-language information retrieval, level-1 machine translation and simple linear interpolation, a significant (18%) reduction in perplexity may be obtained over a Chinese trigram model. We also demonstrate that this method of sharpening the Chinese language model is complementary to other techniques like topic dependent modeling, and the two in combination result in an even greater reduction in perplexity (28%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-180"
  },
  "zhao02b_icslp": {
   "authors": [
    [
     "Bing",
     "Zhao"
    ],
    [
     "Stephan",
     "Vogel"
    ]
   ],
   "title": "Full-text story alignment models for Chinese-English bilingual news corpora",
   "original": "i02_0517",
   "page_count": 4,
   "order": 181,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "In this paper, we describe the full-text story alignment on Chinese-English bilingual corpora of news data to mine potential parallel data for machine translation. Several standard information retrieval methods are tested and two translation-model based alignment models are proposed and studied. Modeling the process of generating the parallel English story from Chinese story gives significant improvements over the standard information retrieval techniques. Refinements of the alignment model are also proposed and tested in detail. On one days bilingual news collection, our methods improved the mean reciprocal rank from 0.31 to 0.68.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-181"
  },
  "sooful02_icslp": {
   "authors": [
    [
     "Jayren J.",
     "Sooful"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "Comparison of acoustic distance measures for automatic cross-language phoneme mapping",
   "original": "i02_0521",
   "page_count": 4,
   "order": 182,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "This paper explores an automated approach to map one phoneme set to another, based on the acoustic distances between the individual phonemes. The main goal of this investigation is to be able to use the data of a source language, to train the initial acoustic models of a target language for which very little speech data may be available. To do this, an automatic technique for mapping the phonemes of the two data sets must be found. Using this technique, it would be possible to accelerate the development of a speech recognition system for a new language. In our study, we compare different acoustic distance measures and assess their ability to quantify the acoustic similarity between phonemes. The distance measures that were considered are the Kullback-Leibler measure, the Bhattacharyya distance metric, the Mahalanobis measure, the Euclidean measure, the L2 metric and the Jeffreys-Matusita distance. We tested the distance measures by comparing the cross-database recognition results obtained on phoneme models created from the TIMIT speech corpus and a locally compiled South African SUN Speech database. It was found that by selecting an appropriate distance measure, an automated procedure to map phonemes from the source language to the target language can be applied, with recognition results comparable to a manual mapping process undertaken by a phonetic expert.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-182"
  },
  "he02_icslp": {
   "authors": [
    [
     "Xiaodong",
     "He"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Maximum expected likelihood based model selection and adaptation for nonnative English speakers",
   "original": "i02_0525",
   "page_count": 4,
   "order": 183,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "In this paper, the problem of fast model adaptation for nonnative speakers is addressed from a perspective of model complexity selection. The key challenge lies in reliable complexity selection when only a small amount of adaptation data is available. A novel maximum expected likelihood (MEL) based technique is proposed to enable model complexity selection from using as little as one adaptation sentence. In MEL, the expectation of log-likelihood is computed based on the mismatch bias between model and data which is measured by a small amount of adaptation data, and model complexity is selected to maximize EL. Experiments were performed on WSJ data of speakers with a wide range of foreign accents. The proposed method led to consistent and significant improvement on recognition accuracy over MLLR for nonnative speakers, without performance degradation on native speakers. The proposed method was able to dynamically select optimal model complexity as the available adaptation data increased.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-183"
  },
  "minematsu02_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Integration of MLLR adaptation with pronunciation proficiency adaptation for non-native speech recognition",
   "original": "i02_0529",
   "page_count": 4,
   "order": 184,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "To recognize non-native speech, larger acoustic/linguistic distortions must be handled adequately in acoustic modeling, language modeling, lexical modeling, and/or decoding strategy. In this paper, a novel method to enhance MLLR adaptation of acoustic models for non-native speech recognition is proposed. In the case of native speech recognition, MLLR speaker adaptation was successfully introduced because it enables efficient adaptation with a small number of adaptation data by using a regression tree of Gaussian mixtures of HMMs. However, as for non-native speech, most of the cases, the regression tree built from the baseline HMMs does not match with pronunciation proficiency of a speaker. This paper provides a solution for this problem, where the speakers proficiency is automatically estimated and the tree suited for the proficiency is built, which can be viewed as proficiency adaptation. Recognition experiments show that MLLR with the new tree raises the averaged error reduction rate up to about 30% from the baseline MLLR performance of approximately 20%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-184"
  },
  "nguyen02b_icslp": {
   "authors": [
    [
     "Thu",
     "Nguyen"
    ],
    [
     "John",
     "Ingram"
    ]
   ],
   "title": "Native and vietnamese production of compound and phrasal stress patterns",
   "original": "i02_0533",
   "page_count": 4,
   "order": 185,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "This study examines prosodic transfer effects in the production of three contrastive English stress patterns at the level of word and phrase prosody by Vietnamese learners of English. Both languages employ distinctive patterns of pitch (F0) and intensity prominence to signal contrasts between otherwise homophonous compound and phrasal constructions, though reversed in headedness (English: blackbird vs black bird; Vietnamese: flower-pink [rose], flower pink [pink flower]). However, English but not Vietnamese requires compounds to conform to temporal constraints of word prosody. Both languages also signal contrasts between broad and narrow (contrastive) focus (e.g. A black bird...not a white one). Comparisons between advanced and beginner Vietnamese learners productions of these three stress patterns reveal good accommodation to L2 pitch and intensity targets but imperfect adaptation to timing constraints that distinguish word from phrasal constructions in English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-185"
  },
  "caspers02_icslp": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "On the function of the late rise and the early fall in dutch dialogue: a perception experiment",
   "original": "i02_0537",
   "page_count": 4,
   "order": 186,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "The question posed in the present paper is whether subjects interpret a short utterance with a late non-prominent rise in pitch (LH%) as having a go on function, prompting the current speaker to continue, whereas the same short utterance spoken with an accent-lending fall (H*L L% or A) is associated with finality, for example, with the answer to a yes-no question. A series of three perception experiments were run with natural data taken from Dutch Map Task dialogues, and the results support the hypothesis that the LH% contour is associated with a go on response, while the falling contour is associated with the answer to a question. Furthermore, LH% is preferred over A in contexts leading to backchannel responses, while there is no preference for either contour in question contexts. Finally, the LH% contour is acceptable in both context types, whereas the accent-lending fall is unacceptable in backchannel contexts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-186"
  },
  "esposito02_icslp": {
   "authors": [
    [
     "Anna",
     "Esposito"
    ],
    [
     "Susan",
     "Duncan"
    ],
    [
     "Francis",
     "Quek"
    ]
   ],
   "title": "Holds as gestural correlates to empty and filled speech pauses",
   "original": "i02_0541",
   "page_count": 4,
   "order": 187,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "Holds are defined as an active configured gestural state where no intended (hand) motion is perceived. We analyzed the audio and video from a cartoon story telling from memory, in order to determine the extent to which gestural holds overlap with speech phenomena such as empty and filled pauses. Data from two female participants are analyzed: an Italian and an American English-speaker, each using her own native language. The results show a general trend where, in both the languages, holds and speech pauses appear to be correlated. These overlap 28% of the time in the Italian and 45% of the time in the English. There are, however, significant differences between the two speakers in the percentage of empty pauses, filled pauses and holds. The Italian subject uses significantly more gestures, more filled pauses, and fewer empty pauses than the American subject.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-187"
  },
  "itoh02_icslp": {
   "authors": [
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Tatsuhiro",
     "Konishi"
    ],
    [
     "Yukihiro",
     "Itoh"
    ]
   ],
   "title": "Linguistic and acoustic changes of user²s utterances caused by different dialogue situations",
   "original": "i02_0545",
   "page_count": 4,
   "order": 188,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "This paper presents the characteristic differences of acoustic and linguistic features observed in different spoken dialogue situations: human-human vs. human-machine interactions. We compare the acoustic and linguistic features of the users speech to a spoken dialogue system and to a human-operator in several landmark setting tasks for a car navigation system. It has been pointed out that speech- based interaction has the potential to distract the drivers attention and degrade safety. On the other hand, it is not clear whether different dialogue situations cause any acoustic or linguistic differences on ones utterances in a speech interface system. We collected a set of spoken dialogues by 10 subject speakers under several dialogue situations. For a car driving condition, we prepared a virtual driving simulation system. We analyzed the characteristic differences of user utterances caused by different dialogue situations or the systems responses to the users. As a result, some prosodic and linguistic features were affected by whether the dialogue partner is a human or a machine, while some prosodic features alone were affected by a car-driving task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-188"
  },
  "ward02_icslp": {
   "authors": [
    [
     "Nigel",
     "Ward"
    ],
    [
     "Satoshi",
     "Nakagawa"
    ]
   ],
   "title": "Automatic user-adaptive speaking rate selection for information delivery",
   "original": "i02_0549",
   "page_count": 4,
   "order": 189,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "Today there are many services which provide information over the phone using a prerecorded or synthesized voice. These voices are invariant in speed. Humans giving information over the telephone, however, tend to adapt the speed of their presentation to suit the needs of the listener. This paper presents a preliminary model of this adaptation. In a corpus of simulated directory assistance dialogs the operators speed in number-giving correlates with the speed of the users initial response and with the users speaking rate. Multiple regression gives a formula which predicts appropriate speaking rates, and these predictions correlate (.46) with the speeds observed in good dialogs in the corpus. An experiment with 18 subjects suggests that users prefer a system which adapts its speed to the user in this way.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-189"
  },
  "skantze02_icslp": {
   "authors": [
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "Coordination of referring expressions in multimodal human-computer dialogue",
   "original": "i02_0553",
   "page_count": 4,
   "order": 190,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "This study examines coordination of referring expressions in multimodal human-computer dialogue, i.e. to what extent users choices of referring expressions are affected by the referring expressions that the system is designed to use. An experiment was conducted, using a semi-automatic multimodal dialogue system for apartment seeking. The user and the system could refer to areas and apartments on an interactive map by means of speech and pointing gestures. Results indicate that the referring expressions of the system have great influence on the users choice of referring expressions, both in terms of modality and linguistic content. From this follows a number of implications for the design of multimodal dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-190"
  },
  "cerrato02_icslp": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ]
   ],
   "title": "A comparison between feedback strategies in human-to-human and human-machine communication",
   "original": "i02_0557",
   "page_count": 4,
   "order": 191,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "The results of a comparative analysis of feedback strategies used in human-human communication and human-machine communication in Swedish are reported in this paper. The aim of this study is twofold: to provide a categorization of feedback expressions based on contextual information and to verify the hypothesis that acoustic characteristics of feedback expressions can be regarded as cues to the interpretation of the function they carry out and the communicative intention they convey.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-191"
  },
  "darves02_icslp": {
   "authors": [
    [
     "Courtney",
     "Darves"
    ],
    [
     "Sharon",
     "Oviatt"
    ]
   ],
   "title": "Adaptation of users' spoken dialogue patterns in a conversational interface",
   "original": "i02_0561",
   "page_count": 4,
   "order": 192,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "The design of robust new interfaces that process conversational speech is a challenging research direction largely because users spoken language is so variable, which is especially true of children. The present research explored whether childrens response latencies before initiating a conversational turn converge with those heard in the text-to-speech (TTS) of a computer partner. A study was conducted in which twenty-four 7-to-10-year-old children conversed with animated characters that responded with different types of TTS voices during an educational software application. Analyses confirmed that, while interacting with opposite TTS voices, childrens average response latencies adapted 18.4% in the direction of their computer partners speech. These adaptations were dynamic, bi-directional, and generalized across different types of users and TTS voices. The long-term goal of this research is the predictive modeling of humancomputer communication patterns to guide the design of well synchronized, robust, and adaptive conversational interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-192"
  },
  "rosenberg02_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Allen",
     "Gorin"
    ],
    [
     "Zhu",
     "Liu"
    ],
    [
     "S.",
     "Parthasarathy"
    ]
   ],
   "title": "Unsupervised speaker segmentation of telephone conversations",
   "original": "i02_0565",
   "page_count": 4,
   "order": 193,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "A process for segmenting 2-speaker telephone conversations by speaker with no prior speaker models is described and evaluated. The process consists of an initial segmentation using acoustic change and pause detection, segment clustering, and iterative modeling of segment clusters and resegmentation. The technique has been evaluated on (6), approximately 3 min long, customer care conversations. The technique does not resolve short (< 2 secs) or overlapping segments very well, but is capable of detecting longer segments (> 4 secs) with miss rates of the order of 10% and confusion rates 2% or less.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-193"
  },
  "sivakumaran02_icslp": {
   "authors": [
    [
     "P.",
     "Sivakumaran"
    ],
    [
     "A.M.",
     "Ariyaeeinia"
    ],
    [
     "J.",
     "Fortuna"
    ]
   ],
   "title": "An effective unsupervised scheme for multiple-speaker-change detection",
   "original": "i02_0569",
   "page_count": 4,
   "order": 194,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "This paper presents an enhanced Bayesian information criterion (BIC)-based algorithm for multiple-speaker-change detection (MSCD) without prior acoustic information on speakers. The enhancement offered by the proposed approach is in terms of effectiveness. This is achieved through the introduction of robustness into the standard BIC procedure, against certain important causes of misclassification. The paper also introduces a new measure, termed effective error rate (EFER), for evaluating the relative performance of MSCD algorithms. It is shown that the proposed measure allows a more meaningful evaluation of MSCD than the conventional ones. The experimental results obtained using this new evaluation measure clearly confirm the effectiveness of the proposed algorithm. The experimental investigation is based on 3 hours of broadcast news material with 445 speaker changes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-194"
  },
  "ajmera02_icslp": {
   "authors": [
    [
     "J.",
     "Ajmera"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "I.",
     "Lapidot"
    ],
    [
     "Iain A.",
     "McCowan"
    ]
   ],
   "title": "Unknown-multiple speaker clustering using HMM",
   "original": "i02_0573",
   "page_count": 4,
   "order": 195,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "An HMM-based speaker clustering framework is presented, where the number of speakers and segmentation boundaries are unknown a priori. Ideally, the system aims to create one pure cluster for each speaker. The HMM is ergodic in nature with a minimum duration topology. The final number of clusters is determined automatically by merging closest clusters and retraining this new cluster, until a decrease in likelihood is observed. In the same framework, we also examine the effect of using only the features from highly voiced frames as a means of improving the robustness and computational complexity of the algorithm. The proposed system is assessed on the 1996 HUB-4 evaluation test set in terms of both cluster and speaker purity. It is shown that the number of clusters found often correspond to the actual number of speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-195"
  },
  "meignier02_icslp": {
   "authors": [
    [
     "Sylvain",
     "Meignier"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ]
   ],
   "title": "Speaker utterances tying among speaker segmented audio documents using hierarchical classification: towards speaker indexing of audio databases",
   "original": "i02_0577",
   "page_count": 4,
   "order": 196,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "Speaker indexing of an audio database consists in organizing the audio data according to the speakers present in the database. It is composed of three steps: (1) segmentation by speakers of each audio document; (2) speaker tying among the various segmented portions of the audio documents; and (3) generation of a speaker-based index. This paper focuses on the second step, the speaker tying task, which has not been addressed in the literature. The result of this task is a classification of the segmented acoustic data by clusters; each cluster should represent one speaker. This paper investigates on hierarchical classification approaches for speaker tying. Two new discriminant dissimilarity measures and a new bottom-up algorithm are also proposed. The experiments are conducted on a subset of the Switchboard database, a conversational telephone database, and show that the proposed method allows a very satisfying speaker tying among various audio documents, with a good level of purity for the clusters, but with a number of clusters significantly higher than the number of speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-196"
  },
  "mariethoz02_icslp": {
   "authors": [
    [
     "Johnny",
     "Mariéthoz"
    ],
    [
     "Samy",
     "Bengio"
    ]
   ],
   "title": "A comparative study of adaptation methods for speaker verification",
   "original": "i02_0581",
   "page_count": 4,
   "order": 197,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "Real-life speaker verification systems are often implemented using client model adaptation methods, since the amount of data available for each client is often too low to consider plain Maximum Likelihood methods. While the Bayesian Maximum A Posteriori (MAP) adaptation method is commonly used in speaker verification, other methods have proven to be successful in related domains such as speech recognition. This paper reports on experimental comparison between three well-known adaptation methods, namely MAP, Maximum Likelihood Linear Regression, and finally Eigen-Voices. All three methods are compared to the more classical Maximum Likelihood method, and results are given for a subset of the 1999 NIST Speaker Recognition Evaluation database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-197"
  },
  "farrell02_icslp": {
   "authors": [
    [
     "Kevin R.",
     "Farrell"
    ]
   ],
   "title": "Speaker verification with data fusion and model adaptation",
   "original": "i02_0585",
   "page_count": 4,
   "order": 198,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "This paper presents methods for adapting models in a data fusionbased speaker verification system. The models that are used in the data fusion system are the neural tree network (NTN), dynamic time warping (DTW), and hidden Markov model (HMM). The models provide information based on discriminant information, distortion measurements, and probabilistic evaluation, respectively. The parameters of these models are updated during the adaptation process using verification data. This allows the models to track changes in the users voice over time and additionally allows the technology to supplement the typically limited data obtained at enrollment. Experiments are performed on voice data collected within landline telephony, wireless telephony, and multimedia environments. Additionally, the adaptation algorithms are evaluated for both cases where the data is known to come from the correct user (supervised) and not known to come from the correct user (unsupervised). For the case where the adaptation data is not known to come from the correct user, threshold criteria is used for determining if the adaptation should occur or not. The adaptation leads to a 20% relative reduction on the equal error rate for the unsupervised scenario and a 40% relative reduction in equal error rate for the supervised scenario.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-198"
  },
  "mirghafori02_icslp": {
   "authors": [
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Larry P.",
     "Heck"
    ]
   ],
   "title": "An adaptive speaker verification system with speaker dependent a priori decision thresholds",
   "original": "i02_0589",
   "page_count": 4,
   "order": 199,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "This paper presents a practical approach to deploying a priori speaker dependent thresholds (SDT) for adaptive speaker verification applications. Our motivations for exploring SDTs are two fold: one is to eliminate the externally pre-set overall system thresholds and replace them with automatically-set internal thresholds calculated at runtime; the second is to counter the verification score shifts resulting from online adaptation. The second motivation is based on the observation that after adaptation, verification scores for both true speakers and impostors increase, which in turn increases the false accept (FA) rates. The rise of FA rates, in an adaptive system, can be costly because of the possibility of model corruption. In this work, an approach similar to ZNORM [3] is used to calculate a threshold for each speaker, which is automatically updated every time the claimant model is adapted. The paper explores various computational effi- ciency strategies to make the deployment of this approach practical for a fielded system. Results of experiments on one Japanese and one English digits database are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-199"
  },
  "roy02_icslp": {
   "authors": [
    [
     "Deb",
     "Roy"
    ],
    [
     "Peter",
     "Gorniak"
    ],
    [
     "Niloy",
     "Mukherjee"
    ],
    [
     "Josh",
     "Juster"
    ]
   ],
   "title": "A trainable spoken language understanding system for visual object selection",
   "original": "i02_0593",
   "page_count": 4,
   "order": 200,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "We present a trainable, visually-grounded, spoken language understanding system. The system acquires a grammar and vocabulary from a \"show-and-tell\" procedure in which visual scenes are paired with verbal descriptions. The system is embodied in a table-top mounted active vision platform. During training, a set of objects is placed in front of the vision system. Using a laser pointer, the system points to objects in random sequence, prompting a human teacher to provide spoken descriptions of the selected objects. The descriptions are transcribed and used to automatically acquire a visuallygrounded vocabulary and grammar. Once trained, a person can interact with the system by verbally describing objects placed in front of the system. The system recognizes and robustly parses the speech and points, in real-time, to the object which best fits the visual semantics of the spoken description.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-200"
  },
  "bechet02_icslp": {
   "authors": [
    [
     "F.",
     "Béchet"
    ],
    [
     "Allen",
     "Gorin"
    ],
    [
     "Jerry",
     "Wright"
    ],
    [
     "D. Hakkani",
     "Tur"
    ]
   ],
   "title": "Named entity extraction from spontaneous speech in how may i help you?",
   "original": "i02_0597",
   "page_count": 4,
   "order": 201,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "The understanding module of a spoken dialogue system must extract, from the speech recognizer output, the kind of request expressed by the caller (the call type) and its parameters (numerical expressions, time expressions or proper-name). The definition of such parameters (called Named Entities, NE) is linked to the dialogue application. Detecting and extracting such contextual NEs for the How May I Help You? application is the subject of this study. By detecting NEs with a statistical tagger on 1-best hypotheses and by extracting their values with local models on word-lattices, we show very significant improvements compared to the traditional approach which uses regular expressions on the 1-best hypothesis only.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-201"
  },
  "bousquetvernhettes02_icslp": {
   "authors": [
    [
     "Caroline",
     "Bousquet-Vernhettes"
    ],
    [
     "Nadine",
     "Vigouroux"
    ]
   ],
   "title": "Recognition error processing for speech understanding",
   "original": "i02_0601",
   "page_count": 4,
   "order": 202,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "The aim of this paper is to propose an extension of the stochastic conceptual model in order to increase the robustness of the understanding process faced with misrecognitions and unknown words. Corpus analysis shows that some misrecognized words are more difficult to interpret than others, so we defined a word ambiguity rate. We performed trial series on train schedule inquiry application to evaluate the understanding rate when faced with misrecognized words and in particular, when these words are city names.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-202"
  },
  "pargellis02_icslp": {
   "authors": [
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Augustine",
     "Tsai"
    ]
   ],
   "title": "Using part-of-speech tags, context thresholding, and trigram contexts to improve the auto-induction of semantic classes",
   "original": "i02_0605",
   "page_count": 4,
   "order": 203,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "A natural dialogue system for human-computer interactions includes an understanding module that defines groups of words and phrases that are semantically similar. New domains usually do not have large, annotated corpora, so it is useful to develop methods of automatically inducing semantic groups (concepts). Classes can be induced from unannotated corpora by means of a context-dependent similarity measure, such as the Kullback-Leibler distance. However, the precision of auto-induced classes is reduced in cases where the statistics are poor, or where words of different parts of speech may occur in similar lexical contexts. We address this issue by augmenting a semantic generalizer with three new modules, a part-of-speech (POS) tagger to preprocess the list of candidate word pairs, trigram instead of bigram contexts, and context thresholding. The subjective quality of auto-induced classes is compared for these three methodologies for a large newspaper text (WSJ) corpus. We show that context thresholding has the biggest impact on inducing higher quality classes. The best results were obtained for a context threshold of 3 extant bigrams and trigrams. For bigram contexts, with POS tags, the precision was 88% for the first 50 clusters, 75% for the first 100 clusters, and 65% for the first 150. Similar results were attained for trigram contexts and no POS tags.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-203"
  },
  "wang02_icslp": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Ciprian",
     "Chelba"
    ],
    [
     "Brendan",
     "Frey"
    ],
    [
     "Leon",
     "Wong"
    ]
   ],
   "title": "Combination of statistical and rule-based approaches for spoken language understanding",
   "original": "i02_0609",
   "page_count": 4,
   "order": 204,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "A Natural User Interface (NUI), where a user can type or speak a request, is a good complement to the well-known Graphical User Interface (GUI). Accurately extracting user intent from such typed or spoken queries is a very difficult challenge. In this paper we evaluate several techniques to extract user intent from typed sentences in the context of the well-known Airline Travel Information (ATIS) domain, where we want to extract which of the possible tasks the user wants to do and the value of the slots associated to that task. In previous work we showed that a Semantic Context Free Grammar (CFG) semiautomatically derived from labeled data can offer very good results. In this paper we evaluate several statistical pattern recognition techniques including Support Vector Machines (SVM), Naïve Bayes classi- fiers and task-dependent n-gram language models. These methods can yield a very low task classification error rate. If used in combination with our CFG system, they can also lead to very low slot error rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-204"
  },
  "xie02_icslp": {
   "authors": [
    [
     "Guodong",
     "Xie"
    ],
    [
     "Chengqing",
     "Zong"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Chinese spoken language analyzing based on combination of statistical and rule methods",
   "original": "i02_0613",
   "page_count": 4,
   "order": 205,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "A combination of statistical and rule methods has been developed for Chinese spoken language analyzing. The analyzing result is a middle semantic frame, which can be converted to different language according peoples needs. We adopt the statistical method in the stage of extracting semantic meaning and the rule method in the stage of mapping the semantic units to middle semantic frame. Experiment shows this method has high robustness and can analyzing Chinese spoken language effectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-205"
  },
  "pfannerer02_icslp": {
   "authors": [
    [
     "Norbert",
     "Pfannerer"
    ]
   ],
   "title": "A maximum entropy semantic parser using word classes",
   "original": "i02_0617",
   "page_count": 4,
   "order": 206,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "This paper describes the parser that is used in the Sail Labs Conversational System, which is a spoken dialog system. This parser is a fully statistical, semantic parser. The probability model of the parser is based on the principle of maximum entropy. The maximum entropy framework allows to combine the available information in a fully automatic way, but the training of maximum entropy models is time consuming. Since the parser needs to be retrained when its vocabulary changes, a straightforward application of this model cannot realistically be used in a dialog system. To solve this problem, words can be combined to classes, and the classes can be used instead of the words for the training of the parser. At runtime, words can be added to the classes at no cost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-206"
  },
  "gurijala02_icslp": {
   "authors": [
    [
     "A.",
     "Gurijala"
    ],
    [
     "J.",
     "R. Deller Jr."
    ],
    [
     "M. S.",
     "Seadle"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speech watermarking through parametric modeling",
   "original": "i02_0621",
   "page_count": 4,
   "order": 207,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "A general formulation for speech watermarking through parametric modeling is suggested, then the paper focuses on a watermarking technique based on linear-predictive (LP) modeling of speech. In the particular strategy employed here, information is embedded by modifying the autocorrelation values of the original speech. The amount of information that can be embedded is subject to fidelity constraints. The modified LP coefficients derived from the new set of autocorrelation values are used for reconstructing the watermarked speech. The perceptual quality of the watermarked speech depends on the relative energy of the embedded watermark, the watermark sequence used, and the LP model order. Robustness of the technique to various signal processing operations and attacks like compression, cropping, and additive noise are studied via experiments on a small speech application. Factors affecting watermark robustness and related security issues are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-207"
  },
  "hong02_icslp": {
   "authors": [
    [
     "Kai Sze",
     "Hong"
    ],
    [
     "Sh-Hussain",
     "Salleh"
    ]
   ],
   "title": "An education software in teaching automatic speech recognition (ASR)",
   "original": "i02_0625",
   "page_count": 4,
   "order": 208,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "This paper presents a new interactive speech recognition education software designed for Microsoft Windows platforms. A set of education tools have been developed with the aim at presenting, visualizing and teaching the foundation knowledge of speech recognition. The new education software consists of 6 main modules: Speech Recorder/End Point Detector, LPC/LPC-Cep, MFCC, Vector Quantization, Dynamic Time Warping and Hidden Markov Models. Utilizing these modules can enhance the understanding of designing speech processing system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-208"
  },
  "xiao02_icslp": {
   "authors": [
    [
     "Benfang",
     "Xiao"
    ],
    [
     "Cynthia",
     "Girand"
    ],
    [
     "Sharon",
     "Oviatt"
    ]
   ],
   "title": "Multimodal integration patterns in children",
   "original": "i02_0629",
   "page_count": 4,
   "order": 209,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "Multimodal interfaces are designed with a focus on flexibility, although very few multimodal systems currently are capable of adapting to major sources of user or environmental variation. The development of adaptive multimodal processing techniques will require empirical guidance on modeling key aspects of individual differences. In the present study, we collected data from 24 7-to-10- year-old children as they interacted using speech and pen input with an educational software prototype. A comprehensive analysis of childrens multimodal integration patterns revealed that they were classifiable as either simultaneous or sequential integrators, although they more often integrated signals simultaneously than adults. During their sequential constructions, intermodal lags also ranged faster than those of adult users. The high degree of consistency and early predictability of childrens integration patterns were similar to previously reported adult data. These results have implications for the development of temporal thresholds and adaptive multimodal processing strategies for childrens applications. The long-term goal of this research is life-span modeling of users integration and synchronization patterns, which will be needed to design future high-performance adaptive multimodal systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-209"
  },
  "scharenborg02_icslp": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Johan de",
     "Veth"
    ]
   ],
   "title": "ASR in a human word recognition model: generating phonemic input for shortlist",
   "original": "i02_0633",
   "page_count": 4,
   "order": 210,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "The current version of the psycholinguistic model of human word recognition Shortlist suffers from two unrealistic constraints. First, the input of Shortlist must consist of a single string of phoneme symbols. Second, the current version of the search in Shortlist makes it difficult to deal with insertions and deletions in the input phoneme string. This research attempts to fully automatically derive a phoneme string from the acoustic signal that is as close as possible to the number of phonemes in the lexical representation of the word. We optimised an Automatic Phone Recogniser (APR) using two approaches, viz. varying the value of the mismatch parameter and optimising the APR output strings on the output of Shortlist. The approaches show that it will be very difficult to satisfy the input requirements of the present version of Shortlist with a phoneme string generated by an APR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-210"
  },
  "wu02c_icslp": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yu-Hsien",
     "Chiu"
    ],
    [
     "Kung-Wei",
     "Cheng"
    ]
   ],
   "title": "Sign language translation using an error tolerant retrieval algorithm",
   "original": "i02_0637",
   "page_count": 4,
   "order": 211,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "This paper addresses an error-tolerant retrieval algorithm for generating sentences from ill-formed sign sequences in Taiwanese Sign Language (TSL). The design methodology is motivated by the kinematics of hand gestures for sign language. In order to increase the input rate and retrieval accuracy, the basic design strategy leads to develop an efficient and effective sign feature retrieval method. In this approach, a Multi-list Code Tree (MCT) data structure for sign feature indexing and an error-tolerant matching algorithm are proposed. For text generation, the optimal path in the word graph, generated from the sign input sequence, is incrementally estimated by using a translation model. Several interface design and evaluation methods are also conducted for empirical study. Evaluation results show that the generation rate and retrieval accuracy for sentence construction are significantly improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-211"
  },
  "turk02b_icslp": {
   "authors": [
    [
     "Oytun",
     "Turk"
    ],
    [
     "Omer",
     "Sayli"
    ],
    [
     "Helin",
     "Dutagaci"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "A sound source classification system based on subband processing",
   "original": "i02_0641",
   "page_count": 4,
   "order": 212,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "A system for classification of audio signals containing speech, music, noise and silence is proposed. Appropriate subband processing is applied for the characterization of each sound source. The algorithm operates in four steps to classify the contents of a given audio signal. The acoustical parameters and statistical measures to be used in the classification process are obtained via an off-line training procedure. The starting and finishing instants of the acoustical events are labelled in the silence/onset detection stages. Acoustical parameters of the given signal are extracted, analysis of variance and classification using the LBG algorithm is performed by generating codebooks of acoustical vectors. Experimental work is carried out on a database containing speech, music, noise and silence. The experiments demonstrate that the system achieves 88% classification success on the average when the sound sources are non-simultaneous.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-212"
  },
  "zhang02b_icslp": {
   "authors": [
    [
     "Ying",
     "Zhang"
    ],
    [
     "Bing",
     "Zhao"
    ],
    [
     "Jie",
     "Yang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Automatic sign translation",
   "original": "i02_0645",
   "page_count": 4,
   "order": 213,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "Large amounts of information is embedded in the natural scenes. Signs are good examples of objects in natural environments which have rich information content. In this paper, we present our efforts in the automatic sign translation. We describe the challenges in the automatic sign translation and introduce the architecture of our current system for automatic detection and translation of Chinese signs. Two data-driven machine translation methods: Example Based Machine Translation (EBMT) and Statistical Machine Translation (SMT) are compared for the task of translating Chinese signs into English. We report the experimental results of both methods that are trained from a small bilingual sign corpus combined with a bilingual glossary. The experiment results indicate that EBMT generates more correct translations while SMT is better at inferring unseen patterns. We are currently working on developing a multi-engine machine translation system that can incrementally learn from the data and combine the results from EBMT and SMT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-213"
  },
  "wenndt02_icslp": {
   "authors": [
    [
     "Stanley J.",
     "Wenndt"
    ],
    [
     "Edward J.",
     "Cupples"
    ],
    [
     "Richard M.",
     "Floyd"
    ]
   ],
   "title": "A study on the classification of whispered and normally phonated speech",
   "original": "i02_0649",
   "page_count": 4,
   "order": 214,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "This research developed a unique process for the automatic classi- fication of speech signals. Instead of classifying speech signals into common categories such as who the speaker is or what language is being spoken, this research examines ways to classify the speech into three broad classes of speech production - normally phonated speech, whispered speech, and softly spoken speech. Since most automated speech processing techniques give erroneous results when presented with a mixture of these classes of speech, this technique can provide class information to improve automated speech processing systems performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-214"
  },
  "tatara02_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Tatara"
    ],
    [
     "Taisuke",
     "Ito"
    ],
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Experiments on recognition of lavalier microphone speech and whispered speech in real world environments",
   "original": "i02_0653",
   "page_count": 4,
   "order": 215,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "In this paper, we present corpora and recognition experiments of the speech recorded in everyday life for the real world speech recognition. A speech corpus of (8),600 sentences from 53 speakers recorded through lavalier microphones in four different environments is built. The data was collected in an office space, a sound-proof room, in cars of different sizes, and on the street. Another corpus consisting of whispered and normal speech of more than (6),000 sentences from 100 speakers recorded through a close-talking microphone is devised. Continuous speech recognition experiments using acoustic models trained by the speech corpus in each environment, attain a recognition accuracy of above 80%. For the whispered speech corpus, the recognition accuracy obtained was 74% using the whispered speech model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-215"
  },
  "iwaki02_icslp": {
   "authors": [
    [
     "Mamoru",
     "Iwaki"
    ],
    [
     "Hiromi",
     "Seki"
    ]
   ],
   "title": "An effect of amplitude modulation on perceptual segregation of tone sequences",
   "original": "i02_0657",
   "page_count": 4,
   "order": 216,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "Auditory stream segregation is know as a process whereby sound elements are separated and integrated into some perceptual objects as a coherent whole, in the auditory scene analysis. Such a perceptual faculty is considered to be dependent on some factors in sounds such as similarity, good continuation, common fate, disjoint allocation, closure, and so on. For example, when we listen to fast alternative sinusoidal tone sequences, a number of separate auditory objects may be perceived, according to their acoustical attributes. It is known that such perception, fusion or fission, has close relation to frequency difference, time pattern and harmonic components in sounds. In this paper, we investigate an effect of amplitude modulation on sequential stream segregation for two close sinusoidal tones in frequency domain. As a result, it is shown that amplitude modulation improves the sequential stream segregation performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-216"
  },
  "sanders02b_icslp": {
   "authors": [
    [
     "Eric",
     "Sanders"
    ],
    [
     "Marina",
     "Ruiter"
    ],
    [
     "Lilian",
     "Beijer"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Automatic recognition of dutch dysarthric speech: a pilot study",
   "original": "i02_0661",
   "page_count": 4,
   "order": 217,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "This paper describes a feasibility study into automatic recognition of Dutch dysarthric speech. Recognition experiments with speaker independent and speaker dependent models are compared, for tasks with different perplexities. The results show that speaker dependent speech recognition for dysarthric speakers is very well possible, even for higher perplexity tasks. Keynotes and Tuesday Sessions\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-217"
  },
  "engwall02_icslp": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Evaluation of a system for concatenative articulatory visual speech synthesis",
   "original": "i02_0665",
   "page_count": 4,
   "order": 218,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "A method for concatenative articulatory visual speech synthesis has been evaluated. The method consists in using concatenated units of articulatory parameter transitions from the middle of one phoneme to the middle of the next as input to a (3) D parametric tongue model. The units were created by segmentation of the Electromagnetic articulography (EMA) measures in a database of 460 phonetically balanced sentences collected at the University of Edinburgh. The evaluation was made against the EMA database on which the movements were based and against X-ray films of three other speakers. The results show that the model replicates the natural movements globally, but that the rare units in the concatenation database may cause large differences between the synthesized and the natural utterance and that the tongue root and tongue tip movements are too restricted in the model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-218"
  },
  "sato02_icslp": {
   "authors": [
    [
     "Marc",
     "Sato"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ]
   ],
   "title": "Intrasyllabic articulatory control constraints in verbal working memory",
   "original": "i02_0669",
   "page_count": 4,
   "order": 219,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "Verbal transformation effect - an auditory imagery task equivalent to Neckers cube in visual imagery - recruits a specific working memory, the so-called articulatory or phonological loop. Is this mechanism sensitive to articulatory control constraints, i.e. phase relationships between vowel and consonant gestures? In our experiment, 56 French students repeatedly pronounced aloud non-sense syllables - all combinations of [@] with [p] and [s] - and were asked to stop as soon as they heard a possible syllable transformation. In agreement with our in-phase predictions, the winner is syllable [ps@], where all gestures can be launched in synchrony. This experiment demonstrates that verbal working memory - a primary candidate as input memory for word learning - is sensitive to articulatory control of syllable phasing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-219"
  },
  "campbell02_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Towards a grammar of spoken language: incorporating paralinguistic information",
   "original": "i02_0673",
   "page_count": 4,
   "order": 220,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "This paper reports on recent developments for the creation and analysis of very large databases of emotional and attitudinally-marked speech for the support of research into concatenative methods for producing synthesised speech which is capable of expressing the range of prosody and phonation styles to emulate human spoken interactions. It addresses the problems of ensuring high spontaneity in the speech corpus while at the same time collecting data that is of high enough audio quality to allow signal analysis by automatic processing techniques. The paper suggests that in order to describe such speech adequately, a new grammar for spoken language will be required.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-220"
  },
  "li02e_icslp": {
   "authors": [
    [
     "Qun",
     "Li"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "An analysis of the causes of increased error rates in children²s speech recognition",
   "original": "i02_2337",
   "page_count": 4,
   "order": 221,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "Previous studies have shown that childrens speech is more difficult to recognize by machine than adults speech. This paper presents the results of experiments which investigate recognition performance variation within a small population of children. Results suggest that recogniser performance on a childs speech is well correlated with a teachers assessment of the childs speaking proficiency. For children whose speech is judged to be good, performance is close to that of adults, but error rates increase by a factor of 4 for children with poor speech. An analysis of actual pronunciations for children with poor speech shows significant divergence from the idealised baseforms in a pronunciation dictionary. It is demonstrated that some improvements can be gained through the use of customized dictionaries. Finally, the effects of bandwidth reduction on recogniser performance are investigated for a range of children with differing speaking styles.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-221"
  },
  "oster02_icslp": {
   "authors": [
    [
     "Anne-Marie",
     "Öster"
    ]
   ],
   "title": "A new computer-based analytical speech perception test for prelingually deaf children and children with speech disorders",
   "original": "i02_2341",
   "page_count": 4,
   "order": 222,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "A computer-based analytical speech perception test for early diagnosis has been developed. The test seeks to evaluate the ability to perceive a range of sound contrasts used in the Swedish language. The test is tailored for measurements with small children and low verbal children by using easy speech stimuli, words selected on the basis of familiarity, and pictures that represent the test items un- ambiguously. Prelingually hearing-impaired children with pure-tone averages worse than 90 dBm show very different results in the speech clinic. Their possibilities to develop intelligible speech have shown to be unrelated to their pure tone audiograms. The development of this test is an effort to find a screening method that can predict the ability to develop intelligible speech. The test is also intended to be used with small children, from 4 years of age, with difficulties to perceive and produce spoken language. The information gained from this test will hopefully provide supplementary information about speech perception skills, auditory awareness and speech intelligibility potentials and specify important recommendations for individualized speech-training programs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-222"
  },
  "fell02_icslp": {
   "authors": [
    [
     "Harriet J.",
     "Fell"
    ],
    [
     "Joel",
     "MacAuslan"
    ],
    [
     "Linda J.",
     "Ferrier"
    ],
    [
     "Susan G.",
     "Worst"
    ],
    [
     "Karen",
     "Chenausky"
    ]
   ],
   "title": "Vocalization age as a clinical tool",
   "original": "i02_2345",
   "page_count": 4,
   "order": 223,
   "p1": "2345",
   "pn": "2348",
   "abstract": [
    "The Early Vocalization Analyzer (EVA) is a computer program that automatically analyses digitized acoustic recordings of infant vocalizations. Using the landmark detection theory of Stevens et al for the recognition of phonetic features in speech, EVA detects syllables in vocalizations produced by infants. Landmarks are grouped into standard syllable patterns and syllables are grouped into utterances. Statistics derived from these groups and the underlying features are used to derive a \"vocalization age\" and two specific screening rules that can clinically distinguish infants who may be at risk for later communication or other developmental problems from typically developing infants in the six to fifteen month age range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-223"
  },
  "cosi02_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Michael M.",
     "Cohen"
    ],
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Baldini: baldi speaks italian!",
   "original": "i02_2349",
   "page_count": 4,
   "order": 224,
   "p1": "2349",
   "pn": "2352",
   "abstract": [
    "In this work, the development of Baldini, an Italian version of Baldi, a computer-animated conversational agent, is presented. Speech synthesis and facial animation examples are shown.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-224"
  },
  "cave02_icslp": {
   "authors": [
    [
     "Christian",
     "Cavé"
    ],
    [
     "Isabelle",
     "Guaïtella"
    ],
    [
     "Serge",
     "Santi"
    ]
   ],
   "title": "Eyebrow movements and voice variations in dialogue situations: an experimental investigation",
   "original": "i02_2353",
   "page_count": 4,
   "order": 225,
   "p1": "2353",
   "pn": "2356",
   "abstract": [
    "This paper deals with the relationships between rapid eyebrow movements and vocal production during dialogue. A sophisticated movement and voice recording and analysis system was used to determine when eyebrow movements occurred relative to the phases of phonation, and how these movements were linked to accentuation-related intonative variations. The results confirmed the previously observed association between rapid eyebrow movements and F0 changes. Temporal relationships between eyebrow movements and the beginning and end of a speakers vocalization suggest that eyebrow movements act as a turn-taking cue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-225"
  },
  "cordoba02_icslp": {
   "authors": [
    [
     "R.",
     "Córdoba"
    ],
    [
     "J.",
     "Macías-Guarasa"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "State clustering improvements for continuous HMMs in a Spanish large vocabulary recognition system",
   "original": "i02_0677",
   "page_count": 4,
   "order": 226,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "In this paper we present a whole set of improvements that have been applied to a large vocabulary isolated-word recognition system using continuous models. This system has been used in the EU funded IDAS project (LE4-8315), where an automated interactive telephonebased directory assistance service has been developed. We cover both improvements in the techniques for continuous HMM reestimation and agglomerative clustering for context-dependent models, all of them applied to our database in Spanish. Specifically, we will show how a new distance between states can greatly improve the performance of the clustering process. We show a new strategy for the clustering itself based in multiple Gaussian clustering which improved the results too. And finally, we present a new way to find the optimum number of Gaussians for each state that can be applied to both context dependent and context independent models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-226"
  },
  "rotovnik02_icslp": {
   "authors": [
    [
     "Tomaz",
     "Rotovnik"
    ],
    [
     "Mirjam Sepesy",
     "Maucec"
    ],
    [
     "Bogomir",
     "Horvat"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "A comparison of HTK, ISIP and julius in slovenian large vocabulary continuous speech recognition",
   "original": "i02_0681",
   "page_count": 4,
   "order": 227,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "In this paper recognition results from different speech decoders are presented for Slovenian large vocabulary speech recognition task. For speech recognition two different types of lexica and language models were used. Word based models were used for baseline system and sub-word (stems and endings) based models for comparison. For all decoders a two-pass decoding strategy was used. With all three decoders better recognition results were achieved using stemending models (3% absolute on average). Experiments also showed slightly better recognition results with Julius decoder, as opposed to two other decoders, and improvement of real-time factor for 65%. Introduce Segmeantal Inner Timewarping into\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-227"
  },
  "jia02_icslp": {
   "authors": [
    [
     "Lei",
     "Jia"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Parametric trajectory segment model for LVCSR",
   "original": "i02_0685",
   "page_count": 4,
   "order": 228,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "In this paper, a parametric trajectory segment model (PTSM) with segmental inner time warping is proposed to improve the recognition accuracy of large vocabulary continuous speech recognition(LVCSR). The proposed PTSM utilizes the state boundary information provided by HMM system during decoding to do segmental inner time warping. Good alignment between different length realizations of a same phone unit can be obtained by this method. Based on the effective alignment, a new distance measure of measuring the average value of the norm of the residual error is used in k-means clustering to decide the parameters of the mixture density of PTSM. For two LVCSR tasks, the HMM system working with the proposed PTSM can give a consistent improvement over either the HMM system working with the traditional PTSM or the HMM system working alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-228"
  },
  "diegueztirado02_icslp": {
   "authors": [
    [
     "F. Javier",
     "Diéguez-Tirado"
    ],
    [
     "Antonio",
     "Cardenal-López"
    ]
   ],
   "title": "Efficient precalculation of LM contexts for large vocabulary continuous speech recognition",
   "original": "i02_0689",
   "page_count": 4,
   "order": 229,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "In a previous work we described a new method to speed up the computation of a Language Model look-ahead in a speech recognizer with a tree organized lexicon. Three different mechanisms were employed to avoid a redundant computation of the probabilities: a node level cache memory, a pre-calculation of active contexts and a perfect hash LM organization. This strategy allowed us to apply a trigram based LM to compute the look-ahead with important computational savings in comparison with the usual bigram or unigram approximation. In this paper we describe several improvements to the pre-calculation of active contexts that allow us to achieve further time reductions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-229"
  },
  "chengalvarayan02b_icslp": {
   "authors": [
    [
     "Rathi",
     "Chengalvarayan"
    ]
   ],
   "title": "Integrating multiple pronunciations during MCE-based acoustic model training for large vocabulary speech recognition",
   "original": "i02_0693",
   "page_count": 4,
   "order": 230,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "In this paper, we report on the implementation of an automatic method for discovering an appropriate pronunciation for each speech utterance of every speaker and integrating this new information into minimum classification error (MCE) based training algorithm. The proposed method allows a lot more flexibility in adapting multiple pronunciations during the existing supervised acoustic model training where the phoneme sequence of a particular word is always fixed irrespective of speaker accents and pronunciation variations. Several large vocabulary recognition results on French SpeechDat-II speech corpus show a consistent string error rate reduction of about 48% and 13% obtained by the proposed integrated method when compared to the MLE-trained and MCE-trained baseline systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-230"
  },
  "laureys02_icslp": {
   "authors": [
    [
     "Tom",
     "Laureys"
    ],
    [
     "Vincent",
     "Vandeghinste"
    ],
    [
     "Jacques",
     "Duchateau"
    ]
   ],
   "title": "A hybrid approach to compounds in LVCSR",
   "original": "i02_0697",
   "page_count": 4,
   "order": 231,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "In several languages compound words form orthographic units, which complicates the task of ensuring good lexical coverage for large vocabulary continuous speech recognition (LVCSR). A common approach to the problem consists of first recognizing the compound constituents, followed by an automatic recompounding process. We describe an accurate compound module, which combines a rule-based approach with statistical pruning. The module is incorporated in a broadcast news recognition task for Dutch and yields an 11% relative decrease in word error rate (WER).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-231"
  },
  "utsuro02_icslp": {
   "authors": [
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Tetsuji",
     "Harada"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A confidence measure based on agreement among multiple LVCSR models - correlation between pair of acoustic models and confidence",
   "original": "i02_0701",
   "page_count": 4,
   "order": 232,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "For many practical applications of speech recognition systems, it is quite desirable to have an estimate of confidence for each hypothesized word. Unlike previous works on confidence measures, this paper studies features for confidence measures that are extracted from outputs of more than one LVCSR models. More specifically, this paper experimentally evaluates the agreement among the outputs of multiple Japanese LVCSR models, with respect to whether it is effective as an estimate of confidence for each hypothesized word. The results of experimental evaluation show that the agreement between the outputs with two LVCSR models with different decoders and acoustic models can achieve quite reliable confidence. Furthermore, among various features of acoustic models based on Gaussian mixture HMMs, it is concluded that ones such as whether or not to have short pause models, as well as different units in HMMs (e.g., triphone model or syllable model) are the most effective in achieving highly reliable confidence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-232"
  },
  "nouza02_icslp": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jindra",
     "Drabkova"
    ]
   ],
   "title": "Combining lexical and morphological knowledge in language model for inflectional (czech) language",
   "original": "i02_0705",
   "page_count": 4,
   "order": 233,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "In this paper we study several possibilities to enhance language modeling in case of inflectional languages, namely Czech. We show that some existing smoothing techniques can be further improved to cope with extremely sparse data. We propose several concepts to combine word-based and class-based language models. In our approach the classes are defined with respect to morphological categories and their syntactic relations are evaluated through bigrams. In speech recognition experiments the combination of word bigrams with class statistics helped to get a moderate performance improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-233"
  },
  "nguyen02c_icslp": {
   "authors": [
    [
     "Long",
     "Nguyen"
    ],
    [
     "Xuefeng",
     "Guo"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Modeling frequent allophones in Japanese speech recognition",
   "original": "i02_0709",
   "page_count": 4,
   "order": 234,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "In this paper, we describe a technique to model frequent allophones in Japanese speech recognition. The Consonant-Vowel syllabic structure (CV) is dominant in Japanese. Based on frequency, the distribution of CV pairs is rather skewed. Isolating out the most frequent allophones through the use of additional phonemes in acoustic modeling can achieve better recognition accuracy. By introducing ten new phonemes for the five most common CV pairs, we achieved a 30% relative reduction in word error rate for spontaneous speech and 6% relative reduction overall for all speech categories in a Japanese broadcast news transcription task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-234"
  },
  "chen02c_icslp": {
   "authors": [
    [
     "Feili",
     "Chen"
    ],
    [
     "Jie",
     "Zhu"
    ],
    [
     "Wentao",
     "Song"
    ]
   ],
   "title": "The structure and its implementation of hidden dynamic HMM for Mandarin speech recognition",
   "original": "i02_0713",
   "page_count": 4,
   "order": 235,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "Speech is time variable stochastic process. But the HMM model is stationary and time-independent. We present a new model as Hidden Dynamic HMM model to describe the dynamic property of speech. A hidden layer of dynamic property is interpolated between the observation and state. Estimated Dynamic Property and Predicted Dynamic Property is introduced to describe the hidden dynamic property. The result shows that the HDHMM model can achieve good improvement in different tasks of speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-235"
  },
  "shinozaki02_icslp": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "A new lexicon optimization method for LVCSR based on linguistic and acoustic characteristics of words",
   "original": "i02_0717",
   "page_count": 4,
   "order": 236,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "This paper proposes a new lexicon optimization method to improve recognition rate of large scale spontaneous speech recognition. Occurrence count and length of a word has strong correlation with dif- ficulty of recognizing the word. First, we investigate the relation and make a word correctness probability model. The proposed method optimizes the lexicon by making compound words or phrases step by step based on the word correctness probability model so as to improve the estimated recognition rate of the system. The optimization method is applied to a large scale Japanese spontaneous speech corpus. Experimental results show that the language model using the optimized lexicon improves the recognition rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-236"
  },
  "langlois02_icslp": {
   "authors": [
    [
     "David",
     "Langlois"
    ],
    [
     "Kamel",
     "Smaïli"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Retrieving phrases by selecting the history: application to automatic speech recognition",
   "original": "i02_0721",
   "page_count": 4,
   "order": 237,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "This paper focuses on statistical language modeling for automatic speech recognition. We present a method which aims at finding linguistic units in corpus. This method, called the Selected History Principle, consists in finding strong distant relationships between words. The new units are phrases made up of basic units of our vocabulary linked by these distant relationships. We adapt the multigram principle to large vocabularies in order to introduce an optimal subset of these sequences into a bigram model. The bigram model using these sequences outperforms the baseline bigram model by 21% in terms of Perplexity, and increases the recognition rate of the large vocabulary system Sirocco by 8.7%. The word error rate is decreased by 12.7%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-237"
  },
  "ahn02_icslp": {
   "authors": [
    [
     "Dong-Hoon",
     "Ahn"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Compact subnetwork-based large vocabulary continuous speech recognition",
   "original": "i02_0725",
   "page_count": 4,
   "order": 238,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "We present an improved method of compactly organizing the decoding network for a semi-dynamic network decoder. In the previous work [1], the network management units called subnetworks were made compact by self-structuring themselves. We improve this subnetwork representation in two aspects by employing the shared-tail topology [2]. Firstly, we localize the decoding algorithm so that it works with a set of subnetworks rather than with the whole decoding network. Secondly, we align unshared suffixes of pronunciations into a shared tail to reduce redundancies. Experimental results on a 20k-word Korean dictation task show that our algorithm significantly reduces the memory requirement and produces additional gains in word accuracy by using aligned shared tails.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-238"
  },
  "dutagaci02_icslp": {
   "authors": [
    [
     "Helin",
     "Dutagaci"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "A comparison of four language models for large vocabulary turkish speech recognition",
   "original": "i02_0729",
   "page_count": 4,
   "order": 239,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "This paper gives a comparison of three language models proposed as alternatives to word-based language model for large vocabulary speech recognition of Turkish. Turkish is an agglutinative language and has morphological productivity. This results in a huge vocabulary size and a large number of out of vocabulary words for unseen test data. The solution is to parse the words, in order to get smaller base units, which are capable of covering the language with relatively small vocabulary size. Three different ways of decomposing words into base units are described: Morphem-based model, stem-endingbased model and syllable-based model. These models are compared with respect to vocabulary size, coverage, number of out of vocabulary words, perplexity and sensitivity to context. For all three models, a significant improvement for those measures are observed compared to the word-based language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-239"
  },
  "hincks02_icslp": {
   "authors": [
    [
     "Rebecca",
     "Hincks"
    ]
   ],
   "title": "Speech recognition for language teaching and evaluating: a study of existing commercial products",
   "original": "i02_0733",
   "page_count": 4,
   "order": 240,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "Educators and researchers in the acquisition of L2 phonology have called for empirical assessment of the progress students make after using new methods for learning [1]. This study investigated whether unlimited access to a speech-recognition-based language learning program would improve the general goodness of pronunciation of a group of middle-aged immigrant professionals studying English in Sweden. Eleven students were given a copy of the program Talk to Me by Auralog as a supplement to a 200-hour course in Technical English, and were encouraged to practice on their home computers. Their development in spoken English was compared with a control group of fifteen students who did not receive software. Talk to Me uses speech recognition to provide conversational practice, visual feedback on prosody and scoring of pronunciation. A significant limitation of commercial systems currently available is their inability to diagnose specific articulatory problems. However, in this course students also met at regular intervals with a pronunciation tutor who could steer them in the right direction for finding the most important sections to practice for their particular problems. Students reported high satisfaction with the software and used it for an average of 12.5 hours. Students were pre- and post-tested with the automatic PhonePass SET-10 test from Ordinate Corp. Results indicate that practice with the program was beneficial to those students who began the course with a strong foreign accent but that students who began the course with intermediate pronunciation did not show the same improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-240"
  },
  "raux02_icslp": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic intelligibility assessment and diagnosis of critical pronunciation errors for computer-assisted pronunciation learning",
   "original": "i02_0737",
   "page_count": 4,
   "order": 241,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "We introduce a novel method to diagnose pronunciation errors that are most critical to the intelligibility of L2 learners. A preliminary study showed that error rates computed by a speech recognitionbased system can be used to characterize intelligibility. We deduce a probabilistic algorithm to derive intelligibility from error rates. We also define an error priority function that indicates which errors are most critical to intelligibility. Experimental results proved the validity of the approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-241"
  },
  "hirata02_icslp": {
   "authors": [
    [
     "Yukari",
     "Hirata"
    ]
   ],
   "title": "Effects of production training with visual feedback on the acquisition of Japanese pitch and durational contrasts",
   "original": "i02_0741",
   "page_count": 4,
   "order": 242,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "This study examined the effects of production training with visual feedback on second language (L2) learners production and perception development. Native speakers of English learning intermediate Japanese practiced producing words and sentences with the CSLPitch Program, during which time they tried to match the pitch and durational contours of their own production to those of the models on the computer screen. Their abilities to produce and perceive novel words and sentences including both pitch and durational contrasts were tested in citation and sentence contexts, and were compared to those of a control group. The results indicated that (1) this production training helped the trained subjects to both produce and perceive the pitch and durational contrasts, and that (2) the trained subjects abilities in the two domains were greatly enhanced in the sentence context as well as in the citation context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-242"
  },
  "minematsu02b_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Donna",
     "Erickson"
    ]
   ],
   "title": "Acoustic modeling of sentence stress using differential features between syllables for English rhythm learning system development",
   "original": "i02_0745",
   "page_count": 4,
   "order": 243,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "This study proposes a new technique for acoustic modeling of stressed/ unstressed syllables in sentence utterances of American English. Here, relative differences of acoustic features between two consecutive syllables characterizing \"stressed\" or \"unstressed\" are introduced into HMM-based acoustic modeling. This is because syllables can be identified as stressed or unstressed only after comparing them with their neighboring syllables. For training syllable HMMs, speech samples were recorded by ourselves because we could not find any database which can be directly used for this modeling. The fourth author put multi-level stress marks (syllable magnitude) on individual syllables of a given sentence set, which was done according to guidelines for teaching English rhythm to non-native speakers of English, proposed and used in class by the fourth author. After the stress mark assignment, the sentences were uttered by her and recorded for the HMM-based modeling. Experiments showed that stress/unstress identification errors were reduced by about 25% in comparison to the modeling technique without the relative differences. With this new technique, an English sentence stress detector is being developed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-243"
  },
  "imoto02_icslp": {
   "authors": [
    [
     "Kazunori",
     "Imoto"
    ],
    [
     "Yasushi",
     "Tsubota"
    ],
    [
     "Antoine",
     "Raux"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "Modeling and automatic detection of English sentence stress for computer-assisted English prosody learning system",
   "original": "i02_0749",
   "page_count": 4,
   "order": 244,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "We address sentence-level stress detection of English for Computer- Assisted Language Learning (CALL) by Japanese students. Stress models are set up by considering syllable structure and position of the syllable in a phrase, which will provide diagnostic information for students. We also propose a two-stage recognition method that first detects the presence of stress and then identifies the stress level using different weighted combinations of acoustic features. The modeling is coherent with conventional linguistic observations. The method achieves stress recognition rate of 95.1% for native and 84.1% for Japanese speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-244"
  },
  "tsubota02_icslp": {
   "authors": [
    [
     "Yasushi",
     "Tsubota"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "Recognition and verification of English by Japanese students for computer-assisted language learning system",
   "original": "i02_1205",
   "page_count": 4,
   "order": 245,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "We address methods for recognizing English spoken by Japanese students as the basis for our Computer-Assisted Language Learning (CALL) system. For automatic phonemic error detection, pronunciation error prediction is executed for a given orthographic text. To improve reliability, speaker adaptation and segment-input pair-wise verification are applied as pre-processing and post-processing, respectively. We also address acoustic modeling as a means for coping with the large acoustic variation seen in nonnative speech. First, English acoustic models are trained using a database of English spoken by Japanese students. Japanese phonemes that are regarded as allophones of English phonemes are then incorporated. We present the results of experimental comparison of these models and confirm the effectiveness of speaker adaptation and pair-wise verification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-245"
  },
  "neri02_icslp": {
   "authors": [
    [
     "Ambra",
     "Neri"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Feedback in computer assisted pronunciation training: technology push or demand pull?",
   "original": "i02_1209",
   "page_count": 4,
   "order": 246,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "In this paper, we examine the type of feedback that currently available Computer Assisted Pronunciation Training (CAPT) systems provide, with a view to establishing whether this meets pedagogically sound requirements. We show that many commercial systems tend to prefer technological novelties that do not always comply with pedagogical criteria and that despite the limitations of todays technology, it is possible to design CAPT systems that are more in line with learners needs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-246"
  },
  "minematsu02c_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Corpus-based analysis of English spoken by Japanese students in view of the entire phonemic system of English",
   "original": "i02_1213",
   "page_count": 4,
   "order": 247,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "English and Japanese are quite different languages both phonetically and linguistically and it is often very difficult for Japanese students to master English pronunciation. To help students improve their pronunciation proficiency, a Japanese national project of \"Advanced Utilization of Multimedia for Education\" has started in 2000 and under this project, a large database of English words and sentences read by 200 Japanese students was built mainly for CALL system development. This paper describes a corpus-based analysis and comparison of American English (AE) and Japanese English (JE) by using WSJ database and the new JE database. Here, Hidden Markov Models (HMMs), which are widely-used acoustic modeling techniques of speech recognition, were firstly made for individual phonemes in the two kinds of English, and then, a tree diagram was drawn for the entire phonemes of each HMM set. The analysis and comparison of the two trees showed many interesting characteristics of JE, some of which are well-known habits observed in JE pronunciation. The authors consider that this study showed statistical differences between AE and JE in view of the entire phonemic system of English for the first time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-247"
  },
  "hardison02_icslp": {
   "authors": [
    [
     "Debra M.",
     "Hardison"
    ]
   ],
   "title": "Computer-assisted second-language speech learning: generalization of prosody-focused training",
   "original": "i02_1217",
   "page_count": 4,
   "order": 248,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "In a pretest-posttest design, native English-speaking learners of French were given three weeks of training focused on prosody using a realtime computerized visual pitch contour display. Native speaker (NS) exemplars stored on hard disk provided feedback including overlay of the NS pitch contour on the subjects productions. These were recorded and later presented to NSs in two conditions to determine the influence of segmental information on their perception of prosody: filtered (unintelligible segmental information) and un- filtered. Ratings were given using 7-point scales for the prosody of filtered samples and both the prosody and segmental accuracy of unfiltered samples. Results revealed a) significant improvement in prosody (filtered and unfiltered samples) after training, b) transfer to segmental improvement, c) generalization to novel sentences, and d) some segmental influence on the NS perception of prosody. In a subsequent memory recall task using a subset of filtered NS training exemplars with reduced intelligibility, subjects identified the exact content of more than 80% of the sentences. Subjects also reported a greater awareness of the various aspects of speech and increased confidence in producing another language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-248"
  },
  "mostow02_icslp": {
   "authors": [
    [
     "Jack",
     "Mostow"
    ],
    [
     "Joseph",
     "Beck"
    ],
    [
     "S. Vanessa",
     "Winter"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Brian",
     "Tobin"
    ]
   ],
   "title": "Predicting oral reading miscues",
   "original": "i02_1221",
   "page_count": 4,
   "order": 249,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "This paper explores the problem of predicting specific reading mistakes, called miscues, on a given word. Characterizing likely miscues tells an automated reading tutor what to anticipate, detect, and remediate. As training and test data, we use a database of over 100,000 miscues transcribed by University of Colorado researchers. We explore approaches that exploit different sources of predictive power: the uneven distribution of words in text, and the fact that most miscues are real words. We compare the approaches ability to predict miscues of other readers on other text. A simple rote method does best on the most frequent 100 words of English, while an extrapolative method for predicting real-word miscues performs well on less frequent words, including words not in the training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-249"
  },
  "kim02d_icslp": {
   "authors": [
    [
     "Chanwoo",
     "Kim"
    ],
    [
     "Wonyong",
     "Sung"
    ]
   ],
   "title": "Implementation of an intonational quality assessment system",
   "original": "i02_1225",
   "page_count": 4,
   "order": 250,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "In this paper, we describe an intonational quality scoring system for foreign language learning. We employed the segmental K-means algorithm to segment the speech into syllables and a pitch detection algorithm to extract the intonational features. We classified the segmented syllables into 5 types according to the shapes of the pitch contours in them. To present visual aids to students, we displayed the classified tonal pitch type of each syllable and the overall pitch movement tendency of the test and reference sentences. We devised an algorithm to obtain a score from the spoken sentence and used this value as a measure for assessing the intonational quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-250"
  },
  "ariki02_icslp": {
   "authors": [
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Jun",
     "Ogata"
    ]
   ],
   "title": "English call system with functions of speech segmentation and pronunciation evaluation using speech recognition technology",
   "original": "i02_1229",
   "page_count": 4,
   "order": 251,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "In communication learning by second language, three abilities have to be improved; listening, speaking and writing abilities. In this sense, it is important to evaluate users pronunciation ability and to detect mispronunciations in English CALL (Computer-Assisted Language Learning) systems. In this paper, we propose three functions (segmentation, phrasing and pronunciation evaluation) in the CALL system using speech recognition technologies. The system was evaluated by the answer to questionnaires for ten learners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-251"
  },
  "mixdorff02_icslp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Patavee",
     "Charnvivit"
    ]
   ],
   "title": "Perception of tone and vowel quantity in Thai",
   "original": "i02_0753",
   "page_count": 4,
   "order": 252,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "The current study examines the interaction of syllable tones and vowel quantity in the production and perception of monosyllabic words of Thai. A speech corpus containing groups of words differing only as to tone type and vowel quantity was designed. These were embedded in a short carrier sentence of five mid tone syllables, with the target word being the center syllable. The utterances were analyzed with respect to the tonal and segmental features of the target words and F0 contours modeled using the Fujisaki model. Analysis shows that all mid tone sequences can be modeled using the phrase component only whereas the remaining tones require either single tone commands of positive or negative polarity, or a command pair. Based on the analysis results, a perception experiment was designed to explore the perceptual space between words of tone/vowel quantity contrasts. Results indicate, inter alia, that vowel quantity is perceived as shorter when words are presented in isolation than when embedded in a carrier sentence. Confusions generally occur more frequently between words of different vowel quantity than of different tones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-252"
  },
  "kinoshita02_icslp": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Duration and F0 as perceptual cues to Japanese vowel quantity",
   "original": "i02_0757",
   "page_count": 4,
   "order": 253,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "Vowel duration and local fundamental frequency changes are investigated as acoustical cues to vowel quantity identification by Japanese listeners. To examine the role of these factors, a perception experiment was carried out. The results indicate that, even though vowel duration serves as a dominant perceptual cue, when vowel quantity cannot be adequately cued by vowel duration alone, the F0 information within the vowel can be used to identify vowel quantity in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-253"
  },
  "muto02_icslp": {
   "authors": [
    [
     "Makiko",
     "Muto"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Effects of intra-phrase position on acceptability of changes in segmental duration in sentence speech",
   "original": "i02_0761",
   "page_count": 4,
   "order": 254,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "Perceptual evaluation experiments were carried out on the acceptability of segmental duration change in a sentence. The intra-phrase positional effects were systematically analyzed using synthesized speech stimuli in which one of the vowel segments was either lengthened or shortened. Statistical analyses showed that acceptability degradation due to duration changes was greater at the initial and medial positions than at the final position in a phrase. Further perceptual experiments suggested that the factors of speaking rate and breath grouping affected the variability of the intra-phrase positional effects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-254"
  },
  "baraccikoja02_icslp": {
   "authors": [
    [
     "Dragana",
     "Barac-Cikoja"
    ],
    [
     "Sally",
     "Revoile"
    ]
   ],
   "title": "Perception of prosodic phrasing by hearing-impaired listeners",
   "original": "i02_0765",
   "page_count": 4,
   "order": 255,
   "p1": "765",
   "pn": "768",
   "abstract": [
    "Perception of intonational phrase integrity by hearing-impaired (HI) listeners was investigated in three experiments. Listeners compared fluently spoken sentences and sentences assembled by concatenating words. Signals consisted of speech and noise that was amplitude modulated by speech envelopes. Listeners were able to perceive prosodic phrasing in speech, but were significantly less accurate with noise signals in spite of their success in matching speech utterances to their noise replicas. Neither speech style (clear vs. conversational) nor the limited practice showed significant effect on performance accuracy. Individual differences could not be attributed either to the listeners preferred communication mode or the degree of hearing loss.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-255"
  },
  "aasland02_icslp": {
   "authors": [
    [
     "Wendi A.",
     "Aasland"
    ],
    [
     "Shari R.",
     "Baum"
    ]
   ],
   "title": "Processing of temporal cues marking phrasal boundaries in individuals with brain damage",
   "original": "i02_0769",
   "page_count": 4,
   "order": 256,
   "p1": "769",
   "pn": "772",
   "abstract": [
    "Two experiments were conducted to examine the ability of left- (LHD) and right-hemisphere-damaged (RHD) patients to use temporal cues in rendering phrase grouping decisions. The phrase \"pink and black and green\" was manipulated to signal a boundary after \"pink\" or after \"black\" by altering pre-boundary word durations and pause durations. Results revealed that LHD patients required longer than normal pause durations to consistently identify the intended grouping, suggesting a higher than normal threshold for perception. Surprisingly, the RHD patients exhibited great difficulty with the task, perhaps due to the limited acoustic cues available in the stimuli.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-256"
  },
  "herbordt02_icslp": {
   "authors": [
    [
     "W.",
     "Herbordt"
    ],
    [
     "J.",
     "Ying"
    ],
    [
     "H.",
     "Buchner"
    ],
    [
     "W.",
     "Kellermann"
    ]
   ],
   "title": "A real-time acoustic human-machine front-end for multimedia applications integrating robust adaptive beamforming and stereophonic acoustic echo cancellation",
   "original": "i02_0773",
   "page_count": 4,
   "order": 257,
   "p1": "773",
   "pn": "776",
   "abstract": [
    "Joint beamforming microphone arrays and multi-channel acoustic echo cancellation (AEC) can be efficiently applied for hands-free speech communication. Especially, systems relying on adaptive generalized sidelobe canceller (GSC) structures are very promising, since they combine high noise-reduction performance with computational efficiency. So far, robustness of the GSC was very challenging, due to reverberation and non-stationarity of desired signals and interferers. In this contribution, we present for the first time a real-time system which integrates GSC and stereophonic AEC. It is robust against desired signal cancellation while highly suppressing interference and acoustic echoes. The realization on a low-cost PC platform, with the microphone array connected directly to the universal serial bus (USB), provides maximum hardware and software compatibility for personalized mobile computing devices and desktop PCs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-257"
  },
  "lu02b_icslp": {
   "authors": [
    [
     "Ching-Ta",
     "Lu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Enhancement of single channel speech using perception-based wavelet transform",
   "original": "i02_0777",
   "page_count": 4,
   "order": 258,
   "p1": "777",
   "pn": "780",
   "abstract": [
    "The soft thresholding function has been used in wavelet packet structure to efficiently remove background noises. However, it suffers from both serious residual noise and speech distortion. The classical method that estimates the threshold of wavelet coefficients assumes the corrupted noise to be white Gaussian noise. In fact, the noise is generally not white in practical environments. This paper proposes a method to adapt wavelet coefficient thresholds (WCTs) of each wavelet subband using both the segmental SNR (SegSNR) and the noise masking thresholds (NMTs). The experiments shows that integrating SegSNR and NMTs to improve the wavelet-based speech enhancement method can efficiently remove the background noise and suppress the residual noise. Removing the infecting noise from the corrupted speech, with a little impact on speech, can be obtained by this proposed scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-258"
  },
  "lin02_icslp": {
   "authors": [
    [
     "L.",
     "Lin"
    ],
    [
     "W. H.",
     "Holmes"
    ],
    [
     "E.",
     "Ambikairajah"
    ]
   ],
   "title": "Speech enhancement based on a perceptual modification of wiener filtering",
   "original": "i02_0781",
   "page_count": 4,
   "order": 259,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "A speech denoising technique based on subband noise estimation and a perceptual modification of Wiener filtering is proposed. The noisy speech is first decomposed into critical band signals by an auditory filterbank and the denoising is carried out on the subband signals. The time varying subband noise variance required for denoising is estimated by tracking the minimum variance of the subband noisy signal. Auditory masking is applied to calculate the optimum denoising gain using a Wiener filtering approach to minimize the perceived speech distortion. This denoising technique is suitable for coloured and non-stationary noise environments. It gives natural sounding speech with a very low level of musical noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-259"
  },
  "attias02_icslp": {
   "authors": [
    [
     "Hagai",
     "Attias"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "A new approach to speech enhancement by a microphone array using EM and mixture models",
   "original": "i02_0785",
   "page_count": 4,
   "order": 260,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "Speech enhancement and recognition in noisy, reverberant conditions is a challenging open problem. We present a new approach to this problem, which is developed in the framework of probabilistic modeling. Our approach incorporates information about the statistical structure of speech signals using a speech model, which is pre-trained on a large dataset of clean speech. The speech model is a component in a larger model describing the observed sensor signals. That model is parametrized by the coefficients of the reverberation filters and the spectra of the sensor noise. We develop an EM algorithm that estimates those parameters from data and constructs a Bayes optimal estimator of the original speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-260"
  },
  "kim02e_icslp": {
   "authors": [
    [
     "Sang G.",
     "Kim"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Acoustic echo cancellation based on m-channel IIR cosine-modulated filter bank",
   "original": "i02_0789",
   "page_count": 4,
   "order": 261,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "In this paper, an acoustic echo canceller (AEC) based on an M-channel infinite-impulse response (IIR) cosine-modulated filter bank (CMFB) with near-perfect reconstruction (PR) property is proposed. The use of a 2-channel lossless lattice section with 1st order allpass filter in the design of the prototype filter that is common to both the analysis and the synthesis filter banks permits the filter bank design to be simple and allows the prototype filter to have better stopband attenuation and sharper roll-off characteristics than that of previous designs of similar complexity. The effectiveness of AEC based on the proposed IIR CMFB is evaluated in terms of the echo return loss enhancement (ERLE).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-261"
  },
  "saruwatari02_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Katsuyuki",
     "Sawai"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Atsunobu",
     "Kaminuma"
    ],
    [
     "Masao",
     "Sakata"
    ]
   ],
   "title": "Speech enhancement in car environment using blind source separation",
   "original": "i02_1781",
   "page_count": 4,
   "order": 262,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "We propose a new algorithm for blind source separation (BSS), in which independent component analysis (ICA) and beamforming are combined to resolve the low-convergence problem through optimization in ICA. The proposed method consists of the following four parts: (1) frequency-domain ICA with direction-of-arrival (DOA) estimation, (2) null beamforming based on the estimated DOA, (3) diversity of (1) and (2) in both iteration and frequency domain, and (4) subband elimination (SBE) based on the independence among the separated signals. The temporal alternation between ICA and beamforming can realize fast- and high-convergence optimization. Also SBE enforcedly eliminates the subband components in which the separation could not be performed well. The experiment in a real car environment reveals that the proposed method can improve the qualities of the separated speech and word recognition rates for both directional and diffusive noises.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-262"
  },
  "potamitis02_icslp": {
   "authors": [
    [
     "I.",
     "Potamitis"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Speech enhancement based on combining perceptual enhancement and short-time spectral attenuation",
   "original": "i02_1785",
   "page_count": 4,
   "order": 263,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "The present paper introduces a cooperating framework between Perceptual Enhancement (PE) and Minimum Mean Square Error Log-Spectral Amplitude Estimation (MMSE-LSA). The link between the two aforementioned spectral modification algorithms is a Speech Absence Probability (SAP) responsible for weighting the fusion of both spectral gain modification algorithms for each spectral bin. It has been noted that PE eliminates residual noise at the expense of spectral deformation of unvoiced speech and high-frequency speech components. On the other hand the LSA technique is based on modeling assumptions that lead to smaller degradation than PE at the expense of some residual noise and an echo-like processing artifact. The proposed hybrid scheme leads to a reduction of residual noise component compared with LSA and a reduction of spectral distortion observed with PE when it is employed at low SNRs. We evaluate the proposed algorithm using White Gaussian and car-noise on the task of improving the quality of speech and demonstrate its robustness at low SNRs.\n",
    "Implementation code and demo files are available at: http://slt.wcl.- ee.upatras.gr/Potamitis/web/LSA_PE_index.htm\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-263"
  },
  "nishiura02_icslp": {
   "authors": [
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Yuka",
     "Okada"
    ],
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Suitable design of adaptive beamformer based on average speech spectrum for noisy speech recognition",
   "original": "i02_1789",
   "page_count": 4,
   "order": 264,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "Recognition of distant-talking speech is indispensable for self-moving robots or teleconference systems. However, background noise and room reverberations seriously degrade the sound capture quality in real acoustic environments. A microphone array is an ideal candidate as an effective method for capturing distant-talking speech. AMNOR (Adaptive Microphone-array for NOise Reduction) was proposed an adaptive beamformer for capturing the desired distant signals in noisy environments by Kaneda et al. Although AMNOR has proven itself effective, it could be further improved if we knew the spectrum characteristics of desired distant signals in advance. Therefore, in this paper we regard speech as a desired distant signal and design AMNOR based on the average speech spectrum for distant-talking speech capture and recognition. As a result of evaluation experiments in real acoustic environments, we could confirm that the ASR (Automatic Speech Recognition) performance was improved 5 ~ 10% by AMNOR based on average speech spectrum in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-264"
  },
  "tam02_icslp": {
   "authors": [
    [
     "King",
     "Tam"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Todd",
     "Schneider"
    ]
   ],
   "title": "Highly oversampled subband adaptive filters for noise cancellation on a low-resource DSP system",
   "original": "i02_1793",
   "page_count": 4,
   "order": 265,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "A real-time subband adaptive noise cancellation system on an ultra low-power miniature DSP system is implemented. The system is targeted at personal communication devices where the speaker may be in a noisy environment. The system is implemented on an ultra lowpower DSP system that incorporates a DSP core and an oversampled WOLA filterbank. Pre-emphasis filters are used to increase the convergence rate of a leaky LMS algorithm in the oversampled subband implementation. System performance is also improved relative to a fullband implementation due to benefits arising from using subband adaptive filters instead of fullband filters. A 10 dB reduction of noise power is achieved in tests using various noise conditions. The entire DSP system consumes 2.1 mW and can be realized in a package size of 6.5 × 3.5 × 2.5 mm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-265"
  },
  "hu02_icslp": {
   "authors": [
    [
     "Yi",
     "Hu"
    ],
    [
     "Philipos C.",
     "Loizou"
    ]
   ],
   "title": "A perceptually motivated subspace approach for speech enhancement",
   "original": "i02_1797",
   "page_count": 4,
   "order": 266,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "A perceptually motivated subspace based approach is proposed for enhancement of speech corrupted by colored noise. The proposed approach takes into account the frequency masking properties of the human auditory system and reduces the perceptual effect of the residual noise. Objective measures and informal listening tests demonstrated improvements over other subspace-based methods when tested with TIMIT sentences corrupted with speech-shaped noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-266"
  },
  "ju02_icslp": {
   "authors": [
    [
     "Gwo-hwa",
     "Ju"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Speech enhancement based on generalized singular value decomposition approach",
   "original": "i02_1801",
   "page_count": 4,
   "order": 267,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "The singular value decomposition (SVD)-based signal-subspace approach for noise reduction has received high interests in recent years. With this approach, we can diagonalize the matrices constructed from noisy speech frames and divide the whole feature-space into signal-subspace and noise-subspace by the singular values obtained from the matrices. We then reconstruct speech from the signalsubspace only. In this way, speech signals can be successfully enhanced. This approach is very effective when the additive noise is white. If the noise is not white, we have to first whiten the noise spectrum prior to SVD-based approach and perform the inverse whitening procedure afterwards. Not only the process is complicate, but extra distortion may be introduced in such a process. In this paper, a generalized SVD (GSVD)-based approach for speech enhancement is proposed, which is useful regardless of whether the added noise is white or not. Experimental results show that this new approach can provide very good performance, specially better than the conventional spectral subtraction algorithm and SVD-based approach, in particular when the additive noise is non-white.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-267"
  },
  "kim02f_icslp": {
   "authors": [
    [
     "Jong Uk",
     "Kim"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Subspace speech enhancement using subband whitening filter",
   "original": "i02_1805",
   "page_count": 4,
   "order": 268,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "A novel subspace approach for speech enhancement using a subband whitening filter is proposed. Previous subspace approaches for enhancement either assumed white noise or used a fullband prewhitening filter before enhancement for colored noise. The previous approaches were successful only in reducing the upper bound of the signal distortion while reducing noise. However, the proposed method minimizes the overall signal distortion while reducing noise. Experimental results show that the proposed method attains higher segmental signal-to-noise ratio (seg_SNR) than that attained by Ephraim et al. and also by the Wiener filter algorithm. In addition, the proposed algorithm requires less computational load than previous subspace approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-268"
  },
  "chang02_icslp": {
   "authors": [
    [
     "Sungwook",
     "Chang"
    ],
    [
     "Sungil",
     "Jung"
    ],
    [
     "Y.",
     "Kwon"
    ],
    [
     "Sung-il",
     "Yang"
    ]
   ],
   "title": "Speech enhancement using wavelet packet transform",
   "original": "i02_1809",
   "page_count": 4,
   "order": 269,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "In this paper, we present denoising algorithm using wavelet packet transform based on VAD(Voice Activity Detection). For the purpose, the weighting parameter estimation method based on speech dominant indicator and the modified Beroutis method are suggested. Subjective evaluation shows that the performance of the proposed algorithm with wavelet bases is better than that with Fourier basis. Log-Domain Speech Feature Enhancement Using\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-269"
  },
  "deng02_icslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Sequential MAP noise estimation and a phase-sensitive model of the acoustic environment",
   "original": "i02_1813",
   "page_count": 4,
   "order": 270,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "In this paper we present an MMSE (minimum mean square error) speech feature enhancement algorithm, capitalizing on a new probabilistic, nonlinear environment model that effectively incorporates the phase relationship between the clean speech and the corrupting noise in acoustic distortion. The MMSE estimator based on this phase-sensitive model is derived and it achieves high efficiency by exploiting single-point Taylor series expansion to approximate the joint probability of clean and noisy speech as a multivariate Gaussian. As an integral component of the enhancement algorithm, we also present a new sequential MAP-based nonstationary noise estimator. Experimental results on the Aurora2 task demonstrate the importance of exploiting the phase relationship in the speech corruption process captured by the MMSE estimator. The phase-sensitive MMSE estimator reported in this paper performs significantly better than phase-insensitive spectral subtraction (54% error rate reduction), and also noticeably better than a phase-insensitive MMSE estimator as our previous state-of-the-art technique reported in [2] (7% error rate reduction), under otherwise identical experimental conditions of speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-270"
  },
  "nakadai02b_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Hiroaki",
     "Kitano"
    ]
   ],
   "title": "Auditory fovea based speech enhancement and its application to human-robot dialog system",
   "original": "i02_1817",
   "page_count": 4,
   "order": 271,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "This paper presents an active direction-pass filter (ADPF) that separates sound from a specified direction by using a pair of microphones. Its application to front-end processing for speech recognition is also reported. The ADPF improves sound source separation by accurate sound direction obtained by multi-modal integration and active motor control that keeps the robot facing to a sound source, because the resolution of the center direction is much higher than that of peripherals, indicating similar property of visual fovea. In order to recognize separated sound streams, a Hidden Markov Model (HMM) based automatic speech recognition is built with multiple acoustic models trained by the output of the ADPF under various conditions. The experimental results by a preliminary dialog system prove that it works well even when two speakers speak simultaneously.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-271"
  },
  "visser02_icslp": {
   "authors": [
    [
     "Erik",
     "Visser"
    ],
    [
     "Manabu",
     "Otsuka"
    ],
    [
     "Te-Won",
     "Lee"
    ]
   ],
   "title": "A spatio-temporal speech enhancement scheme for robust speech recognition",
   "original": "i02_1821",
   "page_count": 4,
   "order": 272,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The schemes performance is illustrated by speech recognition experiments on real recordings in a noisy car environment and compared to conventional techniques like beamforming and spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-272"
  },
  "berthommier02_icslp": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Seungjin",
     "Choi"
    ]
   ],
   "title": "Comparative evaluation of CASA and BSS models for subband cocktail-party speech separation",
   "original": "i02_1825",
   "page_count": 4,
   "order": 273,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "For speech segregation, a recurrent blind separation model (BSS) is tested together with a Computational Auditory Scene Analysis (CASA) model, which is based on the localisation cue and the evaluation of the Time Delay Of Arrival (TDOA). The test database is composed of 332 binary mixture sentences recorded in stereo with a static set-up. These are truncated at 1 second for the simulations. For applying the two models, we divide the frequency domain into a variable number of subbands, which are processed independently. Then, we evaluate the gain, using reference signals recorded in isolation. After a careful analysis, we find similar gains of about 2-3dB for both methods. The variation of the number of subbands allows an optimisation, and we obtain a significant peak at 4 subbands for the CASA model, as well as a maximum at 2 subbands for the BSS model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-273"
  },
  "kim02g_icslp": {
   "authors": [
    [
     "Hyoung-Gook",
     "Kim"
    ],
    [
     "Dietmar",
     "Ruwisch"
    ]
   ],
   "title": "Speech enhancement in non-stationary noise environments",
   "original": "i02_1829",
   "page_count": 4,
   "order": 274,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "This paper presents a speech enhancement using a noise estimation based on the ratio of the noisy speech and its minimum (NSMR) for non-stationary noise environments. The noise estimator is a very simple but highly effective real time approach for single channel noise reduction. The enhanced speech is free of musical tones and reverberation artifacts and sounds very natural compared to methods using other short-time spectrum attenuation techniques. The performance is measured by the segmental signal-to-noise ratio and MOS tests. To judge the performance the recognition accuracy of an Automatic Speech Recognition (ASR) system using Mel-scale Frequency Cepstral Coefficients (MFCC) features is measured with and without noise reduction. In another experiment we apply the NSMR noise reduction method to speech reconstruction at the back-end of a distributed speech recognition (DSR) system under various noise conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-274"
  },
  "mizumachi02_icslp": {
   "authors": [
    [
     "Mitsunori",
     "Mizumachi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "The 2ch hybrid subtractive beamformer applied to line sound sources",
   "original": "i02_1833",
   "page_count": 4,
   "order": 275,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "A small-scale microphone array generally adopts the subtractive beamforming technique, which constructs sharp notches in a spatial beampattern, so that target sound sources are assumed as point sound sources. However, the shapes of actual sound sources are various and not always points. Our mouths are line sound sources or plane sound sources. It is important to deal with line sound sources when we apply the subtractive beamformer to speech signal processing. This paper proposes a hybrid subtractive beamformer that combines several 2ch subtractive beamformers properly in each frequency to expand the width of a sharp notch. The feasibility of the proposed hybrid subtractive beamformer is confirmed by performance tests in suppressing the signals from line sound sources.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-275"
  },
  "yapanel02_icslp": {
   "authors": [
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Xianxian",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "High performance digit recognition in real car environments",
   "original": "i02_0793",
   "page_count": 4,
   "order": 276,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "In this paper, we consider the problem of robust digit recognition in real car environments. We choose to utilize newly-collected CU-Move database [2]. We address the problem using two integrated approaches . First, we consider array processing, enhancement and noise adaptation techniques as an integrated solution. This approach reduced the word error rate (WER) 38.6% and increased word accuracy (WAC) 47.1%, relative to baseline results. Secondly, we use array processing, enhancement, cepstral mean normalization, vocal tract length normalization and MLLR adaptation as an alternative solution. The net gain obtained with this solution is 55.4% reduction in WER and 64.3% increase in WAC, relative to baseline results. The first approach has the advantage of speed since all operations can be performed in real-time, while the second approach maintains high accuracy at the cost of increased computational requirements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-276"
  },
  "shinde02_icslp": {
   "authors": [
    [
     "Tetsuya",
     "Shinde"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Multiple regression of log-spectra for in-car speech recognition",
   "original": "i02_0797",
   "page_count": 4,
   "order": 277,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "This paper describes a new multichannel method of noisy speech recognition, which estimates the log spectrum of speech at a closetalking microphone based on the multiple regression of the log spectra (MRLS) of noisy signals captured by the distributed microphones. Since the method does not assume the arrangement of sound sources and microphones, it can be applied to in-car speech recognition directly. The experimental evaluation shows an error reduction of up to 15% in isolated word recognition accuracy under various driving conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-277"
  },
  "gong02b_icslp": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Lorin",
     "Netsch"
    ]
   ],
   "title": "Experiments on speaker-independent voice command recognition using in-vehicle hands free speech",
   "original": "i02_0801",
   "page_count": 4,
   "order": 278,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "For speaker-independent voice command recognition, the acoustic models can be trained using either task-independent speech corpora or task-specific speech corpora. The first alternative could reuse available speech databases, without collecting task-specific speech data. On the other hand, it is expected to give lower recognition performance due to acoustic and vocabulary mismatches between the speech corpora and the task.\n",
    "This paper addresses the performance difference that can be expected between the two alternatives for a hands-free command recognition task. The databases involved consist of a general American speech database recorded in an office, and a command database recorded in-vehicle hands-free using a distant talking microphone. The evaluation is performed with three parameters: speech model training technique (task-independent vs. task-specific), mismatch compensation techniques, and various driving conditions. The experiments show that model adaptation with task-specific speech data can reduce the word error rates resulting from models trained on task-independent speech data by more than two thirds for all driving conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-278"
  },
  "kadambe02_icslp": {
   "authors": [
    [
     "Shubha",
     "Kadambe"
    ]
   ],
   "title": "Application of over-complete blind source separation for robust automatic speech recognition",
   "original": "i02_0805",
   "page_count": 4,
   "order": 279,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "Spoken dialogue based information retrieval systems that are used in mobile environments are becoming popular. However, mobile environment is dynamically changing and there exists many interfering signals. These two effects result in degradation in automatic speech recognition (ASR) accuracy and hence, degradation in performance of spoken dialogue based information retrieval systems. One way to improve the speech recognition accuracy is to separate the intended speech signal from the interference signals and use the enhanced speech signals in recognition. In this paper, we describe a technique that we applied for speech signal enhancement. We also provide the relative improvement in recognition accuracy that we obtained by using such enhanced speech signals in an ASR system. For speech signal enhancement, we apply the Over-complete Blind Source Separation (OCBSS) technique that we developed. For ASR, a continuous speech recognizer was used. In this paper, we also compare the recognition accuracy results of another BSS technique that is based on Independent Component Analysis (ICA) - JADE-ICA with OCBSS. The results indicate that as the complexity of signal separation problem increases i.e., close to real scenarios, the OCBSS provides about 30% better relative improvement in recognition accuracy as compared to JADE-ICA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-279"
  },
  "beaufays02_icslp": {
   "authors": [
    [
     "Françoise",
     "Beaufays"
    ],
    [
     "Daniel",
     "Boies"
    ],
    [
     "Mitch",
     "Weintraub"
    ]
   ],
   "title": "Porting channel robustness across languages",
   "original": "i02_0809",
   "page_count": 4,
   "order": 280,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "We propose a technique to port channel characteristics from one language to another. This allows us to build acoustic models in a target language that are robust to an environment for which we have no data in that language.\n",
    "The approach consists in training broad phonetic class maximum likelihood linear regression (MLLR) transformations from a source language, and applying them in the target language. These transforms encapsulate the acoustic specificities of the environment without capturing language-specific characteristics that are difficult to port across languages.\n",
    "As a case study, we consider the problem of building in-the-car GSM models for UK English, assuming that we have no GSM, and no car data in UK English, but that we have such data in German. We show that this technique can greatly reduce the error rate of the recognition system on English GSM car data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-280"
  },
  "takahashi02_icslp": {
   "authors": [
    [
     "Yasuhiro",
     "Takahashi"
    ],
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "An efficient dialogue control method using decision tree-based estimation of out-of-vocabulary word attributes",
   "original": "i02_0813",
   "page_count": 4,
   "order": 281,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "This paper proposes a dialogue control method for completing a task with a short dialogue even when user utterances include out-of-vocabulary (OOV) words. When the user utterance includes an OOV word, conventional methods try to complete a task by acquiring an understanding of the OOV word through dialogue. However, completing a task does not always require that the system understands the OOV word through dialogue. Our method eliminates the dialogue for acquiring an understanding of the OOV word and tries to complete a task by estimating attributes of OOV words using a decision tree. It does not allow system utterances using those attributes, which stops the user from uttering the same OOV words. A dialogue simulation indicates that our method can complete a task with dialogues that are shorter than those conventional methods need for completing a task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-281"
  },
  "bellegarda02_icslp": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ]
   ],
   "title": "Semantic inference: a data-driven solution for NL interaction",
   "original": "i02_0817",
   "page_count": 4,
   "order": 282,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "In sufficiently limited domains, natural language interaction is possible even in the absence of actual natural language understanding. This is particularly true for goal-directed command and control, where the understanding task can essentially be cast as an Nway classification problem. (Data-driven) semantic inference is an approach to such tasks which in principle allows for unrestricted command/query formulation. It relies on a latent semantic analysis framework, whereby each unconstrained word string is automatically mapped onto the intended action through a semantic classification against the set of supported concepts. The objective of this paper is to compare this approach with other like-minded Nway classification methods, such as based on finite-state grammars or nearest-neighbor techniques. All experiments are conducted in the context of a desktop user interface control task involving 113 different actions. Results illustrate some of the performance and robustness benefits of semantic inference.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-282"
  },
  "wright02_icslp": {
   "authors": [
    [
     "Jerry",
     "Wright"
    ],
    [
     "Alicia",
     "Abella"
    ],
    [
     "Allen",
     "Gorin"
    ]
   ],
   "title": "Unified task knowledge for spoken language understanding and dialog management",
   "original": "i02_0821",
   "page_count": 4,
   "order": 283,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "Consider a conversational speech system incorporating spoken language understanding (SLU) and dialog manager (DM) modules. A prerequisite of natural dialog is that understanding takes place in context, which in turn necessitates a sharing of task knowledge between the two modules. However, because they are engaged in different activities it is often the case that the task knowledge representation is different for each of them, an undesirable duplication. In this paper we consider AT&Ts How May I Help You? natural dialog system. We present a method for automatically inducing the task knowledge representation used for spoken language understanding from that used for dialog management. Also, the context information needed by the SLU module for understanding each utterance is generated automatically by the DM from its own knowledge representation and the current dialog situation. This enables understanding-in-context to be implemented while avoiding the duplication involved in creating and maintaining separate task knowledge representations. The system has been evaluated using a database comprising 200k dialogs drawn from live customer traffic.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-283"
  },
  "lee02d_icslp": {
   "authors": [
    [
     "Yun-Tien",
     "Lee"
    ],
    [
     "Cheng-Huang",
     "Wu"
    ],
    [
     "Yumin",
     "Lee"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Distributed Chinese keyword spotting and verification for spoken dialogues under wireless environment",
   "original": "i02_0825",
   "page_count": 4,
   "order": 284,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "With the rapid developments of wireless communications, it is highly desired for users to access the network information with spoken dialogue interface via hand-held devices at any time, from anywhere. One possible approach towards this goal is to perform speech feature extraction at the hand-held devices (the clients) and have all other recognition tasks and dialogue functions absorbed by the server. This paper investigated distributed Chinese keyword spotting and verification under this scenario. A \"phonetically distributed\" Mandarin speech database including all possible Mandarin syllables and context relationships with frequencies roughly proportional to those occurring in daily Mandarin conversation is used to train a best set of vector quantization codebooks, such that the syllable recognition accuracy degradation due to quantization errors is minimized. Enhanced Chinese keyword spotting techniques were then developed using utterance verification approaches with weighting parameters optimized by MCE training. Experimental results indicated that the keyword verification approach achieved significant improvements in keyword spotting performance, and the overall results integrating vector quantization, keyword spotting and verification is quite satisfactory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-284"
  },
  "higashinaka02_icslp": {
   "authors": [
    [
     "Ryuichiro",
     "Higashinaka"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "A method for evaluating incremental utterance understanding in spoken dialogue systems",
   "original": "i02_0829",
   "page_count": 4,
   "order": 285,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "In single utterance understanding, which does not include discourse understanding, the concept error rate (CER), or the keyword error rate, has been widely used as an evaluation measure for utterance understanding. However, the CER cannot be used for evaluating systems that understand user utterances based on previous user utterances. In this paper, we propose a method for evaluating incremental utterance understanding, which involves speech recognition, language understanding and discourse processing in spoken dialogue systems, by finding a measure that correlates closely with the systems performance based on dialogue states and their way of update. We defined dialogue performance by task completion time, and performed a multiple linear regression analysis using task completion time as the explained variable and various metrics concerning dialogue states as explaining variables. The obtained multiple regression model fits comparatively well and shows validity as an evaluation measure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-285"
  },
  "kakutani02_icslp": {
   "authors": [
    [
     "Naoko",
     "Kakutani"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Detection and recognition of repaired speech on misrecognized utterances for speech input of car navigation system",
   "original": "i02_0833",
   "page_count": 4,
   "order": 286,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "Recently, car navigation systems with a speech interface have been developed. When we communicate with computers through speech recognition, one can not avoid misrecognition and it is difficult to recover from this condition because the interface is only in the initial state of the art. Detection of users repetition of a misrecognized part can make it easier.\n",
    "We propose a method to detect partial repetition of misrecognized speech using a word spotting technique based on DTW (dynamic time warping) and N-best hypotheses overlapping measure. We achieved 97.0% detection accuracy for repetitions and 93.5% rejection accuracy for non-repetitions. Next, we tried to improve recognition accuracy using detection. Using the choice of vocabulary setup based on the detection, we achieved improvement in recognition performance in adverse conditions with SNR of 7-13 dB from 29.0% to 35.1% for repaired speech and from 55.5% to 59.3% for non-repetitions. When we employed a rescoring strategy for real time processing, we achieved a 33.5% recognition rate for repaired speech and 57.4% for non-repetitions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-286"
  },
  "eklund02_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "Ingressive speech as an indication that humans are talking to humans (and not to machines)",
   "original": "i02_0837",
   "page_count": 4,
   "order": 287,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "Pulmonic ingressive speech is often mentioned anecdotally in the linguistic research. Most previous studies investigating the phenomenon have stressed the paralinguistic function of ingressive speech (IS). This paper studies IS in two corpora of spontaneous Swedish speech. Eight subjects made business travel bookings in two data collections. In one corpus the subjects talked with a real, human travel agent; in the other they spoke with what they believed was a computer, played by a professional actor. The results show that all subjects made use of IS in the human-human setting, while no one used IS in the human-machine setting. These results strengthen the notion that IS is a speech phenomenon that is truly associated with human interactions. The results are discussed from the perspective of possible underlying factors, including discourse structure, gender issues, and possible enhancements in automatic speech-based dialog systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-287"
  },
  "soltau02_icslp": {
   "authors": [
    [
     "Hagen",
     "Soltau"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Compensating for hyperarticulation by modeling articulatory properties",
   "original": "i02_0841",
   "page_count": 4,
   "order": 288,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "In spoken dialogue systems, hyperarticulation occur as an effect to recover previous recognition errors. It is commonly observed that users of automatic speech recognition systems apply similar recovery strategies as in human-human interactions. Previous studies have shown that current speech recognizers dont cover hyperarticulated speech well. As an effect of higher word error rates at hyperarticulated speech, humans try to reinforce this speaking style which results in even more recognition errors. In this study, we investigate the use of articulatory features to compensate hyperarticulated effects. The underlying idea is, that acoustic models for articulatory features are more robust against variations in the speaking style compared to pure phone models. We present a streaming architecture which integrates articulatory features in a standard HMM based system. Using this approach, we achieved an error reduction of 25.1% for hyperarticulated speech and even 8.9% for normal speech without any use of hyperarticulated training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-288"
  },
  "goubanova02_icslp": {
   "authors": [
    [
     "Olga V.",
     "Goubanova"
    ]
   ],
   "title": "Forms of introduction in map task dialogues: case of L2 Russian speakers",
   "original": "i02_0845",
   "page_count": 4,
   "order": 289,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "In the paper forms of introduction (question or non-question) of new information into a discourse by American English and L2 Russian speakers are studied. 48 Map Task dialogues between native and L2 speakers are analyzed. Forms of introduction are found to be influenced by speaker role, order of a map presentation, and native language of a speaker factors. The choice of a particular form of introduction is also influenced by whether some of the landmarks on instruction givers, followers, or both participants maps were lacking labels. Communicative success of a conversation is assessed as deviation scores of an instruction followers route from a givers route. Order of a map presentation is found to affect deviation scores. When a map was introduced for the first time, a route drawn by an instruction follower was less accurate than the one s/he drew for the second time. Furthermore, a route drawn by a L2 instruction follower was less accurate than the one reproduced by a native follower. In addition, factors influencing length of a conversation are studied. L2 speaker proficiency level is found to affect length of a conversation. Conversation length is also affected by difficulty of the task; when a map has some landmarks with missing labels (label altered condition), it took longer for the speakers to complete the task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-289"
  },
  "veilleux02_icslp": {
   "authors": [
    [
     "Nanette M.",
     "Veilleux"
    ]
   ],
   "title": "Bridges: regions between discourse segments",
   "original": "i02_0849",
   "page_count": 4,
   "order": 290,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "A spoken dialogue or discourse can be globally organized into coherent discourse segments in which local salience and coherence properties apply. This paper looks at the regions between these discourse segments in American English spontaneous speech dialogue. These bridge regions do not form a coherent discourse segment themselves. When fluent, their utterances belong to neither the previous nor subsequent discourse segment. Often the utterances are disfluent. Dialogue turns in these bridge regions are characteristically shorter, on average, both in duration and number of words. Moreover, there are fewer pronouns, on average, in bridge turns, compared to the entire dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-290"
  },
  "guillevic02_icslp": {
   "authors": [
    [
     "Didier",
     "Guillevic"
    ],
    [
     "Simona",
     "Gandrabur"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "Robust semantic confidence scoring",
   "original": "i02_0853",
   "page_count": 4,
   "order": 291,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "This paper describes an approach for defining robust, applicationindependent confidence measures for dialogue systems. A conceptlevel confidence score is computed using a Multi-Layer Perceptron (MLP) classifier trained to discriminate between correct and incorrect concepts. Three types of concept-level confidence features are considered: features based on the confidence score of the underlying words, parsing specific features, and novel semantic features (weighted semantic purity and time consistency) that are indicators of the coherence among various semantic recognition hypotheses. Confidence scores at the semantic hypothesis and utterance levels are derived from the confidence scores of the corresponding concepts. We report our results on a database of 40,000 utterances from various application contexts. By using features based only on word scores for concept classification we obtained a 46% correct rejection (CR) rate at a 95% correct acceptance (CA) rate. Adding semantic measures to the classifier boosted the CR rate to 71%, which corresponds to a 46.3% relative improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-291"
  },
  "muller02_icslp": {
   "authors": [
    [
     "Ludek",
     "Müller"
    ],
    [
     "Tomás",
     "Bartos"
    ]
   ],
   "title": "Statistically based approach to rejection of incorrectly recognized words",
   "original": "i02_0857",
   "page_count": 4,
   "order": 292,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "This paper wishes to contribute to the solution of the problem occurring when an automatic speech recognition module does not recognize an input utterance correctly and delivers wrong words to an understanding block and to a dialog manager, which consequently causes a false dialog system response to a user. The solution is based on an introduction of a recognition confidence measure, which evaluates the belief that the recognition result is accurate. In our previous paper [4] the confidence measure was based on a difference between so-called a mumble model [2,4] and a recognition network scores, and on a comparison of this score difference to a heuristically set threshold. In this paper we present a new statistically based technique which uses statistical models and Bayes decision rule. The paper also briefly describes the structure and behavior of the mumble model and its implementation in a real-time telephone dialog system. The main part of this article deals with a feature selection for classification and acceptance/rejection statistical model parameters estimation. The new rejection technique has been evaluated on the Czech yellow-pages database. Experimental results show that the proposed rejection technique achieves approximately 12% equal error rate (EER) in comparison with 20% EER of the previously one, which represents 63% relative EER improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-292"
  },
  "sato02b_icslp": {
   "authors": [
    [
     "Ryo",
     "Sato"
    ],
    [
     "Ryuichiro",
     "Higashinaka"
    ],
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Learning decision trees to determine turn-taking by spoken dialogue systems",
   "original": "i02_0861",
   "page_count": 4,
   "order": 293,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "This paper presents a method for deciding the timing of turn-taking in spoken dialogue systems. This method uses a decision tree learned from the corpus of dialogues between human users and systems in which desirable turn-taking behaviors are annotated by hand. It utilizes a variety of attributes, such as recognition and understanding results and prosodic information. Unlike most of the existing systems it enables spoken dialogue systems to decide the timing of turntaking based on not only pauses but also other features, so that users can speak to the system even if they put pauses in the middle of their utterances. The result of a preliminary experiment shows that the learned decision tree outperforms the baseline strategy, which takes turn at every user pauses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-293"
  },
  "hamimed02_icslp": {
   "authors": [
    [
     "H.",
     "Hamimed"
    ],
    [
     "G.",
     "Damnati"
    ]
   ],
   "title": "Integration of phonetic length properties in the acoustic models of false starts and out-of-vocabulary words",
   "original": "i02_0865",
   "page_count": 4,
   "order": 294,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "In this paper, we examine the idea of new information integration in the acoustic models of false starts and Out-Of-Vocabulary (OOV) words with the aim of reducing the global error rate. The method consists in reproducing partially the acoustic form of false starts and OOV words in the related rejection model. We present for the moment the integration of information about the phonetic length. We estimate from the training corpus the phonetic length distributions of these events and we use the associated probabilities to constraint the output of the rejection model. The obtained results were compared to those of a system based on a standard phonetic loop model. We observed a 4.7% absolute reduction in the global Word Error Rate (WER).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-294"
  },
  "zhao02c_icslp": {
   "authors": [
    [
     "Yibao",
     "Zhao"
    ],
    [
     "Guojun",
     "Zhou"
    ]
   ],
   "title": "N-word-sequence frequency noise mitigation for SLM based on binomial distribution",
   "original": "i02_0869",
   "page_count": 4,
   "order": 295,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "It is often difficult to build a robust Statistical Language Model (SLM) for a domain-specific spoken dialogue system because its very challenging to collect enough data for a specific domain. One solution is to build an SLM based on domain-specific grammar rules which do not need to collect a lot of data. A number of studies have found that this solution is effective and encouraging. However, the statistical information obtained from domain-specific grammar rules cant correctly represent the distribution of n-word-sequences in real applications, and thus resulting in the undesirable performance. It is observed that the n-word-sequence frequency-of-frequency distribution obtained from general-purpose corpus has a smooth curve, while the n-word-sequence frequency-of-frequency obtained from domain grammar rules does not. Based on the assumption that each n-word-sequence in real applications normally follows a binomial distribution, this paper proposes a pair of n-word-sequence frequency smoothing algorithms called Coast Algorithm and Tide Algorithm, which can significantly mitigate the \"noise\" presented in n-word-sequence frequency-of-frequency directly obtained from domain-specific grammar rules. Our experiments with a domainspecific spoken dialog system show that the SLM generated from domain-specific grammar rules but smoothed using the Coast and Tide algorithms can reduce the TER (Tag Error Rate) by 13.02% (relative). Therefore, these two algorithms can improve the system performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-295"
  },
  "lee02e_icslp": {
   "authors": [
    [
     "Chul Min",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Combining acoustic and language information for emotion recognition",
   "original": "i02_0873",
   "page_count": 4,
   "order": 296,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "This paper reports on emotion recognition using both acoustic and language information in spoken utterances. So far, most previous efforts have focused on emotion recognition using acoustic correlates although it is well known that language information also conveys emotions. For capturing emotional information at the language level, we introduce the information-theoretic notion of emotional salience. For acoustic information, linear discriminant classifiers and k-nearest neighborhood classifiers were used in the emotion classi- fication. The combination of acoustic and linguistic information is posed as a data fusion problem to obtain the combined decision. Results using spoken dialog data obtained from a telephone-based human-machine interaction application show that combining acoustic and language information improves negative emotion classification by 45.7% (linear discriminant classifier used for acoustic information) and 32.9%, respectively, over using only acoustic and language information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-296"
  },
  "hacioglu02_icslp": {
   "authors": [
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "A figure of merit for the analysis of spoken dialog systems",
   "original": "i02_0877",
   "page_count": 4,
   "order": 297,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "In this paper, a single metric, which we will call the figure of merit, for the quantitative analysis and comparison of spoken dialog systems is introduced. This figure of merit is the product of the weighted dialog accuracy (expressed as the rate of success) and the weighted dialog efficiency (expressed as the average number of concepts per turn). Actually, it is highly desirable to have a quick and accurate dialog. However, these two requirements are conflicting. That is, an improvement in efficiency is accomplished at the expense of accuracy or vice versa. This makes difficult to compare two different spoken dialog systems or tune a particular system. We believe that this figure of merit would avoid those difficulties. To illustrate its use, we consider spoken dialog systems with different dialog strategies and compare them by performing quantitative analysis based on the finite state models of information items using the proposed metric.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-297"
  },
  "akiba02_icslp": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Atsushi",
     "Fujii"
    ],
    [
     "Tetsuya",
     "Ishikawa"
    ]
   ],
   "title": "Selective back-off smoothing for incorporating grammatical constraints into the n-gram language model",
   "original": "i02_0881",
   "page_count": 4,
   "order": 298,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "Spoken queries submitted to question answering systems usually consist of query contents (e.g. about newspaper articles) and frozen patterns (e.g. WH-words), which can be modeled with N-gram models and grammar-based models, respectively. We propose a method to integrate those different types of models into a single N-gram model. We represent the two types of language models in a single word network. However, common smoothing methods, which are effective for N-gram models, decrease grammatical constraints for frozen patterns. For this problem, we propose a selective back-off smoothing method, which controls a degree to which smoothing is applied depending the network fragment. Additionally, resulting models are compatible with the conventional back-off N-gram models, and thus existing N-gram decoders can easily be used. We show the effectiveness of our method by way of experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-298"
  },
  "zitouni02b_icslp": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Backoff hierarchical class n-gram language modelling for automatic speech recognition systems",
   "original": "i02_0885",
   "page_count": 4,
   "order": 299,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "In this paper, we propose an extension of the backoff word n-gram language model that allows a better likelihood estimation of unseen events. Instead of using the (n-1)-gram to estimate the probability of an unseen n-gram, the proposed approach uses a class hierarchy to define a context which is more general than the unseen n-gram but more specific than the (n-1)-gram. Each node in the hierarchy is a class containing all the words of the descendant nodes (classes). Hence, the closer a node is to the root, the more general the corresponding class is. Performance is evaluated both in terms of test perplexity and word error rate (WER) on a simplified WSJ database. Experiments show an improvement of more than 26% on the unseen events perplexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-299"
  },
  "picard02_icslp": {
   "authors": [
    [
     "Francis",
     "Picard"
    ],
    [
     "Dominique",
     "Boucher"
    ],
    [
     "Guy",
     "Lapalme"
    ]
   ],
   "title": "Constructing small language models from grammars",
   "original": "i02_0889",
   "page_count": 4,
   "order": 300,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "This paper presents a method for constructing small word graphs from regular grammars in a way to reduce the number of vertices in the resulting graph. Our method works at the grammar level instead of intermediate forms like finite automata. It represents a prime alternative to exact minimization algorithms, and is distinguished by its simplicity, its flexibility and by the fact that it avoids the determinization of the resulting graph or automaton.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-300"
  },
  "zhang02c_icslp": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Improve latent semantic analysis based language model by integrating multiple level knowledge",
   "original": "i02_0893",
   "page_count": 4,
   "order": 301,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "We describe an extension to the use of Latent Semantic Analysis (LSA) for language modeling. This technique makes it easier to exploit long distance relationships in natural language for which the traditional n-gram is unsuited. However, with the growth of length, the semantic representation of the history may be contaminated by irrelevant information, increasing the uncertainty in predicting the next word. To address this problem, we propose a multilevel framework dividing the history into three levels corresponding to document, paragraph and sentence. To combine the three levels of information with the n-gram, a Softmax network is used. We further present a statistical scheme that dynamically determines the unit scope in the generalization stage. The combination of all the techniques leads to a 14% perplexity reduction on a subset of Wall Street Journal, compared with the trigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-301"
  },
  "siciliagarcia02_icslp": {
   "authors": [
    [
     "Elvira I.",
     "Sicilia-Garcia"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "Individual word language models and the frequency approach",
   "original": "i02_0897",
   "page_count": 4,
   "order": 302,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "We present a new method of introducing domain knowledge into an n-gram language model. It is based on a combination of language models for individual word domains. Each word model is built from an individual corpus which is formed by extracting those subsets of the entire training corpus which contain that significant word. When testing, significant words are extracted from a cache and their models are combined with a global language model. Different methods of combining the models are described; one new simple method based on combining frequencies rather than probabilities gives promising results and provides a relatively simple method of introducing domain information into an n-gram language model. A 32% reduction in language model perplexity over the standard 3-gram approach is obtained which is similar to results obtained with other more complex domain models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-302"
  },
  "stolcke02_icslp": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "SRILM - an extensible language modeling toolkit",
   "original": "i02_0901",
   "page_count": 4,
   "order": 303,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-303"
  },
  "whittaker02_icslp": {
   "authors": [
    [
     "E. W. D.",
     "Whittaker"
    ],
    [
     "D.",
     "Klakow"
    ]
   ],
   "title": "Efficient construction of long-range language models using log-linear interpolation",
   "original": "i02_0905",
   "page_count": 4,
   "order": 304,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "In this paper we examine the construction of long-range language models using log-linear interpolation and how this can be achieved effectively. Particular attention is paid to the efficient computation of the normalisation in the models. Using the Penn Treebank for experiments we argue that the perplexity performance demonstrated recently in the literature using grammar-based approaches can actually be achieved with an appropriately smoothed 4-gram language model. Using such a model as the baseline, we demonstrate how further improvements can be obtained using log-linear interpolation to combine distance word and class models. We also examine the performance of similar model combinations for rescoring word lattices on a medium-sized vocabulary Wall Street Journal task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-304"
  },
  "corazza02_icslp": {
   "authors": [
    [
     "Anna",
     "Corazza"
    ]
   ],
   "title": "Integration of two stochastic context-free grammars",
   "original": "i02_0909",
   "page_count": 4,
   "order": 305,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "Some problems in speech and natural language processing involve combining two information sources each modeled by a stochastic context-free grammar. Such cases include parsing the output of a speech recognizer by using a context-free language model, finding the best solution among all the possible ones in language generation, preserving ambiguity in machine translation. In these cases usually at least one of the two grammars is nonrecursive. In order to find the best solution while taking into account both grammars, the two probabilities must be integrated. One of the most important advantages of using a non-recursive context-free model is its compactness. Therefore, it is important to exploit this property when searching for the solution. In this paper, an algorithm aiming to this goal is presented, based on a recent work [1] in which the non probabilistic case is considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-305"
  },
  "rayner02_icslp": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Beth Ann",
     "Hockey"
    ],
    [
     "John",
     "Dowding"
    ]
   ],
   "title": "Grammar specialisation meets language modelling",
   "original": "i02_0913",
   "page_count": 4,
   "order": 306,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "CFG-based language models have become popular over the last few years, especially for commercial applications, and there is growing interest in creating complex CFG-based models for mixed initiative systems. On general grounds, it is attractive to attempt to compile these models from domain-independent descriptions written in high-level formalisms such as unification grammar. Experience to date however suggests that compilation from complex unification grammars to CFG has poor scalability properties. We argue that it is possible to attack this problem by first specialising the domain-independent grammar against a corpus using Explanation Based Learning. We describe experiments carried out on a medium vocabulary command and control task, which suggest that language models derived from specialised grammars have much better scalability properties, and also deliver significantly improved run-time performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-306"
  },
  "huang02b_icslp": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Maximum entropy model for punctuation annotation from speech",
   "original": "i02_0917",
   "page_count": 4,
   "order": 307,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "In this paper we develop a maximum-entropy based method for annotating spontaneous conversational speech with punctuation. The goal of this task is to make automatic transcriptions more readable by humans, and to render them into a form that is useful for subsequent natural language processing and discourse analysis. Our basic approach is to view the insertion of punctuation as a form of tagging, in which words are tagged with appropriate punctuation, and to apply a maximum entropy tagger that uses both lexical and prosodic features. We present experimental results on Switchboard data with both reference transcriptions and transcriptions produced by a speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-307"
  },
  "mori02b_icslp": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ]
   ],
   "title": "An automatic sentence boundary detector based on a structured language model",
   "original": "i02_0921",
   "page_count": 4,
   "order": 308,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "In this paper we describe an automatic sentence boundary detector, which inserts a period (sentence boundary marker) to a word sequence output by a speech recognizer. The state-of-the-art automatic sentence boundary detectors insert a period at a position selected by a word tri-gram model from among candidates (long pauses) offered by an acoustic model. In contrast, the automatic sentence boundary detector presented in this paper is based on a structured language model (SLM), which regards a sentence as a word sequence with a syntactic structure. In the experiment we applied our automatic sentence boundary detector to Japanese broadcast lectures and compared the result with an automatic sentence boundary detector based on a word tri-gram model. The accuracy of our detector was 95.7%, which was higher than that for the state-of-the-art detector (95.2%). This result shows that an SLM works better than a word tri-gram model as an automatic sentence boundary detector.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-308"
  },
  "wu02d_icslp": {
   "authors": [
    [
     "Genqing",
     "Wu"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Ling",
     "Jin"
    ]
   ],
   "title": "Improved katz smoothing for language modeling in speech recogniton",
   "original": "i02_0925",
   "page_count": 4,
   "order": 309,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "In this paper, a new method is proposed to improve the canonical Katz back-off smoothing technique in language modeling. The process of Katz smoothing is detailedly analyzed and the global discounting parameters are selected for discounting. Further more, a modified version of the formula for discounting parameters is proposed, in which the discounting parameters are determined by not only the occurring counts of the n-gram units but also the low-order history frequencies. This modification makes the smoothing more reasonable for those n-gram units that have homophonic (same in pronunciation) histories. The new method is tested on a Chinese Pinyin-to-character (where Pinyin is the pronunciation string) conversion system and the results show that the improved method can achieve a surprising reduction both in perplexity and Chinese character error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-309"
  },
  "mori02c_icslp": {
   "authors": [
    [
     "Renato De",
     "Mori"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Christian",
     "Raymond"
    ]
   ],
   "title": "On the use of structures in language models for dialogue",
   "original": "i02_0929",
   "page_count": 4,
   "order": 310,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "The paper describes the combined use of three new language modelling paradigms. They are: generation of plausible trigrams by analogy, explanation-based generation of error-correcting automata, and disambiguation using Semantic Classification Trees. Tangible word error rate reduction is observed by the combined use of these paradigms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-310"
  },
  "erdogan02_icslp": {
   "authors": [
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Semantic structured language models",
   "original": "i02_0933",
   "page_count": 4,
   "order": 311,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "In this study, we propose two novel semantic language modeling techniques for spoken dialog systems. These methods are called semantic concept based language modeling and semantic structured language modeling. In the concept based language modeling, we propose to use long span semantic units to model meaning sequences in spoken utterances. In the latter technique, we use statistical semantic parsers to extract information from a sentence. This information is then utilized in a maximum entropy based language model. The language models are trained and evaluated in the air travel reservation domain. We obtain improvement over a sophisticated class based N-gram language model both in terms of recognition accuracy and perplexity. Interpolation of the proposed techniques with the class-based N-gram LM provides additional improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-311"
  },
  "hirose02_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Makoto",
     "Terao"
    ]
   ],
   "title": "Statistical language modeling with prosodic boundaries and its use for continuous speech recognition",
   "original": "i02_0937",
   "page_count": 4,
   "order": 312,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "A new statistical language modeling was proposed where word ngram was counted separately for the cases crossing and not crossing accent phrase boundaries. Since such counting requires a large speech corpus, which hardly can be prepared, part-of-speech (POS) n-gram was first counted for a small-sized speech corpus for the two cases instead, and then the result is applied to word n-gram counts of a large text corpus to divide them accordingly. Thus, the two types of word n-gram model can be obtained. Using ATR continuous speech corpus by two speakers, perplexity reduction from the baseline model to the proposed model was calculated for the word bi-gram. When accent phrase boundary information of the speech corpus was used, the reduction reached 11%, and when boundaries were extracted using our formerly developed method based on mora- F0 transition modeling, it still exceeded 8%. The reduction around 5% was still observed for sentences not included for the calculation of POS bi-gram and using boundaries automatically extracted from another speakers speech. The obtained bigram was applied to continuous speech recognition, resulted in a two-percentage improvement of word accuracy from when the baseline model was used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-312"
  },
  "iwano02_icslp": {
   "authors": [
    [
     "Koji",
     "Iwano"
    ],
    [
     "Takahiro",
     "Seki"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Noise robust speech recognition using F0 contour extracted by hough transform",
   "original": "i02_0941",
   "page_count": 4,
   "order": 313,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "This paper proposes a noise robust speech recognition method using prosodic information. In Japanese, fundamental frequency (F0) contour represents phrase intonation and word accent information. Consequently, it conveys information about prosodic phrase and word boundaries. This paper first proposes a noise robust F0 extraction method using Hough transform, which achieves high extraction rates under various noise environments. Then it proposes a robust speech recognition method using syllable HMMs which model both segmental spectral features and F0 contours. Speaker-independent experiments are conducted using connected digits uttered by 11 male speakers in various kinds of noise and SNR conditions. The recognition accuracy is improved in all noise conditions, and the best absolute improvement of digit accuracy is about 4.7%. This improvement is achieved due to the more precise digit boundary detection by the robust prosodic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-313"
  },
  "almasganj02_icslp": {
   "authors": [
    [
     "Farshad",
     "Almasganj"
    ],
    [
     "Farhad D.",
     "Dehnavi"
    ],
    [
     "Mahmood",
     "Bijankhan"
    ]
   ],
   "title": "Sharing relative stress of cross-word syllables and lexical stress to spontaneous speech recognition",
   "original": "i02_0945",
   "page_count": 4,
   "order": 314,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "Prosody is a suprasegmental feature of speech that has an undeniable role in human speech perception and generation. However, employing of prosodic features in CSR process mostly is difficult and we must not expect huge accuracy progress by using them. In this way, the main problem arises from high dependency of prosodic patterns to factors like speakers, psychological state of speakers and superposition effects of higher-level prosodic patterns on lower level of them. In our approach, the selected microprosodic feature case is the lexical word stress pattern and relative stresses of crossword syllables. We aim to verify if we succeed to present proper models for the prosodic feature recognition purpose, we can use them to modify speech recognition process. We employed a proper neural network approach to the word and cross-word stress recognition task. Then we incorporated these features into a spontaneous Farsi speech recognition system called SHENAVA-1. We found 1.3% better word accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-314"
  },
  "baron02_icslp": {
   "authors": [
    [
     "Don",
     "Baron"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Automatic punctuation and disfluency detection in multi-party meetings using prosodic and lexical cues",
   "original": "i02_0949",
   "page_count": 4,
   "order": 315,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "We investigate automatic approaches to finding \"hidden\" spontaneous speech events, such as sentence boundaries and disfluencies, in multi-party meetings. Hidden events are characterized prosodically by a large array of automatically extracted energy, duration, and pitch features, and are modeled by decision tree classifiers; lexical cues are modeled by N-gram language models. Both sources of information are combined in a hidden Markov model framework. Results show that combined classifiers achieve higher accuracy than either single knowledge source alone. We also study classifiers that use only the preceding context for predicting events, simulating online processing. We find that prosodic features are more robust than are language model features to this constraint. Finally, we examine the effect of automatic word recognition errors, in both training and testing, on classification accuracy. We find that lexical models degrade much more severely than do prosodic models in this case, again showing the relative robustness of prosodic information for hidden-event detection in natural conversation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-315"
  },
  "sun02_icslp": {
   "authors": [
    [
     "Xuejing",
     "Sun"
    ]
   ],
   "title": "Pitch accent prediction using ensemble machine learning",
   "original": "i02_0953",
   "page_count": 4,
   "order": 316,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "In this study, we applied ensemble machine learning to predict pitch accents. With decision tree as the baseline algorithm, two popular ensemble learning methods, bagging and boosting, were evaluated across different experiment conditions: using acoustic features only, using text-based features only; using both acoustic and text-based features. F0 related acoustic features are derived from underlying pitch targets. Models of four ToBI pitch accent types (High, Downstepped high, Low, and Unaccented) are built at the syllable level. Results showed that in all experiments improved performance was achieved by ensemble learning. The best result was obtained in the third task, in which the overall correct rate increases from 84.26% to 87.17%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-316"
  },
  "escuderomancebo02_icslp": {
   "authors": [
    [
     "D.",
     "Escudero-Mancebo"
    ],
    [
     "C.",
     "González-Ferreras"
    ],
    [
     "V.",
     "Cardeñoso-Payo"
    ]
   ],
   "title": "Quantitative evaluation of relevant prosodic factors for text-to-speech synthesis in Spanish",
   "original": "i02_1165",
   "page_count": 4,
   "order": 317,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "A quantitative comparison of four different proposals for intonation modeling in Spanish is presented. In the framework of a modeling procedure previously introduced by the authors, the stress group is taken as the basic building block and a statistical model is inferred from a corpus for every kind of intonation unit, which is parameterized by means of the four control points of the fitting Bézier function. Applying classical clustering quality assessment metrics to the statistical models predicted under different proposals, an objective comparison is brought among them. From the results, a set of prosodic factors has been taken as the characterization of the stress group and incorporated into a TTS platform, with a reported increase in perceptual and objective quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-317"
  },
  "thubthong02_icslp": {
   "authors": [
    [
     "Nuttakorn",
     "Thubthong"
    ],
    [
     "Boonserm",
     "Kijsirikul"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ]
   ],
   "title": "Tone recognition in Thai continuous speech based on coarticulaion, intonation and stress effects",
   "original": "i02_1169",
   "page_count": 4,
   "order": 318,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "Tone recognition is a critical component for speech recognition in a tone language. One of the main problems of tone recognition in continuous speech is that several interacting factors affect F0 realization of tones. In this paper, we focus on the coarticulatory, intonation, and stress effects. These effects are compensated by the tone information of neighboring syllables, the adjustment of F0 heights and the stress acoustic features, respectively. The experiments, which compare all tone features, were conducted by feedforward neural networks. The highest recognition rates are improved from 84.07% to 93.60% and 82.48% to 92.67% for Thai proper name and Thai animal story corpora, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-318"
  },
  "takagi02_icslp": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Hajime",
     "Kubota"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Combination of pause and F0 information in dependency analysis of Japanese sentences",
   "original": "i02_1173",
   "page_count": 4,
   "order": 319,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "This paper focuses on syntactic information contained in prosodic features extracted from read Japanese sentences, and describes a method of exploiting it in dependency structure analysis. The basic idea is to make a statistical model of prosodic feature distribution for each dependency distance. Then, by using the Bayes theorem, the dependency distance of each phrase is predicted from a given feature value. A multi-dimensional feature of F0 was effective to improve parsing accuracy, which was sampled from the parabola fitted to the log-F0 contour. It was also shown that the performance was improved more by linearly combining post-phrase pause duration information with the F0 information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-319"
  },
  "horiuchi02_icslp": {
   "authors": [
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Tomoko",
     "Ohsuga"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Estimating syntactic structure from F0 contour and pause duration in Japanese speech",
   "original": "i02_1177",
   "page_count": 4,
   "order": 320,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "In this study, we introduce a method for estimating the syntactic structure of Japanese speech from F0 contour and pause duration. We defined a prosodic unit (PU) which is bound by a local minimum point of an F0 contour pattern or pause. Combining PUs repeatedly (a pair of PUs is combined into one PU), a tree structure is gradually generated. Which pair of PUs in a sequence of three PUs should be combined is decided by the discriminant function based on the discriminant analysis of many speech data. We applied the method to the ATR 503 Phonetically Balanced Sentences read by four Japanese speakers. As a result, the correct rate of judgement for each sequence of three PUs is 79% and the estimation accuracy of the entire syntactic structure for each sentence is 26%. We consider this result to be fairly good for the difficult task of estimating a syntactic structure only from prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-320"
  },
  "yamashita02_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Akira",
     "Inoue"
    ]
   ],
   "title": "Extraction of important sentences using F0 information for speech summarization",
   "original": "i02_1181",
   "page_count": 4,
   "order": 321,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "This paper describes speech summarization using F0 information. The speech summarization in this work is realized by the extraction of important sentences from text data transcribed by hand. The important problem in this framework is automatic scoring of sentence importance based on prosodic information from speech wave as well as linguistic information from written text. Prosody conveys nonlinguistic information such as speakers intention and contributes to identify important parts of speech. The prosodic information is represented in terms of F0 parameters of Japanese bunsetsu unit, which is almost equivalent to a prosodic minor phrase. Six kinds of F0 parameters are compared in regard to correlation to the sentence importance and performance of extracting important sentences. Evaluation results show that introduction of F0 parameters is effective to the speech summarization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-321"
  },
  "kitamura02_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Kayo",
     "Itoh"
    ],
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Shigeyoshi",
     "Kitazawa"
    ]
   ],
   "title": "Influence of prosody, context, and word order in the identification of focus in Japanese dialogue",
   "original": "i02_1185",
   "page_count": 4,
   "order": 322,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "This paper studies the influence of prosodic features, context, and word order on the identification of focused clauses in Japanese dialogue, using a psychoacoustic experiment. In the experiment, question and answer speech was used as stimuli. The questions were to create two different contexts in the stimuli, and the answers had focal prominence at different clauses and had different word orders. The experimental results indicate that (1) prosodic characteristics are more significant for focus identification, (2) context has some effect on identification, and (3) it is probable that the word order has some effect on identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-322"
  },
  "kai02_icslp": {
   "authors": [
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Yukari",
     "Nonomura"
    ],
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Tatsuhiro",
     "Konishi"
    ],
    [
     "Yukihiro",
     "Itoh"
    ]
   ],
   "title": "Influence of different dialogue situations on user²s behavior in spoken corrections",
   "original": "i02_1189",
   "page_count": 4,
   "order": 323,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "This study analyzes the acoustic-prosodic features observed in spoken corrections in a task-oriented spoken dialogue. While previous studies have been found that spoken corrections are often predicted as hyperarticulate speech and related to some acoustic-prosodic events, their close analysis were not shown in terms of various dialogue situations in practical spoken dialogue systems and tasks. In this study, a set of spoken dialogue data was collected through both a fully operational spoken dialogue system and a human-human dialogue task with the same task setup. In addition, the influence of a concurrent task of speaking and car-driving on the prosodic features is investigated. The results of statistical analysis and a preliminary experiment of automatic classification showed that the characteristics of the prosodic features differ in the different dialogue situations while some finding are consistent with the observation in the previous studies in human-human and human-machine interactions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-323"
  },
  "yang02_icslp": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Interpreting meaning from context: modeling the prosody of discourse markers in speech",
   "original": "i02_1193",
   "page_count": 4,
   "order": 324,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "In this paper we show how text, context and prosody of discourse markers reflect cognitive and discourse phenomena of uncertainty and certainty, intensity of emotional response, and interactive signals of knowledge state. We demonstrate how the subtle and finely differentiated meanings permeating spontaneous speech are communicated by prosodic variations and that it is the differences in shape that communicate the degree of uncertainty or certainty with respect to the speakers knowledge state, and cognitive status. We then propose a tripartite model of prosody and discourse markers to account for the contextual determination of discourse maker interpretation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-324"
  },
  "bartkova02_icslp": {
   "authors": [
    [
     "Katarina",
     "Bartkova"
    ],
    [
     "David Le",
     "Gac"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Prosodic parameter for speaker identification",
   "original": "i02_1197",
   "page_count": 4,
   "order": 325,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "This study investigates to what extend prosodic parameters are speaker dependent and, therefore, can be used in a speaker identification system. Fundamental frequency, phone duration and phone energy are investigated and modeled under different forms and their effi- ciency to identify successfully the speaker, to whom the model belongs, is evaluated. The speech material used to model and evaluate the prosodic parameters is collected from 28 speakers and consists of free spontaneous speech. For 61% of the speakers one or several very efficient prosodic cues are found yielding an average ranking lower, thus better, than the 3rd best position. For 36% of the speakers the average ranking of the right speaker is between the 3rd and the 5th position. Only one speaker out of 28 is not satisfactorily represented by any of the prosodic parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-325"
  },
  "shigeyoshi02_icslp": {
   "authors": [
    [
     "Kitazawa",
     "Shigeyoshi"
    ],
    [
     "Itoh",
     "Toshihiko"
    ],
    [
     "Kitamura",
     "Tatsuya"
    ]
   ],
   "title": "Juncture segmentation of Japanese prosodic unit based on the spectrographic features",
   "original": "i02_1201",
   "page_count": 4,
   "order": 326,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "A very detailed segmentation of prosodic phrase was carried out in order to construct a Japanese prosodic database. Boundaries correspond to junctures between phrases including C|C and V|V clusters. The \"prosodic phrase\" we introduced as a unit of the segmentation was defined and regarded as a unit of language speech perception. For the exact segmentation, the wide-band spectrum, the narrow-band spectrum, fine speech wave and fundamental frequency shapes and transition of amplitude of the higher order formants were adopted to enumerate the candidate points for the segment boundary. Fine time adjustment by the steps of the respective fundamental period of the speech determined the exact boundary. To maintain the consistency of the segmentation, one person ascertained the entire segment carefully. The database, referred to here as \"Japanese Multext\", contains read style speech and spontaneous style speech by three male speakers and three female speakers in Tokyo dialect.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-326"
  },
  "svec02_icslp": {
   "authors": [
    [
     "Jan G.",
     "Svec"
    ],
    [
     "Frantisek",
     "Sram"
    ]
   ],
   "title": "Kymographic imaging of the vocal fold oscillations",
   "original": "i02_0957",
   "page_count": 4,
   "order": 327,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "Kymographic imaging represents a method of viewing vocal fold oscillations, which is alternative to the routinely used frame-by-frame playback of the video recordings. Instead of video images of the whole vocal folds, in kymography images at only a single line are recorded. The successively recorded line-images are put together to create the resulting kymographic image. This image displays vibratory behavior at the selected part of the vocal folds. The paper briefly reviews four kymographic methods: photokymography, videokymography, videostrobokymography and high-speed digital kymography; and demonstrates the relationship between the kymographic vibratory pattern and the vibratory pattern of the vocal folds known from traditional laryngoscopy. Factors such as the selected position of the active line and the intended type of voice are pointed out to be important in evaluation of the vocal-fold vibratory pattern displayed in kymographic images. Applications in which kymographic imaging has been found helpful are, for instance, evaluation of the microdynamics of the glottal oscillatory cycles, glottal onset and offset, glottal tremor, abduction and adduction speed or measurement of resonance properties of the laryngeal tissues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-327"
  },
  "mady02_icslp": {
   "authors": [
    [
     "K.",
     "Mády"
    ],
    [
     "R.",
     "Sader"
    ],
    [
     "A.",
     "Zimmermann"
    ],
    [
     "P.",
     "Hoole"
    ],
    [
     "A.",
     "Beer"
    ],
    [
     "H.-F.",
     "Zeilhofer"
    ],
    [
     "Ch.",
     "Hannig"
    ]
   ],
   "title": "Assessment of consonant articulation in glossectomee speech by dynamic MRI",
   "original": "i02_0961",
   "page_count": 4,
   "order": 328,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "For the speech ability assessment of glossectomee patients, dynamic MRT was used in order to obtain real time imaging of articulatory movements. At the present state, the method is best suited for continuous consonants, but it can be used for the evaluation of any speech sound. In this study, pre- and postoperative consonant articulation of eight patients with cancer of the oral cavity is compered by means of acoustic analysis and real-time MRI.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-328"
  },
  "wrench02_icslp": {
   "authors": [
    [
     "Alan",
     "Wrench"
    ],
    [
     "Fiona",
     "Gibbon"
    ],
    [
     "Alison M.",
     "McNeill"
    ],
    [
     "Sara",
     "Wood"
    ]
   ],
   "title": "An EPG therapy protocol for remediation and assessment of articulation disorders",
   "original": "i02_0965",
   "page_count": 4,
   "order": 329,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "This paper describes technical and methodological advances in the development of a procedure for measuring changes in accuracy and stability of linguapalatal (tongue-palate) contact patterns during a course of visual feedback therapy using electropalatography (EPG). The procedure is exemplified by a case in which therapy was aimed at resolving a pattern of velar fronting whereby phonetic targets /k, g, 6/ had abnormal alveolar placement [t, d, n]. The EPG remediation and assessment procedure can be implemented using recording, feedback and analysis software designed for the purpose.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-329"
  },
  "patel02_icslp": {
   "authors": [
    [
     "Rupal",
     "Patel"
    ]
   ],
   "title": "How speakers with and without speech impairment mark the question statement contrast",
   "original": "i02_0969",
   "page_count": 4,
   "order": 330,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "We sought to better understand how normal speakers signal the yesno question-statement contrast and whether speakers with severe speech impairment could signal the contrast and how they would do so. We asked a group of eight speakers with severe dysarthria to produce ten, three-syllable phrases in an interrogative tone and in a declarative tone. We asked a group of gender matched normal speakers to perform the same task. We then performed a set of acoustic analyses to identify, which features speakers used, and how these features differed between speaker groups. Our findings indicate that both sets of speakers use multiple prosodic cues to signal the question statement contrast of which F0 is only one cue. Other cues include loudness and syllable duration. We also found that speakers with dysarthria often exaggerated salient prosodic cues and occasionally used alternative cues over which they had more precise control.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-330"
  },
  "zahorian02_icslp": {
   "authors": [
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "A. Matthew",
     "Zimmer"
    ],
    [
     "Fansheng",
     "Meng"
    ]
   ],
   "title": "Vowel classification for computer-based visual feedback for speech training for the hearing impaired",
   "original": "i02_0973",
   "page_count": 4,
   "order": 331,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "A visual speech training aid for persons with hearing impairments has been developed using a Windows-based multimedia computer. The training aid provides real time visual feedback as to the quality of pronunciation for 10 steady-state American English monopthong vowels (/aa/, /iy/, /uw/, /ae/, /er/, /ih/, /eh/, /ao/, /ah/, and /uh/). This training aid is thus referred to as a Vowel Articulation Training Aid (VATA). Neural network (NN) classifiers are used to classify vowels and then provide real time feedback for several displays: a 10-category \"vowel bargraph\" which provides \"discrete\" feedback, an \"ellipse display\" which provides continuous feedback over a 2-D space similar to a formant1-formant2 space, and three game displays (a form of \"tetrus\", controlled by one vowel, a \"chicken crossing the road\", controlled by two vowels, and pacman, controlled by four vowels). Continuous feedback such as this is desirable for speech training to help improve articulation. In this paper we describe the overall speech training system, discuss some algorithmic refinements to the vowel classifier, and report some experiments related to the development of a database used for \"training\" the display.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-331"
  },
  "alku02_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "All-pole modeling of wide-band speech using weighted sum of the LSP polynomials",
   "original": "i02_0977",
   "page_count": 4,
   "order": 332,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "The autocorrelation function of the all-pole filter given by the conventional linear prediction (LP) matches exactly the autocorrelation function of the input signal between indices 0 and m, when the prediction order equals m. This study describes a recently developed technique, Weighted-Sum Line Spectrum Pair (WLSP), where an allpole filter is defined by using a sum of weighted LSP (Line Spectrum Pair) polynomials. WLSP yields a stable all-pole filter of order m, whose autocorrelation function coincides to that of the input between indices 0 and m- 1. By sacrificing the exact matching of the autocorrelations at indexm, WLSP models the autocorrelation of the input at the indices above m more accurately than conventional LP. In the current paper, the performance of WLSP in spectral modelling of wide-band speech is analysed. It is shown that WLSP yields all-pole spectra that model the formant structure of vowels more accurately than conventional LP of the same prediction order.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-332"
  },
  "schoentgen02_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Analysis and synthesis of the phonatory excitation signal by means of a pair of polynomial shaping functions",
   "original": "i02_0981",
   "page_count": 4,
   "order": 333,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "The object of the presentation is a polynomial waveshaping model of the phonatory excitation signal. The phonatory excitation signal is the acoustic signal that is generated at the glottis via the vibrating vocal folds and the pulsatile airflow. A waveshaper is an operator that transforms a sinusoid into any desired waveform. The waveshaper is approximated by means of a pair of polynomials whose coefficients are obtained via a constant linear transformation of the Fourier series coefficients of the shape of a cycle of the desired phonatory signal. The properties of the waveshaping model compared to conventional models of the phonatory signal are the following. (i) The model can be linearly fitted to observed glottal waveforms. (ii) The mean value of the model output is zero and the acoustic excitation is zero when the glottis is closed. (iii) The cycle length is controlled via the instantaneous frequency of the driving sinusoid. (iv) The spectral balance of the output is controlled by the amplitude of the driving sinusoid. (v) The waveshapers coefficients fall into two categories, i.e. parameters that enable the control of the features of the phonatory timbre on the one hand, and coefficients that encode the default cycle shape (phonatory identity) on the other.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-333"
  },
  "vintsiuk02_icslp": {
   "authors": [
    [
     "Taras K.",
     "Vintsiuk"
    ]
   ],
   "title": "Optimal speech signal partition into one-quasiperiodical segments",
   "original": "i02_0985",
   "page_count": 4,
   "order": 334,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "Quasi-periodicity and non-periodicity signal models are proposed. Each hypothetical one-quasiperiodical signal segment is considered as a random distortion of previous or following one taken with the unknown multiplying factor. The problem of optimal current pitch period discrimination and speech signal partition into one-quasiperiodical microsegments consists in the finding of the best onequasiperiod beginnings or the one-quasiperiodical segments under restrictions on both value and changing of current quasiperiod duration and multiplying factor. For this problem solving an effective algorithm based on dynamic programming is proposed and its application for speech signal analysis, recognition and synthesis is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-334"
  },
  "rufiner02_icslp": {
   "authors": [
    [
     "Hugo L.",
     "Rufiner"
    ],
    [
     "Luis F.",
     "Rocha"
    ],
    [
     "John Goddard",
     "Close"
    ]
   ],
   "title": "Sparse and independent representations of speech signals based on parametric models",
   "original": "i02_0989",
   "page_count": 4,
   "order": 335,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "Recently methods for obtaining sparse representations of a signal using overcomplete dictionaries of waveforms have been studied, often motivated by the way the brain seems to process certain sensory signals. Algorithms have been developed using either a specific criterion to choose the waveforms occurring in the representation from a fixed dictionary, or to construct them as part of the method. In the case of speech signals, most approaches do not take into consideration the important temporal correlations that exist; these are known to be well approximated using linear models. Incorporating this type of a priori knowledge of the signal can facilitate the search for a suitable solution and also help with the interpretation of the representation found. In the present paper a method is proposed for obtaining a sparse representation using a generative parametric model. An example, using speech signals, is given reporting the methods efficacy for different coding costs and sparsity measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-335"
  },
  "funaki02_icslp": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ]
   ],
   "title": "Improvement of the ELS-based time-varying complex speech analysis",
   "original": "i02_0993",
   "page_count": 4,
   "order": 336,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "We have already proposed novel robust parameter estimation algorithms of time-varying complex AR (TV-CAR) model for analytic speech signal, which are based on GLS (Generalized Least Square) and its modification of ELS (Extended Least Square). The ELS method is more sophisticated method that can be derived by matrix inversion theorem. We have shown that the methods can achieve robust speech spectrum estimation against additive white Gaussian. However, these methods suffer from spectral gaps between adjacent analysis frames. This paper proposes improved ELS-based TV-CAR speech analysis using forward and backward linear prediction. The experiment with natural speech demonstrates that the improved method can estimate more smooth time-varying speech spectrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-336"
  },
  "chin02_icslp": {
   "authors": [
    [
     "K. K.",
     "Chin"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Maximum mutual information training of hidden Markov models with vector linear predictors",
   "original": "i02_0997",
   "page_count": 4,
   "order": 337,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "HMM makes a piece-wise constant assumption about the temporal evolution of the speech signal. This is not true for speech signals which are known to be highly temporally correlated. Many approaches had been proposed to overcome the limitation of HMMs in modelling temporal context. One of these approaches uses a Vector Linear Predictor (VLP) to model the relationship between a nearby frame and the current frame. In this paper, Maximum Mutual Information Estimation (MMIE) of VLP-HMMs is explored. The MMIE training of VLP-HMMs are evaluated on WSJ data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-337"
  },
  "hamaker02_icslp": {
   "authors": [
    [
     "J. E.",
     "Hamaker"
    ],
    [
     "J.",
     "Picone"
    ],
    [
     "A.",
     "Ganapathiraju"
    ]
   ],
   "title": "A sparse modeling approach to speech recognition based on relevance vector machines",
   "original": "i02_1001",
   "page_count": 4,
   "order": 338,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "In this paper, we compare two powerful kernel-based learning machines, support vector machines (SVM) and relevance vector machines (RVM), within the framework of hidden Markov model-based speech recognition. Both machines provide nonlinear discriminative classification ability: the SVM by kernel-based margin maximization and the RVM using a Bayesian probabilistic framework. The hybrid systems are compared on a vowel classification task and on the continuous speech Alphadigits corpus. In both cases, the RVM system achieves better error rates with significantly fewer parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-338"
  },
  "chelba02_icslp": {
   "authors": [
    [
     "Ciprian",
     "Chelba"
    ],
    [
     "Rachel",
     "Morton"
    ]
   ],
   "title": "Mutual information phone clustering for decision tree induction",
   "original": "i02_1005",
   "page_count": 4,
   "order": 339,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "The paper presents an automatic method for devising the question sets used for the induction of classification and regression trees. The algorithm employed is the well-known mutual information based bottom-up clustering applied to phone bigram statistics. The sets of phones at the nodes in the resulting binary tree are used as question sets for clustering context-sensitive (tri-phone) HMM output distributions in a large vocabulary speech recognizer. The algorithm is shown to perform as well and sometimes significantly better than question sets devised by human experts for a Spanish and German system evaluated on several tasks, respectively. It eliminates the need for linguistic expertise and it provides a faster solution as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-339"
  },
  "horn02_icslp": {
   "authors": [
    [
     "Kevin S. Van",
     "Horn"
    ]
   ],
   "title": "Rethinking derived acoustic features in speech recognition",
   "original": "i02_1009",
   "page_count": 4,
   "order": 340,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "We present a new acoustic model for speech recognition that explicitly accounts for information omitted from current acoustic models: the definitions of derived acoustic features such as estimated derivatives. We incorporate this information using the method of maximum entropy. We find that, compared to the corresponding HMM, our model cuts an already low error rate about in half for a simple task. We also examine the consequences of ignoring the origin of derived features in CDHMM systems, showing that such an omission in the acoustic model can severely distort the effective language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-340"
  },
  "markov02_icslp": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Modeling HMM state distributions with Bayesian networks",
   "original": "i02_1013",
   "page_count": 4,
   "order": 341,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "In current HMM based speech recognition systems, it is difficult to supplement acoustic spectrum features with additional information such as pitch, gender, articulator positions, etc. On the other hand, Bayesian Networks (BN) allow for easy combination of different continuous as well as discrete features by exploring conditional dependencies between them. However, the lack of efficient algorithms has limited their application in continuous speech recognition. In this paper we propose new acoustic model, where HMM are used for modeling of temporal speech characteristics and state probability model is represented by BN. In our experimental system based on HMM/BN model, in addition to speech observation variable, state BN has two more (hidden) variables representing noise type and SNR value. Evaluation results on AURORA2 database showed 36.4% word error rate reduction for closed noise test without using any model adaptation or noise robust methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-341"
  },
  "tsakalidis02_icslp": {
   "authors": [
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Vlasios",
     "Doumpiotis"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Discriminative linear transforms for feature normalization and speaker adaptation in HMM estimation",
   "original": "i02_2585",
   "page_count": 4,
   "order": 342,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "Linear transforms have been used extensively for training and adaptation of HMM-based ASR systems. Recently procedures have been developed for the estimation of linear transforms under the Maximum Mutual Information (MMI) criterion. In this paper we introduce discriminative training procedures that employ linear transforms for feature normalization and for speaker adaptive training. We integrate these discriminative linear transforms into MMI estimation of HMM parameters for improvement of large vocabulary conversational speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-342"
  },
  "okuda02_icslp": {
   "authors": [
    [
     "Kozo",
     "Okuda"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Speaking rate compensation based on likelihood criterion in acoustic model training and decoding",
   "original": "i02_2589",
   "page_count": 4,
   "order": 343,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "In this paper, we propose a speaking rate compensation method using frame period and frame length adaptation. Our method decodes an input utterance using several sets of frame period and frame length parameters for speech analysis. Then, this method selects the best set with the highest score which consists of the acoustic likelihood normalized by frame period, language likelihood and insertion penalty. Furthermore, we apply this approach to the training of the acoustic model. We calculate the acoustic likelihood for each frame period and frame length using Viterbi alignment and select the best one for each training utterance. The proposed speaking rate compensation applied to both the acoustic model creation process and decoding process resulted in accuracy improvement of 2.9% (absolute) for spontaneous lecture speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-343"
  },
  "bacchiani02_icslp": {
   "authors": [
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Combining maximum likelihood and maximum a posteriori estimation for detailed acoustic modeling of context dependency",
   "original": "i02_2593",
   "page_count": 4,
   "order": 344,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "An algorithm is proposed to build large, highly detailed acoustic models for context dependent units using a limited amount of training data. Robustness of the parameter estimates in face of data sparsity is addressed by using MAP distribution smoothing. Context dependent distributions are first clustered using a decision tree-based algorithm with an ML objective. These decision trees are then extended using a MAP objective. Experimental results show an absolute reduction in the word error rate of 0.7% by extending an existing state of the art ML trained context dependent model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-344"
  },
  "huang02c_icslp": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Ramesh",
     "Gopinath"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Peder",
     "Olsen"
    ],
    [
     "Karthik",
     "Visweswariah"
    ]
   ],
   "title": "Large vocabulary conversational speech recognition with the extended maximum likelihood linear transformation (EMLLT) model",
   "original": "i02_2597",
   "page_count": 4,
   "order": 345,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "This paper applies the recently proposed Extended Maximum Likelihood Linear Transformation (EMLLT) model in a Speaker Adaptive Training (SAT) context on the Switchboard database. Adaptation is carried out with maximum likelihood estimation of linear transforms for the means, precisions (inverse covariances) and the feature-space under the EMLLT model. This paper shows the first experimental evidence that significant word-error-rate improvements can be achieved with the EMLLT model (in both VTL and VTL+SAT training contexts) over a state-of-the-art diagonal covariance model in a difficult largevocabulary conversational speech recognition task. The improvements were of the order of 1% absolute in multiple scenarios.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-345"
  },
  "zhang02d_icslp": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Modeling varying pauses to develop robust acoustic models for recognizing noisy conversational speech",
   "original": "i02_2601",
   "page_count": 4,
   "order": 346,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "The frequent appearances and varying acoustics of pauses in noisy conversational speech make it a problem to automatically generate an accurate phonetic transcription of the training data for developing robust acoustic models. This paper presents our proposal to exploit reliable phonetic heuristics of pauses in speech to aid the detection of varying pauses. Based on it, a stepwise approach to optimize pause HMMs was applied to the data of SPINEII project, and achieved a more correct phonetic transcription. The cross-word triphone HMMs developed using this transcription got absolute 5.2% word error reduction when compared to the baseline model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-346"
  },
  "song02b_icslp": {
   "authors": [
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Improving phone-level discrimination in LDA with subphone-level classes",
   "original": "i02_2625",
   "page_count": 4,
   "order": 347,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "Linear Discriminant Analysis (LDA) is a well-known technique to improve the discrimination among classes or reduce the dimensionality with minimum loss of discrimination. It is generally used as a part of the front-end of speech recognizer with the classes defined on phone or subphone level. LDA with subphone-level classes (such as tied states or individual Gaussian densities) shows superior performance to that with phone-level classes. However it does not seem to provide the best discrimination from viewpoint of recognition performance. This paper focuses on improving the discrimination between phones while subphones such as tied states are used as the basic classes of LDA. To this end, this paper proposes a method to control the discrimination ability among subphone classes so as to de-emphasize (or ignore) the discrimination between those belonging to the same phone, while emphasizing the discrimination between those belonging to the different phones. Proposed method is implemented in the framework of Weighted Pairwise Scatter LDA (WPS-LDA) and is evaluated on Korean connected digit recognition task. According to experimental results, proposed method shows higher performance than conventional LDA and WPS-LDA. Performance is the highest when the discrimination between pairwise classes belonging to the same phone is totally ignored. The string error reduction rates of conventional LDA, WPS-LDA and proposed method are 13%, 14.5% and 23%, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-347"
  },
  "ou02_icslp": {
   "authors": [
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "A combined model of statics-dynamics of speech optimized using maximum mutual information",
   "original": "i02_2629",
   "page_count": 4,
   "order": 348,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "The linear prediction (LP) HMM does not make the independent and identical distribution (IID) assumption in the traditional HMM; however it often produces unsatisfactory results. In our previous paper [7], both HMMs modeling strengths and weaknesses were analyzed and a new combined model of statics-dynamics of speech was proposed. It works with LPHMM as the dynamic part and with the traditional IID-based HMM as the static part; in addition, easy implementation and low cost are preserved. In this paper, an optimal combination using maximum mutual information (MMI) is introduced. Our experiments on speaker-independent continuous speech recognition demonstrated that the combined model achieved better performance than both models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-348"
  },
  "takahashi02b_icslp": {
   "authors": [
    [
     "Nobutoshi",
     "Takahashi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Syllable recognition using syllable-segment statistics and syllable-based HMM",
   "original": "i02_2633",
   "page_count": 4,
   "order": 349,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "In our previous research, we demonstrated the validity of segmental unit input hidden Markov model (HMM), which regards successive four frame MEL-cepstrum coefficients as a feature vector. The vector is reduced to lower dimensions using the KL transform. However, the model considers only the correlation between frames in a short section, but not the correlation between the frames over a long section.\n",
    "In this paper, in order to represent the correlation over a long distance, we use the syllable-segment statistics that are calculated by the concatenation of feature vectors, corresponding to each state in a syllable based HMM. By combining this approach with a segmentalunit input HMM, the syllable recognition rate was improved to 87% from 83% for syllables taken from continuous speech, without using a language model. We also showed the effectiveness for continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-349"
  },
  "thirion02_icslp": {
   "authors": [
    [
     "J. W. F.",
     "Thirion"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "Recurrent neural network-enhanced HMM speech recognition systems",
   "original": "i02_2637",
   "page_count": 4,
   "order": 350,
   "p1": "2637",
   "pn": "2640",
   "abstract": [
    "In this paper, we show how speech recognition systems can be improved, using an adaptive model transition penalty term in the Viterbi decoding process. This term is calculated using the phonemic segmentation of the speech signal, where a bi-directional recurrent neural network is used to segment the speech into phonemes. No higher level lexical knowledge (phoneme sequence) is used in the segmentation process. The method is compared to an existing technique, on the state-of-the-art speech recognition system, HTK. It is shown that our technique results in significantly better phoneme recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-350"
  },
  "yun02_icslp": {
   "authors": [
    [
     "Young-Sun",
     "Yun"
    ]
   ],
   "title": "Sharing trend information of trajectory in segmental-feature HMM",
   "original": "i02_2641",
   "page_count": 4,
   "order": 351,
   "p1": "2641",
   "pn": "2644",
   "abstract": [
    "In this paper, the reduction method of number of parameters in the segmental-feature HMM (SFHMM) can be considered. It is reported that the SFHMM shows better results than conventional HMM in the previous studies. However, its number of parameters is greater than that of HMM. Therefore, there is a need for new approach that reduces the number of parameters. The trajectories are used for the acoustic features of the SFHMM. In general, trajectory can be separated by the trend and location. Since the trend means the variation of segmental features and occupies the large portion of SFHMM, if the trend is shared, the number of parameters of SFHMM maybe decreases. We consider the trend tying of segmental features by the quantization. The experiments are performed on TIMIT corpus to examine the effectiveness of the trend tying.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-351"
  },
  "salomon02_icslp": {
   "authors": [
    [
     "Jesper",
     "Salomon"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Jesper",
     "Salomon"
    ]
   ],
   "title": "Framewise phone classification using support vector machines",
   "original": "i02_2645",
   "page_count": 4,
   "order": 352,
   "p1": "2645",
   "pn": "2648",
   "abstract": [
    "We describe the use of Support Vector Machines for phonetic classi- fication on the TIMIT corpus. Unlike previous work, in which entire phonemes are classified, our system operates in a framewise manner and is intended for use as the front-end of a hybrid system similar to ABBOT. We therefore avoid the problems of classifying variablelength vectors. Our frame-level phone classification accuracy on the complete TIMIT test set is competitive with other results from the literature. In addition, we address the serious problem of scaling Support Vector Machines by using the Kernel Fisher Discriminant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-352"
  },
  "stewart02_icslp": {
   "authors": [
    [
     "Darryl",
     "Stewart"
    ],
    [
     "Ming",
     "Ji"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "A state-tying approach to building syllable HMMs",
   "original": "i02_2649",
   "page_count": 4,
   "order": 353,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "A severe sparse data problem is faced when building HMMs of syllable units due to the uneven distribution of syllables in natural speech. In this paper we present a novel approach to building syllable HMMs which attempts to overcome this problem. The method involves tying the states in syllable models using a bottom-up clustering algorithm and a state similarity measure employing phonetic information. Experiments using the new approach on the TIMIT database show that it improves the recognition accuracy of syllable HMMs. We present encouraging results which show that when the state-tying method is used in conjunction with a Multi- Model [11] approach to acoustic modeling a syllable identification accuracy of 53.4% can be achieved. This equates to a phoneme accuracy of 72.8% which is comparable with the best results achieved using triphone HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-353"
  },
  "lee02f_icslp": {
   "authors": [
    [
     "Weifeng",
     "Lee"
    ],
    [
     "C. Chandra",
     "Sekhar"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Recognition of continuous speech segments of monophone units using support vector machines",
   "original": "i02_2653",
   "page_count": 4,
   "order": 354,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "In this paper, we study support vector machine based approaches for acoustic modeling of subword units in continuous speech. Classification of subword unit segments is considered as a multi-class pattern recognition problem. In conventional approaches for multi-class pattern recognition using support vector machines, learning involves discrimination of each class against all the other classes. We propose a close-class-set discrimination method suitable for large-classset pattern recognition problems. In the proposed method, learning involves discrimination of each class against a subset of classes confusable with it and included in its close-class-set. We consider different criteria for identification of close-class-sets. We study the effectiveness of the proposed method in reducing the complexity of multi-class pattern recognition systems. We present our studies on recognition of continuous speech segments of 41 mono-phone units in a large corpus of Japanese speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-354"
  },
  "park02_icslp": {
   "authors": [
    [
     "Junho",
     "Park"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Construction of decision tree from data driven clustering",
   "original": "i02_2657",
   "page_count": 4,
   "order": 355,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "In the acoustic modeling for large vocabulary speech recognition, context-dependent (CD) modeling is essential for realizing both improved recognition performance and rapid search. However, sparse data problem caused by huge number of CD models usually leads the estimated models unreliable. To cope with that, two major contextclustering methods, data-driven and rule-based, have been investigated vigorously. In this paper, we briefly review the two methods and develop a new clustering method based on ID3 decision tree learning algorithm that effectively captures the CD modeling. The proposed scheme essentially constructs a decision rule of preclustered triphones using ID3 algorithm. In particular, the datadriven method is used as a clustering algorithm while its result is used as the learning target of ID3 algorithm. The proposed scheme is shown effective over the database of low unknown-context ratio in terms of recognition performance. For speaker-independent, taskindependent continuous speech recognition task, the proposed method reduced percent accuracy WER by 1.16% comparing to that of the existing rule-based method alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-355"
  },
  "lee02g_icslp": {
   "authors": [
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Yuuichiro",
     "Mera"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Selective multi-path acoustic model based on database likelihoods",
   "original": "i02_2661",
   "page_count": 4,
   "order": 356,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "An efficient multi-path acoustic model based on database likelihoods for spontaneous speech recognition is presented. Although a multipath phone HMM that has several models for different target in parallel is considered effective to express multi-style or speed-variant nature of spontaneous speech, assuming various model to match at every time for all phones may cause mismatch of unintended mo- del, and spoil the model constraints. We propose defining a multipath model that has several different state resolutions only for the distortive phones selectively. The phone set is selected through an analysis of the likelihoods and duration times of phone segments in a spoken dialogue corpus using automatic viterbi alignment. Experiments on three test-sets showed that our multi-path model based on the phone selection can achieve better accuracy than a simple single-path model, whereas a full multi-path model without phone selection causes much degradation of accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-356"
  },
  "stephenson02_icslp": {
   "authors": [
    [
     "Todd A.",
     "Stephenson"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Auxiliary variables in conditional Gaussian mixtures for automatic speech recognition",
   "original": "i02_2665",
   "page_count": 4,
   "order": 357,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "In previous work, we presented a case study using an estimated pitch value as the conditioning variable in conditional Gaussians that showed the utility of hiding the pitch values in certain situations or in modeling it independently of the hidden state in others. Since only single conditional Gaussians were used in that work, we extend that work here to using conditional Gaussian mixtures in the emission distributions to make this work more comparable to state-of-the-art automatic speech recognition. We also introduce a rate-of-speech (ROS) variable within the conditional Gaussian mixtures. We find that, under the current methods, using observed pitch or ROS in the recognition phase does not provide improvement. However, systems trained on pitch or ROS may provide improvement in the recognition phase over the baseline when the pitch or ROS is marginalized out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-357"
  },
  "watanabe02b_icslp": {
   "authors": [
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Naonori",
     "Ueda"
    ]
   ],
   "title": "Constructing shared-state hidden Markov models based on a Bayesian approach",
   "original": "i02_2669",
   "page_count": 4,
   "order": 358,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "In this paper, we propose a method for constructing shared-state triphone HMMs (SST-HMMs) within a practical Bayesian framework. In our method, Bayesian model selection criterion is derived for SSTHMM based on the Variational Bayesian approach. The appropriate phonetic decision tree structure of SST-HMM is found by using the criterion according to a given data set. This criterion, unlike the conventional MDL criterion, is applicable even in the case of insuf- ficient amounts of data. We conduct two experiments on speaker independent word recognition in order to prove the effectiveness of the proposed method. The first experiment demonstrates that the Bayesian approach is valid for determining the tree structure. The second experiment demonstrates that the Bayesian criterion can design SST-HMMs with higher recognition performance than the MDL criterion when dealing with small amounts of data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-358"
  },
  "ogawa02_icslp": {
   "authors": [
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Generalization of state-observation-dependency in partly hidden Markov models",
   "original": "i02_2673",
   "page_count": 4,
   "order": 359,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "Generalized Partly Hidden Markov Model (GPHMM) is proposed by modifying Partly Hidden Markov Model (PHMM), and it is successfully applied to the speech recognition. PHMM, which was proposed in our previous paper, is the novel stochastic model, in which the pairs of the hidden states (H-state) and the observable states (O-state) determine the stochastic phenomena of the current observation and the next state transition. In the formulation of PHMM, we used common pair of H-state and O-state to determine both of these phenomena. In the formulation of GPHMM proposed here, we use common Hstate but different O-states for the current observation and for the next state transition separately. This slight modification brought the big flexibility in the modeling of phenomena. Experimental results showed the effectiveness of GPHMM (without delta parameters): it reduced the word error by 17% compared to triphone HMM (with delta parameters), respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-359"
  },
  "esling02_icslp": {
   "authors": [
    [
     "John H.",
     "Esling"
    ]
   ],
   "title": "Laryngoscopic analysis of tibetan chanting modes and their relationship to register in sino-tibetan",
   "original": "i02_1081",
   "page_count": 4,
   "order": 360,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "To investigate how the articulatory structures of the larynx and pharynx behave in Tibetan chanting, we obtained laryngoscopic video sequences of several phonemic contrasts in Tibetan and of two principal chant modes: a \"high\" chant which adopts raised-larynx posture with the tongue and epiglottis retracted, and a \"deep\" chant which adopts lowered-larynx posture with an open pharyngeal area but constricted ventricular ring immediately above the glottis and ventricular/mucosal channeling of airflow. The two chants reflect different modes of the laryngeal sphincter mechanism: one with aryepiglottic approximation but without trilling; the other with only the ventricular component. Aspects of phonemic patterns found in the three Sino-Tibetan languages we have observed laryngoscopically (Yi, Bai, and Tibetan) can be seen in the way the two chant modes are produced in Tibetan. The high chant resembles \"tense\" register in Yi, while the deep chant resembles \"tense/harsh/low-tone\" register in Bai.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-360"
  },
  "murray02_icslp": {
   "authors": [
    [
     "Kathleen",
     "Murray"
    ],
    [
     "Betina",
     "Simonsen"
    ]
   ],
   "title": "A corpus-based study of danish laryngealization",
   "original": "i02_1085",
   "page_count": 4,
   "order": 361,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "This research complements past progress in modeling/classifying speech acts and laryngealizations (LAs) in different languages, ultimately for the purpose of Danish speech act analysis. Danish is a language whose rich history of phonology is not matched by extensive corpus analysis. Danish also has been known to be difficult to analyze with respect to boundary tones/speech acts. This paper cites and explains these difficulties, offers solutions in terms of a corpus-based approach, and how to use current tools in a different way than previously used in order to emphasize important details. An improved understanding of Danish LAs could allow for the discovery of interesting relationships between traditional prosodic parameters and Danish speech acts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-361"
  },
  "warner02_icslp": {
   "authors": [
    [
     "Natasha",
     "Warner"
    ],
    [
     "Allard",
     "Jongman"
    ],
    [
     "Doris",
     "Mücke"
    ]
   ],
   "title": "Variability in direction of dorsal movement during production of /l/",
   "original": "i02_1089",
   "page_count": 4,
   "order": 362,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "This paper presents articulatory data on the production of /l/ in various environments in Dutch, and shows that the direction of movement of the tongue dorsum varies across environments. This makes it impossible to measure tongue position at the peak of the dorsal gesture. We argue for an alternative method in such cases: measurement of position of one articulator at a time point defined by the gesture of another. We present new data measured this way which confirms a previous finding on the articulation of Dutch /l/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-362"
  },
  "xu02b_icslp": {
   "authors": [
    [
     "Yi",
     "Xu"
    ],
    [
     "Fang",
     "Liu"
    ]
   ],
   "title": "Segmentation of glides with tonal alignment as reference",
   "original": "i02_1093",
   "page_count": 4,
   "order": 363,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "This paper reports an attempt to determine the segmentation of glides using existing knowledge about tonal alignment as reference. It is found that the likely onset of a glide is much earlier than what is acoustically the most obvious, i.e., the point where the formants reach their extremes. It is further found that there is indication that the point of formant extremes may in fact be the point of glide release.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-363"
  },
  "maddieson02b_icslp": {
   "authors": [
    [
     "Ian",
     "Maddieson"
    ],
    [
     "Julie",
     "Larson"
    ]
   ],
   "title": "Variability in the production of glottalized sonorants: data from yapese",
   "original": "i02_1097",
   "page_count": 4,
   "order": 364,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "Phonetic variation in the realization of segments is increasingly recognized to have functional value in signalling matters such as constituency, but patterns of variation of rarer segment types are not as well known as those of common types such as plosives. This paper presents data on variation in production of glottalized sonorants in the Austronesian language Yapese. Initial glottalized sonorants are typically but not invariably pre-glottalized. Final ones are postglottalized. Medial ones are the most variable and may have very attenuated glottalization. Initial ones display greater variation in the degree of glottal constriction than final ones. These positional differences would have potential value in parsing strings of segments into words in Yapese, and may contribute to accounting for an asymmetry in the inventories of sonorant consonants in initial and final positions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-364"
  },
  "tuan02_icslp": {
   "authors": [
    [
     "Vu Ngoc",
     "Tuan"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Sophie",
     "Rosset"
    ]
   ],
   "title": "A phonetic study of vietnamese tones: acoustic and electroglottographic measurements",
   "original": "i02_1101",
   "page_count": 4,
   "order": 365,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "Vietnamese is a tone language using 6 different tones. The phonology of Vietnamese makes use of both melodic tones (pitch movements and registers) and voice quality tones. In this paper we present a phonetic description of the 6 tones of Vietnamese. Acoustic and electroglottographic data are provided. Two speakers (one female, one male) participated in the recordings. Phonetic realization of phonological tones using these acoustic parameters is discussed. The results indicate that several phonetic features are able to make possible the phonological contrasts. The first feature is the mean melodic register (high-low), represented mainly by the fundamental frequency. The second feature is the melodic movement (fall, rise, and fall-rise) represented mainly by the fundamental frequency. The third feature is voice quality, represented mainly by the voice open quotient and energy. Finally, a fourth feature seems to be the tone duration. However, all these features are not equally effective for all the tones, and some features seem correlated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-365"
  },
  "chung02_icslp": {
   "authors": [
    [
     "Hyunsong",
     "Chung"
    ]
   ],
   "title": "Segment duration in spoken korean",
   "original": "i02_1105",
   "page_count": 4,
   "order": 366,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "An experiment was carried out to investigate the contextual effects of linguistic features on segment duration in spoken Korean. Contextual effects such as the segmental effects of surrounding segments, the prosodic phrasal effects and the syllable structure were considered for the investigation. This paper concentrates on segment duration analysis of a news-reading speech style, using a corpus of 670 read sentences collected from one speaker of standard Korean. Classification and Regression Tree (CART) analysis was used to explore the relationship between the context features and the realised duration. Results showed that prosodic phrase features had significant effects on segment duration. Most of the prosodic phrase influences were attributed to the accentual phrase boundary lengthening effect. Vowels either preceding or following the [nasal] feature were significantly shortened. More shortening effects were observed in the nasals than in the homorganic voiced obstruents. The [stiff vocal fold] feature, which covers the aspirated obstruents and tense obstruents in Korean, shortened the following vowel considerably. Subsequent to these linguistic findings, duration models based on Sums-of-Products (SoP) models and CART models were built for textto- speech conversion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-366"
  },
  "zvonik02_icslp": {
   "authors": [
    [
     "Elena",
     "Zvonik"
    ],
    [
     "Fred",
     "Cummins"
    ]
   ],
   "title": "Pause duration and variability in read texts",
   "original": "i02_1109",
   "page_count": 4,
   "order": 367,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "Generating natural sounding synthetic speech from text requires a division of a text into IPs and assigning pauses between those phrases. A difficulty which faces attempts to model pauses quantitatively is high degree of variability exhibited by speakers in pause placement and duration. The present study seeks to investigate if Synchronous Speech (speech elicited when two speakers are asked to read a text together) can be used as a mean to reduce inter-speaker variability providing more reliable data for accurate modeling pause durations at IP breaks. We find reduced variability in pause duration when speakers read a text in synchrony. We also find an apparent dependence of pause duration on the length and/or syntactic complexity of the preceding phrase. The reduction in variability when reading synchronously is most evident for the one pause exhibiting markedly longer mean duration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-367"
  },
  "pfitzinger02b_icslp": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Intrinsic phone durations are speaker-specific",
   "original": "i02_1113",
   "page_count": 4,
   "order": 368,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "This study examines the speakers influence on mean phone durations. As long as speech rate variation is present, the result of such a study would be trivial because every speaker has a particular speech rate that naturally modifies phone durations. Therefore, in order to eliminate its influence on phone duration, we developed a normalization procedure which evens out the local variability of speech rate, and then applied it to a large database of spoken German. As would be expected, general linear model statistical analysis (GLM) showed that speech rate normalization strongly reduced the variance explained by the factor speaker. Nevertheless, the variance explained by the interaction between speaker and phone type remained constant. Consequently, each speaker has individual intrinsic phone durations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-368"
  },
  "tronnier02_icslp": {
   "authors": [
    [
     "Mechtild",
     "Tronnier"
    ]
   ],
   "title": "Preaspirated stops in southern Swedish",
   "original": "i02_1117",
   "page_count": 4,
   "order": 369,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "This paper is concerned with a systematical investigation of speakers of various southern Swedish dialects, showing that preaspiration is more typical for womens speech. Evidence is found in that preaspiration occurs more frequently for this group of speakers and that its duration is generally longer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-369"
  },
  "warner02b_icslp": {
   "authors": [
    [
     "Natasha",
     "Warner"
    ],
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Stop epenthesis at syllable boundaries",
   "original": "i02_1121",
   "page_count": 4,
   "order": 370,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "This paper investigates the production and perception of epenthetic stops at syllable boundaries in Dutch and compares the experimental data with lexical statistics for Dutch and English. This extends past work on epenthesis in coda position [1]. The current work is particularly informative regarding the question of phonotactic constraints influence on parsing of speech variability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-370"
  },
  "raymond02_icslp": {
   "authors": [
    [
     "William D.",
     "Raymond"
    ],
    [
     "Mark",
     "Pitt"
    ],
    [
     "Keith",
     "Johnson"
    ],
    [
     "Elizabeth",
     "Hume"
    ],
    [
     "Matthew",
     "Makashay"
    ],
    [
     "Robin",
     "Dautricourt"
    ],
    [
     "Craig",
     "Hilts"
    ]
   ],
   "title": "An analysis of transcription consistency in spontaneous speech from the buckeye corpus",
   "original": "i02_1125",
   "page_count": 4,
   "order": 371,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "We present a preliminary analysis of transcriber consistency in labeling and segmentation of words and phones in the Buckeye corpus of spontaneous, informal speech. We find that pairwise intertranscriber agreement on exact phone label match was 76%, and segmentation agreement within 20% of phone pair length was 75%, though longer phones are more consistently segmented than shorter phones. Patterns of consistency variation in labeling are observed as a function of phonetic categories that are similar to patterns reported for read speech. More agreement is seen on consonants than on vowels, and on fricatives and labials than on other consonant classes. In general, we find that shorter, more reduced words and phones result in more transcriber disagreement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-371"
  },
  "aoyagi02_icslp": {
   "authors": [
    [
     "Makiko",
     "Aoyagi"
    ]
   ],
   "title": "Contextual effects on voicing judgment of stop consonants in Japanese",
   "original": "i02_1129",
   "page_count": 4,
   "order": 372,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "A perception experiment on stop consonant voicing in (C)V1CV2 was conducted to test the effects of the preceding vowel. Five types of /ki/ as CV1 differing in closure voicing, vowel duration and vowel devoicing were spliced with a da-ta VOT continuum as CV2. Subjects judgment between the spliced kita and kida along a VOT range was compared with that of ta and da in isolation. Strong effects of closure voicing and vowel devoicing were observed. More importantly, voicing judgment gradually shifted towards voiceless with the gradual reduction of the preceding vowel, indicating the parallelism of phonetic, gradient vowel reduction and its perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-372"
  },
  "joto02_icslp": {
   "authors": [
    [
     "Akiyo",
     "Joto"
    ],
    [
     "Motohisa",
     "Imaishi"
    ],
    [
     "Yoshiki",
     "Nagase"
    ],
    [
     "Seiya",
     "Funatsu"
    ]
   ],
   "title": "Discrimination of English vowels in consonantal contexts by native speakers of Japanese and its relations to dynamic information of formants",
   "original": "i02_1133",
   "page_count": 4,
   "order": 373,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "This study examined how differently native speakers of Japanese discriminated between the American English vowels /E/ and /æ/ in /CVd/ syllables with 23 different initial consonants, and how the differing discrimination was related to the changes of formant patterns throughout the vowels in comparison with the Japanese vowels /e/ and /a/. A perceptual test and formant analyses of the English and Japanese vowels were conducted. The results showed that there were significant differences in discrimination across the consonantal contexts: the discrimination of /E/ was significantly poorer when the initial consonant was /tS/, /T/, /dZ/, /j/, /g/, /k/ or /ð/, and that of /æ/, when it was /dZ/, /S/, /d/, /n/ or /t/. It was found that the poorer discrimination was more related to the smaller values of the formant ratios (F2/F1) of /E/ and to the larger values of the formant ratios of /æ/ in the latter part of the vowels. The acoustical closeness between the two English vowels in the latter part of the vowels could be attributed to the poorer discrimination in the particular consonantal contexts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-373"
  },
  "tur02_icslp": {
   "authors": [
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Jerry",
     "Wright"
    ],
    [
     "Allen",
     "Gorin"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Improving spoken language understanding using word confusion networks",
   "original": "i02_1137",
   "page_count": 4,
   "order": 374,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "A natural language spoken dialog system includes a large vocabulary automatic speech recognition (ASR) engine, whose output is used as the input of a spoken language understanding component. Two challenges in such a framework are that the ASR component is far from being perfect and the users can say the same thing in very different ways. So, it is very important to be tolerant to recognition errors and some amount of orthographic variability. In this paper, we present our work on developing new methods and investigating various ways of robust recognition and understanding of an utterance. To this end, we exploit word-level confusion networks (sausages), obtained fromASR word graphs (lattices) instead of the ASR 1-best hypothesis. Using sausages with an improved confidence model, we decreased the call-type classification error rate for AT&Ts How May I Help You (HMIHY) natural dialog system by 38%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-374"
  },
  "li02f_icslp": {
   "authors": [
    [
     "Li",
     "Li"
    ],
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "Improving latent semantic indexing based classifier with information gain",
   "original": "i02_1141",
   "page_count": 4,
   "order": 375,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "In this paper, we describe an approach of using a discriminative term selection process based on information grain (IG) to improve the performance of the latent semantic indexing (LSI). The discriminative power of the term is measured by entropy variations averaged over all categories conditioned upon whether the term is present or absent. The proposed approach is applied to the task of natural language call routing (NLCR), where natural language based classifiers are used to route calls to desired destinations. Various experimental studies are performed. Significant performance gains of 27% on precision and 26.5% on recall are observed. Most importantly, the proposed approach is almost independent of task dependent language resources and robust to term variations, making it highly portable to various information retrieval and natural language understanding tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-375"
  },
  "kuo02b_icslp": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Egbert",
     "Ammicht"
    ]
   ],
   "title": "Discriminative training for call classification and routing",
   "original": "i02_1145",
   "page_count": 4,
   "order": 376,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "Natural language call classification can be performed using a latent semantic indexing (LSI) matrix, a popular vector-space model used in information retrieval. Traditionally this matrix was constructed by counting different words or word sequences found in requests for different destinations, with appropriate heuristic weightings to emphasize words that are salient or important for classification. At ICSLP2000, we introduced discriminative training (DT) of the LSI matrix. DT considers both positive and negative examples during training to minimize the classification error and increase the score separation of the correct hypothesis from competitors. Some parameters become negative after DT, resulting from suppressive learning not traditionally possible: important anti-features are thus obtained. DT improves portability by making the classifier robust to different feature selection and by decreasing the amount of training data needed. Results are reported for call routing and the Switchboard topic identification task, where a 70% relative improvement was obtained after DT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-376"
  },
  "cox02_icslp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Speech and language processing for a constrained speech translation system",
   "original": "i02_1149",
   "page_count": 4,
   "order": 377,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "A system that provides translation from speech to sign language (TESSA) is described. TESSA has been developed to assist a Post Office clerk (who has no knowledge of sign language) in making a transaction with a customer using sign language. The system uses a set of about 370 pre-defined phrases which have been pre-stored and can be signed by a specially-developed avatar. The clerk is unable to memorise all these phrases and so the system attempts to map his or her input speech to the pre-stored phrase that is semantically equivalent to the input phrase. We describe the language processing that was developed to perform this task, and give results obtained using alternative formulations of the phrases from a number of speakers. We then give results for speech input and show the effect of different configurations of the recognisers language model on the results. Best performance was obtained when the language model was closely integrated with the task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-377"
  },
  "chotimongkol02_icslp": {
   "authors": [
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Automatic concept identification in goal-oriented conversations",
   "original": "i02_1153",
   "page_count": 4,
   "order": 378,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "We address the problem of identifying key domain concepts automatically from an unannotated corpus of goal-oriented humanhuman conversations. We examine two clustering algorithms, one based on mutual information and another one based on Kullback- Liebler distance. In order to compare the results from both techniques quantitatively, we evaluate the outcome clusters against reference concept labels using precision and recall metrics adopted from the evaluation of topic identification task. However, since our system allows more than one cluster to associate with each concept an additional metric, a singularity score, is added to better capture cluster quality. Based on the proposed quality metrics, the results show that Kullback-Liebler-based clustering outperforms mutual informationbased clustering for both the optimal quality and the quality achieved using an automatic stopping criterion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-378"
  },
  "levit02_icslp": {
   "authors": [
    [
     "Michael",
     "Levit"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Allen",
     "Gorin"
    ]
   ],
   "title": "Using EM-trained string-edit distances for approximate matching of acoustic morphemes",
   "original": "i02_1157",
   "page_count": 4,
   "order": 379,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "Our research concerns spoken language understanding within the domain of automated telecommunication services. In the recent papers we presented a new methodology for training of statistical language models for recognition and understanding of utterances from large corpora of phone sequences obtained as the output of a taskindependent ASR-system. The advantage of this strategy compared to the traditional word-based strategy is that we dont have to manually transcribe large amounts of data in order to extract acoustic morphemes to train the classifier. Since the baseline strategy suffered high False Rejection Rates caused by finding no acoustic morphemes in the test data, we describe in this paper how approximate matching can be incorporated in the Bayes-classifier to reduce FRR. The experiments are evaluated for \"How May I Help You?\"-task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-379"
  },
  "natarajan02_icslp": {
   "authors": [
    [
     "Premkumar",
     "Natarajan"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Bernhard",
     "Suhm"
    ],
    [
     "Daniel",
     "McCarthy"
    ]
   ],
   "title": "Speech-enabled natural language call routing: BBN call director",
   "original": "i02_1161",
   "page_count": 4,
   "order": 380,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "In this paper we discuss the design and performance of the BBN Call Director product for automatic call routing and the methodology for its deployment. The component technologies for the BBN Call Director are a statistical n-gram speech recognizer and a statistical topic identification system that, together, provide the framework for processing natural language responses from callers. To achieve commercial success, however, a superior deployment process is as important as the technology itself. In order to minimize the operational and financial risk for the call center, BBN has developed a deployment process that provides an integrated methodology for building the business case, optimizing performance, and proving the benefit of natural language call routing. This process is based on quantifying IVR benefit in terms of saved agent labor by analyzing end-to-end recordings of live calls. Routing accuracy is of critical importance because of the direct cost impact of a misrouted call to the call center. Experimental results on real traffic coming into a customer call center indicate that BBN Call Director can reduce the number of misrouted calls by 28%, which translates to a savings of 2 to 4 minutes for each of those calls. For large call centers handling several million calls per year, the corresponding cost savings can be in the millions of dollars.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-380"
  },
  "gao02_icslp": {
   "authors": [
    [
     "Sheng",
     "Gao"
    ],
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Tat-seng",
     "Chua"
    ]
   ],
   "title": "Weighted graph based decision tree optimization for high accuracy acoustic modeling",
   "original": "i02_1233",
   "page_count": 4,
   "order": 381,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In this paper, a novel weighted graph based decision tree optimization algorithm is proposed. The graph is a compact representation of most possible solutions for the decision tree based classifications. The optimal decision tree is then be extracted from it using the Nlevel prediction technique. The prediction technique can incorporate future knowledge about classifications based on the tree and make the current decision on the best question more accurate when tree splitting. Tied triphones based on this method are more accurate than those using popular tree growing method from a point view of maximum likelihood. This approach provides a flexible structure to optimize the decision tree. Our experimental results show that higher performance is obtained with different level predictions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-381"
  },
  "zhang02e_icslp": {
   "authors": [
    [
     "Li",
     "Zhang"
    ],
    [
     "William H.",
     "Edmondson"
    ]
   ],
   "title": "Speech recognition using syllable patterns",
   "original": "i02_1237",
   "page_count": 4,
   "order": 382,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "This paper presents an account of the use of syllable structure as the basis for a novel approach to speech recognition. This contrasts with the serial organization of more conventional phonetic segments, and their use in speech recognition systems. It is demonstrated that working with syllables provides the basis for linguistically motivated speech recognition using the previously reported notion of the Pseudo-Articulatory Representation (PAR). The results are very promising taking into account the preliminary nature of the work and the novelty of the approach. A related paper [1] deals with theoretical issues in greater depth.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-382"
  },
  "brink02_icslp": {
   "authors": [
    [
     "Janus D.",
     "Brink"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "A comparison of L1 and african-mother-tongue acoustic models for south african English speech recognition",
   "original": "i02_1241",
   "page_count": 4,
   "order": 383,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "Speaker accent influences the performance of automatic speech recognition (ASR) systems. Knowledge of accent based acoustic variations can therefore be used in the development of more robust systems. The goal of this project is to characterize the vowels and diphthongs of second language (L2) South African English to aid in the adaptation of existing first language (L1) English recognition systems for better L2 performance. This paper investigates the differences between the vowels and diphthongs of L1 and L2 English in South Africa and is specifically aimed at L2 English speakers with a native African mother tongue for instance isi-Xhosa, isi-Zulu, Tswana or Sepedi. The vowel systems of English, and African languages, as described in the linguistic literature, are compared to predict the expected deviations of L2 South African English from the L1 norm. Acoustic models based on formant and Mel-scaled cepstral features of 80 context dependent phonemes from L1 and L2 speakers are compared. Our findings agree well with those linguistically predicted, in particular, evidence of equivalence-classification, peripheralization of schwa and changes in diphthong strength are observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-383"
  },
  "somervuo02_icslp": {
   "authors": [
    [
     "Panu",
     "Somervuo"
    ]
   ],
   "title": "Speech modeling using variational Bayesian mixture of Gaussians",
   "original": "i02_1245",
   "page_count": 4,
   "order": 384,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "The topic of this paper is speech modeling using the Variational Bayesian Mixture of Gaussians algorithm proposed by Hagai Attias (2000). Several mixtures of Gaussians were trained for representing cepstrum vectors computed from the TIMIT database. The VB-MOG algorithm was compared to the standard EM algorithm. VB-MOG was clearly better, its convergence was faster, there was no tendency to overfitting, and finally, it gave consistently better likelihoods for unseen test data using any given number of the mixture components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-384"
  },
  "chen02d_icslp": {
   "authors": [
    [
     "Tao",
     "Chen"
    ],
    [
     "Chao",
     "Huang"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Jingchun",
     "Wang"
    ]
   ],
   "title": "On the use of Gaussian mixture model for speaker variability analysis",
   "original": "i02_1249",
   "page_count": 4,
   "order": 385,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "Analysis and modeling of speaker variability is important to help understand in-depth inter-speaker variances and to enhance current speech/speaker recognition system. In this paper we introduce adapted Gaussian mixture model (GMM) based speaker representation for the task. Two powerful multivariate statistical analysis methods, principal component analysis (PCA) and independent component analysis (ICA), are used to extract the sources of dominant speaker variability. In addition, analysis of variance (ANOVA) is adopted to evaluate the dominance of a factor in a certain principal/independent component. Further, the generalization ability of our method is investigated by experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-385"
  },
  "jackson02_icslp": {
   "authors": [
    [
     "Philip J.B.",
     "Jackson"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Models of speech dynamics in a segmental-HMM recognizer using intermediate linear representations",
   "original": "i02_1253",
   "page_count": 4,
   "order": 386,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "A theoretical and experimental analysis of a simple multilevel SegmentalHMMis presented in which the relationship between symbolic (phonetic) and surface (acoustic) representations of speech is regulated by an intermediate (articulatory) layer, where speech dynamics are modeled using linear trajectories. Three formant-based parameterizations and measured articulatory positions are considered as intermediate representations, from the TIMIT and MOCHA corpora respectively. The articulatory-to-acoustic mapping was performed by between 1 and 49 linear transformations. Results of phone-classi- fication experiments demonstrate that, by appropriate choice of intermediate parameterization and mappings, it is possible to achieve close to optimal performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-386"
  },
  "zen02_icslp": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Decision tree distribution tying based on a dimensional split technique",
   "original": "i02_1257",
   "page_count": 4,
   "order": 387,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "In this paper, a new clustering technique called Dimensional Split Phonetic Decision Tree (DS-PDT) is proposed. In DSPDT, state distributions are split dimensionally when applying phonetic question. This technique is an extension of the decision tree based acoustic modeling. It gives a proper context-dependent sharing structure of each dimension automatically while maintaining the correlations among the dimensions. In speaker-independent continuous speech recognition experiments, DS-PDT achieved about 8% error reduction over the phonetic decision tree clustering.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-387"
  },
  "huckvale02_icslp": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Speech synthesis, speech simulation and speech science",
   "original": "i02_1261",
   "page_count": 4,
   "order": 388,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "Speech synthesis research has been transformed in recent years through the exploitation of speech corpora - both for statistical modelling and as a source of signals for concatenative synthesis. This revolution in methodology and the new techniques it brings calls into question the received wisdom that better computer voice output will come from a better understanding of how humans produce speech. This paper discusses the relationship between this new technology of simulated speech and the traditional aims of speech science. The paper suggests that the goal of speech simulation frees engineers from inadequate linguistic and physiological descriptions of speech. But at the same time, it leaves speech scientists free to return to their proper goal of building a computational model of human speech production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-388"
  },
  "bulut02_icslp": {
   "authors": [
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "Expressive speech synthesis using a concatenative synthesizer",
   "original": "i02_1265",
   "page_count": 4,
   "order": 389,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "This paper describes an experiment in synthesizing four emotional states - anger, happiness, sadness and neutral - using a concatenative speech synthesizer. To achieve this, five emotionally (i.e., semantically) unbiased target sentences were prepared. Then, separate speech inventories, comprising the target diphones for each of the above emotions, were recorded. Using the 16 different combinations of prosody and inventory during synthesis resulted in 80 synthetic test sentences. The results were evaluated by conducting listening tests with 33 naïve listeners. Synthesized anger was recognized with 86.1% accuracy, sadness with 89.1%, happiness with 44.2%, and neutral emotion with 81.8% accuracy. According to our results, anger was classified as inventory dominant and sadness and neutral as prosody dominant. Results were not sufficient to make similar conclusions regarding happiness. The highest recognition accuracies were achieved for sentences synthesized by using prosody and diphone inventory belonging to the same emotion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-389"
  },
  "shichiri02_icslp": {
   "authors": [
    [
     "Kengo",
     "Shichiri"
    ],
    [
     "Atsushi",
     "Sawabe"
    ],
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Eigenvoices for HMM-based speech synthesis",
   "original": "i02_1269",
   "page_count": 4,
   "order": 390,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "This paper describes an eigenvoice technique for an HMM-based speech synthesis system which can synthesize speech with various voice qualities. In the eigenvoice technique, which has successfully been applied to fast speaker adaptation in an HMM based speech recognition, a large number of speaker dependent HMM sets are represented by a few parameters through a dimensionality reduction technique, e.g., PCA. In this paper, we propose an eigenvoice technique for speech synthesis, and apply it to an HMM-based speech synthesis system in which spectrum and F0 are modeled by HMMs, and synthetic speech generated fromHMMs themselves. The generated spectrum and F0 pattern are shown, and the relation between weights for eigenvoices and voice quality is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-390"
  },
  "marsi02_icslp": {
   "authors": [
    [
     "Erwin",
     "Marsi"
    ],
    [
     "Bertjan",
     "Busser"
    ],
    [
     "Walter",
     "Daelemans"
    ],
    [
     "Veronique",
     "Hoste"
    ],
    [
     "Martin",
     "Reynaert"
    ],
    [
     "Antal van den",
     "Bosch"
    ]
   ],
   "title": "Combining information sources for memory-based pitch accent placement",
   "original": "i02_1273",
   "page_count": 4,
   "order": 391,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "We describe results on pitch accent placement in Dutch text obtained with a memory-based learning approach. The training material consists of newspaper texts that have been prosodically annotated by humans, and subsequently enriched with linguistic features and informational metrics using generally available, low-cost, shallow, knowledge-poor tools. We report on the effects of contextmodelling and the nearest neighbours parameter (k), and show the advantage of combining features of a different nature, where the best performance yields a cross-validated F-score of 82. Evaluation on an independent test corpus shows that our approach outperforms existing TTS systems for Dutch.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-391"
  },
  "swift02_icslp": {
   "authors": [
    [
     "Mary D.",
     "Swift"
    ],
    [
     "Ellen",
     "Campana"
    ],
    [
     "James F.",
     "Allen"
    ],
    [
     "Michael K.",
     "Tanenhaus"
    ]
   ],
   "title": "Eye-fixation as a measure of real-time processing of synthesized words",
   "original": "i02_1277",
   "page_count": 4,
   "order": 392,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "We present experimental evidence from a study in which we monitor eye movements as people respond to pre-recorded instructions generated by a human speaker and by two text-to-speech synthesizers. We replicate findings demonstrating that people process human speech incrementally, making partial commitments as a word unfolds. Specifically, they entertain multiple lexical candidates on the fly depending on segmental overlap in the candidate set. Importantly, incremental understanding is also observed for synthesized text-to-speech instructions. These results, including some suggestive differences in responses with the two text-to-speech systems, establish the potential for using eye-tracking methodology together with synthesized speech stimuli as a powerful theoretical and experimental tool for spoken language processing research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-392"
  },
  "stent02_icslp": {
   "authors": [
    [
     "Amanda",
     "Stent"
    ],
    [
     "Marilyn A.",
     "Walker"
    ],
    [
     "Steve",
     "Whittaker"
    ],
    [
     "Preetam",
     "Maloor"
    ]
   ],
   "title": "User-tailored generation for spoken dialogue: an experiment",
   "original": "i02_1281",
   "page_count": 4,
   "order": 393,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "Recent work on evaluation of spoken dialogue systems suggests that the information presentation phase of complex dialogues is often the primary contributor to dialogue duration. Therefore, better algorithms are needed for the presentation of complex information in speech. This paper evaluates the effect of a user model on generation for three dialogue strategies: SUMMARY, COMPARE and RECOMMEND. We present results showing that (a) both COMPARE and RECOMMEND strategies are effective; and (b) the user model is useful.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-393"
  },
  "roy02b_icslp": {
   "authors": [
    [
     "Deb",
     "Roy"
    ]
   ],
   "title": "A system that learns to describe objects in visual scenes",
   "original": "i02_1285",
   "page_count": 4,
   "order": 394,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "A spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes. The system is trained by a show-and-tell procedure in which visual scenes are paired with natural language descriptions. A set of learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure, word classes, and individual words. Using these structures, a planning algorithm integrates syntactic, semantic, and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes. The learning system is able to generalize from training data to generate expressions which never occurred during training. The output of the generation system is synthesized using word-based concatenative synthesis by drawing from the original training speech corpus. In evaluations of semantic comprehension by human judges, the performance of automatically generated spoken descriptions was comparable to human generated descriptions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-394"
  },
  "mou02_icslp": {
   "authors": [
    [
     "Xiaolong",
     "Mou"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Integration of supra-lexical linguistic models with speech recognition using shallow parsing and finite state transducers",
   "original": "i02_1289",
   "page_count": 4,
   "order": 395,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "This paper proposes a layered Finite State Transducer (FST) framework integrating hierarchical supra-lexical linguistic knowledge into speech recognition based on shallow parsing. The shallow parsing grammar is derived directly from the full fledged grammar for natural language understanding, and augmented with top-level ngram probabilities and phrase-level context-dependent probabilities, which is beyond the standard context-free grammar (CFG) formalism. Such a shallow parsing approach can help balance sufficient grammar coverage and tight structure constraints. The context-dependent probabilistic shallow parsing model is represented by layered FSTs, which can be integrated with speech recognition seamlessly to impose early phrase-level structural constraints consistent with natural language understanding. It is shown that in the JUPITER [1] weather information domain, the shallow parsing model achieves lower recognition word error rates, compared to a regular class ngram model with the same order. However, we find that, with a higher order top-level n-gram model, pre-composition and optimization of the FSTs are highly restricted by the computational resources available. Given the potential of such models, it may be worth pursing an incremental approximation strategy [2], which includes part of the linguistic model FST in early optimization, while introducing the complete model through dynamic composition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-395"
  },
  "shu02_icslp": {
   "authors": [
    [
     "Han",
     "Shu"
    ],
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "EM training of finite-state transducers and its application to pronunciation modeling",
   "original": "i02_1293",
   "page_count": 4,
   "order": 396,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "Recently, finite-state transducers (FSTs) have been shown to be useful for a number of applications in speech and language processing. FST operations such as composition, determinization, and minimization make manipulating FSTs very simple. In this paper, we present a method to learn weights for arbitrary FSTs using the EM algorithm. We show that this FST EM algorithm is able to learn pronunciation weights that improve the word error rate for a spontaneous speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-396"
  },
  "szarvas02_icslp": {
   "authors": [
    [
     "Máté",
     "Szarvas"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Finite-state transducer based hungarian LVCSR with explicit modeling of phonological changes",
   "original": "i02_1297",
   "page_count": 4,
   "order": 397,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "This article describes the design and the experimental evaluation of the first Hungarian large vocabulary continuous speech recognition (LVCSR) system. The architecture of the recognition system is based on the recently proposed weighted finite state transducer (WFST) paradigm. The task domain is the recognition of fluently read sentences selected from a major daily newspaper. Recognition performance is evaluated using both monophone and triphone gender independent acoustic models. The vocabulary units used in the system are morpheme based in order to provide sufficient coverage of the large number of word-forms resulting from affixation and compounding in Hungarian. The language model is a statistical morpheme bigram model. Besides the basic list style pronunciation dictionary model we evaluate a novel phonology modeling component that describes the phonological changes prevalent in fluent Hungarian. Thanks to the flexible transducer-based architecture of the system the phonological component is integrated seamlessly with the basic modules with no need to modify the decoder itself. The proposed phonological model decreases the error rate by 8.32% relatively compared to the baseline triphone system. The morpheme error rate of the best configuration is 17.74% in a 1200 morpheme task with test set perplexity 70.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-397"
  },
  "caseiro02_icslp": {
   "authors": [
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Using dynamic WFST composition for recognizing broadcast news",
   "original": "i02_1301",
   "page_count": 4,
   "order": 398,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "Our first application of weighted finite state transducers to the recognition of broadcast news provided us with an interesting framework to study several problems related to the optimization of the search space. The paper starts by describing how the use of our lexicon and language model \"on-the-fly\" composition algorithm is crucial in extending the transducer approach to large systems. We present an efficient representation for WFSTs, that allowed us to reduce runtime memory requirements, and discuss several types of language model optimizations, including a context-sharing algorithm. Experimental results obtained with the broadcast news corpus collected for European Portuguese illustrate the impact of the various possible optimizations of the components on the performance of the system. A Comparison of Prefix Tree and Finite-State\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-398"
  },
  "dolfing02_icslp": {
   "authors": [
    [
     "Hans J. G. A.",
     "Dolfing"
    ]
   ],
   "title": "Transducer search space modelings for large-vocabulary speech recognition",
   "original": "i02_1305",
   "page_count": 4,
   "order": 399,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "In this paper, we compare a large-vocabulary speech decoder, based on a phonetic prefix tree, with a decoder based on finite-state transducers. On the example of a large-vocabulary, isolated word recognition task without language model, we investigate the error-rates based on different beamwidths for fully optimized, tree-like and factored finite-state transducer networks. The results show that the decoder, based on a fully optimized finite-state network, achieves the same error-rates as the prefix tree decoder, using roughly 50% of active states. In addition, we modify the traditional phonetic prefix tree to a new tree, compatible with standard LVCSR decoders but optimized across all hidden Markov models to remove redundancies, which achieves better error-rates at small beamwidths.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-399"
  },
  "kanthak02_icslp": {
   "authors": [
    [
     "Stephan",
     "Kanthak"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Mehryar",
     "Mohri"
    ]
   ],
   "title": "A comparison of two LVR search optimization techniques",
   "original": "i02_1309",
   "page_count": 4,
   "order": 400,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "This paper presents a detailed comparison between two search optimization techniques for large vocabulary speech recognition - one based on word-conditioned tree search (WCTS) and one based on weighted finite-state transducers (WFSTs). Existing North American Business News systems from RWTH and AT&T representing each of the two approaches, were modified to remove variations in model data and acoustic likelihood computation. An experimental comparison showed that the WFST-based system explored fewer search states and had less runtime overhead than the WCTS-based system for a given word error rate. This is attributed to differences in the pre-compilation, degree of non-determinism, and path weight distribution in the respective search graphs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-400"
  },
  "mohri02_icslp": {
   "authors": [
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "An efficient algorithm for the n-best-strings problem",
   "original": "i02_1313",
   "page_count": 4,
   "order": 401,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "We present an efficient algorithm for solving the n-best-strings problem in a weighted automaton. This problem arises commonly in speech recognition applications when a ranked list of unique recognizer hypotheses is desired. We believe this is the first n-best algorithm to remove redundant hypotheses before rather than after the n-best determination. We give a detailed description of the algorithm and demonstrate its correctness. We report experimental results showing its efficiency and practicality even for large n in a 40,000-word vocabulary North American Business News (NAB) task. In particular, we show that 1000-best generation in this task requires negligible added time over recognizer lattice generation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-401"
  },
  "xiang02_icslp": {
   "authors": [
    [
     "Bing",
     "Xiang"
    ],
    [
     "Toby",
     "Berger"
    ]
   ],
   "title": "Structural Gaussian mixture models for efficient text-independent speaker verification",
   "original": "i02_1317",
   "page_count": 4,
   "order": 402,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "Structural Gaussian mixture models (SGMMs) are proposed for effi- cient text-independent speaker verification. A structural background model (SBM) is constructed first by hierarchically clustering all Gaussian mixture components in a universal background model (UBM). In this way the acoustic space is partitioned into multiple regions in different levels of resolution. For each target speaker, a SGMM can be generated through multi-level maximum a posteriori (MAP) adaptation from the SBM. During test, only a small subset of Gaussian mixture components is scored for each feature vector in order to reduce the computational cost significantly. Furthermore, the scores obtained in different layers of the tree-structured models are combined via a neural network for final decision. Different configurations are compared in the experiments conducted on the telephony speech data used in the NIST speaker verification evaluation. The experimental results show that computational reduction by a factor of 17 can be achieved with equal error rate (EER) reduced by 8% compared with the baseline. The SGMM-SBM also shows some advantages over the recently proposed hash GMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-402"
  },
  "petry02_icslp": {
   "authors": [
    [
     "A.",
     "Petry"
    ],
    [
     "Dante A. C.",
     "Barone"
    ]
   ],
   "title": "Text-dependent speaker verification using lyapunov exponents",
   "original": "i02_1321",
   "page_count": 4,
   "order": 403,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "The characterization of a speech signal using nonlinear dynamical features has been focus of intense research lately. In this work, the results obtained with time-dependent largest Lyapunov exponents (TDLEs) in a text-dependent speaker verification task are reported. The baseline system used 10 cepstral coefficients and 10 delta cepstral coefficients, and it is shown how the addition of TDLEs can improve the systems accuracy. Cepstral mean subtraction (CMS) was applied to all features in the tests, as well as silence removal. The telephone speech corpus used, obtained from a subset of CSLU Speaker Recognition corpus, was composed by 91 different speakers, speaking the same sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-403"
  },
  "benzeghiba02_icslp": {
   "authors": [
    [
     "Mohamed F.",
     "BenZeghiba"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "User-customized password speaker verification based on HMM/ANN and GMM models",
   "original": "i02_1325",
   "page_count": 4,
   "order": 404,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "In this paper, we present a new approach towards user-customized password speaker verification combining the advantages of hybrid HMM/ANN systems, using Artificial Neural Networks (ANN) to estimate emission probabilities of Hidden Markov Models , and Gaussian Mixture Models. In the approach presented here, we indeed exploit the properties of hybrid HMM/ANN systems, usually resulting in high phonetic recognition rates, to automatically infer the baseline phonetic transcription (HMM topology) associated with the user customized password from a few enrollment utterances and using a large, speaker independent, ANN. The emission probabilities of the resulting HMMs are then modeled in terms of speaker specific/adapted multi-Gaussian HMMs or speaker specific/adapted ANN. In the proposed approach, the hybrid HMM/ANN system is used as a model for utterance (password) verification, while still using a speaker independent GMM for speaker verification. Results (EER) are compared to a state-of-the-art text-dependent approach, using multi-Gaussian HMMs only.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-404"
  },
  "xin02_icslp": {
   "authors": [
    [
     "Dong",
     "Xin"
    ],
    [
     "Zhaohui",
     "Wu"
    ],
    [
     "Yingchun",
     "Yang"
    ]
   ],
   "title": "Exploiting support vector machines in hidden Markov models for speaker verification",
   "original": "i02_1329",
   "page_count": 4,
   "order": 405,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "Hidden Markov Models have been proved to be an efficient way for statistically modeling sequence signals. And the Support Vector Machines seem to be a promising candidate to perform the classification task. A new method combining support vector machine and hidden Markov models is proposed. The output of support vector machines are modified as posterior probability using sigmoid function, and act as a probability evaluator in the hidden states of HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-405"
  },
  "mami02_icslp": {
   "authors": [
    [
     "Yassine",
     "Mami"
    ],
    [
     "Delphine",
     "Charlet"
    ]
   ],
   "title": "Speaker identification by location in an optimal space of anchor models",
   "original": "i02_1333",
   "page_count": 4,
   "order": 406,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "The process of speaker recognition is generally based on modeling the characteristics of each speaker. An interesting method for modeling consists in representing a new speaker, not in an absolute manner, but relatively to a set of well trained speaker models which constitutes the new representation space. This paper addresses the task of finding a good representation space for speaker identification. It describes a representation space built either by clustering speakers or by selecting an optimal subset of them. In this representation space, speaker location is then performed by the anchor models technique. We present experimental results and compare them with GMM-based results. We show that clustering and subset selection give good representation spaces. With a little amount of training data, identification by location in a space of virtual voices performs much better than GMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-406"
  },
  "park02b_icslp": {
   "authors": [
    [
     "Alex",
     "Park"
    ],
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "ASR dependent techniques for speaker identification",
   "original": "i02_1337",
   "page_count": 4,
   "order": 407,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "Traditional text independent speaker recognition systems are based on Gaussian Mixture Models (GMMs) trained globally over all speech from a given speaker. In this paper, we describe alternative methods for performing speaker identification that utilize domain dependent automatic speech recognition (ASR) to provide a phonetic segmentation of the test utterance. When evaluated on YOHO, several of these approaches were able outperform previously published results on the speaker ID task. On a more difficult conversational speech task, we were able to use a combination of classifiers to reduce identification error rates on single test utterances. Over multiple utterances, the ASR dependent approaches performed significantly better than the ASR independent methods. Using an approach we call speaker adaptive modeling for speaker identification, we were able to reduce speaker identification error rates by 39% over a baseline GMM approach when observing five test utterances from a speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-407"
  },
  "ding02b_icslp": {
   "authors": [
    [
     "Peng",
     "Ding"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Factor analyzed Gaussian mixture models for speaker identification",
   "original": "i02_1341",
   "page_count": 4,
   "order": 408,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "In this paper, the statistical method of Factor Analysis (FA) is studied on Gaussian Mixture Model (GMM) based speaker identification (SI) system to model the data covariance which is usually neglected due to the training data sparseness. Because the variance of GMM can represents speaker variability, it is very important in SI systems. By FA modeled the data covariance, a relative gain of 39.6% over GMM baseline can be seen at the same amount of training data. Parameter tying is important when data is sparse and is helpful to balance precision and generalization of models. Various tying strategies are studied in this paper too. With the best result, a relative gain of 48.9% gain can be seen. We also present some interpretations of the factors tentatively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-408"
  },
  "jin02_icslp": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Phonetic speaker identification",
   "original": "i02_1345",
   "page_count": 4,
   "order": 409,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper describes the exploration of text-independent speaker identification using novel approaches based on speakers phonetic features instead of traditional acoustic features. Different phonetic speaker identification approaches are discussed in this paper and evaluated using two speaker identification systems: one multilingual system and one single language multiple-engine system. Furthermore, text-independent speaker identification experiments are carried out on a distant-microphone database as well as gender identifi- cation experiments are investigated on the NIST 1999 Speaker Recognition Evaluation dataset. The results show that phonetic features are powerful for speaker identification and gender identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-409"
  },
  "navratil02_icslp": {
   "authors": [
    [
     "Jirí",
     "Navrátil"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ]
   ],
   "title": "DETAC: a discriminative criterion for speaker verification",
   "original": "i02_1349",
   "page_count": 4,
   "order": 410,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "This paper introduces a general criterion applicable to discriminative training of detection systems, and discusses its particular implementation in GMM-based text-independent speaker verification. Based on an analysis of the detection error trade-off curve of a baseline system, we argue that the new criterion extends several conventional methods such as the maximum posterior training by logistic regres- sion and the linear discriminative analysis projection, by a second aspect - \"reshaping\" the Bayes error area in favor of a relevant operating range. Optimization results with relative error reduction of up to 16% are presented on the cellular task of the NIST-2001 speaker recognition evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-410"
  },
  "liu02b_icslp": {
   "authors": [
    [
     "Ming",
     "Liu"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Bei-qian",
     "Dai"
    ]
   ],
   "title": "Hierarchical Gaussian mixture model for speaker verification",
   "original": "i02_1353",
   "page_count": 4,
   "order": 411,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "A novel type of Gaussian mixture model for text-independent speaker verification, Hierarchical Gaussian Mixture Model (HGMM) is proposed in this paper. HGMM aims at maximizing the efficiency of MAP training on the Universal Background Model (UBM). Based on the hierarchical structure, the parameters of one Gaussian component can also be adapted by the observation vectors of neighboring Gaussian components. HGMM can also be considered as a generalized GMM which replaces the Gaussian component in the GMM models with a local GMM. The hierarchical Gaussian mixture description of the local observation space is better than one single Gaussian distribution. Experiment on NIST 99 Evaluation corpus shows that the HGMM achieves an 18% relative reduction in EER compared with the conventional GMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-411"
  },
  "kochanski02_icslp": {
   "authors": [
    [
     "Greg",
     "Kochanski"
    ],
    [
     "Daniel",
     "Lopresti"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "A reverse turing test using speech",
   "original": "i02_1357",
   "page_count": 4,
   "order": 412,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "\"Hackers\" have written malicious programs to exploit online services intended for human users. As a result, service providers need a method to tell whether a web site is being accessed by a human or a machine. We expect a parallel scenario as spoken language interfaces become common.\n",
    "In this paper, we describe a Reverse Turing Test (i.e., an algorithm that can distinguish between humans and computers) using speech. We present a test that depends on the fact that human recognition of distorted speech is far more robust than automatic speech recognition techniques.\n",
    "Our analysis of 18 different sets of distortions demonstrates that there are a variety of ways to make the problem hard for machines. In addition, humans and speech recognition systems make different kinds of mistakes, and this difference can be employed to improve discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-412"
  },
  "ahn02b_icslp": {
   "authors": [
    [
     "Sungjoo",
     "Ahn"
    ],
    [
     "Sunmee",
     "Kang"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "On effective speaker verification based on subword model",
   "original": "i02_1361",
   "page_count": 4,
   "order": 413,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "This paper concerns an effective text-dependent speaker verification method to increase the performance of speaker verification. While various speaker verification methods have already been developed, their effectiveness has not yet been formally proven in terms of achieving acceptable performance level. This paper proposes a weighted likelihood procedure along with a confidence measure based on subword-based text-dependent speaker verification. Our aim is to remedy the low performance problem in speaker verification by exploring a means to strengthen the verification likelihood via subwordbased hypothesis criteria and weighted likelihood method. Experimental results show that the proposed speaker verification method outperforms that of the speaker verification scheme without using the proposed decision by a factor of up to 1.6 times. From these results, the proposed speaker verification method is shown to be very effective and to achieve a reliable performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-413"
  },
  "xiang02b_icslp": {
   "authors": [
    [
     "Bing",
     "Xiang"
    ]
   ],
   "title": "Speaker verification using Gaussian component strings in dynamic trajectory space",
   "original": "i02_1365",
   "page_count": 4,
   "order": 414,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "In this paper, a novel approach is presented for text-independent speaker verification. A quantized acoustic trajectory is constructed for each utterance based on Universal Background Model (UBM). Analysis of the speaker entropy in the trajectory space demonstrates that the segmental trajectory catches some speaker-specific information, which can be used to discriminate different speakers. Gaussian component strings are proposed to reflect the dynamic features in the segmental trajectory. Based on the occurring frequencies of different component strings in the non-target and target speaker data, a Universal Background Trajectory Model (UBTM) and multiple Target Trajectory Models (TTMs) are created for background and target speakers separately. The experiments are conducted on the telephony speech used in the NIST 1999 speaker verification evaluation. The experimental results show that the bi-component strings achieve better performance compared to the uni-component and tri-component strings. The trajectory models can be combined with a general speaker verification system to provide complementary information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-414"
  },
  "heck02_icslp": {
   "authors": [
    [
     "Larry P.",
     "Heck"
    ],
    [
     "Dominique",
     "Genoud"
    ]
   ],
   "title": "Combining speaker and speech recognition systems",
   "original": "i02_1369",
   "page_count": 4,
   "order": 415,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "This paper presents a general framework for the integration of speaker and speech recognizers. The framework poses the problem of combining speech and speaker recognizers as the joint maximization of the a posteriori probability of the word sequence and speaker given the observed utterance. It is shown that the posteriori probability can be expressed as the product of four terms: a likelihood score from a speaker-independent speech recognizer, the (normalized) likelihood score of a text-dependent speaker recognizer, the likelihood of a speaker-dependent statistical language model, and the prior probability of the speaker. Efficient search strategies are discussed, with a particular focus on the problem of recognizing and verifying name-based identity claims over very large populations (e.g., \"My name is John Doe\"). The efficient search approach uses a speaker-independent recognizer to first generate a list of top hypotheses, followed by a resorting of this list based on the combined score of the four terms discussed above. Experimental results on an over-the-telephone speech recognition task show a 34% reduction in the error rate where the test-set consists of users speaking their first and last name from a grammar covering 1 million unique persons.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-415"
  },
  "li02g_icslp": {
   "authors": [
    [
     "Qi",
     "Li"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Jinsong",
     "Zheng"
    ]
   ],
   "title": "Automatic enrollment for speaker authentication",
   "original": "i02_1373",
   "page_count": 4,
   "order": 416,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "Enrollment is a necessary session in speaker verification. Automatic verification of collected training utterances, so called automatic enrollment, is critical to the performance of any speaker verification system. In this paper, we propose one solution including two separate approaches for automatic enrollment. First, at the utterance level, we propose to use an N-best algorithm to perform spoken content verification for training utterance selection. Second, at the word level, we employ an accurate utterance verification algorithm to conduct word verification for training data selection. Finally, we combine the two techniques together and propose a solution for automatic enrollment. Our experiments show that the N-best approach and the utterance verification approach can provide very low error rates. Based on the experimental results, we expect that the combined solution is close to error free and is feasible for real-world applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-416"
  },
  "andorno02_icslp": {
   "authors": [
    [
     "M.",
     "Andorno"
    ],
    [
     "P.",
     "Laface"
    ],
    [
     "Roberto",
     "Gemello"
    ]
   ],
   "title": "Experiments in confidence scoring for word and sentence verification",
   "original": "i02_1377",
   "page_count": 4,
   "order": 417,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "The successful deployment of a telephone speech application cannot only rely on the accuracy of the recognition results, but also on their reliability. Reliable confidence measures are, thus, necessary in all practical applications to decide whether a recognized word - or sentence - should be accepted or rejected. Since most of the applications are based on continuous speech recognition, controlled by grammars, we present the results of a set of experiments aiming at assessing the quality and the limitations of different confidence measures for six different grammars that can be embedded in several applications. We show that using application independent con- fidence scoring techniques, good performance are obtained across all six grammars.\n",
    "We introduce also a sentence level confidence measure that allows a significant reduction of the system error rate due to ill-formed sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-417"
  },
  "huggins02_icslp": {
   "authors": [
    [
     "Mark C.",
     "Huggins"
    ],
    [
     "John J.",
     "Grieco"
    ]
   ],
   "title": "Confidence metrics for speaker identification",
   "original": "i02_1381",
   "page_count": 4,
   "order": 418,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "This paper presents a concept and experimental results for a novel confidence metric for speaker identification (SID). The concept is based on three major audio components: duration, signal-to-noise ratio, and model quality. This concept has been successfully applied to two benchmark closed-set SID systems which demonstrates the broad applicability of the technique. A subset of the TIMIT database was used for this investigation. Derivation and validation of the con- fidence performance are reported in this paper. We also investigate a fusion technique for the composite multi-component confidence metric. Reliability of the confidence metric has been determined to be 94.6% accurate on average.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-418"
  },
  "elenius02_icslp": {
   "authors": [
    [
     "Daniel",
     "Elenius"
    ],
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Characteristics of a low reject mode speaker verification system",
   "original": "i02_1385",
   "page_count": 4,
   "order": 419,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "The performance of a speaker verification (SV) system is normally determined by the false reject (FRR) and false accept (FAR) rates as averages on a population of test speakers. However, information on the FRR distribution is required when estimating the portion of clients that will suffer from an unacceptably high reject rate. This paper studies this distribution in a population using a SV system operating in low reject mode. Two models of the distribution are proposed and compared with test data. An attempt is also made to tune the decision threshold in order to obtain a desired portion of clients having a reject rate lower than a specified value.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-419"
  },
  "bernstein02_icslp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Denis K.",
     "Burnham"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Special session: issues in audiovisual spoken language processing (when, where, and how?)",
   "original": "i02_1445",
   "page_count": 4,
   "order": 420,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "The many aspects of audiovisual (AV) speech processing have attracted a loyal following from a wide range of perspectives and disciplines. Beginning with the landmark NATO workshop in 1995 [50], a series of international special sessions and meetings has fostered the development of numerous lines of AV speech research. This paper is an introduction to the special session and an inventory of several developments in the area. The introduction is organized around three questions concerning human and machine performance-when, where, and how AV speech processing occurs-and a summary of applications, including AV speech recognition and synthesis, AV processing with cochlear implants, and AV effects observed in second language learners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-420"
  },
  "deligne02_icslp": {
   "authors": [
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ]
   ],
   "title": "Audio-visual speech enhancement with AVCDCN (audio-visual codebook dependent cepstral normalization)",
   "original": "i02_1449",
   "page_count": 4,
   "order": 421,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "In this paper, we introduce a non-linear enhancement technique called Audio-Visual Codebook Dependent Cepstral Normalization (AVCDCN) and we consider its use with both audio-only and audiovisual speech recognition.\n",
    "AVCDCN is inspired from CDCN [1] [2], an audio-only enhancement technique that approximates the non-linear effect of noise on speech with a piece-wise constant function. Our experiments show that the use of visual information in AVCDCN allows significant performance gains over CDCN.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-421"
  },
  "bailly02_icslp": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Audiovisual speech synthesis. from ground truth to models",
   "original": "i02_1453",
   "page_count": 4,
   "order": 422,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "We present here the main approaches used to synthesize and drive talking faces. Illustrative systems are described. We distinguish between facial synthesis itself (i.e the manner in which facial movements are rendered on a computer screen), and the way these movements may be controlled and predicted using phonetic input. We then focus on the necessity to capture, model and render with maximum fidelity the intimate coherence of the facial deformations observed on a human face.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-422"
  },
  "vatikiotisbateson02_icslp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Harold",
     "Hill"
    ],
    [
     "Miyuki",
     "Kamachi"
    ],
    [
     "Karen",
     "Lander"
    ],
    [
     "Kevin G.",
     "Munhall"
    ]
   ],
   "title": "The stimulus as basis for audiovisual integration",
   "original": "i02_1457",
   "page_count": 4,
   "order": 423,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "We argue here that examination of the stimulus source is a prerequisite to understanding how audiovisual (AV) stimuli are processed perceptually. This is based on mounting evidence that the act of speech production generates multimodal events whose audible and visible components are highly correlated with each other and the vocal tract source. How this multimodal structuring is exploited perceptually, or not, needs to be demonstrated by conducting studies that take the properties of the stimulus source into account.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-423"
  },
  "rosenblum02_icslp": {
   "authors": [
    [
     "Lawrence D.",
     "Rosenblum"
    ]
   ],
   "title": "The perceptual basis for audiovisual speech integration",
   "original": "i02_1461",
   "page_count": 4,
   "order": 424,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "One question central to understanding the perceptual basis of audiovisual speech integration concerns where in the process the audio and visual streams are combined. Over 20 years of research on this question have provided answers ranging from the integration occurring at the level of the informational input, to occurring only after segment matches are made independently for each modality. In this paper I will present a modality-neutral account of audiovisual speech integration. From this account, speech perception is inherently amodal, and the information for speech is considered kinematic primitives that can be instantiated in any modality. Modality is considered to be invisible to the speech function, and integration occurs as a function of the input information itself. Four classes of support for a modality-neutral account will be presented including a) the primacy/ubiquity of multimodal speech; b) evidence for very early integration from the behavioral and c) neuropsychological data; and d) evidence for informational similitude existent across modalities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-424"
  },
  "hardison02b_icslp": {
   "authors": [
    [
     "Debra M.",
     "Hardison"
    ]
   ],
   "title": "Sources of variability in the perceptual training of /r/ and /l/: interaction of adjacent vowel, word position, talkers² visual and acoustic cues",
   "original": "i02_1465",
   "page_count": 4,
   "order": 425,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "This paper reports some of the findings of a three-week perceptual training experiment involving Japanese and Korean learners of English as a second language. Other studies have shown that under appropriate conditions, adults can learn to identify nonnative speech sounds such as American English /r/ and /l/ with generalization to novel stimuli, transfer to new tasks and retention. Such conditions of \"high variability\" have included multiple talkers voices and a large stimulus training set involving these sounds across word positions. The current study investigated additional sources of variability by exploring the effects of the adjacent vowel and talkers facial cues on perceptual training. Thus, word position, adjacent vowel, and training type [auditory-visual (AV) vs. auditory-only (A-only); multiple talker vs. single talker] were independent variables. Results indicated significantly greater improvement in identification accuracy for AV vs. A-only training. Visual input contributed the most to the bimodal percept for the more difficult pre-training phonetic environments. Significant effects were found for vocalic environment as well as word position and training talker. Findings also revealed successful transfer to novel stimuli, a new talker, production improvement, and earlier word identification in connected speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-425"
  },
  "hazan02_icslp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Anke",
     "Sennema"
    ],
    [
     "Andrew",
     "Faulkner"
    ]
   ],
   "title": "Audiovisual perception in L2 learners",
   "original": "i02_1685",
   "page_count": 4,
   "order": 426,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "This study investigates the use of audiovisual cues in the perception of sound contrasts which have a different phonemic status in the listeners L1 and L2. Two contrasts differing in the distinctiveness of their visual gestures (/b/-/v/ and /p/-/b/) were presented to Spanish learners of English in audio, visual and audiovisual modalities. Overall identification rates were not significantly higher audiovisually than in the audio alone condition for either contrast. For the /b/-/v/ contrast, which is visually marked, listeners showed different patterns of performance. A subset of listeners appeared to have acquired the L2 /b/-/v/ contrast and were sensitive to both the acoustic and visual cues marking the contrast. Those at an earlier stage of acquisition of the L2 contrast generally showed poor sensitivity to the visual as well as the acoustic cues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-426"
  },
  "kirk02_icslp": {
   "authors": [
    [
     "Karen Iler",
     "Kirk"
    ],
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Lorin",
     "Lachs"
    ]
   ],
   "title": "Audiovisual integration of speech by children and adults with cochlear implants",
   "original": "i02_1689",
   "page_count": 4,
   "order": 427,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "The present study examined how prelingually deafened children and postlingually deafened adults with cochlear implants (CIs) combine visual speech information with auditory cues. Performance was assessed under auditory-alone (A), visual-alone (V), and combined audiovisual (AV) presentation formats. A measure of visual enhancement, RA, was used to assess the gain in performance provided in the AV condition relative to the maximum possible performance in the auditory-alone format. Word recognition was highest for AV presentation followed by A and V, respectively. Children who received more visual enhancement also produced more intelligible speech. Adults with CIs made better use of visual information in more diffi- cult listening conditions (e.g., when multiple talkers or phonemically similar words were used). The findings are discussed in terms of the complementary nature of auditory and visual sources of information that specify the same underlying gestures and articulatory events in speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-427"
  },
  "sekiyama02_icslp": {
   "authors": [
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Yoichi",
     "Sugita"
    ]
   ],
   "title": "Auditory-visual speech perception examined by brain imaging and reaction time",
   "original": "i02_1693",
   "page_count": 4,
   "order": 428,
   "p1": "1693",
   "pn": "1696",
   "abstract": [
    "By using the McGurk effect [1], we compared brain activation during audiovisual (AV) speech perception for two sets of conditions differing in the intelligibility of auditory speech (High vs. Low). In the Low intelligibility condition in which speech was harder to hear, the McGurk effect, the visual influence, was much stronger. Functional magnetic resonance imaging (fMRI) also showed that speechreadingrelated visual areas (the left MT and left intraparietal sulcus as observed in the video-only condition) were strongly activated in the Low intelligibility AV condition but not in the High intelligibility AV condition. Thus visual information of the mouth movements was processed more intensively when speech was harder to hear. Reaction time data suggested that when auditory speech is easier to hear, there is a top-down suppression of visual processing that starts earlier than auditory processing. On the other hand, when auditory speech was less intelligible, reaction time data were such that visual mouth movements served as a priming cue. These results provide an insight into a time-spanned scope of the integration process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-428"
  },
  "ponton02_icslp": {
   "authors": [
    [
     "Curtis W.",
     "Ponton"
    ],
    [
     "Edward T.",
     "Auer"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Neurocognitive basis for audiovisual speech perception: evidence from event-related potentials",
   "original": "i02_1697",
   "page_count": 4,
   "order": 429,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "Knowledge about evoked potentials can be deployed to help explain audiovisual (AV) speech perception. The auditory and visual neural pathways are described in this paper, with emphasis on eventrelated potentials (ERPs) and their generation sites. The locations, interconnections, and neurophysiology of the auditory and visual pathways set limits on how and were information from auditory and visual speech signals combines. It is hypothesized that AV enhancement effects can occur at various levels of the cortical pathway, due to subcortical and to trans-cortical connections. Temporally early effects are likely localized to sub-cortical AV interactions that are not specialized for speech. Later temporal effects are more likely attributable to trans-cortical connections and feedback, including speech-specific processing. An experiment was conducted to examine the time course and cortical locations for AV speech interactions. ERPs were obtained during auditory-only (A), visual-only (V), and AV speech conditions. Dipole source modeling was used to localize and measure responses in auditory and visual cortical areas. At an early latency (100 ms), dipoles placed to measure superior temporal cortex activity showed enhancement (super-additivity relative to the sum of A and V responses) to AV speech; whereas, at a later latency (210 ms), the right occipital cortex dipole also showed super-additivity. Given the constraints of the cortical pathways, the early enhancement is interpreted as due to thalamo-cortical AV interactions, whereas, the later enhancement is interpreted as due to trans-cortical AV interactions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-429"
  },
  "lewkowicz02_icslp": {
   "authors": [
    [
     "David J.",
     "Lewkowicz"
    ]
   ],
   "title": "Perception and integration of audiovisual speech in human infants",
   "original": "i02_1701",
   "page_count": 4,
   "order": 430,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "Talking faces constitute a major part of an infants perceptual experience. Through the process of watching and listening while people speak to them, infants have the opportunity to acquire cognitive, linguistic, social, and emotional skills. The acquisition of these higher-level skills depends, in part, on infants ability to perceive and integrate the audible and visible attributes of speech. The present paper describes the findings from a series of studies examining infants perception of audiovisual speech. The focus of these studies was to determine how the audible and visible attributes of speech contribute to infants perception of audiovisual speech across the first year of life. The findings show that responsiveness to various features of faces and voices changes throughout early human development and that intersensory integration of these features depends on the nature of the information, its source modality, and the infants developmental age.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-430"
  },
  "bailly02b_icslp": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Seeing tongue movements from outside",
   "original": "i02_1913",
   "page_count": 4,
   "order": 431,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "Previous studies [10, 11, 20] have shown that a part of the tongue posture variance may be recovered from visible facial movement. Confronting articulatory models of the vocal tract (VT) and of the facial movements to cineradiographic data for the same subject, we examine the visible consequences of speech articulation, and conversely we determined the characteristics of the VT that can directly and robustly be captured from facial movements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-431"
  },
  "wojdel02_icslp": {
   "authors": [
    [
     "Jacek C.",
     "Wojdel"
    ],
    [
     "Pascal",
     "Wiggers"
    ],
    [
     "Leon J.M.",
     "Rothkrantz"
    ]
   ],
   "title": "An audio-visual corpus for multimodal speech recognition in dutch language",
   "original": "i02_1917",
   "page_count": 4,
   "order": 432,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "This paper describes the gathering and availability of an audio-visual speech corpus for Dutch language. The corpus was prepared with the multi-modal speech recognition in mind and it is currently used in our research on lip-reading and bimodal speech recognition. It contains the prompts used also in the well established POLYPHONE corpus and therefore captures the Dutch language characteristics with a reasonable accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-432"
  },
  "wiggers02_icslp": {
   "authors": [
    [
     "Pascal",
     "Wiggers"
    ],
    [
     "Jacek C.",
     "Wojdel"
    ],
    [
     "Leon J.M.",
     "Rothkrantz"
    ]
   ],
   "title": "Medium vocabulary continuous audio-visual speech recognition",
   "original": "i02_1921",
   "page_count": 4,
   "order": 433,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "This paper presents our experiments on continuous audiovisual speech recognition. A number of bimodal systems using feature fusion or fusion within Hidden Markov Models are implemented. Experiments with different fusion techniques and their results are presented. Further the performance levels of the bimodal system and a unimodal speech recognizer under noisy conditions are compared.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-433"
  },
  "heckmann02_icslp": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Kristian",
     "Kroschel"
    ],
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "DCT-based video features for audio-visual speech recognition",
   "original": "i02_1925",
   "page_count": 4,
   "order": 434,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "Encouraged by the good performance of the DCT in audiovisual speech recognition [1], we investigate how the selection of the DCT coeffi- cients influences the recognition scores in a hybrid ANN/HMM audiovisual speech recognition system on a continuous word recognition task with a vocabulary of 30 numbers. Three sets of coefficients, based on the mean energy, the variance and the variance relative to the mean value, were chosen. The performance of these coefficients is evaluated in a video only and an audio-visual recognition scenario with varying Signal to Noise Ratios (SNR). The audio-visual tests are performed with 5 types of additional noise at 12 SNR values each. Furthermore the results of the DCT based recognition are compared to those obtained via chroma-keyed geometric lip features [2]. In order to achieve this comparison, a second audio-visual database without chroma-key has been recorded. This database has similar content but a different speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-434"
  },
  "erdener02_icslp": {
   "authors": [
    [
     "V. Dogu",
     "Erdener"
    ],
    [
     "Denis K.",
     "Burnham"
    ]
   ],
   "title": "The effect of auditory-visual information and orthographic background in L2 acquisition",
   "original": "i02_1929",
   "page_count": 4,
   "order": 435,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "Visual information from the lips and face is an integral part of speech perception. In addition, orthography can play a role in disambiguating the speech signal in foreign/second language (L2) perception and production. The current study investigates the effect of auditory and visual speech information and orthographic depth, the degree to which a language is transparent (high phoneme-grapheme correspondence), or opaque (low phoneme-grapheme correspondence) on L2 acquisition. Speakers of Turkish and Australian English (transparent and opaque orthographies, respectively) were tested for their production of legal non-words in Spanish and Irish (transparent and opaque orthographies, respectively). Transparent orthographic input (Spanish) enhanced pronunciation in L2, and orthographic reproduction. Native speaker ratings of the participants productions also revealed that orthographic input improves accent. Overall results confirm previous findings that visual information enhances speech perception and production, and extend previous results to show the facilitative effects of orthographic input in L2 acquisition under certain conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-435"
  },
  "krahmer02_icslp": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Zsófia",
     "Ruttkay"
    ],
    [
     "Marc",
     "Swerts"
    ],
    [
     "Wieger",
     "Wesselink"
    ]
   ],
   "title": "Perceptual evaluation of audiovisual cues for prominence",
   "original": "i02_1933",
   "page_count": 4,
   "order": 436,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "This paper1 reports on two experiments with a Talking Head that explore the ability of eyebrow movements to cue focus. The first experiment tests how listeners react to synthetic stimuli in which the eyebrow movements coincide with pitch accents versus those in which these two occur on different words. Results show that subjects prefer those utterances in which pitch and eyebrow movements are aligned on the same word. The second experiment investigates whether listeners are sensitive to eyebrow movements when they have to rate the prominence of particular words in audiovisual stimuli. This experiment shows that eyebrow movements both boost the perceived prominence of words that also receive a pitch accent, and downscale the prominence of unaccented words in the immediate context of the accented word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-436"
  },
  "schwartz02_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "Audio-visual scene analysis: evidence for a \"very-early\" integration process in audio-visual speech perception",
   "original": "i02_1937",
   "page_count": 4,
   "order": 437,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "Recent experiments suggest that audio-visual interaction in speech perception could begin at a very early level, in which the visual input could improve the detection of speech sounds embedded in noise [1]. We show here that the \"speech detection\" benefit may result in a \"speech identification\" benefit different from lipreading per se. The experimental trick consists in using a series of lip gestures compatible with a number of different audio configurations, e.g. [y u ty tu ky ku dy du gy gu] in French. We show that the visual identification of this corpus is random, but, when added to the sound merged in a large amount of cocktail-party noise, vision happens to improve the identification of one phonetic feature, i.e. plosive voicing. We discuss this result in terms of audio-visual scene analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-437"
  },
  "zelezny02_icslp": {
   "authors": [
    [
     "Milos",
     "Zelezný"
    ],
    [
     "Petr",
     "Císar"
    ],
    [
     "Zdenek",
     "Krnoul"
    ],
    [
     "Jan",
     "Novák"
    ]
   ],
   "title": "Design of an audio-visual speech corpus for the czech audio-visual speech synthesis",
   "original": "i02_1941",
   "page_count": 4,
   "order": 438,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "Our long-term goal is to design a system for the Czech visual synthesis, that means an animated synthetic face (often called talking head) imitating pronouncing of a speech by a human being. In this paper we present techniques used for acquiring data and building the audio-visual speech corpus, especially its visual part. This process involves the recording of stereoscopic video data and solving of related problems as synchronization. Apart from that, we present simple method of utilization of such corpus using stereo vision principles and modelling shape of the lips by simple triangular mesh.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-438"
  },
  "attina02_icslp": {
   "authors": [
    [
     "Virginie",
     "Attina"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ]
   ],
   "title": "Coordination of hand and orofacial movements for CV sequences in French cued speech",
   "original": "i02_1945",
   "page_count": 4,
   "order": 439,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "This work aims at investigating the coordination in space and time between manual and orofacial gestures involved in the French Cued Speech (FCS) \"Langage Parlé Complété\", an efficient method of communication for hearing-impaired people. Cued CV syllabic sequences were analyzed. Results showed (i) five distinctive positions for vowels and (ii) manual anticipation with respect to lip movements and sound, the hand code being realized at the beginning of the CV syllable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-439"
  },
  "attina02b_icslp": {
   "authors": [
    [
     "Virginie",
     "Attina"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Denis",
     "Beautemps"
    ]
   ],
   "title": "Controling anticipatory behavior for rounding in French cued speech",
   "original": "i02_1949",
   "page_count": 4,
   "order": 440,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "\"Langage Parlé Complété (LPC)\" is the French manual system - corresponding to Cued Speech - used to complement lip reading and thus enhance speech perception for hearing-impaired people. In an anticipatory rounding context, a French speaker was audiovisually recorded while pronouncing and coding [i#yi] sequences with two different pause durations. The effect of pause duration on hand and lip temporal organization was investigated in relation to the wellknown anticipatory phenomenon of the lip gestures on the acoustic signal. Results showed that: (i) the manual cue follows the temporal organization of visible speech; (ii) the manual target position is always ahead of the corresponding lip target.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-440"
  },
  "sodoyer02_icslp": {
   "authors": [
    [
     "David",
     "Sodoyer"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Christian",
     "Jutten"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audio-visual speech sources separation: a new approach exploiting the audio-visual coherence of speech stimuli",
   "original": "i02_1953",
   "page_count": 4,
   "order": 441,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "We present a new approach to the source separation problem in the case of multiple speech signals. The method is based on the use of automatic lipreading: the objective is to extract an acoustic speech signal from other acoustic signals by exploiting its coherence with the speakers lip movements. We show how, if a statistical model of the joint probability of visual and spectral audio input is learnt to quantify the audio-visual coherence, separation can be achieved by maximising this probability. Then, we present a number of separation results on a corpus of vowel-plosive-vowel sequences uttered by a single speaker, embedded in a mixture of other voices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-441"
  },
  "house02_icslp": {
   "authors": [
    [
     "David",
     "House"
    ]
   ],
   "title": "Intonational and visual cues in the perception of interrogative mode in Swedish",
   "original": "i02_1957",
   "page_count": 4,
   "order": 442,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "This paper presents results from two perception experiments designed to investigate intonational cues and visual facial cues to interrogative mode in Swedish. Results from the intonation test indicate that both a widened F0 range on a final focal accent and time alignment properties of the F0 rise and peak make important contributions to the interrogative percept. Results from the audiovisual test showed that vertical head nodding and smiling tended to reinforce declarative intonation while interrogative intonation was not strengthened by hypothesized interrogative visual cues consisting of eyebrow movement and slow vertical head movement. The interaction between audio and visual cues for accentuation and interrogative mode is discussed and some implications of adding the visual modality to the traditional definition of question intonation are explored.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-442"
  },
  "lucey02_icslp": {
   "authors": [
    [
     "Simon",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Vinod",
     "Chandran"
    ]
   ],
   "title": "A link between cepstral shrinking and the weighted product rule in audio-visual speech recognition",
   "original": "i02_1961",
   "page_count": 4,
   "order": 443,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "The weighted product rule has been shown empirically to be of great benefit in audio-visual speech recognition (AVSR), for isolated word recognition tasks. A firm theoretical basis for the selection of effective weights is of considerable interest to the audio-visual speech processing community. In this paper a clear link is established between the selection of effective weightings and the approximately isotropic shrinkage that the distribution of acoustic cepstral features undergo in the presence of additive noise. An elucidation of the theoretical relationship between the cepstral shrinkage and the variance of the HMM audio log-likelihoods is then explored.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-443"
  },
  "endo02_icslp": {
   "authors": [
    [
     "Taku",
     "Endo"
    ],
    [
     "Nigel",
     "Ward"
    ],
    [
     "Minoru",
     "Terada"
    ]
   ],
   "title": "Can confidence scores help users post-editing speech recognizer output?",
   "original": "i02_1469",
   "page_count": 4,
   "order": 444,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "When dictating with speech recognition, most of the users time is spent correcting errors. To decrease the burden we propose new editor functions specifically to speed up the correction process. The idea is to use a recognition confidence measure to predict which words are likely to be in error, to display that information to the user by highlighting suspect words, and to provide a command to let the user jump the cursor to the next suspect word. Simple experiments suggest that these functions can be valuable, even with todays speech recognizers and confidence measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-444"
  },
  "watanabe02c_icslp": {
   "authors": [
    [
     "Masatoshi",
     "Watanabe"
    ],
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "Information retrieval based on speech recognition results",
   "original": "i02_1473",
   "page_count": 4,
   "order": 445,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "This paper proposes a new information retrieval method based on speech recognition results. Nowadays, there is huge speech data like news speech in TV and Radio broadcasts, etc. However, it is not easy to retrieve necessary information from these data because extra informations for retrieval are not attached to most of these data. To solve the problems, in this paper a Fuzzy retrieval method based on speech recognition results is proposed. As speech data is converted into the character data and character-based query is retrieved on the character data, the query can be quickly retrieved. It is necessary to consider the speech recognition error at the retrieval. This paper proposes two techniques considering the speech recognition errors; the first technique is based on the phoneme error tendency in the speech recognition results and the second technique is based on the phoneme spectral distance. This paper shows the effectiveness of Fuzzy retrieval through evaluation experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-445"
  },
  "lemmela02_icslp": {
   "authors": [
    [
     "Saija-Maaria",
     "Lemmelä"
    ],
    [
     "Péter Pál",
     "Boda"
    ]
   ],
   "title": "Efficient combination of type-in and wizard-of-oz tests in speech interface development process",
   "original": "i02_1477",
   "page_count": 4,
   "order": 446,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "Effective methods and tools are needed for designing, developing and testing speech-enabled applications. Traditional methods have been collecting natural dialogues, using Wizard-of-Oz experiments and different type-in tests. Collecting and analysing natural dialogues is always time consuming and in many situations even impossible. Wizard-of-Oz studies need well defined test scenarios and a lot of time to collect and analyse data. Type-in studies are one possibility to collect big amount of data quickly and analyse it semiautomatically. This paper discusses Wizard-of-Oz and type-in data collection methods as implemented at our site. The test set-ups and the identified discrepancies are discussed in details. As our study indicates, testing an initial version of a dialogue flow and the corresponding grammar with the type-in method offers quite reliable measures about the final implementation of the interface for a speech- enabled navigation application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-446"
  },
  "macherey02_icslp": {
   "authors": [
    [
     "Wolfgang",
     "Macherey"
    ],
    [
     "Jörg",
     "Viechtbauer"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Probabilistic retrieval based on document representations",
   "original": "i02_1481",
   "page_count": 4,
   "order": 447,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "Accessing information in multimedia databases encompasses a wide range of applications in which spoken document retrieval (SDR) plays an important role. In the recent past, research increasingly focused on the development of heuristic and probabilistic retrieval metrics that are suitable for retrieving spoken documents. So far, many heuristic retrieval metrics, e.g. the SMART-2 metric, have been proven to be more efficient than most advanced statistical approaches to SDR. In this paper, we propose a new probabilistic approach that is based on interpolations between document representations. This approach can be interpreted as a sort of nearest neighbor concept between documents, where a query is treated as a document. Experiments performed on the TREC-7 and TREC-8 SDR task show comparable or even better results than the SMART-2 metric.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-447"
  },
  "nishimoto02_icslp": {
   "authors": [
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "Radiodoc: a voice-accessible document system",
   "original": "i02_1485",
   "page_count": 4,
   "order": 448,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "We propose a voice-accessible document system and the document format called RadioDoc, which will be simple for both novice and expert users to use. As novice users often have no knowledge of the available commands or hesitate to use them, the RadioDoc system is designed so that they need not use any speech input at all. Expert users, comfortable with voice command, may move back and forth within the document as with CD-player, or use the voice shortcut where the system jumps ahead to the specific information immediately without going through the entire program. We also designed the RadioDoc language and created a tool which converts RadioDoc documents to VoiceXML applications. As the results of our experiments, the success of our system is found in the wide range of the expertise of the users. The development process using RadioDoc language is also turned out to be productive.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-448"
  },
  "goto02_icslp": {
   "authors": [
    [
     "Masataka",
     "Goto"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Speech completion: on-demand completion assistance using filled pauses for speech input interfaces",
   "original": "i02_1489",
   "page_count": 4,
   "order": 449,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "This paper describes a novel speech interface function, called speech completion, that helps a user enter a word or phrase by completing (filling in the rest of) a phrase fragment uttered by the user. Although the concept of completion is widely used in text-based interfaces, there have been no reports of completion being effectively applied to speech. By using a filled pause, we enable a user to effortlessly invoke the speech-completion function which helps the user recall uncertain phrases and saves labor when the input phrase is long. When a user hesitates by lengthening a vowel (a filled pause is uttered) during a phrase, our system immediately displays completion candidates whose beginnings acoustically resemble the uttered fragment so that the user can select the correct one. In our experiments with a system that included a filled-pause detector and a speech recognizer capable of listing candidates, the effectiveness of speech completion was confirmed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-449"
  },
  "wilkie02_icslp": {
   "authors": [
    [
     "Jenny",
     "Wilkie"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "Peter",
     "Littlewood"
    ]
   ],
   "title": "Design of system-initiated digressive proposals for automated banking dialogues",
   "original": "i02_1493",
   "page_count": 4,
   "order": 450,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "System-initiated proposals may be used to introduce new and unsolicited information into the dialogue flow of an automated telephone service in order to advise callers about products in which they may be interested such as short-term loans or overdrafts. Important dialogue design issues surrounding the introduction of such digressive proposals include how to interrupt the callers and where in the dialogue flow it is most suitable to locate a proposal. This paper describes the results from two experiments using a spoken natural language telephone banking application where two delivery strategies and three contrasting locations were investigated. Results showed that the delivery strategy had a stronger effect than the location.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-450"
  },
  "toth02_icslp": {
   "authors": [
    [
     "Arthur R.",
     "Toth"
    ],
    [
     "Thomas K.",
     "Harris"
    ],
    [
     "James",
     "Sanders"
    ],
    [
     "Stefanie",
     "Shriver"
    ],
    [
     "Roni",
     "Rosenfeld"
    ]
   ],
   "title": "Towards every-citizen²s speech interface: an application generator for speech interfaces to databases",
   "original": "i02_1497",
   "page_count": 4,
   "order": 451,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "One of the acknowledged impediments to the widespread use of speech interfaces is the portability problem, namely the considerable amount of labor, data and expertise needed to develop such interfaces in new domains. Under the Universal Speech Interface (USI) project, we have designed unified look-and-feel speech interfaces that employ semi-structured interaction and thus obviate the need for data collection. More importantly, the unified structure of USI-compliant interfaces makes possible the automatic generation of new interfaces from a terse, high-level specification. In this paper, we describe an application generator and accompanying toolkit that allow even non-programmers to generate and use fully functional speech interfaces to their chosen database in less than 15 minutes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-451"
  },
  "iyer02_icslp": {
   "authors": [
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Jeffrey",
     "Ma"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Owen",
     "Kimball"
    ]
   ],
   "title": "Training topic classifiers for conversational speech with limited data",
   "original": "i02_1501",
   "page_count": 4,
   "order": 452,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "In this paper we demonstrate how automatically generated transcriptions can be used to develop an effective topic classification application. Two key contributions of our work are (a) investigating the impact of unsupervised transcriptions on topic classification where the transcription system has been trained with very limited amounts of data, and (b) demonstrating the use of mixture language models that significantly improve topic classification performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-452"
  },
  "nishizaki02_icslp": {
   "authors": [
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Comparing isolately spoken keywords with spontaneously spoken queries for Japanese spoken document retrieval",
   "original": "i02_1505",
   "page_count": 4,
   "order": 453,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "This paper describes a Japanese spoken document retrieval system that uses voice input queries. We prepare two types of spoken queries: isolately spoken keywords and spontaneously spoken queries. To solve a mis-recognition problem of spoken queries, N-best hypotheses of transcripts of queries are used, and keyword candidates are selected from them by mutual information between recognized words. Using both an effective keyword selecting algorithm and spontaneously spoken queries instead of isolately spoken keywords, a re- trieval performance in F-measure improves to 73.4 from72.9, and using N-best hypotheses by recognizing queries, the performance also improves to 73.4 from 68.7.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-453"
  },
  "lai02_icslp": {
   "authors": [
    [
     "Jennifer C.",
     "Lai"
    ],
    [
     "Kwan Min",
     "Lee"
    ]
   ],
   "title": "Choosing speech or touchtone modality for navigation within a telephony natural language system",
   "original": "i02_1509",
   "page_count": 4,
   "order": 454,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "In this paper, we describe the empirical findings from a user study (N=16) that compares the use of touchtone and speech modalities to navigate within a telephone-based message retrieval system. Unlike previous studies comparing these two modalities, the speech system used was a working natural language system. Results indicate that a majority of users preferred interacting by speech even though the touchtone interface was more efficient as measured by the success rate and speed for each task. The interaction with the speech modality was rated as being more satisfying, more entertaining, and more natural than the touchtone modality. This is an interesting demonstration of how user satisfaction may not always correspond completely with efficiency measures in task-based scenarios. The findings underline the importance of examining speech interfaces from other perspectives in addition to efficiency maximization. The paper also describes a speech- based telephony application, which is used for access to unified messaging (email, voicemail, and faxes) and calendar information. The application is currently deployed at IBM Research and in use by more than 200 users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-454"
  },
  "lo02_icslp": {
   "authors": [
    [
     "Wai-Kit",
     "Lo"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Multi-scale and multi-model integration for improved performance in Chinese spoken document retrieval",
   "original": "i02_1513",
   "page_count": 4,
   "order": 455,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "This paper describes our attempt to combine the relative merits of different indexing units (scales) and different retrieval models to improve performance in Chinese spoken document retrieval. Our study includes indexing units from three scales: words, character bigrams and syllable bigrams. We also include two different retrieval models: the HMM-based model and the vector space model (VSM). Our retrieval task is based on the TDT-2 Mandarin collection - news text is used to retrieve relevant Mandarin audio. We experimented with different scales and retrieval models. The HMM-based model retrieves better at the word scale (mAP=0.566). For the VSM, better performance is obtained at the character bigram scale (mAP=0.562). We proceeded with a series of integration experiments where the ranked retrieval lists from different runs are combined by rank-based rescoring. The best retrieval performance (mAP=0.591) is achieved when we integrate the HMM-word and VSM-character configurations. These results suggest that retrieval based on different scales and different models capture different kinds of knowledge, which can be integrated to improve retrieval performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-455"
  },
  "ogata02b_icslp": {
   "authors": [
    [
     "Kohichi",
     "Ogata"
    ],
    [
     "Yorinobu",
     "Sonoda"
    ]
   ],
   "title": "Development of a GUI-based articulatory speech synthesis system",
   "original": "i02_1517",
   "page_count": 4,
   "order": 456,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "The authors have developed a speech synthesis system based on articulation in order to study relationships between the human vocal tract shapes and speech signals. The articulatory speech synthesizer proposed by Sondhi and Schroeter is used to produce speech sounds in the system. By using a GUI (Graphical User Interface) technique, the system can provide easy and interactive operation for users. Therefore, the system is useful not only for speech research but also for teaching materials in speech science. In this paper, effectiveness of the system for understanding articulatory-to-acoustic relationships is shown for steady vowels. Modification of the system for producing continuous vowels is also shown with more realistic description of vocal tract shapes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-456"
  },
  "dang02_icslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Investigation of coarticulation based on electromagnetic articulographic data",
   "original": "i02_1521",
   "page_count": 4,
   "order": 457,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "Coarticulation between the speech organs is a natural phenomenon in human speech. To realize this mechanism in our physiological articulatory model [1], this study attempts to analyze coarticulation involved in continuous speech based on electromagnetic articulographic (EMA) data recorded from three Japanese male subjects. The spatial target of each phoneme in CVC and VCV segments out of the sentences is determined using articulatory and acoustical cues, where V denotes five Japanese vowels, and C represents ten apical (including dental, alveolar, and postalveolar consonants) and two palatal consonants, respectively. A multiple regression analysis was applied to the extracted targets to evaluate \"contributions\" of the surrounding phonemes to the concerned target. It is found that the target of the central phoneme can be represented by a linear function of the preceding and following phonemes with a high accuracy. The result obtained form the segments with the apicals supports Öhmans conclusion derived from VCV utterances, that is, the articulation can be represented by a basic diphthongal gesture with an independent consonant gesture superimposed on its transitional portion [2]. The result from the segments with palatals suggested that the coarticulation caused by deformation was stronger than that induced by anticipation in the target planning stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-457"
  },
  "niikawa02_icslp": {
   "authors": [
    [
     "Takuya",
     "Niikawa"
    ],
    [
     "Takanori",
     "Ando"
    ],
    [
     "Masafumi",
     "Matsumura"
    ]
   ],
   "title": "Frequency dependence of vocal-tract length",
   "original": "i02_1525",
   "page_count": 4,
   "order": 458,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "The purpose of this study was to estimate the vocal-tract length based on the distribution of sound pressure in the three-dimensional (3-D) vocal tract during the pronunciation of Japanese vowels. The 3-D shapes of a vocal tract and a dental crown were measured using Magnetic Resonance Imaging (MRI). A male subject was asked to produce Japanese vowels while wearing a dental crown plate that contained a contrast medium for MRI processing. The distribution of sound pressure was estimated in the 3-D vocal tract using the Finite Element Method (FEM). The distribution of sound pressure shows that a sound wave propagates in the vocal tract as a non-plane wave in the high frequency region. The vocal-tract length was estimated based on the distribution of sound pressure in the 3-D vocal tract. The result suggests that the vocal-tract length is long in the high frequency region.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-458"
  },
  "maeda02_icslp": {
   "authors": [
    [
     "Shinji",
     "Maeda"
    ],
    [
     "Martine",
     "Toda"
    ],
    [
     "Andreas J.",
     "Carlen"
    ],
    [
     "Lyes",
     "Meftahi"
    ]
   ],
   "title": "Functional modeling of face movements during speech",
   "original": "i02_1529",
   "page_count": 4,
   "order": 459,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "We describe a functional modeling of face movements during speech. The data consist of 65 face marker positions in (3) D coordinates measured while a speaker read a corpus. An arbitrary orthogonal factor analysis followed by a principal component analysis on the data resulted in a set of five interpretable factors that explains 87% of variance. The first factor that accounts for the vertical jaw motion dominates open/close movements of the mouth. Two principal factors describe the intrinsic lip gestures; one specifies spread vs. round in horizontal dimension and the other open with protrusion vs. retraction in vertical dimension. Both (horizontal) rounding and (vertical) opening contribute to the lip protrusion, which appears plausible from the biomechanical point of view.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-459"
  },
  "mochida02_icslp": {
   "authors": [
    [
     "Takemi",
     "Mochida"
    ],
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Kouki",
     "Hayashi"
    ],
    [
     "Toshiharu",
     "Kuwae"
    ],
    [
     "Kunihiro",
     "Tanahashi"
    ],
    [
     "Kazufumi",
     "Nishikawa"
    ],
    [
     "Atsuo",
     "Takanishi"
    ]
   ],
   "title": "Control system for talking robot to replicate articulatory movement of natural speech",
   "original": "i02_1533",
   "page_count": 4,
   "order": 460,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "The ultimate goal of our study is to create a new speech production system, in which an anthropomorphic hardware talking robot is handled so as to imitate human articulatory movement. We have developed a software motion simulator, which generates control parameter sequences for the talking robot Waseda Talker No. 2 (WT- 2) using the trajectories of human articulatory organs during continuous utterances measured by an electromagnetic articulograph. This paper mainly describes the motion simulator. In addition to its main function as a generator of control parameters for WT-2, the motion simulator also simulates the resultant acoustic characteristics related to WT-2s vocal tract shape at each instance during the motion. The hardware structure of WT-2 is also described briefly. This comprehensive approach will enable us to study speech production using the features of the vocal tract shape as speech motor tasks, instead of using acoustic features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-460"
  },
  "finan02_icslp": {
   "authors": [
    [
     "Donald S.",
     "Finan"
    ],
    [
     "Anne",
     "Smith"
    ],
    [
     "Michael",
     "Ho"
    ]
   ],
   "title": "Feed the tiger: a method for evoking reliable jaw stretch reflexes in children",
   "original": "i02_1537",
   "page_count": 4,
   "order": 461,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "The human jaw stretch reflex is a short-latency excitatory response of the jaw closing muscles to a rapidly imposed downward stretch of the mandible. Typically, the jaw stretch reflex has been evoked in children by using a hand-held reflex hammer. Understanding the development of the jaw stretch reflex has been hindered due to lack of experimental control of stimulus magnitude, motoneuron pool excitability, and activation of cutaneous mechanoreceptors in the lower face. A new methodology is presented involving closed-loop control of parameters of stimulus displacement, real-time biofeedback of jaw-closer muscle performance, and minimization of effects from activation of extraneous mechanoreceptors. Special consideration was given to developing a task that could be understood and easily performed by young children. To that end, an oscilloscope and a picture of an animal were used to create a \"video game\" played with the mouth. This strategy proved to be highly successful in obtaining reliable jaw-stretch reflexes in children as young as 5 years of age.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-461"
  },
  "kaburagi02_icslp": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Kohei",
     "Wakamiya"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Three-dimensional electromagnetic articulograph based on a nonparametric representation of the magnetic field",
   "original": "i02_2297",
   "page_count": 4,
   "order": 462,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "A measurement method of the three-dimensional electromagnetic articulograph system is presented to investigate the dynamic behavior of articulatory organs which can include lateral or rotational movements. To accurately represent the spatial pattern of the magnetic field, we use a multivariate B-spline function, which smoothly interpolates a given set of calibration data samples. The strength of the received signal is predicted based on the spline field function while considering the tilting effect of the receiver coil relative to the direction of the magnetic field. The position and orientation of the receiver coil are then estimated using an iterative procedure so that the difference between the measured and predicted signal strengths is minimized. Preliminary experiments showed that the mean estimation error of the receiver position is about 0.5 mm when the axis of the receiver coil is parallel with one of the axes of the coordinate system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-462"
  },
  "laprie02_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Introduction of constraints in an acoustic-to-articulatory inversion method based on a hypercubic articulatory table",
   "original": "i02_2301",
   "page_count": 4,
   "order": 463,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "Our acoustic to articulatory inversion method exploits an original articulatory table structured in the form of a hypercube hierarchy. The articulatory space is decomposed into regions where the articulatoryto- acoustic mapping is linear. Each region is represented by a hypercube. The inversion procedure retrieves articulatory vectors corresponding to an acoustic entry from the hypercube table. A dynamic procedure is used to recover the best articulatory trajectory according to a minimum articulatory effort criterion. The inversion ensures that inverse articulatory parameters generate original formant trajectories with a very good precision, but not that they are realistic from a phonetic point of view. This papers shows how additional simple articulatory constraints can be incorporated in the inversion process. Constraints are implemented in the form of bonus attached to the points which verify the constraints imposed. This enables the inversion to be guided towards more realistic inverse articulatory trajectories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-463"
  },
  "hiroya02_icslp": {
   "authors": [
    [
     "Sadao",
     "Hiroya"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Acoustic-to-articulatory inverse mapping using an HMM-based speech production model",
   "original": "i02_2305",
   "page_count": 4,
   "order": 464,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "We present a method that determines articulatory movements from speech acoustics using an HMM (Hidden Markov Model)-based speech production model. The model statistically generates speech acoustics and articulatory movements from a given phonemic string. It consists of HMMs of articulatory movements for each phoneme and an articulatory-to-acoustic mapping for each HMM state. For a given speech acoustics, the maximum a posteriori probability estimate of the articulatory parameters of the statistical model is presented. The methods performance on sentences was evaluated by comparing the estimated articulatory parameters with observed parameters. The average rms error of the estimated articulatory parameters was 1.79 mm with phonemic information and 2.16 mm without phonemic information in an utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-464"
  },
  "hashimoto02_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Hashimoto"
    ]
   ],
   "title": "Modeling articulatory dynamics in autoregressive linear system",
   "original": "i02_2309",
   "page_count": 4,
   "order": 465,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "The LPC analysis conducted on each of the time sequences of four articulatory parameters, two for the tongue and two for the mandible and lips, produces the respective autoregressive linear system. These are the dynamical models of articulation in this study. Their structures are examined with change of mode of utterance, such as word utterances and a sentence utterance. Then, the input pulses to drive each linear system are searched for from view point of devising a rule synthesis of the parameter. The search finds more efficient sequences of the pulses than AR residual pulses which is the basic criterion adopted in this study. A scrutiny of the planer plot of the driving pulses for the word utterances differentiate even tense-lax pairs, /t/-/d/ and /s/-/z/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-465"
  },
  "sciamarella02_icslp": {
   "authors": [
    [
     "Denisse",
     "Sciamarella"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "A study of the two-mass model in terms of acoustic parameters",
   "original": "i02_2313",
   "page_count": 4,
   "order": 466,
   "p1": "2313",
   "pn": "2316",
   "abstract": [
    "We present a study of the acoustic source parameters describing glottal flow waveforms generated by the two-mass model for vocal fold oscillations. Numerical measurements of the acoustic parameters as a function of model parameters are presented. Conclusions are drawn from these results, concerning the correlations between acoustic parameters, the effect of the vocal tract, the different regimes of oscillation of the model, and the possibility of reproducing laryngeal mechanisms with the model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-466"
  },
  "kolossa02_icslp": {
   "authors": [
    [
     "Dorothea",
     "Kolossa"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "Using time-stretched pulses for accurate splitting of speech utterances played back in noisy reverberant environments",
   "original": "i02_1541",
   "page_count": 4,
   "order": 467,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "In the speech recognition area, there is often a need to play back existing speech corpora in a new environment to generate a large amount of close-to-realistic speech data for developing a new application. Such playback speech corpora can be constructed efficiently by first concatenating many speech utterances into big files with a marker tone inserted between two utterances, then playing back and recording them, and finally extracting the individual utterances from the recorded sounds via detection of the marker tone responses. In this paper, we propose to use the TSP (time-stretched pulse) as a marker tone for this purpose. We present a matched filtering based procedure for detecting TSP responses in playback recordings. Using this approach, we have played back the TI46 speech corpus at different distances ranging from 5cm to 1.5m between the loudspeaker and the microphone in a noisy reverberant lab environment. The playback sounds are recorded by an iPAQ Pocket PC. All the speech utterances are successfully extracted with a maximum temporal error of about 1.9ms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-467"
  },
  "maekawa02_icslp": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Yosuke",
     "Igarashi"
    ],
    [
     "Jennifer",
     "Venditti"
    ]
   ],
   "title": "X-JToBI: an extended j-toBI for spontaneous speech",
   "original": "i02_1545",
   "page_count": 4,
   "order": 468,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "We report on the new X-JToBI prosodic labeling scheme, the eXtended version of J_ToBI which has grown out of our work on annotating prosodic features of spontaneous speech. Among the new characteristics of X-JToBI are 1) Exact match between the time-stamp of tone labels and the timing of physical events, 2) Enlargement of the inventory of boundary pitch movements, 3) Extension and ramification of the usage of break indices, and 4) Newly defined labels for filled-pause and non-lexical prominence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-468"
  },
  "strik02_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Walter",
     "Daelemans"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Janienke",
     "Sturm"
    ],
    [
     "F. De",
     "Vriend"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "Dutch HLT resources: from BLARK to priority lists",
   "original": "i02_1549",
   "page_count": 4,
   "order": 469,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "In this paper we report on a project about Dutch Human Language Technologies (HLT) resources. In this project we first defined a socalled BLARK (Basic LAnguage Resources Kit). Subsequently, a survey was carried out to make an inventory and evaluation of existing Dutch HLT resources. Based on the information collected in the survey, a priority list was drawn up of materials that need to be developed to complete the Dutch BLARK. Although the current project only concerns the Dutch language, the method employed and some of the results are also relevant for other languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-469"
  },
  "yang02b_icslp": {
   "authors": [
    [
     "Fan",
     "Yang"
    ],
    [
     "Susan E.",
     "Strayer"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "ACT: a graphical dialogue annotation comparison tool",
   "original": "i02_1553",
   "page_count": 4,
   "order": 470,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "Although there are a number of tools for annotating dialogue, little work has been done in visualizing differences among annotations. Visualization of differences in annotation should help in doing consensus annotation, refining an annotation scheme and training novice annotators. In this paper, we present a graphical Annotation Comparison Tool (ACT), which displays multiple annotations side by side and highlights the differences. The differences our tool currently captures are utterance segmentation, utterance order, speech repairs and utterance categories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-470"
  },
  "yu02_icslp": {
   "authors": [
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Jin Suk",
     "Kim"
    ]
   ],
   "title": "A training prompts generation algorithm for connected spoken word recognition",
   "original": "i02_1557",
   "page_count": 4,
   "order": 471,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "This paper describes an efficient algorithm to generate compact prompts lists for training utterances. In building a connected speech recognizer such as a connected spoken digit recognizer, we have to acquire speech data with various combinations of the words contexts. However, in many speech databases the lists are made by random generators. We provide an efficient algorithm that can generate compact and complete list of words with various contexts. The algorithm begins with a series of unique digits, which is used for difference series of another digits series of wider context. The process is applied recursively until desired context range is achieved. The algorithm can be generalized to any range of contexts, such as tri-words and fourwords. This paper includes proof of optimality and completeness of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-471"
  },
  "cornu02_icslp": {
   "authors": [
    [
     "Etienne",
     "Cornu"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Robert",
     "Brennan"
    ]
   ],
   "title": "A low-resource, miniature implementation of the ETSI distributed speech recognition front-end",
   "original": "i02_1581",
   "page_count": 4,
   "order": 472,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "The purpose of this work is to demonstrate that distributed speech recognition front-ends can be deployed in environments which provide for very little power and CPU resources, with possibly no degradation of speech recognition quality when compared to standard floating-point implementations. The ETSI distributed speech recognition front-end standard is implemented on an ultra low-power miniature DSP system. The efficient implementation of the ETSI algorithm components, i.e. feature extraction, feature compression and multi-framing, is accomplished through the use of three processing units running concurrently. In addition to a DSP core, an input/ output processor creates frames of input speech signals, and a weighted overlap-add (WOLA) filterbank unit performs windowing, FFT and vector multiplications. System evaluation using the TI digits database shows that the performance of the ultra low-power DSP system is equivalent to the reference implementation provided by ETSI.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-472"
  },
  "astrov02_icslp": {
   "authors": [
    [
     "Sergey",
     "Astrov"
    ]
   ],
   "title": "Memory space reduction for hidden Markov models in low-resource speech recognition systems",
   "original": "i02_1585",
   "page_count": 4,
   "order": 473,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "Low-cost recognition systems based on hidden Markov models (HMM) for mobile speech recognizers (mobile phones, PDAs) have a limited quantity of memory and processing power. Furthermore, the resources have to be shared between several applications. In this paper memory efficient HMMs were investigated for low-cost recognition platforms. The feature parameter tying HMM and subspace distribution clustering HMM (SDCHMM) were explored. In order to achieve less memory requirements, a shared codebook approach for feature parameter tying HMM and SDCHMM was developed and its effectiveness was experimentally proved. It was shown that this approach leads to a relative increase of word error rate of less than 10% for 50% of memory reduction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-473"
  },
  "wang02b_icslp": {
   "authors": [
    [
     "Xia",
     "Wang"
    ],
    [
     "Juha",
     "Iso-Sipilä"
    ]
   ],
   "title": "Low complexity Mandarin speaker-independent isolated word recognition",
   "original": "i02_1589",
   "page_count": 4,
   "order": 474,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "This paper addresses the problem of tone recognition in Mandarin Chinese language. The tone, or pitch contour, has a lexical meaning in Mandarin language. This means that changing the pitch contour of a syllable results in totally different meaning for the word. This is especially problematic in speech recognition applications where context does not allow the unambiguous resolution of the meaning. In order to cope with this, we have developed methods that allow us to recognize Mandarin isolated words that differ only by the pitch contour. Low complexity, both in terms of memory and computational complexity, has been another driver for this work. The target platform is an embedded system with limited memory and computation resources. Two new schemes are proposed with significantly lower complexity compared to the baseline system. High recognition accuracy of almost 90% has been retained despite more than 60% reduction in complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-474"
  },
  "kiss02_icslp": {
   "authors": [
    [
     "Imre",
     "Kiss"
    ],
    [
     "Marcel",
     "Vasilache"
    ]
   ],
   "title": "Low complexity techniques for embedded ASR systems",
   "original": "i02_1593",
   "page_count": 4,
   "order": 475,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "This paper deals with the problem of reducing the computational complexity of ASR algorithms for embedded systems. Particularly, three methods for simplifying the computation of state observation likelihoods of continuous density based HMMs are proposed. Feature component masking, variable-rate partial likelihood update and density pruning all result in significant savings in the decoding complexity with marginal impact on the recognition performance. A combination of feature component masking and density pruning was evaluated in a small vocabulary, 25-lingual, speaker independent, isolated word recognition system. With a computational complexity reduction of 62% compared to the baseline system, a marginal, 1.6/6.5% relative error rate increase was obtained without/with online MAP adaptation on the average in clean and noisy operating environments. The presented framework can also be extended to larger vocabulary systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-475"
  },
  "reinhard02_icslp": {
   "authors": [
    [
     "Klaus",
     "Reinhard"
    ],
    [
     "Jochen",
     "Junkawitsch"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Stefan",
     "Dobler"
    ]
   ],
   "title": "Optimization of hidden Markov models for embedded systems",
   "original": "i02_1597",
   "page_count": 4,
   "order": 476,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "This paper presents a method to address a significant real life problem of embedded systems. Such systems are characterized by limited resources. Restrictions are placed on the amount of available memory for the acoustic models as well as a limited computational capacity to perform the needed distance calculations. Hence the trade-off between optimal performance and meeting such requirements needs a tailor-made HMM set solution. Starting from single densities, boosting the performance is normally done by blindly splitting model densities hence doubling the amount of memory for the acoustic models. The approach in this paper proposes an iterative length adaptation (ILA) scheme to change the model length to the needs of the acoustic events. It improves the model accuracy through a decrease of the overall acoustic distance score while minimizing the amount of additional memory for the acoustic model. The improved HMMs result in a decrease of the achieved WER out- performing the acoustic models utilizing two mixture densities per state whereas the number of model densities are only increased by 21% in comparison to the single density models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-476"
  },
  "filali02_icslp": {
   "authors": [
    [
     "Karim",
     "Filali"
    ],
    [
     "Xiao",
     "Li"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Data-driven vector clustering for low-memory footprint ASR",
   "original": "i02_1601",
   "page_count": 4,
   "order": 477,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "It is important to produce automatic speech recognition (ASR) systems that use as few computational and memory resources as possible, especially in low-memory/low-power environments such as for personal digital assistants. One way to achieve this is through parameter quantization. In this work, we compare a variety of novel subvector clustering procedures for ASR system parameter quantization. Specifically, we look at systematic data-driven subvector selection techniques based on entropy minimization, and compare performance on a 150-word isolated word speech recognition task. While the optimal entropy-minimizing quantization methods are intractable, we show that although several of our heuristic techniques are elaborate in their attempt to approximate the optimal clustering, a simple scalar quantization scheme using separate codebooks performs remarkably well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-477"
  },
  "jiang02_icslp": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Utterance verification based on neighborhood information and Bayes factors",
   "original": "i02_1605",
   "page_count": 4,
   "order": 478,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "In this paper, we propose to use neighborhood information inmodel space to perform utterance verification (UV). At first, we present a nested-neighborhood structure for each underlying model in model space and assume the underlying models competing models sit in one of these neighborhoods, which is used to model alternative hypothesis in UV. Bayes factors (BF) is first introduced to UV and used as a major tool to calculate confidence measures based on the above idea. Experimental results in Bell Labs communicator system show that the new method has dramatically improved verification performance when verifying correct words against misrecognized words in recognizers output, relatively more than 20% reduction in equal error rate (EER) when comparing with the standard approach based on likelihood ratio testing and anti-models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-478"
  },
  "lahti02_icslp": {
   "authors": [
    [
     "Tommi",
     "Lahti"
    ],
    [
     "Janne",
     "Suontausta"
    ]
   ],
   "title": "Vocabulary independent OOV detection using support vector machines",
   "original": "i02_1609",
   "page_count": 4,
   "order": 479,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "In this paper, a novel Out-of-Vocabulary (OOV) word detection method relying on phoneme-level acoustic measures and Support Vector Machines (SVM) is proposed. Word level OOV scores are computed from the phoneme level in-vocabulary (IV) and OOV information provided by an HMM based speech recognizer. The OOV word decision is based on the confidence feature vector which is processed by a SVM classifier. The decision thresholds are independent of the used test vocabulary. The performance of the proposed SVM classification scheme was experimentally compared with the word and sub-word level confidence methods. The tests indicate that the SVM based OOV rejection best generalizes the performance on the test set. While all methods were found to provide a similar performance after parameter optimization on the training set, the proposed SVM classification scheme decreased the false acceptance rate on test set by 30.4% compared with the word level confidence method and experimental decision threshold values.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-479"
  },
  "bazzi02_icslp": {
   "authors": [
    [
     "Issam",
     "Bazzi"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "A multi-class approach for modelling out-of-vocabulary words",
   "original": "i02_1613",
   "page_count": 4,
   "order": 480,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "In this paper we present a multi-class extension to our approach for modelling out-of-vocabulary (OOV) words [1]. Instead of augmenting the word search space with a single OOV model, we add several OOV models, one for each class of words. We present two approaches for designing the OOV word classes. The first approach relies on using common part-of-speech tags. The second approach is a datadriven two-step clustering procedure, where the first step uses agglomerative clustering to derive an initial class assignment, while the second step uses iterative clustering to move words from one class to another in order to reduce the model perplexity. We present experiments within the JUPITER weather information domain. Results show that the multi-class model significantly improves performance over using a single OOV class. For an OOV detection rate of 70%, the false alarm rate is reduced from 5.3% for a single class to 2.9% for an eight-class model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-480"
  },
  "duchateau02_icslp": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Unconstrained versus constrained acoustic normalisation in confidence scoring",
   "original": "i02_1617",
   "page_count": 4,
   "order": 481,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "In HMM-based recognition systems for large vocabulary, the observation likelihoods provided by the acoustic models are useful in con- fidence measures if they are properly normalised. This paper compares two normalisation methods for the acoustic model likelihoods: unconstrained normalisation, based on the unconditional observation likelihood, and constrained normalisation, based on the observation likelihoods in a phoneme recognition system in which the phoneme strings are constrained by an N-gram phoneme sequence model. We found on the benchmark 20k word Wall Street Journal recognition task that both normalisations perform equally well at first sight. However their behaviour depends on the length of the word: constrained normalisation outperforms unconstrained normalisation for long words and the opposite holds for short words. With a confidence measure that exploits this fact, the normalised cross entropy metric for confidence measures can be increased from a reference 21.9% (with unconstrained normalisation) to 23.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-481"
  },
  "falavigna02_icslp": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Acoustic and word lattice based algorithms for confidence scores",
   "original": "i02_1621",
   "page_count": 4,
   "order": 482,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "Word confidence scores are crucial for unsupervised learning in automatic speech recognition. In the last decade there has been a flourish of work on two fundamentally different approaches to compute confidence scores. The first paradigm is acoustic and the second is based on word lattices. The first approach is data-intensive and it requires to explicitly model the acoustic channel. The second approach is suitable for on-line (unsupervised) learning and requires no training. In this paper we present a comparative analysis of off-the-shelf and new algorithms for computing confidence scores, following the acoustic and lattice-based paradigms. We compare the performance of these algorithms across three tasks for small, medium and large vocabulary speech recognition tasks and for two languages (Italian and English). We show that word-lattice based algorithm provides consistent and effective performance across automatic speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-482"
  },
  "wang02c_icslp": {
   "authors": [
    [
     "Huei-Ming",
     "Wang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "Error-tolerant spoken language understanding with confidence measuring",
   "original": "i02_1625",
   "page_count": 4,
   "order": 483,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "The inevitable speech recognition errors make spoken dialogue systems not prevail in our daily lives. To reduce the impact of speech recognition errors, an error-tolerant language understanding model with confidence measuring is proposed in this paper. In this model, the exemplary concept sequences are used to provide the clues for detecting and recovering the errors. First, the hypothetical sentences are parsed to the hypothetical concept sequences. Then, the most matched pair of hypothetical and exemplary concept sequences is selected according to different kinds of scores, including the score of the edit operations (i.e., deleting, inserting and substituting concepts). The edit operation score is assessed according to the con- fidence level of the hypothetical concepts. Tested on cellular phone calls, the proposed model improves the precision rate of concepts from 65.09% to 76.32% and the recall rate from 64.21% to 69.13%, in comparison with the concept bigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-483"
  },
  "weil02_icslp": {
   "authors": [
    [
     "Shawn A.",
     "Weil"
    ]
   ],
   "title": "Comparing intelligibility of several non-native accent classes in noise",
   "original": "i02_1629",
   "page_count": 4,
   "order": 484,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "Increased global interaction has led to increased communication between individuals with wide ranging linguistic experience. As a consequence, the ramifications of accent in speech production need to be better understood. It is plausible that differences in speech intelligibility due to accent type affect gross intelligibility in different ways. These differences may be differentially affected by the addition of noise. The intelligibility of talkers of five language backgrounds (Ohio English, Japanese, Taiwanese Mandarin, Indian English, and Russian) was assessed using the full Speech Reception Threshold (SRT) test. These results were compared to a measure of intelligibility without noise. Global differences in intelligibility in noise between accent classes were not found, nor were changes in relative intelligibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-484"
  },
  "ishizuka02_icslp": {
   "authors": [
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Effect of F0 fluctuation and amplitude modulation of natural vowels on vowel identification in noisy environments",
   "original": "i02_1633",
   "page_count": 4,
   "order": 485,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "This paper describes findings showing that the fundamental frequency (F0) fluctuation and amplitude modulation (AM) included in natural vowels contribute to improving the vowel identification rate in the presence of interferer sounds. A vowel identification experiment revealed that even very small F0 fluctuations and AM of vowels significantly improved the vowel identification rate in the presence of interferer sound. In addition, it was found that the effect is not the result of reducing the masked threshold. A vowel detection experiment revealed that the effect of reducing the threshold depends on the kinds of interferers, and that the vowel identification rates do not correlate with the masked thresholds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-485"
  },
  "yoneyama02_icslp": {
   "authors": [
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "Similarities of words in noise in Japanese",
   "original": "i02_1637",
   "page_count": 4,
   "order": 486,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "This paper investigated how Japanese listeners perceived words in a noisy environment. The data of a word identification in noise experiment reported in Yoneyama (2002) were analyzed. There are three main findings. First, more than 86% of the total responses correctly reproduced the pitch accent patterns of the stimulus words, suggesting Japanese listeners hardly misperceived pitch accent patterns of words in a noisy condition. Second, vowels were perceived more correctly than consonants. Third, MDS analyses revealed that similarities of sounds were mapped in terms of phonological features ([±high] and [±back] for vowels; [±voice] and [±sonorant] for consonants). These findings suggest that similarities of sounds that are highly related to the sonority scale of sounds in Japanese and pitch accent patterns contribute to word recognition in a noisy condition in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-486"
  },
  "brungart02_icslp": {
   "authors": [
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Alexander J.",
     "Kordik"
    ],
    [
     "Koel",
     "Das"
    ],
    [
     "Arnab K.",
     "Shaw"
    ]
   ],
   "title": "The effects of F0 manipulation on the perceived distance of speech",
   "original": "i02_1641",
   "page_count": 4,
   "order": 487,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "Recent research has shown that the apparent distance of speech increases systematically with the vocal effort level of the talker. At this point, however, it is not clear how the different acoustic features of speech that change with vocal effort contribute to this increase in apparent distance. In this experiment, pitch-synchronous overlap and add (PSOLA) techniques were used to modify the F0 contours of prerecorded speech signals to match the F0 contours of speech samples that were produced at 6 dB higher output levels and at 6 dB lower output levels. The other characteristics of the speech signals were left unchanged. Psychoacoustic tests of the resulting speech signals show that the synthetic F0 shifts could account for some, but not all, of the apparent changes in distance that occur with a 6 dB change in the output level of the talker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-487"
  },
  "janse02_icslp": {
   "authors": [
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Time-compressing natural and synthetic speech",
   "original": "i02_1645",
   "page_count": 4,
   "order": 488,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "Phoneme detection is a useful tool to compare the perception of perfectly intelligible speech types. As previous research suggests that perception of fast speech is helped by segmental redundancy, we expected the hyperarticulation of synthetic speech to turn into an advantage at a fast rate. Consequently, the processing advantage of natural over synthetic speech was expected to decrease after timecompression. Secondly, detection times were expected to be slower after moderate time-compression because of the higher processing difficulty of fast speech. However, detection times tended to become shorter in the time-compressed condition. This was attributed to shorter durations of syllables and words. Furthermore, the processing advantage of natural over synthetic speech did not decrease, but rather tended to increase. This may be explained by the lack of a speaking effort pattern in synthetic diphone speech, which makes it rather blurred at faster playback rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-488"
  },
  "xue02_icslp": {
   "authors": [
    [
     "Jianxia",
     "Xue"
    ],
    [
     "Sumiko",
     "Takayanagi"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Accounting for perceptual identification of consonants and vowels through acoustic dissimilarity",
   "original": "i02_1649",
   "page_count": 4,
   "order": 489,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "This study was undertaken to examine relationships between acoustic speech measures and auditory phonetic perception. The hypothesis of the study was that physical dissimilarity of the acoustic measures could substantially account for perception. Speech samples of 22 Consonant-/a/ syllables and 14 /h/-Vowel-/d/ syllables were spoken by two talkers. The stimuli were processed by vocoders with two different filterbanks. Forced-choice perceptual identifications were obtained from 6 normal hearing participants for each talker, vocoder, and syllable set. Confusion data were analyzed using multidimensional scaling, and Euclidean distances among stimulus phonemes were computed. For each pair of stimuli (within the same talker, vocoder, and syllable set), physical Euclidean distances were computed within frequency channels and averaged across time. Multilinear regression was used to transform the Euclidean physical distances to perceptual distances. Evaluation using Pearson r showed that the transformed physical distances correlated with perceptual distances between 0.55 and 0.96 (30% to 91% variance accounted for), depending on the talker, vocoder, and syllable set. The results indicated that the distinctiveness of the speech signals can account for perceptual dissimilarity structure by using a linear transformation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-489"
  },
  "wade02_icslp": {
   "authors": [
    [
     "Travis",
     "Wade"
    ],
    [
     "Deborah K.",
     "Eakin"
    ],
    [
     "Russell",
     "Webb"
    ],
    [
     "Arvin",
     "Agah"
    ],
    [
     "Frank",
     "Brown"
    ],
    [
     "Allard",
     "Jongman"
    ],
    [
     "John",
     "Gauch"
    ],
    [
     "Thomas A.",
     "Schreiber"
    ],
    [
     "Joan",
     "Sereno"
    ]
   ],
   "title": "Modeling recognition of speech sounds with minerva2",
   "original": "i02_1653",
   "page_count": 4,
   "order": 490,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "This study investigates the extent to which a localist-distributive hybrid formal model of human memory replicates observed behavioral patterns in perception and recognition of appropriately coded language data. Extending previous research that considered for modeled memorization only items with uniform, undefined randomly generated featural specifications, a MINERVA2 simulation was trained to recognize linguistic events and categories at both acousticphonetic and phonological-featural processing levels. Results of both test conditions parallel two important effects observed in behavioral data and are discussed with respect to speech perception as well as human memory research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-490"
  },
  "kearns02_icslp": {
   "authors": [
    [
     "Ruth",
     "Kearns"
    ],
    [
     "Dennis",
     "Norris"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Syllable processing in English",
   "original": "i02_1657",
   "page_count": 4,
   "order": 491,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "We describe a reaction time study in which listeners detected word or nonword syllable targets (e.g. zoo, trel) in sequences consisting of the target plus a consonant or syllable residue (trelsh, trelshek). The pattern of responses differed from an earlier word-spotting study with the same material, in which words were always harder to find if only a consonant residue remained. The earlier results should thus not be viewed in terms of syllabic parsing, but in terms of a universal role for syllables in speech perception; words which are accidentally present in spoken input (e.g. sell in self) can be rejected when they leave a residue of the input which could not itself be a word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-491"
  },
  "kuijpers02_icslp": {
   "authors": [
    [
     "Cecile",
     "Kuijpers"
    ],
    [
     "Wilma van",
     "Donselaar"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Perceptual effects of assimilation-induced violation of final devoicing in dutch",
   "original": "i02_1661",
   "page_count": 4,
   "order": 492,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "Voice assimilation in Dutch is an optional phonological rule which changes the surface forms of words and in doing so may violate the otherwise obligatory phonological rule of syllable-final devoicing. We report two experiments examining the influence of voice assimilation on phoneme processing, in lexical compound words and in noun-verb phrases. Processing was not impaired in appropriate assimilation contexts across morpheme boundaries, but was impaired when devoicing was violated (a) in an inappropriate (nonassimilatory) context, or (b) across a syntactic boundary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-492"
  },
  "yip02_icslp": {
   "authors": [
    [
     "Michael C.W.",
     "Yip"
    ]
   ],
   "title": "Access to homophonic meanings during spoken language comprehension: effects of context and neighborhood density",
   "original": "i02_1665",
   "page_count": 4,
   "order": 493,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "A gating experiment was conducted to examine the effects of context and neighborhood density information in the processing of Chinese homophones during spoken language comprehension. In this experiment, listeners were presented with successively gated portions of a spoken homophone, embedded in a sentence context, and they identified the homophone on the basis of its increasing amount of acoustic information. Results indicate that context has an early effect on the disambiguation of various homophonic meanings, shortly after the acoustic onset of the word. Second, context interacts with frequency of the individual meanings of a homophone during lexical access. Third, the neighborhood density information helps to narrow down the number of candidates of the target homophone along the temporal course of spoken language processing. Finally, the results are interpreted in terms of interactive activation models of lexical processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-493"
  },
  "magrinchagnolleau02_icslp": {
   "authors": [
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Melissa",
     "Barkat"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Intelligibility of reverse speech in French: a perceptual study",
   "original": "i02_1669",
   "page_count": 4,
   "order": 494,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "We ran an experiment focusing on cognitive implication of reverse speech segments. Nine durations of reverse speech plus a nondistorted control condition have been considered (varying between 20 ms and 180 ms) in order to test the pattern of intelligibility degradation in French. We observed an overall strong negative correlation between the degree of intelligibility and the size of reverse speech windows. These results appear to be very comparable to those obtained in English by Greenberg and Arai (2001), at least on the slope of intelligibility performance decrease. However, intelligibility loss in French is delayed by 20 ms. Apart from con- firming the cognitive ability to restore reverse speech up to a certain point, our study revealed differences that could be interpreted as language specific.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-494"
  },
  "serniclaes02_icslp": {
   "authors": [
    [
     "Willy",
     "Serniclaes"
    ],
    [
     "René",
     "Carré"
    ]
   ],
   "title": "Contextual effects in the perception of fricative place of articulation: a rotational hypothesis",
   "original": "i02_1673",
   "page_count": 4,
   "order": 495,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "According to the interactive model of consonant place of articulation perception, the vowel context not only biases place decisions but also affects the processing of the place cues. Data on the perception of French fricative-vowel syllables suggest that contextual effects are indeed interactive, thereby confirming previous findings in Dutch. Further, these data suggests a new formulation of the contextual rules for place of articulation perception in terms of rotational movements around a central reference corresponding to the neutral vowel (\"schwa\").\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-495"
  },
  "sock02_icslp": {
   "authors": [
    [
     "Rudolph",
     "Sock"
    ],
    [
     "Béatrice",
     "Vaxelaire"
    ],
    [
     "Véronique",
     "Hecker"
    ],
    [
     "Fabrice",
     "Hirsch"
    ]
   ],
   "title": "What relationship between protrusion anticipation and auditory perception?",
   "original": "i02_1677",
   "page_count": 4,
   "order": 496,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "This investigation, while confronting data from two speakers, examines the contribution of kinematic and acoustic attributes of protrusion to early perception of a French rounded vowel. The main question addressed here is to determine the relationship between extension of auditory perception and anticipatory expansion of lip protrusion. If extension of auditory perception does increase with anticipatory expansion of lip protrusion, it would be important to determine the nature of the motor-sensory relation, and also the extent to which speaker-specific characteristics may influence the perceptual behaviour of listeners. The robustness of the temporal extent of the perceptual effects is also evaluated under increased speaking rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-496"
  },
  "carre02_icslp": {
   "authors": [
    [
     "René",
     "Carré"
    ],
    [
     "Jean Sylvain",
     "Liénard"
    ],
    [
     "Egidio",
     "Marsico"
    ],
    [
     "Willy",
     "Serniclaes"
    ]
   ],
   "title": "On the role of the \"schwa\" in the perception of plosive consonants",
   "original": "i02_1681",
   "page_count": 4,
   "order": 497,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "Within an uniform acoustic tube, regions having specific acoustic properties were defined [1, 2] and were at the origin of the Distinctive Region Model (DRM). Such a model has its own intrinsic phonology to produce distinctive sounds. In fact, the regions correspond to the main consonant places of articulation. Such results encourage to study the main predictions of the model: Theoretically, F2 can be used to separate /b/ from /d, g/, on the one hand, and F3 can be used to separate /d/ from /g/, on the other hand. This hypothesis was studied with perception tests. The role of the neutral pattern \"schwa\" (/@/), which corresponds to the uniform tube, as reference in the process of consonant perception is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-497"
  },
  "nguyen02d_icslp": {
   "authors": [
    [
     "Noël",
     "Nguyen"
    ],
    [
     "Ludovic",
     "Jankowski"
    ],
    [
     "Michel",
     "Habib"
    ]
   ],
   "title": "The perception of stop consonant sequences in dyslexic and normal children",
   "original": "i02_2565",
   "page_count": 4,
   "order": 498,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "Previous research has revealed that dyslexic children may be more sensitive to backward-masking effects in auditory perception than control children. In this study, we asked whether a CV transition masks a preceding VC transition to a greater extent in dyslexic children than in controls. The results suggest that dyslexic children are severely impaired on the discrimination of VC sequences, regardless of whether these sequences are followed or not by a CV sequence. These results provide further evidence that dyslexia is associated with a deficit in the perception of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-498"
  },
  "otake02_icslp": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Akemi",
     "Iijima"
    ]
   ],
   "title": "Submoraic awareness by Japanese school children: evidence from a novel game",
   "original": "i02_2569",
   "page_count": 4,
   "order": 499,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "The present study has attempted to investigate the relationship between morae and phonemes with respect to phonological awareness by Japanese school children. The main question addressed in this study is whether Japanese school children can be aware of phonemes as a submoraic unit in Japanese if they are given the information on the internal structure of morae. Two experiments were conducted with two groups of Japanese elementary school children with and without alphabetic knowledge, employing a novel apparatus which can make them understood the relationship intuitively. The results show that Japanese school children regardless of alphabetic knowledge can recognize the relationship between them. This suggests that they can access to the phonemic level if they are provided with the appropriate information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-499"
  },
  "markham02_icslp": {
   "authors": [
    [
     "D.",
     "Markham"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Speaker intelligibility of adults and children",
   "original": "i02_2573",
   "page_count": 4,
   "order": 500,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "A study of speaker intelligibility involving 45 speakers from a homogeneous accent group was carried out with adult, 7-8 year old and 11-12 year old listeners. Ranking of speakers according to their intelligibility was remarkably consistent across listener groups, thus demonstrating that inherent speaker characteristics are the primary contributor to intelligibility. In a second study, listener ratings of speakers with highest and lowest intelligibility on a number of dimensions suggested that spectral differences and differences in the quality of vocal fold excitation may be correlated with intelligibility. Physical measures of long-term spectrum and fundamental frequency were carried out. These did not reveal consistent predictors of intelligibility, but relationships were observed between these and subjective judgements of voice characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-500"
  },
  "yamashita02b_icslp": {
   "authors": [
    [
     "Yasuki",
     "Yamashita"
    ],
    [
     "Hiroshi",
     "Matsumoto"
    ]
   ],
   "title": "Acoustical correlates to SD ratings of speaker characteristics in two speaking styles",
   "original": "i02_2577",
   "page_count": 4,
   "order": 501,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "For synthesizing voice quality expressed by adjectives, this paper investigates acoustical correlates to adjective ratings of speaker characteristics for reading and conversational speech. The results revealed: (1) The speaking styles have little effect on the rates on adjective scales. (2) The effects of formant frequencies and long-term spectrum to adjective ratings are almost independent of speaking styles. (3) The \"busy\" voices have a significant correlation to the standard deviation of logarithmic F0 divided by speech rate (mora per second) for both reading and conversational voices. Furthermore, for conversational speech, the \"busy\" ratings significantly correlate to the F0 as well. (4) For the reading speech, \"articulate\" voices have larger dispersion on the F1-F2 plane than \"inarticulate\" ones. For conversational speech, both the \"articulate\" and \"inarticulate\" voices have small dispersions compared to those for reading speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-501"
  },
  "ormanci02_icslp": {
   "authors": [
    [
     "Eda",
     "Ormanci"
    ],
    [
     "U. Hakan",
     "Nikbay"
    ],
    [
     "Oytun",
     "Turk"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "Subjective assessment of frequency bands for perception of speaker identity",
   "original": "i02_2581",
   "page_count": 4,
   "order": 502,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "In this study, we describe and evaluate a subjective test to determine the importance of subbands in perceiving the speaker identity. Pairs of sound files have been generated where the first sound file is the full-band version of a word and the second file is a subband of the same word in the first part and a subband of a different word in the second part of the experimental work. Words in each pair are uttered by either the same speaker or different speakers. Subjects were asked to score each pair of sound files they listened to according to the similarity of the speakers uttering each word. Also a consistency score is calculated for each subject to render the statistics more reliable. The experiment demonstrates that the second subband (1034 Hz-1895 Hz range) is the most important subband for speaker identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-502"
  },
  "stallard02_icslp": {
   "authors": [
    [
     "David",
     "Stallard"
    ],
    [
     "Premkumar",
     "Natarajan"
    ],
    [
     "Mohammed",
     "Noamany"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Design for a speech-to-speech translator for field use",
   "original": "i02_1705",
   "page_count": 4,
   "order": 503,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "We present the design of a proposed Two-Way Translator that we are developing for simple communication between US military or humanitarian personnel and non-English speakers in a foreign country. The translator uses a system-directed dialogue technique, and is intended for use in a handheld device. A novel aspect of our work is its use of statistical information extraction techniques to assist in the translation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-503"
  },
  "black02_icslp": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Ralf D.",
     "Brown"
    ],
    [
     "Robert",
     "Frederking"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "John",
     "Moody"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Eric",
     "Steinbrecher"
    ]
   ],
   "title": "Rapid development of speech-to-speech translation systems",
   "original": "i02_1709",
   "page_count": 4,
   "order": 504,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "This paper describes building of the basic components, particularly speech recognition and synthesis, of a speech-to-speech translation system. This work is described within the framework of the \"Tongues: small footprint speech-to-speech translation device\" developed at CMU and Lockheed Martin for use by US Army Chaplains.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-504"
  },
  "imamura02_icslp": {
   "authors": [
    [
     "Kenji",
     "Imamura"
    ],
    [
     "Eiichiro",
     "Sumita"
    ]
   ],
   "title": "Bilingual corpus cleaning focusing on translation literality",
   "original": "i02_1713",
   "page_count": 4,
   "order": 505,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "When we automatically acquire translation knowledge from a bilingual corpus, redundant rules are generated due to translation variety. To overcome this problem, we propose bilingual corpus cleaning based on translation literality. Word-level correspondence and phrase-level correspondence are applied as the criteria of literality. Using these criteria, a bilingual corpus was cleaned, and translation knowledge for a pattern-based MT system was acquired from the cleaned corpus. As a result, the translation quality of the MT was improved despite reductions in the the corpus size to about 81% and 87% by using word-level and phrase-level literality scores, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-505"
  },
  "tanaka02_icslp": {
   "authors": [
    [
     "Hideki",
     "Tanaka"
    ],
    [
     "Stephen",
     "Nightingale"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Kenji",
     "Matsumoto"
    ],
    [
     "Masamchi",
     "Nishiwaki"
    ],
    [
     "Tadashi",
     "Kumano"
    ],
    [
     "Takehiko",
     "Maruyama"
    ]
   ],
   "title": "Speech to speech translation system for monologues-data driven approach",
   "original": "i02_1717",
   "page_count": 4,
   "order": 506,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "This paper describes ongoing research on a Japanese-to-English speech- to-speech translation system for \"controlled monologue\", such as TV news and commentary programs in which the speaking styles are controlled as a monologue. We have adopted the data-driven approach since the TV programs in question cover a wide range of topics, and because it seems much too labor intensive to handcraft translation rules. The approach therefore requires a gigantic parallel corpus for the target domain, although the absolute size needed is not so easy to obtain. There are also difficulties inherent in monologue such as the need to handle long sentences, averaging over 25 words, and the realization of simultaneity. These problems are presented in the light of our available corpora and we go on to present the kinds of problems we have to solve. Finally, we present our prospective system architecture and introduce the present status of the work.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-506"
  },
  "gispert02_icslp": {
   "authors": [
    [
     "Adrià de",
     "Gispert"
    ],
    [
     "José B.",
     "Mariño"
    ]
   ],
   "title": "Using x-grams for speech-to-speech translation",
   "original": "i02_1885",
   "page_count": 4,
   "order": 507,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "In this paper, a statistical speech-to-speech translation system, developed at TALP during the last months, is presented. By adapting well-known speech recognition techniques to the specific translation setting, the system is able to integrate speech signal into a finite state transducer that translates statistically domain-constrained Spanish sentences into English ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-507"
  },
  "watanabe02d_icslp": {
   "authors": [
    [
     "Taro",
     "Watanabe"
    ],
    [
     "Eiichiro",
     "Sumita"
    ]
   ],
   "title": "Statistical machine translation decoder based on phrase",
   "original": "i02_1889",
   "page_count": 4,
   "order": 508,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "This paper describes a decoding algorithm for statistical machine translation based on phrases. In the past, the solution to the decoding problem were inspired from that of speech recognizers, translating each input word into one or more output words generating in left-to-right direction. The algorithm presented here iteratively constructs phrases or chunks of cepts until all the input words are consumed. This behavior resulted in computational complexity higher than those with left-to-right constraints, though the translation accuracy is better from the Japanese-to-English translation experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-508"
  },
  "sumita02_icslp": {
   "authors": [
    [
     "Eiichiro",
     "Sumita"
    ],
    [
     "Yasuhiro",
     "Akiba"
    ],
    [
     "Kenji",
     "Imamura"
    ]
   ],
   "title": "Reliability measures for translation quality",
   "original": "i02_1893",
   "page_count": 4,
   "order": 509,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "This paper proposes new reliability measures on translation quality. A reliability measure is important for a translation system to select the best one of multiple translations. The proposed measures are independent of translation systems, so they are applicable to any translation system. Experiments on selection proved the effectiveness of our measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-509"
  },
  "zhou02b_icslp": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Jeffrey",
     "Sorensen"
    ],
    [
     "Zijian",
     "Diao"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Statistical natural language generation for speech-to-speech machine translation systems",
   "original": "i02_1897",
   "page_count": 4,
   "order": 510,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "This paper presents a statistical natural language generation scheme for trainable speech-to-speech machine translation (MT) systems. The natural language generation scheme in the translation systems is based on a maximum entropy (ME) statistical model fully trained from a corpus, allowing flexible translation outputs. In this paper, the system architecture and some of its components, including the parsing, information extraction, and translation etc are briefly overviewed, followed by the descriptions of training and search algorithms for ME based sentence level NLG within the MT context. Details of NLG including feature selection and robustness are also addressed. We have implemented the described system for translating between Chinese speech and English speech in an air travel application domain. Encouraging experimental results have been observed and are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-510"
  },
  "vogel02_icslp": {
   "authors": [
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Alicia",
     "Tribble"
    ]
   ],
   "title": "Improving statistical machine translation for a speech-to-speech translation task",
   "original": "i02_1901",
   "page_count": 4,
   "order": 511,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "In this paper we investigate if statistical machine translation (SMT) is possible when only a small bilingual corpus is available for training the system. Using additional knowledge sources which are not domain-specific improves the performance of the system considerably. We present results on a speech translation task for German to English. Automatic and human evaluation are used to compare the performance of the SMT system to an interlingua-based translation system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-511"
  },
  "rossato02_icslp": {
   "authors": [
    [
     "Solange",
     "Rossato"
    ],
    [
     "Hervé",
     "Blanchon"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Speech-to-speech translation system evaluation: results for French for the NESPOLE! project first showcase",
   "original": "i02_1905",
   "page_count": 4,
   "order": 512,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "In this paper we give the results of a set of evaluations conducted in the context of a speech to speech translation project (NESPOLE!). The chosen situation involves a client (French, German, American) talking to an Italian travel agent (both using their own language) to organize a stay in Italy. Fives series of evaluation were conducted on the same data set. The first series concerned the Automatic Speech Recognition alone. Two other series were about monolingual (back-) translation from ASR outputs on the data set and form transcriptions of the data set. The last ones were about bilingual translation from both the ASR outputs and the transcriptions. The goal of the evaluation was to check the performances of the system at the end of the second year of the project. The fives sets of results concerning the French modules are given and commented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-512"
  },
  "kauers02_icslp": {
   "authors": [
    [
     "Manuel",
     "Kauers"
    ],
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Christian",
     "Fügen"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Interlingua based statistical machine translation",
   "original": "i02_1909",
   "page_count": 4,
   "order": 513,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "In goal oriented spoken language translation, an interlingua based approach has proven quite useful as it (1) reduces overall effort when multiple language pairs are required, (2) can provide a paraphrase of semantic equivalence in the input language, (3) abstracts away from the disfluencies of spoken language to express the speakers intention. On the other hand, interlingua based systems are cumbersome to develop as semantic grammars have to be laboriously prepared for each input language. In this paper, we demonstrate that mappings from input text to interlingua can be learned automatically and that new input languages can be added by language projection. We show that the resulting system also delivers competitive performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-513"
  },
  "nishizawa02_icslp": {
   "authors": [
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Separation of voiced source characteristics and vocal tract transfer function characteristics for speech sounds by iterative analysis based on AR-HMM model",
   "original": "i02_1721",
   "page_count": 4,
   "order": 514,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "A new method was developed for the separation of source and transfer function characteristics of speech sounds, with an aim of utilizing it to \"flexible\" speech synthesis. The method is based on representing source waveform by an HMM, and transfer function by the AR process (AR-HMM model). As compared to methods based on ARX model, where a parametric representation is assumed for source waveform, a better separation is possible. By introducing a process of recursively deleting real poles of AR filters, which represent source waveform features, and including them intoHMMsource waveform, the resulting AR filters may correctly represent transfer function fea- tures. Experiments were conducted for Japanese vowel sounds in continuous speech, and the results were compared with those by conventional LP analysis and ARHMM model analysis without recursive process. After representing obtained source and transfer function features respectively as DFT cepstrum and LPC cepstrum, variations of cepstrum parameters for each vowel sound were compared for the three analysis methods. The smallest variations were obtained by the proposed method, indicating that the proposed method can separate source and transfer function features well, and, thus, has potential ability of generating good quality of speech when applied to \"flexible\" speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-514"
  },
  "narusawa02_icslp": {
   "authors": [
    [
     "Shuichi",
     "Narusawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Automatic extraction of model parameters from fundamental frequency contours of English utterances",
   "original": "i02_1725",
   "page_count": 4,
   "order": 515,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "The generation process model of the fundamental frequency contours (F0 contours) of speech is known to be capable of generating F0 contours quite close to observed ones. The extraction of model parameters from an observed contour, however, requires an iterative process starting from a set of initial parameter values. In order to guarantee a rapid convergence to an optimum solution, the values should be appropriate ones. We already have developed a method of automatically extracting these from given F0 contours, and applied it to Japanese sentences with good results. The method is based on approximating an observed contour by a continuous curve differentiable everywhere. In the present paper, it was applied to English utterances. Experiments were conducted for 4 native speakers utterances with 14.5% and 17.5% of average miss and false alarm rates for the accent commands, and 35.7%and 15.5%for the phrase commands.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-515"
  },
  "murakami02_icslp": {
   "authors": [
    [
     "Takahiro",
     "Murakami"
    ],
    [
     "Munehiro",
     "Namba"
    ],
    [
     "Tetsuya",
     "Hoya"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Pitch extraction of speech signals using an eigen-based subspace method",
   "original": "i02_1729",
   "page_count": 4,
   "order": 516,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "In this paper, we propose a novel method for detecting the fundamental frequencies of speech signals contaminated by noise. The proposed method exploits an eigen-based subspace principle to estimate unknown parameters of the noisy speech signal. In the proposed method, the estimated parameters are used for recovering the spectrum of the signal buried in noise, and then the restored spectrum is used for pitch extraction. Moreover, the proposed method reduces the computational complexity within the subspace method. In the simulation results, it is shown that the proposed method estimates more accurate fundamental frequencies than the conventional pitch estimation methods with saving the computational complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-516"
  },
  "nakatani02_icslp": {
   "authors": [
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Toshio",
     "Irino"
    ]
   ],
   "title": "Robust fundamental frequency estimation against background noise and spectral distortion",
   "original": "i02_1733",
   "page_count": 4,
   "order": 517,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "This paper presents a new method for robust fundamental frequency (F0) estimation in the presence of background noise and spectral distortion. We define degree of dominance and a dominance spectrum based on instantaneous frequencies. The degree of dominance allows us to evaluate the magnitude of individual harmonic components of speech signals relative to background noise while eliminating the influence of spectral distortion. The fundamental frequency is robustly estimated from reliable harmonic components easily selected from the dominance spectra. Experiments are performed using white and multi-talker background noise with and without spectral distortion produced by a SRAEN filter. Results show that the present method is better than the commonly-used methods in terms of correct F0 rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-517"
  },
  "quatieri02_icslp": {
   "authors": [
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "2-d processing of speech with application to pitch estimation",
   "original": "i02_1737",
   "page_count": 4,
   "order": 518,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "In this paper, we introduce a new approach to two-dimensional (2-D) processing of the one-dimensional (1-D) speech signal in the timefrequency plane. Specifically, we obtain the short-space 2-D Fourier transform magnitude of a narrowband spectrogram of the signal and show that this 2-D transformation maps harmonically-related signal components to a concentrated entity in the new 2-D plane. We refer to this series of operations as the \"grating compression transform\" (GCT), consistent with sine-wave grating patterns in the spectrogram reduced to smeared impulses. The GCT forms the basis of a speech pitch estimator that uses the radial distance to the largest peak in the GCT plane. Using an average magnitude difference between pitchcontour estimates, the GCT-based pitch estimator is shown to compare favorably to a sine-wave-based pitch estimator for all-voiced speech in additive white noise. An extension to a basis for two-speaker pitch estimation is also proposed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-518"
  },
  "saraclar02_icslp": {
   "authors": [
    [
     "Murat",
     "Saraclar"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Enrico",
     "Bocchieri"
    ],
    [
     "Vincent",
     "Goffin"
    ]
   ],
   "title": "Towards automatic closed captioning : low latency real time broadcast news transcription",
   "original": "i02_1741",
   "page_count": 4,
   "order": 519,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "In this paper, we present a low latency real-time Broadcast News recognition system capable of transcribing live television newscasts with reasonable accuracy. We describe our recent modeling and efficiency improvements that yield a 22% word error rate on the Hub4e98 test set while running faster than real-time. These include the discriminative training of a feature transform and the acoustic model, and the optimization of the likelihood computation. We give experimental results that show the accuracy of the system at different speeds. We also explain how we achieved low latency, presenting measurements that show the typical system latency is less than 1 second.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-519"
  },
  "prasad02_icslp": {
   "authors": [
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Automatic transcription of courtroom speech",
   "original": "i02_1745",
   "page_count": 4,
   "order": 520,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "In this paper we describe our on-going effort in developing a speech recognition system for transcribing courtroom hearings. Court hearings are a rich source of naturally occurring speech data, much of which is in public domain. The presence of multiple microphones coupled with presence of noise and reverberation makes the problem simultaneously rich and challenging. We have exploited the availability of multiple channels to mitigate, to some extent, the noise problem prevalent in courtroom speech. By using a novel technique for channel change detection, domain-specific language modeling, and unsupervised channel adaptation we have been able to achieve a word error rate (WER) of 36% on actual courtroom hearings. We also report on acoustic modeling experiments using \"legal\" transcripts for 120 hours of court hearings in a lightly supervised mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-520"
  },
  "nguyen02e_icslp": {
   "authors": [
    [
     "Long",
     "Nguyen"
    ],
    [
     "Xuefeng",
     "Guo"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Japanese broadcast news transcription",
   "original": "i02_1749",
   "page_count": 4,
   "order": 521,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "In this paper, we describe the on-going development of a Japanese Broadcast News Transcription system at BBN Technologies. This is a collaboration between BBN and NHK to use automatic speech recognition technology to provide live closed caption for NHKs TV news programs in Japan. We describe what the NHK Broadcast News Corpus comprises and how we adopted transcription technology developed for Hub-4 English broadcast news task to achieve an overall word error rate (WER) of less than 5% for Japanese TV news programs. We also report on how we obtained 30-50% relative WER reduction for weather forecast and sports news by the use of micro-domain lexicons and language models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-521"
  },
  "hecht02_icslp": {
   "authors": [
    [
     "Robert",
     "Hecht"
    ],
    [
     "Jürgen",
     "Riedler"
    ],
    [
     "Gerhard",
     "Backfried"
    ]
   ],
   "title": "German broadcast news transcription",
   "original": "i02_1753",
   "page_count": 4,
   "order": 522,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "We describe a newly created broadcast news (BN) corpus based on programs of seven different German and Austrian TV stations and the development of a German BN transcription system based on this corpus. We report on a series of experiments addressing the fact that German is less suited than English for word-based trigram language models. Furthermore, we investigate various phoneme sets and examine the difference between a transregional standard (Bavarian dialect spoken in southern Germany and Austria) and standard German (Hochdeutsch) on the word error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-522"
  },
  "imai02_icslp": {
   "authors": [
    [
     "Toru",
     "Imai"
    ],
    [
     "Atsushi",
     "Matsui"
    ],
    [
     "Shinichi",
     "Homma"
    ],
    [
     "Takeshi",
     "Kobayakawa"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Speech recognition with a re-speak method for subtitling live broadcasts",
   "original": "i02_1757",
   "page_count": 4,
   "order": 523,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "This paper describes a \"re-speak\" method for subtitling live TV broadcasts using a speech recognition system. Original on-location speech in live sport or music programs contains background noise, spontaneous or emotional speech, and the voices of speakers unknown to the recognition system, all of which cause recognition performance to deteriorate. However, if a different individual, to which the system has been adapted, carefully rephrases the original utterances in a studio, these problems can be largely overcome. Recognition experiments showed that rephrasing the commentary was effective in reducing perplexities and word error rates compared with simply repeating it. Speech recognition using the re-speak method was applied in practice to a music-based variety show and the 2002 Winter Olympic Games in order automatically to produce simultaneous subtitles for hearing-impaired viewers. A word error rate below 5% and a subtitle display delay time below three seconds were achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-523"
  },
  "takamaru02_icslp": {
   "authors": [
    [
     "Keiichi",
     "Takamaru"
    ],
    [
     "Makoto",
     "Hiroshige"
    ],
    [
     "Kenji",
     "Araki"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "Evaluation of the method to detect Japanese local speech rate deceleration applying the variable threshold with a constant term",
   "original": "i02_1761",
   "page_count": 4,
   "order": 524,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "We are aiming to detect local deceleration of Japanese spontaneous conversational speech. We have proposed the variable threshold (VT), which detects local speech rate deceleration from the sequence of time series of mora duration. In this paper, we add a constant term to the VT to detect local deceleration appropriately. The VT is applied to 167 samples of Japanese spontaneous speech taken from a spoken dialogue corpus. The results of the detection are compared with local decelerations which are perceived by a listener. The VT detects 64 phrases among whole 87 decelerated phrases. We confirm the reduction of the incorrect detection of non-decelerated portions by adding a constant term to the VT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-524"
  },
  "kirkham02_icslp": {
   "authors": [
    [
     "Sandra P.",
     "Kirkham"
    ]
   ],
   "title": "Tempo modulations in English: selected pilot study results",
   "original": "i02_1765",
   "page_count": 4,
   "order": 525,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "This paper provides a discussion of selected results of a pilot study founding a tempo project. This project potentially determines the phonetic locations for altering the speech rate for English synthetic speech in order to replicate a proposed natural pattern of tempo modulations based upon phrasal foci. A male educated speaker read Canadian English sentences at 3 speech rates and with varied focal points plus 1 neutral control, which were recorded and then digitized using Multispeech, Model 3700, version 2.2 by Kay Elemetrics Corp. By comparing the durations of the averaged syllables of the focused sentences to those of the control, the proposed tempo pattern is confirmed. The results of the pilot study are promising and warrant further examination in order to provide a model of speech rate that can be incorporated into synthetic speech creating a more natural product.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-525"
  },
  "smith02_icslp": {
   "authors": [
    [
     "Caroline L.",
     "Smith"
    ]
   ],
   "title": "Modeling durational variability in reading aloud a connected text",
   "original": "i02_1769",
   "page_count": 4,
   "order": 526,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "One of the most striking features of speech produced by humans is its enormous variability, especially in the prosody. If speech synthesizers are ever to sound more natural, they must reproduce some of this variability. A portion of the variability can be attributed to known linguistic factors, but a substantial amount remains of unknown origin. Statistical techniques can be used to imitate this variability in a probabilistic way, but it may also be possible to reduce the proportion that is attributed to unknown factors. This study investigates durational variability in ten readings of an extended passage of text by an American English speaker. The focus is on how the structure of topics in the spoken material can explain some variability in the acoustic durations, and on how variability from this and other sources might be modeled in synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-526"
  },
  "hifny02_icslp": {
   "authors": [
    [
     "Yasser",
     "Hifny"
    ],
    [
     "Mohsen",
     "Rashwan"
    ]
   ],
   "title": "Duration modeling for arabic text to speech synthesis",
   "original": "i02_1773",
   "page_count": 4,
   "order": 527,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "Duration modeling is a fundamental task of prosody generation for Text To Speech (TTS) systems. The objective of this task is to predict the duration of a speech unit from its phonological representation. Duration modeling has a significant influence on the intelligibility and the naturalness of the synthesized speech. This paper presents a Neural Network (NN) based approach to predict the duration of Arabic phonemes. The developed model utilizes neural networks to map the relation between the phonological features and duration values.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-527"
  },
  "jokisch02_icslp": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Hongwei",
     "Ding"
    ],
    [
     "Hans",
     "Kruschke"
    ],
    [
     "Guntram",
     "Strecha"
    ]
   ],
   "title": "Learning syllable duration and intonation of Mandarin Chinese",
   "original": "i02_1777",
   "page_count": 4,
   "order": 528,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "The perceived quality of synthetic speech strongly depends on its prosodic naturalness. The current paper presents a neural network based prosody model of Mandarin Chinese. Using a small but especially designed syllable database and an enhanced linguistic feature set, the novel approach enables the training of syllable duration, syllable-based F0 model points and is suitable for the multilingual prosody control in concatenative speech synthesis. The paper describes database design, neural network model, training results for Chinese and the perceptual evaluation. The results indicate the importance of the appropriate database design and the enhanced linguistic feature set. Perceptual tests of resynthesized stimuli using predicted duration values receive MOS comparable to natural speech of about 4.8.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-528"
  },
  "patwardhan02_icslp": {
   "authors": [
    [
     "Pushkar",
     "Patwardhan"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Controlling perceived degradation in spectrum envelope modeling via predistortion",
   "original": "i02_1837",
   "page_count": 4,
   "order": 529,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "The compact representation of the discrete amplitude spectrum of voiced speech by an all-pole model of the spectral envelope is considered. Based on the properties of the all-pole modeling error, the use of spectrum predistortion for improving the perceptual fit at low model orders is motivated. Warping of the frequency scale before modeling of the spectral envelope of narrowband voiced sounds is investigated by subjective listening and objective measures. It is found that, contrary to what is generally accepted, the improvement in perceived quality brought about by frequency warping actually depends to a large extent on the underlying signal spectrum distribution. An objective distance measure based on partial noise loudness is found to show high correlation with subjective judgements of degradation, indicating that auditory frequency masking plays an important role in determining the perceptual accuracy of the spectrum envelope model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-529"
  },
  "veprek02_icslp": {
   "authors": [
    [
     "Peter",
     "Veprek"
    ],
    [
     "Alan B.",
     "Bradley"
    ]
   ],
   "title": "Benefit and cost analysis of using the improved vector quantizer design algorithm for glottal source waveform compression",
   "original": "i02_1841",
   "page_count": 4,
   "order": 530,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "Vector quantizer design is an essential step required in the development of many quantization (compression) and clustering (classification) tasks. The iterative codevector readjustment and reassignment algorithm (ICRRA) is shown to outperform the traditional generalized Lloyd algorithm for the task of glottal source waveform compression. Analysis of the results reveals that the ICRRA offers improved performance at a very high confidence level. The improvement is achieved at the expense of increased computation. Storage requirements of the algorithm, however, remain virtually unchanged for typical conditions. Overall, the ICRRA is particularly suitable for off-line processing where longer execution time is well justified by the benefits of reduced reconstruction distortion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-530"
  },
  "zhong02_icslp": {
   "authors": [
    [
     "Xin",
     "Zhong"
    ],
    [
     "Jon A.",
     "Arrowood"
    ],
    [
     "Mark A.",
     "Clements"
    ]
   ],
   "title": "Speech coding and transmission for improved automatic recognition",
   "original": "i02_1845",
   "page_count": 4,
   "order": 531,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "Automatic recognition of compressed speech in such applications as voice mail or call centers has significantly degraded performance compared to non-compressed data when background noise is present. Recognition of transmitted speech, such as in cellular, voice over IP, or networked PDA input, may also face the problem of frame erasures. There have been various attempts to compensate for these two distortions using receiver-based techniques, but room for improvement may be limited. Since the demand for recognition of coded and transmitted speech is expected to increase significantly in the near future, it is of interest to determine what modifications can be made on the encoder/transmitter side. In this paper we explore issues in designing a speech coder aimed at improving recognition performance over a packet-lossy channel with minimal degradation in perceptual quality. We propose a multiple description version of a speech coder to alleviate distortions caused by frame erasures. We also propose a coder variation that uses mel-cepstral coefficients instead of linear prediction parameters as spectral specifier, allowing better recognition in noisy environments when access to the raw coder parameters is available at the receiver.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-531"
  },
  "nguyen02f_icslp": {
   "authors": [
    [
     "Phu Chien",
     "Nguyen"
    ],
    [
     "Takao",
     "Ochi"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Coding speech at very low rates using straight and temporal decomposition",
   "original": "i02_1849",
   "page_count": 4,
   "order": 532,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "This paper presents a new method for speech coding at rates around 1.2 kbps based on STRAIGHT, a high quality speech analysis-synthesis method. For encoding spectral information, Modified Restricted Temporal Decomposition (MRTD) based vector quantization is used, where MRTD is a method of temporal decomposition for line spectral frequency parameters. Meanwhile, pitch and gain parameters are coded using linear and spline interpolation, respectively. Subjective test results indicate that the performance of the proposed speech coding method is close to that of the 4.8 kbps US Federal Standard (FS-1016) CELP coder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-532"
  },
  "nieminen02_icslp": {
   "authors": [
    [
     "Toni P.",
     "Nieminen"
    ]
   ],
   "title": "Floating-point adaptive multi-rate wideband speech codec",
   "original": "i02_1853",
   "page_count": 4,
   "order": 533,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "The Adaptive Multi-Rate Wideband (AMR-WB) speech codec algorithm has been selected for wideband speech coding in wireless and wireline services by both (3) GPP and ITU-T. This paper describes an implementation of floating point Adaptive Multi-Rate Wideband (AMR-WB) codec that was approved by the Third Generation Partnership Project (3GPP) for multimedia applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-533"
  },
  "halmi02_icslp": {
   "authors": [
    [
     "Omar",
     "Halmi"
    ],
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Driss",
     "Guerchi"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "On improving the performance of analysis-by-synthesis coding using a multi-magnitude algebraic code-book excitation signal",
   "original": "i02_1857",
   "page_count": 4,
   "order": 534,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "In this paper, we propose a new coder based on the algebraic CELP (ACELP) coding technique. Our goal is to improve the quality of the obtained synthetic speech by a modification of the excitation signal of the classical coder. Such modification is accomplished by the variation of both the positions of the excitation impulses, and their amplitudes. The proposed algorithm consists of dividing the 10-ms speech frames into 2-ms sub-frames and assigning one impulse per sub-frame. These impulses are characterized by their optimized positions and amplitudes. Experiments show that our algorithm results in higher-quality speech with lower bit-rate compared to the classical ACELP algorithm. It is shown that an improvement of 0.73 dB in the segmental SNR is achieved by the proposed coder over the conventional ACELP coder. Informal listening tests show that the synthetic speech signal obtained using our algorithm is perceptually closer to the original speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-534"
  },
  "humphreys02_icslp": {
   "authors": [
    [
     "K.",
     "Humphreys"
    ],
    [
     "R.",
     "Lawlor"
    ]
   ],
   "title": "Improved performance speech codec for mobile communications",
   "original": "i02_1861",
   "page_count": 4,
   "order": 535,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "This paper presents the application of a Voice Gender Normalization algorithm to the GSM Speech Codec and describes the refinements that can be made to the Codec as a result. By reducing the dynamic range of the speech signals entering the Codec gender specific adaptations can be made to the Codec to improve its performance in terms of subjective sound quality or its transmitted bit rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-535"
  },
  "yakhnich02_icslp": {
   "authors": [
    [
     "Evgeni",
     "Yakhnich"
    ],
    [
     "Yuval",
     "Bistritz"
    ]
   ],
   "title": "Fixed-length segment coding of LSF parameters",
   "original": "i02_1865",
   "page_count": 4,
   "order": 536,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "This paper presents a method to attain very low bit-rate compression of speech spectral envelope. It is based on fixed-length segment coding. The method utilizes Temporal Decomposition (TD) technique for the compact representation of segments of Line Spectrum Frequencies (LSF) vector followed by split matrix quantization. The TD technique is modified to fit fixed-length segment coding. Computation is reduced by using fixed event functions and because event positions determination requires only a simple search within the short fixed-length segment. Weighted Euclidian distance is used as cost function to better approximate the spectral distance measure. The method achieves low bit rates without significant increase in computation cost. The method has been implemented on coding the spectral envelope for the MELP coder and showed its viability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-536"
  },
  "parsa02_icslp": {
   "authors": [
    [
     "Vijay",
     "Parsa"
    ],
    [
     "Donald G.",
     "Jamieson"
    ]
   ],
   "title": "Interaction of voice over internet protocol speech coders and disordered speech samples",
   "original": "i02_1869",
   "page_count": 4,
   "order": 537,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "Voice over IP (VoIP) is an emerging technology where voice is transmitted over packetized data networks instead of the traditional public switched telephone networks. The effective functioning of a VoIP system depends upon the quality of coding/decoding scheme, and on the networks performance in transmitting the voice packets. Traditional measures of speech coder quality and network performance employ voice samples from normal talkers and it is unclear what impact abnormal voice samples will have on the performance of a VoIP system. The objective of this paper is to investigate the influence of speech samples from talkers who have a form of common voice disorders on the performance of three speech coders typically used in VoIP systems. Our results show that there is a significant difference in the performance of low bit rate speech coders with disordered speech samples, and that the packet loss affects certain coders more than the others.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-537"
  },
  "kelleher02_icslp": {
   "authors": [
    [
     "Holly",
     "Kelleher"
    ],
    [
     "David",
     "Pearce"
    ],
    [
     "Doug",
     "Ealey"
    ],
    [
     "Laurent",
     "Mauuary"
    ]
   ],
   "title": "Speech recognition performance comparison between DSR and AMR transcoded speech",
   "original": "i02_1873",
   "page_count": 4,
   "order": 538,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "In this paper the speech recognition performance obtained when using Distributed Speech Recognition (DSR) architecture is compared to that obtained when the speech is first transcoded using the Adaptive Multi-Rate (AMR) speech codec at 4.75 and 12.2 kbps. In a likeversus- like comparison, made using the Advanced DSR Front-end and the Aurora reference back-end, the DSR architecture gives substantial gains in speech recognition performance. The evaluations measure the change in Word Error Rate (WER) on the Aurora 2 and Aurora 3 databases with \"perfect\" endpoints. The performance with AMR 4.75 is 50% worse than DSR on Aurora 2 and 47% worse on Aurora 3. Even with the higher data rate of AMR 12.2, AMR is 17% worse than DSR on Aurora 2 and 20% worse on Aurora 3.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-538"
  },
  "hirsch02_icslp": {
   "authors": [
    [
     "Hans-Günter",
     "Hirsch"
    ]
   ],
   "title": "The influence of speech coding on recognition performance in telecommunication networks",
   "original": "i02_1877",
   "page_count": 4,
   "order": 539,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "The influence of encoding and decoding speech on automatic speech recognition is investigated in this paper with respect to applications in todays telecommunication networks. The deterioration of recognition performance is presented for several coding schemes in GSM and future mobile networks. The extraction of acoustic features for the recognition is done with the already standardized ETSI frontend and with the advanced robust frontend whose standardization is almost finished. The Aurora2 experiment for recognizing the noisy TIDigits is taken as experimental basis. Finally recognition results are compared to results of subjective listening tests that have been performed for the characterisation of these speech coding schemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-539"
  },
  "moharir02_icslp": {
   "authors": [
    [
     "Gautam",
     "Moharir"
    ],
    [
     "Pushkar",
     "Patwardhan"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Spectral enhancement preprocessing for the HNM coding of noisy speech",
   "original": "i02_1881",
   "page_count": 4,
   "order": 540,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "Low rate coders based on the harmonic-noise model are sensitive to acoustic background noise at low SNRs due to the increase in parameter errors from the analysis of noisy speech. We investigate the use of spectral subtraction enhancement preprocessing on the performance of the sinusoidal model based codec both by objective assessment of parameter errors and the subjective testing of output speech quality and intelligibility. We find that while for noisy speech enhancement, improving speech quality is often accompanied by a decrease in intelligibility, in the context of coding, significant combined improvements are obtained when the speech coder is combined with a speech enhancement preprocessor.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-540"
  },
  "brun02_icslp": {
   "authors": [
    [
     "Armelle",
     "Brun"
    ],
    [
     "Kamel",
     "Smaïli"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Contribution to topic identification by using word similarity",
   "original": "i02_1965",
   "page_count": 4,
   "order": 541,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "In this paper, a new topic identification method, WSIM, is investigated. It exploits the similarity between words and topics. This measure is a function of the similarity between words, based on the mutual information. The performance of WSIM is compared to the cache model and to the well-known SVM classifier. Their behavior is also studied in terms of recall and precision, according to the training size. Performance of WSIM reaches 82.4% correct topic identification. It outperforms SVM(76.2%) and has a comparable performance with the cache model (82.0%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-541"
  },
  "zhou02c_icslp": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speechfind: an experimental on-line spoken document retrieval system for historical audio archives",
   "original": "i02_1969",
   "page_count": 4,
   "order": 542,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "In this study, we present the SpeechFind system, an experimental online spoken document retrieval system for historical audio archives. As part of an on-going U.S. NSF Digital Library Initiative project, entitled the National Gallery of the Spoken Word (NGSW), SpeechFind is intended to serve as an audio index and search engine for spoken word collections spanning the 20th century with as much as 60,000 hours of audio archives. In this paper, we describe the system architecture of SpeechFind, with focus on audio data transcription and information retrieval components. Using a sample test audio data collection from the past 60 years, an evaluation of individual system components and overall performance is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-542"
  },
  "suzuki02_icslp": {
   "authors": [
    [
     "Yoshimi",
     "Suzuki"
    ],
    [
     "Fumiyo",
     "Fukumoto"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "Topic tracking using subject templates",
   "original": "i02_1973",
   "page_count": 4,
   "order": 543,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "Topic tracking, which starts from a few sample stories and finds all subsequent stories that discuss the same topic, is a new challenge for the text categorization task and makes a significant contribution to the accessibility of information, such as archives of news, e-mails, and historical newspapers. Much previous research on topic tracking uses machine learning techniques. However, the small size of the training data, especially positive training stories, presents diffi- culties in training the parameters of the tracking system to produce optimal results. In this paper, we present a method for topic tracking using subject templates to select an optimal training set. The method was tested on the TV news which are the outputs of a speech recognizer, and the result shows the effectiveness of the method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-543"
  },
  "asami02_icslp": {
   "authors": [
    [
     "Katsushi",
     "Asami"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Genichiro",
     "Kikui"
    ]
   ],
   "title": "Topic detection of an utterance for speech dialogue processing",
   "original": "i02_1977",
   "page_count": 4,
   "order": 544,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "This paper proposes a method for topic detection of an utterance for speech dialogue processing including speech translation. There are three important points to consider in realizing topic detection of an utterance. The first is to obtain information from a short utterance. The second is to deal with a broad topic. The third is to have robustness against speech recognition errors. Our topic detection method is suitable as a solution to these problems. To deal with the first two problems, our method detects the topic of an utterance according to the relation factor between the topics and the words included in utterances. The third problem is solved by merging words with different surface forms. To verify the performance of our proposed method, we carried out experiments by using a broadcoverage travel conversation corpus. The experimental results show that the proposed method achieved 79.3% topic detection accuracy for correct transcriptions, and 75.6% accuracy for recognizers outputs whose WER was 12.8%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-544"
  },
  "liu02c_icslp": {
   "authors": [
    [
     "Daben",
     "Liu"
    ],
    [
     "Jeffrey",
     "Ma"
    ],
    [
     "Dongxin",
     "Xu"
    ],
    [
     "Amit",
     "Srivastava"
    ],
    [
     "Francis",
     "Kubala"
    ]
   ],
   "title": "Real-time rich-content transcription of Chinese broadcast news",
   "original": "i02_1981",
   "page_count": 4,
   "order": 545,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "This paper describes the recent development of an Audio Indexing System for Chinese (Mandarin) broadcast news. Key issues of the three major components: automatic speech recognition, speaker identification and named entity extraction are addressed. The Chinese-language-specific challenges are discussed and our solutions are described. The recognition accuracy of the final system is comparable to the best-known state-of-the-art systems, while the throughput is below real time. The accuracy of the speaker identification and named entity extraction is comparable to our English system. The Chinese system currently runs 24×7 on a satellite feed of CCTV-4 broadcast news data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-545"
  },
  "wang02d_icslp": {
   "authors": [
    [
     "Chun-Jen",
     "Wang"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Improved Chinese spoken document retrieval with hybrid modeling and data-driven indexing features",
   "original": "i02_1985",
   "page_count": 4,
   "order": 546,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "Different models retrieve the documents based on different approaches of extracting the underlying content. Different levels of indexing features also offer different functionalities and discriminabilities when retrieving the documents. In this paper, we present results for Chinese spoken document retrieval with hybrid models to integrate the knowledge obtainable from three basic retrieval models, namely, the standard vector space model (VSM), the hidden Markov model (HMM), and the latent semantic indexing (LSI) model. The characteristics of retrieval performance using both word-level and syllablelevel indexing features were extensively explored. In addition, a data-driven approach to derive variable-length indexing features is also presented. Very satisfactory performance can be achieved with these data-driven features while retaining very compact feature set size. Experiments showed that this approach has the potential to identify domain-specific terminologies or newly-generated phrases. It is therefore very useful not only in Chinese document retrieval, but also in detecting out of vocabulary (OOV) words in Chinese. Very encouraging results were obtained when the hybrid models were used with the data-driven indexing features as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-546"
  },
  "larson02_icslp": {
   "authors": [
    [
     "Martha",
     "Larson"
    ],
    [
     "Stefan",
     "Eickeler"
    ],
    [
     "Gerhard",
     "Paaß"
    ],
    [
     "Edda",
     "Leopold"
    ],
    [
     "Jörg",
     "Kindermann"
    ]
   ],
   "title": "Exploring sub-word features and linear support vector machines for German spoken document classification",
   "original": "i02_1989",
   "page_count": 4,
   "order": 547,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "Using sub-word features for spoken document classification raises two potential drawbacks. First, if the speech recognizer recognizes sub-word units directly, the risk arises that word-level discriminative features are irretrievably lost. This effect is aggravated by depressed recognition accuracy, such as that associated with speakerand domain-independent systems. Second, if input documents are expanded by combining subword units into higher-level features, in compensation for lacking word-level discriminators, the size of the classifier input space expands rapidly, inviting the danger of over- fitting. This paper reports results of experiments with a simple, but real-world, binary topic classification task on a corpus of un-edited German-language radio documents. We compare a Naive Bayes classifier to a Linear Support Vector Machine (LSVM) and determine that benefits of sub-word features indeed outweigh potential drawbacks. The LSVM in particular profits from subword features supplemented by higher-order combinations, reflecting its ability to control input space complexity independently of dimension.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-547"
  },
  "wester02_icslp": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Goal-directed ASR in a multimedia indexing and searching environment (MUMIS)",
   "original": "i02_1993",
   "page_count": 4,
   "order": 548,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "This paper describes the contribution of automatic speech recognition (ASR) within the framework of MUMIS (Multimedia Indexing and Searching Environment). The domain is football commentaries. The initial results of carrying out ASR on Dutch and English football commentaries are presented. We found that overall word error rates are high, but application specific words are recognized reasonably well. The difficulty of the ASR task is greatly increased by the high levels of noise present in the material.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-548"
  },
  "logan02_icslp": {
   "authors": [
    [
     "Beth",
     "Logan"
    ],
    [
     "J. M. Van",
     "Thong"
    ]
   ],
   "title": "Confusion-based query expansion for OOV words in spoken document retrieval",
   "original": "i02_1997",
   "page_count": 4,
   "order": 549,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "We present a novel approach to the out of vocabulary (OOV) query problem for audio indexing. Our technique first builds a word index for the audio using speech recognition. It then expands query words into in-vocabulary phrases according to intrinsic acoustic confusability and language model scores. The aim is to mimic the mistakes the speech recognizer makes when transcribing the OOV words. We present results of retrieval experiments on a broadcast news repository of 75 hours. Our results indicate that our approach is promising. Our technique is better than simply using word queries and only slightly worse than a more sophisticated scheme which expands queries into overlapping sequences of phonemes. We can also combine our technique with the phoneme indexing system to further improve performance. Finally, our approach is simple, requires only a word index be built for the audio and has little computational overhead.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-549"
  },
  "wickramaratna02_icslp": {
   "authors": [
    [
     "J. T.",
     "Wickramaratna"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Cluster identification for speaker-environment tracking",
   "original": "i02_2001",
   "page_count": 4,
   "order": 550,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "Cluster Identification is introduced as the process of jointly evaluating clustering and labelling schemes for cluster-labelling scheme selection. Normalized Rand and BBN metrics for comparing clustering performances across varied clustering and labelling schemes are presented. The merits of the metrics are evaluated and applied for speaker-environment tracking in Broadcast News.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-550"
  },
  "pinquier02_icslp": {
   "authors": [
    [
     "Julien",
     "Pinquier"
    ],
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Régine",
     "André-Obrecht"
    ]
   ],
   "title": "Robust speech / music classification in audio documents",
   "original": "i02_2005",
   "page_count": 4,
   "order": 551,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "This paper deals with a novel approach to speech / music segmentation. Three original features, entropy modulation, stationary segment duration and number of segments are extracted. They are merged with the classical (4) Hz modulation energy. The relevance of these features is studied in a first experiment based on a development corpus composed of collected samples of speech and music. Another corpus is employed to verify the robustness of the algorithm. This experiment is made on a TV movie soundtrack and shows performances reaching a correct identification rate of 90%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-551"
  },
  "karneback02_icslp": {
   "authors": [
    [
     "Stefan",
     "Karnebäck"
    ]
   ],
   "title": "Expanded examinations of a low frequency modulation feature for speech/music discrimination",
   "original": "i02_2009",
   "page_count": 4,
   "order": 552,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "A low frequency modulation feature, LFMAD, was examined under several conditions with regard to its robustness on speech/music discrimination. The feature was tested on LF components from 2 Hz to 27 Hz and with different analysis window sizes. This feature performs best when using an analysis window size containing only one period of the LF component to be used. When the music contained much vocals, the error rate increased compared with only instrumental music in the speech/music discrimination task. This effect was found in LFMAD as well as in the MFCC feature, which was used for comparison. Tests were also carried out with signals in additive noise from 30 dB to 0 dB SNR. LFMAD performed better than MFCC in these tests. The error rate was higher for speech signals. There was a bias towards classifying data as music when the test conditions diverged from those of the training condition. This effect is less obvious for LFMAD than for MFCC. The best results in this study were obtained when combining the two features LFMAD and MFCC into a mixed feature. This seems to be a more robust feature regarding the speech/music discrimination ability and could be recommended when scanning data bases of unknown quality for speech events.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-552"
  },
  "ezzaidi02_icslp": {
   "authors": [
    [
     "Hassan",
     "Ezzaidi"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Speech, music and songs discrimination in the context of handsets variability",
   "original": "i02_2013",
   "page_count": 4,
   "order": 553,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "The problem of speech, music and music with songs discrimination in telephony with handsets variability is addressed in this paper. Two systems are proposed. The first system uses three Gaussian Mixture Models (GMM) for speech, music and songs respectively. Each GMM comprises 8 Gaussians trained on very short sessions. Twenty six speakers (13 females, 13 males) have been randomly chosen from the SPIDRE corpus. The music were obtained from a large set of data and comprises various styles. For 138 minutes of testing time, a speech discrimination score of 97.9% is obtained when no channel normalization is used. These performance are obtained for a relatively short analysis frame (32ms sliding window, buffering of 100 ms). When using channel normalization, an important score reduction (on the order of 10 to 20%) is observed. The second system has been designed for applications requiring shorter processing times along with shorter training sessions. It is based on an empirical transformation of the . MFCC that enhances the dynamical evolution of tonality. It yields in average an acceptable discrimination rate of 90% (speech-/music) and 84% (speech, music and songs with music).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-553"
  },
  "scherer02_icslp": {
   "authors": [
    [
     "Klaus R.",
     "Scherer"
    ],
    [
     "D.",
     "Grandjean"
    ],
    [
     "Tom",
     "Johnstone"
    ],
    [
     "Gudrun",
     "Klasmeyer"
    ],
    [
     "Thomas",
     "Bänziger"
    ]
   ],
   "title": "Acoustic correlates of task load and stress",
   "original": "i02_2017",
   "page_count": 4,
   "order": 554,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "It is argued that reliable acoustic profiles of speech under stress can only be found if different types of stress are clearly distinguished and experimentally induced. We report first results of a study with 100 speakers from three language groups, using a computer-based induction procedure that allows distinguishing cognitive load due to task engagement from psychological stress. Findings show significant effects of load, and partly of stress, for speech rate, energy contour, F0, and spectral parameters. It is further suggested that the mean results for the complete sample of speakers do not reflect the amplitude of stress effects on the voice. Future research should isolate and focus on speakers for whom the psychological stress induction has been successful.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-554"
  },
  "rahurkar02_icslp": {
   "authors": [
    [
     "Mandar A.",
     "Rahurkar"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "James",
     "Meyerhoff"
    ],
    [
     "George",
     "Saviolakis"
    ],
    [
     "Michael",
     "Koenig"
    ]
   ],
   "title": "Frequency band analysis for stress detection using a teager energy operator based feature",
   "original": "i02_2021",
   "page_count": 4,
   "order": 555,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "Studies have shown that the performance of speech recognition algorithms severely degrade due to the presence of task and emotional induced stress in adverse conditions. This paper addresses the problem of detecting the presence of stress in speech by analyzing nonlinear feature characteristics in specific frequency bands. The framework of the previously derived Teager Energy Operator(TEO) based feature TEO-CB-AutoEnv is used. A new detection scheme is proposed based on weighted TEO features derived from critical bands frequencies. The new detection framework is evaluated on a military speech corpus collected in a Soldier of the Quarter (SOQ) paradigm. Heart rate and blood pressure measurements confirm subjects were under stress. Using the traditional TEO-CB-AutoEnv feature with an HMM trained stressed speech classifier, we show error rates of 22.5% and 13% for stress and neutral speech detection. With the new weighted sub-band detection scheme, detection error rates are reduced to 4.7% and 4.6% for stress and neutral detection, a relative error reduction of 79.1% and 64.6% respectively. Finally we discuss issues related to generation of stress anchor models and speaker dependency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-555"
  },
  "yuan02_icslp": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Liqin",
     "Shen"
    ],
    [
     "Fangxin",
     "Chen"
    ]
   ],
   "title": "The acoustic realization of anger, fear, joy and sadness in Chinese",
   "original": "i02_2025",
   "page_count": 4,
   "order": 556,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "This paper studies the acoustic realization of anger, fear, joy and sadness in Chinese. An emotion database of total 288 sentences was collected from nine speakers. Four listeners were asked to judge the emotion type of each sentence, choosing from anger, fear, joy, sadness and neutral. The results suggest that there are two dimensions in the acoustic realization of anger, fear, joy and sadness in Chinese. We then conducted acoustic analyses on the database. The acoustic attributes of each emotion in the domains of phonation, articulation and prosody are reported. We concluded that anger and fear are mainly realized on phonation; joy is mainly realized on prosody of F0; and sadness is realized on both of the two dimensions. Articulation and prosody of duration play a secondary role in the acoustic realization of the emotions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-556"
  },
  "tato02_icslp": {
   "authors": [
    [
     "Raquel",
     "Tato"
    ],
    [
     "Rocío",
     "Santos"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "J. M.",
     "Pardo"
    ]
   ],
   "title": "Emotional space improves emotion recognition",
   "original": "i02_2029",
   "page_count": 4,
   "order": 557,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "A number of recent studies have focused on the conceptualized expression of emotions as a three-dimensional space. This paper proposes a new approach to emotion recognition, making use of two of the emotional dimensions and their relationship with different kinds of features. The main idea consists in associating prosodic features, derived from pitch, loudness, and duration, with the activation or arousal dimension, and quality features, i.e. phonation type, articulation manner, voice timbre, with the evaluation and pleasure dimension, in a way that different classification methods can be applied for each specific case. Most important results are achieved for the speaker- independent case and three classes, with a recognition rate close to 80%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-557"
  },
  "chuang02_icslp": {
   "authors": [
    [
     "Ze-Jing",
     "Chuang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Emotion recognition from textual input using an emotional semantic network",
   "original": "i02_2033",
   "page_count": 4,
   "order": 558,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "This paper presents an emotion recognition system with textual input. In this system, an emotional semantic network is proposed to extract the semantic information related to emotion. The semantic network is composed of two subnetworks: a static semantic network and a dynamic semantic network. The static semantic network is established from an existing Chinese knowledge base called HowNet and used to estimate the emotion trigger value of each word. The dynamic semantic network accepts the textual input and dynamically constructs the nodes and links, which represent the emotion carrier and the emotion propagator respectively. Initiated by the emotion trigger value, the emotion in the dynamic semantic network will propagate and finally converge to the final emotion output. Experimental results show an encouraging achievement was obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-558"
  },
  "ang02_icslp": {
   "authors": [
    [
     "Jeremy",
     "Ang"
    ],
    [
     "Rajdip",
     "Dhillon"
    ],
    [
     "Ashley",
     "Krupski"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Prosody-based automatic detection of annoyance and frustration in human-computer dialog",
   "original": "i02_2037",
   "page_count": 4,
   "order": 559,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "We investigate the use of prosody for the detection of frustration and annoyance in natural human-computer dialog. In addition to prosodic features, we examine the contribution of language model information and speaking \"style\". Results show that a prosodic model can predict whether an utterance is neutral versus \"annoyed or frustrated\" with an accuracy on par with that of human interlabeler agreement. Accuracy increases when discriminating only \"frustrated\" from other utterances, and when using only those utterances on which labelers originally agreed. Furthermore, prosodic model accuracy degrades only slightly when using recognized versus true words. Language model features, even if based on true words, are relatively poor predictors of frustration. Finally, we find that hyperarticulation is not a good predictor of emotion; the two phenomena often occur independently.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-559"
  },
  "makarova02_icslp": {
   "authors": [
    [
     "Veronika",
     "Makarova"
    ],
    [
     "Valery A.",
     "Petrushin"
    ]
   ],
   "title": "RUSLANA: a database of Russian emotional utterances",
   "original": "i02_2041",
   "page_count": 4,
   "order": 560,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "This paper describes a database of affective (emotional) utterances for the Russian language. The database contains recordings of 61 speakers (12 male and 49 female) who pronounce ten sentences neutrally (unemotionally) and expressing the following five emotional states: surprise, happiness, anger, sadness and fear. The database is constructed as a material source for linguistic and speech processing research on communicative and emotive-attitudinal aspects of spoken language. It can also be used for creating automatic emotion recognition and production systems on speaker-and-gender- independent, gender-dependent and speaker-dependent levels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-560"
  },
  "oneill02_icslp": {
   "authors": [
    [
     "Ian M.",
     "ONeill"
    ],
    [
     "Michael F.",
     "McTear"
    ]
   ],
   "title": "A pragmatic confirmation mechanism for an object-based spoken dialogue manager",
   "original": "i02_2045",
   "page_count": 4,
   "order": 561,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "Using a relatively simple confirmation strategy based on confirmation statuses, discourse pegs and request templates it is possible to implement an effective mixed initiative, natural language dialogue system. If the basic confirmation strategy is inherited by a set of specialist domain objects, the approach becomes particularly useful, allowing dialogues in several business areas to be conducted in a single implementation. The following paper outlines the basic confirmation mechanism and illustrates how this can be augmented by rules of thumb that are encapsulated in implemented domain experts and that recommend particular strategies for continuing a transaction, given particular combinations of confirmed and uncon- firmed information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-561"
  },
  "torge02_icslp": {
   "authors": [
    [
     "Sunna",
     "Torge"
    ],
    [
     "Stefan",
     "Rapp"
    ],
    [
     "Ralf",
     "Kompe"
    ]
   ],
   "title": "Serving complex user wishes with an enhanced spoken dialogue system",
   "original": "i02_2049",
   "page_count": 4,
   "order": 562,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "In the past years spoken dialogue systems became more and more sophisticated, i.e., they allow for rather complex and flexible dialogues. On the other hand the functionality of devices, applications and services (henceforth applications) and the amount of digital content increased rapidly. Due to the network age (Internet, personal area networks,...) the applications we can/want to control change dynamically as well as the content. Therefore a dialogue system cannot be anymore manually designed for one application before product release. Instead a layer in between is required, which translates the device functionalities in a way that they can be used via a more or less generic dialogue system. Furthermore, a network of devices creates new functionalities not possible with a single device. Users will have complex wishes; often several devices will be necessary to solve such a wish. However, the user in most cases would not like to care about how many and exactly which applications are used to fulfill his wish. From his perspective he sees the functionality provided by a network of devices as a virtual device. In this paper a module is described which builds a link between dialogue system and an ensemble of applications. Functionalities of applications are described in a formal way. A planning component searches for the applications necessary to solve a complex user wish and the sequence of action which has to be performed. Simplicity of the approach is very important. the module has been successfully integrated in a prototype system which was publicly demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-562"
  },
  "chung02b_icslp": {
   "authors": [
    [
     "Grace",
     "Chung"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Integrating speech with keypad input for automatic entry of spelling and pronunciation of new words",
   "original": "i02_2053",
   "page_count": 4,
   "order": 563,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "This paper describes research whose ultimate aim is to support automatic entry of new words into a spoken dialogue system through interaction with a user. This research demonstrates an important step towards this goal, through a procedure which integrates information made available via the telephone keypad with a spoken instance of the target word, to produce a candidate spelling and pronunciation for the word. Through the use of a parsing mechanism applied to a 73,000 word proper name lexicon [4], we have been able to create a finite-state transducer (FST) that maps phonetics to graphemics, which can be composed with an FST derived from the keypad input to greatly reduce the search space. Experiments conducted on both the OGI name corpus [2] and a set of enrollment data obtained from our Mercury system [5] validate the procedure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-563"
  },
  "campana02_icslp": {
   "authors": [
    [
     "Ellen",
     "Campana"
    ],
    [
     "Sarah",
     "Brown-Schmidt"
    ],
    [
     "Michael K.",
     "Tanenhaus"
    ]
   ],
   "title": "Reference resolution by human partners in a natural interactive problem-solving task",
   "original": "i02_2057",
   "page_count": 4,
   "order": 564,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "We examined how listeners circumscribe referential domains for referring expressions by monitoring participants eye movements as they engaged in a natural interactive problem-solving task with another naïve participant. This research had two goals: (1) determine whether existing psycholinguistic methodologies for studying online processing can be extended to interactive conversation, and (2) assess whether studying language in this manner can provide insight into the theoretical limitations of various strategies for reference resolution commonly employed in computational spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-564"
  },
  "ferrer02_icslp": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Is the speaker done yet? faster and more accurate end-of-utterance detection using prosody",
   "original": "i02_2061",
   "page_count": 4,
   "order": 565,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "We examine the problem of end-of-utterance (EOU) detection for realtime speech recognition, particularly in the context of a human-computer dialog system. Current EOU detection algorithms use only a simple pause threshold for making this decision, leading to two problems. First, especially as speech-driven interfaces become more natural, users often pause inside utterances, resulting in a premature cut off by the system. Second, when users really are done, the minimum system wait is always the threshold value, needlessly adding time to the interaction. We have developed a new approach to EOU detection that uses prosodic features to address both of these problems. Prosodic features are modeled by decision trees and combined with an event N-gram language model to obtain a score that measures the likelihood that any non-speech region is an EOU. We find that this approach dramatically improves both the accuracy and speed of online EOU detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-565"
  },
  "gorrell02_icslp": {
   "authors": [
    [
     "Genevieve",
     "Gorrell"
    ],
    [
     "Ian",
     "Lewin"
    ],
    [
     "Manny",
     "Rayner"
    ]
   ],
   "title": "Adding intelligent help to mixed-initiative spoken dialogue systems",
   "original": "i02_2065",
   "page_count": 4,
   "order": 566,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "The rapidly expanding voice recognition industry has so far shown a preference for grammar-based language modelling, despite the better overall performance of statistical language modelling. Given that the advantages of the grammar-based approach make it unlikely to be replaced as the primary solution in the near future, it is natural to wonder whether some combination of the two approaches may prove useful. Here, we describe an implemented system that uses statistical language modelling and a decision-tree classifier to provide the user with some feedback when grammar-based recognition fails. Users of this system had more successful interactions than did users of a control system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-566"
  },
  "shin02_icslp": {
   "authors": [
    [
     "Jongho",
     "Shin"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Laurie",
     "Gerber"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Dani",
     "Byrd"
    ]
   ],
   "title": "Analysis of user behavior under error conditions in spoken dialogs",
   "original": "i02_2069",
   "page_count": 4,
   "order": 567,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "We focus on developing an account of user behavior under error conditions, working with annotated data from real human-machine mixed initiative dialogs. In particular, we examine categories of error perception, user behavior under error, effect of user strategies on error recovery, and the role of user initiative in error situations. A conditional probability model smoothed by weighted ASR error rate is proposed. Results show that users discovering errors through implicit confirmations are less likely to get back on track (or succeed) and take a longer time in doing so than other forms of error discovery such as system reject and reprompts. Further successful user error-recovery strategies included more rephrasing, less contradicting, and a tendency to terminate error episodes (cancel and startover) than to attempt at repairing a chain of errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-567"
  },
  "jiang02b_icslp": {
   "authors": [
    [
     "Yinglong",
     "Jiang"
    ],
    [
     "Peter",
     "Murphy"
    ]
   ],
   "title": "Production based pitch modification of voiced speech",
   "original": "i02_2073",
   "page_count": 4,
   "order": 568,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "Previous research has shown that the voice source is strongly correlated with speech quality [1][2][3]. However in many existing pitch modification algorithms only the impulse train excitation is modi- fied, while the voice source is normally included in vocal tract transfer function and remains unchanged during modification. We present a production based pitch-scale modification scheme, which modifies the voice source waveform extracted from the speech signal. Experiment on synthesized voice sounds shows that compared to previous methods, better quality can be achieved for a wider pitch modification range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-568"
  },
  "sun02b_icslp": {
   "authors": [
    [
     "Xuejing",
     "Sun"
    ]
   ],
   "title": "F0 generation for speech synthesis using a multi-tier approach",
   "original": "i02_2077",
   "page_count": 4,
   "order": 569,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "In this paper we propose a multi-tier phonetic model for intonation synthesis. Four tiers are defined aiming at different components of intonation. The parametric representation for each tier is realized through underlying pitch target. Regression trees are built for each tier upon appropriate feature set. Both numerical evaluation and informal listening test have yielded encouraging results. The synthesized sentences can be accessed at http://mel.speech.northwestern.- edu/sunxj/prosody.htm\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-569"
  },
  "strom02_icslp": {
   "authors": [
    [
     "Volker",
     "Strom"
    ]
   ],
   "title": "From text to prosody without toBI",
   "original": "i02_2081",
   "page_count": 4,
   "order": 570,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "A new method for predicting prosodic parameters, i.e. phone durations and F0 targets, from preprocessed text is presented. The prosody model comprises a set of CARTs, which are learned from a large database of labeled speech. This database need not be annotated with Tone and Break Indices (ToBI labels). Instead, a simpler symbolic prosodic description is created by a bootstrapping method. The method had been applied to one Spanish and two German speakers. For the German voices, two listening tests showed a significant preference for the new method over a more traditional approach of prosody prediction, based on hand-crafted rules.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-570"
  },
  "hirose02b_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Masaya",
     "Eto"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Improved corpus-based synthesis of fundamental frequency contours using generation process model",
   "original": "i02_2085",
   "page_count": 4,
   "order": 571,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "We have been developing corpus-based synthesis of fundamental frequency (F0) contours for Japanese text-to-speech (TTS) conversion systems. Since, in our method, the synthesis is done under the constraint of F0 contour generation process model, a rather good quality is still kept even if the prediction process is done incorrectly. Although it was already shown that the synthesized F0 contours sounded as highly natural as those using heuristic rules arranged by experts, there were occasional cases with low quality depending on sentences to be synthesized. Several features, including a code representing syntactic boundary depth obtainable through an automatic parsing process, were added to input parameters of the statistical methods, and a better prediction was realized. The boundary depth code was shown to be very effective for improving especially phrase component parameter prediction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-571"
  },
  "buhmann02_icslp": {
   "authors": [
    [
     "Jeska",
     "Buhmann"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Lieve",
     "Macken"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Intonation modelling for the synthesis of structured documents",
   "original": "i02_2089",
   "page_count": 4,
   "order": 572,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "Human readings of structured documents exhibit a much richer intonation than that observed in read isolated sentences. It is a challenge to capture this richness in an automatic way using data-driven techniques. In this paper, we extend our previous research on intonation modelling for isolated sentences in different respects: (i) the RNN (Recurrent Neural Network) intonation model is now trained and evaluated on read documents, (ii) the model is evaluated as part of the overall prosody model, (iii) the feature selection process is completely automated, and (iv) the importance of text-level features such as text type, text structure and typesetting are investigated. It is demonstrated that acceptable intonation models can be constructed starting from a database that does not contain any explicit hand labelling of the intonation contours. It also appears that text type and text structure are important features whereas type-setting is not.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-572"
  },
  "meron02_icslp": {
   "authors": [
    [
     "Joram",
     "Meron"
    ]
   ],
   "title": "Applying fallback to prosodic unit selection from a small imitation database",
   "original": "i02_2093",
   "page_count": 4,
   "order": 573,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "This paper presents an extension to a previous work [1], which used an imitation speech database and a prosodic unit selection algorithm, for improving the naturalness of synthesized speech. The basic approach of the system is to combine a rule-generated prosody with a corpus based prosody module, trying to retain both the robustness of the rule prosody, and the naturalness of the human recorded speech units. This combination was achieved by using a database of imitation speech, enabling a higher level of annotation, which is used by a dynamic unit selection algorithm. Although listeners have been shown to prefer the prosody generated with this method over that of the original rule generated prosody, the usual problems related to selection from an undersized training corpus were occasionally present.\n",
    "Instead of increasing the size of the training database, a different solution is investigated here, which is to perform a controlled fallback to the rule prosody, but in a way which is compatible with the unit selection approach. The suggested method has a minimal effect on the required memory size and the amount of computation, and was shown to produce favorable results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-573"
  },
  "tao02_icslp": {
   "authors": [
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Clustering and feature learning based F0 prediction for Chinese speech synthesis",
   "original": "i02_2097",
   "page_count": 4,
   "order": 574,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "The paper describes a Chinese prosody model based on clustering and feature learning method. As the tonal language, the features of Chinese prosody are analyzed in accordance with the various context information. Focusing on the notion of prosody templates, we confirmed that a F0 pattern can be extracted based on various context parameters for each syllable. Statistic algorithm was used for template selection and training. Finally, the paper analyzes the error distribution of the F0 predicting results. Unlike other methods, the approach may give feedback as to exactly what the crucial parameters are that determine the successful choice of patterns. Both acoustic validation test and listening test show that the synthesis results are much closed to human being. And the system has been used widely in applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-574"
  },
  "weber02_icslp": {
   "authors": [
    [
     "Katrin",
     "Weber"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Evaluation of formant-like features for ASR",
   "original": "i02_2101",
   "page_count": 4,
   "order": 575,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "This paper investigates possibilities to automatically find a low-dimensional, formant-related physical representation of the speech signal, which is suitable for automatic speech recognition (ASR). This aim is motivated by the fact that formants have been shown to be discriminant features for ASR. Combinations of automatically extracted formant-like features and conventional, noise- robust, state-of-theart features (such as MFCCs including spectral subtraction and cepstral mean subtraction) have previously been shown to be more robust in adverse conditions than state-of-the-art features alone. However, it is not clear how these automatically extracted formant-like features behave in comparison with true formants. The purpose of this paper is to investigate two methods to automatically extract formant-like features, and to compare these features to hand-labeled formant tracks as well as to standard MFCCs in terms of their performance on a vowel classification task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-575"
  },
  "aldulaimy02_icslp": {
   "authors": [
    [
     "Fadhil H. T.",
     "Al-Dulaimy"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Entropy of energy operator as feature for large vocabulary Mandarin speaker independent speech recognition",
   "original": "i02_2105",
   "page_count": 4,
   "order": 576,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "This work demonstrates the Non-linear Time-frequency distribution of Discrete Time Energy Operator DTEO based on AM-FM demodulation techniques, assuming that the individual component signals are spectrally isolated by each other and can be modeled as discretetime mono-component AM-FM signals. This is proposed to be use with its entropy as an input to a Duration Distribution Based Hidden Markov Module (DDBHMM) in Speaker Independent (SI) Large Vocabulary Mandarin Speech Recognition (SI-LVMSR) system, by combining the feature vectors as output of the front-end detection stage. The goal is to improve the performance of the existing system by combining new features in the baseline feature vector. Using the preemphasized filter in the front end of the present recognizer causes an increase in the noise energy at high frequencies above 4 kHz, which in some cases degrades the recognition accuracy. This paper also involves dealing with this problem by eliminating pre-emphasis filters with entropy of NLDTEO combined with MFCC instead of the traditional techniques. The experiment results show significant reduction (24.96%) in relative error rate, by using the new technique with eliminating the pre-emphasized filter from pre-processing stage. the PEF from the pre-processing stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-576"
  },
  "zhang02f_icslp": {
   "authors": [
    [
     "Yiyan",
     "Zhang"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Huayun",
     "Zhang"
    ]
   ],
   "title": "Improving parametric trajectory modeling by integration of pitch and tone information",
   "original": "i02_2109",
   "page_count": 4,
   "order": 577,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "This paper presents the application of pitch/tone information to improve Parametric Trajectory Modeling (PTM). To simulate the trajectory of pitch in a segment in PTM recognizer, we in fact get its corresponding tone information. From another point of view, tone is a segmental feature and PTM has the excellent framework to incorporate it. So we here introduce the \"soft\" and \"hard\" integration methods to combine them with the base modeling. In the experiment of Mandarin digit classification, they get 22.87% and 33.54% error reduction respectively, and integration of both them together obtains 38.72% error reduction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-577"
  },
  "tolba02_icslp": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Comparative experiments to evaluate the use of auditory-based acoustic distinctive features and formant cues for automatic speech recognition using a multi-stream paradigm",
   "original": "i02_2113",
   "page_count": 4,
   "order": 578,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "This paper presents an evaluation of the use of some auditory-based acoustic distinctive features and formant cues for automatic speech recognition (ASR). Comparative experiments have indicated that the use of either the formant magnitudes or the formant frequencies combined with some auditory-based acoustic distinctive features and the classical MFCCs within a multi-stream statistical framework leads to an improvement in the recognition performance of HMM- based ASR systems. The Hidden Markov Model Toolkit (HTK) was used throughout our experiments to test the use of the new multi-stream feature vector. A series of experiments on speakerindependent continuous-speech recognition have been carried out using a subset of the large read-speech corpus TIMIT. Using such multi-stream paradigm, N-mixture tri-phone models and a bigram language model, we found that the word error rate was decreased by about 6.46%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-578"
  },
  "leung02_icslp": {
   "authors": [
    [
     "Ka-Yee",
     "Leung"
    ],
    [
     "Manhung",
     "Siu"
    ]
   ],
   "title": "Speech recognition using combined acoustic and articulatory information with retraining of acoustic model parameters",
   "original": "i02_2117",
   "page_count": 4,
   "order": 579,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "Articulatory features (AF) are recently proposed as an alternative representation of the acoustic features (ACF) and combining an AF model and an ACF model has been shown to outperform the ACF model. In this paper, we investigated multiple ways to further improve the combination of an AF model and an ACF model. First, we propose a multiple-distribution AF model that increases models resolution by separately modeling different sub-phone segments. We then introduce the asynchrony combination of this multiple-distribution AF model with an ACF model to allow flexible combination of AF model \"states\" with different ACF model states. Second, we incorporate AF information into the ACF model training such that the ACF model is optimized to give the best performance when combining with the AF model for decoding. The combination of both techniques results in an absolute improvement of 2.5% in TIMIT phone recognition over the corresponding ACF model baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-579"
  },
  "wilkinson02_icslp": {
   "authors": [
    [
     "N. J.",
     "Wilkinson"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Improved phone recognition on TIMIT using formant frequency data and confidence measures",
   "original": "i02_2121",
   "page_count": 4,
   "order": 580,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "This paper presents a novel approach to integration of formant frequency and conventional MFCC data in phone recognition experiments on TIMIT. Naive use of format data introduces classification errors if formant frequency estimates are poor, resulting in a net drop in performance. However, by exploiting a measure of confi- dence in the formant frequency estimates, formant data can contribute to classification in parts of a speech signal where it is reliable, and be replaced by conventional MFCC data when it is not. In this way an improvement of 4.7% is achieved. Moreover, by exploiting the relationship between formant frequencies and vocal tract geometry, simple formant-based vocal tract length normalisation reduces the error rate by 6.1% relative to a conventional representation alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-580"
  },
  "kitaoka02b_icslp": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Daisuke",
     "Yamada"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Speaker independent speech recognition using features based on glottal sound source",
   "original": "i02_2125",
   "page_count": 4,
   "order": 581,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "We discussed utilization of features based on the glottal sound source for speaker independent speech recognition. It has been thought that such features as pitch cannot contribute to speaker independent speech recognition because of the dominant speaker dependent factor.\n",
    "In this paper, we tried to utilize pitch, power, LPC residual power, voicing rate, and their regression coefficients as feature parameters for speaker independent speech recognition, and found that regression parameters of F0, power and LPC residual power could improve the performance, especially using covariances between each parameter and conventional MFCC. This showed that the procedure to derive the regression parameters could reduce the speaker dependent factor which appeared as biases of those features, and that the correlation between glottal source information and spectral envelope information (MFCC) worked well.\n",
    "We also tested the parameters on a large-vocabulary continuous speech recognition task and obtained the performance improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-581"
  },
  "omar02_icslp": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ],
    [
     "Ken",
     "Chen"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Yigal",
     "Brandman"
    ]
   ],
   "title": "An evaluation of using mutual information for selection of acoustic-features representation of phonemes for speech recognition",
   "original": "i02_2129",
   "page_count": 4,
   "order": 582,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "This paper addresses the problem of finding a subset of the acoustic feature space that best represents the phoneme set used in a speech recognition system. A maximum mutual information approach is presented for selecting acoustic features to be combined together to represent the distinctions among the phonemes. The overall phoneme recognition accuracy is slightly increased for the same length of feature vector for clean speech and at 10 dB compared to FFT-based Mel-frequency cepstrum coefficients (MFCC) by using acoustic features selected based on a maximum mutual information criterion.\n",
    "Using 16 different feature sets, the rank of the feature sets based on mutual information can predict phoneme recognition accuracy with a correlation coefficient of 0.71 compared to a correlation coef- ficient of 0.28 when using a criterion based on the average pair-wise Kullback-Liebler divergence to rank the feature sets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-582"
  },
  "metze02_icslp": {
   "authors": [
    [
     "Florian",
     "Metze"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "A flexible stream architecture for ASR using articulatory features",
   "original": "i02_2133",
   "page_count": 4,
   "order": 583,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "Recently, speech recognition systems based on articulatory features such as \"voicing\" or the position of lips and tongue have gained interest, because they promise advantages with respect to robustness and permit new adaptation methods to compensate for channel, noise, and speaker variability. These approaches are also interesting from a general point of view, because their models use phonological and phonetic concepts, which allow for a richer description of a speech act than the sequence of HMM-states, which is the prevalent ASR architecture today. In this work, we present a multi-stream architecture, in which CD-HMMS are supported by detectors for articulatory features, using a linear combination of log-likelihood scores. This multi-stream approach results in a 15% reduction of WER on a read Broadcast-News (BN) task and improves performance on a spontaneous scheduling task (ESST) by 7%. The proposed architecture potentially allows for new speaker and channel adaptation schemes, including stream asynchronicity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-583"
  },
  "ljolje02_icslp": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Speech recognition using fundamental frequency and voicing in acoustic modeling",
   "original": "i02_2137",
   "page_count": 4,
   "order": 584,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "Prosody has long been studied as a knowledge source in speech processing. We attempt to directly exploit prosodic correlates in acoustic modeling of speech for large vocabulary recognition. We compare two methods for using the fundamental frequency and voicing parameters. The more complex approach starts by modeling prosodic classes and using a representation of their recognized sequences as acoustic features. The simpler approach simply adds suitably normalized raw values to the conventional mel cepstral coefficients in the observation vectors. The simpler approach achieves modest accuracy gains on HUB-5 Eval-2001 test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-584"
  },
  "karnjanadecha02_icslp": {
   "authors": [
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Patimakorn",
     "Kimsawad"
    ]
   ],
   "title": "A comparison of front-end analyses for Thai speech recognition",
   "original": "i02_2141",
   "page_count": 4,
   "order": 585,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "The main purpose of this work was to find a suitable front-end analysis for Thai speech recognition by comparing the performance of the LPCC, MFCC and DCTC front-ends using several Thai continuous digit recognition tasks. HTK tools were used to build a word-based HMM recognizer which could handle several styles of digit string realization. Experimental results show that MFCC and DCTC perform equally well, and outperform LPCC. Dynamic parameters were also added to the feature sets to improve modeling accuracy. This work can be regarded as a baseline for further study of Thai speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-585"
  },
  "turunen02_icslp": {
   "authors": [
    [
     "Jari",
     "Turunen"
    ],
    [
     "Juha T.",
     "Tanttu"
    ],
    [
     "Pekka",
     "Loula"
    ]
   ],
   "title": "New model for speech residual signal shaping with static nonlinearity",
   "original": "i02_2145",
   "page_count": 4,
   "order": 586,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "In this paper we apply a nonlinear Hammerstein model for speech analysis and coding using frame-based parameter solving. This paper presents a method for calculating the adaptive frame-based coeffi- cients for a Hammerstein model by modifying the residual signal to the desired form. The method is such that the nonlinear and linear coefficients can be obtained without iterations. In experiments the residual information was reduced significantly by using proper nonlinearities and residual shaping in the preprocessing phase.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-586"
  },
  "ho02_icslp": {
   "authors": [
    [
     "Ching-Hsiang",
     "Ho"
    ],
    [
     "Dimitrios",
     "Rentzos"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "Formant model estimation and transformation for voice morphing",
   "original": "i02_2149",
   "page_count": 4,
   "order": 587,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "In this paper we consider the estimation and mapping of time-varying formant model parameters and orders for voice transformation. The model order is the number of perceptually significant formant trajectories estimated from an analysis of the poles of \"over-modelled\" linear prediction models of the source and target speech. A 2-D HMM with NF left-to-right states across frequency and M states across time is used to classify formant observations into NF sequential formant clusters. A formant-based non-uniform frequency warping method is proposed for voice transformation. In this method speech spectrum is divided into NF+1 formant bands. A transformation is estimated for each formant band of a phoneme model. Multi-mixture Gaussians are used to model the distribution of parameters in each formant band. The voice mapping yields perceptually high quality results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-587"
  },
  "megyesi02_icslp": {
   "authors": [
    [
     "Beáta",
     "Megyesi"
    ],
    [
     "Sofia",
     "Gustafson-Capková"
    ]
   ],
   "title": "Production and perception of pauses and their linguistic context in read and spontaneous speech in Swedish",
   "original": "i02_2153",
   "page_count": 4,
   "order": 588,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "We investigate the relationship between prosodic phrase boundaries in terms of pausing and the linguistic structure on morpho-syntactic and discourse levels in spontaneous dialogues as well as in read aloud speech in Swedish. Both the speakers production and the listeners perception of pausing are considered and mapped to the linguistic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-588"
  },
  "manfredi02_icslp": {
   "authors": [
    [
     "Claudia",
     "Manfredi"
    ],
    [
     "Lorenzo",
     "Matassini"
    ]
   ],
   "title": "Non-linear techniques for dysphonic voice analysis and correction",
   "original": "i02_2157",
   "page_count": 4,
   "order": 589,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "This paper aims at finding suitable parameters for dysphonic voice analysis and classification. Moreover, a non-linear noise reduction scheme is proposed, for voice correction. Typical quantities from chaos theory and some conventional ones are evaluated, in order to provide entries for feature vectors in a feature space. Geometric signal separation is applied for voice classification, by means of a properly defined healthy index. This allows moving the feature vector of pathological voices from the sick region to the healthy one, in order to enhance voice quality. The practical advantage is twofold: first, physicians are provided with a better understanding of voice dysfunctions for surgical and rehabilitation purposes, second noninvasive devices for voice denoising could be build up as an aid for dysphonic people.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-589"
  },
  "sasou02_icslp": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Adaptive estimation of time-varying features from high-pitched speech based on an excitation source HMM",
   "original": "i02_2161",
   "page_count": 4,
   "order": 590,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "This paper describes a method of extracting time-varying features that is effective for speech signals with high fundamental frequencies. The proposed method adopts a speech production model that consists of a Time-Varying Auto-Regressive (TVAR) process for an articulatory filter and a Hidden Markov Model (HMM) for an excitation source. The model represents waveform amplitude variations by time-varying gain of the excitation source. The proposed algorithm is given by extending a Viterbi algorithm so that the proposed algorithm can adaptively estimate TVAR coefficients and time-varying gain with decoding the state transition of the excitation source HMM. We applied the proposed method to extracting time-varying features from both synthetic and natural speech, and confirmed its feasibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-590"
  },
  "toda02_icslp": {
   "authors": [
    [
     "Martine",
     "Toda"
    ],
    [
     "Shinji",
     "Maeda"
    ],
    [
     "Andreas J.",
     "Carlen"
    ],
    [
     "Lyes",
     "Meftahi"
    ]
   ],
   "title": "Lip gestures in English sibilants: articulatory - acoustic relationship",
   "original": "i02_2165",
   "page_count": 4,
   "order": 591,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "The supraglottal differences between alveolar and postalveolar English sibilants, beside the place of articulation, involve the length of the constriction (shorter for alveolars) [1, (2), 3], the width of constriction (narrower for alveolars)[2, 4], the presence or absence of a sublingual cavity (absence for alveolars) [1, 2], the depth of tongue groove (deeper for postalveolars) [2], and also lip shape (protruded for postalveolars) [1, (2), 3].\n",
    "This paper is concerned with articulatory data on lip protrusion and lip opening area (rounding) of sibilants uttered by an American speaker, and the related acoustic data. Articulatory-acoustic mapping indicates that only protrusion may be playing a role in lowering the frequencies of postalveolars. An estimation based on a quarterwave length resonance predicts the difference in the tube length between alveolars and postalveolars being twice the difference measured in articulatory data. Some configurations are suggested for further acoustic modeling, in order to better examine the potential contribution of sublingual cavity, in particular, and the effect of its coupling with lips.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-591"
  },
  "malayath02_icslp": {
   "authors": [
    [
     "Naren",
     "Malayath"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Bark resolution from speech data",
   "original": "i02_2169",
   "page_count": 4,
   "order": 592,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "This paper discusses the relevance of non-uniform frequency resolution used by current speech analysis methods like Mel frequency analysis and perceptual linear predictive (PLP) analysis. It is shown that linear discriminant analysis of short-time Fourier spectrum of speech yields spectral basis functions which provide comparatively lower resolution to the high frequency region of spectrum. This is consistent with critical-band resolution and is shown to be caused by the spectral properties of vowel sounds. Further, we show that this non-uniform resolution can be traced to the physiology of speech production mechanism. In ASR experiments, features extracted by the discriminant functions are shown to outperform the conventional features derived by cosine basis functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-592"
  },
  "selouani02_icslp": {
   "authors": [
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Noise-robust speech recognition in car environments using genetic algorithms and a mel-cepstral subspace approach",
   "original": "i02_2173",
   "page_count": 4,
   "order": 593,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "In this paper a new enhancement scheme for highly noise-corrupted data is presented and evaluated. The approach combines the Karhunen- Loève Transform (KLT) and Genetic Algorithms (GAs) in order to enhance noise-corrupted speech. The principle consists of projecting noisy Mel-Frequency Cepstral Coefficients (MFCCs) onto the space generated by the principal axes issued from the KLT analysis and optimized by genetic operators such as crossover and mutations. Results show that the proposed hybrid technique, when included in the front-end of an HTK-based continuous speech recognition system, outperforms that of the conventional recognition process in severe interfering car noise environments. Experiments concern a wide range of Signal-to-Noise-Ratios (SNRs) varying from 16 dB to -4 dB, and use a noisy version of the TIMIT speech database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-593"
  },
  "axelrod02_icslp": {
   "authors": [
    [
     "Scott",
     "Axelrod"
    ],
    [
     "Ramesh",
     "Gopinath"
    ],
    [
     "Peder",
     "Olsen"
    ]
   ],
   "title": "Modeling with a subspace constraint on inverse covariance matrices",
   "original": "i02_2177",
   "page_count": 4,
   "order": 594,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "We consider a family of Gaussian mixture models for use in HMM based speech recognition system. These \"SPAM\" models have state independent choices of subspaces to which the precision (inverse covariance) matrices andmeans are restricted to belong. They provide a flexible tool for robust, compact, and fast acoustic modeling. The focus of this paper is on the case where the means are unconstrained. The models in the case already generalize the recently introduced EMLLT models, which themselves interpolate between MLLT and full covariance models. We describe an algorithm to train both the state-dependent and state-independent parameters. Results are reported on one speech recognition task. The SPAM models are seen to yield significant improvements in accuracy over EMLLT models with comparable model size and runtime speed. We find a 10%relative reduction in error rate over an MLLT model can be obtained while decreasing the acoustic modeling time by 20%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-594"
  },
  "mccowan02_icslp": {
   "authors": [
    [
     "Iain A.",
     "McCowan"
    ],
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Improving speech recognition performance of small microphone arrays using missing data techniques",
   "original": "i02_2181",
   "page_count": 4,
   "order": 595,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "Traditional microphone array speech recognition systems simply recognise the enhanced output of the array. As the level of signal enhancement depends on the number of microphones, such systems do not achieve acceptable speech recognition performance for arrays having only a few microphones. For small microphone arrays, we instead propose using the enhanced output to estimate a reliability mask, which is then used in missing data speech recognition. In missing data speech recognition, the decoded sequence depends on the reliability of each input feature. This reliability is usually based on the signal to noise ratio in each frequency band. In this paper, we use the energy difference between the noisy input and the enhanced output of a small microphone array to determine the frequency band reliability. Recognition experiments with a small array demonstrate the effectiveness of the technique, compared to both traditional microphone array enhancement and a baseline missing data system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-595"
  },
  "gelbart02_icslp": {
   "authors": [
    [
     "David",
     "Gelbart"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Double the trouble: handling noise and reverberation in far-field automatic speech recognition",
   "original": "i02_2185",
   "page_count": 4,
   "order": 596,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "Far-field microphone speech signals cause high error rates for automatic speech recognition systems, due to room reverberation and lower signal-to-noise ratios. We have observed large increases in speech recognition word error rates when using a far-field (3-6 feet)microphone in a conference room, in comparison with recordings from close-talking microphones. In an earlier paper, we showed improvements in far-field speech recognition performance using a long-term log spectral subtraction method to combat reverberation. This method is based on a principle similar to cepstral mean subtraction but uses a much longer analysis window (e.g., 1 s) in order to deal with reverberation. Here we show that a combination of short-term noise filtering and long-term log spectral subtraction can further reduce recognition word error rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-596"
  },
  "couvreur02_icslp": {
   "authors": [
    [
     "Laurent",
     "Couvreur"
    ],
    [
     "Christophe",
     "Ris"
    ]
   ],
   "title": "Model-based independent component analysis for robust multi-microphone automatic speech recognition",
   "original": "i02_2189",
   "page_count": 4,
   "order": 597,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "In this communication, we present a method for noise-robust multimicrophone automatic speech recognition (ASR). It is assumed that the speech source to be recognized is recorded with several microphones in a noisy acoustic environment. The proposed method estimates the short-term subband energies (as they are needed for computing the ASR front-end) of the clean speech source from the ones of the microphone noisy signals. The estimation procedure is based on the concept of Independent Component Analysis (ICA) and it is driven by the acoustic model used by the ASR decoder. The method is shown to be highly robust for a connected digit recognition task in high noise conditions, improving word error rates by more than 50% relatively to the performance of the baseline single-microphone ASR system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-597"
  },
  "yu02b_icslp": {
   "authors": [
    [
     "An-Tze",
     "Yu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Compensation of channel effect on line spectrum frequencies",
   "original": "i02_2193",
   "page_count": 4,
   "order": 598,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "Line Spectrum Frequencies (LSFs) is an effective and efficient representation for low bit-rate (LBR) speech coding. It is also appealing to use LSFs in speech or speaker recognition within a digital communication based system. However, the channel effect on LSFs degrades the recognition performance. This paper attempts to treat the problem of channel effect in LSF domain so that the recognition performance can be improved. A formulation describing the channel effect in channel phase is derived. Based on this formulation, the methods of channel estimation and channel mean subtraction are developed for the reduction of the channel effect. Experiments on connected digits recognition, where the speech signal is affected by channel or microphone, are conducted to show the effectiveness of the proposed methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-598"
  },
  "zhang02g_icslp": {
   "authors": [
    [
     "Huayun",
     "Zhang"
    ],
    [
     "Zhaobing",
     "Han"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Codebook dependent dynamic channel estimation for Mandarin speech recognition over telephone",
   "original": "i02_2197",
   "page_count": 4,
   "order": 599,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "Automatic speech recognition in telecommunications environment still has a lower correct rate compared to its desktop pairs. Improving the performance of telephone-quality speech recognition is an urgent problem for its application in those practical fields. Previous works have shown that the main reason for this performance degradation is the variational mismatch caused by different telephone channels between the testing and training sets. In this paper, we propose an efficient implementation to dynamically compensate this mismatch. This algorithm bases on maximum-likelihood (ML) estimation of telephone channels and dynamically follows the timevariations within the channels. It could deal with both linear channels (like fixed telephone lines) degradation and some noisy nonlinear channels (like some long distance lines and wireless circuit lines, such as GSM) degradation. In our experiments on Mandarin large vocabulary continuous speech recognition (LVCSR) over telephone lines, the average character error rate (CER) decreases more than 20% when applying this algorithm. At the same time, the structural delay and computational consumptions required by this algorithm are limited. The average delay is about 300~400ms. So it could be embedded into practical telephone-based applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-599"
  },
  "gemello02_icslp": {
   "authors": [
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ],
    [
     "Paolo",
     "Pegoraro"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Robust multiple resolution analysis for automatic speech recognition",
   "original": "i02_2201",
   "page_count": 4,
   "order": 600,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "This paper describes the use of denoising techniques in the time domain applied to the outputs of filters corresponding to a Multi Resolution Analysis. The fact that energies of denoised samples are used for Automatic Speech Recognition (ASR) makes soft thresholding particularly attractive especially if Principal Component Analysis (PCA) is applied to the whole tree of energy features. This consideration is supported by experimental results on a very large test set including many speakers uttering proper names from different locations of the Italian public telephone network. The results show that soft thresholding outperforms J-Rasta PLP with a WER reduction, after denoising, of 26%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-600"
  },
  "peinado02_icslp": {
   "authors": [
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Victoria",
     "Sánchez"
    ],
    [
     "José L.",
     "Pérez-Córdoba"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "HMM-based methods for channel error mitigation in distributed speech recognition",
   "original": "i02_2205",
   "page_count": 4,
   "order": 601,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "Distributed Speech Recognition involves the development of techniques to mitigate the degradations that the transmission channel introduces in the speech features. This work proposes an HMM framework from which different mitigation techniques oriented to bursty channels can be derived. In particular, two MMSE-based and a new Viterbi-based mitigation procedures are derived under this framework. Several implementation issues such as the channel SNR estimation or the application of hard decision on the received signal vectors are dealt with. Also, different boundary conditions suitable for the speech recognition application are studied for the different mitigation procedures. The experimental results show that the HMM- based techniques can effectively mitigate channel errors, even in very poor channel conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-601"
  },
  "fingscheidt02_icslp": {
   "authors": [
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Stefanie",
     "Aalburg"
    ],
    [
     "Sorel",
     "Stan"
    ],
    [
     "Christophe",
     "Beaugeant"
    ]
   ],
   "title": "Network-based vs. distributed speech recognition in adaptive multi-rate wireless systems",
   "original": "i02_2209",
   "page_count": 4,
   "order": 602,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "Distributed speech recognition (DSR) is motivated by the fact that codecs used in speech transmission usually reveal a degrading voice quality below some channel quality (carrier-to-interferer ratio C/I), which justifies efficient coding of features with an appropriate channel coding in the mobile terminal. The Adaptive Multi-Rate (AMR) speech codec standardized for GSM and UMTS however delivers an acceptable speech quality way down to C/I ratios of about 4 dB in the GSM full-rate speech channel.\n",
    "In this paper we investigate network-based speech recognition (NSR) using a conventional speech channel with AMR coding as an alternative to a DSR system. This approach is natural and attractive since information services usually require a duplex channel for conversation anyway, furthermore no change to existing mobiles is required. For a GSM full-rate channel it turns out that an NSR system based on AMR coding indeed is comparable to DSR approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-602"
  },
  "bernard02_icslp": {
   "authors": [
    [
     "Alexis",
     "Bernard"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Channel noise robustness for low-bitrate remote speech recognition",
   "original": "i02_2213",
   "page_count": 4,
   "order": 603,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "In remote (or distributed) speech recognition , the recognition features are quantized at the client, and transmitted to the server via wireless or packet-based communication for recognition. In this paper, we investigate the issue of robustness of remote speech recognition applications against channel noise. The techniques presented include: 1) optimal soft decision channel decoding allowing for error detection, 2) weighted Viterbi recognition (WVR) with weighting coefficients based on the channel decoding reliability, 3) frame erasure concealment, and 4) WVR with weighting coefficients based on the quality of the erasure concealment operation. The techniques presented are implemented at the receiver (server), which limit the complexity for the client, and significantly extend the range of channel conditions for which remote recognition can be sustained. As a case study, we illustrate that remote recognition based on perceptual linear prediction (PLP) coefficients is able to provide at less than 500 bps, good recognition accuracy over a wide range of channel conditions. Filtering the Spectral Parameters to Mitigate the\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-603"
  },
  "pelaezmoreno02_icslp": {
   "authors": [
    [
     "C.",
     "Peláez-Moreno"
    ],
    [
     "A.",
     "Gallardo-Antolín"
    ],
    [
     "J.",
     "Vicente-Peña"
    ],
    [
     "F.",
     "Díaz-de-María"
    ]
   ],
   "title": "Influence of transmission errors on ASR systems",
   "original": "i02_2217",
   "page_count": 4,
   "order": 604,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "When voice-enabled remote access to information and services is considered, new sources of distortions -due to modern communication networks- should be taken into account. Focusing on mobile communication systems, speech coding distortion and transmission errors severely affect the performance of automatic speech recognition systems.\n",
    "In this paper, we propose a filtering technique to mitigate the influence of transmission errors on recognition performance. In particular, considering the temporal evolution of each spectral parameter used for recognition as a temporal signal, we postulate that artificial high frequencies are generated in theses signals due to transmission errors. We suggest filtering them out. The proposed technique is assessed for two different parameterisations (type of spectral parameters): MFCC (Mel Frequency Cepstral Coefficients) and LSP parameters (Line Spectral Pairs). The results show that filtering is clearly effective.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-604"
  },
  "tsuge02_icslp": {
   "authors": [
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Masami",
     "Shishibori"
    ],
    [
     "Fuji",
     "Ren"
    ],
    [
     "Kenji",
     "Kita"
    ]
   ],
   "title": "Robust feature extraction in a variety of input devices on the basis of ETSI standard DSR front-end",
   "original": "i02_2221",
   "page_count": 4,
   "order": 605,
   "p1": "2221",
   "pn": "2224",
   "abstract": [
    "This paper reports an evaluation of European Telecommunications Standards Institute (ETSI) standard Distributed Speech Recognition (DSR) front-end through continuous word recognition on a Japanese speech corpus and proposes a method, the Bias Removal Method (BRM), that reduces the distortion between feature vector and VQ codebook. Experimental results show that using non-quantized features in acoustic model training procedure can improve the recognition performance of DSR front-end features and that the proposed method can improve recognition performances of DSR front-end feature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-605"
  },
  "tan02_icslp": {
   "authors": [
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Channel error protection scheme for distributed speech recognition",
   "original": "i02_2225",
   "page_count": 4,
   "order": 606,
   "p1": "2225",
   "pn": "2228",
   "abstract": [
    "This paper describes ongoing research preparing for the widespread deployment of spoken language processing in networks encompassing wired and wireless transmission channels. The paper gives a brief overview of the standardized bit-error protection scheme aimed at minimising channel transmission errors and used within the distributed speech recognition (DSR) paradigm. Within the ETSI-DSR standard, two quantised mel-spectral frames - each of 10 ms duration - are grouped together and protected with a 4-bit Cyclic Redundancy Checking (CRC) forming a frame-pair. However, this causes the entire frame-pair erroneous if a one-bit error only occurs in the frame-pair packet. Over an error-prone transmission channel this format will cause severe problems. To overcome this, the paper presents a one-frame architecture in which a 4-bit CRC is calculated to protect each frame independently. This scheme results in that the overall probability of one frame in error is lower, or that an error occurring in one frame does not affect another frame. A number of simple recognition experiments have been conducted to verify the introduction of the one-frame CRC protection scheme for a number of simulated transmission channel biterror rates (BER) ranging from 0 (no transmission channel involved) to 2.10-2. Experimental results show that the one-frame protection scheme is more robust to channel errors although a slight increase in the error-protection overhead is needed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-606"
  },
  "muthusamy02_icslp": {
   "authors": [
    [
     "Yeshwant",
     "Muthusamy"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Roshan",
     "Gupta"
    ]
   ],
   "title": "The effects of speech compression on speech recognition and text-to-speech synthesis",
   "original": "i02_2229",
   "page_count": 4,
   "order": 607,
   "p1": "2229",
   "pn": "2232",
   "abstract": [
    "Speech is a natural input/output modality for wireless access to information on the web. One way to overcome resource constraints on current wireless devices is to locate the speech recognition and text-to-speech systems on remote servers and transmit compressed speech between the server and wireless clients. To this end, we evaluated the effects of speech compression on the performance of a continuous speech recognizer (with and without noise compensation) and a commercially available text-to-speech system. Standard speech coders such as G.729 (8.0 kb/s), GSM EFR (12.2 kb/s), GSM FR(13.0 kb/s), and G.726 (32.0 kb/s) were used. Our results show that for recognition of digit strings and command phrases recorded in a variety of noise conditions, degradation due to speech coding is more pronounced for clean speech than noisy speech. The GSM coders actually improved recognition performance on noisy speech. For text-to-speech, the GSM EFR was ranked the highest in our A/B listening tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-607"
  },
  "milner02b_icslp": {
   "authors": [
    [
     "Ben",
     "Milner"
    ],
    [
     "Xu",
     "Shao"
    ]
   ],
   "title": "Transform-based feature vector compression for distributed speech recognition",
   "original": "i02_2233",
   "page_count": 4,
   "order": 608,
   "p1": "2233",
   "pn": "2236",
   "abstract": [
    "The technique of distributed speech recognition (DSR) has recently become an interesting area of research. One of the main issues with DSR is the need to compress the feature vector stream, produced on the terminal device, into a sufficiently low bit-rate such that it can be sent across low bandwidth channels. This work proposes a compression technique based upon first transforming a block of feature vectors into a more compact matrix representation. Columns of the resulting matrix that correspond to faster temporal variation can be removed without loss in recognition performance. The number of bits allocated to the remaining coefficients in the matrix is determined automatically, based on a measure of the information present.\n",
    "Experiments show that the transform-based compression gives good recognition accuracy at bit rates of 4800, 2400 and 1200bps. For example at 1200bps the recognition performance is 98.03% compared to 98.57% with uncompressed speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-608"
  },
  "johnston02_icslp": {
   "authors": [
    [
     "Michael",
     "Johnston"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Amanda",
     "Stent"
    ],
    [
     "Gunaranjan",
     "Vasireddy"
    ],
    [
     "Patrick",
     "Ehlen"
    ]
   ],
   "title": "Multimodal language processing for mobile information access",
   "original": "i02_2237",
   "page_count": 4,
   "order": 609,
   "p1": "2237",
   "pn": "2240",
   "abstract": [
    "Interfaces for mobile information access need to allow users flexibility in their choice of modes and interaction style in accordance with their preferences, the task at hand, and their physical and social environment. This paper describes the approach to multimodal language processing in MATCH (Multimodal Access To City Help), a mobile multimodal speech-pen interface to restaurant and subway information for New York City. Finite-state methods for multimodal integration and understanding enable users to interact using pen, speech, or dynamic combinations of the two, and a speech-act based multimodal dialogue manager enables mixed-initiative multimodal dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-609"
  },
  "wang02e_icslp": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "SALT: a spoken language interface for web-based multimodal dialog systems",
   "original": "i02_2241",
   "page_count": 4,
   "order": 610,
   "p1": "2241",
   "pn": "2244",
   "abstract": [
    "This paper describes the Speech Application Language Tags, or SALT, an emerging spoken language interface standard for multimodal or speech-only applications. A key premise in SALT design is speechenabled user interface shares a lot of the design principles and computational requirements with the graphical user interface (GUI). As a result, it is logical to introduce into speech the object-oriented, event-driven model that is known to be flexible and powerful enough in meeting the requirements for realizing sophisticated GUIs. It is hopeful that reusing this rich infrastructure can enable dialog designers to focus more on the core user interface design issues than on the computer and software engineering details. The paper centers the discussion on the Web-based distributed computing environment and elaborates how SALT can be used to implement multimodal dialog systems. How advanced dialog effects (e.g., cross-modality reference resolution, implicit confirmation, multimedia synchronization) can be realized in SALT is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-610"
  },
  "bennett02b_icslp": {
   "authors": [
    [
     "Christina",
     "Bennett"
    ],
    [
     "Ariadna Font",
     "Llitjós"
    ],
    [
     "Stefanie",
     "Shriver"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Building voiceXML-based applications",
   "original": "i02_2245",
   "page_count": 4,
   "order": 611,
   "p1": "2245",
   "pn": "2248",
   "abstract": [
    "The Language Technologies Institute (LTI) at Carnegie Mellon University has, for the past several years, conducted a lab course in building spoken-language dialog systems. In the most recent versions of the course, we have used (commercial) web-based development environments to build systems. This paper describes our experiences and discusses the characteristics of applications that are developed within this framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-611"
  },
  "chai02_icslp": {
   "authors": [
    [
     "Joyce",
     "Chai"
    ]
   ],
   "title": "Operations for context-based multimodal interpretation in conversational systems",
   "original": "i02_2249",
   "page_count": 4,
   "order": 612,
   "p1": "2249",
   "pn": "2252",
   "abstract": [
    "In a multimodal conversation, user inputs are usually abbreviated or imprecise. Only fusing inputs together is inadequate in reaching a full understanding. To address this problem, we have developed a context-based approach for multimodal interpretation. In particular, we present three operations: ordering, covering, and aggregation. Using feature structures that represent intention and attention identified from user inputs and the overall conversation, these operations provide a mechanism to combine multimodal fusion and context-based inference. These operations allow our system to process a variety of user multimodal inputs including those incomplete and ambiguous ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-612"
  },
  "liu02d_icslp": {
   "authors": [
    [
     "Feng",
     "Liu"
    ],
    [
     "Antoine",
     "Saad"
    ],
    [
     "Li",
     "Li"
    ],
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "A distributed multimodal dialogue system based on dialogue system and web convergence",
   "original": "i02_2253",
   "page_count": 4,
   "order": 613,
   "p1": "2253",
   "pn": "2256",
   "abstract": [
    "In this paper, we describe a distributed multimodal dialogue system architecture based on the concept of hybrid-VoiceXML. It utilizes a special hybrid-construct to integrate multiple multimedia, multimodal processes into one dialogue that includes VoiceXML as its voice modality. The hybrid-construct in our approach has several important functions. It provides an additional abstraction layer for dynamic dialogue generation, which can greatly improve the ef- ficiency and flexibility of the dialogue system. Under the proposed approach, the dialogue control between each interaction channel can be exchanged through the interface of a dynamic XML page. Several case studies are performed. It indicates that the proposed hybrid- VoiceXML approach is highly extensible. It can be used to form platform independent and distributed extensions for multimodal dialogue interaction beyond voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-613"
  },
  "katsurada02_icslp": {
   "authors": [
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Yoshihiko",
     "Ootani"
    ],
    [
     "Yusaku",
     "Nakamura"
    ],
    [
     "Satoshi",
     "Kobayashi"
    ],
    [
     "Hirobumi",
     "Yamada"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "A modality-independent MMI system architecture",
   "original": "i02_2549",
   "page_count": 4,
   "order": 614,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "This paper discusses the design of a modality-independent MMI system architecture. In the architecture, the MMI system is divided into three modules: the document server module which holds dialog scenarios and contents, the dialog manager which controls dialog flow, and the front-end module which manages the users inputs and the systems outputs. This division enables us to reuse the document server module and the dialog manager when introducing new terminals with different types of modalities because they are independent of modalities. Moreover, we propose an MMI description language XISL. Since it has the flexibility to describe the users inputs and the systems outputs, it can be used for describing interactions on various terminals without introducing a new description language and its processor. We show a prototype system of an online shopping application implemented on our architecture, and compare the difference between XISL and other languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-614"
  },
  "armaroli02_icslp": {
   "authors": [
    [
     "Cristiana",
     "Armaroli"
    ],
    [
     "Ivano",
     "Azzini"
    ],
    [
     "Lorenza",
     "Ferrario"
    ],
    [
     "Toni",
     "Giorgino"
    ],
    [
     "Luca",
     "Nardelli"
    ],
    [
     "Marco",
     "Orlandi"
    ],
    [
     "Carla",
     "Rognoni"
    ]
   ],
   "title": "An architecture for a multi-modal web browser",
   "original": "i02_2553",
   "page_count": 4,
   "order": 615,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "The very rapid evolution of telecommunication technology is leading to the convergence of fixed and mobile networks and devices. At present, it is very widespread for people to access the Web with Internet connections using HTML and/or WML (Wireless Markup Language) browsers, and present portable devices (e.g. PC/PDA, GPRS/WAP phones) offer a range of features (e.g. large memories, graphical displays, friendly user interfaces, communication interfaces, including the possibility to install Internet browsers) that make them suitable for hosting quite all of the applications that can be normally performed by standard PCs. Their main limitation is related to the reduced input/output capabilities, since they frequently lack of an alphanumeric keyboard and have very small displays. In this case, the development of multi-modal browser with voice input/output capabilities and/or making use of other devices (e.g. graphic pointing, touch screens, small numeric keyboards, etc.) should satisfy a large variety of user requirements.\n",
    "The idea we propose consists in the definition (and consequent realization) of an architecture capable of handling multi-modal browsing through the synchronization of HTML and VoiceXML documents. In doing this, we have to consider issues related to the variability of user/terminal profiles, as well as issues related to the layout adaptation to different presentation modalities (e.g. spatial/temporal axes and hyperlinking). VoiceXML enables users to browse documents by speaking and hearing on a phone, but does not support a graphic interface, as HTML or WML do. We propose to synchronize different documents through a specific platform instead of adding new features to existing HTML, WML or VoiceXML documents. This approach has the advantage of allowing, in a quite general way, multi-modal browsing of existing HTML documents by developing corresponding VoiceXML documents. In any case, we do not exclude the possibility of defining an XML schema that will include, on a general basis, both HTML and VoiceXML syntax, thus allowing a multimodal definition of an application in a single document. What we want to point out is that the system we are proposing can be used in a quite general way, provided that the Markup documents describing the web service to realize are correctly interpreted by specific components. The work presented in this paper has been partially developed inside the E.U. project Homey [2, 3]. The purpose of this project is to monitor the clinical state of chronic patients (in particular patients affected by hypertension pathologies), by means of the telephone, as will be described in section 3. Another application under investigation is to use the multimodal browser for accessing the WebFabIS information system (a system for the data management described in section 3). The benefits resulting from the adoption of mobile devices are currently being investigated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-615"
  },
  "ehlen02_icslp": {
   "authors": [
    [
     "Patrick",
     "Ehlen"
    ],
    [
     "Michael",
     "Johnston"
    ],
    [
     "Gunaranjan",
     "Vasireddy"
    ]
   ],
   "title": "Collecting mobile multimodal data for match",
   "original": "i02_2557",
   "page_count": 4,
   "order": 616,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "Next-generation multimodal systems designed for use in mobile environments present challenges to the task of data collection that are not faced by speech-based systems. We discuss some established data collection and evaluation methods and their limitations in the context of a mobile multimodal system. These limitations are addressed by the \"on-board\" multimodal data collection method developed for MATCH, a multimodal mobile city guide. Our approach exploits MATCHs component architecture in that each component can be redeployed in evaluation and annotation tools, allowing user test sessions to be replayed with a high degree of fidelity without the use of recorded video. Instead, the components themselves perform a dynamic re-enactment of test sessions directed by the script of a comprehensive log file. This method enabled continual user testing and piloting to inform the iterative development process for MATCH.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-616"
  },
  "meng02b_icslp": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Yee Fong",
     "Wong"
    ],
    [
     "Cheong Chat",
     "Chan"
    ]
   ],
   "title": "ISIS: a multi-modal, trilingual, distributed spoken dialog system developed with CORBA, java, XML and KQML",
   "original": "i02_2561",
   "page_count": 4,
   "order": 617,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "ISIS (Intelligent Speech for Information Systems) is a trilingual spoken dialog system in the stocks domain. It supports the three languages commonly used in Hong Kong (Cantonese, Putonghua and English), and serves as a test-bed for our research in various speech and language technologies. ISIS also features combined interaction and delegation dialogs, and automatic assimilation of newly listed stock names into the system s knowledge base. This paper focuses on the architecture and multi-modality of ISIS. We use the CORBA middleware to implement a distributed system that is interoperable across platforms. We also describe the incorporation of KQML (Knowledge Query and Manipulation Language) software agents in ISIS to handle delegation dialogs. The latest enhancement supports multi-modal and mixed-modal input which suit the natural affordances of certain interactions in order to improve usability. Input modalities include speaking, typing or mouse-clicking. Output media include synthesized speech, text, tables and graphics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-617"
  },
  "tsukada02_icslp": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "An acoustic comparison between american English and australian English vowels",
   "original": "i02_2257",
   "page_count": 4,
   "order": 618,
   "p1": "2257",
   "pn": "2260",
   "abstract": [
    "Five vowels /i, E, æ, A(or 6), u/ in isolated /CVC/ words produced by 7 American English (AmE) and 6 Australian English (AusE) talkers were examined with a view to documenting acoustic-phonetic similarities and differences between the two accent types. The effect of Accent was significant on all vowels for at least one of the first two formants. The AusE vowel space was much more compressed relative to the AmE vowel space. In both varieties, however, withincategory clustering was quite good, suggesting that these talkers productions were fairly consistent within their respective vowel systems. Although the two groups differed considerably in vowel quality, they were quite similar in vowel duration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-618"
  },
  "jesus02_icslp": {
   "authors": [
    [
     "Luis M.T.",
     "Jesus"
    ],
    [
     "Christine H.",
     "Shadle"
    ]
   ],
   "title": "A case study of portuguese and English bilinguality",
   "original": "i02_2261",
   "page_count": 4,
   "order": 619,
   "p1": "2261",
   "pn": "2264",
   "abstract": [
    "This study of the acoustic characteristics of European Portuguese and British English fricatives as produced by two bilingual subjects, consisted of time and frequency analysis of words in a carrier sentence. Time - averaged power spectra were calculated and parameterised in order to aid comparisons across speaker, across corpus, and across language, and to gain insight into the production mechanisms underlying the language - specific variations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-619"
  },
  "dioubina02_icslp": {
   "authors": [
    [
     "Olga I.",
     "Dioubina"
    ],
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "An IPA vowel diagram approach to analysing L1 effects on vowel production and perception",
   "original": "i02_2265",
   "page_count": 4,
   "order": 620,
   "p1": "2265",
   "pn": "2268",
   "abstract": [
    "This paper examines the influence of the first language on the accuracy with which trained phoneticians judge isolated monophthongs taken from the phoneme systems of two different languages. The main finding of this study was that the speakers as well as listeners L1 background have a significant influence on judgements which were made by means of the IPA vowel diagram.\n",
    "This paper consists of 6 sections. In section (1) we will recall the basic principles of the IPA vowel diagram focusing on its layout, proportions, and primary cardinal vowels. In section (2) we will refer to the tradition of using the cardinal vowel system in phonetic studies, particularly in order to plot a vowel sound at a certain point within the space of the IPA vowel diagram. In section (3) we will suggest how the influence of the L1 on articulation and perception of vowels of an unknown language can be investigated with the help of the IPA vowel diagram. The methods which we used during the preparation of recordings and perception experiment, as well as subjects who took part in them, will be described in section (4) and a presentation of the main results will follow in section (5). Finally, we will discuss the results with respect to the three basic dimensions of the vowel sounds: height, backness and rounding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-620"
  },
  "helgason02_icslp": {
   "authors": [
    [
     "Pétur",
     "Helgason"
    ],
    [
     "Sjúrðhur",
     "Gullbein"
    ]
   ],
   "title": "Phonological norms in faroese speech synthesis",
   "original": "i02_2269",
   "page_count": 4,
   "order": 621,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "In developing Faroese text-to-speech synthesis, the choice of phonological norms has proved problematic. In particular, the literature on Faroese phonology offered contradictory accounts of the realization of word-medial postvocalic stops. Spontaneous speech data from 4 speakers of the Tórshavn dialect were recorded, and their stop production was analyzed. It emerged that there were detailed differences between the 4 speakers in the stop production patterns. Since there seems to be no single \"correct\" phonological representation of these word-medial stops in the Tórshavn dialect of Faroese, our approach has been eclectic, in that we have chosen those phonological norms for the synthesis which we have judged optimal for the user.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-621"
  },
  "mareuil02_icslp": {
   "authors": [
    [
     "Philippe Boula de",
     "Mareüil"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Studying pronunciation variants in French by using alignment techniques",
   "original": "i02_2273",
   "page_count": 4,
   "order": 622,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "In this contribution, we address the realization of well-known pronunciation variants in French: the so-called mute e (or schwa), liaisons (sandhi phenomena) and mid vowels in non- word-final syllables, whose behavior with regards to vowel harmony is investigated. We compare the occurrences of such pronunciation variants in large corpora of read and spontaneous speech. Their phonetic transcriptions are automatically obtained by aligning the acoustic data with a pronunciation graph, derived from orthographic transcriptions and pronunciation dictionaries which include relevant pronunciation variants. The frequency of occurrence of schwas and liaisons, together with the pronunciation of mid vowels according to their underlying representation are then computed and analysed linguistically.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-622"
  },
  "hansson02_icslp": {
   "authors": [
    [
     "Petra",
     "Hansson"
    ]
   ],
   "title": "Perceived boundary strength",
   "original": "i02_2277",
   "page_count": 4,
   "order": 623,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "In this paper, results from a perception experiment on perceived prosodic boundary strength in spontaneous speech are presented. The analysis of the listeners responses reveals a good agreement in rating the strength of prosodic phrase boundaries on a visual analogue scale (VAS), and a strong correlation between perceived boundary strength and pause duration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-623"
  },
  "jun02_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ]
   ],
   "title": "Syntax over focus",
   "original": "i02_2281",
   "page_count": 4,
   "order": 624,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "This paper compares three factors affecting prosodic phrasing in Korean: syntax, focus and speech rate. The Syntax and Focus constraints have been claimed to be stronger than the Speech Rate constraint, but it is not known which of these - the syntactic constraint (SYNTAX) or the focus constraint (FOCUS) - is stronger. Based on production and perception data, it is found that SYNTAX is stronger than FOCUS, both at normal and fast speech rates, though less so at fast rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-624"
  },
  "ohala02_icslp": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ],
    [
     "Rungpat",
     "Roengpitya"
    ]
   ],
   "title": "Duration related phase realignment of Thai tones",
   "original": "i02_2285",
   "page_count": 4,
   "order": 625,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "When Thai contour tones sit on long and short vowels followed by a nasal consonant, different portions of the tone are superimposed on the vowel and the nasal: more of the contour on a long vowel and less on the shorter following sonorant and vice-versa with a short vowel. This change, which we call phase realignment, presents some complexities because overall the tone is shorter on the rhyme with the short vowel and in other situations the strategy of fitting tones to segmental sequences of varying duration involve truncating one end of the contour. We report here our attempt to quantify this variation and to validate that the modification is just a realignment of different parts of the tonal contour with the segmental sequences it sits on.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-625"
  },
  "bosch02_icslp": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ]
   ],
   "title": "Probabilistic ranking of constraints",
   "original": "i02_2289",
   "page_count": 4,
   "order": 626,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "This paper discusses aspects of ordering, more specifically probabilistic ordering. An algorithm is presented for adaptive learning an optimal ordering based on data, such that the resulting ordering follows the statistical properties in the input data. The properties to be ordered can be phonological rewrite rules, pronunciation rules inferred from observed phonetic data, grammar rules, etc. The novelty in the proposed model is the combination of well-established methods from three different disciplines: parsing, voting theory, and statistics.\n",
    "The set-up of this paper is mainly theoretical, and we follow a quite formal approach. There is a close link with techniques that are currently being developed in the Optimality Theory, one of the mainstream approaches in phonology. The resulting techniques, however, can be applied in a more general domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-626"
  },
  "komatsu02_icslp": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Shinichi",
     "Tokuma"
    ],
    [
     "Won",
     "Tokuma"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Multi-dimensional analysis of sonority: perception, acoustics, and phonology",
   "original": "i02_2293",
   "page_count": 4,
   "order": 627,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "Sonority is an important notion in phonology, but its definition has been controversial. Our analysis showed that sonority can be located in a multi-dimensional perceptual space, and that the dimensions of the space have correspondence to both acoustic parameters and phonological features. In the experiment, a confusion matrix was calculated from the results of consonant perception test for LPC residual signals made from /Ca/ syllables. It is considered that LPC residual signals contain only suprasegmental information and thus the confusion pattern for the signals indicates the consonants similarities in suprasegmental domain. This confusion matrix was analyzed with MDS. The result showed that the consonants can be modeled in a 3-dimensional perceptual space according to their sonority. Its dimensions could be related to acoustic measurements (length, pitch, RMS, HNR) and phonological features ([voice], [sonorant], [continuant]). The result also showed that sonority can be mostly defined within the suprasegmental domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-627"
  },
  "faundezzanuy02_icslp": {
   "authors": [
    [
     "Marcos",
     "Faúndez-Zanuy"
    ],
    [
     "Mattias",
     "Nilsson"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "On the relevance of bandwidth extension for speaker verification",
   "original": "i02_2317",
   "page_count": 4,
   "order": 628,
   "p1": "2317",
   "pn": "2320",
   "abstract": [
    "In this paper, we consider the effect of a bandwidth extension of narrow-band speech signals (0.3-3.4 kHz) to 0.3-8 kHz on speaker verification. Using covariance matrix based verification systems together with detection error trade-off curves, we compare the performance between systems operating on narrowband, wide-band (0-8 kHz), and bandwidth-extended speech. The experiments were conducted using different short-time spectral parameterizations derived from microphone and ISDN speech databases. The studied bandwidthextension algorithm did not introduce artifacts that affected the speaker verification task, and we achieved improvements between 1 and 10 percent (depending on the model order) over the verification system designed for narrow-band speech when mel-frequency cepstral coefficients for the short-time spectral parameterization were used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-628"
  },
  "sabac02_icslp": {
   "authors": [
    [
     "Bogdan",
     "Sabac"
    ]
   ],
   "title": "Speaker recognition using discriminative features selection",
   "original": "i02_2321",
   "page_count": 4,
   "order": 629,
   "p1": "2321",
   "pn": "2324",
   "abstract": [
    "A new method of text-independent speaker recognition using discriminative feature selection is proposed in this paper. The method is compared with the classical UBM-GMM and hybrid codebook approaches. The characteristics of the proposed method are as follows: feature parameters extraction, vector quantization with the growing neural gas (GNG) algorithm and discriminative feature selection (DFS) according to the uniqueness of personal features. The speaker recognition algorithms are evaluated on three databases: A2000, B2000 and King. The PLP cepstral coefficients have been used together with four channel normalization techniques. The test results showed that channel normalization techniques greatly improve the speaker recognition performance and they showed that use of discriminative features selection procedure yields significant performance compared with classical UBM-GMM and codebook hybrid approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-629"
  },
  "kinnunen02_icslp": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "Designing a speaker-discriminative adaptive filter bank for speaker recognition",
   "original": "i02_2325",
   "page_count": 4,
   "order": 630,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "A new filter bank approach for speaker recognition front-end is proposed. The conventional mel-scaled filter bank is replaced with a speaker-discriminative filter bank. Filter bank is selected from a library in adaptive basis, based on the broad phoneme class of the input frame. Each phoneme class is associated with its own filter bank. Each filter bank is designed in a way that emphasizes discriminative subbands that are characteristic for that phoneme. Experiments on TIMIT corpus show that the proposed method outperforms traditional MFCC features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-630"
  },
  "tsang02_icslp": {
   "authors": [
    [
     "Chi-Leung",
     "Tsang"
    ],
    [
     "CMan-Wai",
     "Mak"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "Divergence-based out-of-class rejection for telephone handset identification",
   "original": "i02_2329",
   "page_count": 4,
   "order": 631,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "Research has shown that handset selectors can be used to assist telephone-based speech/speaker recognition. Most handset selectors, however, simply select the most likely handset from a set of known handsets even for speech coming from an unseen handset. This paper proposes a divergence-based handset selector with outof- handset (OOH) rejection capability to identify the unseen handsets. This is achieved by measuring the Jensen difference between the selectors output and a constant vector with identical elements. The resulting handset selector is combined with a feature-based channel compensation algorithm for telephone-based speaker verification. Utterances whose handsets were identified as unseen are either transformed by a global bias vector or normalized by cepstral mean subtraction (CMS). On the other hand, if the handset can be identified (considered as seen), its corresponding transformation parameters will be used to transform the utterances. Experiments based on ten handsets of the HTIMIT corpus show that using the transformation parameters of the seen handsets to transform the utterances with correctly identified handsets and processing those utterances with unseen handsets by CMS achieve the best result.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-631"
  },
  "ho02b_icslp": {
   "authors": [
    [
     "Purdy",
     "Ho"
    ]
   ],
   "title": "A handset identifier using support vector machines",
   "original": "i02_2333",
   "page_count": 4,
   "order": 632,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "In this paper, we present a new approach to handset identification using Support Vector Machines (SVMs) [1]. The inconsistency of audio characteristics among different handsets significantly degrades the performance of speaker recognition [2]. If a speaker recognizer can identify the handset a speaker is using, it can perform the recognition by selecting a model trained specifically on that handset. We present an SVM-based handset identifier that uses the Gaussian kernel and the one-vs-rest approach [1] to separate utterances on one kind of handset from those on the others. We analyze the performance of speaker-dependent and speaker-independent identifiers in classifying 4 different types of handset: carbon-button, electret, cordless, and headset. The test results show that SVMs yield greater than 90% accuracy, and that both speaker-dependent and speaker-independent approaches give comparable results on all test sets. Experiments also show the advantages of SVMs over the previous channel compensation (RASTA [3]) and handset identification (GMM/ML [2, 4]) algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-632"
  },
  "faltlhauser02_icslp": {
   "authors": [
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Günther",
     "Ruske"
    ],
    [
     "M.",
     "Thomae"
    ]
   ],
   "title": "Towards the question: why has speaking rate such an impact on speech recognition performance?",
   "original": "i02_2429",
   "page_count": 4,
   "order": 633,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "It has repeatedly been shown, mostly in terms of WER, that the rate of speech significantly affects speech recognition accuracy. However, the question how is not yet satisfactorily answered. In this paper we scrutinized in which way already modeling accuracy is influenced by the rate of speech. We observed the existence of a rather direct (negative) correlation between the local speech rate (LSR) and the local average HMM score (LAS). This correlation can already be found for utterances in the training database, i.e. utterances that actually were used for the parameter estimation of the acoustic phonetic models. By introducing confidence measures based on likelihood distance we verified that statistical modeling with respect to speech rate seems most accurate in slow speech segments and deteriorates already at average speaking rates. We further found that the correlation is little, yet observable, for the static features and increases with the frame range of delta(delta) features - reaching up to 0.65. The correlation persists regardless of simple monophone models or context dependent triphones. The LSR-LAS dependency can be used to predict LSR on independent test data directly from the acoustic HMM scores. In addition, LAS can be used as an indicator to assess the performance gain of rate dependent HMM models, which seems small (for fast speech) in comparison to the overall score degradation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-633"
  },
  "arcienega02_icslp": {
   "authors": [
    [
     "Mijail",
     "Arcienega"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Robust voiced-unvoiced decision associated to continuous pitch tracking in noisy telephone speech",
   "original": "i02_2433",
   "page_count": 4,
   "order": 634,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "In this paper, a new algorithm for a robust voiced-unvoiced classifi- cation of frames in telephone speech is presented. This algorithm is associated to a continuous pitch contour extraction procedure based on a discrete logarithmic Fourier transform and a dynamic programing search as introduced in [1]. While the pitch extraction procedure is forced to find a pitch value for every frame, even in unvoiced regions, the algorithm presented in this paper is capable of making a robust voiced-unvoiced decision by using multiple features and twostate ergodic hidden Markov modeling (HMM).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-634"
  },
  "yao02b_icslp": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Noise adaptive speech recognition with acoustic models trained from noisy speech evaluated on Aurora-2 database",
   "original": "i02_2437",
   "page_count": 4,
   "order": 635,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "In this paper, we apply the noise adaptive speech recognition for noisy speech recognition in non-stationary noise to the situation that acoustic models are trained from noisy speech. We justify it by that the noise adaptive speech recognition includes iterative processes between a noise parameter estimation step and a model adaptation step, which can possibly do non-linear mapping between the original training space and that for recognition. Experiments were performed on Aurora-2 task with multi-conditional training set which includes noisy utterances. Through experiments, we observed that the noise adaptive speech recognition can have better performance than the baseline system trained from multi-conditional training set without noise adaptive speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-635"
  },
  "chen02e_icslp": {
   "authors": [
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Yiteng (Arden)",
     "Huang"
    ],
    [
     "Qi",
     "Li"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Recognition of noisy speech using normalized moments",
   "original": "i02_2441",
   "page_count": 4,
   "order": 636,
   "p1": "2441",
   "pn": "2444",
   "abstract": [
    "Spectral subband centroid, which is essentially the first-order normalized moment, has been proposed for speech recognition and its robustness to additive noise has been demonstrated before. In this paper, we extend this concept to the use of normalized spectral subband moments (NSSM) for robust speech recognition. We show that normalized moments, if properly selected, yield comparable recognition performance as the cepstral coefficients in clean speech, while deliver a better performance than the cepstra in noisy environments. We also propose a procedure to construct the dynamic moments that essentially embodies the transitional spectral information. We discuss some properties of the proposed dynamic features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-636"
  },
  "chen02f_icslp": {
   "authors": [
    [
     "Chia-Ping",
     "Chen"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Low-resource noise-robust feature post-processing on Aurora 2.0",
   "original": "i02_2445",
   "page_count": 4,
   "order": 637,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "We present a highly effective and extremely simple noise-robust front end based on novel post-processing of standard MFCC features. It performs remarkably well on the Aurora 2.0 noisy-digits database without requiring any increase in model complexity. Compared to the Aurora 2.0 baseline system, our technique improves the average word error rate by 45% in the multi-condition training case, (matched training/testing conditions) and 60% in the clean training case (mismatched training/testing conditions) - this is an improvement that rivals some of the best known results on this database. Our method, moreover, improves the performances in all cases, regardless of clean or noisy speech, matched or mismatched environments. Our technique is entirely general because it makes no assumptions about the existence, type, or level of noise in the speech signal. Moreover, its simplicity means that it should be easy to integrate with other techniques in order to yield further improvements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-637"
  },
  "deng02b_icslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Exploiting variances in robust feature extraction based on a parametric model of speech distortion",
   "original": "i02_2449",
   "page_count": 4,
   "order": 638,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "This paper presents a technique that exploits the denoised speechs variance, estimated during the speech feature enhancement process, to improve noise-robust speech recognition. This technique provides an alternative to the Bayesian predictive classification decision rule by carrying out an integration over the feature space instead of over the model-parameter space, offering a much simpler system implementation and lower computational cost. We extend our earlier work [5] by using a new approach, based on a parametric model of speech distortion and thus free from the use of any stereo training data, to statistical feature enhancement, for which a novel algorithm for estimating the variance of the enhanced speech features is developed. Experimental evaluation using the full Aurora2 test data sets demonstrates an 11.4% digit error rate reduction averaged over all noisy and SNR conditions, compared with the best technique we have developed [2] prior to this work that did not exploit the variance information and that required no stereo training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-638"
  },
  "ghulam02_icslp": {
   "authors": [
    [
     "Muhammad",
     "Ghulam"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Takaharu",
     "Sato"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Improving performance of an HMM-based ASR system by using monophone-level normalized confidence measure",
   "original": "i02_2453",
   "page_count": 4,
   "order": 639,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "In this paper, we propose a novel confidence scoring method that is applied to N-best hypotheses output from an HMM-based classifier. In the first pass of the proposed method, the HMM-based classifier with monophone models outputs N-best hypotheses (word candidates) and boundaries of all the monophones in the hypotheses. In the second pass, an SM (Sub-space Method)-based verifier tests the hypotheses by comparing confidence scores. We discuss how to convert a monophone similarity score of SM into a likelihood score, how to normalize the variations of acoustic quality in an utterance, how to combine an HMM-based likelihood of word level and an SM-based likelihood of monophone level, and also how to accept the correct words and reject OOV words. In the experiments performed on speaker- independent word recognition, the proposed confidence scoring method significantly reduced word error rate from 4.7% obtained by the standard HMM classifier to 2.0%, and it also reduced the equal error rate from 9.0% to 6.5% in an unknown word rejection task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-639"
  },
  "liu02e_icslp": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Model partial pronunciation variations for spontaneous Mandarin speech recognition",
   "original": "i02_2457",
   "page_count": 4,
   "order": 640,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "Modeling pronunciation variations is a critical part of spontaneous Mandarin speech recognition. Such variations include both complete changes and partial changes. Complete changes can usually be modeled by using an alternate phone to replace the canonical phone. Partial changes, which cannot be modeled by conventional methods are variations within the phoneme and include diacritics. In this paper, we propose using partial change phone model (PCPM) as well as auxiliary decision tree to model partial changes. A detailed but robust model can be achieved by merging canonical model with PCPMs through Gaussian distribution reconstruction. The effectiveness of this approach was evaluated on the Hub4NE Mandarin Broadcast News Corpus. The syllable error rate decreased 2.39% absolutely with respect to the baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-640"
  },
  "zheng02c_icslp": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Reducing pronunciation lexicon confusion and using more data without phonetic transcription for pronunciation modeling",
   "original": "i02_2461",
   "page_count": 4,
   "order": 641,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "The multiple-pronunciation lexicon (MPL) is very important to model the pronunciation variations for spontaneous speech recognition. But the introduction of MPL brings out two problems. First, the MPL will increase the among-lexicon confusion and degrade the recognizers performance. Second, the MPL needs more data with phonetic transcription so as to cover as many surface forms as possible. Accordingly, two solutions are proposed, they are the context-dependent weighting method and the iterative forced-alignment based transcription method. The use of them can compensate what the MPL causes and improve the overall performance. Experiments across a naturally spontaneous speech database show that the proposed methods are effective and better than other methods. A Parzen Window Based Derivation of Minimum\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-641"
  },
  "mcdermott02_icslp": {
   "authors": [
    [
     "Erik",
     "McDermott"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "Classification error from the theoretical Bayes classification risk",
   "original": "i02_2465",
   "page_count": 4,
   "order": 642,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "This article shows that the Minimum Classification Error (MCE) criterion function commonly used for discriminative design of speech recognition systems is equivalent to a Parzen window based estimate of the theoretical Bayes classification risk. In this analysis, each training token is mapped to the center of a Parzen kernel in the domain of a suitably defined random variable. The kernels are summed to produce a density estimate; this estimate in turn can easily be integrated over the domain of incorrect classifications, yielding the risk estimate. The expression of risk for each kernel can be seen to correspond directly to the usual MCE loss function. The resulting risk estimate can be minimized by suitable adaptation of the recognition system parameters that determine the mapping from training token to kernel center. This analysis provides a novel link between the MCE empirical cost measured on a finite training set and the theoretical Bayes classification risk.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-642"
  },
  "klautau02_icslp": {
   "authors": [
    [
     "Aldebaro",
     "Klautau"
    ],
    [
     "Nikola",
     "Jevtic"
    ],
    [
     "Alon",
     "Orlitsky"
    ]
   ],
   "title": "Combined binary classifiers with applications to speech recognition",
   "original": "i02_2469",
   "page_count": 4,
   "order": 643,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "Many applications require classification of examples into one of several classes. A common way of designing such classifiers is to determine the class based on the outputs of several binary classifiers. We consider some of the most popular methods for combining the decisions of the binary classifiers, and improve existing bounds on the error rates of the combined classifier over the training set. We also describe a new method for combining binary classifiers. The method is based on stacking a neural network and, when used with support vector machines as the binary learners, substantially decreased the error rate in two vowel classification tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-643"
  },
  "nagorski02_icslp": {
   "authors": [
    [
     "Arkadiusz",
     "Nagórski"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Herman",
     "Steeneken"
    ]
   ],
   "title": "Optimal selection of speech data for automatic speech recognition systems",
   "original": "i02_2473",
   "page_count": 4,
   "order": 644,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "This paper presents a method designed to select a limited set of maximally information rich speech data from a database for optimal training and diagnostic testing of Automatic Speech Recognition (ASR) systems. The method uses Principal Component Analysis (PCA) to map the variance of the speech material in a database into a low-dimensional space, followed by clustering and a selection technique. It appears that a very straightforward implementation of this procedure automatically detects at least two criteria for a classifi- cation of speakers of standard Dutch, viz. gender and the way in which the /r/ is produced. To verify the power of the technique to improve ASR, data sets of equal size selected with this method and obtained randomly were used to train a recognition system on Dutch connected digits. The results show an improvement in the recognition performance when optimal data sets were used, especially for the conditions where the sub-corpora used for training were relatively small.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-644"
  },
  "liotti02_icslp": {
   "authors": [
    [
     "Mario",
     "Liotti"
    ],
    [
     "Lorraine O.",
     "Ramig"
    ],
    [
     "Deanie",
     "Vogel"
    ],
    [
     "Pamela",
     "New"
    ],
    [
     "Chris",
     "Cook"
    ],
    [
     "Peter",
     "Fox"
    ]
   ],
   "title": "Hypophonia in parkinson disease: neural correlates of voice treatment with LSVT revealed by PET",
   "original": "i02_2477",
   "page_count": 4,
   "order": 645,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "This study investigated the neural correlates of hypophonia in individuals with Idiopathic Parkinsons disease (IPD) before and after the Lee Silverman Voice Treatment (LSVT), using 15O-H2O Positron Emission Tomography (PET). Regional cerebral blood flow (rCBF) changes associated with overt speech-motor tasks relative to the resting state were measured in the IPD subjects before and after therapy, and in a group of healthy controls. Before LSVT, patients had strong speech- related activations in motor and premotor cortex, supplementary motor cortex and inferior lateral premotor cortex which were significantly reduced post-LSVT. In addition, significant right-sided activations were present in anterior insular cortex, caudate head, putamen, and dorsolateral prefrontal cortex following LSVT. Finally, the LSVT-induced neural changes were not present with transient experimenter-cued increases of loudness in LSVT-untreated patients. This treatment-dependant functional reorganization suggests a shift from an abnormally effortful to a more automatic implementation of speech-motor actions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-645"
  },
  "duncan02_icslp": {
   "authors": [
    [
     "Susan",
     "Duncan"
    ]
   ],
   "title": "Preliminary data on effects of behavioral and levodopa therapies on speech-accompanying gesture in Parkinson's disease",
   "original": "i02_2481",
   "page_count": 4,
   "order": 646,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "Speakers spontaneously produce meaningful manual, bodily, and facial movements when engaged in interactive communication . This is universally true. Gestures serve as aids to communication and also as embodied representations of meanings, with a role in structuring connected discourse, linked with working memory. Given their communicative and cognitive significance in normal language use, it is important to study the alterations these behaviors undergo in language pathological conditions such as aphasia, right hemisphere communication disorders, and Parkinsons disease (PD); also, to assess the effects of therapies for these disorders on gesture. Here we sketch the connections between (i) the role of gesture in language, (ii) research on the disabling effects of Parkinsons disease on spoken communication, emphasizing voice, dysprosody, and flattened affect, (iii) the link between speech prosody and speech-accompanying gesture, and (iv) the spread of effects of Lee Silverman Voice Treatment (LSVT) therapy beyond amelioration of voice deficits, possibly extending to speech-accompanying gestures. Preliminary analyses of narrative discourse data from four PD patients are summarized. Two of these patients were videotaped for comparison on and off Levodopa medications; two were videotaped pre- and post-LSVT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-646"
  },
  "quek02d_icslp": {
   "authors": [
    [
     "Francis",
     "Quek"
    ],
    [
     "Mary",
     "Harper"
    ],
    [
     "Yonca",
     "Haciahmetoglu"
    ],
    [
     "Lei",
     "Chen"
    ],
    [
     "Lorraine O.",
     "Ramig"
    ]
   ],
   "title": "Speech pauses and gestural holds in Parkinson's disease",
   "original": "i02_2485",
   "page_count": 4,
   "order": 647,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "Parkinsons disease (PD) belongs to a class of neurodegenerative diseases that affect the patients speech, motor, and cognitive capabilities. All three deficits affect the multimodal communication channels of speech and gesture. We present a study on the changes in speech pause patterns and gesture holds before and after treatment. We present the results of a pilot study of two Idiopathic PD patients who have undergone Lee Silverman Voice Treatment (LSVT). We show that there was a consistent change in the location of pauses with respect to semantic sentential utterance units. After treatment, the number and duration of pauses within sentential units decreased while the inter sentential pauses increased. This indicates reduction in hesitation and increase in speech phrasing. We also found a decrease in the number of sentence repairs and the time spent in repairs. For gesture, we found that non-rest holds intersecting with within-sentence pauses appears to decline after treatment, as does the ratio of rest holds during speech against rest holds between sentences. While the work is preliminary, these patterns suggest that multimodal discourse characteristics might provide access to the underlying cognitive state under the load of narrative discourse.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-647"
  },
  "spielman02_icslp": {
   "authors": [
    [
     "Jennifer L.",
     "Spielman"
    ],
    [
     "Lorraine O.",
     "Ramig"
    ],
    [
     "Joan C.",
     "Borod"
    ]
   ],
   "title": "Oro-facial changes in parkinson²s disease following intensive voice therapy (LSVT)",
   "original": "i02_2489",
   "page_count": 4,
   "order": 648,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "Parkinsons disease (PD) is associated with multiple communication deficits which affect both verbal and nonverbal abilities, including vocal loudness, articulatory precision, and facial expression. This paper addresses the effects of intensive voice therapy (Lee Silverman Voice Treatment, LSVT) on communicative acts in PD involving significant oro-facial movement, specifically speech articulation and spontaneous facial expression. Both acoustic measurements and perceptual judgments are presented. The underlying mechanisms thought to be responsible for treatment-related changes are proposed and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-648"
  },
  "logemann02_icslp": {
   "authors": [
    [
     "Jeri",
     "Logemann"
    ],
    [
     "Ralph",
     "Sundin"
    ],
    [
     "Jean",
     "Sundin"
    ]
   ],
   "title": "Swallowing and voice effects of lee silverman voice treatment (LSVT)",
   "original": "i02_2493",
   "page_count": 4,
   "order": 649,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "In addition to voice and articulatory disorders, swallowing disorders have been reported in as many as 95% of individuals with Parkinsons Disease (PD). This paper addresses the effects of intensive voice treatment (Lee Silverman Voice Treatment, LSVT) on swallowing and voice changes. After LSVT there was an overall 51% reduction in the number of swallowing motility disorders. Voice changes included a significant increase in vocal intensity during sustained phonation and reading. The LSVT seemingly improved neuromuscular control of the entire upper aerodigestive tract, improving oral tongue and tongue base function during the oral and pharyngeal phases of swallowing. The underlying mechanisms associated with these changes will be discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-649"
  },
  "will02_icslp": {
   "authors": [
    [
     "Leslie",
     "Will"
    ],
    [
     "Lorraine O.",
     "Ramig"
    ],
    [
     "Jennifer L.",
     "Spielman"
    ]
   ],
   "title": "Application of the lee silverman voice treatment (LSVT) to individuals with multiple sclerosis, ataxic dysarthria, and stroke",
   "original": "i02_2497",
   "page_count": 4,
   "order": 650,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "Reduced speech intelligibility has been observed in Parkinsons disease (PD), ataxic dysarthria, multiple sclerosis (MS) and in individuals who have suffered a cerebral vascular accident (CVA). Data support the effectiveness of the Lee Silverman Voice Treatment (LSVT) on the improvement of vocal function and speech intelligibility in PD. Today, only a small percentage of patients with chronic neurologic disease receive treatment to improve speech and voice. This paper will report improvement in case studies of individuals with MS, ataxic dysarthria and CVA and suggest that the effects of LSVT may not be restricted to the dysarthria associated with PD.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-650"
  },
  "farley02_icslp": {
   "authors": [
    [
     "Becky G.",
     "Farley"
    ]
   ],
   "title": "Think big, from voice to limb movement therapy",
   "original": "i02_2501",
   "page_count": 4,
   "order": 651,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "We will test the efficacy of an innovative physical therapy treatment technique based upon the Lee Silverman Voice Treatment (LSVT). Thus, people with PD will undergo intensive practice of high effort/- large amplitude arm movements while focusing on the sensory awareness of \"movement bigness.\" As LSVT uses extensive practice and feedback/knowledge of results to teach patients the amount of effort needed to consistently perform \"louder\" voice, we will use similar motor learning techniques to teach patients the amount of effort needed to consistently perform \"bigger\" movements. Speech studies have shown that a treatment with a simple focus (think loud) may generalize to affect motor output in other systems (e.g., articulation, speaking rate, swallowing, respiratory mechanics). Thus, we predict that learning to perform bigger arm movements will not only improve arm function during everyday activities, but the effects will generalize to lower limb function and speech motor control.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-651"
  },
  "parsa02b_icslp": {
   "authors": [
    [
     "Vijay",
     "Parsa"
    ],
    [
     "Donald G.",
     "Jamieson"
    ],
    [
     "Karen",
     "Stenning"
    ],
    [
     "Herbert A.",
     "Leeper"
    ]
   ],
   "title": "On the estimation of signal-to-noise ratio in continuous speech for abnormal voices",
   "original": "i02_2505",
   "page_count": 4,
   "order": 652,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "Acoustic measures of vocal function are attractive due to their nonintrusive nature and due to the ease with which they can be obtained. In this paper, we developed an acoustic measure based on linear prediction modeling and filterbank analysis of continuous speech samples. The input speech sample was first modeled using a combination of short-term and long-term all pole linear prediction filters. The input speech sample and the residual signal were then divided into subbands using cosine-modulated or gammatone filterbanks. The Signal-to-Residue Ratio (SRR) was calculated as the weighted combination of the ratios of the input and residual signal energies in the subbands. The performance of the SRR parameter was evaluated with speech samples collected from patients suffering from vocal fold cancer before and after radiation therapy. Results showed that the SRR measure correlates better with the perceptual judgments of voice quality than the global SNR parameter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-652"
  },
  "semenov02_icslp": {
   "authors": [
    [
     "V.",
     "Semenov"
    ],
    [
     "A.",
     "Kovtonyuk"
    ],
    [
     "A.",
     "Kalyuzhny"
    ]
   ],
   "title": "Computationally efficient method of speech enhancement based on block representation of signal in state space and vector quantization",
   "original": "i02_2509",
   "page_count": 4,
   "order": 653,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "A computationally efficient speech enhancement method is proposed. Reduction of computations is achieved due to derived properties of block model of autoregressive (AR) signal. Decreasing of filtering error in comparison with traditional Kalman filter is shown. The problem of estimation of speech AR parameters is also considered. A two-phase computationally efficient estimation procedure, based on vector quantization of AR parameters, is proposed. On the first stage the initial approximation for optimal AR parameters is determined on a small number of AR quantums. Then the value of estimate is improved by efficient iterative procedure. The performance of resulting method is evaluated on real speech signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-653"
  },
  "kondo02_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Kondo"
    ],
    [
     "Kiyoshi",
     "Nakagawa"
    ]
   ],
   "title": "Active speech cancellation for cellular speech",
   "original": "i02_2513",
   "page_count": 4,
   "order": 654,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "We investigated on the possibility of an active cancellation system of unnecessary speech radiation. The intended application of this system is to cancel cellular speech, and also to cancel speech input for recognition-based dictation or speech-command control systems. Both of these applications do not require speech to be radiated into surrounding space, only into the input microphone, and would benefit if global radiation is controlled. Through simulations, it was found that speech cancellation is possible with a secondary source placed close to the mouth generating inversed-phase speech. The inversed-phase speech can be predicted using linear prediction. It was found that 1) over 14 dB cancellation of speech is possible, 2) the cancellation is greater when the distance between the mouth and the secondary sound source is smaller, and 3) higher order LPC yields greater cancellation up to an order of 128. We plan to implement a prototype system using DSPs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-654"
  },
  "muralishankar02_icslp": {
   "authors": [
    [
     "R.",
     "Muralishankar"
    ],
    [
     "A. G.",
     "Ramakrishnan"
    ],
    [
     "P.",
     "Prathibha"
    ]
   ],
   "title": "Warped-LP residual resampling using DCT for pitch modification",
   "original": "i02_2517",
   "page_count": 4,
   "order": 655,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "In this paper, we propose a novel algorithm for pitch modification. The linear prediction (LP) residual is obtained from pitch synchronous frames by inverse filtering the speech signal. Then the Discrete Cosine Transform (DCT) of these residual frames is taken. Based on the desired factor of pitch modification, the dimension of the DCT of the residual is modified by truncating or zero padding, and then the Inverse DCT is obtained. This period modified residual signal is then forward filtered to obtain the pitch modified speech. The mismatch in the positions of the harmonics between the pitch modified signal and the LP spectrum introduce gain variations, which is more pronounced in the case of female speech [2]. This is minimised by modifying the radii of the poles of the filter to smoothen the peaky linear predictive spectrum before forward filtering. Since Warped Linear Prediction (WLP) [7] exhibits frequency resolution close to human hearing, it has been exploited for our pitch modification algorithm. WLP coefficients are used instead of the conventional LP coefficients for pitch modification. Perceptual results show better performance of WLP over conventional LP. The technique has been successfully applied to create interrogative sentences from affirmative sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-655"
  },
  "jung02_icslp": {
   "authors": [
    [
     "E.",
     "Jung"
    ],
    [
     "A.",
     "Schwarzbacher"
    ],
    [
     "K.",
     "Humphreys"
    ],
    [
     "R.",
     "Lawlor"
    ]
   ],
   "title": "Application of real-time AMDF pitch-detection in a voice gender normalisation system",
   "original": "i02_2521",
   "page_count": 4,
   "order": 656,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "Traditionally the interest in voice gender conversion was of a more theoretical nature rather than founded in real- life applications. However, with the increase in mobile communication and the resulting limitation in transmission bandwidth new approaches to minimising data rates have to be developed. Here voice gender normalisation (VGN) presents an efficient method of achieving higher compression rates by using the VGN algorithm to remove gender specific components of a speech signal and thus enhancing the information content to be transmitted.\n",
    "A second application for VGN is in the field of speech controlled systems, where current speech recognition algorithms have to deal with the voice characteristics of a speaker as well as the information content. Here again the use of VGN can remove the speakers voice gender characteristics and thus enhance the message contents. Therefore, such a system would be capable of achieving higher recognition rates while being independent of the speaker. This paper presents the theory of a VGN system and furthermore, outlines an efficient real-time hardware implementation for the use in portable communications equipment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-656"
  },
  "laprie02b_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "A copy synthesis method to pilot the klatt synthesiser",
   "original": "i02_2525",
   "page_count": 4,
   "order": 657,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "This paper presents a copy synthesis method devoted to the control of the Klatt synthesizer. Our method allows speech stimuli to be constructed very easily from natural sounds. We chose the parallel branch of the Klatt synthesizer, which allows us to control formant amplitudes. The analysis strategy of a speech signal is straightforward: the fundamental frequency is calculated so that voiced regions are known and the frication energy is set to the value of the spectral energy above 4000 Hz. After formants have been tracked, the amplitudes of the resonators are measured on a spectrum obtained by an algorithm derived from cepstral smoothing called \"true envelope\". This algorithm has the advantage of approximating harmonics very accurately. The values of the most important acoustic parameters of the Klatt synthesizer are provided by these measurements. Stimuli which have been created by means of this method have a timbre close to that of natural speech. This copy synthesis method is incorporated in our software for speech research called \"Snorri\". Therefore, the user has at his disposal a versatile tool for creating stimuli in the context of the Klatt synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-657"
  },
  "sakamoto02_icslp": {
   "authors": [
    [
     "Masaharu",
     "Sakamoto"
    ],
    [
     "Takashi",
     "Saito"
    ]
   ],
   "title": "Speaker recognizability evaluation of a voicefont-based text-to-speech system",
   "original": "i02_2529",
   "page_count": 4,
   "order": 658,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "We have developed a new text-to-speech system based on the Voice- Font technology. A VoiceFont is a voice dictionary for speech synthesis that holds the acoustic and prosodic characteristics extracted from the voice corpus of a speaker. The text-to-speech system using a VoiceFont is able to synthetically mimic the voice of the donor speaker. In this paper, we evaluated speaker recognizability of the synthetic speech, which means whether the synthetic speech can be recognized as the donor speakers voice. We conducted a subjective evaluation for five VoiceFonts and here report on the evaluation results. The results show that our text-to-speech system based on VoiceFonts can retain the acoustic and prosodic characteristics of the donor speaker and the synthetic speech can be recognized as the donor speakers voice. Furthermore, we report on how much the spectral characteristics, phoneme duration, and pitch frequency affect speaker recognizability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-658"
  },
  "satuevillar02_icslp": {
   "authors": [
    [
     "Antonio",
     "Satué-Villar"
    ],
    [
     "Juan",
     "Fernández-Rubio"
    ]
   ],
   "title": "Time-frequency transforms and beamforming for speaker recognition",
   "original": "i02_2533",
   "page_count": 4,
   "order": 659,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "In this paper we study the advantages of a data transformation using time-frequency transforms (wavelets, in our case) in a wideband beamforming system. For this purpose, we simulate a target signal and interfering jammers that are impinging upon a microphone array in a noisy environment. The target and interfering signals are obtained from a speech database. The simulations presented show that the wavelet transform reduces the time of convergence, even though there is not an unique optimal wavelet filter; it depends on the speech signal. In this paper it can be also infered that the wavelet transform do not affect substantially to the quality of the output signal (evaluated with a speaker recognition method).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-659"
  },
  "kwon02_icslp": {
   "authors": [
    [
     "Soonil",
     "Kwon"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Speaker change detection using a new weighted distance measure",
   "original": "i02_2537",
   "page_count": 4,
   "order": 660,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "Speaker change detection is a key pre-requisite to speaker tracking and speaker adaptation. It detects the points where a speaker identity changes in a multi-speaker audio stream. We first extract the speech segments from an audio stream by segmentation and classi- fication techniques. Using the extracted speech segments, the proposed weighted metric-based technique detects the speaker change points. New weights are originated from Fisher Linear Discriminant Analysis and, when used with Mel Cepstrum feature vectors, it has an effect of subband processing. Experiments were performed with HUB-4 Broadcast News Evaluation English Test Material (1999) and a movie audio track. Results showed that our technique gave about 37.7% improvement compared with Euclidean distance on the broadcast news data and about 27.1% on the movie data; with Mahalanobis distance, the improvements were 37.7% and 25.3% for broadcast news and movie data, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-660"
  },
  "gomezcipriano02_icslp": {
   "authors": [
    [
     "José L.",
     "Gómez-Cipriano"
    ],
    [
     "Roger P.",
     "Nunes"
    ],
    [
     "Dante A. C.",
     "Barone"
    ]
   ],
   "title": "FPGA hardware for speech recognition using hidden Markov models",
   "original": "i02_2541",
   "page_count": 4,
   "order": 661,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "Some speech recognition applications, like speaker verification, dialog recognition or the speech to text transcription could require real time processing and a good precision. Other applications such as toys, automotive vehicles or portable machines still could aggregate the portability and low-power requirements, in addition to physical compactness. These requirements could require a hardware solution for the speech recognition problem. The current work proposes an architecture using hardware based in FPGAs, optimizing the pre-processing and parameter extraction for performing efficient speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-661"
  },
  "irino02_icslp": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "H.",
     "Tagawa"
    ]
   ],
   "title": "Evaluation of a speech recognition / generation method based on HMM and straight",
   "original": "i02_2545",
   "page_count": 4,
   "order": 662,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "We propose a method for integrating speech recognition and generation within a unified framework. The method consists of STRAIGHT, warped-frequency DCT, and an HMM engine. The warped-frequency DCT is used to derive a kind of mel-cepstral coefficient from the smoothed spectrum of STRAIGHT, which is known as a high-quality vocoder. This analysis/synthesis method has potential to improve the performance beyond a conventional method using the MFCC derived from the STFT. We evaluated the method by using speakerdependent speech recognition as well as by the perceptual evaluation of sounds generated by HMM text-to-speech. The recognition rate using the coefficients from the warped-DCT of the STRAIGHT spectrum was almost the same as that obtained using conventional MFCCs. The sound quality was sufficiently good for a fundamental system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-662"
  },
  "vepa02b_icslp": {
   "authors": [
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Objective distance measures for spectral discontinuities in concatenative speech synthesis",
   "original": "i02_2605",
   "page_count": 4,
   "order": 663,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "In unit selection based concatenative speech systems, join Cost, which measures how well two units can be joined together, is one of the main criteria for selecting appropriate units from the inventory. The ideal join cost will measure perceived discontinuity, based on easily measurable spectral properties of the units being joined, in order to ensure smooth and natural-sounding synthetic speech. In this paper we report a perceptual experiment conducted to measure the correlation between subjective human perception and various objective spectrally-based measures proposed in the literature. Our experiments used a state-of-the art unit-selection text-to-speech system: rVoice from Rhetorical Systems Ltd.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-663"
  },
  "hamza02_icslp": {
   "authors": [
    [
     "Wael",
     "Hamza"
    ],
    [
     "Robert",
     "Donovan"
    ]
   ],
   "title": "Data-driven segment preselection in the IBM trainable speech synthesis system",
   "original": "i02_2609",
   "page_count": 4,
   "order": 664,
   "p1": "2609",
   "pn": "2612",
   "abstract": [
    "Unit selection based concatenative speech synthesis has proven to be a successful method of producing high quality speech output. However, in order to produce high quality speech, large speech databases are required. For some applications, this is not practical due to the complexity of the database search process and the storage requirements of such databases. In this paper, we propose a data-driven algorithm to reduce the database size used in concatenative synthesis. The algorithm preselects database speech segments based on statis- tics collected by synthesizing a large number of sentences using the full speech database. The algorithm is applied to the IBM trainable speech synthesis system and the results show that database size can be reduced substantially while maintaining the output speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-664"
  },
  "peng02_icslp": {
   "authors": [
    [
     "Hu",
     "Peng"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Min",
     "Chu"
    ]
   ],
   "title": "Perpetually optimizing the cost function for unit selection in a TTS system with one single run of MOS evaluation",
   "original": "i02_2613",
   "page_count": 4,
   "order": 665,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "This paper proposes a method for optimizing the cost function for unit selection in the corpus-based TTS system by maximizing the correlation between the concatenative cost and the MOS. To do this, a subjective evaluation should be done first. The key point is to log the contextual information of all units appearing in the synthetic utterances evaluated. With this log file, concatenative cost can be recalculated with a cost function in any new definition. Then, the correlation between cost and MOS can serve as a measure for the validity of any change in the cost function, and the cost function is optimized perpetually without any new MOS evaluation. In this paper, the correlation coefficient between cost and MOS improves from -0.822 to -0.897 after optimization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-665"
  },
  "yi02_icslp": {
   "authors": [
    [
     "Jon",
     "Yi"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Information-theoretic criteria for unit selection synthesis",
   "original": "i02_2617",
   "page_count": 4,
   "order": 666,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "In our recent work on concatenative speech synthesis, we have devised an efficient, graph-based search to perform unit selection given symbolic information. By encapsulating concatenation and substitution costs defined at the class level, the graph expands only linearly with respect to corpus size. To date, these costs were manually tuned over pre-specified classes, which was a knowledge-intensive engineering process. In this research paper, we turn to informationtheoretic metrics for automatically learning the costs from data. These costs can be analyzed in a minimum description length (MDL) framework. The performance of these automatically determined weights is compared against that of manually tuned weights in a perceptual evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-666"
  },
  "kawai02_icslp": {
   "authors": [
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ]
   ],
   "title": "Acoustic measures vs. phonetic features as predictors of audible discontinuity in concatenative speech synthesis",
   "original": "i02_2621",
   "page_count": 4,
   "order": 667,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "Most concatenative speech synthesizers employ both acoustic measures and phonetic features to predict the perceptual damage caused by concatenating two waveform segments because no reliable acoustic measure has been found so far. This paper compares the predicting ability of the two kinds of predictor variables. We first conduct a perceptual experiment to measure the naturalness degradation due to signal discontinuity introduced by concatenating waveform segments. Secondly, we predict the score of naturalness degradation from acoustic measures derived from MFCC and/or phonetic features using statistical models such as a multiple regression model. Based on an investigation of the multiple regression coeffi- cients, we found that (1) the phonetic features are more effective and that (2) the acoustic measures do not provide useful information in addition to the phonetic features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-667"
  },
  "wang02f_icslp": {
   "authors": [
    [
     "Hsien-Chang",
     "Wang"
    ],
    [
     "Chieh-Yi",
     "Huang"
    ],
    [
     "Chung-Hsien",
     "Yang"
    ],
    [
     "Jhing-Fa",
     "Wang"
    ]
   ],
   "title": "A study of multi-speaker dialogue system for mobile information retrieval",
   "original": "i02_2677",
   "page_count": 4,
   "order": 668,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "The advances of speech processing technologies make it possible to build spoken dialogue systems (SDS) which may provide people with useful information. However, most current SDS can only deal with one speaker a time, improvements should be made before the SDS can be applied to domains where multiple speakers interact with the system. This paper discusses research issues for developing a multi-speaker dialogue system (MSDS) which is able to retrieve various mobile information in the car environment.\n",
    "The differences between traditional (single speaker) and multi-speaker dialogue system are first addressed. Then, two research topics are studied. 1) Speech source identification, which determines the active speaker. 2) Multi-speaker dialogue management, which interpreter the intention and maintain the dialogue history of the speakers to keep the interaction smooth.\n",
    "Many testers in a car environment attended the experiment for active speaker detection and multi-speaker dialogue system. The experiments showed an encouraging result that the proposed approach of MSDS did work properly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-668"
  },
  "fabbrizio02_icslp": {
   "authors": [
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Dawn",
     "Dutton"
    ],
    [
     "Narendra K.",
     "Gupta"
    ],
    [
     "Barbara",
     "Hollister"
    ],
    [
     "Mazin",
     "Rahim"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Robert",
     "Schapire"
    ],
    [
     "Juergen",
     "Schroeter"
    ]
   ],
   "title": "AT&t help desk",
   "original": "i02_2681",
   "page_count": 4,
   "order": 669,
   "p1": "2681",
   "pn": "2684",
   "abstract": [
    "This paper introduces a new breed of natural language dialog applications which we refer to as the Help Desk. These voice-enabled applications are an evolution from Help Desk services that are currently available on the web or being supported by human agents. The goals of a voice-enabled Help Desk are to route calls to appropriate agents or departments, provide a wealth of information about various products and services, and conduct problem solving or troubleshooting. In this paper we address the challenges in building this class of applications particularly when speech data is limited or unavailable. We will present the TTS Help Desk as an example of a service that has been deployed for automating the customer care component of the AT&T Labs Natural Voices Business\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-669"
  },
  "triassanz02_icslp": {
   "authors": [
    [
     "Roger",
     "Trias-Sanz"
    ],
    [
     "José B.",
     "Mariño"
    ]
   ],
   "title": "Basurde[lite], a machine-driven dialogue system for accessing railway timetable information",
   "original": "i02_2685",
   "page_count": 4,
   "order": 670,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "We present Basurde[lite], a complete human-machine dialogue system for accessing railway information, based on machine-driven dialogues, a small vocabulary and grammar, and word and phrase spotting based on unrestricted grammars. We give preliminary evaluation results.\n",
    "Although machine-driven, the system accepts however some flexi- bility: in some cases the user may give a reply not corresponding to the systems question, and may use out-of-vocabulary words and expressions. The system mimics the user in his choice of time and date formats and language used for place names.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-670"
  },
  "coulston02_icslp": {
   "authors": [
    [
     "Rachel",
     "Coulston"
    ],
    [
     "Sharon",
     "Oviatt"
    ],
    [
     "Courtney",
     "Darves"
    ]
   ],
   "title": "Amplitude convergence in children²s conversational speech with animated personas",
   "original": "i02_2689",
   "page_count": 4,
   "order": 671,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "During interpersonal conversation, both children and adults adapt the basic acoustic-prosodic features of their speech to converge with those of their conversational partner. In this study, 7-to-10- yearold children interacted with a conversational interface in which animated characters used text-to-speech output (TTS) to answer questions about marine biology. Analysis of childrens speech to different animated characters revealed a 29% average change in energy when they spoke to an extroverted loud software partner (E), compared with an introverted soft-spoken one (I). The majority, or 77% of children, adapted their amplitude toward their partners TTS voice. These adaptations were bi-directional, with increases in amplitude observed during I to E condition shifts, and decreases during E to I shifts. Finally, these results generalized across different user groups and TTS voices. Implications are discussed for guiding childrens speech to remain within system processing bounds, and for the future development of robust and adaptive conversational interfaces.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-671"
  },
  "stallard02b_icslp": {
   "authors": [
    [
     "David",
     "Stallard"
    ]
   ],
   "title": "Flexible dialogue management in the talk²ntravel system",
   "original": "i02_2693",
   "page_count": 4,
   "order": 672,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "A central problem for mixed-initiative dialogue management is coping with user utterances that fall outside of the expected sequence of the dialogue. Independent initiative by the user may require a complete revision of the future course of the dialogue, even when the system is engaged in activities of its own, such as querying a database, etc. This paper presents an event-driven, goal-based dialogue manager component we have developed to cope with these challenges. The dialog manager is explicitly architected for asynchronous input and flexible control, and uses a tree-ordered rule language we have developed that also provides for close coupling with discourse processing. The dialogue manager is implemented as part of TalknTravel, a simulated air travel reservation dialogue system we have developed under the DARPA Communicator dialogue research program.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-672"
  },
  "oria02_icslp": {
   "authors": [
    [
     "Daniela",
     "Oria"
    ],
    [
     "Esa",
     "Koskinen"
    ]
   ],
   "title": "E-mail goes mobile: the design and implementation of a spoken language interface to e-mail",
   "original": "i02_2697",
   "page_count": 4,
   "order": 673,
   "p1": "2697",
   "pn": "2700",
   "abstract": [
    "EVOS is a multilingual speech-enabled e-mail client that allows users to access their e-mails by using their voice. In this article we describe the implementation of the spoken language interface to e-mail and we motivate our choice to adopt a system-driven rather than a mixed-initiative interaction style. We also present the results of two usability tests and we compare them to those obtained from the evaluation of a DTMF interface for the same system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-673"
  },
  "yoma02_icslp": {
   "authors": [
    [
     "Néstor Becerra",
     "Yoma"
    ],
    [
     "Angela",
     "Cortés"
    ],
    [
     "Mauricio",
     "Hormazábal"
    ],
    [
     "Enrique",
     "López"
    ]
   ],
   "title": "Wizard of oz evaluation of a dialogue with communicator system in chile",
   "original": "i02_2701",
   "page_count": 4,
   "order": 674,
   "p1": "2701",
   "pn": "2704",
   "abstract": [
    "Results of Wizard of Oz experiments are presented using a Chilean Spanish version of the CU Communicator Travel Planning system in Chile. The experiments evaluated user responses during conversational interaction with the system to plan a domestic flight, and examined user impressions of a diphone text-to-speech synthesis system developed with speech samples from a Chilean speaker. Two synthesizers were employed, with and without pitch contour. The results showed a positive overall attitude toward the system, and that a proper pitch contour improves the subjective intelligibility of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-674"
  },
  "carpenter02_icslp": {
   "authors": [
    [
     "Bob",
     "Carpenter"
    ],
    [
     "Sasha",
     "Caskey"
    ],
    [
     "Krishna",
     "Dayanidhi"
    ],
    [
     "Caroline",
     "Drouin"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "A portable, server-side dialog framework for voiceXML",
   "original": "i02_2705",
   "page_count": 4,
   "order": 675,
   "p1": "2705",
   "pn": "2708",
   "abstract": [
    "We describe a spoken dialog application framework that combines the power and flexibility of server-side Java Servlets and Java Server Pages (JSPs) with the deployment portability, reliability and scalability of standard web (HTTP) servers and VoiceXML clients. Applications are developed by extending a framework of Java classes in order to define dialogs through lower level actions such as speech recognition, audio prompting, speech synthesis, and backend data access. The framework delegates session data management to servlets, embedding frame-based representations for the applications global and session data. Dialog flow is controlled through general constructions such as loops, conditionals, scoped sub-dialogs, along with scoped command, error, and exception handling. Prompting and grammars are configured through simple JSP templates that generate the VoiceXML instructions for the server to return to the client. The framework is designed to be extensible, as demonstrated by the implementation of customizable backup and repeat commands integrated with session data, command handling and grammar scoping.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-675"
  },
  "takahashi02c_icslp": {
   "authors": [
    [
     "S.",
     "Takahashi"
    ],
    [
     "T.",
     "Morimoto"
    ],
    [
     "S.",
     "Maeda"
    ],
    [
     "N.",
     "Tsuruta"
    ]
   ],
   "title": "Spoken dialogue system for home health care",
   "original": "i02_2709",
   "page_count": 4,
   "order": 676,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "This paper reports a Japanese spoken dialogue system aiming at support for home health care services. The targets of this system are aged people who live alone with comparatively good health condition. In order to watch their daily health condition, the system executes medical checks through some examinations and conversations. By generating appropriate discourse plan in collaboration with a medical expert program, the system performs a efficient discourse reducing redundant or needless interactions. This paper describes the architecture and the main features of our system, and presents the results for a dialogue experiment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-676"
  },
  "padrell02_icslp": {
   "authors": [
    [
     "Jaume",
     "Padrell"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "ACIMET: access to meteorological information by telephone",
   "original": "i02_2713",
   "page_count": 4,
   "order": 677,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "ACIMET* (ACcés a Informació Meteorològica per Telèfon) is a real service that gives access to automatically gathered meteorological information by using the human language technology available in TALP center for the Catalan language. The service is addressed to Catalan tourists, for leisure purposes and professionals with a need for immediate, and accurate meteo data (i.e. civil defense). Users request data by establishing a natural, oral dialogue by means of a phone call. The system also provides a warning and alarm service: the user will be informed through a phone call or an SMS whenever certain meteorological conditions take place. The oral dialogue system is evaluated with the PARADISE framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-677"
  },
  "engel02_icslp": {
   "authors": [
    [
     "Ralf",
     "Engel"
    ]
   ],
   "title": "SPIN: language understanding for spoken dialogue systems using a production system approach",
   "original": "i02_2717",
   "page_count": 4,
   "order": 678,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "This paper describes a language understanding module for spoken dialogue systems producing frame based semantic output. The presented approach adapts ideas from production systems to the task of language understanding. It interleaves in a new manner templatedriven cascaded word-to-frame transformation with syntactic analysis. The advantages over conventional parsers are the flexible output structure being independent of the syntactic structure in a wide range, the ability to use different levels of syntactic analysis at the same time and better support for relatively free word order languages like German. Other important properties are robustness, the capability to process complex utterances, and the easy creation of knowledge bases. A preliminary evaluation shows promising results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2002-678"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "fitch02_icslp",
    "young02_icslp"
   ]
  },
  {
   "title": "Speech Recognition in Noise - I",
   "papers": [
    "macho02_icslp",
    "adami02_icslp",
    "kleinschmidt02_icslp",
    "droppo02_icslp",
    "mak02_icslp",
    "segura02_icslp",
    "chen02_icslp",
    "kim02_icslp",
    "hilger02_icslp",
    "chen02b_icslp",
    "ida02_icslp",
    "hung02_icslp",
    "kotnik02_icslp",
    "lieb02_icslp",
    "wu02_icslp",
    "yao02_icslp",
    "docioferandez02_icslp",
    "fujimoto02_icslp",
    "saon02_icslp",
    "jain02_icslp",
    "kitaoka02_icslp",
    "cui02_icslp",
    "evans02_icslp",
    "farooq02_icslp",
    "onoe02_icslp",
    "morris02_icslp",
    "gong02_icslp",
    "moticek02_icslp",
    "cetin02_icslp",
    "hung02b_icslp",
    "yamade02_icslp",
    "siu02_icslp",
    "toma02_icslp",
    "pujolmarsal02_icslp",
    "xu02_icslp",
    "zolnay02_icslp",
    "wet02_icslp",
    "kingsbury02_icslp",
    "zheng02_icslp",
    "arrowood02_icslp",
    "stuttle02_icslp",
    "droppo02b_icslp",
    "lima02_icslp",
    "gadde02_icslp"
   ]
  },
  {
   "title": "Experimental Phonetics",
   "papers": [
    "son02_icslp",
    "aylett02_icslp",
    "serkhane02_icslp",
    "koenig02_icslp",
    "delvaux02_icslp"
   ]
  },
  {
   "title": "Speech Recognition: Adaptation",
   "papers": [
    "kenny02_icslp",
    "pusateri02_icslp",
    "huang02_icslp",
    "kwan02_icslp",
    "nguyen02_icslp",
    "ding02_icslp",
    "kim02b_icslp",
    "li02_icslp",
    "lima02b_icslp",
    "illina02_icslp",
    "plotz02_icslp",
    "niesler02_icslp",
    "li02b_icslp",
    "molau02_icslp",
    "levin02_icslp",
    "ogata02_icslp",
    "zhou02_icslp",
    "torre02_icslp",
    "brugnara02_icslp"
   ]
  },
  {
   "title": "Language Identification",
   "papers": [
    "stockmal02_icslp",
    "jayram02_icslp",
    "maddieson02_icslp",
    "torrescarrasquillo02_icslp",
    "wong02_icslp"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "jing02_icslp",
    "uebler02_icslp",
    "bisani02_icslp",
    "galescu02_icslp",
    "jilka02_icslp",
    "li02c_icslp",
    "kuo02_icslp",
    "rutten02_icslp",
    "wu02b_icslp",
    "yamagishi02_icslp",
    "tsuzaki02_icslp",
    "campillodiaz02_icslp",
    "kim02c_icslp",
    "sethy02_icslp",
    "breen02_icslp",
    "vepa02_icslp",
    "freitas02_icslp",
    "saito02_icslp",
    "hirai02_icslp",
    "ni02_icslp",
    "mori02_icslp",
    "shi02_icslp",
    "meng02_icslp",
    "lu02_icslp",
    "chazan02_icslp",
    "lee02_icslp",
    "shuang02_icslp",
    "song02_icslp",
    "kawahara02_icslp",
    "lee02b_icslp",
    "dong02_icslp",
    "navas02_icslp",
    "low02_icslp",
    "zhao02_icslp",
    "milner02_icslp",
    "kawanami02_icslp"
   ]
  },
  {
   "title": "Multimodal Spoken Language Processing",
   "papers": [
    "buhler02_icslp",
    "kaiser02_icslp",
    "yamakata02_icslp",
    "beskow02_icslp",
    "quek02_icslp",
    "quek02b_icslp",
    "nakadai02_icslp",
    "ma02_icslp",
    "cohen02_icslp",
    "poller02_icslp",
    "daubias02_icslp",
    "liu02_icslp",
    "dybkjr02_icslp",
    "quek02c_icslp"
   ]
  },
  {
   "title": "Perception: Non-Native",
   "papers": [
    "lambacher02_icslp",
    "tajima02_icslp",
    "clarke02_icslp",
    "burnham02_icslp",
    "broersma02_icslp"
   ]
  },
  {
   "title": "Dialog Systems I: Evaluation",
   "papers": [
    "minker02_icslp",
    "walker02_icslp",
    "walker02b_icslp",
    "sanders02_icslp",
    "sidner02_icslp"
   ]
  },
  {
   "title": "Voice Conversion",
   "papers": [
    "watanabe02_icslp",
    "turk02_icslp",
    "mashimo02_icslp",
    "gustafson02_icslp"
   ]
  },
  {
   "title": "Spoken Language Resources",
   "papers": [
    "burger02_icslp",
    "lopezcozar02_icslp",
    "ludwig02_icslp",
    "cucchiarini02_icslp",
    "aman02_icslp",
    "dong02b_icslp",
    "zitouni02_icslp",
    "alvarez02_icslp",
    "demuynck02_icslp",
    "minker02b_icslp",
    "bennett02_icslp",
    "schultz02_icslp",
    "salor02_icslp",
    "martell02_icslp",
    "hosom02_icslp",
    "gupta02_icslp",
    "lefevre02_icslp",
    "pfitzinger02_icslp"
   ]
  },
  {
   "title": "Speech Recognition: Search",
   "papers": [
    "kumar02_icslp",
    "wendt02_icslp",
    "lee02c_icslp",
    "dong02c_icslp",
    "zweig02_icslp",
    "zheng02b_icslp",
    "abdou02_icslp",
    "zhang02_icslp",
    "li02d_icslp"
   ]
  },
  {
   "title": "Auditory Models and Hearing Aids",
   "papers": [
    "pellant02_icslp",
    "kates02_icslp",
    "nelson02_icslp",
    "huettel02_icslp",
    "assmann02_icslp",
    "mackersie02_icslp",
    "johansson02_icslp"
   ]
  },
  {
   "title": "Multi-Lingual and Non-Native Spoken Language Processing",
   "papers": [
    "fischer02_icslp",
    "angkititrakul02_icslp",
    "tian02_icslp",
    "tian02b_icslp",
    "ma02b_icslp",
    "chengalvarayan02_icslp",
    "khudanpur02_icslp",
    "zhao02b_icslp",
    "sooful02_icslp",
    "he02_icslp",
    "minematsu02_icslp",
    "nguyen02b_icslp"
   ]
  },
  {
   "title": "Prosody in Spoken Dialogue Systems",
   "papers": [
    "caspers02_icslp",
    "esposito02_icslp",
    "itoh02_icslp",
    "ward02_icslp",
    "skantze02_icslp",
    "cerrato02_icslp",
    "darves02_icslp"
   ]
  },
  {
   "title": "Speaker Segmentation and Adaptation",
   "papers": [
    "rosenberg02_icslp",
    "sivakumaran02_icslp",
    "ajmera02_icslp",
    "meignier02_icslp",
    "mariethoz02_icslp",
    "farrell02_icslp",
    "mirghafori02_icslp"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "roy02_icslp",
    "bechet02_icslp",
    "bousquetvernhettes02_icslp",
    "pargellis02_icslp",
    "wang02_icslp",
    "xie02_icslp",
    "pfannerer02_icslp"
   ]
  },
  {
   "title": "INTERSPEECH",
   "papers": [
    "gurijala02_icslp",
    "hong02_icslp",
    "xiao02_icslp",
    "scharenborg02_icslp",
    "wu02c_icslp",
    "turk02b_icslp",
    "zhang02b_icslp",
    "wenndt02_icslp",
    "tatara02_icslp",
    "iwaki02_icslp",
    "sanders02b_icslp",
    "engwall02_icslp",
    "sato02_icslp",
    "campbell02_icslp",
    "li02e_icslp",
    "oster02_icslp",
    "fell02_icslp",
    "cosi02_icslp",
    "cave02_icslp"
   ]
  },
  {
   "title": "Large Vocabulary Speech Recognition",
   "papers": [
    "cordoba02_icslp",
    "rotovnik02_icslp",
    "jia02_icslp",
    "diegueztirado02_icslp",
    "chengalvarayan02b_icslp",
    "laureys02_icslp",
    "utsuro02_icslp",
    "nouza02_icslp",
    "nguyen02c_icslp",
    "chen02c_icslp",
    "shinozaki02_icslp",
    "langlois02_icslp",
    "ahn02_icslp",
    "dutagaci02_icslp"
   ]
  },
  {
   "title": "Integration of Speech Technology in Language Learning",
   "papers": [
    "hincks02_icslp",
    "raux02_icslp",
    "hirata02_icslp",
    "minematsu02b_icslp",
    "imoto02_icslp",
    "tsubota02_icslp",
    "neri02_icslp",
    "minematsu02c_icslp",
    "hardison02_icslp",
    "mostow02_icslp",
    "kim02d_icslp",
    "ariki02_icslp"
   ]
  },
  {
   "title": "Perception of Prosody",
   "papers": [
    "mixdorff02_icslp",
    "kinoshita02_icslp",
    "muto02_icslp",
    "baraccikoja02_icslp",
    "aasland02_icslp"
   ]
  },
  {
   "title": "Speech Enhancement I",
   "papers": [
    "herbordt02_icslp",
    "lu02b_icslp",
    "lin02_icslp",
    "attias02_icslp",
    "kim02e_icslp",
    "saruwatari02_icslp",
    "potamitis02_icslp",
    "nishiura02_icslp",
    "tam02_icslp",
    "hu02_icslp",
    "ju02_icslp",
    "kim02f_icslp",
    "chang02_icslp",
    "deng02_icslp",
    "nakadai02b_icslp",
    "visser02_icslp",
    "berthommier02_icslp",
    "kim02g_icslp",
    "mizumachi02_icslp"
   ]
  },
  {
   "title": "Speech Recognition: In-Vehicle",
   "papers": [
    "yapanel02_icslp",
    "shinde02_icslp",
    "gong02b_icslp",
    "kadambe02_icslp",
    "beaufays02_icslp"
   ]
  },
  {
   "title": "Mechanisms for Dialogue Processing",
   "papers": [
    "takahashi02_icslp",
    "bellegarda02_icslp",
    "wright02_icslp",
    "lee02d_icslp",
    "higashinaka02_icslp",
    "kakutani02_icslp",
    "eklund02_icslp",
    "soltau02_icslp",
    "goubanova02_icslp",
    "veilleux02_icslp",
    "guillevic02_icslp",
    "muller02_icslp",
    "sato02b_icslp",
    "hamimed02_icslp",
    "zhao02c_icslp",
    "lee02e_icslp",
    "hacioglu02_icslp"
   ]
  },
  {
   "title": "Language Modeling",
   "papers": [
    "akiba02_icslp",
    "zitouni02b_icslp",
    "picard02_icslp",
    "zhang02c_icslp",
    "siciliagarcia02_icslp",
    "stolcke02_icslp",
    "whittaker02_icslp",
    "corazza02_icslp",
    "rayner02_icslp",
    "huang02b_icslp",
    "mori02b_icslp",
    "wu02d_icslp",
    "mori02c_icslp",
    "erdogan02_icslp"
   ]
  },
  {
   "title": "Prosody and Speech Recognition - I",
   "papers": [
    "hirose02_icslp",
    "iwano02_icslp",
    "almasganj02_icslp",
    "baron02_icslp",
    "sun02_icslp",
    "escuderomancebo02_icslp",
    "thubthong02_icslp",
    "takagi02_icslp",
    "horiuchi02_icslp",
    "yamashita02_icslp",
    "kitamura02_icslp",
    "kai02_icslp",
    "yang02_icslp",
    "bartkova02_icslp",
    "shigeyoshi02_icslp"
   ]
  },
  {
   "title": "Pathology of Voice and Speech Production",
   "papers": [
    "svec02_icslp",
    "mady02_icslp",
    "wrench02_icslp",
    "patel02_icslp",
    "zahorian02_icslp"
   ]
  },
  {
   "title": "Model Based Speech Processing I",
   "papers": [
    "alku02_icslp",
    "schoentgen02_icslp",
    "vintsiuk02_icslp",
    "rufiner02_icslp",
    "funaki02_icslp"
   ]
  },
  {
   "title": "Acoustic Modeling",
   "papers": [
    "chin02_icslp",
    "hamaker02_icslp",
    "chelba02_icslp",
    "horn02_icslp",
    "markov02_icslp",
    "tsakalidis02_icslp",
    "okuda02_icslp",
    "bacchiani02_icslp",
    "huang02c_icslp",
    "zhang02d_icslp",
    "song02b_icslp",
    "ou02_icslp",
    "takahashi02b_icslp",
    "thirion02_icslp",
    "yun02_icslp",
    "salomon02_icslp",
    "stewart02_icslp",
    "lee02f_icslp",
    "park02_icslp",
    "lee02g_icslp",
    "stephenson02_icslp",
    "watanabe02b_icslp",
    "ogawa02_icslp"
   ]
  },
  {
   "title": "Phonetics",
   "papers": [
    "esling02_icslp",
    "murray02_icslp",
    "warner02_icslp",
    "xu02b_icslp",
    "maddieson02b_icslp",
    "tuan02_icslp",
    "chung02_icslp",
    "zvonik02_icslp",
    "pfitzinger02b_icslp",
    "tronnier02_icslp",
    "warner02b_icslp",
    "raymond02_icslp",
    "aoyagi02_icslp",
    "joto02_icslp"
   ]
  },
  {
   "title": "Call Classification and Routing",
   "papers": [
    "tur02_icslp",
    "li02f_icslp",
    "kuo02b_icslp",
    "cox02_icslp",
    "chotimongkol02_icslp",
    "levit02_icslp",
    "natarajan02_icslp"
   ]
  },
  {
   "title": "Acoustic Speech Modeling",
   "papers": [
    "gao02_icslp",
    "zhang02e_icslp",
    "brink02_icslp",
    "somervuo02_icslp",
    "chen02d_icslp",
    "jackson02_icslp",
    "zen02_icslp"
   ]
  },
  {
   "title": "Speech Synthesis: Alternative Views",
   "papers": [
    "huckvale02_icslp",
    "bulut02_icslp",
    "shichiri02_icslp",
    "marsi02_icslp",
    "swift02_icslp",
    "stent02_icslp",
    "roy02b_icslp"
   ]
  },
  {
   "title": "Finite State Transducers Applied to Spoken Language Processing",
   "papers": [
    "mou02_icslp",
    "shu02_icslp",
    "szarvas02_icslp",
    "caseiro02_icslp",
    "dolfing02_icslp",
    "kanthak02_icslp",
    "mohri02_icslp"
   ]
  },
  {
   "title": "Speaker Modeling and Scoring",
   "papers": [
    "xiang02_icslp",
    "petry02_icslp",
    "benzeghiba02_icslp",
    "xin02_icslp",
    "mami02_icslp",
    "park02b_icslp",
    "ding02b_icslp",
    "jin02_icslp",
    "navratil02_icslp",
    "liu02b_icslp",
    "kochanski02_icslp",
    "ahn02b_icslp",
    "xiang02b_icslp",
    "heck02_icslp",
    "li02g_icslp",
    "andorno02_icslp",
    "huggins02_icslp",
    "elenius02_icslp"
   ]
  },
  {
   "title": "Issues in Audio-Visual Spoken Language Processing",
   "papers": [
    "bernstein02_icslp",
    "deligne02_icslp",
    "bailly02_icslp",
    "vatikiotisbateson02_icslp",
    "rosenblum02_icslp",
    "hardison02b_icslp",
    "hazan02_icslp",
    "kirk02_icslp",
    "sekiyama02_icslp",
    "ponton02_icslp",
    "lewkowicz02_icslp",
    "bailly02b_icslp",
    "wojdel02_icslp",
    "wiggers02_icslp",
    "heckmann02_icslp",
    "erdener02_icslp",
    "krahmer02_icslp",
    "schwartz02_icslp",
    "zelezny02_icslp",
    "attina02_icslp",
    "attina02b_icslp",
    "sodoyer02_icslp",
    "house02_icslp",
    "lucey02_icslp"
   ]
  },
  {
   "title": "Speech Technology Applications",
   "papers": [
    "endo02_icslp",
    "watanabe02c_icslp",
    "lemmela02_icslp",
    "macherey02_icslp",
    "nishimoto02_icslp",
    "goto02_icslp",
    "wilkie02_icslp",
    "toth02_icslp",
    "iyer02_icslp",
    "nishizaki02_icslp",
    "lai02_icslp",
    "lo02_icslp",
    "ogata02b_icslp"
   ]
  },
  {
   "title": "Speech Production: Models and Physiology",
   "papers": [
    "dang02_icslp",
    "niikawa02_icslp",
    "maeda02_icslp",
    "mochida02_icslp",
    "finan02_icslp",
    "kaburagi02_icslp",
    "laprie02_icslp",
    "hiroya02_icslp",
    "hashimoto02_icslp",
    "sciamarella02_icslp"
   ]
  },
  {
   "title": "Tools for Spoken Language Resources",
   "papers": [
    "kolossa02_icslp",
    "maekawa02_icslp",
    "strik02_icslp",
    "yang02b_icslp",
    "yu02_icslp"
   ]
  },
  {
   "title": "Speech Recognition - Practical Issues",
   "papers": [
    "cornu02_icslp",
    "astrov02_icslp",
    "wang02b_icslp",
    "kiss02_icslp",
    "reinhard02_icslp",
    "filali02_icslp",
    "jiang02_icslp",
    "lahti02_icslp",
    "bazzi02_icslp",
    "duchateau02_icslp",
    "falavigna02_icslp",
    "wang02c_icslp"
   ]
  },
  {
   "title": "Perception",
   "papers": [
    "weil02_icslp",
    "ishizuka02_icslp",
    "yoneyama02_icslp",
    "brungart02_icslp",
    "janse02_icslp",
    "xue02_icslp",
    "wade02_icslp",
    "kearns02_icslp",
    "kuijpers02_icslp",
    "yip02_icslp",
    "magrinchagnolleau02_icslp",
    "serniclaes02_icslp",
    "sock02_icslp",
    "carre02_icslp",
    "nguyen02d_icslp",
    "otake02_icslp",
    "markham02_icslp",
    "yamashita02b_icslp",
    "ormanci02_icslp"
   ]
  },
  {
   "title": "Speech to Speech Translation - I",
   "papers": [
    "stallard02_icslp",
    "black02_icslp",
    "imamura02_icslp",
    "tanaka02_icslp",
    "gispert02_icslp",
    "watanabe02d_icslp",
    "sumita02_icslp",
    "zhou02b_icslp",
    "vogel02_icslp",
    "rossato02_icslp",
    "kauers02_icslp"
   ]
  },
  {
   "title": "Speech Processing",
   "papers": [
    "nishizawa02_icslp",
    "narusawa02_icslp",
    "murakami02_icslp",
    "nakatani02_icslp",
    "quatieri02_icslp"
   ]
  },
  {
   "title": "Speech Recognition: Broadcast and Courtroom Transcription",
   "papers": [
    "saraclar02_icslp",
    "prasad02_icslp",
    "nguyen02e_icslp",
    "hecht02_icslp",
    "imai02_icslp"
   ]
  },
  {
   "title": "Duration, Tempo, and Intonation",
   "papers": [
    "takamaru02_icslp",
    "kirkham02_icslp",
    "smith02_icslp",
    "hifny02_icslp",
    "jokisch02_icslp"
   ]
  },
  {
   "title": "Speech Coding and Transmission",
   "papers": [
    "patwardhan02_icslp",
    "veprek02_icslp",
    "zhong02_icslp",
    "nguyen02f_icslp",
    "nieminen02_icslp",
    "halmi02_icslp",
    "humphreys02_icslp",
    "yakhnich02_icslp",
    "parsa02_icslp",
    "kelleher02_icslp",
    "hirsch02_icslp",
    "moharir02_icslp"
   ]
  },
  {
   "title": "Spoken Document Retrieval",
   "papers": [
    "brun02_icslp",
    "zhou02c_icslp",
    "suzuki02_icslp",
    "asami02_icslp",
    "liu02c_icslp",
    "wang02d_icslp",
    "larson02_icslp",
    "wester02_icslp",
    "logan02_icslp",
    "wickramaratna02_icslp",
    "pinquier02_icslp",
    "karneback02_icslp",
    "ezzaidi02_icslp"
   ]
  },
  {
   "title": "Acoustic Correlates and Recognition of Emotion",
   "papers": [
    "scherer02_icslp",
    "rahurkar02_icslp",
    "yuan02_icslp",
    "tato02_icslp",
    "chuang02_icslp",
    "ang02_icslp",
    "makarova02_icslp"
   ]
  },
  {
   "title": "Dialog Strategy Design",
   "papers": [
    "oneill02_icslp",
    "torge02_icslp",
    "chung02b_icslp",
    "campana02_icslp",
    "ferrer02_icslp",
    "gorrell02_icslp",
    "shin02_icslp"
   ]
  },
  {
   "title": "Speech Synthesis - Prosody",
   "papers": [
    "jiang02b_icslp",
    "sun02b_icslp",
    "strom02_icslp",
    "hirose02b_icslp",
    "buhmann02_icslp",
    "meron02_icslp",
    "tao02_icslp"
   ]
  },
  {
   "title": "Speech Features",
   "papers": [
    "weber02_icslp",
    "aldulaimy02_icslp",
    "zhang02f_icslp",
    "tolba02_icslp",
    "leung02_icslp",
    "wilkinson02_icslp",
    "kitaoka02b_icslp",
    "omar02_icslp",
    "metze02_icslp",
    "ljolje02_icslp",
    "karnjanadecha02_icslp",
    "turunen02_icslp",
    "ho02_icslp",
    "megyesi02_icslp",
    "manfredi02_icslp",
    "sasou02_icslp",
    "toda02_icslp",
    "malayath02_icslp"
   ]
  },
  {
   "title": "Special Topics in Robust Speech Recognition",
   "papers": [
    "selouani02_icslp",
    "axelrod02_icslp",
    "mccowan02_icslp",
    "gelbart02_icslp",
    "couvreur02_icslp",
    "yu02b_icslp",
    "zhang02g_icslp",
    "gemello02_icslp",
    "peinado02_icslp",
    "fingscheidt02_icslp",
    "bernard02_icslp",
    "pelaezmoreno02_icslp",
    "tsuge02_icslp",
    "tan02_icslp",
    "muthusamy02_icslp",
    "milner02b_icslp"
   ]
  },
  {
   "title": "Distributed Multimodal Dialog Management Using Internet Technologies - I",
   "papers": [
    "johnston02_icslp",
    "wang02e_icslp",
    "bennett02b_icslp",
    "chai02_icslp",
    "liu02d_icslp",
    "katsurada02_icslp",
    "armaroli02_icslp",
    "ehlen02_icslp",
    "meng02b_icslp"
   ]
  },
  {
   "title": "Phonology",
   "papers": [
    "tsukada02_icslp",
    "jesus02_icslp",
    "dioubina02_icslp",
    "helgason02_icslp",
    "mareuil02_icslp",
    "hansson02_icslp",
    "jun02_icslp",
    "ohala02_icslp",
    "bosch02_icslp",
    "komatsu02_icslp"
   ]
  },
  {
   "title": "Feature Extraction for Speaker Recognition",
   "papers": [
    "faundezzanuy02_icslp",
    "sabac02_icslp",
    "kinnunen02_icslp",
    "tsang02_icslp",
    "ho02b_icslp"
   ]
  },
  {
   "title": "Issues in Speech Recognition",
   "papers": [
    "faltlhauser02_icslp",
    "arcienega02_icslp",
    "yao02b_icslp",
    "chen02e_icslp",
    "chen02f_icslp",
    "deng02b_icslp",
    "ghulam02_icslp",
    "liu02e_icslp",
    "zheng02c_icslp",
    "mcdermott02_icslp",
    "klautau02_icslp",
    "nagorski02_icslp"
   ]
  },
  {
   "title": "Speech Pathology Processing and Treatment",
   "papers": [
    "liotti02_icslp",
    "duncan02_icslp",
    "quek02d_icslp",
    "spielman02_icslp",
    "logemann02_icslp"
   ]
  },
  {
   "title": "Speech Pathology Processing",
   "papers": [
    "will02_icslp",
    "farley02_icslp",
    "parsa02b_icslp"
   ]
  },
  {
   "title": "Applications of Speech Signal Processing",
   "papers": [
    "semenov02_icslp",
    "kondo02_icslp",
    "muralishankar02_icslp",
    "jung02_icslp",
    "laprie02b_icslp",
    "sakamoto02_icslp",
    "satuevillar02_icslp",
    "kwon02_icslp",
    "gomezcipriano02_icslp",
    "irino02_icslp"
   ]
  },
  {
   "title": "Speech Synthesis: Unit Selection",
   "papers": [
    "vepa02b_icslp",
    "hamza02_icslp",
    "peng02_icslp",
    "yi02_icslp",
    "kawai02_icslp"
   ]
  },
  {
   "title": "Dialog Systems and Applications",
   "papers": [
    "wang02f_icslp",
    "fabbrizio02_icslp",
    "triassanz02_icslp",
    "coulston02_icslp",
    "stallard02b_icslp",
    "oria02_icslp",
    "yoma02_icslp",
    "carpenter02_icslp",
    "takahashi02c_icslp",
    "padrell02_icslp",
    "engel02_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.2002"
}