{
 "title": "5th European Conference on Speech Communication and Technology (Eurospeech 1997)",
 "location": "Rhodes, Greece",
 "startDate": "22/9/1997",
 "endDate": "25/9/1997",
 "chair": "General Chair: George Kokkinakis",
 "conf": "Eurospeech",
 "year": "1997",
 "name": "eurospeech_1997",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "5th European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1997)",
 "date": "22-25 September 1997",
 "booklet": "eurospeech_1997.pdf",
 "papers": {
  "rossi97_eurospeech": {
   "authors": [
    [
     "Mario",
     "Rossi"
    ]
   ],
   "title": "Is syntactic structure prosodically recoverable?",
   "original": "e97_kn01",
   "page_count": 8,
   "order": 1,
   "p1": "kn01-kn08",
   "pn": "",
   "abstract": [
    "Syntactic structure is defined in its three components. A survey of prosodist's studies on the relationship between prosody and syntax brings out two major trends: those who think that syntactic structure is immaterial for intonation and those who argue for matching rules between syntax and prosody. The recent studies presented here lend weight to a syntactic-based approach to accounting for intonational phrasing and boundary strength. A three-level model consisting of three modules, pragmatic, syntactic and rhythmic, is discussed. The flexibility of the higher units of prosodic structure is explained and predicted. The extent to which syntactic structure can be retrieved from prosodic markers is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-1"
  },
  "zue97_eurospeech": {
   "authors": [
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Conversational interfaces: advances and challenges",
   "original": "e97_kn09",
   "page_count": 10,
   "order": 2,
   "p1": "kn09-kn18",
   "pn": "",
   "abstract": [
    "The last decade has witnessed the emergence of a new breed of human computer interfaces that combines sev- eral human language technologies to enable information access and transactional processing using spoken dialogue. In this paper, I discuss my view on the research issues involved in the development of such interfaces, describe the recent work done in this area at the MIT Laboratory for Computer Science, and outline some of the unmet re- search challenges, including the need to work in real do- mains, spoken language generation, and portability across domains and languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-2"
  },
  "santen97_eurospeech": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Prosodic modelling in text-to-speech synthesis",
   "original": "e97_kn19",
   "page_count": 10,
   "order": 3,
   "p1": "kn19-kn28",
   "pn": "",
   "abstract": [
    "This paper discusses three broad obstacles that must be overcome to improve prosodic quality in text-to-speech systems. First, direct and indirect limits set by the signal processing (\"synthesis\") components. Second, combinatorial and statistical constraints inherent in generalizing from training corpora to unrestricted domains, and that require the integration of contentspecific knowledge and detailed mathematical modeling. Third, the nature of many empirical research issues that must be solved for prosodic modeling to improve: they are often too focused and model-dependent for academe, and too long-term for development organizations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-3"
  },
  "junqua97_eurospeech": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Impact of the unknown communication channel on automatic speech recognition: a review",
   "original": "e97_kn29",
   "page_count": 4,
   "order": 4,
   "p1": "kn29-kn32",
   "pn": "",
   "abstract": [
    "This review article summarizes the main difficulties encountered in Automatic Speech Recognition (ASR) when the type of communication channel is not known. This problem is crucial for the development of successful applications in promising domains such as computer telephony and cars. The main technical problems encountered are due to the speaker and the task (e.g. speaking style, Lombard reflex, vocal tract geometry), the use of microphones with different characteristics, the variable quality of the support channels (e.g. telephone channels are noisy and have different characteristics), reverberation and echoes, the variable distance and direction to the microphone introduced by hands-free recognition, and the ambient noise which distorts the input speech signals. This overview characterizes and emphasizes these problems and highlights some promising directions for future research. Finally, it presents an attempt to characterize the sensitivity of a phoneme recognizer as a function of the source of channel distortion, using the TIMIT database and several of its variants (NTIMIT, CTIMIT, FFMTIMIT).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-4"
  },
  "bellegarda97_eurospeech": {
   "authors": [
    [
     "Jerome",
     "Bellegarda"
    ]
   ],
   "title": "Statistical techniques for robust ASR: review and perspectives",
   "original": "e97_kn33",
   "page_count": 4,
   "order": 5,
   "p1": "kn33-kn36",
   "pn": "",
   "abstract": [
    "Speech recognition performance degrades significantly when a mismatch occurs between training and operating conditions. To reduce this mismatch, it is often necessary to characterize the mapping between two environments. A number of statistical approaches have been developed for this purpose. They can be classified as either predictive or adaptive, depending on what information is available regarding the operating environment. This paper reviews a selected subset from both categories, and discusses possible future directions of improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-5"
  },
  "lippmann97_eurospeech": {
   "authors": [
    [
     "Richard",
     "Lippmann"
    ],
    [
     "Beth A.",
     "Carlson"
    ]
   ],
   "title": "Using missing feature theory to actively select features for robust speech recognition with interruptions, filtering and noise",
   "original": "e97_kn37",
   "page_count": 4,
   "order": 6,
   "p1": "kn37-kn40",
   "pn": "",
   "abstract": [
    "Speech recognizers trained with quiet wide-band speech degrade dramatically with high-pass, low-pass, and notch filtering, with noise, and with interruptions of the speech input. A new and simple approach to compensate for these degradations is presented which uses mel-filter-bank (MFB) magnitudes as input features and missing feature theory to dynamically modify the probability computations performed in Hidden Markov Model recognizers. When the identity of features missing due to filtering or masking is provided, recognition accuracy on a large talker-independent digit recognition task often rises from below 50% to above 95%. These promising results suggest future work to continuously estimate SNR's within MFB bands for dynamic adaptation of speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-6"
  },
  "dupont97_eurospeech": {
   "authors": [
    [
     "Stéphane",
     "Dupont"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Using multiple time scales in a multi-stream speech recognition system",
   "original": "e97_0003",
   "page_count": 4,
   "order": 7,
   "p1": "3",
   "pn": "6",
   "abstract": [
    "In this paper, we propose and investigate a new approach towards using multiple time scale information in automatic speech recognition (ASR) systems. In this framework, we are using a particular HMM formalism able to process different input streams and to recombine them at some temporal anchor points. While the phonological level of recombination has to be defined a priori, the optimal temporal anchor points are obtained automatically during recognition. In the current approach, those parallel cooperative HMMs will focus on different dynamic properties of the speech signal, defined on different time scales. The speech signal is then defined in terms of several information streams, each stream resulting from a particular way of analyzing the speech signal. More specifically, in the current work, models aimed at capturing the syllable level temporal structure are used in parallel with classical phoneme-based models. Tests on different continuous speech databases show significant performance improvements, motivating further research to eficiently use large time span information of the order of 200 ms into our standard 10 ms, phone-based ASR systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-7"
  },
  "wakita97_eurospeech": {
   "authors": [
    [
     "Yumi",
     "Wakita"
    ],
    [
     "Harald",
     "Singer"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Speech recognition using HMM-state confusion characteristics",
   "original": "e97_0007",
   "page_count": 4,
   "order": 8,
   "p1": "7",
   "pn": "10",
   "abstract": [
    "In our previous work, we proposed a re-entry modeling of missing phonemes which are lost during search process. In the reentry modeling, the recognition results are postprocessed and originally recognized phoneme sequences are converted to new phoneme sequences using HMM-state confusion characteristics spanning several phonemes. We confirmed that HMM-state confusions are effective for the re-entry modeling. In this paper, we propose a re- entry modeling during recognition using a multiple pronunciation dictionary where pronunciations are added using HMM-state confusion characteristics. The pronunciations are added considering part-of-speech (POS) dependency of confusion characteris- tics. As a result of continuous recognition experiments, we confirmed that the following two points are effective to improve word recognition rates: (1) confusions are expressed by HMM-state sequences, (2) pronunciations are added considering part-of-speech dependency of confusion characteristics. they cannot cope with the confusion in consideration of the previous and following context of misrecognized sequences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-8"
  },
  "chesta97_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Chesta"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Franco",
     "Ravera"
    ]
   ],
   "title": "Bottom-up and top-down state clustering for robust acoustic modeling",
   "original": "e97_0011",
   "page_count": 4,
   "order": 9,
   "p1": "11",
   "pn": "14",
   "abstract": [
    "In this paper we describe our experience with bottom- up and top- down state clustering techniques for the definition and training of robust acoustic-phonetic units. Using as a test-bed a speaker- independent telephone- speech isolated word recognition task with a vocabulary including 475 city names, we show that similar performances are obtained by tying the HMM states both with an agglomerative or a decision-tree clustering approach. Moreover, better results are obtained by a priori selecting the set of states that can be clustered, rather than relying solely on their acoustical similarity. In the bottom-up approach a stopping criterion for the furthest neighbor clustering procedure is proposed that does not require a threshold. In the top-down approach we show that a careful selected impurity function allows lookahead search to outperforms the classical decision tree growing algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-9"
  },
  "schluter97_eurospeech": {
   "authors": [
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "W.",
     "Macherey"
    ],
    [
     "S.",
     "Kanthak"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Lutz",
     "Welling"
    ]
   ],
   "title": "Comparison of optimization methods for discriminative training criteria",
   "original": "e97_0015",
   "page_count": 4,
   "order": 10,
   "p1": "15",
   "pn": "18",
   "abstract": [
    "In this work we compare two parameter optimization techniques for discriminative training using the MMI criterion: the extended Baum- Welch (EBW) algorithm and the generalized probabilistic descent (GPD) method. Using Gaussian emission densities we found special expressions for the step sizes in GPD, leading to reestimation formula very similar to those derived for the EBW algorithm. Results were produced for both the TI digitstring and the SieTill corpus for continuously spoken American English and German digitstrings. The results for both techniques do not show significant differences. This experimental results support the strong link between EBW and GPD as expected from the analytic comparison.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-10"
  },
  "lee97_eurospeech": {
   "authors": [
    [
     "Clark Z.",
     "Lee"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Clustering beyond phoneme contexts for speech recognition",
   "original": "e97_0019",
   "page_count": 4,
   "order": 11,
   "p1": "19",
   "pn": "22",
   "abstract": [
    "The clustering of using decision trees is generalized to take into account high-level knowledge sources to better model the co- articulation effects in large vocabulary continuous speech recognition. VQ models are used to reduce the computational cost in constructing decision trees. The search algorithm is designed such that it can provide a general type of information for decision trees without compromising the speed. Experiments with a 30k-word dictionary on the WSJ task show that the word error rate can be reduced by considering additional knowledge sources. use much more complex acoustic-phonetic models without compromising the speed in our system. Experiments on the Wall Street Journal task show that it may increase the recognition accuracy to use deci- sion trees with additional knowledge sources.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-11"
  },
  "chengalvarayan97_eurospeech": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Influence of outliers in training the parametric trajectory models for speech recognition",
   "original": "e97_0023",
   "page_count": 4,
   "order": 12,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "In this study, we developed a modified maximum likelihood (ML) algorithm for efficient computation in implemeting the minimum classifcation error (MCE) like training for optimally estimating the state-dependent polynomial coefficients in the trended HMM. We devised a new discriminative training method which controls the in uence of outliers in the training data on the constructed models. The resulting models seem to provide correct recognition for confusable patterns. For alphabet recognition tasks, outlier emphasis resulted in improved performance. An error rate reduction of 14% is achieved for the linear trend and 7.5% is obtained for the constant trend models over the traditional ML training models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-12"
  },
  "holter97_eurospeech": {
   "authors": [
    [
     "Trym",
     "Holter"
    ],
    [
     "Torbjorn",
     "Svendsen"
    ]
   ],
   "title": "Incorporating linguistic knowledge and automatic baseform generation in acoustic subword unit based speech recognition",
   "original": "e97_1159",
   "page_count": 4,
   "order": 13,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "A major challenge in speech recognition based on acoustic subword units is creating a lexicon which is robust to inter- and intra-speaker variations. In this paper we present two different approaches for incorporating simple word-level linguistic knowledge into the labelling step of the training procedure. The proposed systems also utilise a scheme for combined optimisation of baseforms and subword models. For the TI46 database, these methods are shown to greatly improve the performance compared to an acoustic subword based speech recogniser employing unsupervised labelling, and they are found to perform as well as systems utilising whole-word models and context independent phoneme models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-13"
  },
  "beyerlein97_eurospeech": {
   "authors": [
    [
     "Peter",
     "Beyerlein"
    ],
    [
     "Meinhard",
     "Ullrich"
    ],
    [
     "Patricia",
     "Wilcox"
    ]
   ],
   "title": "Modelling and decoding of crossword context dependent phones in the Philips large vocabulary continuous speech recognition system",
   "original": "e97_1163",
   "page_count": 4,
   "order": 14,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "The performance of the Philips system for large vocabulary continuous speech recognition has been improved significantly by crossword N-phone modelling, enhanced clustering of HMM-states during training, consistent handling of untrained HMM-states during decoding and a new effcient crossword N-phone M-gram decoding strategy. We report word error rate reductions of up to 18% on various ARPA test sets as compared to our best within-word triphone system, based on Laplacian densities, Viterbi decoding and _lterbank-LDA features. The following two issues are addressed: a) Transformation of a tree-organized bigram beam- search decoder into an effcient tree- organized decoder capable of handling long-span acoustic contexts as well as long-span language model contexts. b) State-clustering and generalizing of unseen contexts for the case of Laplacian emission probability density functions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-14"
  },
  "hanna97_eurospeech": {
   "authors": [
    [
     "Philip",
     "Hanna"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Peter",
     "O'Boyle"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "Modelling inter-frame dependence with preceeding and succeeding frames",
   "original": "e97_1167",
   "page_count": 4,
   "order": 15,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "This paper explores the modelling of inter-frame dependence as a means of improving the performance of HMMs. More specifically, a model based on the IFD- HMM (Ming & Smith, 1996) that assumes a dependency upon both succeeding and preceeding frames is proposed. The means by which a dependency upon succeeding frames might be integrated into a HMM framework are explored, and a mathematical outline of the proposed extension given. The results of various tests aimed towards exploring the consequences of introducing succeeding frame dependencies are included. It was found that a dependency upon succeeding frames enabled dynamic spectral information, not found in the preceeding frames, to be usefully employed; resulting in a significant increase in the recognition accuracy. Additionally, it was shown that modelling of the dynamic spectral information (using time-lag sequences) was at least as important as improved modelling of the instantaneous spectra (using multiple mixtures).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-15"
  },
  "jones97_eurospeech": {
   "authors": [
    [
     "Rhys James",
     "Jones"
    ],
    [
     "Simon",
     "Downey"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Continuous speech recognition using syllables",
   "original": "e97_1171",
   "page_count": 4,
   "order": 16,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "The vast majority of work in continuous speech recognition uses phoneme-like units as the basic recognition component. The work presented here investigates the practicability of syllable-like units as the building blocks for recognition. A phonetically annotated telephony database is analysed at the syllable level, and a set of syllable-based HMMs are built. Refinements including the introduction of syllable-level bigram probabilities, word- and syllable- level insertion penalties, and the investigation of different model topologies are found to improve recogniser performance. It is found that the syllable-based recogniser gives recognition accuracies of over 60%, which compares with 35% as the baseline accuracy for monophone recognition. It is envisaged that practical applications of syllable recognition could be in a hybrid system, where the most common syllable HMMs would be used in conjunction with whole- word and phoneme models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-16"
  },
  "willett97_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "A new approach to generalized mixture tying for continuous HMM-based speech recognition",
   "original": "e97_1175",
   "page_count": 4,
   "order": 17,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "In this paper we present a new approach for a generalized tying of mixture components for continuous mixture-density HMM-based speech recognition systems. With an iterative pruning and splitting procedure for the mixture components, this approach offers a very accurate and detailed representation of the acoustic space and at the same time keeps the number of parameters reasonably small in favor of a robust parameter estimation and a fast decoding. Contrary to other approaches, it does not require a strict clustering of the pdfs into subsets that share their mixture components, so that it is capable of providing more general and flexible types of mixture tying. We applied the new approach on a semi-continuous HMM (SCHMM)- system for the Resource Management task and improved its recognition performance by 12% and vastly accelerated the decoding because of a much faster likelihood computation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-17"
  },
  "beulen97_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Beulen"
    ],
    [
     "Elmar",
     "Bransch"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "State tying for context dependent phoneme models",
   "original": "e97_1179",
   "page_count": 4,
   "order": 18,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "In this paper several modifications of two methods for parameter reduction of Hidden Markov Models by state tying are described. The two methods represent a data driven clustering triphone states with a bottom up algorithm [3, 9], and a top down method growing decision trees for triphone states [2, 10]. We investigate several aspects of state tying as the possible reduction of the word error rate by state tying, the consequences of different distance measures for the data driven approach and modi_cations of the original decision tree approach such as node merging. The tests were performed on the test corpora for the 5 000 word vocabulary of the WSJ November 92 task and on the evaluation corpora for the 3 000 word VERBMOBIL '95 task. The word error rate by state tying was reduced by 14% for the WSJ task and by 5% for the VERBMOBIL task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-18"
  },
  "duchateau97_eurospeech": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "A novel node splitting criterion in decision tree construction for semi-continuous HMMs",
   "original": "e97_1183",
   "page_count": 4,
   "order": 19,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "In [1], we described how to improve Semi-Continuous Density Hidden Markov Models (SC-HMMs) to be as fast as Continuous Density HMMs (CD-HMMs), whilst outperforming them on large vocabulary recognition tasks with context independent models. In this paper, we extend our work with SC-HMMs to context dependent modelling. We propose a novel node splitting criterion in an approach with phonetic decision trees. It is based on a distance measure between mixture gaussian probability density functions (pdfs) as used in the final tied state SC-HMMs, this in contrast with other criteria which are based on simplified pdfs to manage the algorithm complexity. Results on the ARPA Resource Management task show that the proposed criterion outperforms two of these criteria with simplified pdfs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-19"
  },
  "blomberg97_eurospeech": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Creating unseen triphones by phone concatenation in the spectral, cepstral and formant domains",
   "original": "e97_1187",
   "page_count": 4,
   "order": 20,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "A technique for predicting triphones by concatenation of diphone or monophone models is studied. The models are connected using linear interpolation between endpoints of piece-wise linear parameter trajectories. Three types of spectral representation are compared: formants, filter amplitudes and cepstrum coefficients. The proposed technique lowers the spectral distortion of the phones for all three representations when different speakers are used for training and evaluation. The average error of the created triphones is lower in the filter and cepstrum domains than for formants. This is explained to be caused by limitations in the Analysis-by-Synthesis formant tracking algorithm. A small improvement with the proposed technique is achieved for all representations in the task of reordering N-best sentence recognition candidate lists.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-20"
  },
  "pfau97_eurospeech": {
   "authors": [
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Manfred",
     "Beham"
    ],
    [
     "W.",
     "Reichl"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Creating large subword units for speech recognition",
   "original": "e97_1191",
   "page_count": 4,
   "order": 21,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "This paper deals with the choice of suitable subword units (SWU) for a HMM based speech recognition system. Using demisyllables (including phonemes) as base units, an inventory of domain-specific larger sized subword units, so-called macro-demisyllables (MDS), is created. A quality measure for the automatic decomposition of all single words into subword units is presented which takes into account the trainability of the chosen units. To create the whole inventory an iterative procedure is applied with respect to the predefined quality measure. Each MDS is represented by a dedicated HMM. By tying the densities of specific phonemes, only the number of mixture coefficients and transitions increases in comparison to the original phoneme models. Recogniton experiments within the German Verbmobil evaluation 1996 show that the new simple MDS models are as powerful as standard triphone models, although our MDS models are up to now context-independent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-21"
  },
  "goldberger97_eurospeech": {
   "authors": [
    [
     "Jacob",
     "Goldberger"
    ],
    [
     "David",
     "Burshtein"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Segmental modeling using a continuous mixture of non-parametric models",
   "original": "e97_1195",
   "page_count": 4,
   "order": 22,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "The aim of the research described in this paper is to overcome the modeling limitation of conventional hidden Markov models. We present a segmental model that consists of two elements. The first is a nonparametric representation of both the mean and variance trajectories, which describes the local dynamics. The second element is some parameterized transformation (e.g., random shift) of the trajectory that is global to the segment and models long-term variations such as speaker identity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-22"
  },
  "chang97_eurospeech": {
   "authors": [
    [
     "Jane W.",
     "Chang"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Segmentation and modeling in segment-based recognition",
   "original": "e97_1199",
   "page_count": 4,
   "order": 23,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "Recently, we have developed a probabilistic framework for segment- based speech recognition that represents the speech signal as a network of segments and associated feature vectors [2]. Although in general, each path through the network does not traverse all segments, we argued that each path must account for all feature vectors in the network. We then demonstrated an efficient search algorithm that uses a single additional model to account for segments that are not traversed. In this paper, we present two new extensions to our framework. First, we replace our acoustic segmentation algorithm with \"segmentation by recognition,\" a probabilistic algorithm that can combine multiple contextual constraints towards hypothesizing only the most likely segments. Second, we generalize our framework to \"near-miss modeling\" and describe a search algorithm that can efficiently use multiple models to enforce contextual constraints across all segments in a network. We report experiments in phonetic recognition on the TIMIT corpus in which we achieve a diphone context-dependent error rate of 26.6% on the NIST core test set over 39 classes. This is a 12.8% reduction in error rate from our best previously reported result.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-23"
  },
  "hauenstein97_eurospeech": {
   "authors": [
    [
     "Alfred",
     "Hauenstein"
    ]
   ],
   "title": "Using syllables in a hybrid HMM-ANN recognition system",
   "original": "e97_1203",
   "page_count": 4,
   "order": 24,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "An approach to speech recognition using syllables as basic modelling units is compared to a state-of-the-art system employing phonemes. The technological framework is a hybrid HMM-ANN 1 recognition system applied on small to medium vocabulary recognition tasks. Although the number of units to be classified nearly doubles, it is shown that the syllable can outperform the phoneme slightly but significantly in terms of unit classification capability, measured as frame error rate. Compar- ing the overall system performance (measured in word error rate) the phoneme-based system still performs obviously better for continuous speech tasks, while the syllable-based system is superior for isolated word recognition tasks on cross-database tests. This suggests the need for further work on the understanding of the interaction of knowledge sources on the frame-, word-, and sentence-level in current recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-24"
  },
  "hariharan97_eurospeech": {
   "authors": [
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Juha",
     "Hakkinen"
    ],
    [
     "Kari",
     "Laurila"
    ],
    [
     "Janne",
     "Suontausta"
    ]
   ],
   "title": "Noise robust segment-based word recognition using vector quantisation",
   "original": "e97_1207",
   "page_count": 4,
   "order": 25,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "Segment-based speech recognition systems have been proposed in recent years to overcome some of the deficiencies of the current state- of-the-art HMM based systems. In this paper, we present a segmental speech recogniser, where the speech trajectory segments are modelled using their mean, variance and shape. The shape is chosen from a codebook of global vector quantised trajectories, obtained from uniformly segmented training utterances. Experiments were done for a speaker dependent isolated word recognition application under different noise environments. The results have shown that this segment based approach outperforms HMM based speech recognition systems under similar test conditions. In adverse noise conditions, up to 34% error rate reduction was achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-25"
  },
  "rodriguez97_eurospeech": {
   "authors": [
    [
     "Luis Javier",
     "Rodriguez"
    ],
    [
     "Ines M.",
     "Torres"
    ]
   ],
   "title": "Viterbi based splitting of phoneme HMMs",
   "original": "e97_1211",
   "page_count": 4,
   "order": 26,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "Continuous Speech Recognition Systems (CSR) usually include large sets of context dependent units to model contextual variations in the pronunciation of phones. The goal of this work was to obtain adequate sets of sub-lexical models by using acoustic information but excluding any previous phonological knowledge. At each iteration of a classical Viterbi training scheme each acoustic model was split into a set of more accurate models. This approach was evaluated over a Spanish acoustic phonetic decoding task. The experimental results showed that this approach produces similar recognition rates than classical triphones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-26"
  },
  "marino97_eurospeech": {
   "authors": [
    [
     "José B.",
     "Marino"
    ],
    [
     "Albin",
     "Nogueiras"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "The demiphone: an efficient subword unit for continuous speech recognition",
   "original": "e97_1215",
   "page_count": 4,
   "order": 27,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "In this paper we introduce the demiphone as a contextual phonetic unit for continuous speech recognition. A phone is divided into two parts: a left demiphone that accounts for the left side coarticulation and a right demiphone that copes with the right side context. This new unit discards the dependence between the effects of both side contexts, but provides a better training of the transition between phones. The demiphone can be seen as a heuristic clustering of states that allows a more smoothed training of hidden Markov models and additionally supplies a simple way to create unseen triphones. We report experimental evidence that demiphones outperform the usual combination of triphones, right-side and left-side biphones and monophones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-27"
  },
  "kojima97_eurospeech": {
   "authors": [
    [
     "Hiroaki",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Organizing phone models based on piecewise linear segment lattices of speech samples",
   "original": "e97_1219",
   "page_count": 4,
   "order": 28,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "Aiming at robust speech recognition, we have proposed a framework for \"phonological concept formation,\" which is the task of acquiring an efficient representation of phonemes from spoken word samples without using any transcriptions except for the lexical classification of the words. In order to implement this task, we propose the \"piecewise linear segment lattice (PLSL)\" model for phoneme representation. The structure of this model is a lattice of segments, each of which is represented as regression coefficients of feature vectors within the segment. In order to organize phone models, operations including division, concatenation, blocking and clustering are applied to the models. Feasibility of the method is discussed with experimental results for isolated word recognition. The recognition rate is improved by applying these operations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-28"
  },
  "rogina97_eurospeech": {
   "authors": [
    [
     "Ivica",
     "Rogina"
    ]
   ],
   "title": "Automatic architecture design by likelihood-based context clustering with crossvalidation",
   "original": "e97_1223",
   "page_count": 4,
   "order": 29,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "Most state-of-the-art speech recognizers benefit from some kind of context information in their acoustic modeling [1][2][3]. The most common approach to context clustering is a divisive method that is iteratively building decision trees [4][5]. The problem, when to stop the growing of the tree is usually solved by choosing the maximum number of resulting models that can be supported by the available training data and/or computer memory and CPU power. In this paper we propose a new algorithm, that not only offers an optimized stopping criterion, but also uses a likelihood-based distance measure that optimizes the likelihood of unseen training-data at every splitting of a decision tree node. We evaluate our algorithm on the Wall Street Journal task, and show that it outperforms an algorithm using an entropy-based distance measure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-29"
  },
  "roweis97_eurospeech": {
   "authors": [
    [
     "Sam",
     "Roweis"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Towards articulatory speech recognition: learning smooth maps to recover articulator information",
   "original": "e97_1227",
   "page_count": 4,
   "order": 30,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "We present a novel method for recovering articulator movements from speech acoustics based on a constrained form [9] of a hidden Markov model. The model attempts to explain sequences of high dimensional data using smooth and slow trajectories in a latent variable space. The key insight is that this continuity constraint when applied to speech helps to solve the \"ill-posed\" problem of acoustic to articulatory mapping. By working with sequences of spectra rather than looking only at individual spectra, it is possible to choose between competing articulatory configurations for any given spectrum by selecting the configuration \"closest\" to those at nearby times. We present results of applying this algorithm to recover articulator movements from acoustics using data from the Wisconsin X-ray microbeam project [3]. We find that the recovered traces are highly correlated with the measured articulator movements under a single linear transform. Such recovered traces have the potential to be used for speech recognition, an application we are currently investigating.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-30"
  },
  "tsopanoglou97_eurospeech": {
   "authors": [
    [
     "Anastasios",
     "Tsopanoglou"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "Selection of the most effective set of subword units for an HMM-based speech recognition system",
   "original": "e97_1231",
   "page_count": 4,
   "order": 31,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "In this work we describe several approaches to determine an effective set of subword units for modeling the spoken Greek language. We tried to form a concrete set of basic units which must have the capability of giving a unique phonetic transcription for every input utterance. The results of an extensive set of experiments showed that the use of longer units than phonemes can lead to a significant improvement in a system's performance. Three sets of subword units were finally formed regarding the way we combined the 42 phonemes of the Greek Language. The three approaches showed better results than the baseline phoneme-based system and the most effective one proved to be the second approach in which we used two-phoneme combinations of the types non-vowel/vowel and non-vowel/non- vowel. The phoneme recognition rate of the system increased almost by 9% (reaching a level of 78.65%) for the best situation compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-31"
  },
  "cerisara97_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Jean-Francois",
     "Mari"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Multi-band continuous speech recognition",
   "original": "e97_1235",
   "page_count": 4,
   "order": 32,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "The problem addressed by this paper is to enhance the continuous speech recognizers robustness to noise. For this purpose, the acoustic signal is filtered into several spectral bands, and independent recognition is achieved in each band. Then, the system recombines the results given by each recognizer and delivers a unique solution. The main advantage of this method is to consider the signal only in the bands which are relevant, and to ignore spectral bands which are corrupted by noise. We are developping a speaker-independent continuous speech recognizer based on this principle.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-32"
  },
  "bitar97_eurospeech": {
   "authors": [
    [
     "Nabil N.",
     "Bitar"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "The design of acoustic parameters for speaker-independent speech recognition",
   "original": "e97_1239",
   "page_count": 4,
   "order": 33,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "This paper presents a two-stage procedure, based on the Fisher criterion and automatic classification trees, for designing acoustic parameters (APs) that target phonetic features in the speech signal. This procedure and a subset of the TIMIT 1 training set were used to develop acoustic parameters for the phonetic features: sonorant, syllabic, strident, palatal, alveolar, labial and velar. Results on a subset of the TIMIT test set show that the developed parameters achieve correct phonetic-feature classification rates in the 90 % range with the exception of stop- consonant place of articulation (labial, alveolar and velar) where correct classification is about 73 %. Furthermore, it is shown that by basing the acoustic parameters on relative measures (e.g. an acoustic parameter that measures energy in a frequency band relative to energy in the same band at another time instant) the effect of interspeaker variability (e.g. gender) on the parameters is reduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-33"
  },
  "candille97_eurospeech": {
   "authors": [
    [
     "Laurence",
     "Candille"
    ],
    [
     "Henri",
     "Méloni"
    ]
   ],
   "title": "Adaptation of natural articulatory movements to the control of the command parameters of a production model",
   "original": "e97_0027",
   "page_count": 4,
   "order": 34,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "A number of experiments have shown the importance of the use of speech production models for automatic speech recognition ([1],[4],[6]).This work is very interesting for the concise representation of the sound coarticulation phenomena in continuous speech. Maeda's statistical model [5] has been chosen to conduct our experiments. The first part of the paper focusses on adjusting the model configurations characterizing the French vocalic sounds in an optimum way so as to minimize the acoustic distances from the phonemes produced by a speaker. The second part provides a control strategy for Maeda's model command parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-34"
  },
  "stone97_eurospeech": {
   "authors": [
    [
     "Maureen",
     "Stone"
    ],
    [
     "Andrew",
     "Lundberg"
    ],
    [
     "Edward",
     "Davis"
    ],
    [
     "Rao",
     "Gullapalli"
    ],
    [
     "Moriel",
     "NessAiver"
    ]
   ],
   "title": "Three-dimensional coarticulatory strategies of tongue movement",
   "original": "e97_0031",
   "page_count": 4,
   "order": 35,
   "p1": "31",
   "pn": "34",
   "abstract": [
    "This paper will present three-dimensional tongue \"Volumes,\" reconstructed from three sagittal slices (left, mid, right) made using tagged cine MRI. The Volumes will be animated to show CV movement from the consonants /k/ and /s/ to the vowels /i/, /a/, and /u/.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-35"
  },
  "parlangeau97_eurospeech": {
   "authors": [
    [
     "Nathalie",
     "Parlangeau"
    ],
    [
     "Regine",
     "Andre-Obrecht"
    ]
   ],
   "title": "From laryngographic and acoustic signals to voicing gestures",
   "original": "e97_0035",
   "page_count": 4,
   "order": 36,
   "p1": "35",
   "pn": "38",
   "abstract": [
    "Many researchers have seen in the articulation an intermediate level of representation. In the gestural phonetic theory, units are articulatory gestures. In order to assess this theory with observed parameters, we have defined a robust labelling system (AMULET) of the multi sensor ACCOR speech database. Main articulatory gestures searched are Voice Onset and Voice Termination on both acoustic and laryngographic signals. We present here two efficient Voiced/Unvoiced/Silence detectors for the acoustic signals and a third one for the laryngographic signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-36"
  },
  "vilkman97_eurospeech": {
   "authors": [
    [
     "Erkki",
     "Vilkman"
    ],
    [
     "Raija",
     "Takalo"
    ],
    [
     "Taisto",
     "Maatta"
    ],
    [
     "Anne-Maria",
     "Laukkanen"
    ],
    [
     "Jaana",
     "Nummenranta"
    ],
    [
     "Tero",
     "Lipponen"
    ]
   ],
   "title": "Ultrasonographic measurement of cricothyroid space in speech",
   "original": "e97_0039",
   "page_count": 4,
   "order": 37,
   "p1": "39",
   "pn": "42",
   "abstract": [
    "The physiological background of sentence declination (fundamental frequency, F0, drop) was studied using ultrasound (US) examination of the cricothyroid (CT) space. The US probe was placed anteriorly in the region of middle cricothyroid ligament. The echoes caused by the antero-inferior edge of thyroid and antero-superior edge of the cricoid cartilages were used as points of measurement. The test utterances consisted of three- and five-word sentences. F0, sound level and CT space were measured from recordings. F0 declination and CT space widening showed a phase relationship. E.g., in a long sentence in which the F0 declined from 194 Hz to 85 Hz the CT space change was 4 mm (from 0.83 cm to 1.25 cm). The correlation between the F0 declination and CT space was r=-0.85. The main pitch regulating system connected with CT joint movements seems to contribute to sentence declination production. These biomechanical events can be monitored using the US method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-37"
  },
  "demolin97_eurospeech": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "M.",
     "George"
    ],
    [
     "V.",
     "Lecuit"
    ],
    [
     "T.",
     "Metens"
    ],
    [
     "A.",
     "Soquet"
    ],
    [
     "H.",
     "Raeymaekers"
    ]
   ],
   "title": "Coarticulation and articulatory compensations studied by dynamic MRI",
   "original": "e97_0043",
   "page_count": 4,
   "order": 38,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "This paper presents an ultra fast implementation of Turbo Spin Echo (TSE) to achieve continuous monitoring of the vocal tract with an actual time resolution of 4 images per second. We present preliminary results of two experiments involving coarticulation and articulatory compensations. articulations involved in speech production i.e. lips, tongue, larynx, lower jaw and velum.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-38"
  },
  "badin97_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Enrico",
     "Baricchi"
    ],
    [
     "Anne",
     "Vilain"
    ]
   ],
   "title": "Determining tongue articulation: from discrete fleshpoints to continuous shadow",
   "original": "e97_0047",
   "page_count": 4,
   "order": 39,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "The present study demonstrates the possibility to reconstruct complete midsagittal tongue shapes from the coordinates of three fleshpoints on the tongue and from the position of the jaw. The method is based on the inversion of an articulatory model made on the subject from cineradiographic images, and lead to an average reconstruction error of 1.26mm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-39"
  },
  "zissman97_eurospeech": {
   "authors": [
    [
     "Marc A.",
     "Zissman"
    ]
   ],
   "title": "Predicting, diagnosing and improving automatic language identification performance",
   "original": "e97_0051",
   "page_count": 4,
   "order": 40,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "Language-identification (LID) techniques that use multiple single-language phoneme recognizers followed by n-gram language models have consistently yielded top performance at NIST evaluations. In our study of such systems, we have recently cut our LID error rate by modeling the output of n-gram language models more carefully. Additionally, we are now able to produce meaningful confidence scores along with our LID hypotheses. Finally, we have developed some diagnostic measures that can predict performance of our LID algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-40"
  },
  "corredorardoy97_eurospeech": {
   "authors": [
    [
     "Cristobal",
     "Corredor-Ardoy"
    ],
    [
     "Jean Luc",
     "Gauvain"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Language identification with language-independent acoustic models",
   "original": "e97_0055",
   "page_count": 4,
   "order": 41,
   "p1": "55",
   "pn": "58",
   "abstract": [
    "In this paper we explore the use of language- independent acoustic models for language identification (LID). The phone sequence output by a single language-independent phone recognizer is rescored with language-dependent phonotactic models approximated by phone bigrams. The language-independent phoneme inventory was obtained by Agglomerative Hierarchical Clustering, using a measure of similarity between phones. This system is compared with a parallel language-dependent phone architecture, which uses optimally the acoustic log likelihood and the phonotactic score for language identiffication. Experiments were carried out on the 4-language telephone speech corpus IDEAL, containing calls in British English, Spanish, French and German. Results show that the language-independent approach performs as well as the language-dependent one: 9% versus 10% of error rate on 10 second chunks, for the 4-language task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-41"
  },
  "parris97_eurospeech": {
   "authors": [
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Harvey",
     "Lloyd-Thomas"
    ],
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Jerry H.",
     "Wright"
    ]
   ],
   "title": "Bayesian methods for language verification",
   "original": "e97_0059",
   "page_count": 4,
   "order": 42,
   "p1": "59",
   "pn": "62",
   "abstract": [
    "This paper describes a number of techniques for language verification based on acoustic processing and n-gram language modelling. A new technique is described which uses anti-models to model the general class of languages. These models are then used to normalise the acoustic score giving a 34% reduction in the error rate of the system. An approach to automatically generate discriminative subword strings for language verification is presented. The occurrence of recurrent strings are scored using a Poisson-based significance test. It is shown that when significant sub-strings do occur in the test material they are strong indicators of the target language occurring.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-42"
  },
  "kwan97_eurospeech": {
   "authors": [
    [
     "HingKeung",
     "Kwan"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Use of recurrent network for unknown language rejection in language identification system",
   "original": "e97_0063",
   "page_count": 4,
   "order": 43,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "In the past, we attempted to use a multilayer perceptron neural network as a means to prevent those unknown language inputs from being misidentified as one of the target languages in language identification system. However, the use of multilayer perceptron neural network could not utilize the temporal information from the utterances. Results show that with the use of phonemic unigram as input features to a recurrent neural network of Jordan architecture, a 3 target language identification rate of 98.1% can be achieved. By setting the output thresholds to 0.6 to reject 2 more unknown languages, a lower overall rate of 85.9% is obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-43"
  },
  "andersen97_eurospeech": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Language-identification based on cross-language acoustic models and optimised information combination",
   "original": "e97_0067",
   "page_count": 4,
   "order": 44,
   "p1": "67",
   "pn": "70",
   "abstract": [
    "This work is concerned with the subject of language- identification (LID). Two central issues are addressed. The first is to analyse the trade-off between detailed acoustic modelling and robust estimation of acoustic and language models. The second to find the optimal combination of acoustic and language scores for language identification.. Experiments are carried out using the three languages American-English, German and Spanish from the OGI-TS . database. It is shown that on the average the acoustic modelling is able to recognise 46.3% of the phones correctly across the three languages. Insertion and deletion rate is 35.7% and 6.6%, respectively. Language-identification performance is 82.6% with the full set of acoustic models. The performance is increased to 83.7% after having . conducted 80 iterations of a hierarchical clustering in which phones are merged across the languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-44"
  },
  "navratil97_eurospeech": {
   "authors": [
    [
     "Jiri",
     "Navratil"
    ],
    [
     "Werner",
     "Zühlke"
    ]
   ],
   "title": "Phonetic-context mapping in language identification",
   "original": "e97_0071",
   "page_count": 4,
   "order": 45,
   "p1": "71",
   "pn": "74",
   "abstract": [
    "This paper deals with the problem of exploiting information from a wide phonetic context for the purpose of language identiffication. Two approaches to language modeling are presented here: 1) modified bigrams with a con- text-mapping matrix and 2) language models based on binary decision trees. Both models were incorporated in a phonotactic language identiffier with a double-bigram decoding architecture and were shown to consistently improve the performance of standard bigrams. Measured on the NIST'95 evaluation set, the described system outperforms the state-of-the-art phonotactic components and is, at the same time, computationally less expensive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-45"
  },
  "rahim97_eurospeech": {
   "authors": [
    [
     "Mazin",
     "Rahim"
    ],
    [
     "Yoshua",
     "Bengio"
    ],
    [
     "Yann",
     "LeCun"
    ]
   ],
   "title": "Discriminative feature and model design for automatic speech recognition",
   "original": "e97_0075",
   "page_count": 4,
   "order": 46,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "A system for discriminative feature and model design is presented for automatic speech recognition. Training based on minimum classification error using a single objective function is applied for designing a set of parallel networks performing feature transformation and a set of hidden Markov models performing speech recognition. This paper compares the use of linear and non-linear functional transformations when applied to conventional recognition features, such as spectrum or cepstrum. It also provides a framework for integrated feature and model training when using class-specific transformations. Experimental results on telephone-based connected digit recognition are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-46"
  },
  "rottland97_eurospeech": {
   "authors": [
    [
     "Jörg",
     "Rottland"
    ],
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Large vocabulary speech recognition with context dependent MMI-connectionist / HMM systems using the WSJ database",
   "original": "e97_0079",
   "page_count": 4,
   "order": 47,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "In this paper we present a context dependent hybrid MMI-connectionist / Hidden Markov Model (HMM) speech recognition system for the Wall Street Journal (WSJ) database. The hybrid system is build with a neural network, which is used as a vector quantizer (VQ) and an HMM with discrete probablility density functions, which has the advantage of a faster decoding. The neural network is trained on an algorithm, that tries to maximize the mutual information between the classes of the input features (e.g. phones, triphones, etc.) and the neural firing sequence of the network. The system has been trained on the 1992 WSJ corpus (si-84). Tests were performed on the five- and twentythousand word, speaker independent (si_et) tasks. The error rates of a new context dependend neural network are 29% lower (relative) than the error rates of a standard (k-means) discrete system and the ratesare very close to the best continuous/semi-continuous HMM speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-47"
  },
  "moudenc97_eurospeech": {
   "authors": [
    [
     "Thierry",
     "Moudenc"
    ],
    [
     "Guy",
     "Mercier"
    ]
   ],
   "title": "Automatic selection of segmental acoustic parameters by means of neural-fuzzy networks for reordering the n-best HMM hypotheses",
   "original": "e97_0083",
   "page_count": 4,
   "order": 48,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "We present a neural fuzzy network architecture devoted to the recognition of specific segmental phonetic features.. A neural fuzzy network allows us to select the best acoustic parameters associated with eachfeature and to compute an phonetic segmental plausibility score. Segments result from the alignements provided by an allophone based Markov model. These segmental scores are then processed by a statistical post-processing system for reordering the N-best HMM hypotheses. This post-processing is based on the computation of segmental scores for each solution under the hypotheses of a correct solution and of an incorrect solution. Moreover, we present comparison results between these neural fuzzy network architecture and a classical one, on 3 speaker-independent telephone databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-48"
  },
  "kurimo97_eurospeech": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Comparison results for segmental training algorithms for mixture density HMMs",
   "original": "e97_0087",
   "page_count": 4,
   "order": 49,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "This work presents experiments on four segmental training algorithms for mixture density HMMs. The segmental versions of SOM and LVQ3 suggested by the author are compared against the conventional segmental K-means and the segmental GPD. The recognition task used as a test bench is the speaker dependent, but vocabulary independent automatic speech recognition. The output density function of each state in each model is a mixture of multivariate Gaussian densities. Neural network methods SOM and LVQ are applied to learn the parameters of the density models from the mel- cepstrum features of the training samples. The segmental training improves the segmentation and the model parameters by turns to obtain the best possible result, because the segmentation and the segment classification depend on each other. It suffices to start the training process by dividing the training samples approximatively into phoneme samples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-49"
  },
  "castano97_eurospeech": {
   "authors": [
    [
     "Asuncion",
     "Castano"
    ],
    [
     "Francisco",
     "Casacuberta"
    ]
   ],
   "title": "A connectionist approach to machine translation",
   "original": "e97_0091",
   "page_count": 4,
   "order": 50,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "Connectionist Models can be considered as an encouraging approach to Example-Based Machine Translation. However,] the neural translators developed in the literature are quite complex and require great human effort to classify and prepare training data. This paper presents an effective and more simple text-to-text connectionist translator with which translations from the source to the target language can be directly, automatically and successfully approached. The neural system, which is based on an Elman Simple Recurrent Network, was trained to tackle a simple pseudo-natural Machine Translation task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-50"
  },
  "pican97_eurospeech": {
   "authors": [
    [
     "Nicolas",
     "Pican"
    ],
    [
     "Jean-Francois",
     "Mari"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Continuous speech recognition using a context sensitive ANN and HMM2s",
   "original": "e97_0095",
   "page_count": 4,
   "order": 51,
   "p1": "95",
   "pn": "98",
   "abstract": [
    "The phonetic context has a large effect on phonemes in a continuous speech signal [1]. Therefore recognition systems that model allophones using context-dependent Hidden Markov Models have been implemented [4]. Second-order HMMs (HMM2s have a great ability for the segmentation in the temporal domain [6][7] but have some difficulties in the recognition because the MLE training (Maximum Likelihood Estimation) is not discriminant, whereas the discrimination is one of the abilities of the Artificial Neural Networks models. In the last three years we have developed a new ANN model named OWE (Orthogonal Weight Estimator)[10][11]. The principle of the OWE is a ANN that classifies an input pattern according to contextual environment. This new ANN architecture tackles the problem of context dependent behaviour training. Roughly, the principle is based on main MLP (Multilayered Perceptron) in which each synaptic weight connection value is estimated by another MLP (an OWE) with respect to context representation. In this paper, we present 2 hybrid systems for phoneme recognition. In both systems, 48 context independent HMM2s segment the input signal. In the first system, the OWE performs the labelling of segments and, in the second system, the OWE outputs are the input frames of the HMM2s. Experiments on TIMIT range from 56% to 67% accuracies on the 48 phonemes set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-51"
  },
  "shinoda97_eurospeech": {
   "authors": [
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Acoustic modeling based on the MDL principle for speech recognition",
   "original": "e97_0099",
   "page_count": 4,
   "order": 52,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "Recently context-dependent phone units, such as triphones, have been used to model subword units in speech recognition based on Hidden Markov Models (HMMs). While most such methods employ clustering of the HMM parameters(e.g., subword clustering, state clustering, etc.), to control HMM size so as to avoid poor recognition accuracy due to an insuffciency of training data, none of them provide any effective criterion for the optimal degree of clustering that should be performed. This paper proposes a method in which state clustering is accomplished by way of phonetic decision trees and in which the MDL criterion is used to optimize the degree of clustering. Large- vocabulary Japanese recognition experiments show that the models obtained by this method achieved the highest accuracy among the models of various sizes obtained with conventional clustering approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-52"
  },
  "modi97_eurospeech": {
   "authors": [
    [
     "Piyush",
     "Modi"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Discriminative utterance verification using multiple confidence measures",
   "original": "e97_0103",
   "page_count": 4,
   "order": 53,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "This paper proposes an utterance verification system for hidden Markov model (HMM) based automatic speech recognition systems. A verification objective function, based on a multi-layer-perceptron (MLP), is adopted which combines confidence measures from both the recognition and verification models. Discriminative minimum verification error training is applied for optimizing the parameters of the MLP and the verification models. Our proposed system provides a framework for combining different knowledge sources for utterance verification using an objective function that is consistently applied during both training and testing. Experimental results on telephone-based connected digits are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-53"
  },
  "bocchieri97_eurospeech": {
   "authors": [
    [
     "Enrico",
     "Bocchieri"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Subspace distribution clustering for continuous observation density hidden Markov models",
   "original": "e97_0107",
   "page_count": 4,
   "order": 54,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "This paper presents an efficient approximation of the Gaussian mixture state probability density functions of continuous observation density hidden Markov models (CHMM 's). In CHMM 's, the Gaussian mixtures carry a high computational cost, which amounts to a significant fraction (e.g. 30% to 70%) of the total computation. To achieve higher computation and memory efficiency, we approximate the Gaussian mixtures by (a) decomposition into functions defined on subspaces of the feature space, and (b) clustering the resulting subspace pdf's. Intuitively, when clustering in a subspace of few dimensions, even few function codewords can provide a small distortion. Therefore, we obtain significant reduction of the total computation (up to a factor of two), and memory savings (up to a factor of twelve), without significant changes of the CHMMM's accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-54"
  },
  "nock97_eurospeech": {
   "authors": [
    [
     "H. J.",
     "Nock"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "A comparative study of methods for phonetic decision-tree state clustering",
   "original": "e97_0111",
   "page_count": 4,
   "order": 55,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "Phonetic decision trees have been widely used for obtaining robust context-dependent models in HMM-based systems. There are five key issues to consider when constructing phonetic decision trees: the alignment of data with the chosen phone classes; the quality of the modelling of the underlying data; the choice of partitioning method at each node; the goodness-of-split criterion and the method for determining appropriate tree sizes. A popular existing method usesefficient but crude approximatemethods for each of these. This paper introduces and evaluates more detailed alternatives to the standard approximations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-55"
  },
  "kaltenmeier97_eurospeech": {
   "authors": [
    [
     "Alfred",
     "Kaltenmeier"
    ],
    [
     "Jürgen",
     "Franke"
    ]
   ],
   "title": "Comparing Gaussian and polynomial classification in SCHMM-based recognition systems",
   "original": "e97_0115",
   "page_count": 4,
   "order": 56,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "Semi-continuous Hidden Markov Models (SCHMM) with gaussian distributions are often used in continuous speech or handwriting recognition systems. Our paper compares gaussian and tree-structured polynomial classiffiers which have been successfully used in pattern recognition since many years. In our system the binary classiffier tree is generated by clustering HMM states using an entropy measure. For handwriting recognition, gaussians are clearly outperformed by polynomial classiffication. However, for speech recognition, polynomial classiffication currently performs slightly worse because some system parameters are not yet optimized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-56"
  },
  "girardi97_eurospeech": {
   "authors": [
    [
     "Alexandre",
     "Girardi"
    ],
    [
     "Harald",
     "Singer"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Maximum likelihood successive state splitting algorithm for tied-mixture HMNET",
   "original": "e97_0119",
   "page_count": 4,
   "order": 57,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "This paper describes a new approach to ML-SSS (Maximum Likelihood Successive State Splitting) algorithm that uses tied- mixture representation of the output probability density function instead of a single Gaussian during the splitting phase of the ML-SSS algorithm. The tied-mixture representation results in a better state split gain, because it is able to measure diferences in the phoneme environment space that ML-SSS can not. With this more informative gain the new algorithm can choose a better split state and corresponding data. Phoneme clustering experiments were conducted which lead up to 38% of error reduction if compared to the ML-SSS algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-57"
  },
  "mcdermott97_eurospeech": {
   "authors": [
    [
     "Erik",
     "McDermott"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "String-level MCE for continuous phoneme recognition",
   "original": "e97_0123",
   "page_count": 4,
   "order": 58,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "In this paper, we present results for the Minimum Classification Error (MCE) [1] framework for discriminative training applied to tasks in continuous phoneme recognition. The results obtained using MCE are compared with results for Maximum Likelihood Estimation (MLE). We examine the ability of MCE to attain high recognition performance with a small number of parameters. Phoneme-level and string-level MCE loss functions were used as the optimization criteria for a Prototype- Based Minimum Error Classifier (PBMEC) [2] and an HMM [3]. The former was optimized using Generalized Probabilistic Descent, the latter was optimized using an approximated second order method, the Quickprop algorithm. Two databases were used in this evaluation: 1) the ATR 5240 isolated word datasets for 6 speakers, in both speaker-dependent and multi-speaker mode; 2) the TIMIT database. For both databases, MCE training yielded striking gains in performance and classifier compactness compared to MLE baselines. For instance, through MCE training, performance similar to that of the Maximum Likelihood Successive State Splitting algorithm (ML-SSS) [4] could be obtained with 20 times fewer parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-58"
  },
  "rivlin97_eurospeech": {
   "authors": [
    [
     "Ze'ev",
     "Rivlin"
    ],
    [
     "Ananth",
     "Sankar"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "HMM state clustering across allophone class boundaries",
   "original": "e97_0127",
   "page_count": 4,
   "order": 59,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "We present a novel approach to hidden Markov model (HMM) state clustering based on the use of broad phone classes and an allophone class entropy measure. Most state-of-the-art large- vocabulary speech recognizers are based on context-dependent (CD) phone HMMs that use Gaussian mixture models for the state-conditioned observation densities. A common approach for robust HMM parameter estimation is to cluster HMM states where each state cluster shares a set of parameters such as the components of a Gaussian mixture model. In all the current state clustering algorithms, the HMM states are clustered only within their respective allophone classes. While this makes some intuitive sense, it prevents the clustering of states across allophone class boundaries, even when the states are acoustically similar. Our algorithm allows clustering across allophone class boundaries by defining broad phone groups within which two states from different allophone classes can be clustered together. An allophone class entropy measure is used to control the clustering of states belonging to different allophone classes. Experimental results on three test sets are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-59"
  },
  "mohri97_eurospeech": {
   "authors": [
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Weighted determinization and minimization for large vocabulary speech recognition",
   "original": "e97_0131",
   "page_count": 4,
   "order": 60,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "Speech recognition requires solving many space and time problems that can have a critical effect on the overall system performance. We describe the use of two general new algorithms [5] that transform recognition networks into equivalent ones that require much less time and space in large-vocabulary speech recognition. The new algorithms generalize classical automata determinization and minimization to deal properly with the probabilities of alternative hypotheses and with the relationships between units (distributions, phones, words) at different levels in the recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-60"
  },
  "phillips97_eurospeech": {
   "authors": [
    [
     "Steven",
     "Phillips"
    ],
    [
     "Anne",
     "Rogers"
    ]
   ],
   "title": "Parallel speech recognition",
   "original": "e97_0135",
   "page_count": 4,
   "order": 61,
   "p1": "135",
   "pn": "138",
   "abstract": [
    "Computer speech recognition has been very successful in limited domains and for isolated word recognition. However, widespread use of large-vocabulary continuous- speech recognizers is limited by the speed of current recognizers, which cannot reach acceptable error rates while running in real time. This paper shows how to harness shared memory multiprocessors, which are becoming increasingly common, to increase significantly the speed, and therefore the accuracy or vocabulary size, of a speech recognizer. We describe the parallelization of an existing high-quality speech recognizer, achieving a speedup of a factor of 3, 5 and 6 on 4, 8 and 12 processors respectively for the benchmark North American business news (NAB) recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-61"
  },
  "ortmanns97_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Ortmanns"
    ],
    [
     "Thorsten",
     "Firzlaff"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Fast likelihood computation methods for continuous mixture densities in large vocabulary speech recognition",
   "original": "e97_0139",
   "page_count": 4,
   "order": 62,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "This paper studies algorithms for reducing the computational effort of the mixture density calculations in HMM-based speech recognition systems. These likelihood calculations take about 70 total recognition time in the RWTH system for large vocabulary continuous speech recognition. To reduce the computational cost of the likelihood calculations, we investigate several space partitioning methods. A detailed comparison of these techniques is given on the North American Business Corpus (NAB'94) for a 20 000- word task. As a result, the so-called projection search algorithm in combination with the VQ method reduces the cost of likelihood computation by a factor of about 8 with no significant loss in the word recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-62"
  },
  "demuynck97_eurospeech": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Dirk van",
     "Compernolle"
    ]
   ],
   "title": "A static lexicon network representation for cross-word context dependent phones",
   "original": "e97_0143",
   "page_count": 4,
   "order": 63,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "To cope with the prohibitive growth of lexical tree based search-graphs when using cross-word context dependent (CD) phone models, an efficient novel search-topology was developed. The lexicon is stored as a compact static network with no language model (LM) information attached to it. The static representation avoids the cost of dynamic tree expansion, facilitates the integration of additional pronunciation information (e.g. assimilation rules) and is easier to integrate in existing search engines. Moreover, the network representation also results in a compact structure when words have alternative pronunciations, and due to its construction, it offers partial LM forwarding at no extra cost. Next, all knowledge sources (pronunciation information, language model and acoustic models) are combined by a slightly modified token-passing algorithm, resulting in a one pass time-synchronous recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-63"
  },
  "padmanabhan97_eurospeech": {
   "authors": [
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "L. R.",
     "Bahl"
    ],
    [
     "D.",
     "Nahamoo"
    ],
    [
     "Pieter de",
     "Souza"
    ]
   ],
   "title": "Decision-tree based quantization of the feature space of a speech recognizer",
   "original": "e97_0147",
   "page_count": 4,
   "order": 64,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "We present a decision-tree based procedure to quantize the feature-space of a speech recognizer, with the motivation of reducing the computation time required for evaluating gaussians in a speech recognition system. The entire feature space is quantized into non overlapping regions where each region is bounded by a number of hyperplanes. Further, each region is characterized by the occurence of only a small number of the total alphabet of allophones (sub-phonetic speech units); by identifying the region in which a test feature vector lies, only the gaussians that model the density of allophones that exist in that region need be evaluated. The quantization of the feature space is done in a heirarchical manner using a binary decision tree. Each node of the decision tree represents a region of the feature space, and is further characterized by a hyperplane (a vector v n and a scalar threshold value hn ), that subdivides the region corresponding to the current node into two non-overlapping regions corresponding to the two children of the current node. Given a test feature vector, the process of finding the region that it lies in involves traversing this binary decision tree, which is computationally inexpensive. We present results of experiments that show that the gaussian computation time can be reduced by as much as a factor of 20 with negligible degradation in accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-64"
  },
  "ravishankar97_eurospeech": {
   "authors": [
    [
     "Mosur",
     "Ravishankar"
    ],
    [
     "R.",
     "Bisiani"
    ],
    [
     "E.",
     "Thayer"
    ]
   ],
   "title": "Sub-vector clustering to improve memory and speed performance of acoustic likelihood computation",
   "original": "e97_0151",
   "page_count": 4,
   "order": 65,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "We describe a sub-vector clustering technique to reduce the memory size and computational cost of continuous density hidden Markov models (CHMMs). Acoustic models in modern large-vocabulary, continuous speech recognition systems are typically CHMMs. Systems with 100,000 Gaussian distributions of 40-60 dimensions are common, needing several tens of MB of memory. Computing HMM state likelihoods is several tens of times slower than real time. We show that by clustering and quantizing the Gaussian distributions a few dimensions at a time, both computation and memory costs can be reduced several fold without significant loss of recognition accuracy. On the 1994 Wall Street Journal 20K test set, this technique reduced the acoustic model size by a factor of 9-10, and HMM state output likelihood computation time by a factor of 4-5.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-65"
  },
  "hovell97_eurospeech": {
   "authors": [
    [
     "Simon",
     "Hovell"
    ]
   ],
   "title": "The incorporation of path merging in a dynamic network recogniser",
   "original": "e97_0155",
   "page_count": 4,
   "order": 66,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "In this paper, the incorporation of path merging within BT's dynamic speech recognition architecture[1] is discussed. One of the disadvantages of dynamic network generation is the size of the network generated. This is to a large extent due to the creation of many duplicate network portions. The use of a path merging strategy can redress this problem to some extent. This paper discusses the theory behind path merging, demonstrating a 22% speed improvement on a typical recognition task for no loss in top-N accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-66"
  },
  "novak97_eurospeech": {
   "authors": [
    [
     "Miroslav",
     "Novak"
    ]
   ],
   "title": "Improvement on connected digits recognition using duration constraints in the asynchronous decoding scheme",
   "original": "e97_0159",
   "page_count": 4,
   "order": 67,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "This paper describes the use of an explicit word duration model in the environment of a HMM based time asynchronous stack search decoder. The benefit of the method is demonstrated on the task of connected digit recognition. Analysis of typical errors observed on this task suggests that appropriate word duration modeling can improve recognition accuracy. Duration model based on the Gamma Distribution, applied as a post- processing step during iterations of the search algorithm, reduces the error rate of the baseline system by 14%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-67"
  },
  "stolcke97_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Yochai",
     "Konig"
    ],
    [
     "Mitchel",
     "Weintraub"
    ]
   ],
   "title": "Explicit word error minimization in n-best list rescoring",
   "original": "e97_0163",
   "page_count": 3,
   "order": 68,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "We show that the standard hypothesis scoring paradigm used in maximum-likelihood-based speech recognition systems is not optimal with regard to minimizing the word error rate, the commonly used performance metric in speech recognition. This can lead to sub- optimal performance, especially in high-error-rate environments where word error and sentence error are not necessarily monotonically related. To address this discrepancy, we developed a new algorithm that explicitly minimizes expected word error for recognition hypotheses. First, we approximate the posterior hypothesis probabilities using N-best lists. We then compute the expected word error for each hypothesis with respect to the posterior distribution, and choose the hypothesis with the lowest error. Experiments show improved recognition rates on two spontaneous speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-68"
  },
  "nguyen97_eurospeech": {
   "authors": [
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Efficient 2-pass n-best decoder",
   "original": "e97_0167",
   "page_count": 4,
   "order": 69,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "In this paper, we describe the new BBN BYBLOS ef- ficient 2-Pass N-Best decoder used for the 1996 Hub-4 Benchmark Tests. The decoder uses a quick fastmatch to determine the likely word endings. Then in the second pass, it performs a time-synchronous beam search using a detailed continuous-density HMM and a trigram language model to decide the word starting positions. From these word starts, the decoder, without looking at the input speech, constructs a trigram word lattice, and generates the top N likely hypotheses. This new 2-pass N-Best decoder maintains comparable recognition performance as the old 4-pass N-Best decoder, while its search strategy is simpler and much more efficient.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-69"
  },
  "iwasaki97_eurospeech": {
   "authors": [
    [
     "Tomohiro",
     "Iwasaki"
    ],
    [
     "Yoshiharu",
     "Abe"
    ]
   ],
   "title": "A memory management method for a large word network",
   "original": "e97_0171",
   "page_count": 4,
   "order": 70,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "To improve the performance of continuous speech recognition, it is effective to incorporate grammatical knowledge of task into a word network of a FSN (finite state network) form. But, recently , some of them requires huge memory, so we introduce an ef ficient memory management method for a large word network; distributed FSN model and hiearchical memory model. The system keeps the word network divided to small sub-networks, and activates each sub-network when necessary. Using this method, we can recognize continuously spoken sentences of Japanese addresses, which are made of 390K geographic names, with only 5.6 Mbytes local memory in average.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-70"
  },
  "romano97_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Romano"
    ]
   ],
   "title": "Persistence of prosodic features between dialectal and standard Italian utterances in six sub-varieties of a region of southern Italy (salento): first assessments of the results of a recognition test and an instrumental analysis",
   "original": "e97_0175",
   "page_count": 4,
   "order": 71,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "The aim of the work to be reported here is to attempt to verify that some dialectal varieties of a restricted area show sensible differences in their prosody and that normally the same differences characterise the prosodic system of speakers when performing sentences in Italian. To verify these hypothesis two kind of experiment were carried out: a perceptual recognition test based on sentences differing only by prosodic cues and uttered in Italian by different speakers of the region; a detailed phonetic inspection of the acoustical makeup to detect which cues are most likely to be responsible for the listener success in the recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-71"
  },
  "vereecken97_eurospeech": {
   "authors": [
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Annemie",
     "Vorstermans"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Bert van",
     "Coile"
    ]
   ],
   "title": "Improving the phonetic annotation by means of prosodic phrasing",
   "original": "e97_0179",
   "page_count": 4,
   "order": 72,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "It was established that the performance of our annotation system [8] is affected by the length of the utterances: the error rate, the CPU-load and the memory requirements tend to increase as the utterances get longer. In this contribution the speech signal is first segmented into speech, pauses and noise (breaths, clicks, : : :) and subsequently split in signal phrases prior to the annotation. Experiments on 3 different databases (3 languages) demonstrate that this stategy yields a significant improvement of the annotation accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-72"
  },
  "ode97_eurospeech": {
   "authors": [
    [
     "Cecilia",
     "Ode"
    ]
   ],
   "title": "A descriptive study of prosodic phenomena in Mpur (west Papuan Phylum)",
   "original": "e97_0183",
   "page_count": 4,
   "order": 73,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "A descriptive study of prosody in Mpur (West Papuan Phylum), an unwritten tone language with perceptually five tone contrasts, is presented, using the stylization method (see 1.). Three issues, observed at prosodic boundaries, are analysed and compared to their occurrence in other positions: 1) realization of tone; 2) vowel lengthening; 3) expression of emotive emphasis by means of repeated words, tail-head constructions, clitics and particles (2 and 3 frequently occur in the oral tradition of peoples of New Guinea). Results show that 1) level tones exhibit clearly audible pitch movements (falling or rising) at prosodic boundaries, sometimes with vowel lengthening; 2) vowels may be lengthened up to more than five times their original duration; 3) words may be repeated up to ten times without any change in the realization of tone; in tail-head constructions a reset (a jump upwards or downwards in the course of F0) may be observed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-73"
  },
  "mixdorff97_eurospeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Automated quantitative analysis of F0 contours of utterances from a German ToBI-labeled speech database",
   "original": "e97_0187",
   "page_count": 4,
   "order": 74,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "The present paper proposes a method for automating the analysis of F0 contours using the Fujisaki-model on ToBI-labeled speech data. ToBI-labels are used to preselect the number of necessary phrase and accent commands and align the onsets and o sets ofthese commands with the segments of the utterance. Local optimization is then performed with special regard to `reliable' portions of the F0 contour, for instance, the syllable nuclei. Analysis results are used for formulating quantitative F0 control rules for speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-74"
  },
  "tournemire97_eurospeech": {
   "authors": [
    [
     "Stéphanie de",
     "Tournemire"
    ]
   ],
   "title": "Identification and automatic generation of prosodic contours for a text-to-speech synthesis system in French",
   "original": "e97_0191",
   "page_count": 4,
   "order": 75,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "This paper presents the realisation of an automatically trainable computational prosodic model for French Text-to-Speech Synthesis. The methodology proposes the construction of the model in two steps. The first step consists in predicting fundamental frequency contours and duration of syllables from  prosodic markers using neural networks [17,12]. In this step, the  prosodic markers are automatically extracted from the signal by analysing prosodic realisations [2] and identifying a prosodic alphabet and a set of labelling rules. The second step integrates the model into the CNET Text-to-Speech Synthesis system [7] by using its linguistic levels and predicting  prosodic markers from text and linguistic labels. The system is evaluated by nadve listeners and compared with the actual CNET Text-to-Speech Synthesis system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-75"
  },
  "ni97_eurospeech": {
   "authors": [
    [
     "Jin-Fu",
     "Ni"
    ],
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Quantitative analysis and formulation of tone concatenation in Chinese F0 contours",
   "original": "e97_0195",
   "page_count": 4,
   "order": 76,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "With the aim of constructing a set of prosodic rules enabling to generate high-quality synthetic speech of Chinese, tone concatenation features were investigated for Chinese words. Using a superpositional model developed for Chinese F0 contours, quantitative analyses were conducted on 124 Chinese multi-syllable words to find out features on their F0 contours, especially the ones related to tone concatenation. A set of rules were then introduced for the control of model parameters to generate F0 contours of connected tones using the model. Comparison between F0 contours of natural utterances and those rule- generated for 340 words with various tone combinations showed the validity of the proposed rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-76"
  },
  "brindopke97_eurospeech": {
   "authors": [
    [
     "Christel",
     "Brindöpke"
    ],
    [
     "Arno",
     "Pahde"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "An environment for the labelling and testing of melodic aspects of speech",
   "original": "e97_0199",
   "page_count": 4,
   "order": 77,
   "p1": "199",
   "pn": "202",
   "abstract": [
    "In this paper, we present an environment for labelling and testing of melodic aspects of spoken language. The environment has three modes of application: First, the environment provides labelling facilities for a model-based melodic description for German. Second, it supports a language independent pre-theoretical description of speech melody allowing the development of new melodic categories. Third, our test bed can be used to generate speech samples with controlled melodic parameters for further use in perception experiments. The melodic description facilities (model-based, pre-theoretical) are supported by visual and audible feedback allowing a step-by-step refinement of the melodic description in question.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-77"
  },
  "casacuberta97_eurospeech": {
   "authors": [
    [
     "David",
     "Casacuberta"
    ],
    [
     "Lourdes",
     "Aguilar"
    ],
    [
     "Rafael",
     "Marin"
    ]
   ],
   "title": "PROPAUSE: a syntactico-prosodic system designed to assign pauses",
   "original": "e97_0203",
   "page_count": 5,
   "order": 78,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "In this study, a PROLOG-based computational tool designed to assign pauses in Spanish texts is proposed. Our purpose is to develop a prosodic segmentation algorithm suitable to be implemented in a text- to-speech system for Spanish. By means of the analysis of a corpus of read texts in Spanish, prosodic and syntactic factors guiding the location of orthographically unmarked pauses are identified. These factors are used to design a compu-tational model for assigning pauses in unrestricted texts. The performance of the system has been assessed by means of a comparison between its suggested segmentation and natural speech. The obtained results indicate that the system is able to capture empirical facts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-78"
  },
  "warnke97_eurospeech": {
   "authors": [
    [
     "Volker",
     "Warnke"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Integrated dialog act segmentation and classification using prosodic features and language models",
   "original": "e97_0207",
   "page_count": 4,
   "order": 79,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "This paper presents an integrated approach for the segmentation and classification of dialog acts (DA) in the Verbmobil project. In Verbmobil it is often suficient to recognize the sequence of DAs occurring during a dialog between the two partners. In our previous work [5] we segmented and classified a dialog in two steps: first we calculated hypotheses for the segment boundaries and decided for a boundary if the probabilities exceeded a predefined threshold level. Second we classified the segments into DAs using semantic classification trees or stochastic language models. In our new approach we integrate the segmentation and classification in the A- algorithm to search for the optimal segmentation and classifica tion of DAs on the basis of word hypotheses graphs (WHGs). The hypotheses for the segment boundaries are calculated with the help of a stochastic language model operating on the word chain and a multi-layer perceptron (MLP) classifying prosodic features. The DA classification is done using a category based language model for each DA. For our experiments we used data from the Verbmobil-corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-79"
  },
  "donzel97_eurospeech": {
   "authors": [
    [
     "Monique E. van",
     "Donzel"
    ],
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "Evaluation of prosodic characteristics in retold stories in Dutch by means of semantic scales",
   "original": "e97_0211",
   "page_count": 5,
   "order": 80,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "This paper describes an experiment in which listeners were asked to evaluate various prosodic aspects in retold stories in Dutch, using semantic scales. The aim was to see what features on prosodic level listeners prefer when listening to a retold story in Dutch, and if 'good' and 'bad' speakers can be distinguished in this respect. Results from a factor analysis show that listeners use Voice appreciation, Dynamics, and Articulation quality as main cues in evaluating the retold stories.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-80"
  },
  "bruce97_eurospeech": {
   "authors": [
    [
     "Gosta",
     "Bruce"
    ],
    [
     "Marcus",
     "Filipsson"
    ],
    [
     "Johan",
     "Frid"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "Merle",
     "Horne"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Text-to-intonation in spontaneous Swedish",
   "original": "e97_0215",
   "page_count": 4,
   "order": 81,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "This paper deals with a number of aspects of intonation in spontaneous dialogues in a language technology perspective. The key topics to be addressed are: I) the analysis of global intonation and its interaction with textual structure, II) the implementation of global and textual aspects of discourse intonation in an analysis-by-synthesis environment. We present models for the analyses of intonation and textual content in spontaneous conversations in Swedish. The models are implemented in a computational environment, making it possible to generate F0 contours, which can be imposed on a speech waveform using the PSOLA technique. The result is a text-to-intonation system, where textual and lexical analyses automatically generate hypothetical intonation contours, which can through resynthesis, and eventually be used in a text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-81"
  },
  "morlec97_eurospeech": {
   "authors": [
    [
     "Yann",
     "Morlec"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Véronique",
     "Auberge"
    ]
   ],
   "title": "Synthesising attitudes with global rhythmic and intonation contours",
   "original": "e97_0219",
   "page_count": 4,
   "order": 82,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "We present here a trainable generative model of French prosody. We focus on the sentence level and design SNNs able to generate both rhythmic and intonation contours for diverse attitudes. First results of a perceptual test show that listeners are able to retrieve the right definition of attitudes by listening to synthetic PSOLA stimuli.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-82"
  },
  "gibbon97_eurospeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Claudia",
     "Sassen"
    ]
   ],
   "title": "Prosody-particle pairs as discourse control signs",
   "original": "e97_0223",
   "page_count": 4,
   "order": 83,
   "p1": "223",
   "pn": "226",
   "abstract": [
    "We address the problem of integrating the description of discourse particles and their intonation into an HPSG--based lexicon for spontaneous speech applications, and propose a lexical sign type called prosody-- particle pair which has similar structure to grammatical inflexions and is formally described as a nested attribute-value structure. In our discussion we generalise a known class of `stylised intonations' to include the level intonation of hesitation particles. Previous descriptions of discourse particles, their roles and their relations to intonation, have been informal. Our proposal is the first to model particle-- intonation relations explicitly and in detail as an inflexion--like complex sign in a formal lexicon. The inclusion of the intonation of hesitation phenomena in the class of stylised intonations on formal and functional grounds is also new.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-83"
  },
  "elsner97_eurospeech": {
   "authors": [
    [
     "Anja",
     "Elsner"
    ]
   ],
   "title": "Focus detection with additional information of phrase boundaries and sentence mode",
   "original": "e97_0227",
   "page_count": 4,
   "order": 84,
   "p1": "227",
   "pn": "230",
   "abstract": [
    "In this paper an improved method for detection of focus accents is presented. The focus detection algorithm works with a rule-based approach. The main information source is the fundamental frequency F0 of an utterance. Results for the original version are 79 % recognition rate and 67 % average recognition rate for spontaneous speech. By integration of additional information like phrase boundaries and sentence mode, recognition rate increases by about 3 to 4 percent, depending on the dialogue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-84"
  },
  "bosch97_eurospeech": {
   "authors": [
    [
     "Laura",
     "Bosch"
    ],
    [
     "Nuria",
     "Sebastian-Galles"
    ]
   ],
   "title": "The role of prosody in infants' native-language discrimination abilities: the case of two phonologically close languages",
   "original": "e97_0231",
   "page_count": 4,
   "order": 85,
   "p1": "231",
   "pn": "234",
   "abstract": [
    "In this paper, the capacity of four-month-old infants from monolingual environments to distinguish between two syllable-timed languages is analysed. Catalan and Spanish are both Romance languages which present differences at the segmental level and at the syllable structure level, but show important similarities concerning prosodic structure at the phonological phrase level. Nevertheless, the presence of vowel reduction only in Catalan may determine rhythmic differences which could be detected by infants and used to tell these two languages apart. Two experiments have been run, with normal and low-pass filtered utterances, using a visual orientation procedure with a reaction time measure. Results indicate that infants are able to discriminate even when segmental information has been removed. The distinction seems to be the result of basic differential rhythmic properties between these two languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-85"
  },
  "buder97_eurospeech": {
   "authors": [
    [
     "Eugene H.",
     "Buder"
    ],
    [
     "Anders",
     "Eriksson"
    ]
   ],
   "title": "Prosodic cycles and interpersonal synchrony in American English and Swedish",
   "original": "e97_0235",
   "page_count": 4,
   "order": 86,
   "p1": "235",
   "pn": "238",
   "abstract": [
    "The paper addresses the question of rhythmic structuring of conversational interaction. Conversational speech requires active co- operation and co-ordination of the behavior of two or more speakers. Previous research indicates that one of the mechanisms used by speakers to regulate conversational interaction, is close monitoring and adaptation to rhythmic patterns. When this does not function properly, interaction may be adversely affected or even break down. There are reasons to believe that these mechanisms are used universally across languages, but there are also likely to be patterns that are language-specific. The research project, of which the present paper forms a first published report, is an attempt at separating the universal and language-specific aspects of the regulating rhythmic patterns. Although this research is primarily meant to clarify the mechanisms of conversational interaction from a linguistic/phonetic point of view, its applicability to speech technology is evident. Growing interest in dialogue systems for applications to man-machine communication demands more detailed data on all aspects of natural human conversation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-86"
  },
  "strangert97_eurospeech": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ]
   ],
   "title": "Relating prosody to syntax: boundary signalling in Swedish",
   "original": "e97_0239",
   "page_count": 4,
   "order": 87,
   "p1": "239",
   "pn": "242",
   "abstract": [
    "Two factors were experimentally varied in order to study their effects on silent interval and segment duration at NP-VP boundaries in Swedish sentences. These factors, the syntactic complexity of the NP and VP portions as well as the length of the words in the sentence both had significant effects on silent interval duration. Concerning word length, the general trend was an increase in silent interval duration, when longer words as compared to shorter ones preceded the boundary. Furthermore, silent interval duration increased, while preboundary segment duration decreased, when the NP complexity was increased. Moreover, there was a tendency to decreasing silence duration when the NP had the simplest structure, containing just a noun, and the VP increased in complexity. The same tendency was observed in the consonant preceding the boundary. This adjustment pattern, common to the silent interval and the final consonant, was assumed to occur in order to counteract imbalance in complexity between the NP and VP.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-87"
  },
  "nakai97_eurospeech": {
   "authors": [
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "On representation of fundamental frequency of speech for prosody analysis using reliability function",
   "original": "e97_0243",
   "page_count": 4,
   "order": 88,
   "p1": "243",
   "pn": "246",
   "abstract": [
    "This paper highlights on a method that provides a new prosodic feature called 'F0 reliability field' based on a reliability function of the fundamental frequency (F0 ). The proposed method does not employ any correction process for F0 estimation errors that occur during automatic F0 extraction. By applying this feature as a score function for prosodic analyses like prosodic structure estimation or superpositional modeling of prosodic commands, these prosodic information could be acquired with higher accuracy. The feature has been applied to 'F0 template matching method', which detects accent phrase boundaries in Japanese continuous speech. The experimental results show that compared to the conventional F0 contour, the proposed feature overcomes the harmful influence caused by F0 errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-88"
  },
  "kim97_eurospeech": {
   "authors": [
    [
     "Seong-Hwan",
     "Kim"
    ],
    [
     "Jin-Young",
     "Kim"
    ]
   ],
   "title": "Efficient method of establishing words tone dictionary for Korean TTS system",
   "original": "e97_0247",
   "page_count": 4,
   "order": 89,
   "p1": "247",
   "pn": "250",
   "abstract": [
    "In this paper, we propose an efficient method to establish Word Tone Dictionary(WTD). Vector qantization(VQ) is applied for compressing word tones for compressing word tones, and a phonetic-syntactic distance is adopted for searching the word tone dictionary. Because word tone is a sequence of syllable tones, VQ is used in encoding the syllable tones. As word tones in utterances are specified by their syntactic roles and phonetic features, we propose an adequate distance function to search the appropriate word tone in WTD. It is a combined distance function of syntactic distance and phonetic distance. We tested on a 100-utterance corpus. Preliminary experiments showed that the proposed method could lead to the natural pitch-controlled speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-89"
  },
  "dimperio97_eurospeech": {
   "authors": [
    [
     "Mariapaola",
     "D'Imperio"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Perception of questions and statements in Neapolitan Italian",
   "original": "e97_0251",
   "page_count": 4,
   "order": 90,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "This paper addresses the problem of the perception of two different pitch accents in Italian which signal two utterance types (interrogative and declarative). The questions asked concern whether the major perceptual cue to this category distinction involves only the temporal alignment of the high level target with the syllable or if the category percept also depends on the presence of a rising or falling melodic movement within the syllable nucleus. The results show that the primary perceptual cue for questions is a rise through the vowel, while the primary cue for statements is a fall through the vowel. The results bear upon a general theory of intonation and our understanding of intonation in Italian as well as on current models of tonal perception in speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-90"
  },
  "lin97_eurospeech": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "Dave",
     "Lubensky"
    ],
    [
     "Michael",
     "Picheny"
    ],
    [
     "P. Srinivasa",
     "Rao"
    ]
   ],
   "title": "Key-phrase spotting using an integrated language model of n-grams and finite-state grammar",
   "original": "e97_0255",
   "page_count": 4,
   "order": 91,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "This paper describes a new algorithm for key-phrase spotting applications. The algorithm consists of three processes. The first process is to synergistically integrate N-grams with Finite-State Grammars (FSG) -- the two conventional language models (LM) for speech recognition. All the key phrases to be spotted are covered by the FSG component of the recognizer's LM, while the N-grams are used for decoding surrounding non-key phrases. Secondly, selective weighting is proposed and implemented. The weighting parameters independently control the triggering and completion of FSG on top of N-grams. Finally, the third process involves a word confirmation and rejection logic which determines whether to accept or reject a hypothesized key phrase. The proposed algorithm has been favorably evaluated on two separate experiments. In these experiments, only the FSG part of the LM need be updated for different application tasks while the N-gram part can remain unchanged.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-91"
  },
  "junkawitsch97_eurospeech": {
   "authors": [
    [
     "Jochen",
     "Junkawitsch"
    ],
    [
     "Gunther",
     "Ruske"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Efficient methods for detecting keywords in continuous speech",
   "original": "e97_0259",
   "page_count": 4,
   "order": 92,
   "p1": "259",
   "pn": "262",
   "abstract": [
    "This paper refers to our prosperous development of algorithms for detecting keywords in continuous speech. Two different approaches to define confidence measures are introduced. As an advantage, these definitions are theoretically calculable without artful tuning. Moreover, two distinct decoding algorithms are presented, that incorporate these confidence measures into the search procedure. One is a new possibility of detecting keywords in continuous speech, using the standard Viterbi algorithm without modeling the non-keyword parts of the utterance. The other one is an improved further development of an algorithm described in [1], also without the need of modeling the non-keyword parts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-92"
  },
  "lau97_eurospeech": {
   "authors": [
    [
     "Raymond",
     "Lau"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Providing sublexical constraints for word spotting within the ANGIE framework",
   "original": "e97_0263",
   "page_count": 4,
   "order": 93,
   "p1": "263",
   "pn": "266",
   "abstract": [
    "We describe our recent work in implementing a word-spotting system based on the ANGIE framework and the effects of varying the nature of the sublexical constraints placed upon the word-spotter's filler model. ANGIE is a framework for modelling speech where the morphological and phonological substructures of words are jointly characterized by a context-free grammar and are represented in a multi-layered hierarchical structure. In this representation, the upper layers capture syllabification, morphology, and stress, the preterminal layer represents phonemics, and the bottom terminal categories are the phones. ANGIE provides a flexible framework where we can explore the effects of sublexical constraints within a word-spotting environment. Our experiments with spotting city names in ATIS validate the intuition that increasing the constraints present in the model improves performance, from 85.3 FOM for phone bigram to 89.3 FOM for a word lexicon. They also empirically strengthens our belief that ANGIE provides a feasible framework for various speech recognition tasks, of which word-spotting is one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-93"
  },
  "bartkova97_eurospeech": {
   "authors": [
    [
     "Katarina",
     "Bartkova"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Usefulness of phonetic parameters in a rejection procedure of an HMM-based speech recognition system",
   "original": "e97_0267",
   "page_count": 4,
   "order": 94,
   "p1": "267",
   "pn": "270",
   "abstract": [
    "The aim of this paper was to study the efficiency of sound duration, degree of sound voicing and sound energy in a rejection procedure of an automatic speech recognition system. A modelling of the three parameters was achieved using statistical models estimated on vocabulary words, out-of-vocabulary words and noise tokens. The rejection of out-of-vocabulary words and noises depended on the score obtained by comparing the probability given by the different models. However, such an approach also cause false rejection (rejection of vocabulary words). A trade-off was therefore necessary between the false rejection rate and the false alarm rate on out-of- vocabulary words and noise tokens. The degree of voicing turned out to be the most efficient parameter for rejecting noise tokens; it reduced the HMM false acceptance rate from 6.3% down to 2.3% for the same amount of false rejection rate (9%). The duration parameter provided better performance for laboratory data, reducing the error rate on French numbers from 3.1% to 1.5% for a 5% false rejection rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-94"
  },
  "yamashita97_eurospeech": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Keyword spotting using F0 contour matching",
   "original": "e97_0271",
   "page_count": 4,
   "order": 95,
   "p1": "271",
   "pn": "274",
   "abstract": [
    "This paper describes keyword spotting using prosodic information as well as phonemic information. A Japanese word has its own F0 contour based on the lexical accent type and the F0 contour is preserved in sentences. Prosodic dissimilarity between a keyword and input speech is measured by DP matching of F0 contours. Phonemic score is calculated by a conventional HMM technique. A total score based on these two measures is used for detecting keywords. The F0 contour of the keyword is smoothed by using an F0 model. Evaluation test was carried out on recorded speech of a TV news program. The introduction of prosodic information reduces false alarms by 30% or 50% for wide ranges of the detection rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-95"
  },
  "noth97_eurospeech": {
   "authors": [
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Volker",
     "Warnke"
    ]
   ],
   "title": "A frame and segment based approach for topic spotting",
   "original": "e97_0275",
   "page_count": 4,
   "order": 96,
   "p1": "275",
   "pn": "278",
   "abstract": [
    "In this paper we present a new approach for topic spotting based on subword units (phonemes and feature vectors) instead of words. Classification of topics is done by running topic dependent polygram language models over these symbol sequences and deciding for the one with the best score. We trained and tested the two methods on three different corpora. The first is a part of a media corpus which contains data from TV shows for three different topics (IDS), the second is part of the Switchboard corpus, the third is a collection of human machine dialogs about train timetable information (EVAR corpus). The results on Switchboard are compared with phoneme based approaches which were made at CRIM (Montreal) and DRA (Malvern) and are presented as ROC curves; the results on IDS and EVAR are compared with a word based approach and presented as confusion tables. We show that a surprisingly little amount of recognition accuracy is lost when going from word to subword based topic spotting.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-96"
  },
  "paliwal97_eurospeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Cyclic autocorrelation-based linear prediction analysis of speech",
   "original": "e97_0279",
   "page_count": 4,
   "order": 97,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "In this paper, a new approach for linear prediction (LP) analysis is proposed. This approach makes the assumption that the speech signal is cyclostationary and uses cyclic autocorrelation function for computing LP parameters. Since the cyclic autocorrelation function of a stationary random signal is zero, independent of its statistical description, this analysis is robust to additive noise, white or colored. It is applied to speech recognition. Preliminary results demonstrate its robustness to white additive noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-97"
  },
  "zeljkovic97_eurospeech": {
   "authors": [
    [
     "Ilija",
     "Zeljkovic"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Novel filler acoustic models for connected digit recognition",
   "original": "e97_0283",
   "page_count": 4,
   "order": 98,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "The context-dependent modeling technique is extended to include non-speech filler segments occurring between speech word units. In addition to the conventional context-dependent word or subword units, the proposed acoustic modeling provides an eficient way of accounting for the effects of the surrounding speech on the inter-word non-speech segments, especially for small vocabulary recognition tasks. It is argued that a robust recognition scheme is obtained by explicitly accounting for context-dependent inter-word filler acoustics in training while ignoring their explicit context dependencies during recognition testing. Results on a connected digit recognition task over the telephone network indicate an improvement in the error rate from 2.5% to 0.9% i.e., about 64% word error-rate reduction, using the improved model set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-98"
  },
  "shozakai97_eurospeech": {
   "authors": [
    [
     "Makoto",
     "Shozakai"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "A non-iterative model-adaptive e-CMN/PMC approach for speech recognition in car environments",
   "original": "e97_0287",
   "page_count": 4,
   "order": 99,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "This paper investigates the Cepstrum Mean Normalization(CMN) which has been widely acknowledged useful for compensation of multiplicative distortions. However, the performance of usual CMN is limited because the normalization by a single cepstrum mean vector is not enough to compensate many factors of multiplicative distortion in real environments. To solve this problem, a new method E-CMN is proposed. The method estimates two cepstrum mean vectors, one for speech and the other for non-speech for each speaker and subtracts them from an input cepstrum This method is capable of compensating various kinds of multiplicative distortion collectively to normalize input spectra. Furthermore, a new model-adaptive approach E-CMN/PMC, based on E- CMN and HMM composition, is proposed for environments with additive noise and multiplicative distortions. This method is simplified in a sense that it is possible to add speech models and an additive noise model without any iterative operations. Matching gains for all frequency bands of speech models to the noise model are uniquely estimated as a cepstrum mean vector for speech. The performance of E-CMN/PMC in adverse car environsnents is finally evaluated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-99"
  },
  "torre97_eurospeech": {
   "authors": [
    [
     "Angel de la",
     "Torre"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Pedro",
     "Garcia"
    ]
   ],
   "title": "Discriminative feature extraction for speech recognition in noise",
   "original": "e97_0291",
   "page_count": 4,
   "order": 100,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "Signal representation is crucial for designing a speech recognizer. The feature extractor selects the information to be used by the classifier to perform the recognition. In noisy environments, the data vectors representing the speech signal are changed and the recognizer performance is degraded by two main facts: (1) the mismatch between the training and the recognition conditions and (2) the degradation of the signal to be recognized. In such a situation, the representation of the speech signal plays an important role. In this paper, we analyze the importance of the representation for speech recognition in noise. We apply the Discriminative Feature Extraction (DFE) method to optimize the representation. The experiments presented in this work show that the DFE method, which has been successfully applied in clean environments, leads also to improvements of the speech recognizers in noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-100"
  },
  "brendborg97_eurospeech": {
   "authors": [
    [
     "Michael K.",
     "Brendborg"
    ],
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "Noise robust recognition using feature selective modeling",
   "original": "e97_0295",
   "page_count": 4,
   "order": 101,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "In automatic speech recognition (ASR) systems immunity to additive noise may either be applied at the preprocessing stage or at the pattern matching stage. The Feature Selective Modeling (FSM) approach suggested in this paper is applied in the pattern matching stage, but in contrast to most existing methods, it is optimized on a model basis such that noise robust and phonetically descriptive parameters of a particular model can be set in focus. For sonorant sounds this is done by marking the lowest n mean values of each HMM density function as being sensitive to noise in a log filterbank representation. The noise robustness is obtained by deemphasizing the marked feature dimensions. Two different methods for de-emphasizing - mean value masking and dimensional reduction - are presented and experimentally compared to the PMC-algorithm [2].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-101"
  },
  "abrash97_eurospeech": {
   "authors": [
    [
     "Victor",
     "Abrash"
    ]
   ],
   "title": "Mixture input transformations for adaptation of hybrid connectionist speech recognizers",
   "original": "e97_0299",
   "page_count": 4,
   "order": 102,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "We extend the input transformation approach for adapting hybrid connectionist speech recognizers to allow multiple transformations to be trained. Previous work has shown the efficacy of the linear input transformation approach for speaker adaptation [1][2][3], but has focused only on training global transformations. This approach is clearly suboptimal since it assumes that a single transformation is appropriate for every region in the acoustic feature input space, that is, for every phonetic class, microphone, and noise level. In this paper, we propose a new algorithm to train mixtures of transformation networks (MTNs) in the hybrid connectionist recognition framework. This approach is based on the idea of partitioning the acoustic feature space into R regions and training an input transformation for each region. The transformations are combined probabilistically according to the degree to which the acoustic features belong to each region, where the combination weights are derived from a separate acoustic gating network (AGN). We apply the new algorithm to nonnative speaker adaptation, and present recognition results for the 1994 WSJ Spoke 3 development set. The MTN technique can also be used for noise or microphone robust recognition or for other nonspeech neural network pattern recognition problems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-102"
  },
  "hwang97_eurospeech": {
   "authors": [
    [
     "Tai-Hwei",
     "Hwang"
    ],
    [
     "Lee-Min",
     "Lee"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Adaptation of time differentiated cepstrum for noisy speech recognition",
   "original": "e97_1075",
   "page_count": 4,
   "order": 103,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "In this paper, a noise compensation algorithm using the first order approximation of cepstral function is presented. The derivative term is replaced by the difference of cepstra for the adaptation of wide range variation of noise power. The differences of cepstral mean vectors between the clean and noisy version, termed as the deviation vector, are applied to adapt cepstrum and delta cepstrum. The experimental results show that using deviation vector to adapt the cepstral coefficients can gain a significant improvement over the method based on weighted projection measure. Further improvement can be made by jointly adapting the cepstrum and delta cepstrum.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-103"
  },
  "kanedera97_eurospeech": {
   "authors": [
    [
     "Noboru",
     "Kanedera"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Misha",
     "Pavel"
    ]
   ],
   "title": "On the importance of various modulation frequencies for speech recognition",
   "original": "e97_1079",
   "page_count": 4,
   "order": 104,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "Temporal processing of the time trajectories in the logarithmic spectrum domain, performed in cepstral mean subtraction, in computation of dynamic features in speech, or in RASTA processing, is becoming a common procedure in current ASR. Such temporal processing effectively enhances some components of the modulation spectrum of speech while suppressing others. It is therefore important to know the relative importance of various components of the modulation spectrum of speech. In this study we report on the effect of band-pass filtering of the time trajectories of spectral envelopes on speech recognition. Results indicate the relative importance of different components of the modulation spectrum of speech for ASR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-104"
  },
  "hong97_eurospeech": {
   "authors": [
    [
     "Wei-Tyng",
     "Hong"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "A robust RNN-based pre-classification for noisy Mandarin speech recognition",
   "original": "e97_1083",
   "page_count": 5,
   "order": 105,
   "p1": "1083",
   "pn": "1086",
   "abstract": [
    "This paper addressed the problem of speech signal pre-classification for robust noisy speech recognition. A novel RNN-based pre- classification scheme for noisy Mandarin speech recognition is proposed. The RNN, which is trained to be insensitive to noise-level variation, is employed to classify each input frame into the three broad classes of initial, final and pure-noise. An on-line noise tracking and estimation for noise model compensation is then performed. Besides, a broad-class likelihood compensation based on the RNN outputs is also performed to help the recognition. Experimental results showed that a significant improvement on syllable recognition rate has been achieved under non-stationary noise environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-105"
  },
  "rahim97b_eurospeech": {
   "authors": [
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "A parallel environment model (PEM) for speech recognition and adaptation",
   "original": "e97_1087",
   "page_count": 4,
   "order": 106,
   "p1": "1087",
   "pn": "1090",
   "abstract": [
    "A speech recognitionsystem for modelingan acoustic mismatch across different environments is presented. The basic philos- ophy is to apply discriminative learning techniques to sepa- rate the recognition process, that is represented by a hidden Markov model (HMM), from the environmental process which is denoted by a limited number of translation vectors. Each segment of speech is assigned to an environment and recogni- tion is performed upon projecting the parameters of the HMM to best characterize the acoustic space of that environment. The proposed system provides an interesting framework for better modeling and adaptation of speech signals with varying acoustic conditions. Experimental findings on connected digits recognition for three different environments are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-106"
  },
  "schless97_eurospeech": {
   "authors": [
    [
     "Volker",
     "Schless"
    ],
    [
     "Fritz",
     "Class"
    ]
   ],
   "title": "Adaptive model combination for robust speech recognition in car environments",
   "original": "e97_1091",
   "page_count": 4,
   "order": 107,
   "p1": "1091",
   "pn": "1094",
   "abstract": [
    "We present a new adaptive method for online noise estimation which extends the model combination approach to slowly varying noise conditions. The technique of model combination is reported to improve accuracy in speech recognition without extensive training of noisy speech data. Only training of noise characteristics is needed. However, if the noise characteristics vary over time, calculation of noise parameters once before recognition is not suitable. Therefore the new method of online estimation allows an adaptation to the current noise situation. Furthermore cepstral mean subtraction is added to the model combination scheme. This removes convolutional noise as well. Finally, it is shown how linear discriminant analysis eases handling of dynamical effects for model combination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-107"
  },
  "gerven97_eurospeech": {
   "authors": [
    [
     "Stefaan Van",
     "Gerven"
    ],
    [
     "Fei",
     "Xie"
    ]
   ],
   "title": "A comparative study of speech detection methods",
   "original": "e97_1095",
   "page_count": 4,
   "order": 108,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "This paper adresses the important problem of speech detection. It describes the implementation of 3 speech detection methods and compares their performance under different signal-to-noise ratio (SNR) and stationarity conditions. The method that dynamically adjusts its thresholds is found to be the most reliable, even under very adverse recording conditions. Yet it is of low complexity and has a very moderate processing delay.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-108"
  },
  "doukas97_eurospeech": {
   "authors": [
    [
     "Nikos",
     "Doukas"
    ],
    [
     "Patrick",
     "Naylor"
    ],
    [
     "Tania",
     "Stathaki"
    ]
   ],
   "title": "Voice activity detection using source separation techniques",
   "original": "e97_1099",
   "page_count": 4,
   "order": 109,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "A novel Voice Activity Detector is presented that is based on Source Separation techniques applied to single sensor signals. It offers very accurate estimation of the endpoints in very low Signal to Noise ratio conditions, while maintaining low complexity. Since the procedure is totally iterative, it is suitable for use in real-time applications and is capable of operating in dynamically adapting situations. Results are presented for both White Gaussian and Car Engine background noise. The performance of the new technique is compared with that of the GSM Voice Activity Detector.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-109"
  },
  "taniguchi97_eurospeech": {
   "authors": [
    [
     "Tomohiko",
     "Taniguchi"
    ],
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Voice activity detection using source separation techniques",
   "original": "e97_1103",
   "page_count": 4,
   "order": 110,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "Blind signal separation method based on minimiz- ing mutual information is applied to deal with multi- speaker problem in speech recognition. Recognition experiments performed under di erent acoustic environments, in a soundproof room and a reverberant room, clarify that 1) the method can improve recognition accuracy by about 20% where SNR condition is 0 dB, 2) the method is more effective when many speakers' speech exist than the simple overlapped situation, and that 3) the method does not work well under reverberant conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-110"
  },
  "avendano97_eurospeech": {
   "authors": [
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Sangita",
     "Tibrewala"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Multiresolution channel normalization for ASR in reverberant environments",
   "original": "e97_1107",
   "page_count": 4,
   "order": 111,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "To overcome the problems related with the long impulse responses produced by reverberation, we use a long time window (high frequency resolution) analysis during the channel normalization steps of the feature extraction process in automatic speech recognition (ASR). After normalization, a trade between frequency and time resolution is used to increase the rate at which the time information is sampled (short-time domain), yielding an appropriate domain to derive ASR features. Experiments on data with reverberation times of about 0.5 s show that the new technique achieves significant performance improvement of a speech recognizer under reverberation, with only some performance degradation on clean speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-111"
  },
  "martinez97_eurospeech": {
   "authors": [
    [
     "Rafael",
     "Martinez"
    ],
    [
     "Agustin",
     "Alvarez"
    ],
    [
     "Vilda Pedro",
     "Gomez"
    ],
    [
     "Mercedes",
     "Perez"
    ],
    [
     "Victor",
     "Nieto"
    ],
    [
     "Victoria",
     "Rodellar"
    ]
   ],
   "title": "A speech pre-processing technique for end-point detection in highly non-stationary environments",
   "original": "e97_1111",
   "page_count": 4,
   "order": 112,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "The determination of the precise moment in which speech begins or ends is an important problem in ASR. As showed in [1], small separations from the optimum beginning and ending point, imply a great decrease in the recognition accuracy. The presence of noise [2] [3], specially when its level is high (around 95 dB as in the case of this work), and its characteristics are highly non-stationary, is an added problem, since it can produce false shots (more probable when the noise includes speech sounds). That is the reason why in such conditions, it is important to have a pre-processing stage that removes as much noise as is possible, and that gives some clues that help to build an end-point detector for those environments. The method here presented offers a pre-processing technique for highly noisy and non stationary environments, which at the same time that enhances the speech, gives an equalised version of the SNR improvement (Mean Spectral Energy Difference), whose main characteristic is that large differences in the level of noise are changed to a little ripple, while the presence of speech is distinguished by a large decrease in this Mean Spectral Energy Difference. Following this technique, any End-point Detection approach (explicit, implicit or hybrid [3]) may render acceptable results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-112"
  },
  "dociofernandez97_eurospeech": {
   "authors": [
    [
     "Laura",
     "Docio-Fernandez"
    ],
    [
     "Carmen",
     "Garcia-Mateo"
    ]
   ],
   "title": "Application of several channel and noise compensation techiques for robust speaker recognition",
   "original": "e97_1115",
   "page_count": 4,
   "order": 113,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "This paper is concerned with the problem of Robust Speaker Recognition. An acoustical mismatch between training and testing conditions of hidden Markov model (HMM)-based speaker recognition systems often causes a severe degradation in the recognition performance. In telephone speaker recognition, for example, undesirable signal components due to ambient noise and channel distortion, as well as due to different variations of telephone handsets render the recognizer unusable for real-world applications. The purpose of this paper is to present several compensation techniques to decrease or to remove the mismatch between training and testing environment conditions. Some of the techniques described here have already been successfully applied in Robust Speech Recognition, and our preliminary results show that they are also very encouraging for Speaker Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-113"
  },
  "agaiby97_eurospeech": {
   "authors": [
    [
     "Hany",
     "Agaiby"
    ],
    [
     "Thomas J.",
     "Moir"
    ]
   ],
   "title": "Knowing the wheat from the weeds in noisy speech",
   "original": "e97_1119",
   "page_count": 4,
   "order": 114,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "This paper introduces a word boundary detection algorithm that works in a variety of noise conditions including what is commonly called the 'cocktail party' situation. The algorithm uses the direction of the signal as the main criterion for differentiating between desired-speech and background noise. To determine the signal direction the algorithm calculates estimates of the time delay between signals received at two microphones. These time delay estimates together with estimates of the coherence function and signal energy are used to locate word boundaries. The algorithm was tested using speech embedded in different types and levels of noise including car noise, factory noise, babble noise, and competing talkers. The test results showed that the algorithm performs very well under adverse conditions and with SNR down to -14.5dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-114"
  },
  "kim97b_eurospeech": {
   "authors": [
    [
     "Do Yeong",
     "Kim"
    ],
    [
     "Nam Soo",
     "Kim"
    ],
    [
     "Chong Kwan",
     "Un"
    ]
   ],
   "title": "Model-based approach for robust speech recognition in noisy environements with multiple noise sources",
   "original": "e97_1123",
   "page_count": 4,
   "order": 115,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "In this paper, we consider the hidden Markov model(HMM) parameter compensation in noisy environments with multiple noise sources based on the vector Taylor series (VTS) approach. General formulations for multiple environmental variables are derived and systematic expectation-maximization (EM) solutions are presented in maximum likelihood (ML) sense. It is assumed that each noise source is independent and having Gaussian distribution. To evaluate proposed method, we conduct speaker independent isolated word recognition experiments in various noisy environments. Experimental results show that proposed algorithm ahieves significant improvement. Especially, the proposed method is consistently more effective than the parallel model combination (PMC) based on log-normal approximation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-115"
  },
  "chu97_eurospeech": {
   "authors": [
    [
     "Y.C.",
     "Chu"
    ],
    [
     "Charlie",
     "Jie"
    ],
    [
     "Vincent",
     "Tung"
    ],
    [
     "Ben",
     "Lin"
    ],
    [
     "Richard",
     "Lee"
    ]
   ],
   "title": "Normalization of speaker variability by spectrum warping for robust speech recognition",
   "original": "e97_1127",
   "page_count": 4,
   "order": 116,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "This paper examines techniques for normalization of unseen speakers in recognition. Two implementations of linear spectrum warping were examined: time domain resampling and filter bank scaling. It is shown that for seen speakers, the models trained by unwarped utterances are less sensitive to spectrum warping by filter bank scaling than by resampling. A pitch-based scheme for warping factor estimation has been proposed. The method is shown to be cost-effective in reducing the variability of unseen speakers compared to the ML-based methods. In particular the combination of filter bank scaling with the pitch- based warping factor estimation reduces the error rate of isolated Mandarin digit recognition by more than 30% for unseen speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-116"
  },
  "maes97_eurospeech": {
   "authors": [
    [
     "Stephane H.",
     "Maes"
    ]
   ],
   "title": "LPC poles tracker for music/speech/noise segmentation and music cancellation",
   "original": "e97_1131",
   "page_count": 4,
   "order": 117,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "In automatic speech recognition (ASR) of broadcast news shows the input utterances are often corrupted by background music and noise. This paper proposes a new method of au- tomatic segmentation a speech signals according to the back- ground: music, clean or noisy. LPC analysis is used to extract the poles of the associated transfer function. Based on the time evolution of the poles it is possible to discriminate the contributions of music, speech and noise: music poles are sta- bler longer than speech poles while noise poles have a more unstable behavior than speech poles. Once the background of a signal is identified, poles tagged as non-speech can be sep- arated from speech poles. Using only the speech poles along with the LPC residuals, it is possible to reconstruct a new signal freed of music and noise contributions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-117"
  },
  "kim97c_eurospeech": {
   "authors": [
    [
     "Doh-Suk",
     "Kim"
    ],
    [
     "Jae-Hoon",
     "Jeong"
    ],
    [
     "Soo-Young",
     "Le"
    ],
    [
     "Rhee M.",
     "Kil"
    ]
   ],
   "title": "Comparative evaluations of several front-ends for robust speech recognition",
   "original": "e97_1135",
   "page_count": 4,
   "order": 118,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "Zero-crossings with peak amplitudes (ZCPA) model motivated by human auditory periphery is simple com- pared with other auditory models, but powerful speech analysis tool for robust speech recognition in noisy environments. In this paper, improvement in recog- nition rate of ZCPA model is addressed by incorpo- rating time-derivative features with several different time-derivative window lengths. Experimental results show that ZCPA has relatively higher sensitivity to derivative window length than conventional feature extraction algorithms. Also, experimental compar- isons with several front-ends including some auditory- like schemes in real-world noisy environments demon- strate the robustness of ZCPA model. ZCPA model shows superior performance compared with other front- ends especially in noisy condition corrupted by white Gaussian noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-118"
  },
  "gouvea97_eurospeech": {
   "authors": [
    [
     "Evandro B.",
     "Gouvea"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Speaker normalization through formant-based warping of the frequency scale",
   "original": "e97_1139",
   "page_count": 4,
   "order": 119,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "Speaker-dependent automatic speech recognition systems are known to outperform speaker-independent systems when enough training data are available to model acoustical variability among speakers. Speaker normalization techniques modify the spectral representation of incoming speech waveforms in an attempt to reduce variability between speakers. Recent successful speaker normalization algorithms have incorporated a speaker-specific frequency warping to the initial signal processing stages. These algorithms, however, do not make extensive use of acoustic features contained in the incoming speech. In this paper we study the possible benefits of the use of acoustic features in speaker normalization algorithms using frequency warping. We study the extent to which the use of such features, including specifically the use of formant frequencies, can improve recognition accuracy and reduce computational complexity for speaker normalization. We examine the characteristics and limitations of several types of feature sets and warping functions as we compare their performance relative to existing algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-119"
  },
  "westphal97_eurospeech": {
   "authors": [
    [
     "Martin",
     "Westphal"
    ]
   ],
   "title": "The use of cepstral means in conversational speech recognition",
   "original": "e97_1143",
   "page_count": 4,
   "order": 120,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "Environmental robustness and speaker independence are import issues of current speech recognition research. Channel and speaker adaptation methods do the best job when the adaption is done towards a normalized acoustic model. Normalization methods might make use of the model but primarily inuence the signal such that important information is kept and unwanted distortions are cancelled out. Most large vocabulary conversational speech recognition systems use Cepstral Mean Subtraction (CMS), a channel normalization approach to compensate for the acoustic channel (and also the speaker). In this paper we discuss the basic algorithm and variations of it in the context of conversational speech and report our experience using different approaches on two widely used conversational speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-120"
  },
  "huerta97_eurospeech": {
   "authors": [
    [
     "Juan M.",
     "Huerta"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Compensation for environmental and speaker variability by normalization of pole locations",
   "original": "e97_1147",
   "page_count": 4,
   "order": 121,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "We present a compensation technique that corrects for the effects of noise and variability of speaker and environment on speech recognition accuracy by modifying the positions of the poles representing the speech signal in the z-plane. This modification yields pole locations with statistics that more closely match the statistics of the distribution of clean training speech. The parameters of the mapping are obtained from statistics of the distribution of the poles of the training and testing speech. Compensation is performed by direct modification of both the angle and the radius of pole locations, and also by evaluating the cepstrum along a cirele of radius less than 1 in the z-plane to enhance the salience of spectral peaks. These procedures are evaluated using the DARPA Resource Management database using added white noise. They are shown to compensate for the effects of environmental degradation, patvcularly at low SNRs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-121"
  },
  "puel97_eurospeech": {
   "authors": [
    [
     "Jean-Baptiste",
     "Puel"
    ],
    [
     "Régine",
     "André-Obrecht"
    ]
   ],
   "title": "Cellular phone speech recognition: noise compensation vs. robust architectures",
   "original": "e97_1151",
   "page_count": 4,
   "order": 122,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "This paper addresses the problem of speech recognition through telephonic networks. When the communication channel is unknown, the important mismatch between training data and signal encountered in recognition phase decreases drastically the performances of the recognition systems. In this context, we compare a classical approach: the noise compensation method with novel robust networks modellings aiming to incorporate and manage more variability in the training data. We introduce multi-HMMs and multi-transitions systems, trained with data recorded through analog switched network and cellular phone network. These architectures present best results and succeed in improving the recognizers robustness since they achieve up to 77 % reduction of the error rate for a system trained for switched telephonic network and used with cellular phone. Nevertheless, this modelling requires training data recorded in both environments; when such data are not available, noise cancellation or channel compensation are the only affordable solutions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-122"
  },
  "chiang97_eurospeech": {
   "authors": [
    [
     "Tung-Hui",
     "Chiang"
    ]
   ],
   "title": "Speech recognition in noise using on-line HMM adaptation",
   "original": "e97_1155",
   "page_count": 4,
   "order": 123,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "In this paper, a novel two-stage framework is proposed to copy with speech recognition in adverse environment. First, an on-line HMM composition method which compensates HMMs making use of the on-line testing utterances is proposed in the first stage. By using the proposed method, the dynamic change of environmental noise in each utterance can be well handled. In addition, a classifier trained by using a discriminative learning procedure is incorporated in the second stage to enhance system's discrimination capability. Since the recognition and adaptation processes are carried out in the same session in an unsupervised fashion, this proposed two-stage framework is suitable for practical uses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-123"
  },
  "malliopoulos97_eurospeech": {
   "authors": [
    [
     "Christos",
     "Malliopoulos"
    ],
    [
     "George",
     "Mikros"
    ]
   ],
   "title": "Metrical representations of demarcation and constituency in noun phrases",
   "original": "e97_0303",
   "page_count": 4,
   "order": 124,
   "p1": "303",
   "pn": "306",
   "abstract": [
    "This paper reports on the results of two groups of experiments conducted in order to examine the melodic correlates of demarcation and constituency in subordination and coordination structures. Experimental material were noun phrases of the form \"A of B of C of D\" and \"A, B, C and D\" followed by a short VP. A, B, C and D were noun groups like \"article + noun\" or \"pronoun + noun\" or \"pronoun + adjective + noun\". The relation between grammatical structure and pitch is examined in terms of both phonological interpretation and phonetic data. Also a phonological model of intonation is proposed bearing on the results of the experimental material.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-124"
  },
  "pirker97_eurospeech": {
   "authors": [
    [
     "Hannes",
     "Pirker"
    ],
    [
     "Kai",
     "Alter"
    ],
    [
     "Erhard",
     "Rank"
    ],
    [
     "John",
     "Matiasek"
    ],
    [
     "Harald",
     "Trost"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "A system of stylized intonation contours in German",
   "original": "e97_0307",
   "page_count": 4,
   "order": 125,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "Modeling intonation, i.e., specifying adequate fundamental frequency (F0) contours, remains a challenging task for speech synthesis systems. This paper discusses the development of a system for phonetically specifying intonation contours for German. It deals with the problem of translating an  phonological representation of intonation - namely the tone-sequence model - into a concrete phonetic model. Design options and evaluation methods are discussed\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-125"
  },
  "hirose97_eurospeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Kouji",
     "Iwano"
    ]
   ],
   "title": "A method of representing fundamental frequency contours of Japanese using statistical models of moraic transition",
   "original": "e97_0311",
   "page_count": 4,
   "order": 126,
   "p1": "311",
   "pn": "314",
   "abstract": [
    "A statistical modeling of voice fundamental frequency contours was proposed for the purpose of developing effective ways to utilize prosodic features in speech recognition. In view of the fact that prosodic features should be treated in longer units, the proposed modeling represents the transition in moraic units. A fundamental frequency contour was first segmented into moraic units and then each moraic contour was represented by a code depending on the shape. After modeling fundamental frequency contours for the portions of several morae around boundaries in question based on HMM scheme, experiments on syntactic boundary detection were conducted. Detection rate reached to 89.2 % for the closed condition experiment and was around 85 % for the open (speaker and topic) condition experiment. Experiments on accent type recognition were also conducted yielding around 74 % of correct recognition for the speaker independent cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-126"
  },
  "fotinea97_eurospeech": {
   "authors": [
    [
     "Evita F.",
     "Fotinea"
    ],
    [
     "Michael A.",
     "Vlahakis"
    ],
    [
     "George V.",
     "Carayannis"
    ]
   ],
   "title": "Modeling arbitrarily long sentence-Spanning F0 contours by parametric concatenation of word-Spanning patterns",
   "original": "e97_0315",
   "page_count": 4,
   "order": 127,
   "p1": "315",
   "pn": "318",
   "abstract": [
    "Modeling F0 contours of arbitrarily long and complex sentences of the Greek language may prove to be a difficult task if one considers the various parameters involved, namely focus, position of the prominent vowel within words, syntactic structure and type of expression. None the less, this complexity may be significantly reduced if the expressive requirements of the application area in mind are taken into account. Study of the expressive requirements of the information broadcasting applications revealed that the affirmative type of expression is heavily used and regardless of size and complexity, each sentence-spanning contour may be co-mposed of only four word-spanning patterns. This result not only leads to significant savings in the resources required for a natural sounding speech output but also indicates a highly structured intonative component.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-127"
  },
  "son97_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Strong interaction between factors influencing consonant duration",
   "original": "e97_0319",
   "page_count": 4,
   "order": 128,
   "p1": "319",
   "pn": "322",
   "abstract": [
    "Interactions between factors affecting consonant duration are well known. It has proved difficult to quantify these interactions. The difficulty lies in the enormous amount of speech necessary to resolve all factor combinations and their uneven distribution in speech, i.e., factor confounding. Assuming piecewise independence of factor combinations and an additive duration model, it is possible to reconstruct \"balanced\" mean durations from unbalanced data. Analysis of a corpus of read speech from two speakers allowed us to model the interaction between syllable stress, position in the word, and consonant identity. The strong interactions could be attributed to a \"floor\" in the shortest durations and irregular behavior of Coronal consonants. The distribution of durations of Coronal consonants is linked to a shift to ballistic articulation, i.e., flaps, in reducing circumstances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-128"
  },
  "gros97_eurospeech": {
   "authors": [
    [
     "Jerneja",
     "Gros"
    ],
    [
     "Nikola",
     "Pavesic"
    ],
    [
     "France",
     "Mihelic"
    ]
   ],
   "title": "Speech timing in Slovenian TTS",
   "original": "e97_0323",
   "page_count": 4,
   "order": 129,
   "p1": "323",
   "pn": "326",
   "abstract": [
    "Speech timing at different speaking rates was studied for the Slovenian language and the results were applied for duration modelling in the Slovenian text-to-speech system S5 [1]. In order to enable the synthesiser to pronounce input text with several speaking rates, tests were made to study the impact of speaking rate on syllable duration and duration of individual phonemes and phoneme groups for the Slovenian language [2]. A two-level approach to durational modelling is described. A method for segment duration prediction was developed, which adapts a word with an intrinsic duration to the desired extrinsic duration, taking into account how stretching and squeezing apply to duration of individual segments.\n",
    "s J. Gros, N. Pavesic, F. Mihelic, \"A text-to-speech system for the Slovenian language\", Proc. EUSIPCO'96, Trieste, pp. 1043-1046, 1996.\n",
    "J. Gros, N. Pavesic, F. Mihelic, \"Syllable and segment duration at different speaking rates in the Slovenian language\", Proc. EUROSPEECH'97, Rhodes, 1997.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-129"
  },
  "dorbecker97_eurospeech": {
   "authors": [
    [
     "Matthias",
     "Dorbecker"
    ]
   ],
   "title": "Small microphone arrays with optimized directivity for speech enhancement",
   "original": "e97_0327",
   "page_count": 4,
   "order": 130,
   "p1": "327",
   "pn": "330",
   "abstract": [
    "In many situations of digital speech communication (e.g. hands-free telephony or electronic hearing aids) the speech signal picked up by the microphone is disturbed by acoustic background noise. Therefore, adaptive filtering techniques which aim at the reduction of the disturbing noise are subject of current research activities (e.g. [1]). Although some of the already known adaptive techniques -- especially concepts with two or more microphones -- allow a significant reduction of the noise, most of the adaptive strategies result, particularly at low SNR, in a speech signal with an unnatural character due to time-variant distortions, and the occurrence of musical noise. An alternative approach, which does not affect the speech signal by time-variant distortions, is the application of a microphone array with a fixed directivity pattern aligned to the speaker's position, resulting in a suppression of spatially distributed noise sources. In this contribution it is shown that due to the proposed optimization even with only two microphones a reduction of diffuse noise sound up to 6 dB can be achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-130"
  },
  "inoue97_eurospeech": {
   "authors": [
    [
     "Masaaki",
     "Inoue"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Microphone array design measures for hands-free speech recognition",
   "original": "e97_0331",
   "page_count": 4,
   "order": 131,
   "p1": "331",
   "pn": "334",
   "abstract": [
    "One of the key technologies for natural man-machine interface is hands-free speech recognition. The performance of hands-free distant- talking speech recognition will be seriously degraded by noise and reverberation in real environments. A microphone array is applied to solve the problem. When applying a microphone array to speech recognition, parameters such as number of microphone elements and their spacing interval affect the performance. In order to optimize these parameters, a measure which reflects recognition performance is needed. In this paper, we investigate a measure of a microphone array design for speech recognition through experiments using various kinds of a microphone array design.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-131"
  },
  "akagi97_eurospeech": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Mitsunori",
     "Mizumachi"
    ]
   ],
   "title": "Noise reduction by paired microphones",
   "original": "e97_0335",
   "page_count": 4,
   "order": 132,
   "p1": "335",
   "pn": "338",
   "abstract": [
    "This paper proposes a front-end method for enhancing the target signal by subtracting estimated noise from a noisy signal using paired microphones, assuming that the noise is unevenly distributed with regard to time, frequency, and the direction. Although the Griffiths- Jim type adaptive beamformer has been proposed using the same concept, this method has some drawbacks. For example, sudden noises cannot be reduced because the convergence speed of the adaptive filter is slow; also the signal is distorted in a reverberated environment. The proposed method, however, can over-come the above drawbacks by formulating noises using arrival time differences between paired microphones and by estimating noises analytically using the directions of the noises. The simulated results show that the method with one paired microphone can increase signal-to-noise ratios (SNR) by 10 ~ 20 dB in simulations and can reduce log-spectrum distances by about 5 dB in real noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-132"
  },
  "mahmoudi97_eurospeech": {
   "authors": [
    [
     "Djamila",
     "Mahmoudi"
    ]
   ],
   "title": "A microphone array for speech enhancement using multiresolution wavelet transform",
   "original": "e97_0339",
   "page_count": 4,
   "order": 133,
   "p1": "339",
   "pn": "342",
   "abstract": [
    "This paper addresses the problem of enhancing a speech signal corrupted by interfering signals. A new noise reduction algorithm based on a logarithmic microphone array and the multiresolution wavelet transform is described. The proposed processing is applied in the time-spectral domain with respect to the logarithmic subband decomposition of the spectrum of each microphone signal. The advantage of the proposed method is that both the sub-array based beamforming operation and the postfiltering are performed in the same transform domain without adding FFT processing. Computer simulation results show that our approach is effective for noise reduction. The technique can be used in hands-free voice communication applications operating in an adverse environment. In particular, it can be applied to improve speech signal pick-up for voice communication terminal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-133"
  },
  "nagata97_eurospeech": {
   "authors": [
    [
     "Yoshifumi",
     "Nagata"
    ],
    [
     "Hiroyuki",
     "Tsuboi"
    ]
   ],
   "title": "A two-channel adaptive microphone array with target tracking",
   "original": "e97_0343",
   "page_count": 4,
   "order": 134,
   "p1": "343",
   "pn": "346",
   "abstract": [
    "This paper proposes a new robust adaptive beamformer suitable for a two-channel microphone array. The proposed beamformer is a combination of two Griffith-Jim generalized sidelobe cancellers (GSC) in which the look direction of each GSC is determined by the other GSC's directional response. Moreover, to reduce the degradation of interference suppression performance caused by spatial aliasing, a new arrangement of directional microphones is proposed. The arrangement is simple and effectively reduces the degradation. Simulations demonstrate the effectiveness of the proposed two- channel two-beamformer microphone array.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-134"
  },
  "giuliani97_eurospeech": {
   "authors": [
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Marco",
     "Matassoni"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "Piergiorgio",
     "Svaizer"
    ]
   ],
   "title": "Use of different microphone array configurations for hands-free speech recognition in noisy and reverberant environment",
   "original": "e97_0347",
   "page_count": 4,
   "order": 135,
   "p1": "347",
   "pn": "350",
   "abstract": [
    "In this work hands-free continuous speech recognition based on microphone arrays is investigated. A set of experiments was carried out using arrays having different numbers of omnidirectional microphones as well as different configurations. Both real and simulated array signals, generated by means of the image method, were used. An enhanced input to a recognizer based on Hidden Markov Models was obtained by a time delay compensation module providing a beamformed signal. HMM adaptation was used to realign the recognizer acoustic modeling to the given acoustic condition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-135"
  },
  "wang97_eurospeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Joe",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "YINHE: a Mandarin Chinese version of the GALAXY system",
   "original": "e97_0351",
   "page_count": 4,
   "order": 136,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "The galaxy system is a human-computer conversational system providing a spoken language interface for accessing on-line information. It was initially implemented for English in travel-related domains, including air travel, local city navigation, and weather. We began an effort to develop multilingual systems within the framework of galaxy several years ago. This paper describes our recent work on porting the system to Mandarin Chinese, including speech recognition, language understanding, and language generation components. Overall, the system produced reasonable responses nearly 70% of the time for spontaneous test data collected in a wizard environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-136"
  },
  "bonaventura97_eurospeech": {
   "authors": [
    [
     "Patrizia",
     "Bonaventura"
    ],
    [
     "Filippo",
     "Gallocchio"
    ],
    [
     "Giorgio",
     "Micca"
    ]
   ],
   "title": "Multilingual speech recognition for flexible vocabularies",
   "original": "e97_0355",
   "page_count": 4,
   "order": 137,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "The paper addresses the problem of designing a speech recogniser for multilingual vocabularies. The goal of the research is twofold: future Interactive Voice Recognition (IVR) systems, like a speech activated flight information service, are likely to require multilinguality as a major feature; besides, a general language-independent phonetic inventory might be very useful in bootstrapping phonetic models for a new language for which insufficient training data are available. Metrics were introduced in order to measure cross-language phonetic dissimilarities, and a multilingual phonemic inventory was created. Experiments were run on a speech database including Italian (I), Spanish (S), English (E) and German (G) words. Results clearly show that it is possible to reduce the complexity of a multilingual phonetic recogniser by exploiting phonetic commonalities across different languages, without significant losses in WA for multilingual tasks with respect to single language recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-137"
  },
  "weng97_eurospeech": {
   "authors": [
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "A study of multilingual speech recognition",
   "original": "e97_0359",
   "page_count": 4,
   "order": 138,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "This paper describes our work in developing multilingual (Swedish and English) speech recognition systems in the ATIS domain. The acoustic component of the multilingual systems is realized through sharing Gaussian codebooks across Swedish and English allophones. The language model (LM) components are constructed by training a statistical bigram model, with a common backoff node, on bilingual texts, and by combining two monolingual LMs into a probabilistic finite state grammar. This system uses a single decoder for Swedish and English sentences, and is capable of recognizing sentences with words from both languages. Preliminary experiments show that sharing acoustic models across the two languages has not resulted in improved performance, while sharing a backoff node at the LM component provides flexibility and ease in recognizing bilingual sentences at the expense of a slight increase in word error rate in some cases. As a by-product, the bilingual decoder also achieves good performance on language identification (LID).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-138"
  },
  "billa97_eurospeech": {
   "authors": [
    [
     "Jayadev",
     "Billa"
    ],
    [
     "Kristine",
     "Ma"
    ],
    [
     "John W.",
     "McDonough"
    ],
    [
     "George",
     "Zavaliagkos"
    ],
    [
     "David R.",
     "Miller"
    ],
    [
     "Kenneth N.",
     "Ross"
    ],
    [
     "Amro",
     "El-Jaroudi"
    ]
   ],
   "title": "Multilingual speech recognition: the 1996 byblos callhome system",
   "original": "e97_0363",
   "page_count": 4,
   "order": 139,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "This paper describes the 1996 Byblos Callhome speech recognition system for Spanish and Egyptian Colloquial Arabic. The system uses a combination of Phoneticly Tied-Mixture Gaussian HMMs and State- Clustered Tied-Mixture Gaussian HMMs in a multiple pass decoder. We focus here on the aspects of the system which are language specific and demonstrate the adaptability of the Byblos English system to new languages. Language related issues arising from both dialectal differences as well as differences between transcribed and spoken language are discussed. This system gave the lowest error rates in both Egyptian Colloquial Arabic and Spanish in the October 1996 NIST Callhome evaluation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-139"
  },
  "schultz97_eurospeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Detlef",
     "Koll"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Japanese LVCSR on the spontaneous scheduling task with JANUS-3",
   "original": "e97_0367",
   "page_count": 4,
   "order": 140,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "This paper presents our findings during the development of the recognition engine for the Japanese part of the VERBMOBIL speech-to-speech translation project. We describe an efficient method to bootstrap a large vocabulary speech recognizer for spontaneously spoken Japanese speech from a German recognizer and show that the amount of effort in developing the system could be reduced by using this rapid cross language bootstrapping technique. The Japanese recognizer is integrated into the VERBMOBIL system and shows very promising results achieving 9.3% word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-140"
  },
  "schultz97b_eurospeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Fast bootstrapping of LVCSR systems with multilingual phoneme sets",
   "original": "e97_0371",
   "page_count": 4,
   "order": 141,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "In this paper we described an efficient method to bootstrap continuously spoken, large vocabulary speech recognition systems by multilingual phoneme sets. To evaluate this techniques we collected the multilingual database GlobalPhone which currently consists of 9 different languages. A multilingual recognizer (MULTI) based on the four languages German, English, Japanese and Spanish was developed to serve as a source system. Likewise this system is very useful for language identification and achieves 100% language identification rate. Based on the MULTI system we evaluated our bootstrap technique on such completely different languages as Chinese, Croatian, and Turkish.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-141"
  },
  "pompinomarschall97_eurospeech": {
   "authors": [
    [
     "Bernd",
     "Pompino-Marschall"
    ],
    [
     "Christine",
     "Mooshammer"
    ]
   ],
   "title": "Factors of variation in the production of the German dorsal fricative",
   "original": "e97_0375",
   "page_count": 4,
   "order": 142,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "The articulatory variability of the Standard German (voiceless) dorsal fricative /x/ - surfacing as [c ], [x] or [? ] depending on the position within the syllable and the vowel context (cf. [2, 3]) - is analysed by using electropalatographic and electromagnetic articulographic as well as averaged spectral data. The results are compared to those for the coronal fricatives and the velar plosive in German.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-142"
  },
  "thomas97_eurospeech": {
   "authors": [
    [
     "Kimberly",
     "Thomas"
    ]
   ],
   "title": "EPG and aerodynamic evidence for the coproduction and coarticulation of clicks in Isizulu",
   "original": "e97_0379",
   "page_count": 4,
   "order": 143,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "This study examines both the nature and relative timing of the component gestures of click consonants in IsiZulu, a Bantu language of the Nguni cluster, spoken in South Africa. Differences between the three click types in the timing and rate of rarefaction reflect coproduction differences between their gestures. Coarticulation with adjacent vowels is illustrated by its effect on the rarefaction rate for the alveolar click type. Results thus far show that multigestural segments can coarticulate with adjacent segments despite claims to the contrary [5].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-143"
  },
  "geumann97_eurospeech": {
   "authors": [
    [
     "Anja",
     "Geumann"
    ]
   ],
   "title": "Formant trajectory dynamics in Swabian diphthongs",
   "original": "e97_0383",
   "page_count": 4,
   "order": 144,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "The vowel quality in some diphthongs of Swabian (an upper german dialect) was determined by measurement of first and second formant values. A minimal contrast could be shown between two different diphthong qualities /Ae/ and /«i/, where for Standard German only one is assumed, viz. /ai/. The two diphthong qualities differ only slightly in onset and offset vowel quality, so a better understanding of their relationship was expected from an examination of their dynamic aspects. Our preliminary results suggest that there is indeed a difference in the temporal structure of the two diphthongs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-144"
  },
  "wood97_eurospeech": {
   "authors": [
    [
     "Sidney A. J.",
     "Wood"
    ]
   ],
   "title": "The gestural organization of vowels and consonants: a cinefluorographic study of articulator gestures in Greenlandic",
   "original": "e97_0387",
   "page_count": 3,
   "order": 145,
   "p1": "387",
   "pn": "388",
   "abstract": [
    "This poster presents an analysis of speech articulator movements in Greenlandic speech to test the possible universality of results previously obtained from English, Arabic, Swedish and Bulgarian. Greenlandic is particularly interesting in this context because it is completely unrelated to Indo-European languages and because it offers articulations unfamiliar to speakers of European languages, such as uvular consonants and uvularization of vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-145"
  },
  "anderson97_eurospeech": {
   "authors": [
    [
     "Victoria B.",
     "Anderson"
    ]
   ],
   "title": "The perception of coronals in Western Arrernte",
   "original": "e97_0389",
   "page_count": 4,
   "order": 146,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "This study examined perception of multiple coronal places of articulation by native listeners of Western Arrernte. Three main findings emerged. 1) Coronal nasals and laterals are as perceptually robust as coronal stops. 2) Across manners of articulation, apical alveolars are less perceptually robust than other coronals. 3) Formant transitions from a preceding vowel are necessary to correctly identify apical alveolars and apical postalveolars. Acoustic analysis shows the importance of cues on the preceding vowel side for apical postalveolars, and on the following vowel side for laminal palatoalveolars. Laminal dentals have statistically distinguishable cues on both sides of the segment. Apical alveolars are hardest to characterize acoustically, and may be perceived by default. Low perceptibility of apical alveolars may be a reason for low functional load of the apical contrast. Result 3 corroborates Steriade's idea that contrasts \"must be licensed by the presence of their cues.\" [4]\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-146"
  },
  "espywilson97_eurospeech": {
   "authors": [
    [
     "Carol Y.",
     "Espy-Wilson"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Suzanne E.",
     "Boyce"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Acoustic modelling of American English /r/",
   "original": "e97_0393",
   "page_count": 4,
   "order": 147,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "The low F3 of American English /r/ (typical range 1300-1900 Hz) is accompanied articulatorily by constrictions in the pharyngeal, palatal and labial regions. Because acoustical theory predicts that formants will lower at points of maximum Volume velocity in the vocal tract, and because such points occur in the pharyngeal, palatal and labial regions, many investigators have speculated that the combination of these constrictions accounts for the low F3 of /r/. In this paper, we use the Maeda vocal tract modelling software to compare theoretical predictions of constriction location to data gathered from two American English speakers via Magnetic Resonance Imaging (MRI). We conclude that additional mechanisms are required to explain the acoustics of American English /r/.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-147"
  },
  "hansen97_eurospeech": {
   "authors": [
    [
     "Anya Varnich",
     "Hansen"
    ]
   ],
   "title": "Acoustic parameters optimised for recognition of phonetic features",
   "original": "e97_0397",
   "page_count": 4,
   "order": 148,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "Speaker variability is a major problem in today's state-of- the-art speech recognition systems. Parameterisation of speech in terms of Acoustic Parameters (APs) motivated by phonetic feature theory has shown to be more robustness to speaker variability as compared to cepstral coefficients when tested on the task of broad-class recognition [1]. Also APs has been successfully applied for identification of semivowels [2,3]. The aim of the present study is to investigate the use of APs for phoneme recognition. An extended set of features is used to distinguish between all phonemes in the TIMIT database and APs related to the extended feature set are found in literature. A separability measure is calculated to investigate the importance of the suggested APs for the separation of phonemes and feature classes. Results show that the APs that are the most important for separation of classes of phonetic features are also the most important for separation of phonemes classes. This indicates that phonemes can be recognised on the basis of phonetic features captured by the use of APs. However much work still needs to be done to understand and reliably extract all of the acoustic correlates of the phonetic features applied.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-148"
  },
  "halberstadt97_eurospeech": {
   "authors": [
    [
     "Andrew K.",
     "Halberstadt"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Heterogeneous acoustic measurements for phonetic classification 1",
   "original": "e97_0401",
   "page_count": 4,
   "order": 149,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "In this paper we describe our recent efforts to improve acoustic- phonetic modeling by developing sets of heterogeneous, phone-class- specific measurements, and combining these diverse measurements into a probabilistic classification framework. We first describe a baseline classifier using homogeneous measurements. After comparing selected sub-tasks to known human performance, we define sets of phone-class-specific measurements which improve within-class classification performance. Subsequently, we combine these heterogeneous measurements into an overall context- independent classification framework. We report on a series of phonetic classification experiments using the TIMIT acoustic-phonetic corpus. Our overall frame-work achieves 79.0% accuracy on the NIST core test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-149"
  },
  "milner97_eurospeech": {
   "authors": [
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Cepstral-time matrices and LDA for improved connected digit and sub-word recognition accuracy",
   "original": "e97_0405",
   "page_count": 4,
   "order": 150,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "Previous work has shown that good accuracy improvements can be made for isolated word recognition using cepstral-time matrices as the speech feature instead of the more conventional MFCC-based speech feature augmented with higher order cepstrum. This work extends the performance improvements to UK English connected digit strings and to a sub-word based town names task. Experimental results are presented for a range different sized cepstral-time matrix widths - ranging from a stack width of 3 up to 13 MFCC frames. In addition a variety of columns are selected from the cepstral-time matrix for use as the final speech feature. Tests show that the optimal implementation of the cepstral-time matrix varies according to the specific recognition task. Finally the technique of linear discriminative analysis (LDA) is applied to cepstral-time matrices and is shown to successfully improve recognition performance, as well as reducing the size of the final speech feature. Three different implementations of LDA are described and are demonstrated on isolated digit and sub- word tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-150"
  },
  "vuuren97_eurospeech": {
   "authors": [
    [
     "Sarel van",
     "Vuuren"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Data-driven design of RASTA-like filters",
   "original": "e97_0409",
   "page_count": 4,
   "order": 151,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "We describe use of Linear Discriminant Analysis (LDA) for data-driven automatic design of RASTA-like filters. The LDA applied to rather long segments of time trajectories of critical-band energies yields FIR filters to be applied to these time trajectories in the feature extraction module. Frequency responses of the first three discriminant vectors are in principle consistent with the ad hoc designed RASTA, delta and double-delta filters. On a connected digit task the new features outperform the original RASTA processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-151"
  },
  "nicholson97_eurospeech": {
   "authors": [
    [
     "Simon",
     "Nicholson"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Evaluating feature set performance using the f-ratio and j-measures",
   "original": "e97_0413",
   "page_count": 4,
   "order": 152,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "Several methods of measuring the class separability in a feature space used to model speech sounds are described. A simple one-dimensional feature space is considered first where class discrimination is measured using the F-ratio. Using a conventional feature set comprising static, velocity and acceleration MFCCs a ranking of the discriminative ability of each coefficient is made for both a digit and alphabet vocabulary. These rankings are shown to be quite similar for the two vocabularies. Discrimination measures are extended to multi- dimensional feature spaces using the J-measures. It is postulated that high correlation exists between feature sets which have a good measured class discrimination and those which give good recognition accuracy. Experiments are presented which measure this correlation and use it to predict recognition accuracy for a given set of features. These estimates are shown to be accurate for previously unseen combinations of features. A brief analysis of the effect linear discriminant analysis on the feature space is made using these measures of separability. It is shown that LDA and separability measures are closely linked.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-152"
  },
  "hernando97_eurospeech": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Robust speech parameters located in the frequency domain",
   "original": "e97_0417",
   "page_count": 4,
   "order": 153,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "In this paper, two ways of obtaining more robust spectral parameters are explored. Firstly, an hybridization of both LP and filter-bank approaches is considered, which is capable of improving recognition results for both noisy and clean speech in CDHMM digit recognition. Secondly, better performance may also be achieved by replacing the cepstral coefficients by a recently proposed set of parameters located in the frequency domain which come from a simple filtering of the log band energies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-153"
  },
  "gaillard97_eurospeech": {
   "authors": [
    [
     "Francois",
     "Gaillard"
    ],
    [
     "Frederic",
     "Berthommier"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "A modified zero-crossing method for pitch detection in presence of interfering sources",
   "original": "e97_0445",
   "page_count": 4,
   "order": 154,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "This paper evaluates, in terms of speech signal processing, a non- linear method of pitch detection based on the detection of the zero- crossings of the signals (ZC method), in adverse conditions of interference. First, F0 identification is evaluated according to the relative level of energy between the components in mixtures of pure tones or pairs of vowels; then, we introduce in the double-vowel paradigm a confidence measure based on the standard deviation of inter-zero intervals. Finally, the robustness of this confidence measure is tested in two cases of interference : pure tones + noise, and vowels + noise. We show that the method allows to detect periodicity without any knowledge about the nature of the interfering sources, and then to identify their fundamental frequency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-154"
  },
  "simonin97_eurospeech": {
   "authors": [
    [
     "Jacques",
     "Simonin"
    ],
    [
     "Chafic",
     "Mokbel"
    ]
   ],
   "title": "Using simulated annealing expectation maximization algorithm for hidden Markov model parameters estimation",
   "original": "e97_0449",
   "page_count": 4,
   "order": 155,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "This paper presents the use of a simulated annealing technique during the parameters estimation of a Hidden Markov Model (HMM) in a speech recognition system. This technique allows to move out of a local optimum which characterizes a classical Expectation Maximization (EM) algorithm, and thus to achieve a better estimation with a limited amount of training data. We choose here the Simulated Annealing Expectation Maximization (SAEM) algorithm introducing a simulated annealing technique in the EM method. The SAEM algorithm is compared to the classical EM algorithm, for both task- independent and task-dependent Viterbi training. The evaluation leads to significant improvement of recognition performances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-155"
  },
  "fant97_eurospeech": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Stellan",
     "Hertegard"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Johan",
     "Liljencrants"
    ]
   ],
   "title": "Covariation of subglottal pressure, F0 and glottal parameters",
   "original": "e97_0453",
   "page_count": 4,
   "order": 156,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "Sub- and supraglottal pressures, Psub and Psup, have been recorded during glissando phonation of sustained vowels, isolated vowels, and continuous speech including a one minute Iong reading of a novel. Studies of the covarition of Psub, F0, voice excitation amplitude Ee and overall sound pressure Ieve1 reveal systematic relations some of which can be expressed in closed form by regression equations. Systematic differences in Psub with respect to position in a breathgroup, vowel and consonant category and the degree of stress have been observed. The domain of subglottal increase with stress is of the order of one or a few words rather than a single syllable. The global contour of Psub within a breathgroup and the Ioca1 finestructures of Psub and transglottal pressure, Ptr=Psub-Psup associated with specific articulatory events are described and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-156"
  },
  "delopoulos97_eurospeech": {
   "authors": [
    [
     "Anastasios",
     "Delopoulos"
    ],
    [
     "Maria",
     "Rangoussi"
    ]
   ],
   "title": "The fractal behaviour of unvoiced plosives: a means for classification",
   "original": "e97_0457",
   "page_count": 4,
   "order": 157,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "Investigation of the fractal behaviour of unvoiced plosive consonants leads to interesting observations towards their classification. Experimental evidence of the fractal nature of the speech signals themselves, as well as of their derivatives and cumulative sums prompt the use of the associated fractal dimensions to form a discriminative feature set. The obtained feature set is compact in representation and easy to compute. At the same time, the discriminating capability of this feature set is seen to be promising even for speech signals sampled at 8KHz.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-157"
  },
  "ohno97_eurospeech": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Hideyuki",
     "Taguchi"
    ]
   ],
   "title": "A method for analysis of the local speech rate using an inventory of reference units",
   "original": "e97_0461",
   "page_count": 4,
   "order": 158,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "The speech rate is one of the important prosodic parameters essential for the naturalness of an utterance, yet comparatively little is known on the fine structures of speech rate variation in natural utterances. On the basis of the authors' definition of the relative local speech rate, the present paper describes an analysis of the changes in the local rate of speech units, each produced in isolation, when they are embedded in connected speech. The results, together with those already obtained by the authors, will lead to a complete scheme for speech rate control in speech synthesis by concatenation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-158"
  },
  "fujisaki97_eurospeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Takashi",
     "Yagi"
    ]
   ],
   "title": "Analysis and modeling of fundamental frequency contours of Greek utterances",
   "original": "e97_0465",
   "page_count": 4,
   "order": 159,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "A quantitative model for the process of 0 contour generation, originally developed for Japanese by Fujisaki and his co-workers, has already been shown to be valid for several other languages. The present study aims at testing its applicability to 0 contours of Greek utterances. Analysis of 0 contours of 200 utterances by two native speakers of Greek, produced by reading texts of narrations and conversations, has shown that the model is essentially valid, and suggests the model's usefulness for Text-to-Speech synthesis of Greek.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-159"
  },
  "martinez97b_eurospeech": {
   "authors": [
    [
     "Fernando",
     "Martinez"
    ],
    [
     "Daniel",
     "Tapias"
    ],
    [
     "Jorge",
     "Alvarez"
    ],
    [
     "Paloma",
     "Leon"
    ]
   ],
   "title": "Characteristics of slow, average and fast speech and their effects in large vocabulary continuous speech recognition",
   "original": "e97_0469",
   "page_count": 4,
   "order": 160,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "In this paper we report the characteristics of slow, average and fast speech. The study has been done using the TRESVEL Spanish database. It is composed of 3200 sentences uttered at three different speech rates and contains speech material from 20 male and 20 female speakers. This database has been designed to study, evaluate and compensate the effect of speech rate in Large Vocabulary Continuous Speech Recognition (LVCSR) systems. We report a new measure for the rate of speech (ROS). The ROS is normalised using an appropriate set of constants that depends on the expected duration of each phone. We also report the characteristics of slow, average and fast speech. Finally, we report the degradation in performance of a continuous speech recognition system when the speech rate is low and high, and the evaluation of two compensation techniques. Adaptation of the language weight, insertion penalties and HMM state-transition probabilities for slow speech provides a 21.5% reduction of the word error rate (WER).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-160"
  },
  "lee97b_eurospeech": {
   "authors": [
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Analysis of children's speech: duration, pitch and formants",
   "original": "e97_0473",
   "page_count": 4,
   "order": 161,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "Magnitude and variability of duration, pitch and formant frequencies are computed for speech collected from five to eighteen year-old children. The study confirmed that reduction in magnitude and variability are the primary indicators of speech development. Specifically, children below age ten exhibit wider dynamic range of vowel duration, longer suprasegmental duration, and larger temporal and spectral variations. These trends diminish around age twelve. Children's speech acoustic characteristics fully develop to adult level in both magnitude and variability around age fifteen. Change of formant frequencies in male speakers parallels the growth of the vocal tract, while for female speakers the presence of such a linear trend is not clear. We conclude that the primary factors governing the acoustic patterns during speech development are anatomical maturation of the speech apparatus and speech motor control in terms of agility and precision.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-161"
  },
  "traunmuller97_eurospeech": {
   "authors": [
    [
     "Hartmut",
     "Traunmüller"
    ],
    [
     "Anders",
     "Eriksson"
    ]
   ],
   "title": "A method of measuring formant frequencies at high fundamental frequencies",
   "original": "e97_0477",
   "page_count": 4,
   "order": 162,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "Accurate measurement of formant frequencies is important in many studies of speech perception and production. Errors in formant frequency estimation by eye, using a spectrogram, or automatically, using linear prediction, have been reported to be as high as 60 Hz F0 < 300 Hz. This exceeds the typical auditory difference limens (DLs) for formant frequencies and is also greater than some of the variation that one would like to study, e.g., the acoustic effects of varying vocal effort. The problem becomes substantially worse when F0 is as high as 500 to 600 Hz, which is not uncommon in the speech of women and children at high vocal efforts. In comparison with ordinary linear predictive analysis, the method described here drastically reduces measurement errors, given that the formant frequency is not below or only slightly above F0 (which rarely happens in speech). It thus becomes possible to study formant frequency variation in speech material that hitherto could not be analysed meaningfully since the effects of interest were no larger than the probable errors in measurement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-162"
  },
  "brondsted97_eurospeech": {
   "authors": [
    [
     "Tom",
     "Brondsted"
    ],
    [
     "Jens Printz",
     "Madsen"
    ]
   ],
   "title": "Analysis of speaking rate variations in stress-timed languages",
   "original": "e97_0481",
   "page_count": 4,
   "order": 163,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "This paper analyses speaking rate variations in English and Danish and relates them to problems encountered in speech recognition. Intra speaker variabilities in speech rates are explained with reference to time equalisation of stress groups and utterances. Further, it is shown that certain natural classes of phonemes are more affected by speaking rate variations than others. Keywords: Phoneme modeling, rate of speech, phone duration, time equalisation of stress groups and phrases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-163"
  },
  "micallef97_eurospeech": {
   "authors": [
    [
     "Paul",
     "Micallef"
    ],
    [
     "Ted",
     "Chilton"
    ]
   ],
   "title": "Automatic identification of phoneme boundaries using a mixed parameter model",
   "original": "e97_0485",
   "page_count": 4,
   "order": 164,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "The identification of phoneme boundaries in continuous speech is an important problem in areas of speech recognition and synthesis. The use of robust parameters to allow a trained data set obtained from one language to be used for boundary identification in another language is being investigated. In particular the use of mixed time- frequency rate parameters, and the training on the change of the rate parameters at acoustic boundaries is reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-164"
  },
  "koval97_eurospeech": {
   "authors": [
    [
     "Serguei",
     "Koval"
    ],
    [
     "Veronika",
     "Bekasova"
    ],
    [
     "Michael",
     "Khitrov"
    ],
    [
     "Andrey",
     "Raev"
    ]
   ],
   "title": "Pitch detection reliability assessment for forensic applications",
   "original": "e97_0489",
   "page_count": 4,
   "order": 165,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "For some tasks (e.g. ,forensic applications) it is vital important to know real pitch. So there is a problem to check-up the correctness of any concrete pitch contour for long speech records for any pitch detection method. Besides it would be useful to see the degree of signal periodicity without strong decision voice/noise. The new homomorphic method of signal periodicity degree detection with analysis frame length in proportion to the time-lag is described. The powerfull working approach to visual analysis of speech signal periodicity is proposed. This Foicograms method of speech periodicity representation ensures the practical correctness of pitch estimation and allows to find periodicity degree for poor quality signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-165"
  },
  "hu97_eurospeech": {
   "authors": [
    [
     "Zhihong",
     "Hu"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Efficient estimation of perceptual features for speech recognition",
   "original": "e97_0493",
   "page_count": 4,
   "order": 166,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "A number of studies have shown that a pair of perceptual effective formants can be defined to capture most of the phonetic information present in vowels. Various methods of computing the effective formant values were proposed. However, many of them depend on the accuracy of conventional formant estimation. In this work, we study methods of automatically estimating perceptual effective formants without estimating the actual formant values and compare the results with the perceptually measured effective formant values. The preliminary results show that the method is effective in estimating the perceptual effective formants. Classification experiments using perceptual effective formants as explicit features do not demonstrate any advantages. However, using the perceptual effective second formant value as input to our formant estimation algorithm can help to correct up to 44% of the formant tracking errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-166"
  },
  "malayath97_eurospeech": {
   "authors": [
    [
     "Narendranath",
     "Malayath"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Alexander",
     "Kain"
    ]
   ],
   "title": "Towards decomposing the sources of variability in speech",
   "original": "e97_0497",
   "page_count": 4,
   "order": 167,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "In this paper a method to decompose a conventional feature space (LPC-cepstrum) into subspaces which carry information about the linguistic and speaker variability is presented. Principal component analysis is used to study the correlation between these sub-spaces. Oriented principal component analysis (OPCA) is then used to estimate a sub-space which is relatively speaker- independent. A method to estimate the dimensionality of the speaker independent sub- space is also presented. Original features can now be projected into the speaker independent sub-space to make them less sensitive to speaker variations. Finally the effectiveness of the proposed method in suppressing the speaker dependence is studied by experiments conducted on two different databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-167"
  },
  "chengalvarayan97b_eurospeech": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Use of vector-valued dynamic weighting coefficients for speech recognition: maximum likelihood approach",
   "original": "e97_0501",
   "page_count": 4,
   "order": 168,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "In this paper, an integrated approach to vector dynamic feature extraction is proposed in the design of a hidden Markov model (HMM) based speech recognizer. The integrated model we developed in this study generalizes the conventional, currently widely used dynamic-parameter technique, which has been confined strictly to the preprocessing domain only, in two significant ways. First, the new model contains state-dependent, vector-valued weighting functions responsible for transforming static speech features into the dynamic ones in a slowly time-varying manner. Second, a novel maximum- likelihood based training algorithm is developed for the model that allows joint optimization of the state-dependent, vector-valued weighting functions and the remaining conventional HMM parameters. The experimental results on alphabet classification demonstrate the effectiveness of the new model relative to standard HMM using dynamic features that have not been subject to optimization during training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-168"
  },
  "beet97_eurospeech": {
   "authors": [
    [
     "S. W.",
     "Beet"
    ],
    [
     "L.",
     "Baghai-Ravary"
    ]
   ],
   "title": "Automatic segmentation: data-driven units of speech",
   "original": "e97_0505",
   "page_count": 4,
   "order": 169,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "An algorithm is presented which allows non parametric representations of speech to be automatically segmented into units of comparable duration and character to manually-defined phonemes. The consistency of this segmentation across speakers, and across telephone channels, is investigated and the implications of adopting such forms of data for automatic speech recognition are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-169"
  },
  "bajic97_eurospeech": {
   "authors": [
    [
     "Dejan",
     "Bajic"
    ]
   ],
   "title": "On robust time-varying AR speech analysis based on t-distribution",
   "original": "e97_0509",
   "page_count": 4,
   "order": 170,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "In this paper a new robust non-recursive algorithm for parameter estimation of AR model of speech signal is proposed. The proposed algorithm takes into account the quasi-periodic excitation for voiced speech and assumes the t-distribution with small degrees of freedom a of the excitation signal. The method is based on the covariance linear prediction with sliding window. Experiments on both synthesized and natural speeches have shown that the proposed robust algorithm gives estimates with smaller variance and bias, compared to the conventional non-robust algorithm. The choice of a=3 induces to the most efficient estimation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-170"
  },
  "tambakas97_eurospeech": {
   "authors": [
    [
     "Dimitris",
     "Tambakas"
    ],
    [
     "Iliana",
     "Tzima"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A simple phoneme energy model for the Greek language and its application to speech recognition",
   "original": "e97_0513",
   "page_count": 4,
   "order": 171,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "This paper deals with the improvement of autosegmentation algorithms by establishing and implementing a simple energy model. This model consists of rules which describe the variation of the phoneme energy at the phoneme boundaries due to the phoneme context. The efficient estimation of phoneme boundaries results to the improvement of the accuracy of phoneme-based, large vocabulary speech recognition systems, as proven from experiments in the Greek language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-171"
  },
  "noad97_eurospeech": {
   "authors": [
    [
     "James E. H.",
     "Noad"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "A macroscopic analysis of an emotional speech corpus",
   "original": "e97_0517",
   "page_count": 4,
   "order": 172,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "Macroscopic analysis of a corpus of emotional Standard Southern British speech signals has been performed to measure any changes in average fundamental frequency, speech rate, energy and first formant frequency. Seven acted emotional states were recorded and analysed for one male and one female speaker. Differences between neutral and emotional speech were found which agree with changes others have mentioned in the literature. Only the emotion sadness was found to be consistently and obviously different from neutral, while high activity emotions (such as elation and hot anger) could be distinguished from sadness. Additional measures are being developed which will further discriminate the emotions from one another. Results obtained to date are being evaluated for use in a speech synthesiser system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-172"
  },
  "shimodaira97_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Akihiro",
     "Kumata"
    ]
   ],
   "title": "Restoration of pitch pattern of speech based on a pitch generation model",
   "original": "e97_0521",
   "page_count": 4,
   "order": 173,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "In this paper a model-based approach for restoring a continuous fundamental frequency (F 0 ) contour from the noisy output of an F 0 extractor is investigated. In contrast to the conventional pitch trackers based on numerical curve-fitting, the proposed method employs a quantitative pitch generation model, which is often used for synthesizing F 0 contour from prosodic event commands for estimating continuous F 0 pattern. An inverse filtering technique is introduced for obtaining the initial candidates of the prosodic commands. In order to find the optimal command sequence from the commands efficiently, a beam- search algorithm and an N-best technique are employed. Preliminary experiments for a male speaker of the ATR B-set database showed promising results both in quality of the restored pattern and estimation of the prosodic events.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-173"
  },
  "agranovski97_eurospeech": {
   "authors": [
    [
     "A. V.",
     "Agranovski"
    ],
    [
     "O. Y.",
     "Berg"
    ],
    [
     "D. A.",
     "Lednov"
    ]
   ],
   "title": "The research of correlation between pitch and skin galvanic reaction at change of human emotional state",
   "original": "e97_0525",
   "page_count": 4,
   "order": 174,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "[Abstract missing]\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-174"
  },
  "montacie97_eurospeech": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ],
    [
     "Fabrice",
     "Lefèvre"
    ]
   ],
   "title": "K-NN versus Gaussian in HMM-based recognition system",
   "original": "e97_0529",
   "page_count": 4,
   "order": 175,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "For many years, the K-Nearest Neighbours method (K-NN) is known as one of the best probability density function (pdf) estimator. A fast K-NN algorithm has been developed and tested on the TIMIT database with a gain in computational time of 99;8%. The K-NN decision principle has been assessed on a frame by frame phonetic identification. A method to integrate K-NN estimator pdf in a HMM-based system is proposed and tested on an acoustic-phonetic decoding task. Finally, preliminary experiments are performed on the HMM topology inference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-175"
  },
  "doval97_eurospeech": {
   "authors": [
    [
     "Boris",
     "Doval"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Benoit",
     "Diard"
    ]
   ],
   "title": "Spectral methods for voice source parameters estimation",
   "original": "e97_0533",
   "page_count": 4,
   "order": 176,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "A spectral approach is proposed for voice source parameters representation and estimation. Parameter estimation is based on decomposition of the periodic and the aperiodic components of the speech signal, and on spectral modelling of the periodic component. The paper focusses on parameters estimation for the periodic component of the glottal flow. A new anticausal all-pole model of the glottal flow is derived. Glottal flow is seen as an anticausal 2-pole filter followed by a spectral tilt filter. The anticausal filter has complex poles, instead of the real poles that are usually assumed. Time-domain and frequency domain parameters are linked by analytic formulas. Two spectral domain algorithms are proposed for estimation of open quotient. The first one is based on measurement of the first harmonics, and the second one is based on spectral modelling. Experimental results demonstrate the accuracy of the estimation procedures\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-176"
  },
  "vrecken97_eurospeech": {
   "authors": [
    [
     "Olivier van der",
     "Vrecken"
    ],
    [
     "Nicolas",
     "Pierret"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Vincent",
     "Pagel"
    ],
    [
     "Fabrice",
     "Malfrere"
    ]
   ],
   "title": "A simple and efficient algorithm for the compression of MBROLA segment databases",
   "original": "e97_0421",
   "page_count": 4,
   "order": 177,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "Most state-of-the-art TTS synthesizers are based on a technique known as synthesis by concatenation, in which speech is produced by concatenating elementary speech units. The design of a high-quality TTS system implies the storage of a large number of segments. To facilitate the storage of these segments, this paper proposes a very low complexity coder to compress unit databases with a toll quality. A particular interest has been taken in the databases used by the MBROLA synthesizer, composed of fixed-length pitch periods with constrained harmonic phases. The coder developed here uses this special characteristic to reach compression rates from 7 to 9 without degrading the speech quality produced by the synthesizer, and with very limited computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-177"
  },
  "zolfaghari97_eurospeech": {
   "authors": [
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "A segmental formant vocoder based on linearly varying mixture of Gaussians",
   "original": "e97_0425",
   "page_count": 4,
   "order": 178,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "This paper describes a low bit-rate segmental formant vocoder. The formants are estimated using mixture of Gaussians whose means are constrained to vary linearly with time within a segment. A new method of smoothing the power spectrum has been used in order to improve modelling with mixtures of Gaussians. Pitch is estimated using the autocorrelation function, and voicing is detected using the autocorrelation function method and the energy in the spectrum. Optimal segment boundaries are obtained using a dynamic programming procedure based on the power normalised log-likelihood of the segment. Magnitude-only sinusoidal synthesis is then used to synthesise speech from the estimated spectrum. Using multiple codebooks an average bit-rate of 500 bps has been obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-178"
  },
  "chennoukh97_eurospeech": {
   "authors": [
    [
     "Samir",
     "Chennoukh"
    ],
    [
     "Daniel",
     "Sinder"
    ],
    [
     "Gael",
     "Richard"
    ],
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "Voice mimic system using an articulatory codebook for estimation of vocal tract shape",
   "original": "e97_0429",
   "page_count": 4,
   "order": 179,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "Voice mimic systems using articulatory codebooks require an initial estimate of the vocal tract shape in the vicinity of the global optimum. For this purpose, we need to gather a large set of corresponding articulatory and acoustic data in the articulatory codebook. Thus, searching and accessing the codebook becomes a dificult task. In this paper, the design of an articulatory codebook is presented where an acoustic network sub-samples the acoustic space such that vocal tract model shapes are ordered and clustered in the network according to acoustic parameters. Another issue addressed in this paper concerns estimating the trajectory of vocal tract shapes as they change with time. Since the inverse mapping from acoustic parameters to model shape does not have a unique solution, several vocal tract shape variations are possible. Therefore, a dynamic optimization of trajectories has been developed. This optimization uses dynamic properties of each articulatory parameter to estimate the next position.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-179"
  },
  "mudugamuwa97_eurospeech": {
   "authors": [
    [
     "Damith J.",
     "Mudugamuwa"
    ],
    [
     "Alan B.",
     "Bradley"
    ]
   ],
   "title": "Adaptive transform coding for linear predictive residual",
   "original": "e97_0433",
   "page_count": 4,
   "order": 180,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "In voice coding applications where there is no constraint on the encoding delay, such as store and forward message systems or voice storage, segment coding techniques can be used to achieve a reduction in data rate without compromising the level of distortion. For low data rate linear predictive coding schemes, increasing the encoding delay allows one to exploit any long term temporal stationarities on an inter&ame basis, thus reducing the transmission bandwidth or storage needs of the speech signal. Transform coding has previously been applied to exploit both the inter and intra frame correlation, but has been limited to short term spectral redundancy [ 1 ] [2]. This paper investigates the potential for data rate reduction through extending the use of segment coding techniques to identify redundancies in the LPC residual. Initial tests indicate a potential 40% average reduction in data rate for a given subjective speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-180"
  },
  "takahashi97_eurospeech": {
   "authors": [
    [
     "Akira",
     "Takahashi"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ],
    [
     "Paolino",
     "Usai"
    ],
    [
     "David",
     "Atkinson"
    ]
   ],
   "title": "Performance evaluation of objective quality measures for coded speech",
   "original": "e97_0437",
   "page_count": 4,
   "order": 181,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "A performance evaluation method for objective measures estimating subjective quality of coded speech is proposed and applied the comparison of existing objective quality measures. The measure based on Bark spectrum distortion performs the best. Comparing its estimation error with the statistical reliability of subjective quality assessment shows that objective quality measurement can be as reliable as subjective measurements for some testing conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-181"
  },
  "ismail97_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Ismail"
    ],
    [
     "Keith",
     "Ponting"
    ]
   ],
   "title": "Between recognition and synthesis - 300 bits/second speech coding",
   "original": "e97_0441",
   "page_count": 4,
   "order": 182,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "This paper describes a system for speech coding designed to operate at 300 bits/sec and below. A continuous speech recogniser is used to transcribe incoming speech as a sequence of sub-word units termed acoustic segments. Prosodic information is combined with segment identity to form a serial data stream suitable for transmission. A rule- based system maps segment identity and prosodic information to parameters suitable for driving a parallel formant speech synthesiser. Acoustic segment Hidden Markov Models (HMMs) are shown to perform as well as conventional phone HMMs during recognition. A segment error rate of 3.8 % was achieved in a speaker-dependent, task-dependent configuration. An average data rate of 262 bits/sec was obtained. Speech from the synthesiser was better than obtainable from a purely textual representation though not as good as 2400 bit/sec Linear Predictive Coding (LPC) vocoded speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-182"
  },
  "villette97_eurospeech": {
   "authors": [
    [
     "Stephane",
     "Villette"
    ],
    [
     "Milos",
     "Stefanovic"
    ],
    [
     "Ian",
     "Atkinson"
    ],
    [
     "Ahmet",
     "Kondoz"
    ]
   ],
   "title": "High quality split-band LPC vocoder and its fixed point real time implementation",
   "original": "e97_1243",
   "page_count": 4,
   "order": 183,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "A split band vocoder in which the LP excitation is split into voiced and unvoiced frequencies is presented. In doing this the coder's performance during both mixed voicing and speech containing acoustic noise is greatly improved, producing soft natural sounding speech. In addition a variable rate version which achieves an average rate of 1.4 kb/s is detailed. The issue of fixed point real time implementation of this coder is also presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-183"
  },
  "chang97b_eurospeech": {
   "authors": [
    [
     "Wen-Whei",
     "Chang"
    ],
    [
     "Hwai-Tsu",
     "Chang"
    ],
    [
     "Wan-Yu",
     "Meng"
    ]
   ],
   "title": "Missing packet recovery techniques for DM coded speech",
   "original": "e97_1247",
   "page_count": 4,
   "order": 184,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "The packet loss effects of DM coded speech can be mitigated by either using an embedded DM system (EDM) or using a tree search interpolator. This paper provides theoretical and experimental results for EDM coding of autoregressive sources under random error conditions. For the tree interpolation, we explore the benefits of delayed decoding by using an interpolative DM code generator to form a tree of sample possibilities given the remaining adjacent samples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-184"
  },
  "vu97_eurospeech": {
   "authors": [
    [
     "Hai Le",
     "Vu"
    ],
    [
     "Laszlo",
     "Lois"
    ]
   ],
   "title": "Spectral sensitivity of LSP parameters and their transformed coefficients",
   "original": "e97_1251",
   "page_count": 4,
   "order": 185,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "In this paper, the optimal transformation and quantization of Line Spectrum Pair (LSP) are accomplished. Based upon the interframe and intraframe correlation properties of the LSPs, the Karhunen-Loeve (KL) transformation is adopted by Principal Component Analysis (PCA) neural network. The spectral sensitivity of the LSP and transformed coefficients are investigated in order to develop better scalar and vector quantizers for these coefficients. Using PCA network with spectral sensitivity guided quantizers we show that this new approach leads to as good as or better distortion compared to other methods for speech coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-185"
  },
  "ramasubramanian97_eurospeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Reducing the complexity of the LPC vector quantizer using the k-d tree search algorithm",
   "original": "e97_1255",
   "page_count": 4,
   "order": 186,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "Linear predictive coding (LPC) parameters are widely used in various speech coding applications for representing the spectral envelope information of speech. Transparent quantization of the LPC parameters (average spectral distortion of 1 dB) can be achieved at 24 bits/frame using the split vector LPC quantizer (SVLPC) which quantizes 10-dimensional line spectral frequency (LSF) vectors in two parts. However, SVLPC su ers from a high computational complexity in quantizing each part (one of dimension 4 and the other of dimension 6) using independent codebooks of size 4096 (corresponding to a rate of 12 bits/part). This limits the practical real- time application of the coder. In this paper, we reduce the computational complexity of the split vector quantizer by 2 orders of magnitude using the fast K-dimensional (K-d) tree search algorithm under the bucket-Voronoi intersection (BVI) search framework. This is of signiFIcant importance in rendering the SVLPC amenable for practical real-time coding applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-186"
  },
  "lemma97_eurospeech": {
   "authors": [
    [
     "Aweke N.",
     "Lemma"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Ed F.",
     "Deprettere"
    ]
   ],
   "title": "Quantization using wavelet based temporal decomposition of the LSF",
   "original": "e97_1259",
   "page_count": 4,
   "order": 187,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "The quantization of linear prediction coefficients (LPC) is an important aspect in low bit rate speech coding. In this work, we introduce a new approach, which exploits the temporal dependencies in the line spectral frequencies (LSF). We approximate each LSF track using expansion into wavelet basis functions. As the LSF vary fairly smoothly as functions of time, they perform very well when interpolated. By vector quantizing the resulting wavelet expansion coefficients, the interpolated LSF tracks could be quantized with a distortion of 0.91 dB using only 15.6 bits per 20 ms update (780 bits per second). This is about 4 bits per update less than the results obtained with previously described procedures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-187"
  },
  "xydeas97_eurospeech": {
   "authors": [
    [
     "Costas S.",
     "Xydeas"
    ],
    [
     "Gokhan H.",
     "Ilk"
    ]
   ],
   "title": "A novel 1.7/2.4 kb/s DCT based prototype interpolation speech coding system",
   "original": "e97_1263",
   "page_count": 4,
   "order": 188,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "In this paper a novel DCT prototype interpolation synthesis process is presented and used to model the input speech signal. The compression efficiency of the DCT when applied to prototype pitch segments, leads to 1.7/2.4 kb/s DCT-PIC systems which can deliver decoded speech of high communication quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-188"
  },
  "choi97_eurospeech": {
   "authors": [
    [
     "Yong-Soo",
     "Choi"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Sang-Wook",
     "Park"
    ],
    [
     "Jae-Ha",
     "Yoo"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "Improved regular pulse VSELP coding of speech at low bit-rates",
   "original": "e97_1267",
   "page_count": 4,
   "order": 189,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "This paper describes an improved RP-VSELP (IRP-VSELP) speech coding. The RP-VSELP is classifed as a fast VSELP since it produces a comparable speech quality to the VSELP with much simplified system complexity. The new RP-VSELP coder proposed in this paper has additional new features, such as a fast codebook search obtained by employing backward filtering and pitch-adaptive regular pulse excitation. Due to new features added to the original RP-VSELP, the proposed method not only reduces the complexity of the original RP- VSELP but also provides an improved speech quality. Throughout objective and subjective tests, IRP-VSELP outperformed RP-VSELP as the ref erence coder. Simulation results are presented to verify the performance of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-189"
  },
  "cho97_eurospeech": {
   "authors": [
    [
     "Yong Duk",
     "Cho"
    ],
    [
     "Hong Kook",
     "Kim"
    ],
    [
     "Moo Young",
     "Kim"
    ],
    [
     "Sang Ryong",
     "Kim"
    ]
   ],
   "title": "Joint estimation of pitch, band magnitudes, and v\\UV decisions for MBE vocoder",
   "original": "e97_1271",
   "page_count": 4,
   "order": 190,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "The multiband excitation (MBE) vocoder represents speech signal with a pitch, band magnitudes, and a voice / unvoice (V/UV) decision for each spectral band. In the conventional MBE model, model parameters are sequentially estimated in two steps. The pitch and band magnitudes are firstly estimated on the assumption of voiced speech model by the analysis-by-synthesis (AbS) in frequency domain, and then the V/UVs are decided. However, the synthetic spectrum by the above assumption may have large spectral distortion if the speech frame is strongly unvoiced such as transient region. In this paper, we propose joint estimation method which estimates and decides all the model parameters in AbS loop. For this, voiced or unvoiced speech models for each band are used during the analysis procedure. After estimating the parameters with the two speech models, a model for each band is selected so as to produce smaller spectral estimation error. By analyzing the short time spectrum and the long time spectrogram, it is shown that the reproduced speech of the proposed model is superior to that of the conventional one. In addition, through informal listening test we also confirm the superiority of the proposed model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-190"
  },
  "kovesi97_eurospeech": {
   "authors": [
    [
     "Balazs",
     "Kovesi"
    ],
    [
     "Samir",
     "Saoudi"
    ],
    [
     "Jean Marc",
     "Boucher"
    ],
    [
     "Gábor",
     "Horvath"
    ]
   ],
   "title": "A new distance measure in LPC coding: application for real time situations",
   "original": "e97_1275",
   "page_count": 4,
   "order": 191,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "The distance measure has a great importance in the phase of the construction of a vector quantizer for LSP parameters as well as in the coding phase. Due to its complexity, the meaningful spectral distance is seldom used for the purpose of quantization. The weighted squared Euclidean distances are mathematicallv more tractahle and are commonly used. Significant differences can be found in the performanance of different distances. The aim of this paper is to study different distance measures used in the field of LSP Coding. A new weighted Euclidean distance will he proposed that not only replaces the spectral distance hut estimates well its exact value. However, the use of squared distances will he justified as well. In a real time application. often weights can not be calculated according to the input vector computation must be done according to the code- words, before coding. This causes some problems in case of split vector quantization or multi stage vector quantization. Some solutions will he given at the end of this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-191"
  },
  "vepyek97_eurospeech": {
   "authors": [
    [
     "Peter",
     "Vepyek"
    ],
    [
     "Alan B.",
     "Bradley"
    ]
   ],
   "title": "Consideration of processing strategies for very-low-rate compression of wideband speech signals with known text transcription",
   "original": "e97_1279",
   "page_count": 4,
   "order": 192,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "This paper addresses the problem of very-low-rate compression of digitized wideband speech signals for storage. It concentrates on applications where the text transcription of the speech corpus is available and where high quality of recovered speech is required. Following the problem statement, all unique features of the task are analysed and possible methods of implementation discussed. As a result, a novel speech compression technique is proposed, its general structure is presented, and its characteristics are considered. The new compression technique - hybrid speech compression - takes full advantage of the available text transcription. The proposed hybrid compression approach utilises an optimum balance of Text To Speech (TTS) synthesis technology with dynamic speech conversion to yield a data stream comprising original text enriched by prosodic features and conversion control information. The proposed speech compression method aims to achieve an extremely low data rate while preserving a high quality of the compressed wideband speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-192"
  },
  "gortz97_eurospeech": {
   "authors": [
    [
     "Norbert",
     "Görtz"
    ]
   ],
   "title": "Zero-redundancy error protection for CELP speech codecs",
   "original": "e97_1283",
   "page_count": 4,
   "order": 193,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "In this paper the possibilities of channel-error protection for transmission of CELP-coded speech over highly disturbed channels without additional bits for error-control are discussed. Algorithms are given which do not require explicit channel models and work without additional delay and almost no additional complexity. Time-based and mutual dependencies of the speech codec parameters are exploited for channel-error detection and parameter extrapolation at the decoder. The algorithms are optimized by informal listening tests rather than by maximization of a mathematically tractable measure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-193"
  },
  "matmti97_eurospeech": {
   "authors": [
    [
     "Ridha",
     "Matmti"
    ],
    [
     "Milan",
     "Jelinek"
    ],
    [
     "Jean-Pierre",
     "Adoul"
    ]
   ],
   "title": "Low bit rate speech coding using an improved HSX model",
   "original": "e97_1287",
   "page_count": 4,
   "order": 194,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "This paper presents some improvements to the mixed Harmonic and Stochastic eXcitation (HSX) algorithm in the context of low bit rate speech coding (around 2.4 kbit/s). The dominant issue is the modeling of the excitation signal in order to improve the quality of the synthesized speech signal without increasing neither the bit rate nor the complexity. The pitch tracking algorithm is revised in order to increase the robustness and to reduce the complexity. The voicing analysis algorithm is also refined. Informal listening of the synthesized speech at 2.4 kbitls shows a significant improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-194"
  },
  "ribeiro97_eurospeech": {
   "authors": [
    [
     "Carlos M.",
     "Ribeiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Phonetic vocoding with speaker adaptation",
   "original": "e97_1291",
   "page_count": 4,
   "order": 195,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "This paper describes a phonetic vocoding scheme which relies on speaker adaptation to capture important speaker characteristics. These are typically lost in phonetic vocoders which transmit only information about the phones which are recognized, together with some prosodic information. In our scheme, however, additional speaker characteristics are transmitted in vowel regions (average values of LSP coefficients for each phone). This additional information yielded potentially good speaker recognizability results, in informal listening tests, while still achieving a rather low average bit rate, suitable for many transmission and storage applications. This work extends our previous phonetic vocoding scheme described in [5]. The vocoder is now fully quantized and the number of transmitted parameters had been significantly reduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-195"
  },
  "baudoin97_eurospeech": {
   "authors": [
    [
     "Geneviève",
     "Baudoin"
    ],
    [
     "Jan",
     "Cernocky"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Quantization of spectral sequences using variable length spectral segments for speech coding at very low bit rate",
   "original": "e97_1295",
   "page_count": 4,
   "order": 196,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "This paper deals with the coding of spectral envelope parameters for very low bit rate speech coding (inferior to 500 bps). In order to obtain a sufficient intelligibility, segmental techniques are necessary. Variable dimension vector quantization is one of these. We propose a new interpretation of already published research from Chou- Lockabaugh [2] and Cernocky- Baudoin-Chollet [4,6] on the quantization of variable length sequences of spectral vectors, named respectively Variable to Variable length Vector Quantization (VVVQ) and Multigrams Quantization (MGQ). This interpretation gives a meaning to the Lagrange multiplier used in the optimization criterion of the VVVQ, and should allow new developments as, for example, new modelization of the probability density of the source. We have also studied the influence of the limitation of the delay introduced by the method. It was found that a maximal delay of 400 ms is generally sufficient. Finally, we propose the introduction of long sequences in the segmental codebook by linear interpolation of shorter ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-196"
  },
  "ghaemmaghami97_eurospeech": {
   "authors": [
    [
     "Shahrokh",
     "Ghaemmaghami"
    ],
    [
     "Mohamed",
     "Deriche"
    ],
    [
     "Boualem",
     "Boashash"
    ]
   ],
   "title": "On modeling event functions in temporal decomposition based speech coding",
   "original": "e97_1299",
   "page_count": 4,
   "order": 197,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "Temporal Decomposition (TD) is an efficient technique for modeling speech spectral evolution through orthogonalization of the matrix of spectral parameters which reduces the amount of spectral information in TD-based speech coding. We have shown in earlier work that ``event'' functions can be approximated by fixed-width Gaussian functions with a minor degradation in the reconstructed speech, leading to further bit-rate reduction in such systems. In this paper, through perceptually-based spectral distortion measurement, we show the impact of events shape on the speech quality, and propose a new composite function and discuss its effect on the coder performance using different combinations of spectral parameters in event detection and speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-197"
  },
  "torres97_eurospeech": {
   "authors": [
    [
     "Soledad",
     "Torres"
    ],
    [
     "F. Javier",
     "Casajús-Quirós"
    ]
   ],
   "title": "Phase quantization by pitch-cycle waveform coding in low bit rate sinusoidal coders",
   "original": "e97_1303",
   "page_count": 4,
   "order": 198,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "A new phase coding algorithm is introduced in this paper, which works in the pitch-cycle waveform domain. It provides accurate phase coding at low bit cost. Its performance is analyzed inside a multiband excitation coder with improved onset representation. In this context, the introduction of original phase information by means of the proposed coding algorithm provides noticeable quality improvement without increasing the total bit rate of the coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-198"
  },
  "botinis97_eurospeech": {
   "authors": [
    [
     "Antonis",
     "Botinis"
    ],
    [
     "Marios",
     "Fourakis"
    ],
    [
     "John W.",
     "Hawks"
    ]
   ],
   "title": "A perceptual study of the greek vowel space using synthetic stimuli",
   "original": "e97_1307",
   "page_count": 4,
   "order": 199,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "Four female native speakers of Modern Greek listened to 465 synthetic vowel tokens with Fl frequencies ranging from 250 to 800 Hz and F2 frequencies ranging from 900 to 2900 Hz in 50 Hz steps. They were asked to identify each stimulus as one of the five vowels of Modern Greek or to reject it if they thought it could not be a vowel of their language. The subjects rejected about 64 percent of the tokens as not possible vowels. The remaining points were plotted in an F I by F2 space with the codes assigned by each subject and in a composite space, where only the points identified with the same response by at least three subjects were used. The results replicated those of Hawks and Fourakis [1], except that the code for the vowel [e] was assigned to many more points than the codes for the other wowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-199"
  },
  "han97_eurospeech": {
   "authors": [
    [
     "Woo-Jin",
     "Han"
    ],
    [
     "Sung-Joo",
     "Kim"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Mixed multi-band excitation coder using frequency domain mixture function (FDMF) for a low-bit rate speech coding",
   "original": "e97_1311",
   "page_count": 4,
   "order": 200,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "This paper describes the Mixed Multi-Band Excitation coder used for a low bit-rate speech coding. In MBE coders, there are significant differences of the fine structure between the original and the synthetic spectrum. They are mainly due to the exclusive partition of voiced and unvoiced regions in frequency domain and the decision procedure based on the experimental threshold. The MMBE uses frequency domain mixture function (FDMF) to overcome these drawbacks of the MBE coder. Also, two analysis methods, which do not need any decision procedure based on a threshold, are presented. The performance evaluation results show that the 2.6kbps MMBE coder reduces the average spectral distortion by a clear margin comparing to the 2.9kbps MBE coder. The computational load of the proposed coder is sufficiently small for a real-time implementation on the modern DSP chip.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-200"
  },
  "fingscheidt97_eurospeech": {
   "authors": [
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Olaf",
     "Scheufen"
    ]
   ],
   "title": "Robust GSM speech decoding using the channel decoder's soft output",
   "original": "e97_1315",
   "page_count": 4,
   "order": 201,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "In the digital mobile radio system GSM (Global System for Mobile Communications) there is a need for reducing the subjective effects of residual bit errors by error concealment techniques. Due to the fact that the standard does not specify these algorithms bit exactly, there is room for new solutions to improve the decoding process. This contribution presents a new approach for optimum estimation of speech codec parameters [7] applied to the GSM system. It requires a soft-output channel decoder (e.g. soft-output Viterbi algorithm -- SOVA [8]) providing a bit reliability information for the proposed parameter estimation process. Additionally, a priori knowledge about the residual redundancy in the sequence of codec parameters is exploited. The new method includes an inherent muting mechanism leading to a graceful degradation of speech quality in case of adverse transmission conditions. If the channel is error free, bit exactness as required by the GSM standard is preserved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-201"
  },
  "seymour97_eurospeech": {
   "authors": [
    [
     "Carl W.",
     "Seymour"
    ],
    [
     "Tony A.",
     "Robinson"
    ]
   ],
   "title": "A low-bit-rate speech coder using adaptive line spectral frequency prediction 1319",
   "original": "e97_1319",
   "page_count": 4,
   "order": 202,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "This paper describes two aspects of a linear predictive coding (LPC) vocoder developed for operation on wide- band speech. The method for encoding the LPC parameters, based on the use of an adaptive predictor, is pre- sented together with an extension to the vocoder model which enables it to operate on speech sampled at 16kHz rather than 8kHz. Good-quality operation on wide-band speech is achieved with an increase in bit rate of about 500 bits/s. Diagnostic rhyme test (DRT) results demonstrate the improvement in intelligibility gained through coding speech at the higher sample rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-202"
  },
  "ding97_eurospeech": {
   "authors": [
    [
     "Wen",
     "Ding"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Optimising unit selection with voice source and formants in the CHATR speech synthesis system",
   "original": "e97_0537",
   "page_count": 4,
   "order": 203,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "High quality corpus-based synthetic speech requires minimization of prosodic and acoustic distortions between an ideal phoneme sequence and the actual waveform segments used to reproduce it. Our synthesis system concatenates phoneme-sized wave- form segments, without signal processing, selected from a large-scale speech database according to both prosodic and phonetic contextual suitability. This paper describes an approach to optimising such unit selection in speech synthesis by using voice source parameters and formant information, instead of selection based on cepstral features. We present results showing that formants and voice source parameters are more effective as acoustic features in the unit selection. These features can be estimated automatically from speech waveforms using the ARX joint estimation method. Results are compared with mel- frequency cepstrum coefficients (MFCC), previously used for unit selection, and both objective and subjective experiments showed that the new features outperformed the previous ones, and confirmed that the synthesized speech sounded much more natural.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-203"
  },
  "abe97_eurospeech": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "A new framework to provide high-controllability speech signal and the development of a workbench for it",
   "original": "e97_0541",
   "page_count": 4,
   "order": 204,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "This paper proposes a new framework to enhance the access to and control of speech signals. To enhance accessibility, the proposed framework assigns multi-layered tags such as orthographic transcriptions, and phonetic transcriptions. The tags also make it possible to precisely synchronize a speech signal with animation. In terms of control, the proposed framework provides hybrid speech; combining both human speech and speech synthesis-by-rule. Its quality ranges from simple TTS (the worst case) to encoded natural speech (the best case) depending on the resources available: texts, fundamental frequency(Fo) contour, power contour, phoneme duration, and so on. To create speech messages based on the proposed framework, we developed a workbench employing speech synthesis and recognition techniques. Important features of the workbench are a powerful GUI(Graphical User Interface) with which to manipulate prosodic information and a function to synthesize speech in trial-and- error manner. An evaluation by creating speech messages shows the good performance of the workbench.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-204"
  },
  "banga97_eurospeech": {
   "authors": [
    [
     "Eduardo R.",
     "Banga"
    ],
    [
     "Carmen",
     "Garcia-Mateo"
    ],
    [
     "Xavier",
     "Fernandez-Salgado"
    ]
   ],
   "title": "Shape-invariant prosodic modification algorithm for concatenative text-to-speech synthesis",
   "original": "e97_0545",
   "page_count": 4,
   "order": 205,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "Concatenative text-to-speech systems require an algorithm that allows prosodic modifications of the speech units during the concatenation process. Nowadays, sinusoidal modeling seems to be a promising technique to achieve very flexible algorithms that provide high quality synthetic speech. The main difficulty of these type of algorithms is the treatment of the phase information, since an inadequate processing of this information gives rise to reverberation and audible artefacts. In this contribution we discuss the application of a shape-invariant sinusoidal model [1] to a text-to-speech system based on concatenation of speech units.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-205"
  },
  "hwang97b_eurospeech": {
   "authors": [
    [
     "Shaw-Hwa",
     "Hwang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ],
    [
     "Saga",
     "Chang"
    ]
   ],
   "title": "An RNN-based spectral information generation for Mandarin text-to-speech",
   "original": "e97_0549",
   "page_count": 3,
   "order": 206,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "In this paper, an RNN-based spectral model is proposed to generate spectral parameters for Mandarin text-to-speech(TTS). The RNN is employed to learn the relations between the linguistic features and the spectral parameters. The phoneme-to-spectral parameter rules and the coarticulation rules between each two adjacent phones are automatically learned and memorized into the weights of RNN. The synthesized speech sounds more fluent and smooth. The RNN is divided into two parts. The first part is synchronized with syllable and is expected to simulate the phoneme-to-spectral parameter rules. The second part is synchronized with frame and is expected to simulate the coarticulation rules between each two adjacent phones. The line spectrum pair(LSP) parameters and the normalized energy contour are taken as target value. Training with large database, the synthetic LSP and energy contour match to the original LSP and energy contours quite well. Moreover, an RNN-based prosodic model which was proposed in our previous study was combined to the spectral model to efficiently simulate the spectral and prosodic information generation. Lastly, the LPC-based Mandarin TTS is implemented to examine the performance of our spectral model. The synthetic speech sounds fluent and natural. The coarticulation effect between each two adjacent phones which makes synthesized speech sounds un- fluent and echo-like was improved. However, due to the simple structure of LPC-based synthesizer, the clarity of synthetic speech can be improved by using the other spectral parameter as target value. For example, the modify mel-cepstrum parameter[5, 6, 7] or the FFT- based spectral parameter can also be learned by RNN and synthesizes more clarity speech. This is a initial work on the RNN-based spectral model for text-to-speech. Some advantages of our spectral model can be found. First, large memory space of synthesis unit in traditional TTS is replaced by small memory space of RNN's weights. Second, the coarticulation effect can be alleviated and produces more fluent speech. Third, the RNN-based prosodic and spectral information generator[8, 9] can be easily combined to formed a more compact RNN-based TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-206"
  },
  "santen97b_eurospeech": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Adam L.",
     "Buchsbaum"
    ]
   ],
   "title": "Methods for optimal text selection",
   "original": "e97_0553",
   "page_count": 4,
   "order": 207,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "Construction of both text-to-speech synthesis (TTS) and au-tomatic speech recognition (ASR) systems involves usage of speech data bases. These data bases usually consist of read text, which means that one has significant control over the content of the data base. Here we address how one can take advantage of this control, by discussing a number of variants of \"greedy\" text selection methods and showing their application in a variety of examples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-207"
  },
  "gimenezdelosgalanes97_eurospeech": {
   "authors": [
    [
     "Francisco M.",
     "Gimenez de los Galanes"
    ],
    [
     "David",
     "Talkin"
    ]
   ],
   "title": "High resolution prosody modification for speech synthesis",
   "original": "e97_0557",
   "page_count": 4,
   "order": 208,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "In this paper we will introduce RTIPS, a system for arbitrary high-resolution modification of the prosodic variables of speech: fundamental frequency, rhythm (segmental duration) and intensity. It is based on the Resample and ovelap-add (R-OLA) algorithm for fundamental frequency and duration modification of speech. The algorithm works pitch-synchronously in order to accurately modify the pitch contour, and it uses estimates of the glottal closure instants (epochs) as the synchronism marks. This technique is very similar to other OLA-based methods for time or pitch modification, but because of the introduction of the resampling step, voice quality (especially for high-pitched voices) is much more natural after resynthesis, at any given output sampling frequency. The reliability of the R-OLA algorithm is highly depen- dent on the accuracy of the method used for epoch detection, so this preprocessing step has to be carefully designed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-208"
  },
  "karaali97_eurospeech": {
   "authors": [
    [
     "Orhan",
     "Karaali"
    ],
    [
     "Gerald",
     "Corrigan"
    ],
    [
     "Ira",
     "Gerson"
    ],
    [
     "Noel",
     "Massey"
    ]
   ],
   "title": "Text-to-speech conversion with neural networks: a recurrent TDNN approach",
   "original": "e97_0561",
   "page_count": 4,
   "order": 209,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "This paper describes the design of a neural network that performs the phonetic-to-acoustic mapping in a speech synthesis system. The use of a time-domain neural network architecture limits discontinuities that occur at phone boundaries. Recurrent data input also helps smooth the output parameter tracks. Independent testing has demonstrated that the voice quality produced by this system compares favorably with speech from existing commercial text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-209"
  },
  "hogberg97_eurospeech": {
   "authors": [
    [
     "Jesper",
     "Högberg"
    ]
   ],
   "title": "Data driven formant synthesis",
   "original": "e97_0565",
   "page_count": 4,
   "order": 210,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "In this study we introduce combined data driven and rule based methods to synthesise speech. The aim is to improve on the coarticulatory modelling by adapting the KTH TTS system to data from one speaker. Regression trees are trained on a manually corrected speech database to provide predictions for vowel formant frequencies. At runtime, the TTS system produces formant frequency trajectories that are derived from weighted contributions from both the rules and the regression trees. The weighting strategy allows flexible adjustment of the synthesis parameters and thus of the quality of the output speech. An informal perceptual test was conducted to compare the performance of the hybrid approach to that of the traditional rule based system. A great majority of the test subjects judged the speech output of the hybrid system to be more natural than the competing rule derived speech. The speech produced by the hybrid system was also generally preferred.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-210"
  },
  "king97_eurospeech": {
   "authors": [
    [
     "Simon",
     "King"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Florian",
     "Höfer"
    ]
   ],
   "title": "Speech synthesis using non-uniform units in the Verbmobil project",
   "original": "e97_0569",
   "page_count": 4,
   "order": 211,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "We describe a concatenative speech synthesiser for British English which uses the HADIFIX [8] inventory structure originally developed for German by Portele. An inventory of non-uniform units was investigated with the aim of improving segmental quality compared to diphones. A combination of soft (diphone) and hard concatenation was used, which allowed a dramatic reduction in inventory size. We also present a unit selection algorithm which selects an optimum sequence of units from this inventory for a given phoneme sequence. The work described is part of the concept-to-speech synthesiser for the language and speech project Verbmobil which is funded by the German Ministry of Science (BMBF).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-211"
  },
  "trancoso97_eurospeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "M. Ceu",
     "Vianna"
    ]
   ],
   "title": "On the pronunciation mode of acronyms in several European languages",
   "original": "e97_0573",
   "page_count": 4,
   "order": 212,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "The paper describes our research work concerning the pronunciation mode of acronyms in German, French, and Portuguese. Most of the rules are related with the well-formedness of the constituents and the minimum and maximum weight thresholds required for reading and spelling an acronym. The results of the tests for the three languages were considered very promising, reaching decision errors below 4%. The rule set was also applied to a very small English corpus, with relative success. We believe that further optimisation is still possible, if language specific parametrisation is taken into account, in particular for the languages where a limited corpus of acronyms was available.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-212"
  },
  "rietveld97_eurospeech": {
   "authors": [
    [
     "Toni",
     "Rietveld"
    ],
    [
     "Joop",
     "Kerkhoff"
    ],
    [
     "M. J. W. M.",
     "Emons"
    ],
    [
     "E.J.",
     "Meijer"
    ],
    [
     "Angelien A.",
     "Sanderman"
    ],
    [
     "Agaath M. C.",
     "Sluijter"
    ]
   ],
   "title": "Evaluation of speech synthesis systems for Dutch in tele-communication applications in GSM and PSTN networks",
   "original": "e97_0577",
   "page_count": 4,
   "order": 213,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "In this contribution the subjective evaluation of three Text-To-Speech systems (two diphone and one allophone system) is reported in three'transmission conditions: standard telephone (PSTN) and GSM. The three TTS-systems realised three different texts: Travel information, Stock Exchange Reports and E-mail messages. The subjects had to carry out three tasks: a) to give preference judgements on the three TTS-systems and b) to rate the readings on 16 five-point scales. The rankorder on the scale of general quality was: Public Transport Stock Exchange E-mail reading, in both transmission conditions. The GSM-transmission tends to decrease the perceptual scores on a number of subjective scales, In the transliteration task significantly more errors were made in the GSM- condition than in the PSTN-condition. In both conditions less errors were made with the diphone TTS-systems than with the allophone system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-213"
  },
  "angelini97_eurospeech": {
   "authors": [
    [
     "Bianca",
     "Angelini"
    ],
    [
     "Claudia",
     "Barolo"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "Stefano",
     "Sandri"
    ]
   ],
   "title": "Automatic diphone extraction for an Italian text-to-speech synthesis system",
   "original": "e97_0581",
   "page_count": 4,
   "order": 214,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "This paper describes a system for the automatic extraction of diphone units from given speech utterances. The method is based on an automatic phonetic segmentation and on a subsequent rule-driven diphone boundary detection. The phonetic segmenter, developed at IRST, was trained and tested both in speaker independent and speaker dependent mode. A rule formalism, involving acoustic parameters, arithmetical and logical operators, was defined to express the acoustic/phonetic knowledge acquired during previous experiences on manual diphone segmentation. A specialized tool for rule parsing was designed that processes a given sequence of automatically derived phone boundaries using a corresponding sequence of predefined acoustic parameters. Several sets of rules were developed that include both general principles and specific details concerning the content of the diphone database of \"Eloquens\"N, the CSELT text-to-speech synthesis system for the Italian language. The accuracy was evaluated by comparing the manual and the automatic segmentations of the speech utterances of a female speaker, resulting in nearly 95% of correct boundary position, given a tolerance of 20 ms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-214"
  },
  "keller97_eurospeech": {
   "authors": [
    [
     "Eric",
     "Keller"
    ]
   ],
   "title": "Simplification of TTS architecture vs. operational quality",
   "original": "e97_0585",
   "page_count": 4,
   "order": 215,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "Many applications in mobile telephony and portable computing require high-quality speech synthesis systems with a very modest computational footprint. Our text-to-speech system for French gives satisfactory performance in phonetisation and prosody with considerably reduced computational resources. Using the Mons (Belgium) diphone data base, the program's current version runs in real time on Pentium-type PCs or Mac PPCs. The code requires 442 k, minimum RAM requirement is 4700 k, the minimum disk requirement is 5560 k. The phonetisation and prosody processing has been brought to a first level of optimal compromise between quality and computational footprint. Major further reductions in space requirements would probably necessitate a re-evaluation of sound generation procedures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-215"
  },
  "fries97_eurospeech": {
   "authors": [
    [
     "Georg",
     "Fries"
    ],
    [
     "Antje",
     "Wirth"
    ]
   ],
   "title": "Felix - a TTS system with improved pre-processing and source signal generation",
   "original": "e97_0589",
   "page_count": 4,
   "order": 216,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "Felix is our recent PC-based TTS research-system for testing, analyzing, and evaluating TTS algorithms. The object-oriented interface allows efficient algorithm improvement and overall system prototyping by combining different modules. The re- sults of each TTS-processing step can be monitored and all kinds of data may be reviewed and modified. The paper will outline the algorithms currently implemented in the Felix system, focusing on lexical analysis, duration modeling, and source signal generation, where we suggest ways to improve intelligibility and naturalness of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-216"
  },
  "edgington97_eurospeech": {
   "authors": [
    [
     "Mike",
     "Edgington"
    ]
   ],
   "title": "Investigating the limitations of concatenative synthesis",
   "original": "e97_0593",
   "page_count": 4,
   "order": 217,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "Concatenative text-to-speech (TTS) systems are now quite widespread through the availability of simple time- domain speech modification algorithms. Many of these systems produce intelligible speech with a higher degree of naturalness than that achieved by the previous generation of formant synthesis systems. This perceived improvement in quality has lead to the view in some circles that TTS is a solved problem, at least for many practical applications. Three experiments are reported in this paper, all performed with a concatenative TTS system. These experiments investigated aspects of the concatenative model by respectively addressing copy synthesis of emotional speech, modelling glottalisation, and the effect of speech database design on the quality of synthesised speech. This paper suggests that the lack of an explicit speech model in most concatenative synthesis strategies fundamentally limits the usefulness of many current systems to the relatively restricted task of 'neutral' spoken renderings of text, where deficiencies in other system components usually mask the limitations of the synthesis strategy itself.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-217"
  },
  "teixeiradejesus97_eurospeech": {
   "authors": [
    [
     "Luis Miguel",
     "Teixeira de Jesus"
    ],
    [
     "Gavin C.",
     "Cawley"
    ]
   ],
   "title": "Speech coding and synthesis using parametric curves",
   "original": "e97_0597",
   "page_count": 4,
   "order": 218,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "Accurate modeling of co-articulation, the context- sensitive merging of the boundaries between allophones in continuous speech, is vital for natural sounding speech synthesis. This paper describes initial research investigating the use of Bezier Curves to form models of co- articulation in human speech. A 12th order, pitch synchronous line spectral pair (LSP) [1] analysis is performed on a corpus of 239 phonetically balanced sentences of English speech. The resulting data are divided to form an inventory of the diphones occurring in the speech database. The trajectory of each line spectral pair parameter through each diphone can then be represented by a single cubic Bezier curve segment, found using the Levenberg- Marquardt curve fitting method [2, 3]. Results are presented showing the accuracy of Bezier models of the coarticulation between different types of speech sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-218"
  },
  "black97_eurospeech": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Automatically clustering similar units for unit selection in speech synthesis",
   "original": "e97_0601",
   "page_count": 4,
   "order": 219,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "This paper describes a new method for synthesizing speech by concatenating sub-word units from a database of labelled speech. A large unit inventory is created by automatically clustering units of the same phone class based on their phonetic and prosodic context. The appropriate cluster is then selected for a target unit offering a small set of candidate units. An optimal path is found through the candidate units based on their distance from the cluster center and an acoustically based join cost. Details of the method and justification are presented. The results of experiments using two different databases are given, optimising various parameters within the system. Also a comparison with other existing selection based synthesis techniques is given showing the advantages this method has over existing ones. The method is implemented within a full text-to-speech system offering efficient natural sounding speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-219"
  },
  "jiang97_eurospeech": {
   "authors": [
    [
     "Li",
     "Jiang"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Improvements on a trainable letter-to-sound converter",
   "original": "e97_0605",
   "page_count": 4,
   "order": 220,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "Letter-to-sound (LTS) conversion is important for both text-to-speech (TTS) and automatic speech recognition (ASR). In this paper we discuss some improvements we have made on our trainable LTS converter. We use a classification and regression tree (CART) to automatically configure the most salient phonological rules needed for the LTS conversion. We address problems in growing multiple trees and use of phonotactic information for better generalization. The experiments were carried on both the NETTALK database and the CMU dictionary. With improved techniques, the conversion error rate at the phoneme level and word level was reduced by 15% and 20% respectively. For both tasks, the phoneme conversion error rate was reduced to about 8%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-220"
  },
  "bae97_eurospeech": {
   "authors": [
    [
     "Myungjin",
     "Bae"
    ],
    [
     "Kyuhong",
     "Kim"
    ],
    [
     "Woncheol",
     "Lee"
    ]
   ],
   "title": "On a cepstral pitch alteration technique for prosody control in the speech synthesis system with high quality",
   "original": "e97_0609",
   "page_count": 4,
   "order": 221,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "In the area of the speech synthesis techniques, the waveform coding methods maintain the intelligibility and naturalness of synthetic speech. In order to apply the waveform coding techniques to synthesis by rule, we must be able to alter the pitches of synthetic speech. In this paper, we propose a new pitch altering method that compensates phase distortion of the cepstral pitch alteration method with time scaling method in the time domain. This method can remove some spectrum distortion which is occurred in conjunction point between the waveforms. Also, we can obtain little spectrum distortion below 1.18% for pitch alteration of 200%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-221"
  },
  "stylianou97_eurospeech": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Thierry",
     "Dutoit"
    ],
    [
     "Juergen",
     "Schroeter"
    ]
   ],
   "title": "Diphone concatenation using a harmonic plus noise model of speech",
   "original": "e97_0613",
   "page_count": 4,
   "order": 222,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "In this paper we present a high-quality text-to-speech system using diphones. The system is based on a Harmonic plus Noise (HNM) representation of the speech signal. HNM is a pitch-synchronous analysis-synthesis system but does not require pitch marks to be determined as necessary in PSOLA-based methods. HNM assumes the speech signal to be composed of a periodic part and a stochastic part. As a result, different prosody and spectral envelope modification methods can be applied to each part, yielding more natural-sounding synthetic speech. The fully parametric representation of speech using HNM also provides a straightforward way of smoothing diphone boundaries. Informal listening tests, using natural prosody, have shown that the synthetic speech quality is close to the quality of the original sentences, without smoothing problems and without buzziness or other oddities observed with other speech representations used for TTS.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-222"
  },
  "sabah97_eurospeech": {
   "authors": [
    [
     "Gérard",
     "Sabah"
    ]
   ],
   "title": "The \"sketchboard\": a dynamic interpretative memory and its use for spoken language understanding",
   "original": "e97_0617",
   "page_count": 4,
   "order": 223,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "Blackboards allow various knowledge sources to be triggered in an opportunistic way, but does not allow higher modules to feedback information to lower level modules. The solution presented here remedies this shortcoming, since our Sketchboard implements reactive feedback loops. Within the Sketchboard, modules are considered from two points of view: either they build a result (a sketch, possibly rough and vague) or they give back a response to the modules from which they received their input data. This response signals the degree of confidence the module has towards its own result. These relations are generalized across all the modules that interact when solving a problem. As higher and higher level modules are triggered, the initial sketch become more and more precise, taking into account the higher modules knowledge. Conceived for natural language processing, the Sketchboard is also useful for spoken language understanding as shown by a detailed example.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-223"
  },
  "zhou97_eurospeech": {
   "authors": [
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Wu",
     "Chou"
    ],
    [
     "Andrew",
     "Pargellis"
    ]
   ],
   "title": "Speech technology integration and research platform: a system study",
   "original": "e97_0621",
   "page_count": 4,
   "order": 224,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "We present a generic speech technology integration platform for application development and research across different domains. The goal of the design is two-fold: On the application development side, the system provides an intuitive developer's interface defined by a high level application definition language and a set of convenient speech application building tools. It allows a novice developer to rapidly deploy and modify a spoken language dialogue application. On the system research and development side, the system uses a thin, 'broker' layer to separate the system application programming interface from the service provider interface. It makes the system easy to incorporate new technologies and new functional components. We also use a domain independent acoustic model set to cover US English phones for general speech applications. The system grammar and lexicon engine creates grammars and lexicon dictionaries on the fly to enable a practically unrestricted vocabulary for many recognition and synthesis applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-224"
  },
  "geller97_eurospeech": {
   "authors": [
    [
     "Dieter",
     "Geller"
    ],
    [
     "Markus",
     "Lieb"
    ],
    [
     "Wolfgang",
     "Budde"
    ],
    [
     "Oliver",
     "Muelhens"
    ],
    [
     "Manfred",
     "Zinke"
    ]
   ],
   "title": "Speech recognition on SPHERIC - an IC for command and control applications",
   "original": "e97_0625",
   "page_count": 4,
   "order": 225,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "SPHERIC is a new IC that has been designed specially for automatic speech recognition applications in Consumer Electronics with a vocabulary of up to 126 words. It allows real-time recognition of both speaker dependent and independent words spoken continuously or in an isolated way. Key word spotting and playback of coded messages and user trained words are additional features. After a short system overview the hardware architecture and software structure are presented in this paper. The techniques for reducing computation time and necessary memory size are examined in more detail. Finally, the implemented speech recognition algorithm is described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-225"
  },
  "mccandless97_eurospeech": {
   "authors": [
    [
     "Michael K.",
     "McCandless"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "MUSE: a scripting language for the development of interactive speech analysis and recognition tools",
   "original": "e97_0629",
   "page_count": 4,
   "order": 226,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "Speech research is a complex endeavor, as reflected in the numerous tools and specialized languages the modern researcher needs to learn. These tools, while adequate for what they have been designed for, are difficult to customize or extend in new directions, even though this is often required. We feel this situation can be improved and propose a new scripting language, MUSE, designed explicitly for speech research, in order to facilitate exploration of new ideas. MUSE is designed to support many modes of research from interactive speech analysis through compute-intensive speech understanding systems, and has facilities for automating some of the more difficult requirements of speech tools: user interactivity, distributed computation, and caching. In this paper we describe the design of the MUSE language and our current prototype MUSE interpreter.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-226"
  },
  "witt97_eurospeech": {
   "authors": [
    [
     "Silke",
     "Witt"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Language learning based on non-native speech recognition",
   "original": "e97_0633",
   "page_count": 4,
   "order": 227,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "This work presents methods of assessing non-native speech to aid computer-assisted pronunciation teaching. These methods are based on automatic speech recognition (ASR) techniques using Hidden Markov Models. Confidence scores at the phoneme level are calculated to provide detailed information about the pronunciation quality of a foreign language student. Experimental results are given based on both artificial data and a database of non-native speech, the latter being recorded specifically for this purpose. The presented results demonstrate the metrics' capability to locate and assess mispronunciations at the phoneme level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-227"
  },
  "kilian97_eurospeech": {
   "authors": [
    [
     "Ute",
     "Kilian"
    ],
    [
     "Klaus",
     "Bader"
    ]
   ],
   "title": "Task modelling by sentence templates",
   "original": "e97_0637",
   "page_count": 4,
   "order": 228,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "Speech recognition applications always face the problem of changing vocabulary and functionality. The use of speech recognition systems will become more attractive if the system user is able to define or redefine the task himself in a suitable manner. Modelling a new task normally requires the experience of a human expert and a lot of time. Aditionally, the expert always has to be contacted if system changes become necessary. In this paper we present a fully operational system for continuous speech recognition with a powerful user interface. Most of the internal aspects of the speech recognition system are hidden. The task may be divided into different subtasks corresponding to dialogue states. Each subtask is defined by a set of expected user utterances based on sentence templates. This definition is automatically transformed into a lexicon and a language model used by the speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-228"
  },
  "kitaazawa97_eurospeech": {
   "authors": [
    [
     "Shigeyoshi",
     "Kitaazawa"
    ],
    [
     "Hideya",
     "Ichikawa"
    ],
    [
     "Satoshi",
     "Kobayashi"
    ],
    [
     "Yukihiro",
     "Nishinuma"
    ]
   ],
   "title": "Extraction and representation rhythmic components of spontaneous speech",
   "original": "e97_0641",
   "page_count": 4,
   "order": 229,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "Speech speed is measured and displayed with our specific algorithm TEMAX (Temporal Evaluation and Measurement Algorithm by KS). The TEMAX-gram, a sonagraphic output of speech envelope, the DFT using a 1-second window is convenient to set off isosyllabic characteristics. For Japanese traces 2 dark bars, called rhythmic formants: RF1 and RF2: the first one, around 8 Hz, and the second one, at halfway. RF1 corresponds to speech rate, RF2 represents the bimoraic rhythmic foot. As far as English, its isochronic characteristics are observable with a 2-seconds window as RF1. Furthermore, using a 1-second window the periodicity of syllables between stress is displayed as RF2.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-229"
  },
  "kim97d_eurospeech": {
   "authors": [
    [
     "Yoon",
     "Kim"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ]
   ],
   "title": "Automatic pronunciation scoring of specific phone segments for language instruction",
   "original": "e97_0645",
   "page_count": 4,
   "order": 230,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "The aim of the work described in this paper is to develop methods for automatically assessing the pronunciation quality of specific phone segments uttered by students learning a foreign language. From the phonetic time alignments generated by SRI's Decipher^TM HMM- based speech recognition system, we use various probabilistic models to produce pronunciation scores for the phone utterance. We evaluate the performance of the proposed algorithms by measuring how well the machine-produced scores correlate with human judgments on a large database. Of the various algorithms considered, the one based on phone log-posterior-probability produced the highest correlation (r xy = 0.72) with the human ratings, which was comparable with correlations between human raters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-230"
  },
  "ronen97_eurospeech": {
   "authors": [
    [
     "Orith",
     "Ronen"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Automatic detection of mispronunciation for language instruction",
   "original": "e97_0649",
   "page_count": 4,
   "order": 231,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "This work is part of a project aimed at developing a speech recognition system for language instruction that can assess the quality of pronunciation, identify pronunciation problems, and provide the student with accurate feedback about specific mistakes. Previous work was mainly concerned with scoring the quality of pronunciation. In this work we focus on automatic detection of mispronunciation. While scoring quantifies the mispronunciation, detection identifies the occurrence of a specific problem. Detecting pronunciation problems is necessary for providing feedback to the student. We use pronunciation scoring techniques to evaluate the performance of our mispronunciation model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-231"
  },
  "alvarez97_eurospeech": {
   "authors": [
    [
     "Agustin",
     "Alvarez"
    ],
    [
     "Rafael",
     "Martinez"
    ],
    [
     "Victor",
     "Nieto"
    ],
    [
     "Victoria",
     "Rodellar"
    ],
    [
     "Pedro",
     "Gomez"
    ]
   ],
   "title": "Continuous formant-tracking applied to visual representations of the speech and speech recognition",
   "original": "e97_0653",
   "page_count": 4,
   "order": 232,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "Through the present paper, a methodology to create Visual Representations of Speech for Speech Perception Enhancement Applications, based on the use of a Continuous Formant- Tracking Algorithm, is presented. The specific mathematical and computational issues introduced for such treatment are given, and a specific case for Computer-Aided Language Learning oriented to the Phonetic Specificities of English for Spanish Speakers is also presented. This specific technique may also be used in statistically normalizing Speech Data for Speech Recognition Systems. In this context, an example of a Robust to Noise Speech Recognizer, which uses Eormant Dynamic Information is shown.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-232"
  },
  "kawai97_eurospeech": {
   "authors": [
    [
     "Goh",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A CALL system using speech recognition to train the pronunciation of Japanese long vowels, the mora nasal and mora obstruents",
   "original": "e97_0657",
   "page_count": 5,
   "order": 233,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "We developed a CALL (computer-aided language learning) system for teaching the pronunciation of Japanese long vowels, the mora nasal and mora obstruents to non-native speakers of Japanese. Long vowels and short vowels are spectrally almost identical but their phone durations differ significantly. Similar conditions exist between mora nasals and non-mora nasals, and between mora and non-mora obstruents. Our system uses speech recognition to measure the durations of each phone and compares them with distributions of native speakers while correcting for different speech rates. Results show that learners quickly capture the relevant duration cues. The amount of learning time spent on acquiring these durational skills is well within the time constraints of TJSL (teaching Japanese as a second language) curricula.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-233"
  },
  "nouza97_eurospeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Miroslav",
     "Holada"
    ],
    [
     "Daniel",
     "Hajek"
    ]
   ],
   "title": "An educational and experimental workbench for visual processing of speech data",
   "original": "e97_0661",
   "page_count": 4,
   "order": 234,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "In the article the focus is put on educational aspects of the speech processing science. A set of tools that have been developed with the aim at presenting, visualizing and explaining basic topics of speech recognition is described. The set consists of programs, like a signal analysis unit, a dynamic time warping algorithm (DTW) explorer and hidden Markov model (HMM) investigation tools, that are integrated into a single environment and allow for easy and highly illustrative learning through experiments with real speech data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-234"
  },
  "choi97b_eurospeech": {
   "authors": [
    [
     "Yong-Soo",
     "Choi"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Sung-Youn",
     "Kim"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "A 3 channel digital CVSD bit-rate conversion system using a general purpose DSP",
   "original": "e97_0665",
   "page_count": 4,
   "order": 235,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "This paper presents a bit-rate conversion system for an efficient communication between two CVSD systems with different bit-rates. To ensure the robustness to external noises, the presented system is implemented in digital domain using a general purpose digital signal processor (DSP). In order to overcome the problems caused by different bit-rate and time-constants, several methods are considered in this study. In addition, a significant simplification of the system complexity is obtained by introducing the IIR filter to the decimation/interpolation process. The use of the IIR filter provides comptational advantages over the conversion system employing FIR filters, because the linear phase is not a critical issue in this application. By modifying the algorithm based on the IIR filter, a 3- channel full-duplex conversion algorithm was successfully implemented on a single DSP. Experimentals results are presented to exihibit the consistent and reliable performance of the bit-rate conversion system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-235"
  },
  "delmonte97_eurospeech": {
   "authors": [
    [
     "Rodolfo",
     "Delmonte"
    ],
    [
     "Mirela",
     "Petrea"
    ],
    [
     "Ciprian",
     "Bacalu"
    ]
   ],
   "title": "SLIM prosodic module for learning activities in a foreign language",
   "original": "e97_0669",
   "page_count": 4,
   "order": 236,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "The Prosodic Module of SLIM has been created in order to solve problems related to segmental and suprasegmental features of spoken English in a courseware for computer-assisted foreign language learning called SLIM - an acronym for Multimedia Interactive Linguistic Software, developed at the University of Venice. It is composed of two different sets of Learning Activities, the first one dealing with phonetic and prosodic problems at word segmental level, the second one dealing with prosodic problems at utterance suprasegmental level. The main goal of Prosodic Activities is to ensure feed-back to the student intending to improve his/her pronunciation in a foreign language. The programme works by comparing two signals, the master and the student ones, where the master has been previously edited by a human tutor inserting orthographic syllabic information at segmentation marks automatically computed by the underlying acoustic segmenter called Prosodics(see 1). When a student, after listening and evaluating the master signal tries to mimic the original utterance or word the system assigns a score and, if needed spots a mistake and indicates what it consists of. The elements of comparison are constituted by the acoustic correlates of prosodic features such as intonational contour, sentence accent and word stress, rhythm and duration at word and sentence level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-236"
  },
  "kaspar97_eurospeech": {
   "authors": [
    [
     "Bernhard",
     "Kaspar"
    ],
    [
     "Karlheinz",
     "Schuhmacher"
    ],
    [
     "Stefan",
     "Feldes"
    ]
   ],
   "title": "Barge-in revised",
   "original": "e97_0673",
   "page_count": 4,
   "order": 237,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "We consider speech dialogues, allowing for simultaneous input (via speech recognition) and output (via speech synthesis or pre-recorded prompts), often referred to as \"barge in\". We start with a collection of dialogue situations, where simultaneous input and output is useful. It is argued, that a variety of possible system behaviour is necessary in order to take into account these situations adequately. We then define a formalism, that allows to control this system behaviour. We end up with reporting some experience gathered both in lab tests and a in real world pilot.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-237"
  },
  "akbar97_eurospeech": {
   "authors": [
    [
     "Mohammad",
     "Akbar"
    ]
   ],
   "title": "Waveedit, an interactive speech processing environment for microsoft windows platform",
   "original": "e97_0677",
   "page_count": 4,
   "order": 238,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "This paper presents a new interactive speech processing environment designed for Microsoft Window platforms. It will be shown that how an integrated speech processing environment was made following Windows Interface Design Guidelines. The environment integrates many traditional time and frequency domain analysis algorithms as well as basic functions like recording, listening and labeling. Choosing Component Object Model (COM) as the architectural framework assures high maintainability, scripting capability and further expandability of this environment. Extensive use of the system in laboratory has shown how this interactive environment improves users performance in their every day speech processing tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-238"
  },
  "ehsani97_eurospeech": {
   "authors": [
    [
     "Farzad",
     "Ehsani"
    ],
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Amir",
     "Najmi"
    ],
    [
     "Ognjen",
     "Todic"
    ]
   ],
   "title": "Subarashii: Japanese interactive spoken language education",
   "original": "e97_0681",
   "page_count": 4,
   "order": 239,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "Subarashii is a system that uses automatic speech recognition (ASR) to offer first-level, computer-based exercises in the Japanese language for beginning high school students. Building the Subarashii system has identified strengths and limitations of ASR technology and has led to some novel methods in the development of materials for computer- based interactive spoken language education.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-239"
  },
  "goddeau97_eurospeech": {
   "authors": [
    [
     "David",
     "Goddeau"
    ],
    [
     "William",
     "Goldenthal"
    ],
    [
     "Chris",
     "Weikart"
    ]
   ],
   "title": "Deploying speech applications over the web",
   "original": "e97_0685",
   "page_count": 4,
   "order": 240,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "At Digital Equipment Corporation's Cambridge Research Lab (CRL), the Speech Interaction Group has been focusing on building speech applications for deployment over the World-Wide Web. Web-based speech applications require the browser to capture and transmit speech to remote servers for back-end processing, maintain application state, and present multi-media responses. This paper describes the group's strategy for delivering speech applications built around a mechanism, the digital Voice Plugin, for capturing and transmitting audio from a browser. It describes a conversational application implemented within this framework and discusses the problems of delivering these systems on the Web. In addition, we brie y touch upon some other Web-based speech applications that have been developed at CRL.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-240"
  },
  "schalkwyk97_eurospeech": {
   "authors": [
    [
     "Johan",
     "Schalkwyk"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "Sarel van",
     "Vuuren"
    ],
    [
     "Pieter",
     "Vermeulen"
    ]
   ],
   "title": "CSLUsh: an extendible research environment",
   "original": "e97_0689",
   "page_count": 4,
   "order": 241,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "The CSLU shell (CSLUsh), is a collection of modular building blocks which aim to provide the user with a powerful, extendible, research, development and implementation environment. Implemented in C with standardized Tcl/Tk interfaces to provide a scripting and visualization environment, it allows a exible cast for both research algorithms and system deployment. This shell is the architecture on which the CSLU Toolkit is built and may be downloaded for non-commercial use from http://www.cse.ogi.edu/CSLU/toolkit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-241"
  },
  "ferenczi97_eurospeech": {
   "authors": [
    [
     "Tibor",
     "Ferenczi"
    ],
    [
     "Geza",
     "Nemeth"
    ],
    [
     "Gabor",
     "Olaszy"
    ],
    [
     "Zoltan",
     "Gaspar"
    ]
   ],
   "title": "A flexible client-server model for multilingual CTS/TTS development",
   "original": "e97_0693",
   "page_count": 4,
   "order": 242,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "The efficiency of the development of CTS/TTS systems is influenced by the features and services of the software development tools used in the development process. A development system should be highly flexible, informative and user friendly to fulfil all or almost all the requirements the researcher could have. In this paper we present a development systenr, MVoxDev, that can provide an informative and flexible environment for the development of multilingual CTS/TTS systents. The development system gives aid to inspect and modify all the constituent parts of the CTS/TTS system as a client of the developed CTS/TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-242"
  },
  "laine97_eurospeech": {
   "authors": [
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Critically sampled PR filterbanks of nonuniform resolution based on block recursive FAMlet transform",
   "original": "e97_0697",
   "page_count": 4,
   "order": 243,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "A new block recursive algorithm is introduced for effective FAMlet transform implementation. When the Fourier transform is combined with the algorithm a nonuniform resolution filterbank is created. The algorithm allows to approximate frequency resolutions of any type, the ERB-rate scale included. The signals can be vector based critically down sampled which allows a perfect reconstruction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-243"
  },
  "minematsu97_eurospeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Nariaki",
     "Ohashi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Automatic detection of accent in English words spoken by Japanese students",
   "original": "e97_0701",
   "page_count": 4,
   "order": 244,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "Acoustic realization of word accent differs among languages. While, in Japanese, it is fully represented by an F 0 contour of a word, English word accent is characterized by power, duration, F 0 , vowel quality and so forth. In addition to the difference in syllable structure between the two languages, that in word accent makes it even more difficult for Japanese students to master correct pronunciation of English words. It indicates that the development of an automatic evaluation method of English word accent, as one of English teaching tools, will be helpful especially to Japanese students. In this paper, as the first step to the development, a detection method of accent in English words spoken by Japanese is proposed, where syllable-size HMMs are built using positional information of the syllables and adequately detected syllable boundaries are used for the detection. Results of accent detection experiments show 90 % and 93 % as detection rates of Japanese students and native speakers respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-244"
  },
  "taniguchi97b_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Taniguchi"
    ],
    [
     "Allan A.",
     "Reyes"
    ],
    [
     "Hideyuki",
     "Suzuki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "An English conversation and pronunciation CAI system using speech recognition technology",
   "original": "e97_0705",
   "page_count": 4,
   "order": 245,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper describes an English conversation and pronunciation CAI using speech recognition techniques. This system was intended to recognize user's utterances and to respond to him properly according to the recognized results. In the case of a learner with unskilled pronunciation, because of differences in the phonemic system between his mother tongue and the second language, the speech recognition system cannot run normally. After this improvement, evaluation experiments were conducted. The results indicate that learners' ability in speaking and in listening to English is improved by using the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-245"
  },
  "sutton97_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Sutton"
    ],
    [
     "Ed",
     "Kaiser"
    ],
    [
     "A.",
     "Cronk"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "Bringing spoken language systems to the classroom",
   "original": "e97_0709",
   "page_count": 4,
   "order": 246,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "Currently, there are few opportunities for people to learn about and experiment with the latest spoken language technology. Furthermore, most research and development activities are restricted to a handful of academic and industrial labs. In order to make the technology less exclusive, it must become more accessible to the general population. This is now feasible with the development of the CSLU Toolkit which combines easy-to-use authoring tools with state-of- the-art human language technology. In this paper, we focus on the educational role of the toolkit and describe how it is being used in several local schools.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-246"
  },
  "cucchiarini97_eurospeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Automatic assessment of foreign speakers' pronunciation of dutch",
   "original": "e97_0713",
   "page_count": 4,
   "order": 247,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "The aim of the research reported on here is to develop a system for automatic assessment of foreign speakers' pronunciation of Dutch. In this paper similar studies carried out for English are first examined. Subsequently, suggestions are made for partly improving the methodology that is usually adopted in research on automatic pronunciation assessment. Finally, an experiment is presented in which automatic scores of telephone speech produced by native and nonnative speakers are compared with scores assigned by human raters. The approach used in this experiment is compared with those of previous studies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-247"
  },
  "holzrichter97_eurospeech": {
   "authors": [
    [
     "John F.",
     "Holzrichter"
    ],
    [
     "Greg C.",
     "Burnett"
    ]
   ],
   "title": "Use of low power EM radar sensors for speech articulator measurements",
   "original": "e97_0717",
   "page_count": 4,
   "order": 248,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "Very low power electromagnetic (EM) wave sensors are being used to measure speech articulator motions such as the vocal fold oscillations, jaw, tongue, and the soft palate. Data on vocal fold motions, that correlate well with established laboratory techniques, as well as data on the jaw, tongue and soft palate are shown The vocal fold measurements together with a Volume air flow model are being used to perform pitch synchronous estimates of the voiced transfer functions using ARMA techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-248"
  },
  "epps97_eurospeech": {
   "authors": [
    [
     "Julien",
     "Epps"
    ],
    [
     "Annette",
     "Dowd"
    ],
    [
     "John",
     "Smith"
    ],
    [
     "Joe",
     "Wolfe"
    ]
   ],
   "title": "Real time measurements of the vocal tract resonances during speech",
   "original": "e97_0721",
   "page_count": 4,
   "order": 249,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "The formants of speech sounds are usually attributed to resonances of the vocal tract. Formant frequencies are usually estimated by inspection of spectrograms or by automated techniques such as linear prediction. In this paper we measure the frequencies of the first two resonances of the vocal tract directly, in real time, using acoustic impedance spectrometry. The vocal tract is excited by a carefully calibrated, broad band, acoustic current signal applied outside the lips while the subject is speaking. The sound pressure response is analysed to give the resonant frequencies. We compare this new method (Real- time Acoustic Vocal tract Excitation or RAVE) with linear prediction and we report the vocal tract resonances for eleven vowels of Australian English. We also report preliminary results of using feedback from vocal tract excitation as a speech trainer, and its effect on improving the pronunciation of foreign vowel sounds by monolingual anglophones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-249"
  },
  "cavalcantealbano97_eurospeech": {
   "authors": [
    [
     "Eleonora",
     "Cavalcante Albano"
    ],
    [
     "Patricia",
     "Aparecida Aquino"
    ]
   ],
   "title": "Linguistic criteria for building and recording units for concatenative speech synthesis in brazilian portuguese",
   "original": "e97_0725",
   "page_count": 4,
   "order": 250,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "A unit inventory for concatenative speech synthesis in Brazilian Portuguese was built on the basis of an analysis of segment-prosody interactions. Segments are viewed as full or reduced depending on stress, syllable structure and phonological boundaries. Demisyllabic units preserve the integrity of segments reduced due to syllable structure. Intersyllabic units preserve the integrity of segments reduced due to stress and boundaries. Integrity of vowel clusters is also preserved, but nasal vowels and diphthongs are successfully concatenated to oral onsets. The resulting units were recorded in carrier words and sentences designed on phonotactic and grammatical grounds. Good quality concatenation is achieved even before the addition of prosodic treatment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-250"
  },
  "kvale97_eurospeech": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ],
    [
     "Arne Kjell",
     "Foldvik"
    ]
   ],
   "title": "four-and-twenty, twenty-four. what's in a number?",
   "original": "e97_0729",
   "page_count": 4,
   "order": 251,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "This paper investigates \"what's in a number\", i.e. how natural numbers are pronounced in several European languages. As regards reading numbers above 20, 29 languages read the decade first and then the digit, e.g. twenty-four, and 10 languages read the digit before the decade, e.g. four-and-twenty. Two languages, Norwegian and Czech, use both systems, and 9 languages use (partly) a vigesimal system. An analysis of the Norwegian part of the European SpeechDat database showed that reading the decade first is used more in formal than in non-formal (spontaneous) speech and that typographic layout of digits influenced the reading of them.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-251"
  },
  "moraes97_eurospeech": {
   "authors": [
    [
     "Joao Antonio de",
     "Moraes"
    ]
   ],
   "title": "Vowel nasalization in Brazilian Portuguese: an articulatory investigation",
   "original": "e97_0733",
   "page_count": 4,
   "order": 252,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "This study investigates, from an articulatory point of view, the extent to which the different nasalization processes in Brazilian Portuguese (BP) - phonemic, allophonic and coarticulatory - are the result of phonological, language specific rules or a purely phonetic transitional phenomenon between an oral vowel and a nasal consonant. The study revealed that the magnitude of the velic gestures is similar in phonemic and in allophonic nasalization, which suggests that both processes are the result of the application of phonological nasalization rules in BP. On the other hand, in coarticulatory nasalization the degree of velic opening reached during the vowel is smaller, suggesting that in this case we have a purely transitional, coarticulatory phenomenon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-252"
  },
  "steriopolo97_eurospeech": {
   "authors": [
    [
     "Elena",
     "Steriopolo"
    ]
   ],
   "title": "Rhythmic organization pecularities of the spoken text",
   "original": "e97_0737",
   "page_count": 2,
   "order": 253,
   "p1": "737",
   "pn": "738",
   "abstract": [
    "Having analyzed various peculiarities of phonetic word formation and speech, we established that phonetic words are constructed according to specific laws. It pertains to the phonetic word formation patterns and the parts of speech adherents of both stressed word and its proclitic parts. The phonetic image of the spoken text description units practically corresponds to the phonetic nature of the traditional concept of word as a lexical unit, containing unstressed syllables differently structured in several hierarchies, and the main stressed syllable, the position of which determines the phonetic word accentual type.\n",
    ""
   ]
  },
  "rueber97_eurospeech": {
   "authors": [
    [
     "Bernhard",
     "Rueber"
    ]
   ],
   "title": "Obtaining confidence measures from sentence probabilities",
   "original": "e97_0739",
   "page_count": 4,
   "order": 254,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "The paper addresses the issue whether the \"probabilities\" delivered by a speech recognizer can be directly used as a measure for the confidence of the recognition. As current recognizers have to commit a lot of modelling assumptions and because of estimation problems due to sparse data this certainly is questionable. Nevertheless, this investigation shows, in the framework of recognizing semantic items in the Philips automatic telephone exchange board system PADIS, that there exists a useful correlation between probabilities and confidences. The method proposed works out as a generalization of the more standard method of using likelihood ratios between the first- and second-best recognition path. It offers as distinct advantages a) the integration of all available knowledge sources, and b) the direct and theoretically sound computation of confidence measures on all levels of interest.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-253"
  },
  "zu97_eurospeech": {
   "authors": [
    [
     "Yiqing",
     "Zu"
    ]
   ],
   "title": "Sentence design for speech synthesis and speech recognition database by phonetic rules",
   "original": "e97_0743",
   "page_count": 4,
   "order": 255,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "This paper describes the processing of 2465 sentences (or utterences) which are collected by phonetical rules from a big corpus--recent years' newspaper, \"People's Daily\" and etc., as materials of speech recognition and speech synthesis database. In these sentences, both phonetic phenomena and sentence patterns are included. We first consider the phonetic distribution among syllables: inter-syllabic diphones, inter-syllabic triphones and final-initial structure. The syllabic balance ensures the intra-syllabic phenomena such as phonemes, initial/final and consonant/vowel. There are roughly 17 kinds of sentence patterns which appear in our sentence set. We have also created a set of phonetically balanced 2-4 syllable phrases which includes all of the tone structures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-254"
  },
  "draxler97_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Susanne",
     "Burger"
    ]
   ],
   "title": "Identification of regional variants of high German from digit sequences in German telephone speech",
   "original": "e97_0747",
   "page_count": 4,
   "order": 256,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "From the German SpeechDat(M) database of telephone speech the digit sequences items that were spoken as chains of individual digits were extracted. From these digit strings, a subset of 39 strings was selected by dialect experts and according to the region information provided by the speaker. The German federal states were used as region classes because this information can easily be provided by the speaker. 7 test persons were asked to listen to the subset of digit strings and to classify them by region. It was found that the overall success rate for the classification is 40%; if the regions neighboring the correct region are also counted as correct, the success rate is 68%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-255"
  },
  "kavitskaya97_eurospeech": {
   "authors": [
    [
     "Darya",
     "Kavitskaya"
    ]
   ],
   "title": "Aerodynamic constraints on the production of palatalized trills: the case of the Slavic trilled [r]",
   "original": "e97_0751",
   "page_count": 4,
   "order": 257,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "Production of a trill depends on several articulatory and aerodynamic constraints. These constraints can be held responsible for various sound changes in Slavic languages which all involve depalatalization or frication of Proto-Slavic palatalized trilled r. As soon as a trill is affected by palatalization, the aerodynamic conditions are changed and the possibility of trill production lowers. Small deviations in aperture size and air velocity can lead to a failure of a trill. This paper proposes a phonetic explanation for the depalatalization and/or frication of the Proto-Slavic palatalized trilled r by considering the detrimental effects of articulatory and aerodynamic constraints on the production of a palatalized trill.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-256"
  },
  "seong97_eurospeech": {
   "authors": [
    [
     "Cheol-jae",
     "Seong"
    ],
    [
     "Sanghun",
     "Kim"
    ]
   ],
   "title": "An experimental phonetic study of the interrelationship between prosodic phrase and syntactic structure",
   "original": "e97_0755",
   "page_count": 4,
   "order": 258,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "The boundaries found in the target 100 travelling domain dialogue sentences were labelled automatically according to the relative 9-stepped phonetic depth. The text database was also tagged with syntactic information. Having established 4 kinds of acoustic features, we arranged the prosodic aspect which can be depicted as a continuous change of duration and intonation across the penultimate, boundary, and post-boundary syllables along the X-Y two dimensional scale. Majority of the syntactic pairs seemed to have a characteristic that the intonation tends to fluctuate from rising to falling and, simultaneously, the duration showed of a short-long-short or a long-short-short pattern in the same syllable string of penultimate- boundary-post_boundary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-257"
  },
  "heid97_eurospeech": {
   "authors": [
    [
     "Sebastian J. G. G.",
     "Heid"
    ]
   ],
   "title": "Individual differences between vowel systems of German speakers",
   "original": "e97_0759",
   "page_count": 4,
   "order": 259,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "Based on formant measurements of more than 10000 vowels from 16 German speakers, vowel quality differences between the speakers have been analyzed. The main result of this investigation is that different speakers show not only different formant values (as one would expect due to individual differences), but exhibit different arrangements in their vowel systems. These differences are demonstrated by examples from the general distribution of vowels, the structure of vowel prototypes, and the differing number of degrees of tongue height for different speakers. The problem of vowel normalization for that data will also be demonstrated and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-258"
  },
  "batliner97_eurospeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Tempo and its change in spontaneous speech",
   "original": "e97_0763",
   "page_count": 4,
   "order": 260,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "In this paper, we give a first account of speech tempo and its change in spontaneous speech in a very large data base (Verbmobil, i.e., human-human appointment dialogs). As features representing speech tempo, we computed mean normalized speech duration (speaking rate) and normalized phone duration in different ways. The importance of these features is evaluated with an automatic classification of boundaries and accents where different sets of prosodic features (including also information about F0, energy, pause, etc.) were used. The best results (83% for accents, 88% for boundaries, two classes each) could be achieved when all features were used. For the 2nd issue change of tempo was labelled manually. We present the characterizing feature values for changes from slow to fast and from fast to slow, as well as the results of an automatic classifcation of change of tempo (72% for three classes). Finally, we discuss the possible function of change of tempo and its use in automatic speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-259"
  },
  "petek97_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ],
    [
     "Rastislav",
     "Sustarsic"
    ]
   ],
   "title": "A corpus-based approach to diphthong analysis of standard Slovenian",
   "original": "e97_0767",
   "page_count": 4,
   "order": 261,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "This paper presents an inventory and relative frequency estimation of glides on the 527,190 word-form Standard Slovenian lexicon. Detailed acoustic-phonetic measurements for the first four most frequent glides /ai/, /au/, /ou/, and /ei/ in stressed syllables are given. Inspection of their formant trajectory plots enabeled measurements of the first four formants in the onset and offset steady-states. Normalized duration patterns for the onset steady-state, glide and offset steady-state are also given. Results represent a broader view to the recently published JIPA paper [4] and are an initial step towards the decision on the most appropriate allophonic symbols to be used in narrow transcription for the glides of Standard Slovenian.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-260"
  },
  "aguilar97_eurospeech": {
   "authors": [
    [
     "Lourdes",
     "Aguilar"
    ],
    [
     "Julia A.",
     "Gimenez"
    ],
    [
     "Maria",
     "Machuca"
    ],
    [
     "Rafael",
     "Marin"
    ],
    [
     "Montse",
     "Riera"
    ]
   ],
   "title": "Catalan vowel duration",
   "original": "e97_0771",
   "page_count": 4,
   "order": 262,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "The temporal organization of discourse has produced a great deal of works in several languages pointing to different aims: from studies where the identification of cues about the planning of linguistic message is treated to studies in which duration models for text-to- speech systems are proposed. This work is a first step towards the description of Catalan vowel duration. Considering the Catalan vowel system, two subsystems can be distinguished according to stress: stressed vowels: /i/, /e/, /E/, /a/, /O/, /o/, /u/, and unstressed vowels /i/, /u/, /@/. The purpose of the present study is to provide data for Catalan vowels in order to achieve a data-oriented description and at the same time a predictive model suitable to be implemented in a TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-261"
  },
  "caputo97_eurospeech": {
   "authors": [
    [
     "Maria Rosaria",
     "Caputo"
    ]
   ],
   "title": "The intonation of vocatives in spoken Neapolitan Italian",
   "original": "e97_0775",
   "page_count": 4,
   "order": 263,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "This paper examines the phonology and phonetics of intonational patterns of vocatives functioning as calls in discourse. In addition it defines the relevant discourse context for the study of the vocatives and it examines the relations between discourse contexts and intonational patterns. The paper is based on a corpus of spontaneous speech. The analysis shows the existence of many different patterns (rises, falls, and levels). The presence of an interrogative vs a non- interrogative discourse context accounts for (respectively) the occurrence of rises and falls, while the level patterns exhibit context- neutrality. The paper concludes stating that the vocatives with non- level intonation function as modal clues in discourse.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-262"
  },
  "magnocaldognetto97_eurospeech": {
   "authors": [
    [
     "Emanuela",
     "Magno Caldognetto"
    ],
    [
     "Claudio",
     "Zmarich"
    ],
    [
     "Franco",
     "Ferrero"
    ]
   ],
   "title": "A comparative acoustic study of spontaneous and read Italian speech",
   "original": "e97_0779",
   "page_count": 4,
   "order": 264,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "This paper presents an acoustic study of spontaneous and read Italian speech based on the analysis of monologues and corresponding read transcribed texts, each produced by three different subjects. The speaking styles were examined in terms of articulation, speech, fluency and word rate indices; typology of pauses and their cooccurrence; mean and range of F0 values; classification of phonetic events resulting from adjacency of vowels situated at word boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-263"
  },
  "refice97_eurospeech": {
   "authors": [
    [
     "Mario",
     "Refice"
    ],
    [
     "Michelina",
     "Savino"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "A contribution to the estimation of naturalness in the intonation of Italian spontaneous speech",
   "original": "e97_0783",
   "page_count": 4,
   "order": 265,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "This paper focusses on the intonation of yes-no questions in a local non-standard variety of Italian: that spoken in Bari. It has been claimed in any early study [7, 8] that Bari Italian (BI) has a fmal rise on yes-no questions. However, subsequent accounts of BI [5, 6] have found a predominance of fmal falls on such questions. Since the former study was based on a corpus read aloud and the latter on spontaneous dialogues, it was decided to compare read and spontaneous questions produced by the same speaker. Spontaneous questions by six speakers were extracted from recordings of task-oriented dialogues and presented for reading, both in list form and in specially contructed contexts. It was found that the fmal tonal contour was predominantly falling in the spontaneous questions and predominantly rising in the corresponding read questions. These results throw light on the discrepancy in the literature as to the typical yes-no question intonation for this variety of Italian. It is argued that the falling fmal contour is more natural than the rising one since it is typical of spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-264"
  },
  "moosmuller97_eurospeech": {
   "authors": [
    [
     "Sylvia",
     "Moosmüller"
    ]
   ],
   "title": "Diphthongs and the process of monophthongization in Austrian German: a first approach",
   "original": "e97_0787",
   "page_count": 4,
   "order": 266,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "Both Standard Austrian German and the Austrian dialects are affected by an ongoing change which turns the diphthongs /aE/ and /AO/ into the monophthongs /E:/ and /O:/ respectively. However, this process shows different assimilation patterns according to the two main dialect regions in Austria: In the South Bavarian dialect region, the offset of the diphthong is assimilated towards the onset, whereas in the Middle Bavarian dialect region, the onset is assimilated towards the offset. The present study provides a detailed description of the diphthongs in both reading and spontaneous speech material. In order to give an answer to the question concerning the two different assimilation patterns, historical speech material of the late fifties has been analyzed additionally.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-265"
  },
  "hoskins97_eurospeech": {
   "authors": [
    [
     "Steve",
     "Hoskins"
    ]
   ],
   "title": "The prosody of broad and narrow focus in English: two experiments",
   "original": "e97_0791",
   "page_count": 4,
   "order": 267,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "In English, the focus of a sentence is an important factor in determining the prosody of an utterance. Some linguistic analyses of focus [9][10][11] claim that (1) prosodic representation of focus is determined by pitch accents, (2) the distribution of pitch accents is determined by the size of the focus constituent, and (3) one prosodic realization may be ambiguous for several focus constituents. In this study, two experiments were conducted to test the interaction of focus with certain structures: verb phrases and noun phrases. Duration and f0 measurements within these phrases were analyzed, and a prosodic analysis was conducted. Results show that speakers tend to distinguish broad and narrow focus using several prosodic strategies, where different pitch accent types and patterns within the phrases signal the different focus conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-266"
  },
  "turk97_eurospeech": {
   "authors": [
    [
     "Alice",
     "Turk"
    ],
    [
     "Laurence",
     "White"
    ]
   ],
   "title": "The domain of accentual lengthening in Scottish English",
   "original": "e97_0795",
   "page_count": 4,
   "order": 268,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "This study describes speech production experiments designed to determine the domain of accentual lengthening in Scottish English. Results suggest that accentual lengthening affects not only the syllable which bears the pitch accent (phrasal stress), but extends rightwards beyond this syllable. Secondly, the amount of lengthening on a syllable adjacent to a pitch accent appears to depend upon its membership in a pitch accented unit. Several candidates for the accentual-lengthening unit are entertained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-267"
  },
  "bessac97_eurospeech": {
   "authors": [
    [
     "Mariette",
     "Bessac"
    ],
    [
     "Geneviève",
     "Caelen-Haumont"
    ]
   ],
   "title": "Spontaneous dialogue: some results about the F0 predictions of a pragmatic model of information processing",
   "original": "e97_0799",
   "page_count": 4,
   "order": 269,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "This paper presents the first results of a semantic-pragmatic model which assigns a specific label to the relevant words of dialogue utterances and predicts their F0 value. The originality of this work lies in the kind of utterances the model has been designed for: dialogue utterances. The labels of the model represent the degrees of both the expected/unexpected and known/unknown aspects of the lexical information while the predicted value of F0 represents the corresponding weight of that information. The aim of this work is 1) to observe the real values of F0 for each label and 2) to compare the prediction of the model to the real values. The real values correspond to the 3 relevant F0 indices (Maximum F0, DF0 and mean F0). In this paper, only the levels 2 and 3 are discussed because they represent most of the population.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-268"
  },
  "demolin97b_eurospeech": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "Bernard",
     "Teston"
    ]
   ],
   "title": "Phonetic characteristics of double articulations in some Mangbutu-efe languages",
   "original": "e97_0803",
   "page_count": 4,
   "order": 270,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "This paper examines double articulations in three African languages. Mamvu.Lese and Efe. all belonging to the Central Sudanic language family. The phonetic inventory of these languages exhibit some very interesting facts, among which the most striking are voiceless labiovelars stops involving a trilled release and a labiouvelar stop which shows the combination of a voiceless and a voiced part in the same consonant. Acoustic and aerodynamic measurements describing the production of these sounds are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-269"
  },
  "hernaez97_eurospeech": {
   "authors": [
    [
     "Inmaculada",
     "Hernaez"
    ],
    [
     "Inaki",
     "Gaminde"
    ],
    [
     "Borja",
     "Etxebarria"
    ],
    [
     "Pilartxo",
     "Etxebarria"
    ]
   ],
   "title": "Intonation modeling for the southern dialects of the Basque language",
   "original": "e97_0807",
   "page_count": 4,
   "order": 271,
   "p1": "807",
   "pn": "809",
   "abstract": [
    "In this paper we present part of the analysis performed on intonation for the Basque language. After a brief description of the most relevant characteristics of the language, criteria for corpus fulfilment and speakers selection is described. Results of the analysis show the importance of the F0 drop in focus positioning. A first classification of the selected varieties is done according to the accent position and F0 values relationships.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-270"
  },
  "oboyle97_eurospeech": {
   "authors": [
    [
     "Peter",
     "O'Boyle"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Marie",
     "Owens"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "From phone identification to phone clustering using mutual information",
   "original": "e97_2391",
   "page_count": 4,
   "order": 272,
   "p1": "2391",
   "pn": "2394",
   "abstract": [
    "In this paper we show how a confusion matrix derived from phone identification experiments can be used to automatically generate phone clusters. These clusters can be applied when constructing triphone models to overcome the sparse data problem. Two techniques are presented; firstly an hierarchical clustering technique is described; then an open clustering technique is presented. Both of these use mutual information calculated on a probability distribution derived from the confusion matrix as a measure of phone similarity. Sample results from each technique are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-271"
  },
  "berrah97_eurospeech": {
   "authors": [
    [
     "Ahmed-Reda",
     "Berrah"
    ],
    [
     "Rafael",
     "Laboissiere"
    ]
   ],
   "title": "Phonetic code emergence in a society of speech robots: explaining vowel systems and the MUAF principle",
   "original": "e97_2395",
   "page_count": 4,
   "order": 273,
   "p1": "2395",
   "pn": "2398",
   "abstract": [
    "The purpose of this paper is to explain how it is possible to simulate the emergence of a common phonetic code in a society of speech robots using evolutionary techniques. Simulations of the prediction of vowel systems and the explanation of the Maximum Use of Available distinctive Features (MUAF) principle are discussed. These experimental results show how simple local rules of interaction between robots may explain some of the universals characteristics of the phonological structure of world's languages. On going work aiming to answer more complex questions, such as the use of supplementary features in large vowel systems, is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-272"
  },
  "moen97_eurospeech": {
   "authors": [
    [
     "Inger",
     "Moen"
    ],
    [
     "Hanne Gram",
     "Simonsen"
    ]
   ],
   "title": "Effects of voicing on /t,d/ tongue/palate contact in English and norwegian",
   "original": "e97_2399",
   "page_count": 4,
   "order": 274,
   "p1": "2399",
   "pn": "2402",
   "abstract": [
    "Our paper addresses the question of cross-linguistic similarities and differences in the articulatory patterns of plosives. An EPG investigation of the English and Norwegian plosives /t/ and /d/ shows a larger contact area between tongue and palate for /t/ than for /d/ in both languages. The investigation also shows a more laminal articulation, larger contact areas, for both plosives in Norwegian compared to English. We suggest that the same general phonetic- physiological factors may explain the larger contact areas for /t/ than for /d/ in both languages. The oral air pressure is stronger during the articulation of /t/ than of /d/. In order to prevent air from escaping between the tongue and the palate, a firmer contact is needed for voiceless than for voiced plosives. The larger contact areas for the Norwegian plosives compared to the English ones are interpreted as the result of different phonological patterns in the two languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-273"
  },
  "ladefoged97_eurospeech": {
   "authors": [
    [
     "Peter",
     "Ladefoged"
    ],
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "Fieldwork techniques for relating formant frequency, amplitude and bandwidth",
   "original": "e97_2403",
   "page_count": 4,
   "order": 275,
   "p1": "2403",
   "pn": "2406",
   "abstract": [
    "An analysis-by-synthesis method for finding formant bandwidths from vowel spectra has been implemented on a solar-powered computer used in fieldwork, thus enabling linguists to test hypotheses about differences between sets of vowels while working with speakers of the language. The procedure has been tested on the two sets of vowels that occur in Degema, a language spoken in Nigeria.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-274"
  },
  "wang97b_eurospeech": {
   "authors": [
    [
     "Xue",
     "Wang"
    ],
    [
     "Louis C.W.",
     "Pols"
    ]
   ],
   "title": "Word juncture modelling based on the TIMIT database",
   "original": "e97_2407",
   "page_count": 4,
   "order": 276,
   "p1": "2407",
   "pn": "2410",
   "abstract": [
    "In this study, we develop data-based word juncture models, which account for the pronunciation variations at word boundaries, as an optional form of phonological rules. We used the American English TIMIT database. Issues in generating the models and using them in a continuous recognition task are discussed. A comparison is given between the coverage of the pronunciation variations by the models and by a set of phonological rules. There is a fairly good agreement between the models and the rules in predicting the pronunciation variations, whereas the models cover a larger set of variation phenomena. Furthermore, use of the models improved recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-275"
  },
  "ueyama97_eurospeech": {
   "authors": [
    [
     "Motoko",
     "Ueyama"
    ]
   ],
   "title": "The phonology and phonetics of second language intonation: the case of \"Japanese English\"",
   "original": "e97_2411",
   "page_count": 4,
   "order": 277,
   "p1": "2411",
   "pn": "2414",
   "abstract": [
    "A production experiment was conducted in order to examine the acquisition of English intonation by native speakers of Japanese, and the results were analyzed within the framework developed by Pierrehumb../pdf/Th3b [3] and her colleagues. The results suggest that second language intonation is acquired on two different levels: learners first acquire the categorical patterns of the foreign intonation, and only later learn to produce native-like continuous intonational streams. This supports models in which the speech cognitive system is split into two sub-modules: a phonological component (characterized by categorical units) and a phonetic component (implementing the phonological units as a continuous articulatory/acoustic stream).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-276"
  },
  "fetter97_eurospeech": {
   "authors": [
    [
     "Pablo",
     "Fetter"
    ],
    [
     "Udo",
     "Haiber"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "A low-cost phonetic transcription method",
   "original": "e97_0811",
   "page_count": 4,
   "order": 278,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "In this paper our goal is to find the phonetic transcription of spoken utterances. We present a method which uses information extracted directly from the word-based search to compute the most likely phoneme sequence. Utterances are transcribed during recognition, so that the phonetic representation of the input is available after the search. Using this method, the computational cost of the word-based search remains almost unaltered, and the phonetic transcription is obtained almost for free.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-277"
  },
  "chase97_eurospeech": {
   "authors": [
    [
     "Lin",
     "Chase"
    ]
   ],
   "title": "Word and acoustic confidence annotation for large vocabulary speech recognition",
   "original": "e97_0815",
   "page_count": 4,
   "order": 279,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "We present improvements in confidence annotation of automatic speech recognizer output for large vocabulary, speaker- independent systems. Several strong additions to the set of predictor variables used for this purpose are discussed. Extensions which allow prediction of separate tvpes of errors, as opposed to the simple presence of an error, are presented. A new development, acoustic confidenceannotation, is explored, in which a predictor is built that indicates the likely successes and failures of the acoustic models alone. Four separate learning mechanisms are compared in terms of their ability to provide good confidence annotations from the same set of predictor variables. Performance figures are reported on both read news (the North American Business news corpus) and conversational telephone speech (the Switchboard corpus), both in American English. The Sphinx-II system [1] is used for the NAB tests. The Janus system [2J is used for the Switchboard tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-278"
  },
  "bergen97_eurospeech": {
   "authors": [
    [
     "Zachary",
     "Bergen"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "A senone based confidence measure for speech recognition",
   "original": "e97_0819",
   "page_count": 4,
   "order": 280,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "This paper describes three experiments in using frame level observation probabilities as the basis for word confidence annotation in an HMM speech recognition system. One experiment is at the word level, one uses word classes, and the other uses phone classes. In each experiment we categorize hypotheses into correct and incorrect categories by aligning a best recognition hypothesis with the known transcript. The confidence of error prediction for each class is a measure of the resolvability between the correct and incorrect histograms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-279"
  },
  "bernstein97_eurospeech": {
   "authors": [
    [
     "Erica",
     "Bernstein"
    ],
    [
     "Ward R.",
     "Evans"
    ]
   ],
   "title": "OOV utterance detection based on the recognizer response function",
   "original": "e97_0823",
   "page_count": 3,
   "order": 281,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "This paper addresses the problem of out of vocabulary (OOV) utterance detection for spoken language systems in an open microphone environment. This problem is becoming crucial as use of spoken language systems grows beyond the research laboratory. In the past this problem has been addressed in the context of keyword spotting, e.g., for connected digits in a telephone environment and more recently in OOV word detection in a large vocabulary continuous speech recognition system. We develop a novel technique for designing a lexical garbage model that takes advantage of application specific knowledge and any potential bias in the recognizer. We do this through the formulation of a recognizer response function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-280"
  },
  "kemp97_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "Estimating confidence using word lattices",
   "original": "e97_0827",
   "page_count": 4,
   "order": 282,
   "p1": "827",
   "pn": "830",
   "abstract": [
    "For many practical applications of speech recognition systems, it is desirable to have an estimate of confidence for each hypothesized word, i.e. to have an estimate which words of the speech recognizer's output are likely to be correct and which are not reliable. Many oftoday's speech recognition systems use word lattices as a compact representation of a set of alternative hypothesis. We exploit the use of such word lattices as information sources for the measure-of- confidence tagger JANKA [1]. In experiments on spontaneous human- to-human speech data the use of word lattice related information significantly improves the tagging accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-281"
  },
  "siu97_eurospeech": {
   "authors": [
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Fred",
     "Richardson"
    ]
   ],
   "title": "Improved estimation, evaluation and applications of confidence measures for speech recognition",
   "original": "e97_0831",
   "page_count": 4,
   "order": 283,
   "p1": "831",
   "pn": "834",
   "abstract": [
    "This paper describes our approach to the estimation of confidence in the words generated by a speech recognition system. We describe the models and the features employed for confidence estimation. In addition we discuss the characteristics of an information -theoretic metric for assessing the performance of the confidence measure. We provide a simple application of confidence measures in which we rank the performance of speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-282"
  },
  "hussain97_eurospeech": {
   "authors": [
    [
     "Salleh",
     "Hussain"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Improved speaker verification system with limited training data on telephone quality speech",
   "original": "e97_0835",
   "page_count": 4,
   "order": 284,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "A hybrid neural network is proposed for speaker verification (SV). The basic idea in this system is the usage of vector quantization preprocessing as the feature extractor. The experiments were carried out using a neural network model(NNM) with frame labelling performed from a client codebook known as NNM-C. Improved performance for NNM-C with more inputs and proper alignment of the speech signals supports the hypothesis that a more detailed representation of the speech patterns proved helpful for the system. The flexibility of this system allows an equal error rate (EER) of 11.2% on a single isolated digit and 0.7% on a sequence of 12 isolated digits. This paper also compares neural network speaker verification system with the more conventional method like Hidden Markov models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-283"
  },
  "li97_eurospeech": {
   "authors": [
    [
     "Qi",
     "Li"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Verbal information verification",
   "original": "e97_0839",
   "page_count": 4,
   "order": 285,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "Traditionally, speaker authentication has focused on two categories of techniques: speaker verification and speaker identification. In this paper, we introduce a third category called verbal information verification (VIV) in which a claimed speaker's utterances are verified against the key information in the speaker's registered profile to decide whether the claimed identity should be accepted or rejected. The proposed VIV technique can be used independently or combined with the traditional speaker verification techniques to achieve flexible and improved speaker authentication. Instead of accomplishing VIV through recognizing the key information, the proposed VIV algorithm is based on the concept of sequential utterance verification. In a telephone speaker authentication experiment on 100 speakers and using three pass-utterances in response to three categories of questions, the proposed VIV system achieved 0.00% equal-error rate, compared to 30% false rejection rate on an automatic speech recognition approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-284"
  },
  "sarma97_eurospeech": {
   "authors": [
    [
     "Sridevi V.",
     "Sarma"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A segment-based speaker verification system using SUMMIT",
   "original": "e97_0843",
   "page_count": 4,
   "order": 286,
   "p1": "843",
   "pn": "846",
   "abstract": [
    "The main goal of this work is to develop a competitive segment- based speaker verification system that is computationally efficient. To achieve our goal, we modified SUMMIT [12] to suit our needs. The speech signal was first transformed into a hierarchical segment network using frame-based measurements. Next, acoustic models for 168 speakers were developed for a set of 6 broad phoneme classes. The models represented feature statistics with diagonal Gaussians, preceded by principle component analysis. The feature vector included segment-averaged MFCCs, plus three prosodic measurements: energy, fundamental frequency (F0), and duration. The size and content of the feature vector were determined through a greedy algorithm while optimizing overall speaker verification performance. We were able to achieve a performance of 2.74% equal error rate (EER) using cohorts during testing; and 1.59% EER using all speakers during testing. We reduced computation significantly through the use of a small number of features, a small number of phonetic models per speaker, few model parameters, and few competing speakers during testing (when cohorts are used).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-285"
  },
  "sokolov97_eurospeech": {
   "authors": [
    [
     "Michael",
     "Sokolov"
    ]
   ],
   "title": "Speaker verification on the world wide web",
   "original": "e97_0847",
   "page_count": 4,
   "order": 287,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "This paper describes a system for controlling access to web resources built using well-known speaker verification techniques. We describe the implementation of a speech verification server and an associated authentication module for the Apache web server. Speaker verification requires two inputs: a sample of the user's speech and an identity claim for the user; typically the user's name. However a more convenient system would not require a user name to be entered. We present the results of an attempt to implement speech-only authentication using open set speaker identification. We explore the effect of database size on performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-286"
  },
  "lindberg97_eurospeech": {
   "authors": [
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Håkan",
     "Melin"
    ]
   ],
   "title": "Text-prompted versus sound-prompted passwords in speaker verification systems",
   "original": "e97_0851",
   "page_count": 4,
   "order": 288,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "The problem of how to prompt a client for a password in an automatic, prompted speaker verification system is addressed. Text-prompting of four-digit sequences is compared to speech-prompting of the same sequences, and speech-prompting of four digits is compared to speech-prompting of five digits. Speech recordings are analyzed by comparing speaker verification performance and by inspecting the number and type of speaking errors that subjects made. From the experiment it is clear that text-prompting gives the subjects an easier task and fewer speaking errors are produced in that context. When enrolling clients with text-prompted speech and performing verification with an HMM-based system, the average EER was larger for speech-prompted items compared to text-prompted items, but changes in individual EERs varies across the test population.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-287"
  },
  "schmidt97_eurospeech": {
   "authors": [
    [
     "Michael",
     "Schmidt"
    ],
    [
     "John",
     "Golden"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "GMM sample statistic log-likelihoods for text-independent speaker recognition",
   "original": "e97_0855",
   "page_count": 5,
   "order": 289,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "A novel approach to scoring Gaussian mixture mod- els is presented. Feature vectors are assigned to the individual Gaussians making up the model and log-likelihoods of the separate Gaussians are computed and summed. Furthermore, the log-likelihoods of the individual Gaussians can be decomposed into sample weight, mean, and covariance log-likelihoods. Correlation likelihoods can also be computed. The results of the various systems are comparable on text- independent speaker recognition experiments despite the fact that the models and scoring are all quite di erent. By decomposing log-likelihoods of models into various sample statistic log-likelihoods, it is possible to diagnose which part of the model has the greatest discriminative power, whether the location of the Gaussians or their shapes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-288"
  },
  "rietveld97b_eurospeech": {
   "authors": [
    [
     "Toni",
     "Rietveld"
    ],
    [
     "Carlos",
     "Gussenhoven"
    ]
   ],
   "title": "The influence of phrase boundaries on perceived prominence in two-peak intonation contours",
   "original": "e97_0859",
   "page_count": 4,
   "order": 290,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "Dutch listeners rated the perceived prominence of the second Fo peak in a number of artificial two-peak intonation contours. When the two peaks were contained within a single intonation phrase, as signalled by the shape of the contour between the two peaks, and the pitch range of the contour was neutral, the perceived prominence of the second peak appeared to be determined by the Fo of the first. However, when the two peaks occurred in separate intonation phrases, or when the pitch range was wide, no such effect was found. The results suggest that the prominence of peaks is judged as a function of the prominence of the contour of its intonation phrase, at least in contours with normal pitch range. These findigs confirm the effect of pitch range found in the literature for English, and extend them by their demonstration that the effect is chiefly to be attributed to the phonological shape of the contour, and only secondarily to its pitch range.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-289"
  },
  "caspers97_eurospeech": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "Testing the meaning of four dutch pitch accent types",
   "original": "e97_0863",
   "page_count": 4,
   "order": 291,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "The Grammar of Dutch Intonation (\"GDI\", [1]), the model I adopted to describe intonational phenomena, provides an inventory of accent- lending and boundary-marking pitch configurations for Dutch, but little is known about the factors influencing the choice within these two categories. The present study aims to provide some insight into this issue, by way of experimentally testing  linguistic propositions regarding the meaning of a number of accent-lending intonation patterns in Dutch. Two perception experiments have been carried out to test four form-meaning hypotheses and the results confirm the basic correctness of the semantic proposals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-290"
  },
  "mersdorf97_eurospeech": {
   "authors": [
    [
     "Joachim J.",
     "Mersdorf"
    ],
    [
     "Thomas",
     "Domhover"
    ]
   ],
   "title": "A perceptual study for modelling speaker-dependent intonation in TTS and dialog systems",
   "original": "e97_0867",
   "page_count": 4,
   "order": 292,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "In general, most of the developed prosody and intonation models were obtained from a statistical analysis of F0 curves and resynthesis by TTS. But there is yet another chance improving quality and naturalness: effective results can also be obtained by analysing the listeners' common sense about natural intonational behavior. Therefore, we use a digital process that generates signals representing only the melody of the original speech signal. Comprehensive listening experiments become possible to analyse and compare the perception of natural and synthetic intonation. Based on the results of some listening experiments a statistical analysis of the F0 curves was carried out, regarding that a speaker-individual intonation model needs more quantitative F0 information than traditional descriptions. The aim is an prosodical speaker-dependent model for synthetic speech and dialog systems. Furthermore, this flexible approach should not be limited to speaker-individual intonation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-291"
  },
  "auberge97_eurospeech": {
   "authors": [
    [
     "Veronique",
     "Aubergé"
    ],
    [
     "Tuulikki",
     "Grepillat"
    ],
    [
     "A.",
     "Rilliard"
    ]
   ],
   "title": "Can we perceive attitudes before the end of sentences? the gating paradigm for prosodic contours",
   "original": "e97_0871",
   "page_count": 4,
   "order": 293,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "In previous works, we proposed, on the basis of acoustic analysis and synthesis, that intonation can be cognitively and linguistically described by a lexicon of prosodic contours. The aim of the present work is to validate on the French language such an approach in perception processing. Two experiments are described hereafter. The first one consists in evaluating on complete utterances the identification of the six attitudes currently implemented in the ICP TTS system. The second one is a gating experiment which aims at showing a stable and early identification of attitudes. Perception results are commented and compared to the acoustic forms to analyse the relation between the perceptive and prosodic points of unicity if any.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-292"
  },
  "heldner97_eurospeech": {
   "authors": [
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Eva",
     "Strangert"
    ]
   ],
   "title": "To what extent is perceived focus determined by F0-cues?",
   "original": "e97_0875",
   "page_count": 3,
   "order": 294,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "Two experiments were designed to investigate the perceptual strength of an F0-rise relative to other possible local and global cues to focus in Swedish. The contribution of F0 relative to other possible local cues was investigated by manipulating the F0-contour in naturally produced Swedish sentences. Manipulations involved a gradual reduction of the F0-rise in focused words and a gradual addition of an F0-rise in non-focused words. The possible influence of global cues was explored by varying the amount of global information. In the first experiment, subjects were presented with complete sentences. In the second experiment, they heard sentences with the last part excluded. The results indicate that the F0-rise is neither necessary nor sufficient to perceive focus; other cues, both local and global, appear also to play a role.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-293"
  },
  "house97_eurospeech": {
   "authors": [
    [
     "David",
     "House"
    ],
    [
     "Dik",
     "Hermes"
    ],
    [
     "Frédéric",
     "Beaugendre"
    ]
   ],
   "title": "Temporal-alignment categories of accent-lending rises and falls",
   "original": "e97_0879",
   "page_count": 4,
   "order": 295,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "This paper presents a theoretical framework for a model of perceived accentuation categories. This framework is based on the combined results of a series of experiments on accentuation boundaries in Dutch, French and Swedish and on theoretical work on tonal perception in speech. We propose a model in which several different language- dependent categories of accentuation are represented as the falling or rising pitch movement is advanced through the syllable. The perceived category depends upon whether an onset of the pitch movement or a pitch jump is perceived, whether or not the particular category is represented in the language in question, and whether or not the movement also serves as a cue for phrasing. Dutch and Swedish display similarities in accentuation categories while French differs from these two languages. These differences are explained by the conflict between cues for accentuation and phrasing and the differing intonational structure of French. The proposed perceptual categories have general implications for the understanding and description of accentuation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-294"
  },
  "lau97b_eurospeech": {
   "authors": [
    [
     "Raymond",
     "Lau"
    ],
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Webgalaxy - integrating spoken language and hypertext navigation",
   "original": "e97_0883",
   "page_count": 4,
   "order": 296,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "The growth in the quantity of information and services offered online has been phenomenal. Nevertheless, access mechanisms have remained relatively primitive, requiring users to primarily point and click their way through a forest of Web links and to expend valuable cognitive capacities to track the geography of the Web space. Conversational systems can provide an intuitive, flexible multi-modal interface to online resources. The explosive growth of the World Wide Web, the continuing standardization of Web related technologies, and the growing penetration of Internet access enable us to embed a very thin client inside a standard Web browser, making conversational interfaces available to a much wider audience. This paper presents WebGALAXY, a conversational spoken language system for access to selected on-line resources from within a typical browser. A thin Java based client is employed as the front end with much of the speech and natural language processing occuring on remote servers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-295"
  },
  "carey97_eurospeech": {
   "authors": [
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Graham D.",
     "Tattersall"
    ]
   ],
   "title": "Pitch estimation of singing for re-synthesis and musical transcription",
   "original": "e97_0887",
   "page_count": 4,
   "order": 297,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "This paper describes an algorithm which allows singing to be analysed in real time using a PC and then re-synthesised by the computer using whistled notes. The singing can also be transcribed as a series of notes on a musical stave using a MIDI file as interface. Pitch amplitude and spectral change parameters are derived from the input waveform. A sequence of musical notes is derived from a set of parameters using a set of rules. The system is designed as an entertaining, yet educational tool for children, and will be embodied in an interactive multi-media system. In its electronic form the paper has attached files demonstrating the results of the re-synthesis algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-296"
  },
  "jones97b_eurospeech": {
   "authors": [
    [
     "Christian Martyn",
     "Jones"
    ],
    [
     "Satnam Singh",
     "Dlay"
    ]
   ],
   "title": "Automated lip synchronisation for human-computer interaction and special effect animation",
   "original": "e97_0891",
   "page_count": 4,
   "order": 298,
   "p1": "891",
   "pn": "894",
   "abstract": [
    "The research presents MARTI (Man-machine animation real-time interface) for the realisation of automated special effect animation and human computer interaction. The future developments of the Internet, video communications and multi-media, virtual reality, and animation will rely on the derivation of a natural human-machine interface in order to submerse people, irrespective of technical know-how, into the latest technology, and allow them to interact with computers and one another using their own personality and idiosyncrasies. MARTI introduces novel research in a number of engineering fields to realise the first natural interface and animation system capable of high performance for real-users and real-world applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-297"
  },
  "hemphill97_eurospeech": {
   "authors": [
    [
     "Charles T.",
     "Hemphill"
    ],
    [
     "Yeshwant K.",
     "Muthusamy"
    ]
   ],
   "title": "Developing web-based speech applications",
   "original": "e97_0895",
   "page_count": 4,
   "order": 299,
   "p1": "895",
   "pn": "898",
   "abstract": [
    "We have developed a speech interface to the Web that allows easy access to information and an approach to intelligent user agents. The meclianisms developed apply to other multimedia applications where speech can serve as an input modality. We describe the benefits of our recognition system to speech-application developers: (1) Developers need not know about speech - in the simplest case, developers simply define HTML links. (2) Developers need not worry about word pronunciations since the system provides these. Developers may specify grammass in a simple BNF syntax and the system automatically converts these for use by the recognizer. (3) Developers with programming skills may use a Web server or the Java programming language to easily produce more sophisticated speech interfaces. (4) Developers reap the benefits of portability through general HTML browsers and languages such as Java. Java also simplifies the development of portable graphical interfaces that couple with speech input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-298"
  },
  "verhelst97_eurospeech": {
   "authors": [
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Automatic post-synchronization of speech utterances",
   "original": "e97_0899",
   "page_count": 4,
   "order": 300,
   "p1": "899",
   "pn": "902",
   "abstract": [
    "The paper considers a prototype for automatic post- synchronization that consists of two basic components. As a first step, dynamic time warping is applied to compute the time-correspondence between an original utterance and an utterance that serves as the timing reference signal. In a second step, a time-scaling algorithm modifies the time structure of the original utterance accordingly. Informal diagnostic evaluation has shown that good results are obtained if the similarity between the acoustic-phonetic contents of the utterances is high. Possible ways for improving robustness against acoustic-phonetic differences, such as those that result from different coarticulation, are suggested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-299"
  },
  "robertribes97_eurospeech": {
   "authors": [
    [
     "Jordi",
     "Robert-Ribes"
    ],
    [
     "Rami G.",
     "Mukhtar"
    ]
   ],
   "title": "Automatic generation of hyperlinks between audio and transcript",
   "original": "e97_0903",
   "page_count": 4,
   "order": 301,
   "p1": "903",
   "pn": "906",
   "abstract": [
    "We present a prototype that enables the generation of hyperlinks between audio and the corresponding transcript. The main issue in generating such hyperlinks is determining common time points in the transcript and the audio (this is also called aligning). The system is speaker independent and can deal with inexact transcripts. It combines inaccurate modules in such a way that the fmal results are extremely satisfactory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-300"
  },
  "moller97_eurospeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Rainer",
     "Schonweiler"
    ]
   ],
   "title": "Analysis of infant cries for the early detection of hearing impairment",
   "original": "e97_1759",
   "page_count": 4,
   "order": 302,
   "p1": "1759",
   "pn": "1762",
   "abstract": [
    "Auditory and instrumental analysis of cries from normally hearing and profoundly hearing-impaired infants (2-11 month) is presented. Results from listening experiments lead to the assumption that differences exist between cries from the two infant groups. Attributes expressing the difference are related to the emotional state of the infant, to prosodic features, and to voice quality. Signal analysis of the cries confirms these findings showing statistically significant differences for spectral parameters and those describing the melody contour of the cries. The usability of neural networks for an automatic classification and discrimination of cries is discussed. If the tendencies shown here hold true for other data sets, the findings can be used to develop a new screening method detecting hearing impairment and auditory perception disorders at a very early age.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-301"
  },
  "hatzis97_eurospeech": {
   "authors": [
    [
     "A.",
     "Hatzis"
    ],
    [
     "P.D.",
     "Green"
    ],
    [
     "S.J.",
     "Howard"
    ]
   ],
   "title": "Optical logo-therapy (OLT): a computer-based real time visual feedback application for speech training",
   "original": "e97_1763",
   "page_count": 4,
   "order": 303,
   "p1": "1763",
   "pn": "1766",
   "abstract": [
    "Traditional speech training methods can prove cumbersome because of the difficulty of providing the subject with good feedback, maintaining her/his motivation over long periods and stabilising the improvement in articulation. In this work we provide visual feedback based on displaying a trajectory in a 2-Dimensional 'phonetic space'. Data are presented from a small-scale efficacy study, which illustrate the use of OLT in speech therapy for misarticulated sibilant fricatives. Results for the contrasting articulations are compared and the potential of OLT as a therapeutic technique is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-302"
  },
  "lin97b_eurospeech": {
   "authors": [
    [
     "Sung-Chien",
     "Lin"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Ming-Chiuan",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ],
    [
     "Ker-Jiann",
     "Chen"
    ]
   ],
   "title": "Intelligent retrieval of very large Chinese dictionaries with speech queries",
   "original": "e97_1767",
   "page_count": 4,
   "order": 304,
   "p1": "1767",
   "pn": "1770",
   "abstract": [
    "To retrieve a Chinese word from a Chinese dictionary, it needs the user to know exactly the first character of the desired word. Because there is more than 10,000 Chinese characters, this makes the Chinese dictionary relatively difficult to be used. To reduce the problem, this paper presents intelligent retrieval techniques for very large Chinese dictionaries with speech queries. The proposed techniques properly integrate the technologies of Mandarin speech recognition and Chinese information retrieval with a syllable-based approach utilizing the mono-syllabic structure of the language. Moreover, it is very nice to provide the function of retrieving all relevant word entries from the dictionaries using speech queries describing \"general concepts\" of the desired words. To achieve the challenging function, the techniques of relevance feedback are also included. Based on these techniques, a retrieval system was implemented successfully on a Pentium PC for a very large Chinese dictionary which includes 160,000 word entries and the total length of the lexical information under the word entries exceeds 20,000,000 words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-303"
  },
  "leonardi97_eurospeech": {
   "authors": [
    [
     "Fulvio",
     "Leonardi"
    ],
    [
     "Giorgio",
     "Micca"
    ],
    [
     "Sheyla",
     "Militello"
    ],
    [
     "Mario",
     "Nigra"
    ]
   ],
   "title": "Preliminary results of a multilingual interactive voice activated telephone service for people-on-the-move",
   "original": "e97_1771",
   "page_count": 4,
   "order": 305,
   "p1": "1771",
   "pn": "1774",
   "abstract": [
    "The EURESCOM P502 project, Multilingual Interactive Voice Activated (MIVA) telephone services, launched in 1995 for a three-years term, aimed at designing and experimenting on an automatic multilingual telephone assistant for people-on-the-move, that provided them with instructions about the use of most important telephone services in the country they are traveling. The core information provided by the system is: emergency services, international and national calls, card calls. Six European Telecom research laboratories were involved in the project: CNET, the project leader; British Telecom, Deutsch Telekom, KPN, Portugal Telecom and CSELT. The final prototype has to include a language selection module and a menu-driven procedure, using a common structure of the information contents in all languages. Several factors are currently being investigated, such as the impact of a talk-through capability, the effect of the cellular network as well as the usage of different national networks on the ASR performance, and the optimization of the dialogue strategy at the system interface level. The prototypes are in the process of being tested within the individual national research units, and cross-country tests will follow. As a further benefit, the potential savings which can be obtained by sharing the costs of development of ASR-based multilingual telephone services, will be estimated successively. A final field trial of the national implementations of the systems has to be carried out starting October '97 for a thorough evaluation of the multilingual services.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-304"
  },
  "dubois97_eurospeech": {
   "authors": [
    [
     "Jean-Christophe",
     "Dubois"
    ],
    [
     "Yolande",
     "Anglade"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Assessment of an operational dialogue system used by a blind telephone switchboard operator",
   "original": "e97_1775",
   "page_count": 4,
   "order": 306,
   "p1": "1775",
   "pn": "1778",
   "abstract": [
    "This paper presents the assessment of a dialogue system which is used daily by a blind telephone switchboard operator. The purpose of the system is to provide this operator with some information about company members called by external correspondents (phone extensions, department...). In order to realize this system, we have to solved two main problems: on the one hand the recognition of confusable letters (like P and T) and on the second hand, the access to a database with a name -possibly misspelled -. In the paper, we present the assessment which allowed us to measure the system performances but also to validate the two methods designed for this project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-305"
  },
  "rubio97_eurospeech": {
   "authors": [
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Pedro",
     "Garcia"
    ],
    [
     "Angel de la",
     "Torre"
    ],
    [
     "Jose C.",
     "Segura"
    ],
    [
     "Jesus",
     "Diaz-Verdejo"
    ],
    [
     "Maria C.",
     "Benitez"
    ],
    [
     "Victoria",
     "Sanchez"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Juan M.",
     "Lopez-Soler"
    ],
    [
     "Jose L.",
     "Perez-Cordoba"
    ]
   ],
   "title": "STACC: an automatic service for information access using continuous speech recognition through telephone line",
   "original": "e97_1779",
   "page_count": 4,
   "order": 307,
   "p1": "1779",
   "pn": "1782",
   "abstract": [
    "This work presents the STACC, Sistema Telefonico Automatico de Consulta de Calificaciones Automatic Telephone System for Consulting Marks). This system has been developed at our laboratory during 1996 and implements a service through telephone line that allows the students to consult by speech their marks after the exams by means of a simple phone call. This experience provided us an interesting point of view about the problems of real applications of speech technology. In this work we describe the system and some statistics about the use of STACC by the students are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-306"
  },
  "lopezcozar97_eurospeech": {
   "authors": [
    [
     "Ramon",
     "Lopez-Cozar"
    ],
    [
     "Pedro",
     "Garcia"
    ],
    [
     "Jesus",
     "Diaz-Verdejo"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "A voice activated dialogue system for fast-food restaurant applications",
   "original": "e97_1783",
   "page_count": 4,
   "order": 308,
   "p1": "1783",
   "pn": "1786",
   "abstract": [
    "We present a preliminary version of a voice dialogue system suitable to deal with client orders and questions in fast-food restaurants. The system consists of two main sub-systems, namely a dialogue sub-system and a voice interface. The dialogue sub-system is a natural language processing system that may be considered a rule-based expert system, whose behaviour is decided from a recorded dialogue corpus obtained at a real restaurant. In this paper we present a general description of both sub-systems, and focus on knowledge representation, grammar, and module structure of the dialogue sub-system. An introduction to the natural language generation mechanism used is introduced, and future work is mentioned. Finally some conclusions are shown.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-307"
  },
  "shields97_eurospeech": {
   "authors": [
    [
     "Paul W.",
     "Shields"
    ],
    [
     "Douglas R.",
     "Campbell"
    ]
   ],
   "title": "Multi-microphone sub-band adaptive signal processing for improvement of hearing aid performance",
   "original": "e97_1787",
   "page_count": 4,
   "order": 309,
   "p1": "1787",
   "pn": "1790",
   "abstract": [
    "A scheme for binaural pre-processing of speech signals for input to a standard linear hearing aid has been proposed. The system is based on that of Toner & Campbell [1] who applied the Least Mean Squares (LMS) algorithm in sub-bands to speech signals from various acoustic environments and signal to noise ratios (SNR). The processing scheme attempts to take advantage of the multiple inputs to perform noise cancellation. The use of sub-bands enables a diverse processing mechanism to be employed, where the wide-band signal is split into smaller frequency limited sub-bands, which can subsequently be processed according to their signal characteristics. The results of a series of intelligibility tests are presented from experiments in which acoustic speech and noise data, generated in a simulated room was tested on hearing impaired volunteers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-308"
  },
  "piroth97_eurospeech": {
   "authors": [
    [
     "Hans Georg",
     "Piroth"
    ],
    [
     "Thomas",
     "Arnhold"
    ]
   ],
   "title": "Tactile transmission of intonation and stress",
   "original": "e97_1791",
   "page_count": 4,
   "order": 310,
   "p1": "1791",
   "pn": "1794",
   "abstract": [
    "The development of technical communication devices for hearing-impaired or deaf persons is one of the main topics in actual research on aids for the disabled and elderly. Besides the well-known advances in the construction of analogue and digital hearing aids for the hard-of-hearing, there is also a long tradition of research on tactile speech aids. From the beginning of this century, many attempts have been made to resolve the problem of speech substitution for deaf people and those suffering from severe hearing loss. A series of experiments was carried out to determine a robust coding method for tactile transmission of F0 and stress to support speech perception of deaf or severely hearing impaired persons unable to extract suprasegmental speech features by the auditory sense.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-309"
  },
  "huttunen97_eurospeech": {
   "authors": [
    [
     "Kerttu",
     "Huttunen"
    ],
    [
     "Pentti",
     "Korkko"
    ],
    [
     "Martti",
     "Sorri"
    ]
   ],
   "title": "Hearing impairment simulation: an interactive multimedia programme on the internet for students of speech therapy",
   "original": "e97_1795",
   "page_count": 4,
   "order": 311,
   "p1": "1795",
   "pn": "1798",
   "abstract": [
    "Students of speech therapy and audiology are often faced with the difficulty of obtaining a realistic view of the speech reception abilites of the hearing impaired. Illustrating speech recognition defects is a demanding task for the teaching staff, too. To improve the students' awareness of the effects of hearing defects, an interactive multimedia programme allowing simulation of various types of hearing impairment was constructed and placed on an Internet media server for online use. The simulated hearing-impaired speech material was produced using digital signal processing (e.g. mixing and filtering of speech and noise) and multimedia and audio technologies which enable streaming of sound files on the Internet. In the programme, word recognition scores for several degrees and types of hearing impairment in varying conditions of background noise and reverberation time can also be computed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-310"
  },
  "ciocea97_eurospeech": {
   "authors": [
    [
     "Sorin",
     "Ciocea"
    ],
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Lisa",
     "Crevier-Buchman"
    ]
   ],
   "title": "Analysis of dysarthric speech by means of formant-to-area mapping",
   "original": "e97_1799",
   "page_count": 4,
   "order": 312,
   "p1": "1799",
   "pn": "1802",
   "abstract": [
    "This article presents a preliminary study of dysarthric speech by means of formant-to-area mapping. Dysarthria is a speech impairment which is the result of paralysis or ataxia of the speech muscles. Formant-to-area mapping is the inference of the shape of a tract model via observed formant frequencies. The corpus is composed of vowel-vowel sequences [iaia] produced by speakers suffering from amyotrophic lateral sclerosis (ALS) and normal speakers. The results show that the shapes and movements of the acoustically mapped area function models are typical of the motions and postures of the vocal tracts of ALS speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-311"
  },
  "lobanov97_eurospeech": {
   "authors": [
    [
     "Boris M.",
     "Lobanov"
    ],
    [
     "Simon V.",
     "Brickle"
    ],
    [
     "Andrey V.",
     "Kubashin"
    ],
    [
     "Tatiana V.",
     "Levkovskaja"
    ]
   ],
   "title": "An intelligent telephone answering system using speech recognition",
   "original": "e97_1803",
   "page_count": 4,
   "order": 313,
   "p1": "1803",
   "pn": "1806",
   "abstract": [
    "The computer system described in this paper answers incoming telephone calls and employs speaker-independent speech recognition to identify callers. The users of the system can define caller-specific treatment and change this treatment using a graphical user interface. Apart from relating and receiving spoken messages, the system also offers advanced telephony features such as paging and call forwarding, providing the required subscription services are available from the telephone company. Standard interfaces to the telephony and audio hardware are used, so that the system runs on a desktop PC equipped with a voice-enabled modem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-312"
  },
  "ackermann97_eurospeech": {
   "authors": [
    [
     "Ulla",
     "Ackermann"
    ],
    [
     "Bianca",
     "Angelini"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Speedata: a prototype for multilingual spoken data-entry",
   "original": "e97_1807",
   "page_count": 4,
   "order": 314,
   "p1": "1807",
   "pn": "1810",
   "abstract": [
    "In this work we describe the development and evaluation of SpeeData, a prototype for multilingual spoken data-entry. The SpeeData project aims at developing a demonstrator that provides a user-friendly interface for spoken data-entry in two languages: Italian and German. A real world application domain is considered, which is the Land Register of an Italian region in which both languages are officially spoken. Original topics of this paper are the interaction modality for spoken data-entry, the evaluation of a data-entry system, bilingual speech recognition, bilingual speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-313"
  },
  "karjalainen97_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Peter",
     "Boda"
    ],
    [
     "Panu",
     "Somervuo"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Applications for the hearing-impaired: evaluation of finnish phoneme recognition methods",
   "original": "e97_1811",
   "page_count": 4,
   "order": 315,
   "p1": "1811",
   "pn": "1814",
   "abstract": [
    "It has been hypothesized that the Finnish language is well suited to speech-to-text conversion for the communication aids of the hearing impaired. In a related study it was shown that, depending on context, 10 to 20 % of phoneme errors can be tolerated with good comprehension when reading text converted from raw phonemic recognition. Two sets of phoneme recognition experiments were carried out in this study in order to evaluate the performance of existing speech recognition systems in this ap-plication. For telephone bandwidth speech both systems showed speaker dependent error scores of about 10 % or below, thus supporting the feasibility of the application. For speaker independent cases the error rate was typically more than 20 % which is too high for effortless and fluent communication.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-314"
  },
  "alarotu97_eurospeech": {
   "authors": [
    [
     "Nina",
     "Alarotu"
    ],
    [
     "Mietta",
     "Lennes"
    ],
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Anja",
     "Malm"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "Applications for the hearing-impaired: comprehension of finnish text with phoneme errors",
   "original": "e97_1815",
   "page_count": 4,
   "order": 316,
   "p1": "1815",
   "pn": "1818",
   "abstract": [
    "This study simulates the phoneme errors made by speech recognizers and determines the phoneme error level at which a reasonable comprehension of text can still be achieved. Finnish is written almost phonemically and Finnish-speakers have no trouble in comprehending phonemic text. Phonemically corrupted text was presented to normal-hearing, hearing-impaired as well as deaf subjects and their comprehension levels were measured. According to this study, current speech recognition methods allow for limited applications in this field.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-315"
  },
  "ehrlich97_eurospeech": {
   "authors": [
    [
     "Ute",
     "Ehrlich"
    ],
    [
     "Gerhard",
     "Hanrieder"
    ],
    [
     "Ludwig",
     "Hitzenberger"
    ],
    [
     "Paul",
     "Heisterkamp"
    ],
    [
     "Klaus",
     "Mecklenburg"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Access - automated call center through speech understanding system",
   "original": "e97_1819",
   "page_count": 4,
   "order": 317,
   "p1": "1819",
   "pn": "1822",
   "abstract": [
    "This paper will describe the results of a high sophisticated speech application project. ACCeSS is an EU project with Greek and German partners. Our Greek partners are Knowledge S.A. and the University of Patras. In this paper we report about the German application of the project. The project addresses a first step for automation of call centers for personal intensive applications in insurances. New forms of insurance operation are more and more using the telephone or direct mailing for the contact between an insurance and its customers. This makes the business more efficient and more direct than the classical operation with agents. Routine contractual details can easily be handled in such a way and all other cases of insurance actions can be realised with such communication media, too. A direct insurance company running a large call center is involved in the project and pays attention to the functionality of the system. The user needs are analysed using Wizard-of-Oz experiments. The system structure, a sketch of the algorithms and modules, and first results of evaluation will be given. Keywords: speech dialogue, Call Center, Wizard-of-Oz, dialogue strategy, semantics, ACCeSS\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-316"
  },
  "anthony97_eurospeech": {
   "authors": [
    [
     "E. Richard",
     "Anthony"
    ],
    [
     "Charles",
     "Bowen"
    ],
    [
     "Margot T.",
     "Peet"
    ],
    [
     "Susan",
     "Tammaro"
    ]
   ],
   "title": "Integrating a radio model with a spoken language interface for military simulations",
   "original": "e97_1823",
   "page_count": 4,
   "order": 318,
   "p1": "1823",
   "pn": "1826",
   "abstract": [
    "We incorporated a simulated military radio into a spoken language interface to a distributed simulation environment for military commander training. The resulting architecture bypassed the inherent problem of acoustic mismatch that arises in integrating radio output with a speech recognition front end, while at the same time preserving the realism of speech synthesis output through a military radio. We assessed the utility of formal evaluation methods to benchmark the impact of the radio model on a commercial speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-317"
  },
  "falavigna97_eurospeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "On field experiments of continuous digit recognition over the telephone network",
   "original": "e97_1827",
   "page_count": 4,
   "order": 319,
   "p1": "1827",
   "pn": "1830",
   "abstract": [
    "In this paper a continuous digit recognizer over the telephone network in real time will be de- scribed. The activity has allowed the realization of a system, installed in some Italian telephone exchanges, for providing semi-automatic collect call services. Data collection has also been per- formed, and a field database was built. Either a continuous digit recognition task and a confirmation task, requiring rejection, have been defined. Recognition results are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-318"
  },
  "menendezpidal97_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Menendez-Pidal"
    ],
    [
     "James B.",
     "Polikoff"
    ],
    [
     "H.Timothy",
     "Bunnell"
    ]
   ],
   "title": "An HMM-based phoneme recognizer applied to assessment of dysarthric speech",
   "original": "e97_1831",
   "page_count": 3,
   "order": 320,
   "p1": "1831",
   "pn": "1834",
   "abstract": [
    "This paper describes work on the development of an HMM-based system for automatic speech assessment, particularly of dysarthric speech. As a first step, we compare recognizer performance on a closed-set, forced choice identification test of dysarthric speech with performance on the same test by untrained listeners. Results indicate that HMM recognition accuracy averaged over all utterances of a dysarthric talker is well-correlated with measures of overall talker intelligibility. However, on an utterance-by-utterance basis, the pattern of errors obtained from the human subjects and the machine, while significantly correlated, accounts for, at best, only about 25 percent of the variance. Potential methods for improving this performance are considered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-319"
  },
  "torre97b_eurospeech": {
   "authors": [
    [
     "Celinda de la",
     "Torre"
    ],
    [
     "Gonzalo",
     "Alonso"
    ]
   ],
   "title": "Multiapplication platform based on technology for mobile telephone network services",
   "original": "e97_1835",
   "page_count": 4,
   "order": 321,
   "p1": "1835",
   "pn": "1838",
   "abstract": [
    "This paper describes a new platform developed at Telefonica I+D, based on Speech Technology, for being integrated in the Mobile Telephone Network, and specially suitable for quick incorporation of new customer demanded services. This multiapplication platform has been conceived as \"call-basis\" driven, that means, that the number called by the user will determine which application must be run at each time in order to serve him. The system permits dynamically redistribute the lines assigned to each application following traffic criteria. In the other hand, the use of dynamic libraries allow a quick procedure to update, incorporate or eliminate applications. The integration of computers with the telephone network into commercial equipments has become feasible due to the proliferation of inexpensive Personal Computers and advances in speech processing. The availability of the Speech Technology Products of Telefonica of Spain [1] [2] [3] [4] has made possible to develop speech based servers for new telephone services in a rapid and reasonable way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-320"
  },
  "os97_eurospeech": {
   "authors": [
    [
     "Els den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "David",
     "James"
    ],
    [
     "Richard",
     "Winski"
    ],
    [
     "Kurt",
     "Fridh"
    ]
   ],
   "title": "Field test of a calling card service based on speaker verification and automatic speech recognition",
   "original": "e97_1839",
   "page_count": 4,
   "order": 322,
   "p1": "1839",
   "pn": "1842",
   "abstract": [
    "In this research we have studied several human factors problems that are connected to the deployment of speaker verification technology in telecommunication services. We investigate the perception of the safety of a calling card service when it is protected by speaker verification on the 14 digit card number, and compare it to the perceived safety of speaker verification and PIN. Moreover, we compare a voice based interface to the service with a DTMF based interface. The results are crucial for guiding the introduction and deployment of speaker verification technology in actual applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-321"
  },
  "julia97_eurospeech": {
   "authors": [
    [
     "Luc E.",
     "Julia"
    ],
    [
     "Adam J.",
     "Cheyer"
    ]
   ],
   "title": "Speech: a privileged modality",
   "original": "e97_1843",
   "page_count": 3,
   "order": 323,
   "p1": "1843",
   "pn": "1846",
   "abstract": [
    "Ever since the publication of Bolt's ground-breaking \"Put-That There\" paper [1], providing multiple modalities as a means of easing the interaction between humans and computers has been a desirable attribute of user interface design. In Bolt's early approach, the style of modality combination required the user to conform to a rigid order when entering spoken and gestural commands. In the early 1990s, the idea of synergistic multimodal combination began to emerge [4], although actual implemented systems (generally using keyboard and mouse) remained far from being synergistic. Next-generation approaches involved time-stamped events to reason about the fusion of multimodal input arriving in a given time window, but these systems were hindered by time-consuming matching algorithms. To overcome this limitation, we proposed [6] a truly synergistic application and a distributed architecture for flexible interaction that reduces the need for explicit time stamping. Our slot-based approach is command directed, making it suitable for applications using speech as a primary modality. In this article, we use our interaction model to demonstrate that during multimodal fusion, speech should be a privileged modality, driving the interpretation of a query, and that in certain cases, speech has even more power to override and modify the combination of other modalities than previously believed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-322"
  },
  "gauvain97_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Transcription of broadcast news",
   "original": "e97_0907",
   "page_count": 4,
   "order": 324,
   "p1": "907",
   "pn": "910",
   "abstract": [
    "In this paper we report on our recent work in transcrib-ing broadcast news shows. Radio and television broad-casts contain signal segments of various linguistic and acoustic natures. The shows contain both prepared and spontaneous speech. The signal may be studio quality or have been transmitted over a telephone or other noisy channel (ie., corrupted by additive noise and nonlinear distorsions), or may contain speech over music. Transcription of this type of data poses challenges in dealing with the continuous stream of data under varying conditions. Our approach to this problem is to segment the data into a set of categories, which are then processed with category specific acoustic models. We describe our 65k speech recognizer and experiments using different sets of acoustic models for transcription of broadcast news data. The use of prior knowledge of the segment bound-aries and types is shown to not crucially affect the performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-323"
  },
  "alleva97_eurospeech": {
   "authors": [
    [
     "Fil",
     "Alleva"
    ],
    [
     "Xuedong",
     "Huang"
    ],
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Li",
     "Jiang"
    ]
   ],
   "title": "Can continuous speech recognizers handle isolated speech?",
   "original": "e97_0911",
   "page_count": 4,
   "order": 325,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "Continuous speech is far more natural and efficient than isolated speech for communication. However, for current state-of-the-art of automatic speech recognition systems, isolated speech recognition (ISR) is far more accurate than continuous speech recognition (CSR). It is a common practice in the speech research community to build CSR systems using only CSR data. In doing this we ignore the fact that isolated (a.k.a. discrete) speech is a special case of continuous speech. A slowing of the speaking rate is a natural reaction for a user faced with the high error rates of current CSR systems. Ironically, CSR systems typically have a much higher word error rate when speakers slow down since the acoustic models are usually derived exclusively from continuous speech corpora. In this paper, we summarize our efforts to improve the robustness of our speaker-independent CSR system without suffering a recognition accuracy penalty. In particular the multi-style trained system described in this paper attains a 7.0% word error rate for a test set consisting of both isolated and continuous speech, in contrast to the 10.9% word error rate achieved by the same system trained only on continuous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-324"
  },
  "matsuok97_eurospeech": {
   "authors": [
    [
     "Tatsuo",
     "Matsuok"
    ],
    [
     "Yuichi",
     "Taguchi"
    ],
    [
     "Katsutoshi",
     "Ohtsuki"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Toward automatic transcription of Japanese broadcast news",
   "original": "e97_0915",
   "page_count": 4,
   "order": 326,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "In this paper, we report on the automatic recognition of Japanese broadcast-news speech. We have been working on large-vocabulary continuous speech recognition (LVCSR) for Japanese newspaper speech transcription and have achieved good performance. We have recently applied our LVCSR system to transcribing Japanese broadcast-news speech. We extended the vocabulary from 7k words to 20k words and trained the language models using newspaper texts and broadcast-news manuscripts. These two language models were applied to our evaluation speech sets. The language model trained using broadcast-news manuscripts achieved better results for broadcast-news speech than the language model trained using newspaper texts, which achieved better results for newspaper speech. We achieved a word error rate of 19.7% for anchor-speaker's speech by using a bigram language model and a trigram language model both trained using broadcast-news manuscripts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-325"
  },
  "cettolo97_eurospeech": {
   "authors": [
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Anna",
     "Corazza"
    ]
   ],
   "title": "Automatic detection of semantic boundaries",
   "original": "e97_0919",
   "page_count": 4,
   "order": 327,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "In spoken language systems, the segmentation of utter- ances into coherent linguistic/semantic units is very use- ful, as it makes easier processing after the speech recog- nition phase. In this paper, a methodology for semantic boundary prediction is presented and tested on a corpus of person-to-person dialogues. The approach is based on bi- nary decision trees and uses text context, including broad classes of silent pauses, filled pauses and human noises. Best results give more than 90% precision, almost 80% recall and about 3% false alarms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-326"
  },
  "bauche97_eurospeech": {
   "authors": [
    [
     "Etienne",
     "Bauche"
    ],
    [
     "Bojana",
     "Gajic"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Tatsuo",
     "Matsuoka"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Connected digit recognition in spontaneous speech",
   "original": "e97_0923",
   "page_count": 4,
   "order": 328,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "In this paper, we describe a new recognition system for 4-digit-strings in Japanese under fluent speech conditions. In particular, we introduce several methods to solve the problems related to the spontaneity of speech: discrimination of speech and background noise, out-of- vocabulary words, pauses between digits, etc. These methods led to an error rate reduction of 76%, compared to a simple start- and end-point detection based recognizer using non-refined models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-327"
  },
  "kubala97_eurospeech": {
   "authors": [
    [
     "Francis",
     "Kubala"
    ],
    [
     "Hubert",
     "Jin"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Advances in transcription of broadcast news",
   "original": "e97_0927",
   "page_count": 4,
   "order": 329,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "In this paper, we describe our recent work in automatic transcription of broadcast news programming from ra- dio and television. This is a very challenging recogni- tion problem because of the frequent and unpredictable changes that occur in speaker, speaking style, topic, chan- nel, and background conditions. Faced with such a prob- lem, there is a strong tendency to try to carve the in- put into separable classes and deal with each one inde- pendently. We have chosen instead to rely on condition- independent models and adaptive algorithms to deal with this highly variable data. In addition, we have developed effective techniques to automatically segment the input waveform and cluster the segments into data sets contain- ing similar speakers and conditions to support unsuper- vised adaptation on the test. Using this general approach, we achieved the best overall word error rate of 31.8% on the 1996 DARPA Hub-4 Unpartitioned Evaluation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-328"
  },
  "cambierlangeveld97_eurospeech": {
   "authors": [
    [
     "Tina",
     "Cambier-Langeveld"
    ],
    [
     "Marina",
     "Nespor"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "The domain of final lengthening in production and perception in Dutch",
   "original": "e97_0931",
   "page_count": 4,
   "order": 330,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "Two production experiments investigating possible factors influencing the domain of final lengthening are described. Results indicate that final lengthening is generally confined to the final syllable, except when its rhyme contains only a schwa, in which case the penultimate rhyme is lengthened as well. Apparently, only the weight of the final syllable influences the size of the domain which is lengthened. Next, a perceptual acceptability experiment was run. Results indicate that while listeners are sensitive to differences in the amount of final lengthening, they are not very sensitive to the way this is distributed over the preboundary segments. Apparently, the specific distribution of final lengthening in production has no com-municative function, but is the result of the human speech mechanism, together with restrictions on the expandability of segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-329"
  },
  "meunier97_eurospeech": {
   "authors": [
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Voicing assimilation as a cue for cluster identification",
   "original": "e97_0935",
   "page_count": 4,
   "order": 331,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "It is well known now that speech chain is not constituted by discrete units. Speech sounds have an influence on other sounds directly in contact with them. We hypothesize that this influence is not noise but plays an important role for perception. An experiment is managed to evaluate the relative importance of two kinds of cues: those of phonetic distinctive features (voiced and unvoiced) and those of voicing assimilation (for liquids). Our results confirm that voicing assimilation of liquids plays an important role to identifie clusters: a/ the absence of assimilation cues increases reaction times; b/ subjects use assimilation cues in preference to distinctive features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-330"
  },
  "riele97_eurospeech": {
   "authors": [
    [
     "Saskia M.M. te",
     "Riele"
    ],
    [
     "Manon",
     "Loef"
    ],
    [
     "O. van",
     "Herwijnen"
    ]
   ],
   "title": "On the perceptual relevance of degemination in Dutch",
   "original": "e97_0939",
   "page_count": 4,
   "order": 332,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "The aim of this study is to determine whether the phonological process of degemination, in which one of two adjacent and identical consonants is deleted, is perceptually complete when it applies over word boundaries. Measurements on the duration of the boundary consonant have shown durational differences between two-word phrases with underlying single and double consonants, even at fast speech rates. Results of a pseudo-gating experiment using a binary forced choice task show that correct segmentation of two-word phrases with underlying single or double consonants, spoken at a fast speech rate, does not exceed chance level. We conclude, therefore, that degemination actually occurs in Dutch and that this process is perceptually complete. Implications for word recognition will also be discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-331"
  },
  "fougeron97_eurospeech": {
   "authors": [
    [
     "Cecile",
     "Fougeron"
    ],
    [
     "Donca",
     "Steriade"
    ]
   ],
   "title": "Does deletion of French SCHWA lead to neutralization of lexical distinctions?",
   "original": "e97_0943",
   "page_count": 4,
   "order": 333,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "This study explores the hypothesis that relatively invariant properties characterizing lexical items include non contrastive phonetic details such as the amount of linguopalatal contact, or aspects of inter- gestural timing. We show that, in French, a sequence of consonants resulting from the loss of schwa maintains some of the fine articulatory characteristics of the lexical form containing schwa. Such characteristics distinguish this sequence of consonants from an underlying cluster. Thus, we show that \"d'rôle\" 'some role', with the apostrophe indicating schwa loss, remains articulatorily distinct from \"drôle\" 'funny'. A perception experiment shows that the two types of sequences (CC and C'C) are only marginally discriminable by French listeners. However, when the subjects identify correctly the two types of sequences, the distinct characteristics identified in production correlate with the listeners' judgments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-332"
  },
  "bruyninckx97_eurospeech": {
   "authors": [
    [
     "Marielle",
     "Bruyninckx"
    ],
    [
     "Bernard",
     "Harmegnies"
    ]
   ],
   "title": "An approach of the catalan palatals discrimination based on durational patterns of spectral evolution",
   "original": "e97_0947",
   "page_count": 4,
   "order": 334,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "Catalan has been studied by several authors, who gave phonologic as well as phonetic descriptions of the language. In most studies, authors have nevertheless focussed on categories of sounds, considered each in turn, rather than on possible associations of sounds. Implementations of this knowing may therefore raise problems, since acoustical patterns of dvnamical evolution are not well known. The paper addresses the particular case of the []-[lj] and [n]-[nj] acoustical differences. The data show that the differences to be found are mainly related to reorganisations of the time function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-333"
  },
  "gros97b_eurospeech": {
   "authors": [
    [
     "Jerneja",
     "Gros"
    ],
    [
     "Nikola",
     "Pavesic"
    ],
    [
     "France",
     "Mihelic"
    ]
   ],
   "title": "Syllable and segment duration at different speaking rates in the Slovenian language",
   "original": "e97_0951",
   "page_count": 4,
   "order": 335,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "Speech timing at different speaking rates was studied for the Slovenian language and the results were applied in the two level duration prediction model in the Slovenian text-to-speech system S5 [1]. In order to provide the synthesiser with the possibility to pronounce input text with several speaking rates, tests were made to study the impact of speaking rate on syllable duration and duration of individual phonemes and phoneme groups for the Slovenian language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-334"
  },
  "li97b_eurospeech": {
   "authors": [
    [
     "Wei-Ying",
     "Li"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Hybrid networks based on RBFN and GMM for speaker recognition",
   "original": "e97_0955",
   "page_count": 4,
   "order": 336,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "In this paper, a hybrid network based on the combination of Radial Basis Function Networks (RBFNs) and Gaussian Mixture Models (GMMs) is proposed and used for speaker recognition. The hybrid network is a hierarchical one, where a GMM is built for each speaker and an RBFN is built for each group of speakers. The GMMs and RBFNs are trained independently. The RBFNs are used as a first stage coarse classifier and the GMMs are used as the final classifier. For each RBFN, only the first several candidates are chosen to take part in the final classification. The hybrid system is used for the SPIDRE database speaker recognition. Some experiments were carried out to choose the proper structure and parameters of RBFNs and GMMs. After using RBFNs, about 40% speakers were excluded without decreasing the performance. If the most confusable speaker sets in GMMs are grouped into RBFNs, the performance of GMMs can be increased more by using RBFNs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-335"
  },
  "he97_eurospeech": {
   "authors": [
    [
     "Jialong",
     "He"
    ],
    [
     "Li",
     "Liu"
    ],
    [
     "Günther",
     "Palm"
    ]
   ],
   "title": "A discriminative training algorithm for Gaussian mixture speaker models",
   "original": "e97_0959",
   "page_count": 4,
   "order": 337,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "The Gaussian mixture speaker model (GMM) is usually trained with the expectation-maximization (EM) algorithm to maximize the likelihood (ML) of observation data from an individual class. The GMM trained based the ML criterion has weak discriminative power when used as a classifier. In this paper, a discriminative training procedure is proposed to fine-tune the parameters in the GMMs. The goal of the training is to reduce the number of misclassified vector groups. Since a vector group can be thought as derived from a short sentence, this training procedure optimize the speaker identification performance more directly. Even though the algorithm itself is based on an heuristic idea, it works fine for many practical problems. Besides, the training speed is very fast. In an evaluation experiment with the YOHO database, when each speaker is modeled with 8 mixtures, the identification rate increases from 83.8% to 92.4% after applying this discriminative training algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-336"
  },
  "reynolds97_eurospeech": {
   "authors": [
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "Comparison of background normalization methods for text-independent speaker verification",
   "original": "e97_0963",
   "page_count": 4,
   "order": 338,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "This paper compares two approaches to background model representation for a text-independent speaker verification task using Gaussian mixture models. We compare speaker-dependent background speaker sets to the use of a universal, speaker-independent background model (UBM). For the UBM, we describe how Bayesian adaptation can be used to derive claimant speaker models, providing a structure leading to significant computational savings during recognition. Experiments are conducted on the 1996 NIST Speaker Recognition Evaluation corpus and it is clearly shown that a system using a UBM and Bayesian adaptation of claimant models produces superior performance compared to speaker-dependent background sets or the UBM with independent claimant models. In addition, the creation and use of a telephone handset-type detector and a procedure called hnorm is also described which shows further, large improvements in verification performance, especially under the difficult mismatched handset conditions. This is believed to be the first use of applying a hand-set- type detector and explicit handset- type normalization for the speaker verification task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-337"
  },
  "kimball97_eurospeech": {
   "authors": [
    [
     "Owen",
     "Kimball"
    ],
    [
     "Michael",
     "Schmidt"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Jason",
     "Waterman"
    ]
   ],
   "title": "Speaker verification with limited enrollment data",
   "original": "e97_0967",
   "page_count": 4,
   "order": 339,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "New methods for speaker verification that address the problems of limited training data and unknown telephone channel are presented. We describe a system for studying the feasibility of telephone based voice signatures for electronic documents that uses speaker verification with a fixed test phrase but very limited data for training speaker models. We examine three methods for speaker verification that address these characteristics in different ways, including text- independent mixture models, a broad phonetic category model that has some of the properties of both text-dependent and text-independent approaches, and a text-dependent approach based on speaker adaptation. The speaker-adaptive approach is shown to have significantly better performance when the training and test channel conditions are mismatched, resulting in better overall performance across all conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-338"
  },
  "bimbot97_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Hans-Peter",
     "Hutter"
    ],
    [
     "Cedric",
     "Jaboulet"
    ],
    [
     "Johan W.",
     "Koolwaaij"
    ],
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Jean-Benoit",
     "Pierrot"
    ]
   ],
   "title": "Speaker verification in the telephone network: research activities in the cave project",
   "original": "e97_0971",
   "page_count": 4,
   "order": 340,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "This paper summarizes the main results from the Speaker Verification (SV) research pursued so far in the CAVE project. Different state-of the art SV algo- rithms were implemented in a common HMM frame- work and compared on two databases : YOHO (of fice environment speech) and SESP (telephone speech). This paper is concerned with the different design issues for LR-HMM-based SV algorithms which emerged from our investigations and which led to our current SV sys- tem, which delivers Equal Error Rates below 0.5 % on a very realistic telephone speech database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-339"
  },
  "kuitert97_eurospeech": {
   "authors": [
    [
     "Mark",
     "Kuitert"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Speaker verification with GSM coded telephone speech",
   "original": "e97_0975",
   "page_count": 4,
   "order": 341,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "In this paper we investigate the impact on the performance of Speaker Verification (SV) systems of the signal and channel coding in GSM cellular telephone networks. In this study only the effects of the codec are investigated. This is done by transcoding the signals in an existing speech corpus, recorded in the fixed network, to GSM. We compared text dependent SV performance of systems trained with A-law speech and tested with A-law and GSM speech, as well as systems trained with GSM speech and tested with GSM speech. All SV systems compared were based on continuous density Gaussian mixtures HMM models, differing in acoustic resolution. We have compared several parameter representations derived from FFT and LPC based spectral estimates. It is shown that (and why) LPC based estimates are to be preferred. It is also shown that it pays to extend the analysis bandwidth to the full 4 kHz offered by the digital telephone network. The major conclusion of our research is that the impact of GSM coding on the parameter representations is marginal and can effectively be ignored.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-340"
  },
  "rosenberg97_eurospeech": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "S.",
     "Parthasarathy"
    ]
   ],
   "title": "Speaker identification with user-selected password phrases",
   "original": "e97_1371",
   "page_count": 4,
   "order": 342,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "An open-set speaker identification system is described in which general-text, sentence-long phrases are used as passwords. Customers are allowed to select their own password phrases and the system has no knowledge of the text. Passwords are represented by phone transcriptions and whole-phrase Hidden Markov Models (HMM's). Phrase identification, carried out using both speaker dependent and speaker independent models, constitutes an identity claim. Verification of the claim uses likelihood ratio scoring with speaker independent phone HMM's providing the background model score. An evaluation has been carried out over a database of password phrases spoken by 250 speakers. 100 of the speakers are test speakers. In an experimental trial, each test speaker is designated as a customer or an imposter and speaks the phrase associated with the customer. The imposter set for each customer consists of same-gender test speakers excluding the customer. At a 5% reject level, the rate of imposter identification is approximately 4%. The misidentification rate for both customers and imposters is less than 0.1%. The closed-set identification error rate is less than 1%, while the average verification equal-error rate is approximately 3%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-341"
  },
  "olsen97_eurospeech": {
   "authors": [
    [
     "Jesper O.",
     "Olsen"
    ]
   ],
   "title": "Speaker verification based on phonetic decision making",
   "original": "e97_1375",
   "page_count": 4,
   "order": 343,
   "p1": "1375",
   "pn": "1378",
   "abstract": [
    "Speaker verification based on phone modelling is examined in this paper. Phone modelling is attractive, because different phonemes have different levels of usefulness for speaker recognition, and because phone modelling essentially makes a speaker verification algorithm text inde-pendent. The speaker verification system used here is based on a two stage approach, where speech recognition (segmentation) is separated from the actual speaker modelling. Hidden Markov Models are employed in the initial stage, whereas Radial Basis Function networks are used in the second for modelling speaker identity. The system is evaluated on a large realistic telephone database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-342"
  },
  "ariyaeeinia97_eurospeech": {
   "authors": [
    [
     "A. M.",
     "Ariyaeeinia"
    ],
    [
     "P.",
     "Sivakumaran"
    ]
   ],
   "title": "Analysis and comparison of score normalisation methods for text-dependent speaker verification",
   "original": "e97_1379",
   "page_count": 4,
   "order": 344,
   "p1": "1379",
   "pn": "1382",
   "abstract": [
    "This paper presents an investigation into the relative effectiveness of various score normalisation methods for speaker verification. The study provides a thorough analysis of different approaches for normalising verification scores, and comparatively examines these under identical experimental conditions. The experiments are based on the use of subsets of the Brent (telephone quality) speech database, consisting of repetitions of isolated digit utterances zero to nine spoken by native English speakers. Based on the experimental results it is demonstrated that amongst the considered methods, a particular form of the cohort normalisation method provides the best performance in terms of the verification accuracy. The paper discusses details of the experimental study and presents an analysis of the results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-343"
  },
  "jauquet97_eurospeech": {
   "authors": [
    [
     "Frederic",
     "Jauquet"
    ],
    [
     "Patrick",
     "Verlinde"
    ],
    [
     "Claude",
     "Vloeberghs"
    ]
   ],
   "title": "Automatic speaker recognition on a vocoder link",
   "original": "e97_1383",
   "page_count": 4,
   "order": 345,
   "p1": "1383",
   "pn": "1386",
   "abstract": [
    "Automatic speaker recognition on a vocoder link has rarely been explicitly tested. In this paper, we show how the automatic speaker recognition could be used on a vocoder link. In a first experiment where we consider the \"coder-link-decoder\" speech system as a black box, a classic speaker recognition method (applied on the reconstructed speech) is shown to be able to provide an objective measurement of the voice quality of the vocoder. In a second experiment, the same speaker recognition method is directly applied on the information contained in the coded frames. In latter case, the recognition scores provide an interesting analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-344"
  },
  "bimbot97b_eurospeech": {
   "authors": [
    [
     "Frederic",
     "Bimbot"
    ],
    [
     "Dominiqne",
     "Genoud"
    ]
   ],
   "title": "Likelihood ratio adjustment for the compensation of model mismatch in speaker verification",
   "original": "e97_1387",
   "page_count": 4,
   "order": 346,
   "p1": "1387",
   "pn": "1390",
   "abstract": [
    "Cet article presente une methode d'ajustement des seuils de verification du locuteur basee sur un modele Gaussien des distributions du logarithme du rapport de vraisemblance. L'article expose les hypotheses sous lesquelles ce modele est valide, indique plusieurs methodes d'ajustement des seuils, et en illustre les apports et les limites par des experiences de verification sur une base de donnees de 20 locuteurs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-345"
  },
  "sonmez97_eurospeech": {
   "authors": [
    [
     "M. Kemal",
     "Sönmez"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Mitchel",
     "Weintraub"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "A lognormal tied mixture model of pitch for prosody based speaker recognition",
   "original": "e97_1391",
   "page_count": 4,
   "order": 347,
   "p1": "1391",
   "pn": "1394",
   "abstract": [
    "Statistics of pitch have recently been used in speaker recognition systems with good results. The success of such systems depends on robust and accurate computation of pitch statistics in the presence of pitch tracking errors. In this work, we develop a statistical model of pitch that allows unbiased estimation of pitch statistics from pitch tracks which are subject to doubling and/or halving. We first argue by a simple correlation model and empirically demonstrate by QQ plots that \"clean\" pitch is distributed with a lognormal distribution rather than the often assumed normal distribution. Second, we present a probabilistic model for estimated pitch via a pitch tracker in the presence of doubling/halving, which leads to a mixture of three lognormal distributions with tied means and variances for a total of four free parameters. We use the obtained pitch statistics as features in speaker verification on the March 1996 NIST Speaker Recognition Evaluation data (subset of Switchboard) and report results on the most difficult portion of the database: the \"one-session\" condition with males only for both the claimant and imposter speakers. Pitch statistics provide 22% reduction in false alarm rate at 1 % miss rate and I 1 % reduction in false alarm rate at 10% miss rate over the cepstrum-only system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-346"
  },
  "campbell97_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Tony",
     "Hebert"
    ],
    [
     "Ezra",
     "Black"
    ]
   ],
   "title": "Parsers, prominence, and pauses",
   "original": "e97_0979",
   "page_count": 4,
   "order": 348,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "We present results of a comparison between two prosody prediction algorithms, showing that the incorporation of information from a parser results in significantly improved performance for our text-to- speech synthesiser. We used a stochastic tree-based parser to generate a tagged and bracketed representation of the input text, and then interpreted this higher-level information to produce a ToBI-type prosodic annotation of the text. From this annotation an intonation contour was predicted for use in synthesising the speech. Results show that prediction of prosodic phrasing and focal prominence are improved by 56% and 62% respectively over previous methods compared against a human reading of the same test utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-347"
  },
  "bechet97_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Marc",
     "El-Bèze"
    ]
   ],
   "title": "Automatic assignment of part-of-speech to out-of-vocabulary words for text-to-speech processing",
   "original": "e97_0983",
   "page_count": 4,
   "order": 349,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "Working with large corpora of text highlights the need for the special treatment of Out-Of-Vocabulary (OOV) words. This paper describes a strategy for processing OOV words within a Text-To-Speech (TTS) framework of the French language. A probabilistic module, called \"Devin\", guesses a Part-Of-Speech (POS) for each OOV word according to the morphological structure of the word and the context in which it occurs. These POS can be either syntactic or semantic. The semantic labels represent the categories of each proper-name (family name, town name, etc.) and its linguistic origin which has a strong influence on its pronunciation. According to these POS, the system chooses the correct set of rules which will be employed by the rule- based grapheme-to-phoneme transcriber of the TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-348"
  },
  "gilifivela97_eurospeech": {
   "authors": [
    [
     "Barbara",
     "Gili Fivela"
    ],
    [
     "Silvia",
     "Quazza"
    ]
   ],
   "title": "Text-to-prosody parsing in an Italian speech synthesizer. recent improvements",
   "original": "e97_0987",
   "page_count": 4,
   "order": 350,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "This paper describes recent improvements of a Prosodic Analyzer, designed to provide the CSELT Italian text-to-speech system ELOQUENSa with a better handling of prosody. Based on lexical tagging, the Analyzer builds up the prosodic structure of the sentence, inserting proper prosodic markers at word boundaries. The approach belongs to the family of TTS-oriented, 'heuristic' strategies, inferring prosody directly from the building blocks of syntax and exploiting lexical and rhythmical language-dependent information. Latest improvements concern the linguistic knowledge base of the Analyzer, which has been enhanced and optimized. A formal evaluation of the Analyzer's performances is also presented in the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-349"
  },
  "krenn97_eurospeech": {
   "authors": [
    [
     "Brigitte",
     "Krenn"
    ]
   ],
   "title": "Tagging syllables",
   "original": "e97_0991",
   "page_count": 4,
   "order": 351,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "Syllabification is viewed as a tagging task. Phonemes constituting a syllable are treated like words in a sentence. Each phoneme is annotated with informa- tion representing the phoneme itself, and its posi- tion within a syllable. Within a number of tagging experiments, the specificity of linguistic information represented in the tag set is varied. The annotation scheme which encodes an onset-nucleus-coda model is shown to lead to the best tagging results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-350"
  },
  "black97b_eurospeech": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Assigning phrase breaks from part-of-speech sequences",
   "original": "e97_0995",
   "page_count": 4,
   "order": 352,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "One of the important stages in the process of turning unmarked text into speech is the assignment of appropriate phrase break boundaries. Phrase break boundaries are important to later modules including accent assignment, duration control and pause insertion. A number of different algorithms have been proposed for such a task, ranging from the simple to the complex. These different algorithms require different information such as part of speech tags, syntax and even semantic understanding of the text. Obviously these requirements come at differing costs and it is important to trade off difficulty in finding particular input features versus accuracy of the model. The simplest models are deterministic rules. A model simply inserting phrase breaks after punctuation is rarely wrong in assignment, but massively underpredicts as it will allow overly long phrases when the text contains no punctuation. More complex rule-driven models such as [1] involve much more detailed rules and require the input text to be parsed. On the other hand statistically based models offer the advantages of automatic training which make movement to a new domain or language much easier. Simple direct CART models using features such as punctuation, part of speech, accent positions etc. can produce reasonable results [5]. Other more complex stochastic methods optimising assignment over whole utterances (e.g. [8]) have also been developed. An important restriction that sometimes is ignored in these algorithms is that the inputs to the phrase break assignment algorithm have to be available at phrase break assignment time, and themselves be predictable from raw text. For example, some algorithms require accent assignment information but we believe accent assignment can only take place after prosodic boundaries are identified. A second example is the requirement of syntactic parsing of the input without providing a syntactic parser to achieve this. Thus we have ensured that both our phrase break assignment algorithm is properly placed within a full text to speech system and that the prediction of any required inputs is included in our tests. A second requirement for our algorithm was introduced by our observation that many phrase break assignment algorithms attempt to estimate the probability of a break at some point based only on local information. However, what may locally appear as a reasonable position for a break may in fact be less suitable than the position after the next word. That is, assignment should not be locally optimised but globally optimised over the whole utterance. For example in the sentence I wanted to go for a drive in the country. a potential good place for assignment may locally appear to be between \"drive\" and \"in\" based on part of speech information. However in the sentence I wanted to go to a drive in. such a position is unsuitable. Another example is a uniform list of nouns. Breaks between nouns are unusual but given a long list of nouns (e.g. the numbers 1 to 10) it then becomes reasonable to insert a phrase break. Thus we wish our model to have reasonable input requirements, use predicted values for the inputs as part of the test and consider global optimisation of phrase break assignment over the whole utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-351"
  },
  "widera97_eurospeech": {
   "authors": [
    [
     "Christina",
     "Widera"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Maria",
     "Wolters"
    ]
   ],
   "title": "Prediction of word prominence",
   "original": "e97_0999",
   "page_count": 4,
   "order": 353,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "Control of prosody is essential for the synthesis of natural sounding speech. Text-to-speech systems tend to accent too many words when taking into account only the distinction between open-class and closed-class words. In the prominence-based approach [1], the degree of accentuation of a syllable is described in terms of a gradual prominence parameter. This paper presents the calculation of the prominence level of words based on their word class, the classes of the surrounding words, and their position in a clause. Rules predicting word prominence are derived from statistical analysis of a prosodic database. The hand-crafted rules are compared with the results of several machine learning algorithms on the same material. Furthermore, a perceptual test and an analysis of the resulting speech signals are carried out\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-352"
  },
  "kuwabara97_eurospeech": {
   "authors": [
    [
     "Hisao",
     "Kuwabara"
    ]
   ],
   "title": "Acoustic and perceptual properties of phonemes in continuous speech as a function of speaking rate",
   "original": "e97_1003",
   "page_count": 4,
   "order": 354,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "An investigation has been made for individual phonemes focusing mainly on their duration in continuous speech spoken in different rates: fast, normal, and slow. Fifteen short sentences uttered by four male speakers have been used as the speech material which comprises a total of 291 morae. Normal speaking rate (n-speech) is, on average, 150 milliseconds/mora (or 400 morae/minute) and the four speakers have been asked to read the sentences twice as fast as (f-speech) and 1/2 times as slow as (s-speech) the normal speed in reference to the n- speech. Among consonants, the greatest influence has been found to occur on the syllabic nasal /N/ and the least on the voiceless stop /t/ in f-speech. For the s-speech, /N/ has also been found to be the greatest but the least is voiced stop /d/. The ratio of duration between consonant and vowel of a CV-syllable in the f-speech is kept almost the same as that in the n-speech while vowel lengthening becomes significantly large in the s-speech. As it is expected, formant frequencies of vowels differ significantly between the three rates. Five vowels tend to be close together on the F1-F2 plane as the speaking rate becomes fast reflecting the neutralization of vowels. However, average difference of the third formant has been found to be very small.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-353"
  },
  "narayanan97_eurospeech": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Yong",
     "Song"
    ]
   ],
   "title": "New results in vowel production: MRI, EPG, and acoustic data",
   "original": "e97_1007",
   "page_count": 4,
   "order": 355,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "MRI, EPG, and acoustic data for the vowels /a, i, u/ are analyzed. The vocal tract geometry, tongue shapes, and inter-subject variability are studied. The data are used for studying the articulatory-acoustic relations for these sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-354"
  },
  "arai97_eurospeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "The temporal properties of spoken Japanese are similar to those of English",
   "original": "e97_1011",
   "page_count": 4,
   "order": 356,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "The languages of the world are generally classified into two types on the basis of their segmental timing. \"Syllable-timed\" languages, such as Japanese, are considered isochronous, exhibiting a highly regular pattern of syllabic duration. In contrast are the \"stress-timed\" languages, such as English, whose syllable timing varies greatly, both within and across sentential domains. The present study demonstrates that, even in a language as theoretically isochronous as Japanese, the duration of syllabic segments is as variable as their English counterparts. Moreover, the variability of moraic duration is as high as that observed for syllabic units. Two measures of segmental timing, syllable duration and the low-frequency modulation spectrum, indicate that the coarse temporal characteristics of English and Japanese are remarkably similar. Such common properties may reflect inherent temporal characteristics of physiological mechanisms underlying the production and perception of speech that are shared by all languages of the world.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-355"
  },
  "esposito97_eurospeech": {
   "authors": [
    [
     "Anna",
     "Esposito"
    ]
   ],
   "title": "The amplitudes of the peaks in the spectrum: data from /a/ context",
   "original": "e97_1015",
   "page_count": 4,
   "order": 357,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "This work is devoted to the study of the properties of the sound spectrum at the release of Italian stop consonants in vocalic contexts.The aim is to check if the amplitudes of the peaks in the spectrum can be used as acoustic attributes of the place of articulation of the consonants. This information is useful for defining an automatic algorithm which can discriminate among different place of articulation using simple data such as the values, in dB, of the maximum peaks in different frequency ranges. Moreover, different measurements have been performed (the spectra are computed at the release, averaged over 10 msec after the release, and using a smoothed spectrum) in order to define which measure retains more information about peak amplitudes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-356"
  },
  "bolfanstosic97_eurospeech": {
   "authors": [
    [
     "Natalija",
     "Bolfan-Stosic"
    ],
    [
     "Mladen",
     "Hedjever"
    ]
   ],
   "title": "Acoustical characteristics of speech and voice in speech pathology",
   "original": "e97_1019",
   "page_count": 4,
   "order": 358,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "Thirty six hoarse voices of preschool children and fifty speech productions of school children was analyzed using an acoustic analysis by Bruel and Kjaer, Real-time Frequency Analyzer, Type 2123. Thirty six specific oscilograms of sustained vowel productions were divided to oscilograms shapes in three subgroups of specific shimmer values, inside the same group. The purpose of this part of our research is a help in recognition and usage of acoustical terms in diagnostic of disordered voices. Therefore, we obtained \"staccato shimmer\", the \"narrow\" total intensity or shimmer, and finally the \"wide\" shimmer with following oscillations of jitter. The differences in fundamental frequency and intensity between three subgroups of different shimmer were analysed using one-way variance analysis. The purpose of the second part of this research has been to examine and analyze temporal segments in normal and disordered speech. Temporal segments of school children's speech have been measured starting from the subsound level (VOT - voice onset time and SGD - stop gap duration), and also at the levels of sound, syllable and word. Principal axis analysis showed specific differences between normal and pathological speech in all types of variables.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-357"
  },
  "kipp97_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Kipp"
    ],
    [
     "Maria-Barbara",
     "Wesenick"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Pronuncation modeling applied to automatic segmentation of spontaneous speech",
   "original": "e97_1023",
   "page_count": 4,
   "order": 359,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "In this paper 1 two different models of pronunciation are presented: the first model is based on a rule set compiled by an expert, while the second is statistically based, exploiting a survey about pronunciation variants occurring in training data. Both models generate pronunciation variants from the canonic forms of words. The two models are evaluated by applying them to the task of automatic segmentation of speech and then comparing the results to manual segmentations of the same speech data. Results show that correspondence between manual and automatic segmentations can be significantly improved if pronunciation variants are taken into account. The statistical model outperforms the rule based model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-358"
  },
  "downey97_eurospeech": {
   "authors": [
    [
     "Simon",
     "Downey"
    ],
    [
     "Richard",
     "Wiseman"
    ]
   ],
   "title": "Dynamic and static improvements to lexical baseforms",
   "original": "e97_1027",
   "page_count": 4,
   "order": 360,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "One limitation of many speaker independent recognition systems is their dependence on a single baseform dictionary to model word pronunciations. These dictionaries typically contain only a single (or 'ideal') pronunciation for each word. Previous work on improving dictionary models to include multiple pronunciations has met with mixed success - the alternatives may increase ambiguity in some cases. This paper investigates two approaches to improve lexical baseforms. The first is a 'bottom-up' approach in which 'ideal' transcriptions of utterances looked up in a pronunciation dictionary are compared to phonemic level hand-annotated transcriptions. Analysing the differences between the two transcriptions reveals many coomon mispronunciations, accent-based alternatives, false-starts and incorrect word substitutions. Each of these problems is illustrated in the paper, where it is also shown that unfamiliar words are prone to large numbers of alternative pronunciations. The second approach is more 'top-down'. Phonologically developed rules and transforms are described which modify the lexical representation of the utterance and a pronunciation network is thus derived. This approach has the advantage of being able to explicitly model cross-word coarticulation effects, whereas the former approach models them implicitly to a certain extent. The relative merits of each technique are investigated using a set of experiments performed on a phonetically rich database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-359"
  },
  "hauenstein97b_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Hauenstein"
    ]
   ],
   "title": "Signal driven generation of word baseforms from few examples",
   "original": "e97_1031",
   "page_count": 4,
   "order": 361,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "The work described in this paper attempts to automatically generate word baseforms as used in the pronunciation dictionaries of large vocabulary speech recognition systems. The input to the algorithm consists of several sample utterances per word. No additional information, like e.g. word spelling, is used. The task involves determining a suitable inventory of subword units (SWU) as well as determining the baseforms themselves. Experiments show that improvements over a triphone based dictionary are possible with less than ten sample utterances per word if test and training vocabularies are different. A possible application would be a system based on a fixed inventory of HMM-models that needs to be adapted to different vocabularies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-360"
  },
  "botha97_eurospeech": {
   "authors": [
    [
     "Elizabeth C.",
     "Botha"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Modeling the acoustic differences between L1 and L2 speech: the short vowels of africaans and south-african English",
   "original": "e97_1035",
   "page_count": 4,
   "order": 362,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "The acoustic differences between Afrikaans and South African English, spoken as first (L1) and second (L2) language, are measured for nine short vowels. The spoken language data base of 22 male speakers, collected for comparative studies, is described. The features used in an initial comparison of the isolated vowels and vowels in CVC words are the first three formant values and ratios. Significant differences are found between the production of /e/ and /y/ by Afrikaans and English mother-tongue speakers, and to a lesser extent between /i/, /c/ and /u/. Several interesting trends that seem to contradict popular beliefs concerning South African accents are observed. Directions for future research and the application of the envisioned L1-L2 model in speech technology are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-361"
  },
  "vaxelaire97_eurospeech": {
   "authors": [
    [
     "Béatrice",
     "Vaxelaire"
    ],
    [
     "Rudolph",
     "Sock"
    ]
   ],
   "title": "Laryngeal movements and speech rate: an x-ray investigation",
   "original": "e97_1039",
   "page_count": 4,
   "order": 363,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "This is an investigation on the production of VCV sequences with special emphasis on the displacement of the unit larynx-hyoid bone. X-ray data obtained for two subjects at two speaking rates show: that there is a positive correlation between the displacement of the larynx and that of the hyoid bone; that larynx position is lower for high vowels compared with their lower counterparts; that the larynx adopts a higher initial position in fast speech. Varying speech rate allows to uncover robust laryngeal trajectories underlying the production of these VCV sequences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-362"
  },
  "eriksson97_eurospeech": {
   "authors": [
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Pär",
     "Wretling"
    ]
   ],
   "title": "How flexible is the human voice? - a case study of mimicry",
   "original": "e97_1043",
   "page_count": 4,
   "order": 364,
   "p1": "1043",
   "pn": "1046",
   "abstract": [
    "The investigation presented here is a case study of mimicry in which a professional impersonation artist imitated three well-known Swedish public figures. The speech material consisted of recorded material taped from radio/TV shows, imitations of these speeches in which the artist tried to mimic the speeches as closely as possible, and the same speech material recorded with the artist using his own natural voice. The aim of the study was to investigate how closely the imitations matched selected acoustic parameters of the original recordings. It was found that he was able to mimic global speech rate very closely, but timing at the segmental level showed little or no change in the direction of the targets. Mean fundamental frequency and variation matched the targets very closely. Target formant frequencies were attained with varying success. For two of the three target voices the vowel space of the imitation was intermediate between that of the artist's own voice and the target. In the third case there was no apparent reduction in distance. With respect to individual vowels it was generally, but not always, the case that the formant frequencies of the mimicked vowels were closer to the original than those of the artist's own voice.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-363"
  },
  "strik97_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "The effect of low-pass filtering on estimated voice source parameters",
   "original": "e97_1047",
   "page_count": 4,
   "order": 365,
   "p1": "1047",
   "pn": "1050",
   "abstract": [
    "Voice source parameters are often obtained by parametrizing glottal flow signals. However, before parametrization these glottal flow signals are usually low-pass filtered. As low-pass filtering changes the shape of the glottal pulses, it will also cause an error in the estimated voice source parameters. The present article presents results of our research on the effect of low-pass filtering on the estimated voice source parameters. We will first present an evaluation method which makes it possible to study the effect of low-pass filtering in detail. The evaluation results show that low-pass filtering leads to an error in all estimated voice source parameters.. However, the magnitude of the errors differs for the various voice source parameters, and also depends on the estimation method used. We will show that the errors can be reduced substantially by choosing the appropriate estimation method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-364"
  },
  "fosnot97_eurospeech": {
   "authors": [
    [
     "Susan M.",
     "Fosnot"
    ]
   ],
   "title": "Vowel development of /i/ and /u/ in 15-36 month old children at risk and not at risk to stutter",
   "original": "e97_1051",
   "page_count": 4,
   "order": 366,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "A study was designed to compare the high front /i/ and high back /u/ vowel in children at risk and not at risk to stutter. Recordings were made of children playing with parents for 10 minutes between 15 and 36 months of age. Anatomical and linguistic influences did not differ across subjects with the exception of the 24 month period. At risk children were slightly taller at 24 months. Spontaneous utterances from each child were digitized into a CSL, Model 4300. The F1 and F2 of the steady-state portion of each /i/ and /u/ vowel was measured. Not-at-risk children demonstrated values typical of normally- developing children. Repeated measure ANOVAs showed that children who were at risk to stutter had significantly higher formant values for F1 for both /i/ and /u/ vowels. These results suggest that the tongue height is lower than it should be for the high vowels. Formant frequencies for F2 for both /i/ and /u/ were significantly higher also reflecting a more forward tongue position for the front and the back vowels in at-risk children.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-365"
  },
  "wrench97_eurospeech": {
   "authors": [
    [
     "Alan",
     "Wrench"
    ],
    [
     "Alan",
     "McIntosh"
    ],
    [
     "William",
     "Hardcastle"
    ]
   ],
   "title": "Optopalatograph: development of a device for measuring tongue movement in 3D",
   "original": "e97_1055",
   "page_count": 4,
   "order": 367,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "This paper identifies and investigates potential sources of measurement error using a prototype of a device for measuring tongue-palate distance, contact and pressure across the whole of the hard palate. The Optopalatograph (OPG) is similar in principle to the Glossometer and similar in configuration to the Electropalatograph. It uses optical fibres to relay light to and from the palate and distance sensing is achieved by measuring the amount of light reflected from the surface of the tongue. A high power halogen light source is currently used to compensate for light attenuation and losses in the system. This source is not readily switchable and we evaluate the error in the measured light intensity when all the sources are on simultaneously. We conclude that the halogen-based OPG is a practical device with a worst case error of 10% in estimated distance values below 5mm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-366"
  },
  "gutierrezarriola97_eurospeech": {
   "authors": [
    [
     "Juana M.",
     "Gutierrez-Arriola"
    ],
    [
     "Francisco M.",
     "Gimenez de los Galanes"
    ],
    [
     "Mohammed H.",
     "Savoji"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "Speech synthesis and prosody modification using segmentation and modelling of the excitation signal",
   "original": "e97_1059",
   "page_count": 4,
   "order": 368,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "In previous work we have presented a new method for improving the quality of LPC synthetic speech, where the excitation signal was modelled with a polynomial function followed by an adaptive filter. This scheme provides the properties of mathematical models which permits avoiding the problems related to prosody control [1], [2]. In order to reduce the storage needs, a segmentation technique was developed which grouped together several pitch periods based on spectral similarity. For every segment the same coefficient set (both the polynomial function and the post-processing filter) was used. These techniques were applied to a codification/decodification task were the resulting speech quality was promising [1], [2]. In this paper we present some results concerning prosodic modification, i.e. duration and fundamental frequency arbitrary changes which show the suitability of these methods for text-to-speech applications. We also present some results of the extension of the model to unvoiced segments of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-367"
  },
  "savariaux97_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Louis-Jean",
     "Boë"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "How can the control of the vocal tract limit the speaker's capability to produce the ultimate perceptive objectives of speech? 1063",
   "original": "e97_1063",
   "page_count": 5,
   "order": 369,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "In this paper an extension of the lip-tube experiment proposed by Savariaux et al. (1990) is presented and analyzed. The question underlying the design of this experiment is whether speakers are able to produce an [u] with a large lip opening. Nine native speakers of French repeated the original experiment, and then were asked to produce the vowel [u] starting from [o] vocal tract configuration. It was shown that more subjects achieved the compensation when they shifted their articulation from [o] to [u]. The issue of a possible constraint imposed by a learned standard articulatory pattern is discussed in relation with the notion of the internal representation of the articulatory-to-acoustic relations. Proposals in favor of a standard pattern for [u] that would be velopalatal rather than velopharyngeal are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-368"
  },
  "jovanovic97_eurospeech": {
   "authors": [
    [
     "Goran S.",
     "Jovanovic"
    ]
   ],
   "title": "A step toward general model for symbolic description of the speech signal 1067",
   "original": "e97_1067",
   "page_count": 4,
   "order": 370,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "The paper presents an improved and extended version of previously defined general model for symbolic description of the speech signal. In the first part of the paper we formally define symbolic description segments that correspond to the lower speech coding levels (word and subword speech signal segments). In the second part of the paper we perform an analysis of practical applicability of the proposed model. Experimental evidence confirmed that one way to develop automatic procedure for symbolic description of the speech signal is by the use of IFC-guided speech signal processing, which provides specific focusing structural analysis. We believe that presented experimental results are inspiring from the standpoint of new research projects, especially in the field of automatic speech recognition and efficient speech coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-369"
  },
  "furukawa97_eurospeech": {
   "authors": [
    [
     "Kiyoshi",
     "Furukawa"
    ],
    [
     "Masayuki",
     "Nakazawa"
    ],
    [
     "Takashi",
     "Endo"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Referring in long term speech by using orientation patterns obtained from vector field of spectrum pattern",
   "original": "e97_1071",
   "page_count": 4,
   "order": 371,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "We proposed a new expression of speech feature called orientation patterns which keeps its ability of detection higher in averaging of time domain. Because of this, we achieved to reduce number of frames in reference and input pattern in DP matching algorithm, then the calculation load were reduced. We constructed long term speech retrieval system by using this new expression. This system has RIFCDP as base matching algorithm which was already proposed. RIFCDP is an algorithm for spotting similar intervals between arbitrary reference pattern and arbitrary input pattern sequence synchronously with input frames.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-370"
  },
  "barnett97_eurospeech": {
   "authors": [
    [
     "J.",
     "Barnett"
    ],
    [
     "S.",
     "Anderson"
    ],
    [
     "J.",
     "Broglio"
    ],
    [
     "M.",
     "Singh"
    ],
    [
     "R.",
     "Hudson"
    ],
    [
     "S. W.",
     "Kuo"
    ]
   ],
   "title": "Experiments in spoken queries for document retrieval",
   "original": "e97_1323",
   "page_count": 4,
   "order": 372,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "We report the results of three experiments using the errorful output of a large vocabulary continuous speech recognition (LVCSR) system as the input to a statistical information retrieval (IR) system. Our goal is to allow a user to speak, rather than type, query terms into an IR engine and still obtain relevant documents. The purpose of these experiments is to test whether IR systems are robust to errors in the query terms introduced by the speech recognizer. If the correctly recognized words in the search query outweigh the misinformation from the incorrectly recognized words, the relevant documents will still be retrieved. This paper presents evidence that speech-driven IR can be effective, although with a reduced precision. We also find that longer spoken queries produce higher precision retrieval than shorter queries. For queries containing many (50-60) search terms and a recognizer word error rate (WER) of 27.9%, the precision at 30 documents retrieved is degraded by only 11.1%. For roughly the same WER, however, we find that queries shorter than 10-15 words suffer more than a 30% loss of precision.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-371"
  },
  "seide97_eurospeech": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Andreas",
     "Kellner"
    ]
   ],
   "title": "Towards an automated directory information system",
   "original": "e97_1327",
   "page_count": 4,
   "order": 373,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "This paper describes a design and feasibility study for a large-scale automatic directory information system with a scalable architecture. The current demonstrator, called PADIS-XL 1 , operates in realtime and handles a database of a medium-size German city with 130,000 listings. The system uses a new technique of taking a combined decision on the joint probability over multiple dialogue turns, and a dialogue strategy that strives to restrict the search space more and more with every dialogue turn. During the course of the dialogue, the last name of the desired subscriber must be spelled out. The spelling rec- ognizer permits continuous spelling and uses a context-free grammar to parse common spelling expressions. This paper describes the system architecture, our maximum a-posteriori (MAP) decision rule, the spelling grammar, and the dialogue strategy. We give results on the SPEECHDAT and SIETILL databases on recognition of first names by spelling and on jointly deciding on the spelled and the spoken name. In a 35,000-names setup, the joint decision reduced name-recognition errors by 31%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-372"
  },
  "larsen97_eurospeech": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ]
   ],
   "title": "A strategy for mixed-initiative dialogue control",
   "original": "e97_1331",
   "page_count": 4,
   "order": 374,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper presents and discusses a strategy for mixed-initiative dialogue management within a home banking application. The strategy tries to utilise the guidance of system-directed dialogues, while accommodating user initiated focus shifts by the inclusion of short-cuts in the dialogue. The paper reports on two experiments, one with a simulated speech recogniser (WOZ), and the second with a fully automated system. Both experiments shows that users use the possibility for short-cuts, even when not instructed of their existence. A tendency towards user habituation is also demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-373"
  },
  "hugunin97_eurospeech": {
   "authors": [
    [
     "Jim",
     "Hugunin"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "On the design of effective speech-based interfaces for desktop applications",
   "original": "e97_1335",
   "page_count": 4,
   "order": 375,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "Is speech a useful input modality for applications where the user has easy access to a full-size keyboard and mouse? This study shows that a well-designed speech interface can be more effective than a standard desktop application's traditional interface. Subjects are able to build a set of three spreadsheet tables 50% faster using a spoken dialog interface, and they report significantly greater enjoyment in using that interface. However, these advantages cannot be achieved by simply bolting a speech recognition system onto an application's existing interface. We found that this latter approach led to an insignificant 4% increase in efficiency and a devastating 64% increase in errors compared to the standard keyboard and mouse interface. In short, speech-based interfaces have the potential to substantially improve our interactions with computers, but they require significant interface redesign to take advantage of the unique properties of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-374"
  },
  "denecke97_eurospeech": {
   "authors": [
    [
     "Matthias",
     "Denecke"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Dialogue strategies guiding users to their communicative goals",
   "original": "e97_1339",
   "page_count": 4,
   "order": 376,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "Much work has been done in dialogue modeling for Human - Computer Interaction. Problems arise in situations where disambiguation of highly ambiguous data base output is necessary. We propose to model the task rather than the dialogue itself. Furthermore, we propose underspecified representations to represent relevant data and to serve as a base for generating clarification questions that guide the user eficiently to arrive at his communicative goal. In this paper, we establish a connection between underspecified representations as representations of disjunctions and clarification questions. Our approach to clarifying dialogues differs from other approaches in that the form of the clarification dialogues is entirely determined by the domain modeling and by the underspecified representations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-375"
  },
  "issar97_eurospeech": {
   "authors": [
    [
     "Sunil",
     "Issar"
    ]
   ],
   "title": "A speech interface for forms on WWW",
   "original": "e97_1343",
   "page_count": 5,
   "order": 377,
   "p1": "1343",
   "pn": "1346",
   "abstract": [
    "There is a wide variety of forms that a user encounters on the world wide web (WWW). In this paper, we describe the design of a speech interface that can be used over the web to fill forms. This presents several problems, for example, communicating with speech recognizer, parsing one or more forms embedded in text, generating appropriate language models and dictionary entries, and presenting appropriate information (responses and queries) to the user. Many database and non-database retrieval tasks can be viewed as form- filling tasks. Goddeau [2] also describes a form-based dialogue manager for spoken language understanding tasks. This tends to support our belief that a speech interface for forms is an important first step in the design of distributed spoken language systems, which can assist the user in problem solving activities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-376"
  },
  "flammia97_eurospeech": {
   "authors": [
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Learning the structure of mixed initiative dialogues using a corpus of annotated conversations 1",
   "original": "e97_1871",
   "page_count": 4,
   "order": 378,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "This paper reports an ongoing effort to derive linear discourse structures from a corpus of telephone conversations. First, we would like to determine how reliably human annotators can tag discourse segments in dialogues. Second, we begin to investigate how to build machine models for performing this annotation task. To carry out our research, we use a corpus of transcribed and annotated human-human dialogues in a specific information retrieval domain (Movie theater schedules). We conducted an experiment in which 25 different dialogues have each been antotated by at least seven different people. We found that the average precision and recall among annotators in placing segment boundaries is 84.3%, and in assigning segment purpose labels is 80.1%. A simple discourse segment parser based on finite state machines is able to cover 56% of the same dialogues. When the finite state grammar is able to analyse a dialogue, it agrees with human annotators in placing segment boundaries with 59.4% precision and 66.4% recall, and it agrees in segment label accuracy at the 59% level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-377"
  },
  "pieraccini97_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Esther",
     "Levin"
    ],
    [
     "Wieland",
     "Eckert"
    ]
   ],
   "title": "AMICA: the AT&t mixed initiative conversational architecture",
   "original": "e97_1875",
   "page_count": 4,
   "order": 379,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "In this paper we show how it is possible to design and implement a general architecture that is suitable for the rapid development of human/machine natural language, mixed initiative dialogue systems. The architecture proposed here relies on the assumption that a dialogue system can be modularized into different actions or functions that can be designed separately and implement basic aspects of the dialogue behavior, and a strategy that is fairly independent of the particular application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-378"
  },
  "abella97_eurospeech": {
   "authors": [
    [
     "Alicia",
     "Abella"
    ],
    [
     "Allen L.",
     "Gorin"
    ]
   ],
   "title": "Generating semantically consistent inputs to a dialog manager",
   "original": "e97_1879",
   "page_count": 4,
   "order": 380,
   "p1": "1879",
   "pn": "1882",
   "abstract": [
    "Spoken dialog systems interpret a user's request and engage in conversation if the need arises. It is the responsibility of the dialog manager to determine if this need is present and how to proceed. Our spoken dialog system is constructed to sufficiently understand a user's response to the open-ended prompt 'How may I help you?' in order to route a caller to an appropriate destination, with subsequent processing for information retrieval or call completion. In this paper we describe how to structure the relationships among the call types into an inheritance hierarchy. We then describe an algorithm which exploits this hierarchy and the output of a spoken language understanding module to generate a set of semantically consistent inputs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-379"
  },
  "levin97_eurospeech": {
   "authors": [
    [
     "Esther",
     "Levin"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "A stochastic model of computer-human interaction for learning dialogue strategies",
   "original": "e97_1883",
   "page_count": 4,
   "order": 381,
   "p1": "1883",
   "pn": "1886",
   "abstract": [
    "Recent progress in the field of spoken natural language understanding expanded the scope of spoken language systems to include mixed initiative dialogue. Currently there are no agreed upon theoretical foundations for the design of such systems. In this work we propose a stochastic model of computer-human interactions. This model can be used for learning and adaptation of the dialogue strategy and for objective evaluation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-380"
  },
  "boros97_eurospeech": {
   "authors": [
    [
     "Manuela",
     "Boros"
    ],
    [
     "Maria",
     "Aretoulaki"
    ],
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Elmar",
     "Noth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Semantic processing of out-of-vocabulary words in a spoken dialogue system",
   "original": "e97_1887",
   "page_count": 4,
   "order": 382,
   "p1": "1887",
   "pn": "1890",
   "abstract": [
    "One of the most important causes of failure in spoken dialogue systems is usually neglected: the problem of words that are not covered by the system's vocabulary (out-of-vocabulary or OOVwords). In this paper a methodology is described for the detection, classification and processing of OOV words in an automatic train timetable information system [2]. The various extensions that had to be effected on the different modules of the system are reported, resulting in the design of appropriate dialogue strategies, as are encouraging evaluation results on the new versions of the word recogniser and the linguistic processor.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-381"
  },
  "maier97_eurospeech": {
   "authors": [
    [
     "Elisabeth",
     "Maier"
    ]
   ],
   "title": "Clarification dialogues in VERBMOBIL",
   "original": "e97_1891",
   "page_count": 4,
   "order": 383,
   "p1": "1891",
   "pn": "1894",
   "abstract": [
    "In this paper we describe the possibility to carry out clarification dialogues in the framework of the face-to-face translation system verbmobil. We focus on a special subtype of clarification dialogues which occur when the system has insufficient information to continue processing. The clarification dialogues currently incorporated in our system concern three aspects: (i) phonological ambiguities, (ii) unknown words, and (iii) semantic inconsistencies. We describe each of these subdialogues in detail and discuss the extensions and changes that had to be made to the overall system in order to allow for clarification dialogues. An outlook on future developments concludes the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-382"
  },
  "arslan97_eurospeech": {
   "authors": [
    [
     "Levent M.",
     "Arslan"
    ],
    [
     "David",
     "Talkin"
    ]
   ],
   "title": "Voice conversion by codebook mapping of line spectral frequencies and excitation spectrum",
   "original": "e97_1347",
   "page_count": 4,
   "order": 384,
   "p1": "1347",
   "pn": "1350",
   "abstract": [
    "This paper presents a new scheme for developing a voice conversion system that modifies the utterance of a source speaker to sound like speech from a target speaker. We refer to the method as Speaker Transformation Algorithm using Segmental Codebooks (STASC). Two new methods are described to perform the transformation of vocal tract and glottal excitation characteristics across speakers. In addition, the source speaker's general prosodic characteristics are modified using time-scale and pitch-scale modification algorithms. Informal listening tests suggest that convincing voice conversion is achieved while maintaining high speech quality. The performance of the proposed system is also evaluated on a standard Caussian mixture model based speaker identification system, and the results show that the transformed speech is assigned higher likelihood by the target speaker model when compared to the source model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-383"
  },
  "mokbel97_eurospeech": {
   "authors": [
    [
     "C.",
     "Mokbel"
    ],
    [
     "G.",
     "Gravier"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Optimal state dependent spectral representation for HMM modeling : a new theoretical framework",
   "original": "e97_1351",
   "page_count": 4,
   "order": 385,
   "p1": "1351",
   "pn": "1354",
   "abstract": [
    "In this paper we propose a theoretical framework to extend classical continuous density HMM in order to consider different spectral representations depending on the state. We stress the need for a reference space and for spectral transformations between the model spectral representation spaces and the reference space. We show that this framework permits to obtain more precise pdfs in the reference space. Preliminary speech recognition experiments for two spectral representations MFCC and linear frequency scale cepstral coefficients show no improvements ; however they identify that the choice of the spectral representations is crucial and the determination of the spaces transformations is a complex problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-384"
  },
  "potamianos97_eurospeech": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "Speech analysis and synthesis using an AM-FM modulation model",
   "original": "e97_1355",
   "page_count": 4,
   "order": 386,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "In this paper, the AM-FM modulation model is applied to speech analysis, synthesis and coding. The multiband demodulation pitch tracking algorithm is proposed that produces smooth and accurate fundamental frequency contours. The AM-FM modulation vocoder represents speech as the sum of resonance signals modeled by their amplitude envelope and instantaneous frequency signals. Eficient modeling and coding (at 4.8-9.6 kbits/sec) algorithms are proposed for the amplitude envelope and instantaneous frequency signals. Amplitude and frequency modulations of the speech resonances are shown to be perceptually important for natural speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-385"
  },
  "mawass97_eurospeech": {
   "authors": [
    [
     "Khaled",
     "Mawass"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Synthesis of fricative consonants by audiovisual-to-articulatory inversion",
   "original": "e97_1359",
   "page_count": 4,
   "order": 387,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "We present here results of audio-visual to articulatory inversion for French fricatives embedded into VCVs. The inversion technique is evaluated using both experimental and synthetic data. The final synthesis is assessed by a perceptual categorisation test. Synthetic stimuli have similar scores as natural ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-386"
  },
  "claes97_eurospeech": {
   "authors": [
    [
     "Tom",
     "Claes"
    ],
    [
     "Ioannis",
     "Dologlou"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "New transformations of cepstral parameters for automatic vocal tract length normalization in speech recognition",
   "original": "e97_1363",
   "page_count": 4,
   "order": 388,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "This paper proposes a method to transform acoustic models (HMM gaussian mixtures) that have been trained on a certain group of speakers for use on speech from a different group of speakers. Cepstral features are transformed on the basis of assumptions regarding the difference in vocal tract length (VTL) between the groups of speakers (VTL normalisation, VTLN). Firstly, the VTL of these groups has been estimated based on the average third formant F . Secondly, the linear acoustic theory of speech production has been applied to warp the spectral characteristics of the existing models so as to match the incoming speech. The mapping is composed of subsequent non-linear submappings. By locally linearizing it, a linear approximation was obtained which is accurate as long as warping is reasonably small. The method has been tested for the TI digits database, containing adult and kids speech, consisting of isolated digits and digit strings of different length. The word error rate when trained on adults and tested on kids with transformed adult models is decreased by more than a factor of 2 compared to the non-transformed case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-387"
  },
  "dobrisek97_eurospeech": {
   "authors": [
    [
     "S.",
     "Dobrisek"
    ],
    [
     "F.",
     "Mihelic"
    ],
    [
     "N.",
     "Pavesic"
    ]
   ],
   "title": "A multiresolutionally oriented approach for determination of cepstral features in speech recognition",
   "original": "e97_1367",
   "page_count": 4,
   "order": 389,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "This paper presents an effort to provide a more efficient speech signal representation, which aims to be incorporated into an automatic speech recognition system. Modified cepstral coefficients, derived from a multiresolution auditory spectrum are proposed. The multiresolution spectrum was obtained using sliding single point discrete Fourier transformations. It is shown that the obtained spectrum values are similar to the results of a nonuniform filtering operation. The presented cepstral features are evaluated by introducing them into a simple phone recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-388"
  },
  "haulick97_eurospeech": {
   "authors": [
    [
     "Tim",
     "Haulick"
    ],
    [
     "Klaus",
     "Linhard"
    ],
    [
     "Peter",
     "Schrogmeier"
    ]
   ],
   "title": "Residual noise suppression using psychoacoustic criteria",
   "original": "e97_1395",
   "page_count": 4,
   "order": 390,
   "p1": "1395",
   "pn": "1398",
   "abstract": [
    "Speech enhancement techniques using spectral subtraction have the drawback of generating residual noise with a musical character, so-called musical noise. We developed a new post-processing method for suppressing this musical residual noise. In this method, the auditory masking threshold is calculated twice, once before the spectral subtraction and once again afterwards. This ensures that all audible spectral signal components above the thresholds are detected. Audible components which are only present at the output are candidates for musical noise. Depending on their spectral bandwidth and time duration, they may be processed additionally. Using this post-processing, the distortion of the speech signal is not noticeable and musical noise is not audible even at low signal-to-noise ratios of about 0 dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-389"
  },
  "yegnanarayana97_eurospeech": {
   "authors": [
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "Carlos",
     "Avendano"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "P. Satyanarayana",
     "Murthy"
    ]
   ],
   "title": "Processing linear prediction residual for speech enhancement",
   "original": "e97_1399",
   "page_count": 4,
   "order": 391,
   "p1": "1399",
   "pn": "1402",
   "abstract": [
    "In this paper we propose a method for enhancement of speech in the presence of additive noise. The objective is to selectively enhance the high SNR regions in the noisy speech in the temporal and spectral domains, without causing significant distortion in the resulting enhanced speech. This is proposed to be done at three different levels: (a) At the gross level, by identifying the regions of speech and noise in the temporal domain, (b) At the finer level, by identifying the regions of high and low SNR portions in the noisy speech, and (c) At the short-time spectrum level, by enhancing the spectral peaks over spectral valleys. Processing of noisy speech for enhancement involves mostly weighting the LP residual samples. The weighted residual samples are used to excite the time- varying LP filter to produce enhanced speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-390"
  },
  "gustafsson97_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Gustafsson"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Combined acoustic echo control and noise reduction for mobile communications",
   "original": "e97_1403",
   "page_count": 4,
   "order": 392,
   "p1": "1403",
   "pn": "1406",
   "abstract": [
    "In this paper an acoustic echo compensator with an additional frequency domain adaptive filter for combined residual echo and noise reduction is proposed. The algorithm delivers high echo attenuation as well as high near end speech quality over a wide range of signal-to-noise conditions. The system makes use of a standard time domain echo compensator of low order, after which the proposed adaptive filter, which is motivated by means of a minimum mean square error approach, is placed in the sending path. In contrast to other combined systems [1, 2, 3], our method uses an explicit estimate of the power spectral density of the residual echo after echo compensation. The separate estimations of the power spectral densities of the residual echo and the background noise, respectively, are then flexibly combined, such that in the processed signal a low level of intentionally left background noise will effectively mask the residual echo.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-391"
  },
  "lee97c_eurospeech": {
   "authors": [
    [
     "Ki Yong",
     "Lee"
    ],
    [
     "Jae Yeol",
     "Rheem"
    ]
   ],
   "title": "A nonstationary autoregressive HMM and its application to speech enhancement",
   "original": "e97_1407",
   "page_count": 4,
   "order": 393,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "Since speech sounds, such as fricative, glides, liquids, diphthongs, and transition regions between phones, reveal the most notable nonstationary nature, we propose the nonstationary autoregressive (AR) HMM with state-dependent polynomial function for modeling the nature of speech. Then, the nonstationary AR model has parameters depend on the states of the Markov chain. It is designed to handle the speech signal at the frame level, where it is represented by the signal, rather than dealing with feature vectors directly. Also, we proposed a new speech enhancement based on the nonstationary AR HMM and the IMM algorithm under white noise condition. The proposed enhancement is the weighted sum of the parallel Kalman filters with interacting rule by IMM algorithm. The simulation results shows that the proposed method offers performance gains relative to the previous results [7] with slightly increased complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-392"
  },
  "yoma97_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Spectral subtraction and mean normalization in the context of weighted matching algorithms",
   "original": "e97_1411",
   "page_count": 4,
   "order": 394,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "Additive and convolutional noises are the main problems to be solved in order to make speech recognition successful in real applications. A model for additive noise is used to deduce a spectral subtraction (SS) estimation and to show that the channel transfer function could be effectively removed alter the additive noise being cancelled by SS. Then, SS and mean normalization are tested in combination with a weighting procedure to reduce the influence ol the rectilying lunction. All the experiments were done in the context ol weighted matching algorithms and the approaches proved effective in cancelling both additive noise and the transmission channel function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-393"
  },
  "tsoukalas97_eurospeech": {
   "authors": [
    [
     "D. E.",
     "Tsoukalas"
    ],
    [
     "J.",
     "Mourjopoulos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Improving the intelligibility of noisy speech using an audible noise suppression technique",
   "original": "e97_1415",
   "page_count": 4,
   "order": 395,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "This paper presents some novel results concerning the problem of enhancing speech degraded by wideband additive noise. The enhancement scheme proposed in this work is based on the utilisation of the Auditory Masking mechanism as a measure for the definition and subsequent suppression of the frequency audible noise components. Accordingly, the enhancement technique minimises only those noise components responsible for audible signal degradations, so that the underlying speech signal quality is only minimally degraded. Extensive subjective and objective tests have shown that, after enhancement, the intelligibility of the processed signal can be improved even at very low S/N ratios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-394"
  },
  "girin97_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Girin"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Noisy speech enhancement by fusion of auditory and visual information: a study of vowel transitions",
   "original": "e97_2555",
   "page_count": 4,
   "order": 396,
   "p1": "2555",
   "pn": "2558",
   "abstract": [
    "This paper deals with a noisy speech enhancement technique based on the fusion of auditory and visual information. We first present the global structure of the system, and then we focus on the tool we used to melt both sources of information. The whole noise reduction system is implemented in the context of vowel transitions corrupted with white noise. A complete evaluation of the system in this context is presented, including distance measures, gaussian classification scores, and a perceptive test. The results are very promising.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-395"
  },
  "engelsberg97_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Engelsberg"
    ],
    [
     "Thomas",
     "Gulzow"
    ]
   ],
   "title": "Spectral subtraction using a non-critically decimated discrete wavelet transform",
   "original": "e97_2559",
   "page_count": 4,
   "order": 397,
   "p1": "2559",
   "pn": "2562",
   "abstract": [
    "The method of spectral subtraction has become very popular in speech enhancement. It is performed by modifying the spectral amplitudes of the disturbed signal. The spectral analysis of the signal is usually done by a Discrete Fourier Transformation (DFT). We propose a spectral transformation with nonuniform bandwidth to take into account the characteristics of the human ear. The spectral analysis and synthesis is performed by a non-critically decimated discrete wavelet transform. Critical subsampling is not performed to avoid errors due to aliasing. A significant drawback of spectral-subtraction methods are tonal residual noises in speech pauses with unnatural sound. The application of the proposed wavelet transform results in reduced residual noise with subjectively more comfortable sound.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-396"
  },
  "chien97_eurospeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Bayesian affine transformation of HMM parameters for instantaneous and supervised adaptation in telephone speech recognition",
   "original": "e97_2563",
   "page_count": 4,
   "order": 398,
   "p1": "2563",
   "pn": "2566",
   "abstract": [
    "This paper proposes a Bayesian affine transformation of hidden Markov model (HMM) parameters for reducing the acoustic mismatch problem in telephone speech recognition. Our purpose is to transform the existing HMM parameters into its new version of specific telephone environment using affine function so as to improve the recognition rate. The maximum a posteriori (MAP) estimation which merges the prior statistics into transformation is applied for estimating the transformation parameters. Experiments demonstrate that the proposed Bayesian affine transformation is effective for instantaneous adaptation and supervised adaptation in telephone speech recognition. Model transformation using MAP estimation performs better than that using maximum-likelihood (ML) estimation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-397"
  },
  "lawrence97_eurospeech": {
   "authors": [
    [
     "Craig",
     "Lawrence"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Integrated bias removal techniques for robust speech recognition \\lambda",
   "original": "e97_2567",
   "page_count": 4,
   "order": 399,
   "p1": "2567",
   "pn": "2570",
   "abstract": [
    "In this paper, we present a family of maximum likelihood (ML) techniques that aim at reducing an acoustic mismatch between the training and testing conditions of hid- den Markov model (HMM)-based automatic speech recognition (ASR) systems. We propose a codebook-based stochastic matching (CBSM) approach for bias removal both at the feature level and at the model level. CBSM associates each bias with an ensemble of HMM mixture components that share similar acoustic characteristics. It is integrated with hierarchical signal bias removal (HSBR) and further extended to accommodate for N-best candidates. Experimental results on connected digits, recorded over a cellular network, shows that the proposed system reduces both the word and string error rates by about 36% and 31%, respectively, over a baseline system not incorporating bias removal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-398"
  },
  "langmann97_eurospeech": {
   "authors": [
    [
     "Detlev",
     "Langmann"
    ],
    [
     "Alexander",
     "Fischer"
    ],
    [
     "Friedhelm",
     "Wuppermann"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Thomas",
     "Eisele"
    ]
   ],
   "title": "Acoustic front ends for speaker-independent digit recognition in car environments",
   "original": "e97_2571",
   "page_count": 4,
   "order": 400,
   "p1": "2571",
   "pn": "2574",
   "abstract": [
    "This paper describes speaker-independent speech recognition experiments concerning acoustic front end processing on a speech database that was recorded in 3 different cars. We investigate different feature analysis approaches (mel-filter bank, mel-cepstrum, perceptually linear predictive coding) and present results with noise compensation techniques based on spectral subtraction. Although the methods employed lead to considerable error rate reduction the error analysis shows that low signal-to-noise ratios are still a problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-399"
  },
  "delphinpoulat97_eurospeech": {
   "authors": [
    [
     "Lionel",
     "Delphin-Poulat"
    ],
    [
     "Chafic",
     "Mokbel"
    ]
   ],
   "title": "Signal bias removal using the multi-path stochastic equalization technique",
   "original": "e97_2575",
   "page_count": 4,
   "order": 401,
   "p1": "2575",
   "pn": "2578",
   "abstract": [
    "We propose using Hidden Markov Models (HMMs) associated with the cepstrum coefficients as a speech signal model in order to perform equalization or noise removal. The MUlti-path Stochastic Equalization (MUSE) framework allows one to process data at the frame level: it is an on-line adaptation of the model. More precisely, we apply this technique to perform bias removal in the cepstral domain in order to increase the robustness of automatic speech recognizers. Recognition experiments on two databases recorded on both PSN and GSM networks show the efficiency of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-400"
  },
  "miksic97_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Miksic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Subband echo cancellation in automatic speech dialog systems",
   "original": "e97_2579",
   "page_count": 4,
   "order": 402,
   "p1": "2579",
   "pn": "2582",
   "abstract": [
    "Echo cancellation has been most widely studied for hands-free telephony and for cancelling line echos in telephone central offices. The problem of echo cancelling in speech dialog systems is similar, however it has some specific requirements. In this contribution, a subband echo cancellation structure is proposed which can be integrated in the feature extraction part of a recognizer. A NLMS gradient-based adaptation is performed in frequency subbands that can either be derived directly from FFT analysis of input speech signal, or by using a proposed reduced-subband approach where the number of subbands is reduced in order to lessen the aliasing effect of the FFT. A double-talk detector is proposed based on the estimated error function for decision on stopping the adaptation. Finally, a new approach of combining echo cancellation and noise reduction is proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-401"
  },
  "tolba97_eurospeech": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Speech enhancement via energy separation",
   "original": "e97_2583",
   "page_count": 4,
   "order": 403,
   "p1": "2583",
   "pn": "2586",
   "abstract": [
    "This work presents a novel technique to enhance speech signals in the presence of interfering noise. In this paper, the amplitude and frequency (AM- FM) modulation model [7] and a multi-band analysis scheme [5] are applied to extract the speech signal parameters. The enhancement process is performed using a time-warping function B(n) that is used to warp the speech signal. B(n) is extracted from the speech signal using the Smoothed Energy Operator Separation Algorithm (SEOSA) [4]. This warping is capable of increasing the SNR of the high frequency harmonics of a voiced signal by forcing the the quasiperiodic nature of the voiced component to be more periodic, and consequently is useful for extracting more robust parameters of the signal in the presence of noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-402"
  },
  "unoki97_eurospeech": {
   "authors": [
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A method of signal extraction from noisy signal",
   "original": "e97_2587",
   "page_count": 4,
   "order": 404,
   "p1": "2587",
   "pn": "2590",
   "abstract": [
    "This paper presents a method of extracting the desired signal from a noise-added signal as a model of acoustic source segregation. Using physical constraints related to the four regularities proposed by Bregman, the proposed method can solve the problem of segregating two acoustic sources. Two simulations were carried out using the following signals: (a) a noise-added AM complex tone and (b) a noisy synthetic vowel. It was shown that the proposed method can extract the desired AM complex tone from noise- added AM complex tone in which signal and noise exist in the same frequency region. The SD was reduced an average of about 20 dB. It was also shown that the proposed method can extract a speech signal from noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-403"
  },
  "sika97_eurospeech": {
   "authors": [
    [
     "Jiri",
     "Sika"
    ],
    [
     "Vratislav",
     "Davidek"
    ]
   ],
   "title": "Multi-channel noise reduction using wavelet filter bank",
   "original": "e97_2591",
   "page_count": 4,
   "order": 405,
   "p1": "2591",
   "pn": "2594",
   "abstract": [
    "This paper deals with the problem of estimation of a speech signal corrupted by an additive noise when observations from two microphones are available. The basic method for noise reduction using the coherence function is modified by using wavelets. The both observations are splitted by filter bank in five narrow bands through the whole used bandwidth (0...4kHz). The coherence functions are then computed for each band and the output speech estimation is reconstructed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-404"
  },
  "abdallah97_eurospeech": {
   "authors": [
    [
     "Imad",
     "Abdallah"
    ],
    [
     "Silvio",
     "Montresor"
    ],
    [
     "Marc",
     "Baudry"
    ]
   ],
   "title": "Speech signal detection in noisy environement using a local entropic criterion",
   "original": "e97_2595",
   "page_count": 4,
   "order": 406,
   "p1": "2595",
   "pn": "2598",
   "abstract": [
    "This paper describes an original method for speech/non-speech detection in adverse conditions. Firstly, we define a time-dependent function called Local Entropic Criterion [1] based on Shannon's entropy [2]. Then we present the detection algorithm and show that at Signal to Noise Ratio (SNR) above 5 dB, it offers a segmentation comparable to the one obtained in clean conditions. We finally, describe how at very low SNR ( < 0 dB) , it permits to detect speech units masked by noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-405"
  },
  "moreno97_eurospeech": {
   "authors": [
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Brian",
     "Eberman"
    ]
   ],
   "title": "A new algorithm for robust speech recognition: the delta vector taylor series approach",
   "original": "e97_2599",
   "page_count": 4,
   "order": 407,
   "p1": "2599",
   "pn": "2602",
   "abstract": [
    "In this paper we present a new model-based compensation technique called Delta Vector Taylor Series (DVTS). This new technique is an extension and improvement over the Vector Taylor Series (VTS) approach [7] that addresses several of its limitations. In particular, we present a new statistical representation for the distribution of clean speech feature vectors based on a weighted vector codebook. This change to the underlying probability density function (PDF) allows us to produce more accurate and stable solutions for our algorithm. The algorithm is also presented in a EM-MAP framework where some the environmental parameters are treated as random variables with known PDF's. Finally, we explore a new compensation approach based on the use of convex hulls. We evaluate our algorithm in a phonetic classification task on the TIMIT [5] database and also in a small vocabulary size speech recognition database. In both databases artificial and natural noise is injected at several signal to noise ratios (SNR). The algorithm achieves matched performance at all SNR's above 10 dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-406"
  },
  "cole97_eurospeech": {
   "authors": [
    [
     "David",
     "Cole"
    ],
    [
     "Miles",
     "Moody"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Robust enhancement of reverberant speech using iterative noise removal",
   "original": "e97_2603",
   "page_count": 4,
   "order": 408,
   "p1": "2603",
   "pn": "2606",
   "abstract": [
    "We suggest a new technique for the enhancement ofsingle channel reverberant speech. Previous methods have used either waveform deconvolution or modulation envelope deconvolution. Waveform deconvolution requires calculation of an inverse room response, and is impractical due to variation with source or receiver movement. Modulation envelope deconvolution has been claimed to be position independent, but our research indicates that envelope restoration in fact degrades intelligibility of the speech. Our method uses the observation that the smoothed segmental spectral magnitude of the room response is less variable with position. This is used to estimate the reverberant component of the signal, which is removed iteratively using conventional noise reduction algorithms. The enhanced output is not perceptibly affected by positional changes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-407"
  },
  "jones97c_eurospeech": {
   "authors": [
    [
     "D. J.",
     "Jones"
    ],
    [
     "Scott D.",
     "Watson"
    ],
    [
     "K. G.",
     "Evans"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ],
    [
     "R. A.",
     "Reeve"
    ]
   ],
   "title": "A network speech echo canceller with comfort noise",
   "original": "e97_2607",
   "page_count": 4,
   "order": 409,
   "p1": "2607",
   "pn": "2610",
   "abstract": [
    "This paper describes a proposed comfort noise system for a network echo canceller. In this system, any residual echo is suppressed using a single threshold centre-clipper, but instead of transmitting silence to the far-end of the network, a synthetic version of the background sounds is sent. This masks any 'noise modulation' or 'noise pumping' that may otherwise occur. The background sounds are characterised using linear prediction. Periods when only background sounds are present are identified by a modified GSM Voice Activity Detector (VAD). Informal listening tests have shown that this 'synthetic background' is preferable to the transmission of silence or pseudo-random noise that is not spectrally shaped to match the original background.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-408"
  },
  "hussain97b_eurospeech": {
   "authors": [
    [
     "Amir",
     "Hussain"
    ],
    [
     "Douglas R.",
     "Campbell"
    ],
    [
     "Thomas J.",
     "Moir"
    ]
   ],
   "title": "A new metric for selecting sub-band processing in adaptive speech enhancement systems",
   "original": "e97_2611",
   "page_count": 4,
   "order": 410,
   "p1": "2611",
   "pn": "2614",
   "abstract": [
    "A multi-microphone adaptive speech enhancement system employing diverse sub-band processing is presented. A new robust metric is developed, which is capable of real-time implementation, in order to automatically select the best form of processing within each sub-band. It is based on an adaptively estimated inter-channel Magnitude Squared Coherence (MSC) relationship, which is used to detect the level of correlation between in-band signals from multiple sensors during noise-alone periods in intermittent speech. This paper reports recent results of comparative experiments with simulated anechoic data extended to include simulated reverberant data. The results demonstrate that the method is capable of significantly outperforming conventional noise cancellation schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-409"
  },
  "kobatake97_eurospeech": {
   "authors": [
    [
     "Hidefumi",
     "Kobatake"
    ],
    [
     "Hideta",
     "Suzuki"
    ]
   ],
   "title": "Estimation of LPC cepstrum vector of speech contaminated by additive noise and its application to speech enhancement",
   "original": "e97_2615",
   "page_count": 4,
   "order": 411,
   "p1": "2615",
   "pn": "2618",
   "abstract": [
    "This paper presents a new method for speech enhancement. It is well known that Wiener filtering is effective in reducing additive noises and the proposed method is based on it. This paper focuses on the design of Wiener filter, where we place emphasis on the recovery of original formant characteristics and the smooth transition of speech spectrum. Transformation method of LPC cepstrum vector extracted from noisy speech to reduce noise effects is given, which gives an estimated LPC cepstrum vector of original speech. Sharpening of formant peaks and eliminating false spectral peaks are necessary for high quality speech restoration and they are realized by the proposed method. Experiments of noise reduction have been performed, whose results show the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-410"
  },
  "tibrewala97_eurospeech": {
   "authors": [
    [
     "Sangita",
     "Tibrewala"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Multi-band and adaptation approaches to robust speech recognition",
   "original": "e97_2619",
   "page_count": 4,
   "order": 412,
   "p1": "2619",
   "pn": "2622",
   "abstract": [
    "In this paper we present two approaches to deal with degradation of automatic speech recognizers due to acoustic mismatch in training and testing environments. The first approach is based on the multi-band approach to automatic speech recognition (ASR). This approach is shown to be inherently robust to frequency selective degradation. In the second approach, we present a conceptually simple unsupervised feature adaptation technique, based on recursive estimation of means and variances of the cepstral parameters to compensate for the noise effects. Both techniques yield significant reduction in error rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-411"
  },
  "masgrau97_eurospeech": {
   "authors": [
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Luis",
     "Vicente"
    ]
   ],
   "title": "Non-quadratic criterion algorithms for speech enhancement",
   "original": "e97_2623",
   "page_count": 4,
   "order": 413,
   "p1": "2623",
   "pn": "2626",
   "abstract": [
    "A new algorithm for speech enhancement based on the iterative Wiener filtering method due to Lim-Oppenheim [1] is presented. We propose the use of a generalized non-quadratic cost function in addition to the classical MSE term (quadratic term). The proposed cost function includes two signal-error cross- correlation terms and a L2 norm term of the filter weights. The signal-error cross- correlation terms reduce both the residual noise and the signal distortion in the enhanced speech. The L2 norm term of the filter weights reduces the overall gain of the filter, decreasing the weight noise variance and removing the side lobe of the filter response. Two solutions to the new cost function are presented: the classical non-causal type (ideal Wiener), working in the frequency domain; and a causal finite length in the time domain. In both cases, as Lim's algorithm, the filter output of each iteration is used as \"noiseless\" speech signal for the following one. Simulation results demonstrate the effectiveness of these algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-412"
  },
  "wright97_eurospeech": {
   "authors": [
    [
     "J. H.",
     "Wright"
    ],
    [
     "Allen L.",
     "Gorin"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Automatic acquisition of salient grammar fragments for call-type classification",
   "original": "e97_1419",
   "page_count": 4,
   "order": 414,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "We present an algorithm for the automatic acquisition of salient grammar fragments in the form of finite state machines (FSMs). Salient phrase fragments are selected using a significance test, then clustered using a combination of string and semantic distortion measures. Each cluster is then compactly represented as an FSM. Flexibility is enhanced by permitting approximate matches to paths through each FSM. Multiple fragment detections are exploited by means of a neural network. The methodology is applied to the \"How may I help you?\" (HMIHY) call-type classification task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-413"
  },
  "minker97_eurospeech": {
   "authors": [
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "Stochastically-based natural language understanding across tasks and languages",
   "original": "e97_1423",
   "page_count": 4,
   "order": 415,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "A stochastically-based method for natural language understanding has been ported from the American ATIS (Air Travel Information Services) to the French MASK (Multimodal-Multimedia Automated Service Kiosk) task. The porting was carried out by designing and annotating a corpus of semantic representations via a semi-automatic iterative labeling. The study shows that domain and language porting is rather flexible, since it is sufficient to train the system on data sets specific to the application and language. A limiting factor of the current implementation is the quality of the semantic representation and the use of query preprocessing strategies which strongly suffer from human influence. The performances of the stochastically-based and a rule-based method are compared on both tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-414"
  },
  "riley97_eurospeech": {
   "authors": [
    [
     "Michael",
     "Riley"
    ],
    [
     "Fernando",
     "Pereira"
    ],
    [
     "Mehryar",
     "Mohri"
    ]
   ],
   "title": "Transducer composition for context-dependent network expansion",
   "original": "e97_1427",
   "page_count": 4,
   "order": 416,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "Context-dependent models for language units are essential in high-accuracy speech recognition. However, standard speech recognition frameworks are based on the substitution of level models for higher-level units. Since substitution cannot express context-dependency constraints, actual recognizers use restrictive model-structure assumptions and specialized code for context-dependent models, leading to decreased flexibility and lost opportunities for automatic model optimization. Instead, we propose a recognition framework that builds in the possibility of context dependency from the start by using weighted finite-state transduction rather than substitution. The framework is mented with a general demand-driven transducer composition algorithm that allows great flexibility in model structure, form of context dependency and network expansion method, while achieving competitive recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-415"
  },
  "lieske97_eurospeech": {
   "authors": [
    [
     "Christian",
     "Lieske"
    ],
    [
     "Johan",
     "Bos"
    ],
    [
     "Martin",
     "Emele"
    ],
    [
     "Bjorn",
     "Gambac"
    ],
    [
     "C.J.",
     "Rupp"
    ]
   ],
   "title": "Giving prosody a meaning",
   "original": "e97_1431",
   "page_count": 4,
   "order": 417,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "Systems for spoken-language understanding can use prosodic information on the speech recognition side as well as the linguistic processing side. In the former case, prosody improves recognition accuracy and speed. In the latter case, it contributes to the computation of meaning. Interfacing prosodic processing to language analysis has so far been mainly concerned with speeding up the parsing process. The actual integration of prosodic information into the semantic part of a language understanding system, or into the transfer part of a translation system, has mostly been left aside. We describe how prosody has been used in the syntactic-semantic and transfer modules of the Verbmobil spoken dialogue translation system. On the syntactic-semantic side, prosody is currently used for the solution of three different problems: insertion of clause boundaries, selection of sentence mood (declarative, question, etc), and assignment of semantic focus. On the transfer side, the prosodic information is allowed to in uence the lexical choice of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-416"
  },
  "papineni97_eurospeech": {
   "authors": [
    [
     "Kishore A.",
     "Papineni"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "Todd R.",
     "Ward"
    ]
   ],
   "title": "Feature-based language understanding",
   "original": "e97_1435",
   "page_count": 4,
   "order": 418,
   "p1": "1435",
   "pn": "1438",
   "abstract": [
    "We consider translating natural language sentences into a formal language using a system that is data-driven and built automatically from training data. We use features that capture correlations between automatically determined key phrases in both languages. The features and their associated weights are selected using a training corpus of matched pairs of source and target language sentences to maximize the entropy of the resulting conditional probability model. Given a source-language sentence, we select as the translation a target-language candidate to which the model assigns maximum probability. We report results in Air Travel Information System (ATIS) domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-417"
  },
  "amengual97_eurospeech": {
   "authors": [
    [
     "Juan Carlos",
     "Amengual"
    ],
    [
     "Jose Miguel",
     "Benedi"
    ],
    [
     "Klaus",
     "Beulen"
    ],
    [
     "Francisco",
     "Casacuberta"
    ],
    [
     "Asuncion",
     "Castano"
    ],
    [
     "Antonio",
     "Castellanos"
    ],
    [
     "Victor M.",
     "Jimenez"
    ],
    [
     "David",
     "Llorens"
    ],
    [
     "Andres",
     "Marzal"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Federico",
     "Prat"
    ],
    [
     "Enrique",
     "Vida"
    ],
    [
     "Juan Miguel",
     "Vila"
    ]
   ],
   "title": "Speech translation based on automatically trainable finite-state models",
   "original": "e97_1439",
   "page_count": 4,
   "order": 419,
   "p1": "1439",
   "pn": "1442",
   "abstract": [
    "This paper extends previous work exploring the use of Subsequential Transducers to perform speech-input translation in limited-domain tasks. This is done following an integrated approach in which a Subsequential Transducer replaces the input-language model of a conventional speech recognition system, and is used both as language and translation model. This way, the search for the recognised sentence also produces the corresponding translation. A corpus-based approach is adopted in order to build the required models from training data. Experimental results are presented for the translation task considered in the EUTRANS project: one in the hotel domain with more than 500 words per language and language perplexities near to 10.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-418"
  },
  "gotoh97_eurospeech": {
   "authors": [
    [
     "Yoshihiko",
     "Gotoh"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Document space models using latent semantic analysis",
   "original": "e97_1443",
   "page_count": 4,
   "order": 420,
   "p1": "1443",
   "pn": "1446",
   "abstract": [
    "In this paper, an approach for constructing mixture language models (LMs) based on some notion of semantics is discussed. To this end, a technique known as latent semantic analysis (LSA) is used. The approach encapsulates corpus-derived semantic information and is able to model the varying style of the text. Using such information, the corpus texts are clustered in an unsupervised manner and mixture LMs are automatically created. This work builds on previous work in the field of information retrieval which was recently applied by Bellegarda et. al. to the problem of clustering words by semantic categories. The principal contribution of this work is to characterize the document space resulting from the LSA modeling and to demonstrate the approach for mixture LM application. Comparison is made between manual and automatic clustering in order to elucidate how the semantic information is expressed in the space. It is shown that, using semantic information, mixture LMs performs better than a conventional single LM with slight increase of computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-419"
  },
  "martin97_eurospeech": {
   "authors": [
    [
     "Sven C.",
     "Martin"
    ],
    [
     "Jörg",
     "Liermann"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Adaptive topic - dependent language modelling using word - based varigrams",
   "original": "e97_1447",
   "page_count": 4,
   "order": 421,
   "p1": "1447",
   "pn": "1450",
   "abstract": [
    "This paper presents two extensions of the standard interpolated word trigram and cache model, namely the extension of the trigram model by useful word m-grams with m > 3 resulting into a varigram model, and the addition of topic-specific trigram models. We give the criteria for selecting useful m-grams and for partitioning the training corpus into topic-specific subcorpora. We apply both extensions, separately and in combination, to corpora of 4 and 39 million words taken from the Wall Street Journal Corpus and show that high reductions in perplexity of up to 19 % on the largest corpus are achieved. We also performed some recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-420"
  },
  "bellegarda97b_eurospeech": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ]
   ],
   "title": "A latent semantic analysis framework for large-Span language modeling",
   "original": "e97_1451",
   "page_count": 4,
   "order": 422,
   "p1": "1451",
   "pn": "1454",
   "abstract": [
    "A new framework is proposed to construct large-span, semantically-derived language models for large vocabulary speech recognition. It is based on the latent semantic analysis paradigm, which seeks to automatically uncover the salient semantic relationships between words and documents in a given corpus. Because of its semantic nature. a latent semantic language model is well suited to complement a conventional. more syntactically-oriented n-gram. An integrative formulation is proposed for the combination of the two paradigms. The performance of the resulting integrated language model. as measured by perplexity, compares favorably with the corresponding n-gram performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-421"
  },
  "schwartz97_eurospeech": {
   "authors": [
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Francis",
     "Kubala"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "A maximum likelihood model for topic classification of broadcast news",
   "original": "e97_1455",
   "page_count": 4,
   "order": 423,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "We describe a new algorithm for topic classification that allows discrimination among thousands of topics. A mixture of topics explicitly models the fact that each story has multiple topics, that different words are related to different topics, and that most of the words are not related to any topic. The resulting model, trained by EM, has sharper distributions of words that result in more accurate topic classification. We tested the algorithm on transcribed broadcast news texts. When trained on one year of stories containing over 5,000 different topics and tested on new (later) stories the first choice topic was among the manually annotated choices 76% of the time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-422"
  },
  "popovici97_eurospeech": {
   "authors": [
    [
     "Cosmin",
     "Popovici"
    ],
    [
     "Paolo",
     "Baggia"
    ]
   ],
   "title": "Language modelling for task-oriented domains",
   "original": "e97_1459",
   "page_count": 4,
   "order": 424,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "This paper is focused on the language modelling for task-oriented domains and presents an accurate analysis of the utterances acquired by the Dialogos spoken dialogue system. Dialogos allows access to the Italian Railways timetable by using the telephone over the public network. The language modelling aspects of specificity and behaviour to rare events are studied. A technique for getting a language model more robust, based on sentences generated by grammars, is presented. Experimental results show the benefit of the proposed technique. The increment of performance between language models created using grammars and usual ones, is higher when the amount of training material is limited. Therefore this technique can give an advantage especially for the development of language models in a new domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-423"
  },
  "lin97c_eurospeech": {
   "authors": [
    [
     "Sung-Chien",
     "Lin"
    ],
    [
     "Chi-Lung",
     "Tsai"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Ker-Jiann",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Chinese language model adaptation based on document classification and multiple domain-specific language models",
   "original": "e97_1463",
   "page_count": 4,
   "order": 425,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "Adaptation of language models to the specific subject domains is definitely important for real speech recognition applications. In this paper, a Chinese language model adaptation approach is presented mainly based on document classification and multiple domain- specific language models. The proposed document classification method using the perplexity value and word bigram coverage value as primary measures are able to model word associations and syntactic behavior in classifying documents into the clusters and thus creates more effective domain-specific language models. The adaptation of language model in speech recognition can be therefore effectively achieved by the proper selection of the most appropriated domain-specific language model. Preliminary tests have been made in application to Mandarin speech recognition and shown its exciting performance of the proposed approach in creating real applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-424"
  },
  "langlais97_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Langlais"
    ]
   ],
   "title": "Estimating prosodic weights in a syntactic-rhythmical prediction system",
   "original": "e97_1467",
   "page_count": 4,
   "order": 426,
   "p1": "1467",
   "pn": "1470",
   "abstract": [
    "This paper concerns the study of information derived from the melodic, temporal and intensity characteristics of the material to be recognized in a speech recognition system, in French. More precisely, it describes experiments we achieved at the suprasegmental levels with a system that outperform automatic correlation between prosodic labels and linguistic organization of a message to decode. Firstly an overview of the system is described along with the results of experiments carried out to determine which prosodic indexes are best-suited for syntactic and rhythmycal prediction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-425"
  },
  "ozeki97_eurospeech": {
   "authors": [
    [
     "Kazuhiko",
     "Ozeki"
    ],
    [
     "Kazuyuki",
     "Kousaka"
    ],
    [
     "Yujie",
     "Zhang"
    ]
   ],
   "title": "Syntactic information contained in prosodic features of Japanese utterances",
   "original": "e97_1471",
   "page_count": 4,
   "order": 427,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "This paper is concerned with measuring the amount of syntactic information contained in prosodic features of Japanese utterances. Five prosodic features are employed, and the statistical relationship between those features and the inter-phrase dependency distance is estimated by using training data. Then parsing experiments are conducted in two different ways:one utilizing the posterior distribution of the interphrase dependency distance given the prosodic feature values, and the other without using such information. It has been shown that significant improvement in parsing accuracy is attained by utilizing the prosodic information, and that the duration of pause between adjacent phrases is more effective than prosodic features related to the fundamental frequency and the power.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-426"
  },
  "chung97_eurospeech": {
   "authors": [
    [
     "Grace",
     "Chung"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Hierarchical duration modelling for speech recognition using the ANGIE framework",
   "original": "e97_1475",
   "page_count": 4,
   "order": 428,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "We describe a novel hierarchical duration model for speech recognition. The modelling scheme is based on the angie framework, a exible unified sublexical representation for speech applications. Our duration model captures contextual factors that in uence duration of sublexical units at multiple linguistic levels simultaneously, using both relative and absolute duration information. The modelling procedure involves a normalization scheme which produces a new measure for relative speaking rate at a word level. This may be used to explore phenomena in speech timing and we present studies on secondary effects of speaking rate here. This duration model demonstrates its ability to aid speech recognition in phonetic recognition experiments where it has yielded a relative improvement of up to 7.7%. In word spotting, a study employing duration as a post-processor in disambiguating between 2 acoustically similar keywords reduces relative error by 68%. Furthermore, a fully integrated duration model in an angie based word spotter improves performance by 21.5%. All gains are over and above any gains realized from standard phone duration models present in the baseline system. All experiments were conducted in the atis domain, using continuous spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-427"
  },
  "strom97_eurospeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Anja",
     "Elsner"
    ],
    [
     "Wolfgang",
     "Hess"
    ],
    [
     "Walter",
     "Kasper"
    ],
    [
     "Alexandra",
     "Klein"
    ],
    [
     "Hans Ulrich",
     "Krieger"
    ],
    [
     "Jörg",
     "Spilker"
    ],
    [
     "Hans",
     "Weber"
    ],
    [
     "Gunther",
     "Gorz"
    ]
   ],
   "title": "On the use of prosody in a speech-to-speech translator",
   "original": "e97_1479",
   "page_count": 4,
   "order": 429,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "In this paper a speech-to-speech translator from German to English is presented. Beside the traditional processing steps it takes advantage of acoustically detected prosodic phrase boundaries and focus. The prosodic phrase boundaries reduce search space during syntactic parsing and rule out analysis trees during semantic parsing. The prosodic focus faciliates a \"shallow\" translation based on the best word chain in cases where the deep analysis fails.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-428"
  },
  "heuven97_eurospeech": {
   "authors": [
    [
     "Vincent J. van",
     "Heuven"
    ],
    [
     "Judith",
     "Haan"
    ],
    [
     "Jos J.A.",
     "Pacilly"
    ]
   ],
   "title": "Automatic recognition of sentence type from prosody in dutch",
   "original": "e97_1483",
   "page_count": 4,
   "order": 430,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "This paper investigates to what extent statements, Wh-questions, Yes/No-questions and declarative questions in Dutch can be automatically discriminated on the basis of global and local F0-parameters. Global parameters were the slope and mean pitch of upper and lower trend lines that were fitted through F0-curves; local parameters were onset and offset F0 of a terminal question-marking pitch rise. Results indicate that women mark the interrogative status of a sentence more often and perceptually more saliently. Generally, global downtrend parameters are better predictors of sentence type than parameters of the final rise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-429"
  },
  "munteanu97_eurospeech": {
   "authors": [
    [
     "Paul",
     "Munteanu"
    ],
    [
     "Bertrand",
     "Caillaud"
    ],
    [
     "Jean-Francois",
     "Serignat"
    ],
    [
     "Genevicve",
     "Caelen-Haumont"
    ]
   ],
   "title": "Automatic word demarcation based on prosody",
   "original": "e97_1487",
   "page_count": 4,
   "order": 431,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "This paper presents a work on the acquisition of the prosodic knowledge that will be incorporated in a Word Prosody agent of a distributed speech understanding system (MICRO). The multiagent architecture of MICRO, based on wholistic analytic double processing, is first described. MICRO uses prosody with a rather new view. This group of agents quickly produces information that will be used by the analytic pathway (acoustic-phonetic analysis, lexical access, syntactic and semantic analysis, ...) as anchor points or for lexical hypotheses filtering or sorting. We discuss the role of the Word Prosody agent in this architecture and the induced requirements for its design. Then, we present some experiments that were made in order to decipher the prosodic encoding of word boundaries and lexical categories.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-430"
  },
  "kataoka97_eurospeech": {
   "authors": [
    [
     "A.",
     "Kataoka"
    ],
    [
     "S.",
     "Kurihara"
    ],
    [
     "S.",
     "Sasaki"
    ],
    [
     "S.",
     "Hayashi"
    ]
   ],
   "title": "A 16-kbit/s wideband speech codec scalable with g.729",
   "original": "e97_1491",
   "page_count": 4,
   "order": 432,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "A wideband speech scalable codec is proposed for improving the flexibility in telecommunication networks. This coder is scalable with G.729 (ITU 8-kbit/s standard). Its decoder can process the incoming bitstream at three bit rates (8, 12, and 16 kbit/s) and provide a choice of speech types (wideband and telephone-band). The codec has a split-band structure, where both bands are coded by analysis-by-synthesis techniques. This paper proposes two types of scalable codec: a separate one and a composite one. It also proposes a new method (an additional adaptive codebook) for predicting pitch, while maintaining scalability with the G.729 codec. Subjective testing for wideband speech showed that the quality of the proposed codec at 16-kbit/s is equivalent to that of the 64-kbit/s G.722, and at 12-kbit/s is better than that of the 48-kbit/s G.722. Testing has further demonstrated that the 8-kbit/s coder provides high quality for telephone-band speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-431"
  },
  "lynch97_eurospeech": {
   "authors": [
    [
     "M.",
     "Lynch"
    ],
    [
     "E.",
     "Ambikairajah"
    ],
    [
     "A.",
     "Davis"
    ]
   ],
   "title": "Comparison of auditory masking models for speech coding",
   "original": "e97_1495",
   "page_count": 4,
   "order": 433,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "In this paper various auditory masking models recently developed for audio coding are compared and evaluated for telephone bandwidth speech coding applications. Four such models are outlined and their performance evaluated using a Wavelet Packet Transform based subband coder. The models are compared on the basis of the resulting perceptual speech quality and bit rate requirements. Results show that masking models 3 and 4 outlined in this paper provide near transparent quality at the lowest bit rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-432"
  },
  "amodio97_eurospeech": {
   "authors": [
    [
     "A.",
     "Amodio"
    ],
    [
     "G.",
     "Feng"
    ]
   ],
   "title": "Wideband speech coding based on the MBE structure",
   "original": "e97_1499",
   "page_count": 4,
   "order": 434,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "This paper deals with the adaptation to wideband of the MBE coder which was initially developed for the telephone band. As the constraints of quality and bit rate for a wideband and a telephone band coder are different, and as the signal characteristics on these two bands are different too, we must reconsider the coder structure. Several improvements are proposed, some of which were already proposed for the telephone band such as the phonetic classification of the frames or the multi-harmonic modelling of the spectrum. We also propose in order to reach a good quality, especially for high frequency voices, to model and synthesize, as part of the signal, the initial error between the synthetic and original spectra.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-433"
  },
  "guimaraes97_eurospeech": {
   "authors": [
    [
     "Marcos Perreau",
     "Guimaraes"
    ],
    [
     "Nicolas",
     "Moreau"
    ],
    [
     "Madeleine",
     "Bonnet"
    ]
   ],
   "title": "Perceptual filter comparisons for wideband and FM bandwidth audio coders",
   "original": "e97_1503",
   "page_count": 4,
   "order": 435,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "High quality music coders commonly use auditory masked thresholds to account for the characteristics of the human ear. Perceptual filters (based upon linear signal prediction used in speech coders) are compared to filters using masked thresholds. Using listening tests, we have noticed that the second method does not provide better perceptual results. A natural way of proceeding would be to define a better psychoacoustical model. However, an intermediate method is presented here which allows additional degrees of freedom in a standard technique. The roots of the whitening filter are treated individually.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-434"
  },
  "chan97_eurospeech": {
   "authors": [
    [
     "Cheung-Fat",
     "Chan"
    ],
    [
     "Man-Tak",
     "Chu"
    ]
   ],
   "title": "Wideband coding of speech using neural network gain adaptation",
   "original": "e97_1507",
   "page_count": 4,
   "order": 436,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "In this paper, a high-quality wideband speech coder is proposed. The coding structure resembles a LD-CELP coder, however, several novel improvements are made. The gain adapter for the stochastic codebook is driven by a neural network and it updates the excitation gain in a sample-by-sample fashion. The purpose of incorporating a neural network is to exploit both the intra- and inter-frame correlation of speech signal in a non-linear manner. A psychoacoustic model instead of a simple perceptual weighting filter is used to shape the quantization noise. Simulation result shows that the proposed coder can achieve transparent coding of wideband speech at 16 kbps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-435"
  },
  "salavedra97_eurospeech": {
   "authors": [
    [
     "Josep M.",
     "Salavedra"
    ]
   ],
   "title": "Wideband-speech APVQ coding from 16 to 32 kbps",
   "original": "e97_1511",
   "page_count": 4,
   "order": 437,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "This paper describes a coding scheme for broadband speech (sampling frequency 16KHz). We present a wideband speech encoder called APVQ (Adaptive Predictive Vector Quantization). It combines Subband Coding, Vector Quantization and Adaptive Prediction as it is represented in Fig.1. Speech signal is split in 16 subbands by means of a QMF filter bank and so every subband is 500Hz wide. This APVQ encoder can be seen either as a vectorial extension of a conventional ADPCM encoder or as a scalar Subband AVPC encoder [1],[3]. In this scheme, signal vector is formed with one sample of the normalized prediction error signal coming from different subbands and then it is vector quantized. Prediction error signal is normalized by its gain and normalized prediction error signal is the input of the VQ and therefore an adaptive Gain-Shape VQ is considered. This APVQ Encoder combines the advantages of Scalar Prediction and those of Vector Quantization. We evaluate wideband speech coding in the range from 1 to 2 bits/sample.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-436"
  },
  "hung97_eurospeech": {
   "authors": [
    [
     "Wei-Wen",
     "Hung"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "A comparative analysis of blind channel equalization methods for telephone speech recognition",
   "original": "e97_1515",
   "page_count": 4,
   "order": 438,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "A blind channel equalization method called signal bias removal (SBR) has been proposed and proved to be effective in compensating the channel effect in telephone speech recognition. However, we found that the SBR method didn't work well when additive noise and multiplicative distortion are taken into account at the same time. In this paper, we propose a new method called modified signal bias removal (MSBR) which tries to overcome the problem described above in the SBR method. Some experiments are conducted to evaluate the effectiveness of the MSBR method. Experimental results show that the MSBR method outperforms the SBR no matter additive noise is considered or not in a telephone speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-437"
  },
  "hung97b_eurospeech": {
   "authors": [
    [
     "Wei-Wen",
     "Hung"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "HMM retraining based on state duration alignment for noisy speech recognition",
   "original": "e97_1519",
   "page_count": 4,
   "order": 439,
   "p1": "1519",
   "pn": "1522",
   "abstract": [
    "It is known that incorporating the temporal information of state durations into the HMM can achieve higher recognition performance. However, when a speech signal is contaminated by ambient noises, it is very possible for a state to stay too long or too short in decoding a state sequence even if state durations are adopted in the models. This phenomenon will severely reduce the efficiency of modeling techniques for state durations. To overcome this problem, a proportional alignment decoding (PAD) method combining with state duration statistics is proposed and proved experimentally to be effective when the speech signal is distorted by ambient noises. Instead of using Viterbi decoding algorithm, the PAD method is used for state decoding in the retraining phase of a conventional HMM and produce a new set of state duration statistics. This state duration alignment scheme is more efficient to prevent a state from occupying too long or too short in recognition phase.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-438"
  },
  "komori97_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Hiroki",
     "Yamamoto"
    ],
    [
     "Masayuki",
     "Yamada"
    ]
   ],
   "title": "Fast parallel model combination noise adaptation processing",
   "original": "e97_1523",
   "page_count": 4,
   "order": 440,
   "p1": "1523",
   "pn": "1526",
   "abstract": [
    "In this paper, a fast PMC (Parallel Model Combination) noise adaptation method is proposed for continuous HMM base speech recognizer. The proposed method is realized as a direct reduction of the number of PMC processing times by introducing the distribution composition with the spatial relation of distributions. The proposed method is compared with the basic PMC algorithm in recognition accuracy and adaptation processing time on telephone speech. The result showed that the proposed method saved around 65% (70.9%, - 62.7%) of PMC computation amount with almost no degradation of recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-439"
  },
  "endo97_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Endo"
    ],
    [
     "Shigeki",
     "Nagaya"
    ],
    [
     "Masayuki",
     "Nakazawa"
    ],
    [
     "Kiyoshi",
     "Furukawa"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Speech recognition module for CSCW using a microphone array",
   "original": "e97_1527",
   "page_count": 4,
   "order": 441,
   "p1": "1527",
   "pn": "1530",
   "abstract": [
    "This report proposes a recognition module for use in CSCW that suffers little degradation in recognition performance even when more than one person speaks at the same time and they speak at a distance from a microphone. This is accomplished by controlling directionality using a microphone array and estimating transmission characteristics from speakers to microphones. On the basis of evaluation performed by word spotting from continuous speech, it has been found that this module raises the recognition rate by (1) 30% in an environment where two people are speaking at the same time, and (2) by 15% when people speak at a distance of 160 cm from a microphone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-440"
  },
  "han97b_eurospeech": {
   "authors": [
    [
     "Jiqing",
     "Han"
    ],
    [
     "Munsung",
     "Han"
    ],
    [
     "Gyu-Bong",
     "Park"
    ],
    [
     "Jeongue",
     "Park"
    ],
    [
     "Wen",
     "Gao"
    ]
   ],
   "title": "Relative mel-frequency cepstral coefficients compensation for robust telephone speech recognition",
   "original": "e97_1531",
   "page_count": 4,
   "order": 442,
   "p1": "1531",
   "pn": "1534",
   "abstract": [
    "It is a crucial factor to find the robust and simple computation methods for the actual application of telephone speech recognition. In this paper, we propose a new channel compensation method, which uses a RASTA-like band-pass filter on the mel-frequency cepstral coefficients for robust telephone speech recognition. It is shown from the experiments that the proposed method, comparing with the RASTA processing, reduces the computational complexity without losing performance, and it is also better than CMS and two level CMS on the performance. We also verify that it is an effective approach to suppress very low modulation frequencies for robust telephone speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-441"
  },
  "yamamoto97_eurospeech": {
   "authors": [
    [
     "Seiichi",
     "Yamamoto"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ]
   ],
   "title": "Robust speech detection method for speech recognition system for telecommunication networks and its field trial",
   "original": "e97_1535",
   "page_count": 4,
   "order": 443,
   "p1": "1535",
   "pn": "1538",
   "abstract": [
    "Input speech to speech recognition systems may be contaminated not only by various ambient noise but also by various irrelevant sounds generated by users such as coughing, tongue clicking, mouth-noises and certain out-of-task utterances. The authors have developed a speech detection method using the likelihood of partial sentences for detecting task utterance in speech contaminated with these irrelevant sounds. This paper describes this new speech detection method and reports on a field trial of speech recognition systems with the proposed speech detection method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-442"
  },
  "mauuary97_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Mauuary"
    ],
    [
     "Lamia",
     "Karray"
    ]
   ],
   "title": "The tuning of speech detection in the context of a global evaluation of a voice response system",
   "original": "e97_1539",
   "page_count": 4,
   "order": 444,
   "p1": "1539",
   "pn": "1542",
   "abstract": [
    "Field evaluations of automatic speech recognition (ASR) systems clearly demonstrate the importance of efficient rejection procedures for filtering out-of-vocabulary tokens. High performance speech recognition systems also require efficient speech detection. This paper presents an original framework for a global evaluation of speech recognition systems allowing to tune the speech detection module of an ASR system. A global evaluation allows to measure the performances of the speech recognition system from the user point of view and to identify the weak modules of an ASR system. Global evaluations are carried out on PSN (Public Switch Network) and GSM (Global System Mobile) databases. On the PSN database, global evaluation is used to choose the best value for the speech detector threshold. The results also show, that for this optimal value, the rejection of out-of-vocabulary words is currently the main problem to be solved for building high performance speech recognition systems for large public telecommunication applications. On GSM database, global evaluation is used to evaluate the benefits of speech enhancement before speech detection. Results show that the use of spectral subtraction as the speech enhancement technique before the detection drastically improves the speech detection, and consequently the global speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-443"
  },
  "chen97_eurospeech": {
   "authors": [
    [
     "C. Julian",
     "Chen"
    ],
    [
     "Ramesh A.",
     "Gopinath"
    ],
    [
     "Michael D.",
     "Monkowski"
    ],
    [
     "Michael A.",
     "Picheny"
    ],
    [
     "Katherine",
     "Shen"
    ]
   ],
   "title": "New methods in continuous Mandarin speech recognition",
   "original": "e97_1543",
   "page_count": 4,
   "order": 445,
   "p1": "1543",
   "pn": "1546",
   "abstract": [
    "We describe new methods for speaker-independent, continuous mandarin speech recognition based on the IBM HMM-based continuous speech recognition system (1-3): First, we treat tones in mandarin as attributes of certain phonemes, instead of syllables. Second, instantaneous pitch is treated as a variable in the acoustic feature vector, in the same way as cepstra or energy. Third, by designing a set of word-segmentation rules to convert the continuous Chinese text into segmented text, an effective trigram language model is trained(4). By applying those new methods, a speaker-independent, very-large-vocabulary continuous mandarin dictation system is demonstrated. Decoding results showed that its performance is similar to the best results for US English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-444"
  },
  "spina97_eurospeech": {
   "authors": [
    [
     "Michelle S.",
     "Spina"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Automatic transcription of general audio data: effect of environment segmentation on phonetic recognition 1",
   "original": "e97_1547",
   "page_count": 4,
   "order": 446,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "The task of automatically transcribing general audio data is very different from those usually confronted by current automatic speech recognition systems. The general goal of our work is to determine the optimal training strategy for recognizing such data. Specifically, we have studied the effects of different speaking environments on a phonetic recognition task using data collected from a radio news program. We found that if a single-recognizer is to be used, it is more effective to use a smaller amount of homogeneous, clean data for training. This approach yielded a decrease in phonetic recognition error rate of over 26% over a system trained with an equivalent amount of data which contained a variety of speaking environments. We found that additional gains can be made with a multiple- recognizer system, trained with environment-specific data. Overall, we found that this approach yielded a decrease in error rate of nearly 2%, with some individual speaking environments' error rate decreasing by over 7%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-445"
  },
  "ng97_eurospeech": {
   "authors": [
    [
     "Alfred Ying Pang",
     "Ng"
    ],
    [
     "L. W.",
     "Chan"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Automatic recognition of continuous Cantonese speech with very large vocabulary",
   "original": "e97_1551",
   "page_count": 4,
   "order": 447,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "This paper presents the first published results for automatic recognition of continuous Cantonese speech with very large vocabulary. The size of the vocabulary covered by this system is about the same asthat encountered in the Hong Kong local Chinese newspaper, Wen Hui Bao (). The system covers 6335 Chinese characters () and a large number of Chinese words () can be formed by combining these Chinese characters. The input to the system is the end pointed speech waveform of a sentence or phrase, the output is the Big5 coded Chinese characters. In the development of the recognition system, we have devised new methods in 1) construction of a continuous Cantonese speech database, 2) lexical tone recognition in continuous Cantonese speech, and 3)integration of lexical tone and base syllable recognition results. The speaker dependent recognition rates for Chinese character, base syllable and lexical tone are 90.94%, 94.73% and 69.7% respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-446"
  },
  "gong97_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Source normalization training for HMM applied to noisy telephone speech recognition",
   "original": "e97_1555",
   "page_count": 4,
   "order": 448,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "We refer to environment e as some combination of speaker, handset, transmission channel and noise background condition, and regard any practical situation of a speech recognizer as a mixture of environments. A speech recognizer may be trained on multi-environment data. It may also need to adapt the trained acoustic models to new conditions. How to train an HMM with multi-environment data and from what seed model to start an adaptation are two questions of great importance. We propose a new solution to speech recognition which is based on, for both training and adaptation, a separate modeling of phonetic variation and environment variations. This problem is formulated under hidden Markov process, where we assume, - Speech x is generated by some canonical (independent ofenvironmental factors) distributions, - An unknown linear transformation We and a bias be, specific to environment e, is applied to x with probability P(e), - x cannot be observed, what we observe is the outcome of the transformation: o = Wex + be. Under maximum-likelihood (ML) criterion, by application of EM algorithm and the extension of Baum's forward and backward variables and algorithm, we obtained a joint solution to the parameters of the canonical distributions, the transformations and the biases, which is novel. For special cases, on a noisy telephone speech database, the new formulation is compared to per-utterance cepstral mean normalization (CMN) technique and shows more than 20% word error rate improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-447"
  },
  "neto97_eurospeech": {
   "authors": [
    [
     "Joao P.",
     "Neto"
    ],
    [
     "Ciro A.",
     "Martins"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "The development of a speaker independent continuous speech recognizer for portuguese",
   "original": "e97_1559",
   "page_count": 4,
   "order": 449,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "The development and evaluation of large vocabulary, speaker-independent continuous speech recognition systems are mainly done for the American English language. In this paper we present the work done to date in the development of an hybrid large vocabulary, speaker-independent continuous speech recognition system for the European Portuguese language. Due to the lack of a large appropriate speech and text database to be used in the development of that system we started collecting a large database and at the same time began developing a baseline system based on a smaller database. On this baseline system we applied techniques for automatic segmentation and labeling, in parallel with the development of a basic lexicon and language model for Portuguese. In the last part of this paper we also present the first steps of our work over the new database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-448"
  },
  "chase97b_eurospeech": {
   "authors": [
    [
     "Lin",
     "Chase"
    ]
   ],
   "title": "Blame assignment for errors made by large vocabulary speech recognizers",
   "original": "e97_1563",
   "page_count": 4,
   "order": 450,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "This paper describes an approach to identifying the reasons that speech recognition errors occur. The algorithm presented requires an accurate word transcript of the utterances being analyzed. It places errors into one of the categories: 1) due to out­of­ vocabulary (OOV) word spoken, 2) search error, 3) homophone substitution, 4) language model overwhelming correct acoustics, 5) transcript/pronunciation problems, 6) confused acoustic models, or 7) miscellaneous/not possible to categorize. Some categorizations of errors can supply training data to automatic corrective training methods that refine acoustic models. Other errors supply language model and lexicon designers with examples that identify potential improvements. The algorithm is described and results on the combined evaluation test sets from 1992­1995 of the North American Business (NAB) [1] [2] [3] corpus using the Sphinx­II recognizer [4] are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-449"
  },
  "nakamura97_eurospeech": {
   "authors": [
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Predicting speech recognition performance",
   "original": "e97_1567",
   "page_count": 4,
   "order": 451,
   "p1": "1567",
   "pn": "1570",
   "abstract": [
    "Predicting speech recognition performance in place of expensive recognition experiments is a very useful approach for the research and development of speech recognition systems. In this paper, we propose a method to predict speech recognition performance when using new test data and/or a new acoustic model. Performance prediction tests showed that the proposed method can accurately predict recognition performance, thus saving a large amount of computer resources.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-450"
  },
  "watson97_eurospeech": {
   "authors": [
    [
     "Scott D.",
     "Watson"
    ],
    [
     "Barry M.G.",
     "Cheetham"
    ],
    [
     "P.A.",
     "Barrett"
    ],
    [
     "W.T.K.",
     "Wong"
    ],
    [
     "A.V.",
     "Lewi"
    ]
   ],
   "title": "A voice activity detector for the ITU-t 8kbit/s speech coding standard g.729",
   "original": "e97_1571",
   "page_count": 4,
   "order": 452,
   "p1": "1571",
   "pn": "1574",
   "abstract": [
    "Voice Activity Detectors (VAD's) are widely used in speech technology applications where available transmission or storage capacity is limited (e.g. mobile, DCME, etc.) and must be utilised with maximum economy. Modern day digital speech coding algorithms can provide toll quality speech at bit-rates as low as 8kbit/s (e.g. ITU-T G.729) and the use of a VAD can achieve further economy in average bit-rate. This paper presents a modified version of the GSM VAD, for use with the ITU-T 8kbit/s speech coding algorithm CS-ACELP, which makes an active/inactive decision for every 10 ms coding frame. The performance of the proposed voice activity detector is compared to that of the GSM coder in terms of VAD errors and subjective quality. Results indicate that the modified VAD has similar performance to the standardised GSM VAD while operating with G.729 parameters and coding frame size.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-451"
  },
  "muthusamy97_eurospeech": {
   "authors": [
    [
     "Yeshwant K.",
     "Muthusamy"
    ],
    [
     "John J.",
     "Godfrey"
    ]
   ],
   "title": "Vocabulary-independent recognition of American Spanish phrases and digit strings",
   "original": "e97_1575",
   "page_count": 4,
   "order": 453,
   "p1": "1575",
   "pn": "1578",
   "abstract": [
    "We have developed a speech interface to the Web that allows easy access to information and an approach to intelligent user agents. The mechanisms developed apply to other multimedia applications where speech can serve as an input modality. We describe the benefits of our recognition system to speech-application developers: (1) Developers need not know about speech |in the simplest case, developers simply de_ne HTML links. (2) Developers need not worry about word pronunciations since the system provides these. Developers may specify grammars in a simple BNF syntax and the system automatically converts these for use by the recognizer. (3) Developers with programming skills may use a Web server or the Java programming language to easily produce more sophisticated speech interfaces. (4) Developers reap the benefits of portability through general HTML browsers and languages such asJava. Java also simplifies the development of portable graphical interfaces that couple with speech input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-452"
  },
  "meyer97_eurospeech": {
   "authors": [
    [
     "Michael",
     "Meyer"
    ],
    [
     "Hermann",
     "Hild"
    ]
   ],
   "title": "Recognition of spoken and spelled proper names",
   "original": "e97_1579",
   "page_count": 4,
   "order": 454,
   "p1": "1579",
   "pn": "1582",
   "abstract": [
    "Many speech applications, most prominently telephone directory assistance, require the recognition of proper names. However, the recognition of increasingly large sets of spoken names is dificult: Besides technical limitations, very large recognition vocabularies contain many easily confused words or even homophones. Therefore, proper names are often spelled or both spoken and spelled. In this paper we compare the performance for proper name recognition when a name is spoken only, spelled only, or both spoken and spelled. In the latter case, information about the same name is provided in two different representations. We address methods to exploit this redundancy and propose techniques to handle the recognition of large lists of spoken and spelled proper names.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-453"
  },
  "kobayashi97_eurospeech": {
   "authors": [
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "HMM compensation for noisy speech recognition based on cepstral parameter generation",
   "original": "e97_1583",
   "page_count": 4,
   "order": 455,
   "p1": "1583",
   "pn": "1586",
   "abstract": [
    "This paper proposes a technique for compensating both static and dynamic parameters of continuous mixture density HMM to make it robust to noise. The technique is based on cepstral parameter generation from HMM using dynamic parameters. The generated cepstral vector sequences of speech and noise are combined to yield noisy speech cepstral vector sequence, and the dynamic parameters are calculated from the obtained cepstral vector sequence. Model parameters for noisy speech HMM are obtained using the statistics of the noisy speech parameter sequences. We use the mixture transition probability for estimating the parameters of the compensated model. Experimental results show the effectiveness of the proposed technique in the noisy speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-454"
  },
  "nokas97_eurospeech": {
   "authors": [
    [
     "George",
     "Nokas"
    ],
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "On the robustness of the critical-band adaptive filtering method for multi-source noisy speech recognition",
   "original": "e97_1587",
   "page_count": 4,
   "order": 456,
   "p1": "1587",
   "pn": "1590",
   "abstract": [
    "In this paper we study the influence of the sub-band adaptive filtering speech enhancement method on speech recognition systems in multi-source noisy environment using a speaker and a noise reference microphone. In extensive experiments, the recognition score of a speaker independent isolated word speech recognition system based on a continuous density HMM (CDHMM) has been measured in the presence of real life noises in various SNRs. In all experiments the results show improvement in the mean recognition score when the sub-band adaptive filtering LMS method is used in comparison to the full-band LMS method. This improvement increases when changing types of noise distort the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-455"
  },
  "guan97_eurospeech": {
   "authors": [
    [
     "Cun-tai",
     "Guan"
    ],
    [
     "Shu-hung",
     "Leung"
    ],
    [
     "Wing-hong",
     "Lau"
    ]
   ],
   "title": "A space transformation approach for robust speech recognition in noisy environments",
   "original": "e97_1591",
   "page_count": 4,
   "order": 457,
   "p1": "1591",
   "pn": "1594",
   "abstract": [
    "To improve the robustness of speech recognition in additive noisy environments, an SVD based space transformation approach is proposed. It is shown that with this approach, not only the signal-to-noise ratio is improved but also a significant recognition error reduction is achieved. A multiple model based on the proposed method is developed and it can provide high recognition rate for a large range of SNRs. Recognition experiments on a speaker-dependent mono-syllabic database with additive noise show that, this new approach outperforms LPC cepstrum, MFCC, and OSALPC cepstrum significantly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-456"
  },
  "vaich97_eurospeech": {
   "authors": [
    [
     "Tzur",
     "Vaich"
    ],
    [
     "Arnon",
     "Cohen"
    ]
   ],
   "title": "Robust isolated word recognition using WSP-PMC combination",
   "original": "e97_1595",
   "page_count": 4,
   "order": 458,
   "p1": "1595",
   "pn": "1598",
   "abstract": [
    "A new robust algorithm for isolated word recognition in low SNR environments is suggested . The algorithm, called WSP, is described here for left to right models with no skips. It is shown that the algorithm outperforms the conventional HMM in the SNR range of 5 to 20db, and the PMC algorithm in the range 0 to -9db.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-457"
  },
  "raptis97_eurospeech": {
   "authors": [
    [
     "Spyros",
     "Raptis"
    ],
    [
     "George V.",
     "Carayannis"
    ]
   ],
   "title": "Fuzzy logic for rule-based formant speech synthesis",
   "original": "e97_1599",
   "page_count": 4,
   "order": 459,
   "p1": "1599",
   "pn": "1602",
   "abstract": [
    "Fuzzy set theory and fuzzy logic has been initiated by Zadeh back in 1965 to pennit the treatment of vague, imprecise, and ill-defined knowledge in an concise manner. One of the unique advantages of fuzzy logic is that it is capable of directly incorporating and utilizing qualitative and heuristic knowledge in the form of causal if then production rules for reasoning and inference. On the other hand, rule-based speech synthesis based on formants makes considerable use of rules for numerous of the tasks it involves, e.g. graphemic to phonemic transcription, coarticulation, concatenation, and duration rules etc. These rules also take the if then form with their antecedent (condition) part describing the context of the rule and their decedent an appropriate action to be taken. The main motivation for introducing fuzzy logic in the synthesis-by-rule paradigm, is its ability to host and treat uncertainty and imprecision both in the condition part of the rule as well as its decedent part. This may be argued to significantly reduce the number of required rules while rendering them more meaningful and human-like.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-458"
  },
  "jourlin97_eurospeech": {
   "authors": [
    [
     "Pierre",
     "Jourlin"
    ],
    [
     "Juergen",
     "Luettin"
    ],
    [
     "Dominique",
     "Genoud"
    ],
    [
     "Hubert",
     "Wassner"
    ]
   ],
   "title": "Integrating acoustic and labial information for speaker identification and verification",
   "original": "e97_1603",
   "page_count": 4,
   "order": 460,
   "p1": "1603",
   "pn": "1606",
   "abstract": [
    "This paper describes a multimodal approach for speaker verification. The system consists of two classifiers, one using visual features and the other using acoustic features. A lip tracker is used to extract visual information from the speaking face which provides shape and intensity features. We describe an approach for normalizing and mapping different modalities onto a common confidence interval. We also describe a novel method for integrating the scores of multiple classifiers. Verification experiments are reported for the individual modalities and for the combined classifier. The performance of the integrated system outperformed each sub-system and reduced the false acceptance rate of the acoustic sub-system from 2.3% to 0.5%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-459"
  },
  "ng97b_eurospeech": {
   "authors": [
    [
     "Kenney",
     "Ng"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Subword unit representations for spoken document retrieval",
   "original": "e97_1607",
   "page_count": 4,
   "order": 461,
   "p1": "1607",
   "pn": "1610",
   "abstract": [
    "This paper investigates the feasibility of using subword unit representations for spoken document retrieval as an alternative to using words generated by either keyword spotting or word recognition. Our investigation is motivated by the observation that word-based retrieval approaches face the problem of either having to know the keywords to search for a priori, or requiring a very large recognition vocabulary in order to cover the contents of growing and diverse message collections. In this study, we examine a range of subword units of varying complexity derived from phonetic transcriptions. The basic underlying unit is the phone; more and less complex units are derived by varying the level of detail and the length of sequences of the phonetic units. We measure the ability of the different subword units to effectively index and retrieve a large collection of recorded speech messages. We also compare their performance when the underlying phonetic transcriptions are perfect and when they contain phonetic recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-460"
  },
  "teissier97_eurospeech": {
   "authors": [
    [
     "Pascal",
     "Teissier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Anne",
     "Guerin-Dugue"
    ]
   ],
   "title": "Non-linear representations, sensor reliability estimation and context-dependent fusion in the audiovisual recognition of speech in noise",
   "original": "e97_1611",
   "page_count": 4,
   "order": 462,
   "p1": "1611",
   "pn": "1614",
   "abstract": [
    "The paper involves the recognition of French audiovisual vowels at various signal-to-noise ratios (SNRs). It deals with a new non-linear preprocessing of the audio data which enables an estimation of the reliability of the audio sensor in relation to SNR, and a significant increase in the recognition performances at the output of the fusion process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-461"
  },
  "renevey97_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Securized flexible vocabulary voice messaging system on unix workstation with ISDN connection",
   "original": "e97_1615",
   "page_count": 4,
   "order": 463,
   "p1": "1615",
   "pn": "1618",
   "abstract": [
    "This paper considers applications of automatic speech recognition and speaker verification techniques in developing efficient Voice Messaging Systems for Integrated Services Digital Network (ISDN) based communication systems. The prototype demonstrator presented was developed in the framework of cooperative project which involves two research institutes: the Signal Processing Laboratory (LTS) of the Swiss Federal Institute of Technology Lausanne (EPFL) and the Dalle Molle Institute for Perceptive Artificial Intelligence, Martigny (IDIAP), and three industrial partners: the Advanced Communication Services (aComm), SunMicrosystems (Switzerland) and the Swiss Telecom PTT [1]. The project is supported by the Commission for Technology and Innovation (CTI). The goal of the project is to make available basic technologies for automatic speech recognition and speaker verification on multi-processor SunSPARC workstation and SwissNet (ISDN) platform to industrial partners. The developed algorithms provide the necessary tools to design and implement workstation oriented voice messaging demonstrators for telephone quality Swiss French. The speech recognition algorithms are based on speaker independent exible vocabulary technology and speaker verification is performed by a number of techniques executed in parallel, and combined for optimal decision. The recognition results obtained validate the exible vocabulary approach which offers the potential to build word models for any application vocabulary from a single set of phonetic sub-word units trained with the Swiss French Polyphone database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-462"
  },
  "mokbel97b_eurospeech": {
   "authors": [
    [
     "Houda",
     "Mokbel"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Automatic derivation of multiple variants of phonetic transcriptions from acoustic signals",
   "original": "e97_1619",
   "page_count": 4,
   "order": 464,
   "p1": "1619",
   "pn": "1622",
   "abstract": [
    "This paper deals with two methods for automatically finding multiple phonetic transcriptions of words, given sample utterances of the words and an inventory of context-dependent subword units. The two approaches investigated are based on an analysis of the N-best phonetic decoding of the available utterances. In the set of transcriptions resulting fromthe N-best decoding of all the utterances, the first method selects the K most frequent variants (Frequency Criterion) , while the second method selects the K most likely ones (Maximum Likelihood Criterion). Experiments carried out on speaker-independent recognition showed that the performance obtained with the \"Maximum Likelihood Criterion\" is not much different from that obtained with manual transcriptions. In the case of speaker-dependent speech recognition, the estimate of the 3 most likely transcription variants of each word, yields promising results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-463"
  },
  "nakamura97b_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Ron",
     "Nagai"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Improved bimodal speech recognition using tied-mixture HMMs and 5000 word audio-visual synchronous database",
   "original": "e97_1623",
   "page_count": 4,
   "order": 465,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "This paper presents methods to improve speech recognition accuracy by incorporating automatic lip reading. The paper improves lip reading accu- racy by following approaches; 1)collection of image and speech synchronous data of 5240 words, 2)feature extraction of 2-dimensional power spectra around a mouth and 3)sub-word unit HMMs with tied-mixture distribution(Tied-Mixture HMMs). Experiments through 100 word test show the performance of 85% by lipreading alone. It is also shown that tied-mixture HMMs improve the lip reading accuracy. The speech recognition experiments are carried out over various SNR integrating audio-visual information. The results show the integration always realizes better performance than that using either audio or visual information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-464"
  },
  "depambour97_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Depambour"
    ],
    [
     "Regine",
     "Andre-Obrecht"
    ],
    [
     "Bernard",
     "Delyon"
    ]
   ],
   "title": "On the use of phone duration and segmental processing to label speech signal",
   "original": "e97_1627",
   "page_count": 4,
   "order": 466,
   "p1": "1627",
   "pn": "1630",
   "abstract": [
    "This paper presents recent work on continuous speech labelling. We propose an original automatic labelling system where elementary phone models take a segmental analysis and the phone duration into account. These models are initialized by a short speaker-independent training stage in order to constitute a model database. From the standard phonetic transcription, phonological rules are gathered to process the various pronunciations. For each new corpus or speaker, a new quick unsupervised adaptation stage is performed to re-estimate the models, and then follows the correct labelling. We assess this system by labelling a difficult corpus (sequences of connected spelled letter) and sentences of one speaker of the BREF80 corpus. These results are quite promising, in the two experiments less than 9% of phonetic boundaries are incorrectly located.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-465"
  },
  "paping97_eurospeech": {
   "authors": [
    [
     "Martin",
     "Paping"
    ],
    [
     "Thomas",
     "Fahnle"
    ]
   ],
   "title": "Automatic detection of disturbing robot voice- and ping pong-effects in GSM transmitted speech",
   "original": "e97_1631",
   "page_count": 4,
   "order": 467,
   "p1": "1631",
   "pn": "1634",
   "abstract": [
    "This contribution reports about a method to automatically detect the disturbing Robot Voice and Ping Pong effect which occur in GSM transmitted speech. Both effects are caused by the frame substitution technique, recommended by the GSM standard: in these cases the transmitted speechmay be modulated by a disturbing 50 Hz component. These modulations can be detected very easily in the frequency domain. By a framewise comparision of the modulation amplitude of an undisturbed clean speech signal with a test signal it is possible to locate the occurrence of Robot Voice and Ping Pong very precisely. Comparing human perception to the outcome of the proposed algorithm shows a high degree of correspondence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-466"
  },
  "martino97_eurospeech": {
   "authors": [
    [
     "Joseph Di",
     "Martino"
    ]
   ],
   "title": "Speech synthesis using phase vocoder techniques",
   "original": "e97_1635",
   "page_count": 4,
   "order": 468,
   "p1": "1635",
   "pn": "1638",
   "abstract": [
    "A new light is thrown on the Portnoff [1] speech signal time-scale modification algorithm. It is shown in particular that the Portnoff algorithm easily accommodates expansion factors bigger than 2 without causing reverberation nor chorusing. The modified Portnoff algorithm, which draws on spectral modification techniques due to Seneff [2], has been tested on several speech signals. The quality of the synthesized signal is totally satisfactory even for big expansion factors. The article gives a brief summary of the Portnoff algorithm and spells out the modifications introduced. It is shown that the phase unwrapping procedure constitutes a crucial point of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-467"
  },
  "sarukkai97_eurospeech": {
   "authors": [
    [
     "Ramesh R.",
     "Sarukkai"
    ],
    [
     "Craig",
     "Hunter"
    ]
   ],
   "title": "Integration of eye fixation information with speech recognition systems",
   "original": "e97_1639",
   "page_count": 4,
   "order": 469,
   "p1": "1639",
   "pn": "1643",
   "abstract": [
    "In this paper, a semi-tight coupling between visual and auditory modalities is proposed: in particular, eye fixation information is used to enhance the output of speech recognition systems. This is achieved by treating natural human eye fixations as diectic references to symbolic objects, and passing on this information to the speech recognizer. The speech recognizer biases its search towards these set of symbols/words during the best word sequence search process. As an illustrative example, the TRAINS interactive planning assistant system has been used as a test-bed; eye-fixations provide important cues to city names which the user sees on the map. Experimental results indicate that eye fixations help reduce speech recognition errors. This work suggests that integrating information from different interfaces to bootstrap each other would enable the development of reliable and robust interactive multi-modal human- computer systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-468"
  },
  "nakatoh97_eurospeech": {
   "authors": [
    [
     "Yoshihisa",
     "Nakatoh"
    ],
    [
     "M.",
     "Tsushima"
    ],
    [
     "T.",
     "Norimatsu"
    ]
   ],
   "title": "Generation of broadband speech from narrowband speech using piecewise linear mapping",
   "original": "e97_1643",
   "page_count": 4,
   "order": 470,
   "p1": "1643",
   "pn": "1646",
   "abstract": [
    "This paper proposes a recovery method of broadband speech form narrowband speech based on piecewise linear mapping. In this method, narrowband spectrum envelope of input speech is transformed to broadband spectrum envelope using linearly transformed matrices which are associated with several spectrum spaces. These matrices were estimated by speech training data, so as to minimize the mean square error between the transformed and the original spectra. This algorithm is compared the following other methods, (1)the codebook mapping, (2)the neural network. Through the evaluation by the spectral distance measure, it was found that the proposed method achieved a lower spectral distortion than the other methods. Perceptual experiments indicates a good performance for the reconstructed broadband speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-469"
  },
  "rogers97_eurospeech": {
   "authors": [
    [
     "Ian E.C.",
     "Rogers"
    ]
   ],
   "title": "An assessment of the benefits active noise reduction systems provide to speech intelligibility in aircraft noise environments",
   "original": "e97_1647",
   "page_count": 4,
   "order": 471,
   "p1": "1647",
   "pn": "1650",
   "abstract": [
    "The high noise levels being experienced in some military fast jet aircraft and helicopters generally result in a reduction in the intelligibility of speech communications. A study has been conducted to assess the effect of reducing noise levels at the ear, by the use of current Active Noise Reduction (ANR) systems, on speech intelligibility in aircraft noise environments. The results of this study indicate that ANR would improve speech intelligibility in both types of aircraft. The assessment has been conducted using Diagnostic Rhyme Test (DRT) and Articulation Index (AI) techniques. The study has also allowed the correlation between DRT and AI test results to be investigated. A more detailed account of the work reported in this paper is provided at [1].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-470"
  },
  "beskow97_eurospeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Scott",
     "McGlashan"
    ]
   ],
   "title": "OLGA - a dialogue system with an animated talking agent",
   "original": "e97_1651",
   "page_count": 4,
   "order": 472,
   "p1": "1651",
   "pn": "1654",
   "abstract": [
    "The object of the Olga project is to develop an interactive 3D animated talking agent. A futuristic application scenario is interactive digital TV, where the Olga agent would guide naive users through the various services available on the network. The current application is a consumer information service for microwave ovens. Olga required the development of a system with components from many different fields: multimodal interfaces, dialogue management, speech recognition, speech synthesis, graphics, animation, facilities for direct manipulation and database handling. To integrate all knowledge sources Olga is implemented with separate modules communicating with a central dialogue interaction manager. In this paper we mainly describe the talking animated agent and the dialogue manager. There is also a short description of the preliminary speech recogniser used in the project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-471"
  },
  "robbe97_eurospeech": {
   "authors": [
    [
     "Sandrine",
     "Robbe"
    ],
    [
     "Noelle",
     "Carbonell"
    ],
    [
     "Claude",
     "Valot"
    ]
   ],
   "title": "Towards usable multimodal command languages: definition and ergonomic assessment of constraints on users' spontaneous speech and gestures",
   "original": "e97_1655",
   "page_count": 4,
   "order": 473,
   "p1": "1655",
   "pn": "1658",
   "abstract": [
    "Within the framework of a prospective ergonomic approach, we simulated two multimodal user interfaces, in order to study the usability of constrained vs spontaneous speech in a multimodal environment. The first experiment, which served as a reference, gave subjects the opportunity to use speech and gestures freely, while subjects in the second experiment had to comply with multimodal constraints. We first describe the experimental setup and the approach we adopted for designing the artificial command language used in the second experiment. We then present the results of our analysis of the subjects' utterances and gestures, laying emphasis on their implementation of linguistic constraints. The conclusions of the empirical assessment of the usability of this multimodal command language built from a restricted subset of natural language and simple designation gestures is associated with recommendations which may prove useful for improving the usability of oral human-computer interaction in a multimodal environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-472"
  },
  "suhm97_eurospeech": {
   "authors": [
    [
     "Bernhard",
     "Suhm"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Exploiting repair context in interactive error recovery",
   "original": "e97_1659",
   "page_count": 3,
   "order": 474,
   "p1": "1659",
   "pn": "1662",
   "abstract": [
    "In current speech applications, facilities to correct recognition errors are limited to either choosing among alternative hypotheses (either by voice or by mouseclick) or respeaking. Information from the context a repair is ignored. We developed a method which improves the accuracy of correcting speech recognition errors interactively by taking into account the context of the repair interaction. The basic idea is to use the same language modeling information used in the initial decoding of continuous speech input for decoding (isolated word) repair input. The repair is not limited to speech, but the user can choose to switch modality, for instance spelling or handwriting a word. We implemented this idea by rescoring N-best lists obtained from decoding the repair input using language model scores for trigrams which include the corrected word. We evaluated the method on a set of repairs by respeaking, spelling and handwriting which we collected with our prototypical continuous speech dictation interface. The method can increase the accuracy of repair significantly, compared to recognizing the repair input as independent event.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-473"
  },
  "reveret97_eurospeech": {
   "authors": [
    [
     "Lionel",
     "Reveret"
    ],
    [
     "Frederique",
     "Garcia"
    ],
    [
     "Christian",
     "Benoit"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "An hybrid image processing approach to liptracking independent of head orientation",
   "original": "e97_1663",
   "page_count": 4,
   "order": 475,
   "p1": "1663",
   "pn": "1666",
   "abstract": [
    "This paper examines the influence of head orientation in liptracking. There are two main conclusions: First, lip gesture analysis and head movement correction should be processed independently. Second, the measurement of articulatory parameters may be corrupted by head movement if it is performed directly at the pixel level. We thus propose an innovative technique of liptracking which relies on a \"3D active contour\" model of the lips controlled by articulatory parameters. The 3D model is projected onto the image of a speaking face through a camera model, thus allowing spatial re-orientation of the head. Liptracking is then performed by automatic adjustment of the control parameters, independently of head orientation. The final objective of our study is to apply a pixel-based method to detect head orientation. Nevertheless, we consider that head motion and lip gestures are detected by different processes, whether cognitive (by humans) or computational (by machines). Due to this, we decided to first develop and evaluate orientation-free liptracking through a non video-based head motion detection technique which is here presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-474"
  },
  "goff97_eurospeech": {
   "authors": [
    [
     "Bertrand Le",
     "Goff"
    ]
   ],
   "title": "Automatic modeling of coarticulation in text-to-visual speech synthesis",
   "original": "e97_1667",
   "page_count": 4,
   "order": 476,
   "p1": "1667",
   "pn": "1670",
   "abstract": [
    "We have developed a visual speech synthesizer from unlimited French text, and synchronized it to an audio text-to-speech synthesizer also developed at the ICP (Le Goff & Benoit, 1996). The front-end of our synthesizer is a 3-D model of the face whose speech gestures are controlled by eight parameters: Five for the lips, one for the chin, two for the tongue. In contrast to most of the existing systems which are based on a limited set of prestored facial images, we have adopted the parametric approach to coarticulation first proposed by Cohen and Massaro (1993). We have thus implemented a coarticulation model based on spline-like functions, defined by three coefficients, applied to each target in a library of 16 French visemes. However, unlike Cohen & Massaro (1993), we have adopted a data-driven approach to identify the many coefficients necessary to model coarticulation. To do so, we systematically analyzed an ad-hoc corpus uttered by a French male speaker. We have then run an intelligibility test to quantify the benefit of seeing the synthetic face (in addition to hearing the synthetic voice) under several conditions of background noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-475"
  },
  "adjoudani97_eurospeech": {
   "authors": [
    [
     "Ali",
     "Adjoudani"
    ],
    [
     "Thierry",
     "Guiard-Marigny"
    ],
    [
     "Bertrand Le",
     "Goff"
    ],
    [
     "Lionel",
     "Reveret"
    ],
    [
     "Christian",
     "Benoit"
    ]
   ],
   "title": "A multimedia platform for audio-visual speech processing",
   "original": "e97_1671",
   "page_count": 4,
   "order": 477,
   "p1": "1671",
   "pn": "1674",
   "abstract": [
    "In the framework of the European ESPRIT Project MIAMI (\"Multimodal Integration for Advanced Multimedia Interfaces\"), a platform has been developed at the ICP to study the various combinations of audio-visual speech processing, including real-time lip motion analysis, real-time synthesis of models of the lips and of the face, audiovisual speech recognition of isolated words, and text-to-audio-visual speech synthesis in French. All these facilities are implemented on a network of three SGI computers. Not only this platform is a usefull research tool to study the production and the perception of visible speech as well as audio-visual integration by humans and by the machines, but it is also a nice testbed to study man-machine multimodal interaction and very low bit rate audio-visual speech communication between humans.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-476"
  },
  "fujisaki97b_eurospeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Hiroyuki",
     "Kameda"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Takuya",
     "Ito"
    ],
    [
     "Ken",
     "Tajima"
    ],
    [
     "Kenji",
     "Abe"
    ]
   ],
   "title": "An intelligent system for information retrieval over the internet through spoken dialogue",
   "original": "e97_1675",
   "page_count": 4,
   "order": 478,
   "p1": "1675",
   "pn": "1678",
   "abstract": [
    "For the purpose of coping with the affluence of information available over the Internet, an efficient, accurate and user-friendly system for information retrieval is mandatory. This paper presents an intelligent system based on the use of spoken dialogue as the main channel for user-system interface, use of key concepts, processing of unknown words, automatic acquisition of various kinds of knowledge for improving the performance, and agent technologies for system realization. Details of functions required for the agents are also described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-477"
  },
  "yardimci97_eurospeech": {
   "authors": [
    [
     "Yasemin",
     "Yardimci"
    ],
    [
     "A. Enis",
     "Cetin"
    ],
    [
     "Rashid",
     "Ansari"
    ]
   ],
   "title": "Data hiding in speech using phase coding",
   "original": "e97_1679",
   "page_count": 4,
   "order": 479,
   "p1": "1679",
   "pn": "1682",
   "abstract": [
    "The human auditory system is insensitive to phase information of the speech signal. By taking advantage of this fact data such as the transcript, some keywords, and copyright information can be embedded into the speech signal by altering the phase in a predefined manner. In this paper, an all-pass filtering based data embedding scheme is developed for speech signals. Since all-pass filters modify only the phase without effecting the magnitude response they are employed to diffuse data into the speech signal by filtering different portions of the speech signal with different all-pass filters. The embedded data can be retrieved by tracking the zeros of the all-pass filters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-478"
  },
  "burnham97_eurospeech": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "John",
     "Fowler"
    ],
    [
     "Michelle",
     "Nicol"
    ]
   ],
   "title": "CAVE: an on-line procedure for creating and running auditory-visual speech perception experiments-hardware, software, and advantages",
   "original": "e97_1683",
   "page_count": 4,
   "order": 480,
   "p1": "1683",
   "pn": "1686",
   "abstract": [
    "The McGurk effect or fusion illusion, in which mismatched auditory and visual speech sound components are perceived as an emergent phone, is extensively used in auditory-visual speech perception research. The usual method of running experiments involves time-consuming preparation of dubbed videotapes. This paper describes an alternative, the Computerised Auditory-Visual Experiment (CAVE), in which audio dubbing occurs on-line. Its advantages include reduced preparation time, greater flexibility, and on-line collection of response type and latency data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-479"
  },
  "schiel97_eurospeech": {
   "authors": [
    [
     "Florian",
     "Schiel"
    ],
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Hans G.",
     "Tillmann"
    ]
   ],
   "title": "The bavarian archive for speech signals: resources for the speech community",
   "original": "e97_1687",
   "page_count": 4,
   "order": 481,
   "p1": "1687",
   "pn": "1690",
   "abstract": [
    "This paper gives an overview of the activities at the Bavarian Archive of Speech Signals (BAS) that was founded as a non-profit organization in 1995. The main purpose of BAS is the development of aComplete Phonetic Theory (CPT) of German based on the empirical exploitation of very large databases of spoken German. However, on our way to that goal BAS will act as a focal point for all computer readable speech resources in the German language and distribute these resources to the speech community. These resources are intended to cover the speech part of the German language, i.e. speech data, labeling and segmentations, knowledge about pronunciation. In the following we give a concise overview of what resources are presently available at BAS, how they were produced, how they can be obtained from BAS, how we use these resources in various scientific activities and a brief summary of ongoing projects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-480"
  },
  "draxler97b_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "WWWTranscribe - a modular transcription system based on the world wide web",
   "original": "e97_1691",
   "page_count": 4,
   "order": 482,
   "p1": "1691",
   "pn": "1694",
   "abstract": [
    "WWWTranscribe is a transcription system based on the WWW. It is platform independent and allows network access to speech databases. Its modular structure make it flexible, and it connects easily to existing signal processing applications or database management systems. WWWTranscribe consists of static HTML documents containing forms. To these forms CGI applications are attached that perform data processing and that dynamically create subsequent HTML documents. The system has been developed for the orthographic annotation of the German SpeechDat(II) telephone speech database. In its current implementation, it automatically creates SAM-PA annotation files according to the SpeechDat(II) database specifications [5], [4]. Variants of the system are being used for transcription by other SpeechDat(II) partners.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-481"
  },
  "engberg97_eurospeech": {
   "authors": [
    [
     "Inger S.",
     "Engberg"
    ],
    [
     "Anya Varnich",
     "Hansen"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Design, recording and verification of a danish emotional speech database",
   "original": "e97_1695",
   "page_count": 4,
   "order": 483,
   "p1": "1695",
   "pn": "1698",
   "abstract": [
    "A database of recordings of Danish Emotional Speech, DES, has been recorded and analysed. DES has been collected in order to evaluate how well the emotional state in emotional speech is identified by humans. The results sets a standard for identifying Danish emotional speech. DES contains recordings from four actors, two of each gender. Actors were used for the recordings as they were believed to be able to realistically convey a number of emotions, namely: neutral, surprise, happiness, sadness and anger. The recordings from each actor consist of two isolated words, nine sentences and two passages. The complete database comprises approximately 30 minutes of speech. A listening test with 20 listeners was conducted. The emotions were on the average identified correctly in 67,3% of the cases, with a [66,0 - 68,6] 95% confidence interval. An analysis reveals that most confusion occurred between surprise and happiness and between neutral and sadness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-482"
  },
  "eskenazi97_eurospeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ],
    [
     "C.",
     "Hogan"
    ],
    [
     "J.",
     "Allen"
    ],
    [
     "R.",
     "Frederking"
    ]
   ],
   "title": "Issues in database creation: recording new populations, faster and better labelling",
   "original": "e97_1699",
   "page_count": 5,
   "order": 484,
   "p1": "1699",
   "pn": "1702",
   "abstract": [
    "As speech recognition systems become more accurate, they are used for more diverse applications. These applications often involve populations who never used a recogniser before and for whom the standard data for adult male, adult female, or mixed adult speech is not very representative. This paper will deal with issues concerning the collection and processing of data from those new speaker populations and from speakers of different languages. It deals with data collected for various projects, such as the KIDS database [1] and the Diplomat project [2]. It specifically discusses issues related to obtaining quantitatively and qualitatively sufficient amounts of speech from diverse speaker populations. Since the speech of these individuals is very different from the speech collected in the past, we assume that some hand labelling may be necessary and therefore also address the issue of ameliorating the labelling process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-483"
  },
  "feldes97_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Feldes"
    ],
    [
     "Bernhard",
     "Kaspar"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Design and analysis of a German telephone speech database for phoneme based training",
   "original": "e97_1703",
   "page_count": 4,
   "order": 485,
   "p1": "1703",
   "pn": "1706",
   "abstract": [
    "Based on the Sotscheck text corpus, we developped a new corpus that was specifically optimised for training phoneme-based recognition systems. Particular attention was payed on good coverage of phone transitions. Even though the resulting corpus is only slightly enlarged, it shows an increased phonetic coverage while maintaining a good phonetic balance. Results of phonetic statistical analysis and of experiments for training an allophone-based recognizer are reported here.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-484"
  },
  "neto97b_eurospeech": {
   "authors": [
    [
     "Joao P.",
     "Neto"
    ],
    [
     "Ciro A.",
     "Martins"
    ],
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "The design of a large vocabulary speech corpus for portuguese",
   "original": "e97_1707",
   "page_count": 4,
   "order": 486,
   "p1": "1707",
   "pn": "1710",
   "abstract": [
    "The last years show a great development of large vocabulary, speaker-independent continuous speech recognition systems and some research in multilingual aspects. To allow that development to also be extended to the European Portuguese language we decided to develop and collect a large database of continuous speech based on a large amount of text. In the development of this new Portuguese database our aim was to create a corpus equivalent in size to WSJ0. We selected the database texts from the PUBLICO newspaper, which is characterized by a broad coverage of matters and different writing styles. The recording population was selected from a large engineering school, assuring a large variability of speakers. The recordings are being done as we write this paper and we expect to release the database in CD format in September 1997.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-485"
  },
  "nord97_eurospeech": {
   "authors": [
    [
     "Lennart",
     "Nord"
    ],
    [
     "Britta",
     "Hammarberg"
    ],
    [
     "Elisabet",
     "Lundstrom"
    ]
   ],
   "title": "Continued investigations of laryngectomee speech in noise - measurements and intelligibility tests",
   "original": "e97_1711",
   "page_count": 4,
   "order": 487,
   "p1": "1711",
   "pn": "1714",
   "abstract": [
    "The speech of nine laryngectomized persons is analysed. Both tracheo-esophageal and esophageal speakers are included. The speech performance of the subjects is evaluated while they are reading texts aloud with varying amounts of noise in their ears. One hypothesis is that the amount of noise will influence the articulation skill and voice behaviour of the speakers. Preliminary results show that some of the tracheo-esophageal speakers were able to raise their voice level as much as the normal laryngeal speakers. The esophageal speakers on the other hand were usually not able to produce as strong voice levels during the text readings. Acoustic speech parameters, such as sound pressure and spectral characteristics were measured and compared among the subjects. when the speakers were forced to use a great deal of effort, they spent air rapidly and had to use shorter stretches of speech with many pauses for inhalation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-486"
  },
  "rothkrantz97_eurospeech": {
   "authors": [
    [
     "L.J.M.",
     "Rothkrantz"
    ],
    [
     "W.A.Th.",
     "Manintveld"
    ],
    [
     "M.M.M.",
     "Rats"
    ],
    [
     "R.J. van",
     "Vark"
    ],
    [
     "J.P.M. de",
     "Vreught"
    ],
    [
     "H.",
     "Koppelaar"
    ]
   ],
   "title": "An appreciation study of an ASR inquiry system",
   "original": "e97_1715",
   "page_count": 4,
   "order": 488,
   "p1": "1715",
   "pn": "1718",
   "abstract": [
    "Human factors play an important role in the applications of speech technology. Ina Wizard of Oz experiment, 64 telephonic inquiry systems were simulated by systematic manipulation of 6 human factors. To assess the impact of those factors on the appreciation, an appreciation scale was developed based on an questionnaire. In an experiment 414 respondents were requested to call one of the simulated systems and a system operated by human operators. The respondents rated these systems on an appreciation scale. In this paper a description of the experiment is given and the results of a statistical analysis of the appreciation scores is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-487"
  },
  "bensaber97_eurospeech": {
   "authors": [
    [
     "Kamel",
     "Bensaber"
    ],
    [
     "Paul",
     "Munteanu"
    ],
    [
     "Jean-Francois",
     "Serignat"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Object-oriented modeling of articulatory data for speech research information systems",
   "original": "e97_1719",
   "page_count": 4,
   "order": 489,
   "p1": "1719",
   "pn": "1722",
   "abstract": [
    "In this paper we present a general framework, based on the Object-oriented paradigm, for modeling and designing a model of speech data representation, and we propose a particular use of it for cineradiographic data, including sagittal views of the vocal tract, frontal pictures of the lips, and acoustic signals. We introduced semantics to represent relationships between speech objects. Thus we adopted the concepts of primary data, that means either the raw data (recorded signals and images) or their related descriptive data (information on speakers, corpora and recording conditions), and of derived data, such as vocal-tract's contours, sagittal distances, area functions, or any other possible measurements taken from X-rays pictures. Indeed, the notion of derived data model has been useful for users, to manage raw data and results of data analysis in the same way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-488"
  },
  "kim97e_eurospeech": {
   "authors": [
    [
     "Woosung",
     "Kim"
    ],
    [
     "Myoung-Wan",
     "Koo"
    ]
   ],
   "title": "A Korean speech corpus for train ticket reservation aid system based on speech recognition",
   "original": "e97_1723",
   "page_count": 4,
   "order": 490,
   "p1": "1723",
   "pn": "1726",
   "abstract": [
    "This paper describes the Korean speech corpus for train ticket reservation aid system based on speech recognition. Two sets of speech corpus were collected. One was based on human-human(H-H) dia- logues and the other was based on human-computer(H- C) dialogues. WOZ(Wizard of Oz) experiment was carried out to collect speech corpus based on H-C spoken dialogue. A total of 298 speaker data was col- lected for H-C corpus and a total of 100 speaker data was collected for H-H corpus. Since the basic unit of grammar in Korean is a morpheme, Korean-language model based on a morpheme was designed in addition to a word-based language model. Linguistic analysis results show that people respond differently when they are talking to a computer compared to when talking to a human. Also language-model analysis results reveal that a morpheme-based language model gives 50% reduction in perplexity(PP) over a word-based one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-489"
  },
  "dutton97_eurospeech": {
   "authors": [
    [
     "Dawn",
     "Dutton"
    ],
    [
     "Candace",
     "Kamm"
    ],
    [
     "Susan",
     "Boyce"
    ]
   ],
   "title": "Recall memory for earcons",
   "original": "e97_1727",
   "page_count": 4,
   "order": 491,
   "p1": "1727",
   "pn": "1730",
   "abstract": [
    "Our voice enabled telecommunications service, Annie, is a prototyping system that gives users the ability to access a variety of telephone-based services by voice. The user interface of Annie uses an anthropomorphic \"personal assistant\" metaphor. The user can maintain a \"conversation-like\" dialog with Annie, but user input is limited by the grammar-constrained automatic speech recognition (ASR) technology used in the service. Because the grammars change depending on the state the user is in, the system must provide clear recognition feedback and orienting information throughout the dialog. Verbal recognition feedback is tedious and time-consuming for the frequent, expert user. This paper describes an experiment that explores the feasibility of providing non-verbal recognition feedback and orienting information through the use of earcons, or auditory icons. Users of Annie were exposed to five earcons presented in parallel with verbal recognition feedback for a minimum of five days. Subsequently, users were asked to recall the identity of each of the five earcons alone. Subjects were able to reliably recall each of the earcons. Since users could recall the earcons, it is feasible that the non-verbal earcons could replace the lengthier verbal recognition feedback.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-490"
  },
  "mella97_eurospeech": {
   "authors": [
    [
     "O.",
     "Mella"
    ],
    [
     "D.",
     "Fohr"
    ]
   ],
   "title": "Semi-automatic phonetic labelling of large corpora",
   "original": "e97_1731",
   "page_count": 4,
   "order": 492,
   "p1": "1731",
   "pn": "1734",
   "abstract": [
    "The aim of the present paper is to present a methodology to semi-automatically label large corpora. This methodology is based on three main points: using several concurrent automatic stochastic labellers, decomposing the labelling of the whole corpus into an iterative refining process and building a labelling comparison procedure which takes into account phonologic and acoustic-phonetic rules to evaluate the similarity of the various labelling of one sentence. After having detailed these three points, we describe our HMM-based labelling tool and we describe the application of that methodology to the Swiss French POLYPHON database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-491"
  },
  "grocholewski97_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Grocholewski"
    ]
   ],
   "title": "CORPORA - speech database for Polish diphones",
   "original": "e97_1735",
   "page_count": 4,
   "order": 493,
   "p1": "1735",
   "pn": "1738",
   "abstract": [
    "In the paper the attempts for creating the first databases for Polish are presented. Among two databases, supported by The Polish National Research Committee, and COPERNICUS project (1304 \"BABEL: a Multi-Language Database\" for Polish, Bulgarian, Estonian, Hungarian, Romanian) the first of them is presented in detail. The speech material contains 365 utterances (alphabet letters, digits, 200 first names, 114 sentences) uttered by 45 speakers. In the paper the design ideas, recording conditions, annotation rules, the method of automatic segmentation and labelling used in CORPORA are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-492"
  },
  "muller97_eurospeech": {
   "authors": [
    [
     "Christel",
     "Müller"
    ],
    [
     "Thomas",
     "Ziem"
    ]
   ],
   "title": "Multilingual speech interfaces (MSI) and dialogue design environments for computer telephony services",
   "original": "e97_1739",
   "page_count": 4,
   "order": 494,
   "p1": "1739",
   "pn": "1742",
   "abstract": [
    "Today voice processing systems especially in the area of telecommunication are used by a wide range of customers in more and more countries. The most important facts for people who create applications based on voice processing systems are rapid prototyping, updating, modification and extension of dialogues as well as their standardised components. At Deutsche Telekom research lab of speech systems and computer telephony a graphical user interface for designing computer telephony applications was developed as part of a CTI (Computer Telephony Integration) model on top of standardised interfaces for speech and other CT components in a client server architecture. The approach to this CTI model ensures the independence of different resources e.g. ASR, TTS and other. The most complicated part of this CTI model was the realisation of the resource management layer for multilingual speech interfaces because different ASR technologies do not support unified recognition functions and resource parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-493"
  },
  "hansen97b_eurospeech": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Sahar E.",
     "Bou-Ghazale"
    ]
   ],
   "title": "Getting started with SUSAS: a speech under simulated and actual stress database",
   "original": "e97_1743",
   "page_count": 4,
   "order": 495,
   "p1": "1743",
   "pn": "1746",
   "abstract": [
    "It is well known that the introduction of acoustic background distortion and the variability resulting from environmentally induced stress causes speech recognition algorithms to fail. In this paper, we discuss SUSAS: a speech database collected for analysis and algorithm formulation of speech recognition in noise and stress. The SUSAS database refers to Speech Under Simulated and Actual Stress, and is intended to be employed in the study of how speech production and recognition varies when speaking during stressed conditions. This paper will discuss (i) the formulation of the SUSAS database, (ii) baseline speech recognition using SUSAS data, and (iii) previous research studies which have used the SUSAS data base. The motivation for this paper is to familiarize the speech community with SUSAS, which was released April 1997 on CD-ROM through the NATO RSG.10.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-494"
  },
  "taylor97_eurospeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Michael",
     "Tanenblatt"
    ],
    [
     "Amy",
     "Isard"
    ]
   ],
   "title": "A markup language for text-to-speech synthesis richard sproat",
   "original": "e97_1747",
   "page_count": 4,
   "order": 496,
   "p1": "1747",
   "pn": "1750",
   "abstract": [
    "Text-to-speech synthesizers must process text, and therefore require some knowledge of text structure. While many TTS systems allow for user control by means of ad hoc 'escape sequences', there remains to date no adequate and generally agreed upon system-independent standard for marking up text for the purposes of synthesis. The present paper is a collaborative effort between two speech groups aimed at producing such a standard, in the form of an SGML-based markup language that we call STML-Spoken Text Markup Language. The primary purpose of this paper is not to present STML as a fait accompli, but rather to interest other TTS research groups to collaborate and contribute to the development of this standard.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-495"
  },
  "itahashi97_eurospeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Naoko",
     "Ueda"
    ],
    [
     "Mikio",
     "Yamamoto"
    ]
   ],
   "title": "Several measures for selecting suitable speech CORPORA",
   "original": "e97_1751",
   "page_count": 4,
   "order": 497,
   "p1": "1751",
   "pn": "1754",
   "abstract": [
    "We make statistical investigations of various speech corpora to extract useful information re ecting the contents of the corpus so that we can create a sort of guidelines for selecting the most suitable corpus. A word is not separated by spaces in the Japanese text. Accordingly, we adopt n-gram counting methods to extract frequent mora sequences instead of words. A mora roughly corresponds to a syllable. By investigating the frequencies of 1 to 10-mora sequences in the existing six corpora, we can find the distinction between the written and the spoken languages, keywords and topics of dialogues. This paper shows that the simple statistical investigation makes it possible to represent the contents of the corpus to some extent without conducting a complicated job such as morphological analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-496"
  },
  "chatzi97_eurospeech": {
   "authors": [
    [
     "Irene",
     "Chatzi"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Greek speech database for creation of voice driven teleservices",
   "original": "e97_1755",
   "page_count": 4,
   "order": 498,
   "p1": "1755",
   "pn": "1758",
   "abstract": [
    "In this paper we present the collection of Greek speech data over the telephone network from 5;000 speakers in order to form a speech database (SpeechDatII.GR). This work is embedded in the Language Engineering Project LE2-4001 SpeechDat, in which all official European languages and some major dialectal variants are represented. The design of the speech database allows the development of word, phoneme and syllable based speech recognizers that can be used for a large variety of real speaker independent applications. In particular it will provide a realistic basis for training and assessment of both isolated and continuous speech recognizers for telephone speech, which is a prerequisite for developing voice driven teleservices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-497"
  },
  "huo97_eurospeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Combined on-line model adaptation and Bayesian predictive classification for robust speech recognition",
   "original": "e97_1847",
   "page_count": 4,
   "order": 499,
   "p1": "1847",
   "pn": "1850",
   "abstract": [
    "In this paper, we study a class of robust automatic speech recognition problem in which mismatches between training and testing conditions exist but an accurate knowledge of the mismatch mechanism is unknown. The only available information is the test data along with a set of pretrained speech models and the decision parameters. We try to compensate for the abovementioned mismatches by jointly adopting a dynamic system design strategy called on-line Bayesian adaptation to incrementally improve the estimation of the model parameters used in the recognizer, and a robust decision strategy called Bayesian predictive classification to average over the remaining uncertainty in model parameters. We report on a series of experimental results to show the viability and effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-498"
  },
  "aubert97_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Aubert"
    ],
    [
     "Eric",
     "Thelen"
    ]
   ],
   "title": "Speaker adaptive training applied to continuous mixture density modeling",
   "original": "e97_1851",
   "page_count": 4,
   "order": 500,
   "p1": "1851",
   "pn": "1854",
   "abstract": [
    "Speaker Adaptive Training (SAT) has been investigated for mixture density estimation and applied to large vocabulary continuous speech recognition. SAT integrates MLLR adaptation in the HMM training and aims at reducing inter-speaker variability to get enhanced speaker-independent models. Starting from BBN's work on compact models, we derive a one-pass Viterbi formulation of SAT that performs joint estimation of MLLR-based transformations and density parameters. The computational complexity is analyzed and an approximation based on using inverse affine transformations is discussed. Compared to applying MLLR on standard SI models, our experimental results achieve lower error rates as well as reduced decoding costs, for both supervised batch and unsupervised incremental adaptation. In the latter case, it is shown that the enrollment of a new speaker can be sped up by selecting among the transformations that were estimated from the training speakers, the one that best fits with the first test utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-499"
  },
  "illina97_eurospeech": {
   "authors": [
    [
     "Irina",
     "Illina"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Speaker normalization training for mixture stochastic trajectory model",
   "original": "e97_1855",
   "page_count": 4,
   "order": 501,
   "p1": "1855",
   "pn": "1858",
   "abstract": [
    "In this paper we are interested in speaker and environment adaptation techniques for speaker independent (SI) continuous speech recognition. These techniques are used to reduce mismatch between training and the testing conditions, using a small amount of adaptation data. In addition to reducing this mismatch during the adaptation, we propose to reduce the variation due to speakers or environments during the training itself in the context of Speaker Normalisation (SN) approach, using MLLR transformation. SN also includes a combination of the context-dependent, phone dependent and broad phonetic class dependent information. The use of linear regression to model broad phonetic class dependent information assures our model to be used in the case that the adaptation data or training data is not given for some phonetic symbols. SN is developed for Mixture Stochastic Trajectory Model, a segment based model. The approach can be used for speaker, gender or environment normalization. We show the performance of SN compared to SI recognition and to MLLR speaker adaptation, through experiments on continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-500"
  },
  "digalakis97_eurospeech": {
   "authors": [
    [
     "V.",
     "Digalakis"
    ]
   ],
   "title": "On-line adaptation of hidden Markov models using incremental estimation algorithms",
   "original": "e97_1859",
   "page_count": 4,
   "order": 502,
   "p1": "1859",
   "pn": "1862",
   "abstract": [
    "The mismatch that frequently occurs between the training and testing conditions of an automatic speech recognizer can be efficiently reduced by adapting the parameters of the recognizer to the testing conditions. The maximum likelihood adaptation algorithms for continuous -density hidden-Markov-model (HMM) based speech recognizers are fast, in the sense that a small amount of data is required for adaptation. They are, however, based on reestimating the model parameters using the batch version of the expectation-maximization (EM) algorithm. The multiple iterations required for the EM algorithm to converge make these adaptation schemes computationally expensive and not suitable for on-line applications, since multiple passes through the adaptation data are required. In this paper we show how incremental versions of the EM and the segmental k- means algorithm can be used to improve the convergence of these adaptation methods so that they can be used in on-line applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-501"
  },
  "kannan97_eurospeech": {
   "authors": [
    [
     "Ashvin",
     "Kannan"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Modeling dependency in adaptation of acoustic models using multiscale tree processes",
   "original": "e97_1863",
   "page_count": 4,
   "order": 503,
   "p1": "1863",
   "pn": "1866",
   "abstract": [
    "To adapt the large number of parameters in a speech recognition acoustic model with a small amount of data, some notion of parameter dependence is needed. We present a dependence model to relate parameters in a parsimonious framework using a Gaussian multiscale process defined by the evolution of a linear stochastic dynamical system on a tree. To adapt all classes from all adaptation data, we formulate adaptation as optimal smoothing of the tree process. This approach is used to adapt two types of models: Gaussians, and Gaussian processes (segment models) characterized by a polynomial mean trajectory. Recognition results presented on the Switchboard corpus show improvements in supervised and unsupervised modes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-502"
  },
  "heck97_eurospeech": {
   "authors": [
    [
     "Larry",
     "Heck"
    ],
    [
     "Ananth",
     "Sankar"
    ]
   ],
   "title": "Acoustic clustering and adaptation for robust speech recognition",
   "original": "e97_1867",
   "page_count": 4,
   "order": 504,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "We describe an algorithm based on acoustic clustering and acoustic adaptation to significantly improve speech recognition performance. The method is particularly useful when speech from multiple speakers is to be recognized and the boundary between speakers is not known. We assume that each test data segment is relatively homogeneous with respect to the acoustic background and speaker. These segments are then grouped using an agglomerative acoustic clustering algorithm. The idea is to group together all test segments that are acoustically similar. The speech recognition models are then adapted separately to each test data cluster. Finally these adapted models are used to recognize the data from that cluster. This algorithm was used in SRI's system for the 1996 DARPA Hub4 partitioned evaluation. Experimental results are presented on the 1996 H4 development data set. It was found that an improvement of 9.5% was achieved by using this algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-503"
  },
  "martin97b_eurospeech": {
   "authors": [
    [
     "Alvin",
     "Martin"
    ],
    [
     "George",
     "Doddington"
    ],
    [
     "Terri",
     "Kamm"
    ],
    [
     "Mark",
     "Ordowski"
    ],
    [
     "Mark",
     "Przybocki"
    ]
   ],
   "title": "The DET curve in assessment of detection task performance",
   "original": "e97_1895",
   "page_count": 4,
   "order": 505,
   "p1": "1895",
   "pn": "1898",
   "abstract": [
    "We introduce the DET Curve as a means of representing performance on detection tasks that involve a tradeoff of error types. We discuss why we prefer it to the traditional ROC Curve and offer several examples of its use in speaker recognition and language recognition. We explain why it is likely to produce approximately linear curves. We also note special points that may be included on these curves, how they are used with multiple targets, and possible further applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-504"
  },
  "klaus97_eurospeech": {
   "authors": [
    [
     "Harald",
     "Klaus"
    ],
    [
     "Ekkehard",
     "Diedrich"
    ],
    [
     "Astrid",
     "Dehnel"
    ],
    [
     "Jens",
     "Berger"
    ]
   ],
   "title": "Speech quality evaluation of hands-free terminals",
   "original": "e97_1899",
   "page_count": 4,
   "order": 506,
   "p1": "1899",
   "pn": "1902",
   "abstract": [
    "This paper describes a new methodology for the speech quality assessment of hands-free terminals and discusses the results of a pilot study performed in 1996 at the Berlin laboratories for speech quality assessment at the Technology Centre of Deutsche Telekom. Up to now, critical speech quality aspects of hands-free terminals are usually assessed with conversational tests. With the test method proposed here, much more efficient listening only tests can be applied to evaluate various speech quality aspects of hands-free terminals. In the pilot study, a series of conversational tests, specific double talk tests and listening only experiments were performed. The paper descibes the recording environment and equipment, the auditory test methodology and the results of the listening only experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-505"
  },
  "pallett97_eurospeech": {
   "authors": [
    [
     "David S.",
     "Pallett"
    ],
    [
     "Jonathan G.",
     "Fiscus"
    ],
    [
     "William M.",
     "Fisher"
    ],
    [
     "John S.",
     "Garofolo"
    ]
   ],
   "title": "Use of broadcast news materials for speech recognition benchmark tests",
   "original": "e97_1903",
   "page_count": 4,
   "order": 507,
   "p1": "1903",
   "pn": "1906",
   "abstract": [
    "This paper reports on the use of materials derived from radio and television news broadcasts for research and testing purposes for large vocabulary Continuous Speech Recognition (CSR) technology. Tests using these materials have been implemented by NIST on behalf of the DARPA-funded speech recognition research community in 1995 and 1996, and are expected to continue for the next several years. Four research groups participated in the 1995 tests, and nine groups (at eight sites) participated in the 1996 tests. This paper documents properties of the training and test materials, describes a detailed annotation and transcription protocol that has been used for more than 100 hours of recorded data that has been made available through the Linguistic Data Consortium (LDC), and discusses test protocols and results of both the 1995 and 1996 Benchmark Tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-506"
  },
  "fraser97_eurospeech": {
   "authors": [
    [
     "Norman M.",
     "Fraser"
    ]
   ],
   "title": "Spoken dialogue system evaluation: a first framework for reporting results",
   "original": "e97_1907",
   "page_count": 4,
   "order": 508,
   "p1": "1907",
   "pn": "1910",
   "abstract": [
    "There are no agreed standards for reporting the performance of spoken dialogue systems. This paper proposes a core set of metrics to be used for this purpose. For this set, operational definitions are supplied, to regularise their application. The intention in proposing this framework is not that it should be exhaustive, nor that it should be perfect, but rather that it should provide a practical starting point, thereby allowing initial system comparison to be achieved quickly and with some measure of confidence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-507"
  },
  "bernsen97_eurospeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Hans",
     "Dybkjaer"
    ],
    [
     "Laila",
     "Dybkjaer"
    ],
    [
     "Vytautas",
     "Zinkevicius"
    ]
   ],
   "title": "Generality and transferability. two issues in putting a dialogue evaluation tool into practical use",
   "original": "e97_1911",
   "page_count": 4,
   "order": 509,
   "p1": "1911",
   "pn": "1914",
   "abstract": [
    "This paper presents a first set of test results on the generality and transferability of an evaluation tool which can ensure the habitability and usability of spoken dialogues. Building on the assumption that most, if not all, dialogue design errors can be viewed as problems of non-cooperative system behaviour, the tool has two closely related aspects to its use. Firstly, it may be used for the diagnostic evaluation of spoken human-machine dialogue. Secondly, it can be used to guide early dialogue design in order to prevent dialogue design errors from occurring in the implemented system. We describe the development and in-house testing of the tool, and present results of ongoing work on testing its generality and transferability on an external corpus, i.e. an early Wizard of Oz corpus from the development of the Sundial spoken language dialogue system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-508"
  },
  "leeuwen97_eurospeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "Within-speaker variability of the word error rate for a continuous speech recognition system",
   "original": "e97_1915",
   "page_count": 4,
   "order": 510,
   "p1": "1915",
   "pn": "1918",
   "abstract": [
    "The variance of the performance of a continuous speech recognition system subjected to replica utterances of the same sentence spoken by the same speaker has been investigated. In an experiment with three different speech recognition systems in three different languages with two different grammar conditions it is shown that the sentence word error rate has a variance that can be described in terms of binomial statistics. The distribution of the measured variance shows a remarkable correspondence to the parameter-free theoretical distribution. It is therefore concluded that for the word error rate of a continuous speech recognition system binomial statistics apply.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-509"
  },
  "huckvale97_eurospeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Christian",
     "Benoit"
    ],
    [
     "C.",
     "Bowerman"
    ],
    [
     "Anders",
     "Eriksson"
    ],
    [
     "M.",
     "Rosner"
    ],
    [
     "M.",
     "Tatham"
    ],
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "Opportunities for computer-aided instruction in phonetics and speech communication provided by the internet",
   "original": "e97_1919",
   "page_count": 4,
   "order": 511,
   "p1": "1919",
   "pn": "1922",
   "abstract": [
    "Spoken language engineering is starting to deliver technological products to the commercial market and has an important future role in supporting the multilingual structures of modern Europe. The field will be driven forward by basic science and applied research by experts drawn from a variety of backgrounds; among them: linguistics, psychology, computer science and electrical engineering. The wide range of expertise required in this discipline brings difficulties for our educational systems, but also challenges us to use our knowledge of technology and communication to improve the quality and effectiveness of teaching and learning. This paper investigates how resources currently available on the Internet could be exploited in the education of phonetics and speech communication. It discusses the technology, outlines the requirements for computer-aided learning in the field, gives a taxonomy of the available components with examples, and criticises the main weaknesses in the current provision.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-510"
  },
  "bloothooft97_eurospeech": {
   "authors": [
    [
     "Gerrit",
     "Bloothooft"
    ]
   ],
   "title": "The landscape of future education in speech communication sciences",
   "original": "e97_1923",
   "page_count": 4,
   "order": 512,
   "p1": "1923",
   "pn": "1926",
   "abstract": [
    "After many years of successful cooperation in Europe on education in Phonetics and Speech Communication, in 1996 a new thematic network was created with the aim to reflect on future developments in education in Speech Communication Sciences. The network constitutes of 80 European academic institutions and has the support of major international organisations. The activities of the thematic network will be presented followed by a short overview of the results obtained in the first year. Forthcoming actions are proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-511"
  },
  "sjolander97_eurospeech": {
   "authors": [
    [
     "Kare",
     "Sjölander"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "An integrated system for teaching spoken dialogue systems technology",
   "original": "e97_1927",
   "page_count": 4,
   "order": 513,
   "p1": "1927",
   "pn": "1930",
   "abstract": [
    "Spoken language systems are highly complex and teaching of students in this subject matter and in the underlying technologies could benefit greatly from instructional software. The aim of this work has been to give students hands-on experience via a fully functioning spoken dialogue system as a teaching aid. This dialogue system was built using our toolkit for spoken language technology as a dedicated laboratory environment for students. The system was used in a lab which was part of two courses on spoken language technology given by our department. Students were given some initial guidance on how to modify and extend the system but most of their work was unsupervised. Overall, the laboratory system was a success and we plan to improve and extend it for the coming academic year. Thanks to the rapid prototyping and development character of our toolkit we might even use it and the modules from the system as a software basis for student projects in spoken language technology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-512"
  },
  "beck97_eurospeech": {
   "authors": [
    [
     "Janet",
     "Beck"
    ],
    [
     "Bernard",
     "Camilleri"
    ],
    [
     "Hilde",
     "Chantrain"
    ],
    [
     "Anu",
     "Klippi"
    ],
    [
     "Marianne",
     "Leterme"
    ],
    [
     "Matti",
     "Lehtihalmes"
    ],
    [
     "PeterSchneider",
     "PeterSchneider"
    ],
    [
     "Wilhelm",
     "Vieregge"
    ],
    [
     "Eva",
     "Wigforss"
    ]
   ],
   "title": "Communication science within education for logopedics/speech and language therapy in europe: the state of the art",
   "original": "e97_1931",
   "page_count": 4,
   "order": 514,
   "p1": "1931",
   "pn": "1934",
   "abstract": [
    "The paper presents the main issues from the working group on on Speech and Language Therapy (SLT) within the SOCRATES/ ERASMUS Thematic Network \"Speech Communication Science\". SLT is today a profession which has undergone a dramatic evolution in recent decades, as developments in clinical skills have been integrated with an increasingly high level of academic and research expertise. The short term aim of the working group has therefore been to evaluate current practice in education in communication science for speech and language therapists throughout Europe. A questionnaire was designed with the aim of eliciting some factual information about speech commmunication science education for SLT, as well as attitudes about the role of this area within the broader context of SLT education. Responses were received from institutions representing eight countries, but the response rate has so far been very poor. This has reinforced the aim of developing internet connections, both within the profession and as an interface with other related disciplines. A web site, developed and supported at Lund University, provides basic information about all SLT education courses within Europe. The working group has also decided to adopt a new strategy for the second year's work within the Network. Data will be collected and summerized in a multilingual style through the selection of a responsible persons from each European country. There will be a report of these activities in the beginning of January 1998.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-513"
  },
  "green97_eurospeech": {
   "authors": [
    [
     "Phil",
     "Green"
    ],
    [
     "Carlos",
     "Espain"
    ]
   ],
   "title": "Education in spoken language engineering in europe",
   "original": "e97_1935",
   "page_count": 4,
   "order": 515,
   "p1": "1935",
   "pn": "1938",
   "abstract": [
    "We summarise the results of a survey of Spoken Language Engineering Education in Europe. We highlight the multidisciplinary nature of the field and the consequences this has for teaching. The survey indicates a wide variety in the breadth and depth of coverage of SLE.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-514"
  },
  "hazan97_eurospeech": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Wim van",
     "Dommelen"
    ]
   ],
   "title": "A survey of phonetics education in Europe",
   "original": "e97_1939",
   "page_count": 4,
   "order": 516,
   "p1": "1939",
   "pn": "1942",
   "abstract": [
    "A survey has been carried out to obtain an overview of phonetics education in Europe. A mainly web-based questionnaire was used to collect both quantitative and qualitative information. Responses were obtained from 78 institutes in 25 European countries. This paper summarises some of the findings of the survey.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-515"
  },
  "tu97_eurospeech": {
   "authors": [
    [
     "Xin",
     "Tu"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "Matching training and testing criteria in hybrid speech recognition systems",
   "original": "e97_1943",
   "page_count": 4,
   "order": 517,
   "p1": "1943",
   "pn": "1946",
   "abstract": [
    "Inconsistency between training and testing criteria is a drawback of the hybrid artifcial neural network and hidden Markov model (ANN/HMM) approach to speech recognition. This paper presents an effective method to address this problem by modifying the feedforward neural network training paradigm. Word errors are explicitly incorporated in the training procedure to achieve improved word recognition accuracy. Experiments on a continuous digit database show a reduction in word error rate of more than 17% using the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-516"
  },
  "dupont97b_eurospeech": {
   "authors": [
    [
     "Stephane",
     "Dupont"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Olivier",
     "Deroo"
    ],
    [
     "Vincent",
     "Fontaine"
    ],
    [
     "Jean-Marc",
     "Boite"
    ],
    [
     "L.",
     "Zanoni"
    ]
   ],
   "title": "Context independent and context dependent hybrid HMM/ANN systems for vocabulary independent tasks",
   "original": "e97_1947",
   "page_count": 4,
   "order": 518,
   "p1": "1947",
   "pn": "1950",
   "abstract": [
    "In this paper, hybrid HMM/ANN systems are used to model context dependent phones. In order to reduce the number of parameters as well as to better catch the dynamics of the phonetic segments, we combine (context dependent) diphone models with context independent phone models. Transitions from phone to phone are modeled as generalized context dependent distributions while phonetic units are context independent models trained on the less coarticulated middle part of each phone. Words are thus modeled as a sequence of probability distributions alternatively representing the middle part of the phonemes and the transitions from phone to phone. A single neural network is used to estimate both context independent phone probabilities and generalized context dependent diphone (phone to phone transition) probabilities. Resulting systems are compared to classical context independent phone-based HMM/ANN systems with the same number of parameters. The Phonebook isolated word database has been used for training the systems. Testing is done on small (75 words), medium (600 words) and large (8000 words) lexicons. Test words were not present in the training vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-517"
  },
  "hennebert97_eurospeech": {
   "authors": [
    [
     "J.",
     "Hennebert"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Hervè",
     "Bourlard"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Estimation of global posteriors and forward-backward training of hybrid HMM/ANN systems",
   "original": "e97_1951",
   "page_count": 4,
   "order": 519,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "The results of our research presented in this paper are two-fold. First, an estimation of global posteriors is formalized in the framework of hybrid HMM/ANN systems. It is shown that hybrid HMM/ANN systems, in which the ANN part estimates local posteriors, can be used to modelize global model posteriors. This formalization provides us with a clear theory in which both REMAP and \"classical\" Viterbi trained hybrid systems are uni_ed. Second, a new forward- backward training of hybrid HMM/ANN systems is derived from the previous formulation. Comparisons of performance between Viterbi and forward- back- ward hybrid systems are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-518"
  },
  "williams97_eurospeech": {
   "authors": [
    [
     "Gethin",
     "Williams"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Confidence measures for hybrid HMM/ANN speech recognition",
   "original": "e97_1955",
   "page_count": 4,
   "order": 520,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "In this paper we introduce four acoustic confidence measures which are derived from the output of a hybrid HMM/ANN large vocabulary continuous speech recognition system. These confidence measures, based on local posterior probability estimates computed by an ANN, are evaluated at both phone and word levels, using the North American Business News corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-519"
  },
  "cook97_eurospeech": {
   "authors": [
    [
     "Gary D.",
     "Cook"
    ],
    [
     "Steve R.",
     "Waterhouse"
    ],
    [
     "A.J.",
     "Robinson"
    ]
   ],
   "title": "Ensemble methods for connectionist acoustic modelling",
   "original": "e97_1959",
   "page_count": 4,
   "order": 521,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "In this paper we investigate a number of ensemble methods for improving the performance of connectionist acoustic models for large vocabulary continuous speech recognition. We discuss boosting, a data selection technique which results in an ensemble of models, and mixtures-of-experts. These techniques have been applied to multi-layer perceptron acoustic models used to build a hybrid connectionist-HMM speech recognition system. We present results on a number of ARPA benchmark tasks, and show that the ensemble methods lead to considerable improvements in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-520"
  },
  "fritsch97_eurospeech": {
   "authors": [
    [
     "Jürgen",
     "Fritsch"
    ],
    [
     "Michael",
     "Finke"
    ]
   ],
   "title": "Improving performance on switchboard by combining hybrid HME/HMM and mixture of Gaussians acoustic models",
   "original": "e97_1963",
   "page_count": 4,
   "order": 522,
   "p1": "1963",
   "pn": "1966",
   "abstract": [
    "This paper presents results of our efforts on combining standard mixture of Gaussians acoustic modeling [10] with a context-dependent hybrid connectionist HME/HMM architecture [3, 4] for the Switchboard corpus. Using a score normalization scheme which is independent of the stream's modeling paradigm and adaptive methods for combining multiple probability distributions, we achieve a relative decrease in word error rate of 3.5% and 9.3%, compared to each of the single stream systems. As opposed to multiple acoustic streams based on mixture of Gaussians, the integration of hybrid NN/HMM based modeling appears to be advantageous since the differences in modeling techniques and training algorithms allow to capture different aspects of the speech signal. Small dependence among emission probability estimates is considered essential for potential gains in interpolated systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-521"
  },
  "witschel97_eurospeech": {
   "authors": [
    [
     "Petra",
     "Witschel"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "Experiments in adaptation of language models for commercial applications",
   "original": "e97_1967",
   "page_count": 4,
   "order": 523,
   "p1": "1967",
   "pn": "1970",
   "abstract": [
    "To improve recognition accuracy for large vocabulary speech recognition systems we use language models based on linguistic classes (extended POS). In this paper an adaptation technique is presented, which profits from linguistic knowledge about unknown words of new domain. Switching from basis domain to new domain we keep the bigram probabilities of linguistic classes fixed and adapt only monograms of word probabilities. In our experiments we use three different corpora: financial columns of a newspaper corpus and two medical corpora (computer tomography and magnetic resonance). Adapted language models show an improvement of test-set perplexity of 48% to 51% compared to the case of putting unknown words into the language model \"unknown\" class.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-522"
  },
  "kneser97_eurospeech": {
   "authors": [
    [
     "Reinhard",
     "Kneser"
    ],
    [
     "Jochen",
     "Peters"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Language model adaptation using dynamic marginals",
   "original": "e97_1971",
   "page_count": 4,
   "order": 524,
   "p1": "1971",
   "pn": "1974",
   "abstract": [
    "A new method is presented to quickly adapt a given language model to local text characteristics. The basic approach is to choose the adaptive models as close as possible to the background estimates while constraining them to respect the locally estimated unigram probabilities. Several means are investigated to speed up the calculations. We measure both perplexity and word error rate to gauge the quality of our model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-523"
  },
  "iyer97_eurospeech": {
   "authors": [
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Transforming out-of-domain estimates to improve in-domain language models",
   "original": "e97_1975",
   "page_count": 4,
   "order": 525,
   "p1": "1975",
   "pn": "1978",
   "abstract": [
    "Standard statistical language modeling techniques suffer from sparse-data problems when applied to real tasks in speech recognition, where large amounts of domain-dependent text are not available. In this work, we introduce a modified representation of the standard word n-gram model using part-of-speech (POS) labels that compensates for word and POS usage differences across domains. Two different approaches are explored: (i) imposing an explicit transformation of the out-of-domain n-gram distributions before combining with an in-domain model, and (ii) POS smoothing of multi-domain n-gram components. Results are presented on a spontaneous speech recognition task (Switchboard), showing that the POS smoothing framework reduces word error rate and perplexity over a standard word n-gram model on in-domain data, with increased gains using multi-domain models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-524"
  },
  "rao97_eurospeech": {
   "authors": [
    [
     "P. Srinivasa",
     "Rao"
    ],
    [
     "Satya",
     "Dharanipragada"
    ],
    [
     "Salim",
     "Roukos"
    ]
   ],
   "title": "MDI adaptation of language models across corpora",
   "original": "e97_1979",
   "page_count": 4,
   "order": 526,
   "p1": "1979",
   "pn": "1982",
   "abstract": [
    "The amount of text data available from a corpus for training language models is usually limited. Data from larger general or related corpora can be utilized to improve the performance of the language model on the corpus of interest. We explore one method of adapting a prior model from a large corpus to a smaller one of interest. Perplexity results of adapting a prior model constructed using the NAB corpus to the Switchboard and ATIS corpora are presented and compared with those of interpolated models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-525"
  },
  "ries97_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Ries"
    ]
   ],
   "title": "A class based approach to domain adaptation and constraint integration for empirical m-gram models",
   "original": "e97_1983",
   "page_count": 4,
   "order": 527,
   "p1": "1983",
   "pn": "1986",
   "abstract": [
    "The first class based adaptation approaches [FGH + 97, Ueb97] take the use of classes in the construction of statistical m-gram models one significant step further than just using them as a smoothing technique: The m-gram of classes is trained on the large background corpus while the word likelihoods given the class are estimated on the small target corpus. To make full use of this technique a specialized clusteralgorithm has been developed [FGH + 97, Ueb97]. In this paper we extend class adaptation to make use of the m-gram distribution of the target domain. As a second independent contribution this paper introduces an efficient morphing algorithm, that tries to achieve adaptation by using a stochastic mapping of words between the vocabularies of the respective domains. As a result we can show, that for small adaptation steps class based adaptation is a very useful technique. For larger adaptation steps the perplexity of the modified model is greatly improved, yet no improvement over the unadapted model was observed when used in linear interpolation. Whether this is due to the fact that we use class based adaptation or that we do just modify the unigram distribution is still unresolved, although the new stochastic mapping technique might help to give an answer to this question in the future.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-526"
  },
  "seymore97_eurospeech": {
   "authors": [
    [
     "Kristie",
     "Seymore"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Using story topics for language model adaptation",
   "original": "e97_1987",
   "page_count": 5,
   "order": 528,
   "p1": "1987",
   "pn": "1990",
   "abstract": [
    "The subject matter of any conversation or document can typically be described as some combination of elemental topics. We have developed a language model adaptation scheme that takes a piece of text, chooses the most similar topic clusters from a set of over 5000 elemental topics, and uses topic specific language models built from the topic clusters to rescore N-best lists. We are able to achieve a 15% reduction in perplexity and a small improvement in WER by using this adaptation. We also investigate the use of a topic tree, where the amount of training data for a specific topic can be judiciously increased in cases where the elemental topic cluster has too few word tokens to build a reliably smoothed and representative language model. Our system is able to fine-tune topic adaptation by interpolating models chosen from thousands of topics, allowing for adaptation to unique, previously unseen combinations of subjects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-527"
  },
  "luettin97_eurospeech": {
   "authors": [
    [
     "Juergen",
     "Luettin"
    ]
   ],
   "title": "Towards speaker independent continuous speechreading",
   "original": "e97_1991",
   "page_count": 4,
   "order": 529,
   "p1": "1991",
   "pn": "1994",
   "abstract": [
    "This paper describes recent speechreading experiments for a speaker independent continuous digit recognition task. Visual feature extraction is performed by a lip tracker which recovers information about the lip shape and information about the grey- level intensity around the mouth. These features are used to train visual word models using continuous density HMMs. Results show that the method generalises well to new speakers and that the recognition rate is highly variable across digits as expected due to the high visual confusability of certain words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-528"
  },
  "goldenthal97_eurospeech": {
   "authors": [
    [
     "William",
     "Goldenthal"
    ],
    [
     "Keith",
     "Waters"
    ],
    [
     "Jean-Manuel Van",
     "Thong"
    ],
    [
     "Oren",
     "Glickman"
    ]
   ],
   "title": "Driving synthetic mouth gestures: phonetic recognition for faceme!",
   "original": "e97_1995",
   "page_count": 4,
   "order": 530,
   "p1": "1995",
   "pn": "1998",
   "abstract": [
    "The goal of this work is to use phonetic recognition to drive a synthetic image with speech. Phonetic units are identified by the phonetic recognition engine and mapped to mouth gestures, known as visemes, the visual counter-part of phonemes. The acoustic waveform and visemes are then sent to a synthetic image player, called FaceMe! where they are rendered synchronously. This paper provides background for the core technologies involved in this process and describes asynchronous and synchronous prototypes of a combined phonetic recognition/FaceMe! system which we use to render mouth gestures on an animated face.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-529"
  },
  "rogozan97_eurospeech": {
   "authors": [
    [
     "Alexandrina",
     "Rogozan"
    ],
    [
     "Paul",
     "Deleglise"
    ]
   ],
   "title": "Continuous visual speech recognition using geometric lip-shape models and neural networks",
   "original": "e97_1999",
   "page_count": 4,
   "order": 531,
   "p1": "1999",
   "pn": "2002",
   "abstract": [
    "This paper describes a new approach for automatic speechreading. First, we use efficient, but effective representation of visible speech: a geometric lip-shape model. Then we present an automatic objective method to merge phonemes that appear visually similar into visemes for our speaker. In order to determine visemes, we trained SOM using the Kohonen algorithm on each phoneme extracted from our visual database. We go into the presentation of our visual speech recognition systems based on heuristics and neural networks (TDNN or JNN) trained to discriminate visual information. On a continuous spelling task, visual-alone recognition performance of about 37 % was achieved using the TDNN and about 33 % using the JNN one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-530"
  },
  "beskow97b_eurospeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Martin",
     "Dahlquist"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Magnus",
     "Lundeberg"
    ],
    [
     "Karl-Erik",
     "Spens"
    ],
    [
     "Tobias",
     "Öhman"
    ]
   ],
   "title": "The teleface project multi-modal speech-communication for the hearing impaired",
   "original": "e97_2003",
   "page_count": 4,
   "order": 532,
   "p1": "2003",
   "pn": "2006",
   "abstract": [
    "The Teleface Project, a project that aims at evaluating the possibilities for a telephone communication aid for hard of hearing persons, is presented as well as the different parts of the project: audio-visual speech synthesis, visual speech measurement and multimodal speech intelligibility studies. The experiments showed a noticeable intelligibility advantage for the addition of the face information, both for natural and synthetic faces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-531"
  },
  "stiefelhagen97_eurospeech": {
   "authors": [
    [
     "Rainer",
     "Stiefelhagen"
    ],
    [
     "Uwe",
     "Meier"
    ],
    [
     "Jie",
     "Yang"
    ]
   ],
   "title": "Real-time lip-tracking for lipreading",
   "original": "e97_2007",
   "page_count": 4,
   "order": 533,
   "p1": "2007",
   "pn": "2010",
   "abstract": [
    "This paper presents a new approach to lip tracking for lipreading. Instead of only tracking features on lips, we propose to track lips along with other facial features such as pupils and nostril. In the new approach, the face is first located in an image using a stochastic skin-color model, the eyes, lip-corners and nostrils are then located and tracked inside the facial region. The new approach can effectively improve the robustness of lip-tracking and simplify automatic detection and recovery of tracking failure. The feasibility of the proposed approach has been demonstrated by implementation of a lip tracking sys- tem. The system has been tested by a database that contains 900 image sequences of different speakers spelling words. The system has successfully extract lip regions from the image sequences to obtain training data for the audio-visual speech recognition system. The system has been also applied to extract the lip region in real-time from live video images to obtain the visual input for an audio-visual speech recognition system. On test sequences we have achieved a reduction of the number of frames with tracking failures by a factor of two using detection and prediction of outliers in the set of found features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-532"
  },
  "reveret97b_eurospeech": {
   "authors": [
    [
     "Lionel",
     "Reveret"
    ]
   ],
   "title": "From raw images of the lips to articulatory parameters: a viseme-based prediction",
   "original": "e97_2011",
   "page_count": 4,
   "order": 534,
   "p1": "2011",
   "pn": "2014",
   "abstract": [
    "This paper presents a method for the extraction of articulatory parameters from direct processing of raw images of the lips. The system architecture is made of three independent parts. First, a new greyscale mouth image is centred and downsampled. Second, the image is aligned and projected onto a basis of artificial images. These images are the eigenvectors computed from a PCA applied on a set of 23 reference lip shapes. Then, a multilinear interpolation predicts articulatory parameters from the image projection coefficients onto the eigenvectors. In addition, the projection coefficients and the predicted parameters were evaluated by an HMM-based visual speech recogniser. Recognition scores obtained with our method are compared to reference scores and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-533"
  },
  "mathieu97_eurospeech": {
   "authors": [
    [
     "Bruno",
     "Mathieu"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Adaptation of Maeda's model for acoustic to articulatory inversion",
   "original": "e97_2015",
   "page_count": 4,
   "order": 535,
   "p1": "2015",
   "pn": "2018",
   "abstract": [
    "We are working on performing acoustic to articulatory inversion by using Maeda's model. The purpose of this work is to adapt the model to a new speaker. The adaptation quality is assessed by verifying that vowels uttered by the speaker lie inside the vocalic space defined by the model. It is with this aim in view that we realized a series of MR images for eleven oral French vowels (/i, e, , y, , oe, a, , , o, u/). The adaptation may include modifications of: scale factors for the pharynx and the mouth cavity, the wall of the vocal tract and coefficients for the calculation of the area function from a sagittal shape. The scale factors have been determined by superimposing Maeda's model on the MR images. The wall has been obtained by calculating the mean value of the exterior contours of the vocal tract in the image series. As some discrepancies between natural and synthetic vowels remained the wall contour has been iteratively optimized by means of formant sensibility functions calculated for each section in the vocal tract. The inversion is carried out by means of a table-lookup procedure constrained by the smoothness of articulatory trajectories.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-534"
  },
  "payan97_eurospeech": {
   "authors": [
    [
     "Yohan",
     "Payan"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Why should speech control studies based on kinematics be considered with caution? insights from a 2d biomechanical model of the tongue.",
   "original": "e97_2019",
   "page_count": 5,
   "order": 536,
   "p1": "2019",
   "pn": "2022",
   "abstract": [
    "A 2D biomechanical model of the tongue is used to simulate movement sequences and speech signals in Vowel-to-Vowel transitions. The analysis is focused on how central commands and biomechanics can interact and influence the physical speech signals. In particular, it is shown how complex velocity profiles can be explained by the biomechanics, and how the low-pass filtering effect of the biomechanics can give an account of the vocalic reduction phenomenon that is observed during speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-535"
  },
  "sanguineti97_eurospeech": {
   "authors": [
    [
     "Vittorio",
     "Sanguineti"
    ],
    [
     "Rafael",
     "Laboissiere"
    ],
    [
     "David J.",
     "Ostry"
    ]
   ],
   "title": "An integrated model of the biomechanics and neural control of the tongue, jaw, hyoid and larynx system",
   "original": "e97_2023",
   "page_count": 4,
   "order": 537,
   "p1": "2023",
   "pn": "2026",
   "abstract": [
    "A model of the sagittal plane motion of the tongue, jaw, hyoid bone and larynx is presented, based on the version of equilibrium point hypothesis. The focus is on the organization of control signals underlying vocal tract motions. A number of muscle synergies or 'basic motions' of the system are identified. It is shown that systematic sources of variation in a X-ray data base of midsagittal vocal tract motions can be accounted for with six independent commands, each corresponding to a direction of articulator motion. It is further shown that hyoid position and orientation can be predicted from the application of other vocal tract commands and need not be explicitly controlled. The dynamics of individual commands are also assessed. It is shown that the dynamic effects are not neglectable in speech-like movements because of the different dynamic behavior of soft and bony structures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-536"
  },
  "mohammad97_eurospeech": {
   "authors": [
    [
     "M.",
     "Mohammad"
    ],
    [
     "E.",
     "Moore"
    ],
    [
     "J.N.",
     "Carter"
    ],
    [
     "C.H.",
     "Shadle"
    ],
    [
     "S.J.",
     "Gunn"
    ]
   ],
   "title": "Using MRI to image the moving vocal tract during speech",
   "original": "e97_2027",
   "page_count": 4,
   "order": 538,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "Magnetic Resonance Imaging (MRI) has been used to measure the shape of the vocal tract during speech in several recent studies. Its safety to the subject, high quality imaging of soft tissue, and the ability to select relatively thin imaging planes at any angle are significant advantages over other imaging methods used for speech research. The most significant disadvantage is the long exposure time. As a result most studies have focused on obtaining high-resolution images of the vocal tract Volume for static sounds, such as vowels [1], fricatives [5, 6], nasals, the closed phase of plosives [7] and liquids [3,7]. In this paper we will describe our method of obtaining MR images of a moving vocal tract in which we post-synchronise the MR data using a recorded speech signal and thus reconstruct the images without using the MR machine's built-in processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-537"
  },
  "vatikiotisbateson97_eurospeech": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Hani",
     "Yehia"
    ]
   ],
   "title": "Unified physiological model of audible-visible speech production",
   "original": "e97_2031",
   "page_count": 4,
   "order": 539,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "In this paper, vocal tract and orofacial motions are measured during speech production in order to demonstrate that vocal tract motion can be used to estimate its orofacial counterpart. The inversion, i.e. vocal tract behavior estimation from orofacial motion, is also possible, but to a smaller extent. The numerical results showed that vocal tract motion accounted for 96% of the total variance observed in the joint system, whereas orofacial motion accounted for 77%. This analysis is part of a wider study where a dynamical model is being developed to express vocal tract and orofacial motions as a function of muscle activity. This model, currently implemented through multilinear second order autoregressive techniques is described briefly. Finally, the strong direct influence that vocal tract and facial motions have on the energy of the speech acoustics is exemplified.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-538"
  },
  "loevenbruck97_eurospeech": {
   "authors": [
    [
     "Hélène",
     "Loevenbruck"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Motor control information recovering from the dynamics with the EP hypothesis",
   "original": "e97_2035",
   "page_count": 4,
   "order": 540,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "A global inversion procedure from the acoustic signal to motor commands is presented here based on a postural target invariance hypothesis. Using a model of vowel production, dynamic motor commands were inferred for a vowel sequence pronounced under different levels of emphasis stress and rate. The results enable to assign a prosodic role to the dynamic parameters of the model and thus to discriminate between slow vs fast or stressed vs unstressed utterances. Reliability of the results was assessed by computing the sensitivity of the model around the inferred motor commands and running perceptual tests on the synthetic stimuli generated from these values.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-539"
  },
  "komori97b_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Masayuki",
     "Yamada"
    ],
    [
     "Hiroki",
     "Yamamoto"
    ]
   ],
   "title": "Speaker adaptation for context-dependent HMM using spatial relation of both phoneme context hierarchy and speakers",
   "original": "e97_2039",
   "page_count": 4,
   "order": 541,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "To realize good speaker adaptation for context dependent HMM using small-size training data, reasonable adaptation of unseen models have to be realized using the relation of appeared models and the training data. In the paper, a new speaker adaptation method for context dependent HMM using two spatial constraints is proposed: 1) spatial relation of the phoneme context hierarchical models, and 2) spatial relation between speaker specific models and speaker independent models. Several implementations based on the idea are proposed and are evaluated under 520 word speech recognition. 25 words are used for adaptation par speaker. The best result improved 30% error rate showing the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-540"
  },
  "yamada97_eurospeech": {
   "authors": [
    [
     "Masayuki",
     "Yamada"
    ],
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Hiroki",
     "Yamamoto"
    ]
   ],
   "title": "Fast algorithm for speech recognition using speaker cluster HMM",
   "original": "e97_2043",
   "page_count": 4,
   "order": 542,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "This paper describes a high speed algorithm for a speech recognizer based on speaker cluster HMM. The speaker cluster HMM, which enables to deal with variety among speakers, have been reported to show good performance. However, the computation amount grows in proportion to the number of clusters, when the speaker cluster HMM is used in speaker independent recognition, where the recognition processes must be run in parallel using every speaker cluster HMM. To reduce the computation, we introduced the multi-pass search for searching on the broad space covering lexical and speaker variation. Furthermore, the output probability recalculation is introduced to reduce the state output probability computation. We had some experiments on 1000 word speaker independent continuous telephone speech recognition. The result in the case where 7 speaker clusters are used shows about 30% of computation reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-541"
  },
  "hazen97_eurospeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "A comparison of novel techniques for instantaneous speaker adaptation",
   "original": "e97_2047",
   "page_count": 4,
   "order": 543,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "This paper introduces two novel techniques for instantaneous speaker adaptation, reference speaker weighting and consistency modeling. An approach to hierarchical speaker clustering using gender and speaking rate as the clustering criteria is also presented. All three methods attempt to utilize the underlying within-speaker correlations that are present between the acoustic realizations of different phones. By accounting for these correlations a limited amount of adaptation data can be used to adapt the models of every phonetic acoustic model including those for phones which have not been observed in the adaptation data. In instantaneous adaptation experiments using the DARPA Resource Management corpus, a reduction in word error rate of 20% has been achieved using a combination of these new techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-542"
  },
  "yamaguchi97_eurospeech": {
   "authors": [
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Fast adaptation of acoustic models to environmental noise using jacobian adaptation algorithm",
   "original": "e97_2051",
   "page_count": 4,
   "order": 544,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "This paper describes Jacobian adaptation (JA) of acoustic models to environmental noise and its experimental evaluation. JA is based on a \"noise adaptation\" idea, which is acoustic model adaptation from initial noise A to target noise B , and uses Jacobian matrices to relate changes in environmental noise with changes in the \"speech+noise\" acoustic model. It is experimentally shown that JA performs well compared with existing techniques such as HMM composition, particularly when only a short sample (shorter than 1 sec) of the target noise is given, and that JA is very advantageous in terms of computational cost. Moreover, this paper describes JA used in combination with noise spectral subtraction and shows that improving SNR by spectral subtraction leads to higher efficiency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-543"
  },
  "zeljkovic97b_eurospeech": {
   "authors": [
    [
     "Ilija",
     "Zeljkovic"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Unsupervised HMM adaptation based on speech-silence discrimination",
   "original": "e97_2055",
   "page_count": 4,
   "order": 545,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "An unsupervised, sentence-level, discriminative, HMM adaptation algorithm based on silence- speech classification is presented. Silence and speech regions are determined either using an end- pointer or using the segmentation obtained from the recognizer in a first pass. A unsupervised discriminative training procedure using the gradient descent algorithm, with N-best competing strings with word insertions is then used to improve the discrimination between silence and speech. Experiments on connected digits show about 40-80 % reduction in insertion errors, a small amount of reduction in substitution errors, and a negligible effect on deletion errors. In addition, experiments on noisy speech showed about 70% word error rate reduction, thus demonstrating the robustness of the proposed adaptation technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-544"
  },
  "afify97_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Correlation based predictive adaptation of hidden Markov models",
   "original": "e97_2059",
   "page_count": 5,
   "order": 546,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "Hidden Markov model (HMM) adaptation is currently of interest, to overcome the degradation effect of speaker and/or channel mismatches in practical speech recognition systems. The Bayesian framework provides a theoretically optimal formulation for combining adaptation data and prior knowledge, but it suffers from the drawback of being incapable of adapting parameters of the models that have no observations in the adaptation speech. In this article we present a new predictive (in the sense of influencing unobserved distribution parameters) adaptation algorithm for the mean vectors of an HMM. We also point out some theoretical relationships between the proposed method and other techniques used in the context of predictive model adaptation. The efficacy of the proposed approach is demonstrated in speaker adaptation experiments for both an isolated word task, and TIMIT phonetic recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-545"
  },
  "diakoloukas97_eurospeech": {
   "authors": [
    [
     "Vassilios",
     "Diakoloukas"
    ],
    [
     "Vassilios",
     "Digalakis"
    ]
   ],
   "title": "Adaptation of hidden Markov models using multiple stochastic transformations",
   "original": "e97_2063",
   "page_count": 4,
   "order": 547,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "The recognition accuracy in recent large vocabulary Automatic Speech Recognition (ASR) systems is highly related to the existing mismatch between the training and test sets. For example, dialect differences across the training and testing speakers result to a significant degradation in recognition performance. Some popular adaptation approaches improve the recognition performance of speech recognizers based on hidden Markov models with continuous mixture densities by using linear transforms to adapt the means, and possibly the covariances of the mixture Gaussians. In this paper, we propose a novel adaptation technique that adapts the means and, optionally, the covariances of the mixture Gaussians by using multiple stochastic transformations. We perform both speaker and dialect adaptation experiments, and we show that our method significantly improves the recognition accuracy and the robustness of our system. The experiments are carried out with SRI's DECIPHER speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-546"
  },
  "gales97_eurospeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Transformation smoothing for speaker and environmental adaptation",
   "original": "e97_2067",
   "page_count": 4,
   "order": 548,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "Recently there has been much work done on how to transform HMMs, trained typically in a speaker-independent fashion on clean training data, to be more representative of data from a particular speaker or acoustic environment. These transforms are trained on a small amount of training data, so large numbers of components are required to share the same transform. Normally, each component is constrained to only use one transform. This paper examines how to optimally, in a maximum likelihood sense, assign components to transforms and allow each component, or component grouping, to make use of many transformations. The theory for obtaining both \"weights\" for each transform and transforms given a set of weights is given. The techniques are evaluated on both speaker and environmental adaptation tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-547"
  },
  "fontaine97_eurospeech": {
   "authors": [
    [
     "Vincent",
     "Fontaine"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Jean-Marc",
     "Boite"
    ]
   ],
   "title": "Nonlinear discriminant analysis for improved speech recognition",
   "original": "e97_2071",
   "page_count": 4,
   "order": 549,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "Linear Discriminant Analysis (LDA) has been widely applied to speech recognition resulting in improved recognition performance and improved robustness. LDA designs a linear transformation that projects a m-dimensional space on a m-dimensional space (m < n) such that the class separability is maximum. This paper presents new results related to our previous work [6] on nonlinear discriminant analysis (NLDA) based on the discriminant properties of Artificial Neural Networks (ANN) and more particularly MLP. Experiments performed on the isolated word large vocabulary Phone- book database show that NLDA provides a method for designing discriminant features particularly efficient as well for continuous densities HMM as for hybrid HMM/ANN recognizers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-548"
  },
  "tchorz97_eurospeech": {
   "authors": [
    [
     "Jurgen",
     "Tchorz"
    ],
    [
     "Klaus",
     "Kasper"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Bilger",
     "Kollmeier"
    ]
   ],
   "title": "On the interplay between auditory-based features and locally recurrent neural networks for robust speech recognition in noise",
   "original": "e97_2075",
   "page_count": 4,
   "order": 550,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "The combination of a model of auditory perception (PEMO) as feature extractor and of a Locally Recurrent Neural Network (LRNN) as classifier yields promising ASR results in noise. Our study focuses on the interplay between both techniques and their ability to complement each other in the task of robust speech recognition. We performed recognition experiments with modifications of PEMO processing concerning amplitude compression and envelope modulation filtering. The results show that the distinct and sparse peaks of PEMO speech representation which are well maintained in noise are sufficient cues for LRNN-based recognition due to LRNN's ability to exploit information which is distributed over time. Enhanced envelope modulation bandpass filtering of PEMO feature vectors better reflects the average modulation spectrum of speech and further decreases the influence of noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-549"
  },
  "morgan97_eurospeech": {
   "authors": [
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Eric",
     "Fosler"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Speech recognition using on-line estimation of speaking rate",
   "original": "e97_2079",
   "page_count": 4,
   "order": 551,
   "p1": "2079",
   "pn": "2082",
   "abstract": [
    "In this paper, we describe a rate of speech estimator that is derived directly from the acoustic signal. This measure has been developed as an alternative to lexical measures of speaking rate such as phones or syllables per second, which, in previous work, we estimated using a first recognition pass; the accuracy of our earlier lexical rate estimate depended on the quality of recognition. Here we show that our new measure is a good predictor of word error rate, and in addition, correlates moderately well with lexical speech rate. We also show that a simple modification of the model transition probabilities based on this measure can reduce the error rate almost as much as using lexical phones per second calculated from manually transcribed data. When we categorized test utterances based on speaking rate thresholds computed from the training set, we observed that a different transition probability value was required to minimize the error rate in each speaking rate bin. However, the reduction of error provided by this approach is still small in comparison with the increases in error observed for unusually fast or slow speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-550"
  },
  "holmes97_eurospeech": {
   "authors": [
    [
     "John N.",
     "Holmes"
    ],
    [
     "Wendy J.",
     "Holmes"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Using formant frequencies in speech recognition",
   "original": "e97_2083",
   "page_count": 4,
   "order": 552,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "Formant frequencies have rarely been used as acoustic features for speech recognition, in spite of their phonetic significance. For some speech sounds one or more of the formants may be so badly defined that it is not useful to attempt a frequency measurement. Also, it is often difficult to decide which formant labels to attach to particular spectral peaks. This paper describes a new method of formant analysis which includes techniques to overcome both of the above difficulties. Using the same data and HMM model structure, results are compared between a recognizer using conventional cepstrum features and one using three formant frequencies, combined with fewer cepstrum features to represent general spectral trends. For the same total number of features, results show that including formant features can offer increased accuracy over using cepstrum features only.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-551"
  },
  "zhan97_eurospeech": {
   "authors": [
    [
     "Puming",
     "Zhan"
    ],
    [
     "Martin",
     "Westphal"
    ],
    [
     "Michael",
     "Finke"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Speaker normalization and speaker adaptation - a combination for conversational speech recognition",
   "original": "e97_2087",
   "page_count": 4,
   "order": 553,
   "p1": "2087",
   "pn": "2090",
   "abstract": [
    "Speaker normalization and speaker adaptation are two strategies to tackle the variations from speaker, channel, and environment. The vocal tract length normalization (VTLN) is an effective speaker normalization approach to compensate for the variations of vocal tract shapes. The Maximum Likelihood Linear Regression(MLLR) is a recent proposed method for speaker-adaptation. In this paper, we propose a speaker-specific Bark scale VTLN method, investigate the combination of the VTLN with MLLR, and present an iterative procedure for decoding the combined system of VTLN and MLLR. The results show that: (1) the new VTLN method is very effective with which the word error rate can be reduced up to 11%; (2) the combination of VTLN and MLLR can provide up to 15% word error reduction; (3) both VTLN and MLLR are more effective for the push-to-talk data than for the cross-talk data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-552"
  },
  "gao97_eurospeech": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Speaker adaptation based on pre-clustering training speakers",
   "original": "e97_2091",
   "page_count": 5,
   "order": 554,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "A new strategy for speaker adaptation is described that is based on: (1) pre-clustering all the speakers in the training set acoustically into clusters; (2) for each speaker cluster, a system is built using the data from the speakers who belong to the cluster; (3) when a test speaker's data is available, we find a subset of these clusters, closest to the test speaker; (4) we transform each of the selected clusters to bring it closer to the test speaker's acoustic space; (5) we build a speaker-adapted model using transformed cluster models. This method solves the problem of excessive storage for the training speaker models [1] , as it is relatively inexpensive to store a model for each cluster. Also as each cluster contains a number of speakers, parameters of the models for each cluster can be robustly estimated. The algorithm has been evaluated on a large vocabulary system and comparied to existing algorithms. The imporvement over existing algorithms such as MLLR [2] is statistically significant.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-553"
  },
  "lincoln97_eurospeech": {
   "authors": [
    [
     "Mike",
     "Lincoln"
    ],
    [
     "Stephen",
     "Cox"
    ],
    [
     "Simon",
     "Ringland"
    ]
   ],
   "title": "A fast method of speaker normalisation using formant estimation",
   "original": "e97_2095",
   "page_count": 4,
   "order": 555,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "It has recently been shown that normalisation of vocal tract length can significantly increase recognition accuracy in speaker independent automatic speech recognition systems. An inherent difficulty with this technique is in automatically estimating the normalisation parameter from a new speaker's speech and previous techniques have typically relied on an exhaustive search to estimate this parameter. In this paper, we present a method of normalising utterances by a linear warping of mel filter bank channels in which the normalisation parameter is estimated by fitting formant estimates to a probabilistic model. This method is fast, computationally inexpensive and requires only a limited amount of data for estimation. It generates normalisations which are close to those which would be found by an exhaustive search. The normalisation is applied to a phoneme recognition task using the TIMIT database and results show a useful improvement over an unnormalised speaker independent system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-554"
  },
  "welling97_eurospeech": {
   "authors": [
    [
     "Lutz",
     "Welling"
    ],
    [
     "N.",
     "Haberland"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Acoustic front-end optimization for large vocabulary speech recognition",
   "original": "e97_2099",
   "page_count": 4,
   "order": 556,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "In this paper we describe experiments with the acoustic front{end of our large vocabulary speech recognition system. In particular, two aspects are studied: 1) linear transforms for feature extraction and 2) the modelling of the emission probabilities. Experiments are reported on a 5000 - word task of the ARPA Wall Street Journal database. For the linear transforms our main results are: a) Filter{bank coefficients yield a word error rate of 9.3%. b) A cepstral decorrelation reduces the error rate from 9.3% to 8.0%. c) By applying a linear discriminant analysis (LDA) a further reduction in the error rate from 8.0% to 7.1% is obtained. d) Recognition results are similar for a LDA applied to filter{bank outputs and to cepstral coefficients. The experiments with density modelling gave the following results: a) Gaussian and Laplacian densities yield similar error rates. b) One single vector of variances or absolute deviations outperforms density-specific or mixture- specific vectors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-555"
  },
  "logan97_eurospeech": {
   "authors": [
    [
     "B. T.",
     "Logan"
    ],
    [
     "A. J.",
     "Robinson"
    ]
   ],
   "title": "Improving autoregressive hidden Markov model recognition accuracy using a non-linear frequency scale with application to speech enhancement",
   "original": "e97_2103",
   "page_count": 4,
   "order": 557,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "A new method to improve the accuracy of Autoregressive Hidden Markov Model (AR-HMM) based recognition systems is proposed. The technique uses the bilinear transform to warp the frequency scale of the observation vectors, hence it uses a better perceptual measure to compare the observation vectors to the trained models. Results presented for the E-set letters from the ISOLET database and the first speaker dependent task of the Resource Management (RM) database show that this technique improves recognition accuracy considerably. However, in the case of the RM system, the recognition results still fall short of those obtained from a similar mel-frequency cepstral (MFCC) based system without delta parameters. Reasons for the inferior performance of the AR-HMM system are proposed and future research directions are suggested. The models built for the RM task are incorporated into an existing enhancement algorithm to form a large vocabulary speaker dependent enhancement system. Preliminary results are presented for this system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-556"
  },
  "nitta97_eurospeech": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Akinori",
     "Kawamura"
    ]
   ],
   "title": "Designing a reduced feature-vector set for speech recognition by using KL/GPD competitive training",
   "original": "e97_2107",
   "page_count": 4,
   "order": 558,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "The hybrid algorithm of SMQ (Statistical Matrix Quantization) and HMM shows high performance in vocabulary-unspecific, speaker-independent speech recognition, however, it needs lots of computation and memory at the stage of the segment quantizer of SMQ. In this paper, we propose a newly developed, two-stage segment quantizer with a feature extractor based on KL expansion and a classifier, that can be trained by using competitive training of KL/GPD. Result of experiments shows 1/30 - 1/40 reduction in both computation time and a memory size with the same performance that the old version of SMQ shows.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-557"
  },
  "chen97b_eurospeech": {
   "authors": [
    [
     "Scott Shaobing",
     "Chen"
    ],
    [
     "Peter",
     "DeSouza"
    ]
   ],
   "title": "Speaker adaptation by correlation (ABC)",
   "original": "e97_2111",
   "page_count": 4,
   "order": 559,
   "p1": "2111",
   "pn": "2114",
   "abstract": [
    "This paper describes a new rapid speaker adaptation algorithm using a small amount of adaptation data. This algorithm, termed adaptation by correlation (ABC), exploits the intrinsic correlation among speech units to update the speech models. The algorithm updates the means of each Gaussian based on its correlation with means of the Gaussians which are observed in the adaptation data; the updating formula is derived from the theory of least squares. Our experiments on the ARPA NAB-94 evaluation (Eval-94) and the ARPA Hub4-96 (Hub4- 96) tasks indicate that ABC seems more stable than MLLR when the amount of data for adaptation is very small (~ 5 seconds), and that ABC seems to enhance MLLR when they are combined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-558"
  },
  "ainsworth97_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "Georg F.",
     "Meyer"
    ]
   ],
   "title": "Preliminary experiments on the perception of double semivowels",
   "original": "e97_2115",
   "page_count": 5,
   "order": 560,
   "p1": "2115",
   "pn": "2118",
   "abstract": [
    "A number of previous studies have shown that it is possible to recognise two vowel sounds spoken simultaneously if their pitches, onsets or the spatial locations of their sources differ sufficiently. Experiments have been carried out to explore the perception of isolated syllables containing the glides /w/ and /j/ spoken at the same time. It was found that consonants in concurrent syllables which contained the same vowel but were spoken with different pitches could not be reliably identified. However if the vowels and their pitches differed, the glides could be recognised about 70% of the time. This suggests that the neuronal mechanisms underlying the separation of simultaneous consonants employ other features as well as pitch differences\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-559"
  },
  "schiller97_eurospeech": {
   "authors": [
    [
     "Niels O.",
     "Schiller"
    ]
   ],
   "title": "Does syllable frequency affect production time in a delayed naming task?",
   "original": "e97_2119",
   "page_count": 4,
   "order": 561,
   "p1": "2119",
   "pn": "2122",
   "abstract": [
    "In a delayed naming task the effect of syllable frequency on the production time of syllables was investigated. Participants first heard either a low- or a high-frequency syllable and were then asked to repeat this syllable as often as they could for a time span of eight seconds. Mean production times per syllable were determined. When the segmental make-up of high- and low-frequency syllables was completely matched, there was no frequency effect on production time. It is concluded that syllable frequency does not play a role on the articulatory-motor level in speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-560"
  },
  "morris97_eurospeech": {
   "authors": [
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "William J.",
     "Barry"
    ],
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Jacques",
     "Koreman"
    ]
   ],
   "title": "Human and machine identification of consonantal place of articulation from vocalic transition segments",
   "original": "e97_2123",
   "page_count": 5,
   "order": 562,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "One of the most difficult problems in the first stages of automatic speech recognition (ASR) is the identification of consonantal place of articulation (CPA). It is known that the acoustic correlates for CPA reside largely in the pattern of formant transitions preceding v ocal tract closure and following release, b ut common speech preprocessing techniques make only a limited attempt to capture these spectral dynamics in the representation which they pass on for recognition. In order to test alternative preprocessing strategies, we have prepared a multilingual set of VC and CV vocalic transition segments and then compared the baseline performance of human perception of CP A in this dataset with the performance of tw o common ASR techniques. Representaions initially tested were concatenated mel cepstra and mel ceptra plus cepstral differences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-561"
  },
  "barker97_eurospeech": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Modelling the recognition of spectrally reduced speech",
   "original": "e97_2127",
   "page_count": 4,
   "order": 563,
   "p1": "2127",
   "pn": "2130",
   "abstract": [
    "Progress in robust automatic speech recognition may benefit from a fuller account of the mechanisms and representations used by listeners in processing distorted speech. This paper reports on a number of studies which consider how recognisers trained on clean speech can be adapted to cope with a particular form of spectral distortion, namely reduction of clean speech to sine-wave replicas. Using the Resource Management corpus, the first set of recognition experiments confirm the high information content of sine-wave replicas by demonstrating that such tokens can be recognised at levels approaching those for natural speech if matched conditions apply during training. Further recognition tests show that sine-wave speech can be recognised using natural speech models if a spectral peak representation is employed in concert with occluded speech recognition techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-562"
  },
  "pallier97_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Pallier"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Nuria",
     "Sebastian-Galles"
    ]
   ],
   "title": "Prosodic structure and phonetic processing: a cross-linguistic study",
   "original": "e97_2131",
   "page_count": 4,
   "order": 564,
   "p1": "2131",
   "pn": "2134",
   "abstract": [
    "Dutch and Spanish differ in how predictable the stress pattern is as a function of the segmental content: it is correlated with syllable weight in Dutch but not in Spanish. In the present study, two experiments were run to compare the abilities of Dutch and Spanish speakers to separately process segmental and stress information. It was predicted that the Spanish speakers would have more difficulty focusing on the segments and ignoring the stress pattern than the Dutch speakers. The task was a speeded classification task on CVCV syllables, with blocks of trials in which the stress pattern could vary versus blocks in which it was fixed. First, we found interference due to stress variability in both languages, suggesting that the processing of segmental information cannot be performed independently of stress. Second, the effect was larger for Spanish than for Dutch, suggesting that that the degree of interference from stress variation may be partially mitigated by the predictability of stress placement in the language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-563"
  },
  "son97b_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The correlation between consonant identification and the amount of acoustic consonant reduction",
   "original": "e97_2135",
   "page_count": 4,
   "order": 565,
   "p1": "2135",
   "pn": "2138",
   "abstract": [
    "Reduction causes changes in the acoustics of consonant realizations that affect their identification. In this study we try to identify some of the acoustic parameters that are correlated with this change in identification. Speaking style is used to manipulate the degree of reduction. Pairs of otherwise identical intervocalic consonants from read and spontaneous utterances are presented to subjects in an identification experiment. The resulting identification scores are correlated to five different acoustical measures that are affected by the amount of consonant reduction: Segmental duration, spectral Center of Gravity, intervocalic sound energy difference, intervocalic F2 slope difference, and the amount of vowel reduction in the syllable kernel. The identification differences between the read and spontaneous realizations are compared with the differences in each of the acoustic measures. It showed that only segmental duration and the spectral Center of Gravity are significantly correlated to identification scores.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-564"
  },
  "bonneau97_eurospeech": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Relevant spectral information for the identification of vowel features from bursts",
   "original": "e97_2139",
   "page_count": 4,
   "order": 566,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "This paper presents a first attempt to extract relevant spectral information for the identification of the following vocalic context from French stop bursts. For this purpose, we studied the acoustic spectra of bursts used in a perceptual experiment which showed that listeners were able to identify vocalic features from bursts [1]. The corpus was made up of stimuli of 20-25 ms duration extracted from natural monosyllabic words which combined the initial stops /p,t,k/ with the vowels /i,a,u/. The low frequency limit of the frication noise as well as the frequency of the most prominent peak of the burst appeared to be very interesting cues for the identification of the vocalic context. Using these cues, most of contexts (/i/ from /t,k/, /u/ from /p,k/ and /a/ from /p,t/) have been very well classified without specification of the consonant.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-565"
  },
  "li97c_eurospeech": {
   "authors": [
    [
     "Aijun",
     "Li"
    ]
   ],
   "title": "Perceptual study of intersyllabic formant transitions in synthesized V1-V2 in standard Chinese",
   "original": "e97_2143",
   "page_count": 4,
   "order": 567,
   "p1": "2143",
   "pn": "2146",
   "abstract": [
    "Transition between vowels is related to speech continuity[8]. Research shows that the formant intensity between syllables varies in Standard Chinese (SC)[5]. We can classify the intensity of intersyllabic formant juncture/transition into three categories from strong to weak by using the consonant of the second syllable. Different categories take various roles in synthesizing speech: the more intense the formant transition is, the more important role it will take in the synthesis. This paper reports the results of perceptual experiments on intersyllabic formant transitions of one of the categories when the second syllable is a zero-initial syllable (i.e. begins with a vowel).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-566"
  },
  "skljarov97_eurospeech": {
   "authors": [
    [
     "Oleg P.",
     "Skljarov"
    ]
   ],
   "title": "Role of perception of rhythmically organized speech in consolidation process of long-term memory traces (LTM-traces) and in speech production controlling",
   "original": "e97_2147",
   "page_count": 4,
   "order": 568,
   "p1": "2147",
   "pn": "2150",
   "abstract": [
    "In this paper, proceeding from established during comparative research of speech of the normal persons and stutterers equations of a logistical type [1,2,3], hypothesis about discrete character of perception of a speech signal, occurrence of homeostatic state of speech reproduction system (speech memory), and also about ways revealing of such memory are offered. The results prove to be true by being available experimental data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-567"
  },
  "lugt97_eurospeech": {
   "authors": [
    [
     "Arie H. van der",
     "Lugt"
    ]
   ],
   "title": "Sequential probabilities as a cue for segmentation",
   "original": "e97_2151",
   "page_count": 4,
   "order": 569,
   "p1": "2151",
   "pn": "2154",
   "abstract": [
    "A large amount of psycholinguistic research, phonetic research and research in speech technology has been dedicated to the problem of segmentation: how is speech segmented into words? The work reported here extends earlier findings by McQueen & Cox ([1]), who found that phonotactics are used by listeners as a cue to the location of word boundaries. The present investigation addresses the question of whether people can also use less extreme sequential probabilities as a segmentation cue. Hearing a combination of sounds that often occurs at the end of a word or syllable may facilitate recognition of a following word; hearing a combination of sounds that occurs often at the beginning of a word or syllable may facilitate recognition of a preceding word. In a word-spotting task some indications were found that people are sensitive to sequential probabilities. However, no effects were found that strongly support the hypothesis that people do indeed use these distributional properties of the lexicon in the segmentation of spoken language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-568"
  },
  "jansens97_eurospeech": {
   "authors": [
    [
     "Susan",
     "Jansens"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "Guus de",
     "Krom"
    ]
   ],
   "title": "Perception and acoustics of emotions in singing",
   "original": "e97_2155",
   "page_count": 4,
   "order": 570,
   "p1": "2155",
   "pn": "2158",
   "abstract": [
    "In this experiment, the acoustic correlates of perceived emotions in singing were investigated. Singers were instructed to sing one phrase in a neutral way and in the emotions anger, joy, fear, and sadness. Listeners rated the strength of the perceived emotions for each fragment. Principal component analyses were performed on the listeners' ratings. The derived factors were interpreted as listening strategies; and a listener's factor loading as an indicator of the extent to which that listener used that strategy. Using the original ratings and the factor loadings, the phrases were assigned composite ratings for each emotion. Acoustic measures of spectral balance, vibrato, duration and intensity were related to the composite ratings using multiple regression analyses. It was found that anger was associated with the presence of vibrato; joyous phrases had vibrato, a short final duration, and a shallow spectral slope; sadness was associated with absence of vibrato, long duration, and a low intensity, whereas fear was related to a steep spectral slope.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-569"
  },
  "pallier97b_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Pallier"
    ]
   ],
   "title": "Phonemes and syllables in speech perception: size of attentional focus in French",
   "original": "e97_2159",
   "page_count": 4,
   "order": 571,
   "p1": "2159",
   "pn": "2162",
   "abstract": [
    "A study by Pitt and Samuel (1990) found that English speakers could narrowly focus attention onto a precise phonemic position inside spoken words [1]. This led the authors to argue that the phoneme, rather than the syllable, is the primary unit of speech perception. Other evidence, obtained with a syllable detection paradigm, has been put forward to propose that the syllable is the unit of perception; yet, these experiments were ran with French speakers [2]. In the present study, we adapted Pitt & Samuel's phoneme detection experiment to French and found that French subjects behave exactly like English subjects: they too can focus attention on a precise phoneme. To explain both this result and the established sensitivity to the syllabic structure, we propose that the perceptual system automatically parses the speech signal into a syllabically-structured phonological representation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-570"
  },
  "tokuma97_eurospeech": {
   "authors": [
    [
     "Shinichi",
     "Tokuma"
    ]
   ],
   "title": "Quality of a vowel with formant undershoot: a preliminary perceptual study",
   "original": "e97_2163",
   "page_count": 4,
   "order": 572,
   "p1": "2163",
   "pn": "2166",
   "abstract": [
    "In this study vowels in /CVC/ environments are compared with steady state vowels to investigate the perceived vowel quality change caused by undershoot. This study uses a perceptual task, whereby listeners match constant /CVC/ stimuli of /bVb/ or /dVd/ to variable /#V#/ stimuli, using a schematic grid on a PC screen. The grid represents an acoustic vowel diagram, and the subjects change the F1/F2 frequencies of /#V#/ by moving a mouse. The main results of the study show that while subjects referred to the trajectory peak of the /CVC/ stimuli in vowel quality perception, their performance was also affected by the formant trajectory range of the stimuli. When the formant trajectory range was small, they selected a value between the edge and peak frequencies, while they selected a value outside the trajectory range when it was large.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-571"
  },
  "koster97_eurospeech": {
   "authors": [
    [
     "Mariette",
     "Koster"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Segmental and suprasegmental contributions to spoken-word recognition in dutch",
   "original": "e97_2167",
   "page_count": 4,
   "order": 573,
   "p1": "2167",
   "pn": "2170",
   "abstract": [
    "Words can be distinguished by segmental differences or by suprasegmental differences or both. Studies from English suggest that suprasegmentals play little role in human spoken-word recognition; English stress, however, is nearly always unambiguously coded in segmental structure (vowel quality); this relationship is less close in Dutch. The present study directly compared the effects of segmental and suprasegmental mispronunciation on word recognition in Dutch. There was a strong effect of suprasegmental mispronunciation, suggesting that Dutch listeners do exploit suprasegmental information in word recognition. Previous findings indicating the effects of mis-stressing for Dutch differ with stress position were replicated only when segmental change was involved, suggesting that this is an effect of segmental rather than suprasegmental processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-572"
  },
  "behne97_eurospeech": {
   "authors": [
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Peter E.",
     "Czigler"
    ],
    [
     "Kirk P. H.",
     "Sullivan"
    ]
   ],
   "title": "Perception of vowel duration and spectral characteristics in Swedish",
   "original": "e97_2171",
   "page_count": 4,
   "order": 574,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "This project re-examines the perceptual weight of vowel duration and the first two vowel formant frequencies as determinants of phonologically short and long vowels in Swedish. Based on listeners' responses to synthesized sets of materials for [I]-[i:] [ ]-[o:] and [a]- [a:], results indicate that vowel duration is of primary importance for distinguishing [I) from [i:] and [ ] from [o:], whereas both formant frequencies and vowel duration were found to influence the perception of [a] from [a:].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-573"
  },
  "neagu97_eurospeech": {
   "authors": [
    [
     "Adrien",
     "Neagu"
    ],
    [
     "Gerard",
     "Bailly"
    ]
   ],
   "title": "Relative contributions of noise burst and vocalic transitions to the perceptual identification of stop consonants",
   "original": "e97_2175",
   "page_count": 4,
   "order": 575,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "A set of three perceptual experiments is described. These experiments were designed to provide identification scores on CV sequences for French. Original stimuli were augmented with acoustic \"monsters\" where burst were excised or replaced. The first identification task shows that information carried by vocalic transitions can be overwritten by burst information. The importance of this phenomenon is inversely proportional to vowel aperture. The second experiment shows that these results are almost insensitive to relative amplitudes between the burst and the vowel. In the third experiment we manipulated the voice onset time (VOT) of the monsters using high quality analysis-resynthesis. Stimuli with a very short VOT were perceived as bilabials but VOT manipulation did not affect the /t/-/k/ confusions. These experiments claim for a dynamic model of stop identification where burst and vocalic transitions both contribute and compete to the phonetic decision.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-574"
  },
  "kitagawa97_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Kitagawa"
    ],
    [
     "Makoto",
     "Hashimoto"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Effect of speaker familiarity and background noise on acoustic features used in speaker identification",
   "original": "e97_2179",
   "page_count": 4,
   "order": 576,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "In order to investigate the relationship between human perception in speaker identification and acoustic features (fundamental frequency (f0), spectrum, and duration) under various communication conditions, this paper describes several perception experiments and an approach to predict the perceptual contribution rate of each feature. Factors taken into account in this paper are: (1) speaker familiarity and (2) background noise. As a result, it is shown that: (1) the perceptual contribution rate increases as the distance of anacoustic feature increases, (2) the spectral contribution rates for familiar speakers are larger than those for unfamiliar speakers, (3) the contribution of f0 tends to increase as the noise increases, and (4) in case of the same S/N ratio, the contribution of f0 in the computer room noise environment is larger than in the car noise environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-575"
  },
  "pitermann97_eurospeech": {
   "authors": [
    [
     "Michel",
     "Pitermann"
    ]
   ],
   "title": "Dynamic versus static specification for the perceptual identity of a coarticulated vowel",
   "original": "e97_2183",
   "page_count": 4,
   "order": 577,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "This paper presents a perceptual experiment on stimuli synthesized by means of a vocal tract area function model. The purpose was to compare the contribution of dynamic against static information to the identity of a coarticulated vowel. Three sources of information were perceptually analyzed: (i) the vowel nucleus; (ii) the acoustical contrast between the vowel nucleus and the \"stationary\" parts of its immediate context; (iii) and the transitions linking the stable parts of the speech signal. The results show that the vocoids were better identified by dynamic information. This backs up the perceptual overshoot model proposed in Lindblom and Studdert-Kennedy (1967). However, this conclusion must be confirmed by further experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-576"
  },
  "plauche97_eurospeech": {
   "authors": [
    [
     "Madelaine",
     "Plauche"
    ],
    [
     "Cristina",
     "Delogu"
    ],
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Asymmetries in consonant confusion",
   "original": "e97_2187",
   "page_count": 4,
   "order": 578,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "Both historical sound change and laboratory confusion studies show strong asymmetries of consonant confusions. In particular, /ki/ commonly changes to /ti/, and /pi/ to /ti/, but not the reverse. It is hypothesized that such asymmetries arise when two sounds are acoustically similar except for one or more differentiating cues, which are subject to a highly directional perceptual error. This perceptual entropy can be explained as follows: if sound x possesses a cue that y lacks, listeners are more likely to miss this \"all-or-none\" cue than to introduce it spuriously. /k/ and /t/ before /i/ have similar formant transitions but differ in their burst spectra. /p/ and /t/ before /i/ also have similar formant transitions but differ in the intensity of their bursts. The importance of these differentiating features for listeners' perception were verified in a confusion study. The implications of the inversely related effects of perceptual and physical entropy for phonetic theory and speech technology is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-577"
  },
  "dumay97_eurospeech": {
   "authors": [
    [
     "Nicolas",
     "Dumay"
    ],
    [
     "Monique",
     "Radeau"
    ]
   ],
   "title": "Rime and syllabic effects in phonological priming between French spoken words",
   "original": "e97_2191",
   "page_count": 4,
   "order": 579,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "Phonological priming between spoken words was examined using CVCVC bisyllabic pseudoword primes and word or pseudoword targets. The influence of different types of overlap was compared, prime and target sharing the coda, the rime or the final syllable. The task was target shadowing. Two priming conditions were used, the auditory targets being preceded by auditory primes in unimodal and by visual primes in crossmodal situation. Priming effects were obtained under unimodal stimulation only. A strong facilitation occurred with syllable overlap while a smaller facilitation was found with rime overlap. Coda overlap produced no effect. The absence of effect in crossmodal stimulation argues that the final overlap effects occur before the semantic system. Concerning the underlying units, a comparison of our results with those obtained from CCVC monosyllables with overlaps in phonemic length similar to those we used, suggests that both rime and syllabic units per se are involved in the effects of final similarity between spoken words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-578"
  },
  "zhu97_eurospeech": {
   "authors": [
    [
     "Weizhong",
     "Zhu"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Roles of static and dynamic features of formant trajectories in the perception of talk indedivduality",
   "original": "e97_2195",
   "page_count": 4,
   "order": 580,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "Experiments were performed to investigate perceptual contributions of static and dynamic features of vocal tract characteristics to talker individuality. An ARX (Auto-regressive with exogenous input) speech production model was used to extract separately voice source and vocal tract parameters from a Japanese sentence, /aoiueoie/ (\"Say blue top\" in English). The Discrete Cosine Transform (DCT) was applied to resolve formant trajectories of the speech signal into static and dynamic components. The perceptual contributions were quantitatively studied by systematically replacing the corresponding formant components extracted from Japanese sentences uttered by three males. Results of the experiments show that the static (average) characteristic of the vocal tract is a primary cue to talker individuality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-579"
  },
  "lin97d_eurospeech": {
   "authors": [
    [
     "Chih-mei",
     "Lin"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Russell",
     "Ritenour"
    ]
   ],
   "title": "Database management and analysis for spoken dialog systems: methodology and tools",
   "original": "e97_2199",
   "page_count": 4,
   "order": 581,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "A methodology for creating and managing an integrated database for spoken dialog systems is proposed. Using an example of a telecommunication service application, details of organizing, maintaining, and visualizing the dialog system data are presented. Examples illustrating the use of the unified database structure for dialog reproduction and performance evaluation are provided.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-580"
  },
  "kamm97_eurospeech": {
   "authors": [
    [
     "Candace",
     "Kamm"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Dawn",
     "Dutton"
    ],
    [
     "Russell",
     "Ritenour"
    ]
   ],
   "title": "Evaluating spoken dialog systems for telecommunication services",
   "original": "e97_2203",
   "page_count": 4,
   "order": 582,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "This paper presents a case study analyzing the results of an on-going trial of a prototype mixed-initiative spoken dialog system for telephony control and messaging. System usage and performance data were captured at three points in time. Information from multiple data sources, including spoken utterances, system call logs, speech recognizer output, and subjective surveys was evaluated to determine the relationship between aspects of system performance and user perceptions of the system. This report provides several examples using these data sources in combination to identify key areas to focus on in modifying the system, application, and/or user interface in order to significantly improve system usability and user satisfaction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-581"
  },
  "pouteau97_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Pouteau"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Jan",
     "Landsbergen"
    ]
   ],
   "title": "Robust spoken dialogue management for driver information systems",
   "original": "e97_2207",
   "page_count": 4,
   "order": 583,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "Considering the limitations of Speech Recognition for the development of user-system dialogues in real applications, robustness is a primary objective. In this paper, we describe the most essential characteristics of the Dialogue Manager of a driver information system that is controlled by voice, mainly showing how its design has been driven by the characteristics of voice in such a dialogue. We present the main methods used by the Dialogue Manager to come to an effective balance between robustness and efficiency. We illustrate them with examples from the first implementation of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-582"
  },
  "lee97d_eurospeech": {
   "authors": [
    [
     "Yue-Shi",
     "Lee"
    ],
    [
     "Hsin-Hsi",
     "Chen"
    ]
   ],
   "title": "Using acoustic and prosodic cues to correct Chinese speech repairs",
   "original": "e97_2211",
   "page_count": 4,
   "order": 584,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "Speech repairs introduce much noise in spoken language processing. Properly correcting speech repairs can help the speech recognizer to avoid the textual errors, and prevent the interpretation errors during the subsequent processing. Because the task of repair processing cannot defer to the latter (word segmentation, part-of-speech tagging and sentence parsing) stages, this paper employs acoustic and prosodic cues to correct Chinese repetition and addition repairs. The experimental results show that the precision rate of 93.87% (76.09%) and the recall rate of 90.65% (70%) can be achieved for correcting Chinese repetition (addition) repairs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-583"
  },
  "dahlback97_eurospeech": {
   "authors": [
    [
     "Nils",
     "Dahlbäck"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "Integrating domain specific focusing in dialogue models",
   "original": "e97_2215",
   "page_count": 4,
   "order": 585,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "Natural language interaction requires dialogue models that allow for efficient and robust human computer interaction. Most systems today use some kind of speech-act based dialogue model. While successful in a number of applications, these models have known limitations, both from linguistic and computational points of view, which has led a number of workers to suggest using the dialogue participants goals/intentions to model the dialogue. In this paper we suggest that amending speech act based models with sophisticated domain knowledge makes it possible to extend their applicability. Two kinds of domain knowledge are identified, one is the Domain Model; a structure of the discourse 'world', and the other is the Conceptual Model which contains domain specific general information about the concepts and their relationships in the domain. These extensions have been utilized in the LINLIN dialogue manager and the paper presents results from customizing the dialogue manager to two different applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-584"
  },
  "walker97_eurospeech": {
   "authors": [
    [
     "Marilyn",
     "Walker"
    ],
    [
     "Donald",
     "Hindle"
    ],
    [
     "Jeanne",
     "Fromer"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Craig",
     "Mestel"
    ]
   ],
   "title": "Evaluating competing agent strategies for a voice email agent",
   "original": "e97_2219",
   "page_count": 4,
   "order": 586,
   "p1": "2219",
   "pn": "2222",
   "abstract": [
    "This paper reports experimental results comparing a mixed-initiative to a system-initiative dialog strategy in the context of a personal voice email agent. To independently test the effects of dialog strategy and user expertise, users interact with either the system-initiative or the mixed-initiative agent to perform three successive tasks which are identical for both agents. We report performance comparisons across agent strategies as well as over tasks. This evaluation utilizes and tests the PARADISE evaluation framework, and discusses the performance function derivable from the experimental data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-585"
  },
  "byron97_eurospeech": {
   "authors": [
    [
     "Donna K.",
     "Byron"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "Discourse marker use in task-oriented spoken dialog \\lambda",
   "original": "e97_2223",
   "page_count": 4,
   "order": 587,
   "p1": "2223",
   "pn": "2226",
   "abstract": [
    "Discourse markers, also known as cue words, are used extensively in human-human task-oriented dialogs to signal the structure of the discourse. Previous work showed their importance in monologues for marking discourse structure, but little attention has been paid to their importance in spoken dialog systems. This paper investigates what discourse markers signal about the up- coming speech, and when they tend to be used in task-oriented dialog. We demonstrate that there is a high correlation between specific discourse markers and specific conversational moves, between discourse marker use and adjacency pairs, and between discourse markers and the speaker's orientation to information presented in the prior turn.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-586"
  },
  "zue97b_eurospeech": {
   "authors": [
    [
     "Victor W.",
     "Zue"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Lee",
     "Hetherington"
    ],
    [
     "Edward",
     "Hurley"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Rafael",
     "Schloming"
    ],
    [
     "Philipp",
     "Schmid"
    ]
   ],
   "title": "From interface to content: translingual access and delivery of on-line information",
   "original": "e97_2227",
   "page_count": 4,
   "order": 588,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "This paper describes our initial implementation of a system to provide world-wide weather information over the telephone. The information is gathered from several different sites on the Web, preprocessed, and cached locally into a relational database to make access both fast and selective. Our natural language tools, originally developed for processing user queries, are used here for understanding content, and for subsequently translating it into languages other than English. The system is operational, and we have been collecting data from real users via a toll-free number. We report here on an initial evaluation both of the full system in English and of the quality of the responses in German.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-587"
  },
  "alexandersson97_eurospeech": {
   "authors": [
    [
     "Jan",
     "Alexandersson"
    ],
    [
     "Norbert",
     "Reithinger"
    ]
   ],
   "title": "Learning dialogue structures from a corpus",
   "original": "e97_2231",
   "page_count": 4,
   "order": 589,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "This paper demonstrates some aspects of a plan processor which is a subcomponent of the dialogue module of verb-mobil. We describe how we transfer results from the re- search area of grammar extraction for the semi-automatic acquisition of plan operators for turn classes. We exploit statistical knowledge acquired during learning the grammar and incorporate top down predictions to enhance the correct analysis of turn classes described. A first evaluation shows a relative recognition rate of around 70% on unseen data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-588"
  },
  "reithinger97_eurospeech": {
   "authors": [
    [
     "Norbert",
     "Reithinger"
    ],
    [
     "Martin",
     "Klesen"
    ]
   ],
   "title": "Dialogue act classification using language models",
   "original": "e97_2235",
   "page_count": 4,
   "order": 590,
   "p1": "2235",
   "pn": "2238",
   "abstract": [
    "Pragmatically important information as e.g. dialogue acts that describe the illocution of an utterance depend in traditional processing approaches on error prone syntactic/semantic processing. We present a statistically based method for dialogue act classification that has word strings as input. An experimental evaluation shows that this method can be successfully used to determine dialogue acts. The overall recognition rate in the experiments is in the range of 65%-67% for German test data, and 74% for an experiment with English dialogues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-589"
  },
  "pernel97_eurospeech": {
   "authors": [
    [
     "Didier",
     "Pernel"
    ]
   ],
   "title": "User's multiple goals in spoken dialogue",
   "original": "e97_2239",
   "page_count": 4,
   "order": 591,
   "p1": "2239",
   "pn": "2242",
   "abstract": [
    "This paper deals with a problematic not deeply studied as yet: user's goals interaction. A situation of multiple goals occurs as soon as the user utters a new goal whereas the previous one has not been solved yet. We propose an algorithm to identify the kind of multiple goals according to the task state and to the goals themselves. We define ten strategies to process those situations. Three meta-strategies order the strategies relevant for given situations. The system checks the preconditions of strategies to be sure they can be triggered. When a strategy is applied, the system updates the dialogue history and the task state. Some strategies push a goal in a stack and pop it when the first processed goal is fully reached.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-590"
  },
  "suzuki97_eurospeech": {
   "authors": [
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Seiji",
     "Inokuchi"
    ],
    [
     "K.",
     "Ishii"
    ],
    [
     "Michio",
     "Okada"
    ]
   ],
   "title": "Chatting with interactive agent",
   "original": "e97_2243",
   "page_count": 4,
   "order": 592,
   "p1": "2243",
   "pn": "2246",
   "abstract": [
    "Conventional spoken dialogue systems are based on goal-oriented techniques(8). The recent expansion of application fields such as cyber space, internet, etc, necessitates the creation of new interaction styles between humans and autonomous agents. Interaction with autonomous agents creates new possibilities for spontaneous conversation in spoken dialogue systems. Within this context, we regard spontaneous, informal chatting behavior as one aspect of spoken dialogue(4)(5). According to this view, an essential property of chatting is the emergence of topics and goals situated within the context of interactions among participants rather than as the result of explicit goals. In this paper, we propose a spoken dialogue system with chatting properties and illustrate sample chatting between a human and a virtual interface agent called Talking Eye using a prototype system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-591"
  },
  "churcher97_eurospeech": {
   "authors": [
    [
     "Gavin E.",
     "Churcher"
    ],
    [
     "Eric S.",
     "Atwell"
    ],
    [
     "Clive",
     "Souter"
    ]
   ],
   "title": "Generic template for the evaluation of dialogue management systems",
   "original": "e97_2247",
   "page_count": 4,
   "order": 593,
   "p1": "2247",
   "pn": "2250",
   "abstract": [
    "We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components. The generic model is abstracted from a number of real application systems targetted at very different domains. Our research aim in developing this generic template is to investigate a new approach to the evaluation of Dialogue Management Systems. Rather than attempting to measure accuracy/speed of output, we propose principles for the evaluation of the underlying theoretical linguistic model of Dialogue Management in a given system, in terms of how well it fits our generic template for Dialogue Management Systems. This is a measure of 'genericness' or 'application-independence' of a given system, which can be used to moderate accuracy/speed scores in comparisons of very unlike DMSs serving different domains. This relates to (but is orthogonal to) Dialogue Management Systems evaluation in terms of naturalness and like measurable metrics; it follows more closely emerging qualitative evaluation techniques for NL grammatical parsing schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-592"
  },
  "niimi97_eurospeech": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Yutaka",
     "Kobayashi"
    ]
   ],
   "title": "Analysis of interactive strategy to recover from misrecognition of utterances including multiple information items",
   "original": "e97_2251",
   "page_count": 4,
   "order": 594,
   "p1": "2251",
   "pn": "2254",
   "abstract": [
    "This paper proposes and analyzes mathematically an interactive strategy to recover from misrecognition of utterances including multiple information items through a short conversation with a speaker. First the speech recognizer in a dialogue system recognizes an utterance and evaluates the reliability of each item contained in it. The dialogue system accepts only those items of which the reliability is high, while it rejects the items which are unreliably recognized, or confirms the content of them. The paper, given the performance of the recognizer, derives two quantities P ac and N, which can describe the performance of the dialogue system using this interactive strategy: P ac is the probability that all information items included in user's utterance are conveyed to the system correctly, and N is the average number of turns taken between the user and the system until all the items are accepted.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-593"
  },
  "mathieu97b_eurospeech": {
   "authors": [
    [
     "Francois-Arnould",
     "Mathieu"
    ],
    [
     "Bertrand",
     "Gaiffe"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "A referential approach to reduce perplexity in the vocal command system comppa",
   "original": "e97_2255",
   "page_count": 5,
   "order": 595,
   "p1": "2255",
   "pn": "2258",
   "abstract": [
    "The reliability of automatic speech recognition systems depends mainly on the local perplexity of the language to recognise. In the framework of vocal command dialogue systems, we propose an approach based on pragmatic, mainly through a precise treatment of referential expressions, which we use in order to reduce dynamically the local perplexity that the recognition process is confronted with. Therefore, we take into account not only the left context of the current hypothesis but also the state of the application. The article justifies the architecture we propose, describes the treatments and shows the resulting reduction of perplexity when using contextual information as compared to that obtained when using only semantic ones. Keywords: vocal command system - natural language - pragmatics - language perplexity - reference calculus\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-594"
  },
  "thanopoulos97_eurospeech": {
   "authors": [
    [
     "Aristomenis",
     "Thanopoulos"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Linguistic processor for a spoken dialogue system based on island parsing techniques",
   "original": "e97_2259",
   "page_count": 4,
   "order": 596,
   "p1": "2259",
   "pn": "2262",
   "abstract": [
    "In this paper we present the Linguistic Analysis Component of a Spoken Dialogue System designed for robustness and flexibility. The dialogue takes place in the Greek Language through the public telephone network and is performed in two different applications. The analysis is based on Island Parsing, Pattern Matching and Frame-based Representation techniques. The main knowledge sources are a Semantic Network and Frame-Slot structures thoroughly connected with each other. Simple bigram grammar rules have been also used to assist the parsing process as well as to evaluate the recognition output.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-595"
  },
  "mellor97_eurospeech": {
   "authors": [
    [
     "Brian",
     "Mellor"
    ],
    [
     "Chris",
     "Baber"
    ]
   ],
   "title": "Modelling of speech-based user interfaces",
   "original": "e97_2263",
   "page_count": 4,
   "order": 597,
   "p1": "2263",
   "pn": "2266",
   "abstract": [
    "The capability profiles of commercial automatic speech recognition (ASR) systems are rapidly improving in terms of vocabulary size, noise robustness and user population. Most contemporary applications of ASR use interfaces relying solely on the speech mode of interaction (over telephone channels for example). Many applications will, however, benefit from using speech input in conjunction with other interaction devices such as trackballs, keyboards and touch-screens. In this paper, we present an interface modelling approach based on a critical path analysis of the interface design. The approach has been developed to model multi-modal interactions using combinations of input devices. Degradation of unit performances allow the effects of environmental factors on the overall interface performance to be predicted. The model is verified by comparison with experimental trials carried out on a number of multi-modal applications. It is demonstrated that the model is able to predict the main performance metric (task completion time) to within 10% of the experimental values.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-596"
  },
  "hockey97_eurospeech": {
   "authors": [
    [
     "Beth Ann",
     "Hockey"
    ],
    [
     "Deborah",
     "Rossen-Knill"
    ],
    [
     "Beverly",
     "Spejewski"
    ],
    [
     "Matthew",
     "Stone"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Can you predict responses to yes/no questions? yes, no, and stuff",
   "original": "e97_2267",
   "page_count": 4,
   "order": 598,
   "p1": "2267",
   "pn": "2270",
   "abstract": [
    "We analyze what functions as a YES response and a NO response for different yes/no questions. This problem is surprisingly complex: respondents do not always produce overt yes or no lexical items in sponse to a yes/no question. In addition, when spondents don't include a clear yes or no word, they may mean to communicate a clear YES or NO ing, or something else. We find that the classification of yes/no questions described in (Carletta et al., 1995) for the Edinburgh map task corpus correlates well with whether a response will be a bare yes or no, a yes or no plus additional speech, or just speech out an overt yes or no. Correlation with responses described simply as as \"direct\" or \"indirect\" is less good. We also find that, under the three-way rization, the strength of a question's expectation for a YES response predicts the form of the response.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-597"
  },
  "moller97b_eurospeech": {
   "authors": [
    [
     "Jens-Uwe",
     "Möller"
    ]
   ],
   "title": "Dia-moLE: an unsupervised learning approach to adaptive dialogue models for spoken dialogue systems",
   "original": "e97_2271",
   "page_count": 4,
   "order": 599,
   "p1": "2271",
   "pn": "2274",
   "abstract": [
    "The Dialogue Model Learning Environment supports an engineering-oriented approach towards dialogue modelling for a spoken-language interface. Major steps towards dialogue models is to know about the basic units that are used to construct a dialogue model and possible sequences. In difference to many other approaches a set of dialogue acts is not predefined by any theory or manually during the engineering process, but is learned from data that are available in an avised spoken dialogue system. The architecture is outlined and the approach is applied to the domain of appointment scheduling. Even though based on a word correctness of about 70% predictability of dialogue acts in Dia-MoLE turns out to be comparable to human-assigned dialogue acts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-598"
  },
  "gustafson97_eurospeech": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Anette",
     "Larsson"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "K.",
     "Hellman"
    ]
   ],
   "title": "How do system questions influence lexical choices in user answers?",
   "original": "e97_2275",
   "page_count": 4,
   "order": 600,
   "p1": "2275",
   "pn": "2278",
   "abstract": [
    "This paper describes some studies on the effect of the system vocabulary on the lexical choices of the users. There are many theories about human-human dialogues that could be useful in the design of spoken dialogue systems. This paper will give an overview of some of these theories and report the results from two experiments that examines one of these theories, namely lexical entrainment. The first experiment was a small Wizard of Oz-test that simulated a tourist information system with a speech interface, and the second experiment simulated a system with speech recognition that controlled a questionnaire about peoples plans for their vacation. Both experiments show that the subjects mostly adapt their lexical choices to the system questions. Only in less than 5% of the cases did they use an alternative main verb in the answer. These results encourage us to investigate the possibility to add an adaptive language model in the speech recognizer in our dialogue system, where the probabilities for the words used in the system questions are increased.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-599"
  },
  "yuo97_eurospeech": {
   "authors": [
    [
     "Kuo-Hwei",
     "Yuo"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Gaussian mixture models with common principal axes and their application in text-independent speaker identification",
   "original": "e97_2279",
   "page_count": 4,
   "order": 601,
   "p1": "2279",
   "pn": "2282",
   "abstract": [
    "Gaussian mixture models (GMM's) have been demonstrated as one of the powerful statistical methods for speaker identification. In GMM method, the covariance matrix is usually assumed to be diagonal. That means the feature components are relatively uncorrelated. This assumption may not be correct. This paper concentrates on finding an orthogonal speaker-dependent transformation to reduce the correlation between feature components. This transformation is based on the eigenvectors of the within-class scatter matrix which is attained in each stage of iterative training of GMM parameters. Hence the transformation matrix and GMM parameters are both updated in each iteration until the total log-likelihood converges. An experimental evaluation of the proposed method is conducted on a 100-person connected digit database for text independent speaker identification. The experimental result shows a reduction in the error rate by 42% when 7-digit utterances are used for testing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-600"
  },
  "dersch97_eurospeech": {
   "authors": [
    [
     "Dominik R.",
     "Dersch"
    ],
    [
     "Robin W.",
     "King"
    ]
   ],
   "title": "Speaker models designed from complete data sets: a new approach to text-independent speaker verification",
   "original": "e97_2283",
   "page_count": 4,
   "order": 602,
   "p1": "2283",
   "pn": "2286",
   "abstract": [
    "In this paper we present a new approach to text independent speaker verification. Speaker models are created from complete data sets, derived from a set of sentences. A decision on an identity claim is based on the calculation of the mean next neighbour distance between a speaker model and a test utterance. A Vector quantization technique serves to efficiently extract this frame based similarity measure. It is the purpose of this paper to investigate this new approach and test its performance on a large database as a function of a number of parameters, i.e., the number of data vectors in each model and the length of the test utterance. The best results on a set of 108 speakers are 0:93% false rejection rate and 0:98% false acceptance rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-601"
  },
  "vergin97_eurospeech": {
   "authors": [
    [
     "Rivarol",
     "Vergin"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "A double Gaussian mixture modeling approach to speaker recognition",
   "original": "e97_2287",
   "page_count": 4,
   "order": 603,
   "p1": "2287",
   "pn": "2290",
   "abstract": [
    "The first motivation for using Gaussian mixture models for text-independent speaker identification is based on the observation that a linear combination of gaussian basis functions is capable of representing a large class of sample distributions. While this technique gives generally good results, little is known about which specific part of a speech signal best identifies a speaker. This contribution suggests a procedure, based on the Jensen divergence measure, to automatically extract from the input speech signal the part that best contribute to identify a speaker. It is shown, by results obtained, that this technique can significantly increase the performance of a speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-602"
  },
  "afify97b_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "An acoustic subword unit approach to non-linguistic speech feature identification",
   "original": "e97_2291",
   "page_count": 5,
   "order": 604,
   "p1": "2291",
   "pn": "2294",
   "abstract": [
    "Automatic identification of non-linguistic speech features (e.g. the speaker or the language of an utterance) are currently of practical interest. In this paper, we first impose a set of requirements that we think a statistical model used in non-linguistic feature identification should satisfy. Namely, these requirements are capturing both short and long term correlations in addition to maintaining a certain acoustic resolution. A model satisfying these requirements, and in the same time having the attractive feature of requiring no transcribed speech material during training is proposed. Experimental evaluation of the approach in speaker recognition on the TIMIT database is presented, where recognition rates up to 99.2 % are achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-603"
  },
  "tadj97_eurospeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Yu",
     "Fang"
    ]
   ],
   "title": "N-best GMM's for speaker identification",
   "original": "e97_2295",
   "page_count": 4,
   "order": 605,
   "p1": "2295",
   "pn": "2298",
   "abstract": [
    "In this paper, we present and compare two alternative post-processing approaches to generate rules decision for text-dependent speaker identification based on Gaussian Mixture Models (GMM). The first approach, a linear programming method, is used to minimize a cost on a combined scores obtained from the N-Best GMM output probabilities. The second, more heuristic, is based on combination of output score probabilities to generate a decision rules. Statistical tools have been developed to explore the relative importance of these approaches on recognition accuracy. Experiments on Spidre database are presented to show the effects of these two approaches on the speaker identification performance (including the number of the N-Best hypothesis and handset variability). The linear programming approach does not show any improvement, however, a combined statistical approaches has demonstrated an improvement of more than 11% comparing to our standard performance system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-604"
  },
  "gravier97_eurospeech": {
   "authors": [
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Chafic",
     "Mokbel"
    ],
    [
     "Gerard",
     "Chollet"
    ]
   ],
   "title": "Model dependent spectral representations for speaker recognition",
   "original": "e97_2299",
   "page_count": 4,
   "order": 606,
   "p1": "2299",
   "pn": "2302",
   "abstract": [
    "We investigate the use of variable resolution spectral analysis for speaker recognition. The spectral resolution is simply determined by a unique parameter. A speaker can therefore be represented by this parameter and a stochastic model, which means that each speaker is represented in a different acoustic space. For speaker verification tasks, the likelihood ratio compared to a threshold should not depend on the representation space, so that likelihood ratios remain comparable. We experimented different spectral resolution with several classifiers but we had no improvement in the results and the classifiers turned out not to be very sensitive to the different feature sets.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-605"
  },
  "auckenthaler97_eurospeech": {
   "authors": [
    [
     "Roland",
     "Auckenthaler"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Equalizing sub-band error rates in speaker recognition",
   "original": "e97_2303",
   "page_count": 4,
   "order": 607,
   "p1": "2303",
   "pn": "2306",
   "abstract": [
    "Recent work in ASR shows that band splitting, forming multiple paths with recombination at the decision stage, can give recognition accuracy comparable with the conventional full-band approach. One of the many interesting questions with band-splitting relates to the bandwidths of each sub-band, and the use of frequency warping functions such as mel. This paper examines the use of mel and linear frequency scales in the context of band-splitting and speaker recognition. We demonstrate how sub-band error profiles can lead to a new scale, which is between linear and mel, giving both an equalised sub-band error profile and an improved overall recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-606"
  },
  "slomka97_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Slomka"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Automatic gender identification under adverse conditions",
   "original": "e97_2307",
   "page_count": 4,
   "order": 608,
   "p1": "2307",
   "pn": "2310",
   "abstract": [
    "This paper evaluates 63 Automatic Gender Identification (AGI) systems for text-independent clean speech segments, coded speech and speech segments affected by reverberation. The AGI systems contain a Linear Classifier (LC) with inputs from a combination of two average pitch detection methods and paired Gaussian Mixture Models trained with mel-cepstral, autocorrelation, reflection and log area ratios parameterised speech data. An AGI system is built which is able to handle the LPC10, CELP and GSM coders with no significant loss in accuracy and reduce the impact of even severe reverberation by subjecting the training data of the LC with a different room response. Using speech segments with an average duration of 890ms (after silence removal), the best AGI system had an accuracy of 98.5% averaged over all clean and adverse conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-607"
  },
  "lavner97_eurospeech": {
   "authors": [
    [
     "Yizhar",
     "Lavner"
    ],
    [
     "Isak",
     "Gath"
    ],
    [
     "Judith",
     "Rosenhouse"
    ]
   ],
   "title": "Acoustic features and perceptive processes in the identification of familiar voices",
   "original": "e97_2311",
   "page_count": 4,
   "order": 609,
   "p1": "2311",
   "pn": "2314",
   "abstract": [
    "The present study aims at examining the relative importance of various acoustic features as cues to familiar speaker identification. The study also attempts to examine the validity of the prototype model, as the key to human speaker recognition. To this aim 20 speakers were recorded. Their voices were modified using an analysis-synthesis system, which enabled analysis and modification of the glottal waveform, of the pitch, and of the formants. A group of 30 listeners had to identify the speakers in an open-set experiment. The results suggest that on average, the contribution of the vocal tract features is more important than that of the glottal source features. Examination of individual speakers reveals that changes of identical features affect differently the identification of various speakers. This finding suggests that for each speaker a different group of acoustic features serves as cue to the vocal identity, and along with other predictions that were found to be valid, supports the adequacy of the prototype model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-608"
  },
  "rodriguezlinares97_eurospeech": {
   "authors": [
    [
     "Leandro",
     "Rodriguez-Linares"
    ],
    [
     "Carmen",
     "Garcia-Mateo"
    ]
   ],
   "title": "On the use of acoustic segmentation in speaker identification",
   "original": "e97_2315",
   "page_count": 4,
   "order": 610,
   "p1": "2315",
   "pn": "2318",
   "abstract": [
    "In this paper, we present a novel architecture for a Speaker Recognition system over the telephone. The proposed system introduces acoustic information into a HMM-based recognizer. This is achieved by using a phonetic classifier during the training phase. Three broad phonetic classes: voiced frames, unvoiced frames and transitions, are defined. We design speaker templates by the parallel connection of the outputs of the single state HMM´s and by the combination of the single state HMM's into a four state HMM after estimation of the transition probabilities. The results show that this architecture performs better than others without phonetic classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-609"
  },
  "steeneken97_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Speaker recognition by humans and machines",
   "original": "e97_2319",
   "page_count": 4,
   "order": 611,
   "p1": "2319",
   "pn": "2322",
   "abstract": [
    "Speaker recognition with human listeners and with an automatic system were compared. Eight male and eight female speakers were involved. Also the effect of the speech quality was investigated: wide band, telephone band and two signal-to-noise conditions of 6dB and OdB. conditions with noise (SNR +6 dB, 0 dB). For this purpose noise samples were used with a spectrum shaped according to the long-term speech spectrum. The automatic speaker recognition was based on an algorithm which uses a description of the signal by the co-variance in the spectral domain. It was found that for both methods the male speakers are slightly better recognized. One to two words are sufficient, in the wide band condition, for correct subjective recognition. The automatic recognition requires a slightly longer utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-610"
  },
  "kumpf97_eurospeech": {
   "authors": [
    [
     "Karsten",
     "Kumpf"
    ],
    [
     "Robin W.",
     "King"
    ]
   ],
   "title": "Foreign speaker accent classification using phoneme-dependent accent discrimination models and comparisons with human perception benchmarks",
   "original": "e97_2323",
   "page_count": 4,
   "order": 612,
   "p1": "2323",
   "pn": "2326",
   "abstract": [
    "This paper reports on the development of a foreign speaker accent classification system based on phoneme class specific accent discrimination models. This new approach to the problem of automatic accent classification allows fast and reliable prediction of the speaker accents for continuous speech through exploitation of the accent specific information at the phoneme level. The system was trained and evaluated on a corpus representing three speaker groups with native Australian English (AuE), Lebanese Arabic (LA) and South Vietnamese (SV) accents. The speaker accent classification rates achieved by our system come close to the benchmarks set by human listeners.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-611"
  },
  "liu97_eurospeech": {
   "authors": [
    [
     "Li",
     "Liu"
    ],
    [
     "Jialong",
     "He"
    ],
    [
     "Günther",
     "Palm"
    ]
   ],
   "title": "A comparison of human and machine in speaker recognition",
   "original": "e97_2327",
   "page_count": 4,
   "order": 613,
   "p1": "2327",
   "pn": "2330",
   "abstract": [
    "Speaker recognition experiments have been conducted with the publicly available YOHO database to compare the performance of human listeners and computers. Two types of listening experiments have been performed, one is the forced-choice speaker discrimination test which is corresponding to the task of speaker identification. The second experiment of speaker recognition by human listeners is the same-different judgment which is similar to the task of speaker verification. It is shown that the human listeners perform well for the same-different judgment task, but the error rate of speaker discrimination is relatively large. Besides, human listeners are more robust to session variability, while the machine's performance falls off largely when the reference and test utterances are from different recording sessions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-612"
  },
  "goddijn97_eurospeech": {
   "authors": [
    [
     "Simo M. A.",
     "Goddijn"
    ],
    [
     "Guus de",
     "Krom"
    ]
   ],
   "title": "Evaluation of second language learners' pronunciation using hidden Markov models",
   "original": "e97_2331",
   "page_count": 4,
   "order": 614,
   "p1": "2331",
   "pn": "2334",
   "abstract": [
    "In this study, Hidden Markov Models (HMMs) were used to evaluate pronunciation. Native and non-native speakers were asked to pronounce ten Dutch words. Each word was subsequently evaluated by an expert listener. Her main task was to decide whether a word was spoken by a native or a non-native speaker. For each word type, two versions of prototype HMMs were defined: one to be trained on tokens produced by a single native speaker, and another to be trained on tokens produced by a group of native speakers. For testing the different types of HMM, forced recognition was performed using native and non-native judged tokens. We expected that recognition with multi- speaker HMMs would allow a more effective discrimination between native and non-native tokens than recognition with single-speaker models. A comparison of Equal Error Rates partly confirmed this hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-613"
  },
  "eberman97_eurospeech": {
   "authors": [
    [
     "Brian",
     "Eberman"
    ],
    [
     "Pedro J.",
     "Moreno"
    ]
   ],
   "title": "Delta vector taylor series environment compensation for speaker recognition",
   "original": "e97_2335",
   "page_count": 4,
   "order": 615,
   "p1": "2335",
   "pn": "2338",
   "abstract": [
    "The performance of speaker recognition algorithms drops significantly when testing and training acoustic environments differ. This decrease is caused by the statistical mismatch between the statistics representing the speaker and the testing acoustic data. This paper reports our preliminary results on the application of a novel environmental compensation algorithm to the problem of speaker recognition and identification. This new technique, called the Delta Vector Taylor Series (DVTS) approach, improves performance at signal-to-noise ratios below 20dB. The algorithm imposes a model of how the envi- ronment modifies speaker statistics and uses Expectation- Maximization (EM) to solve a joint maximum likelihood formulation for the speaker recognition problem over both the speakers and the environment. We report experimental results on a subset of the TIMIT and NTIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-614"
  },
  "hume97_eurospeech": {
   "authors": [
    [
     "Jonathan",
     "Hume"
    ]
   ],
   "title": "Wavelet-like regression features in the cepstral domain for speaker recognition",
   "original": "e97_2339",
   "page_count": 4,
   "order": 616,
   "p1": "2339",
   "pn": "2342",
   "abstract": [
    "This paper investigates the effects of using multiple time intervals for the calculation of regression coefficients. The technique that we have used is referred to as Wavelet-Like regression (WLR). Using this approach we have found that the underlying time series in the cepstral domain differs slightly depending upon the index of the series, and that by employing a technique that accounts for this, such as WLR, we may achieve an incremental improvement in recognition performance, at negligble extra costs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-615"
  },
  "chengalvarayan97c_eurospeech": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Minimum classification error linear regression (MCELR) for speaker adaptation using HMM with trend functions",
   "original": "e97_2343",
   "page_count": 4,
   "order": 617,
   "p1": "2343",
   "pn": "2346",
   "abstract": [
    "In this paper, we report our recent work on applications of the combined MLLR and MCE approach to estimating the time-varying polynomial Gaussian mean functions in the trended HMM. We call this integrated approach as the minimum classification error linear regression (MCELR), which has been described in this study. The transformation matrices associated with each polynomial coefficients are calculated to minimize the recognition error of the adaptation data and is developed using the gradient descent algorithm. A speech recognizer based on these results is implemented in speaker adaptation experiments using TI46 corpora. Results show that the trended HMM always outperforms the standard HMM and that adaptation of linear regression coefficients is always better when fewer than three adaptation tokens are used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-616"
  },
  "fakotakis97_eurospeech": {
   "authors": [
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "Anastasios",
     "Tsopanoglou"
    ]
   ],
   "title": "A continuous HMM text-independent speaker recognition system based on vowel spotting",
   "original": "e97_2347",
   "page_count": 4,
   "order": 618,
   "p1": "2347",
   "pn": "2350",
   "abstract": [
    "This paper presents a text-independent speaker recognition system based on vowel spotting and Continuous Mixture Hidden Markov Models. The same modeling technique is applied both to vowel spotting and speaker identification/verification procedures. The system is evaluated on two speech databases, TIMIT and NTIMIT, resulting in high accuracy rates. Closed-set identification accuracy on TIMIT and NTIMIT databases is 98.09% and 59.32%, respectively. Concerning the verification experiments, accuracy of 98.28% for TIMIT, and 83.04% for NTIMIT databases is obtained. The nearly real time response of the classification procedure, the low memory requirements and the small amount of training and testing data are some of the additional advantages of the proposed speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-617"
  },
  "koolwaaij97_eurospeech": {
   "authors": [
    [
     "Johan W.",
     "Koolwaaij"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "On the independence of digits in connected digit strings",
   "original": "e97_2351",
   "page_count": 4,
   "order": 619,
   "p1": "2351",
   "pn": "2354",
   "abstract": [
    "One of the frequently used assumptions in Speaker Verification is that two speech segments (phonemes, subwords, words) are considered to be independent. And therefore, the log-likelihood of a test utterance is just the sum of the log-likelihoods of the speech segments in that utterance. This paper reports about cases in which this observation-independence assumption seems to be violated, namely for those test utterances which call a certain speech model more than once. For example, a pin code which contains a non-unique digit set performs worse in verification than a pin code which consists of four different digits. Results illustrate that violating the independence assumption too much might result in increasing EERs while more information (in form of digits) is added to the test utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-618"
  },
  "koolwaaij97b_eurospeech": {
   "authors": [
    [
     "Johan W.",
     "Koolwaaij"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "A new procedure for classifying speakers in speaker verification systems",
   "original": "e97_2355",
   "page_count": 4,
   "order": 620,
   "p1": "2355",
   "pn": "2358",
   "abstract": [
    "In this paper we propose a new measure to classify speakers with respect to their behaviour in speaker recognition systems. Taking the proposal made by EAGLES as a point of departure we show that it fails to yield results that are consistent between closely related speaker recognition methods and between different amounts of speech available for the recognition task. We show that measures based on a straight- forward confusion matrices, that take only the 1-best classification into account, cannot result in consistent classifications. As an alternative we propose a measure based on n-best scores in a speaker identification paradigm, and show that it yields more consistent performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-619"
  },
  "montacie97b_eurospeech": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "Sound channel video indexing",
   "original": "e97_2359",
   "page_count": 4,
   "order": 621,
   "p1": "2359",
   "pn": "2362",
   "abstract": [
    "We present in this paper preliminary results using speaker recognition and speech recognition techniques, designed at LIP6, to index audio data of video movies. The assumption that only one person is speaking at the same time is made. In a first approach, we work on dialogue unsupervised indexing using speaker recognition techniques. For this purpose, we develop Silence/Noise/Music/Speech detection algorithms in order to cut audio data in segments that we hope to be homogeneous in terms of speaker appartenance. In a second approach, we develop a supervised audio data indexing method knowing the movie script.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-620"
  },
  "hernando97b_eurospeech": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "CDHMM speaker recognition by means of frequency filtering of filter-bank energies",
   "original": "e97_2363",
   "page_count": 4,
   "order": 622,
   "p1": "2363",
   "pn": "2366",
   "abstract": [
    "Recently, the set of spectral parameters of every speech frame that result from filtering the frequency sequence of mel-scaled filter-bank energies with a simple first-order high-pass FIR filter have proved to be an efficient speech representation in terms of both speech recognition rate and computational load. In this paper, we apply the same technique to speaker recognition. Frequency filtering approximately equalizes the cepstrum variance, enhancing the oscillations of the spectral envelope curve that are most effective for discriminating between speakers. In this way, even better speaker identification results than using conventional mel-cepstrum were observed in continuous observation Gaussian density HMM, especially in noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-621"
  },
  "humphries97_eurospeech": {
   "authors": [
    [
     "J. J.",
     "Humphries"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Using accent-specific pronunciation modelling for improved large vocabulary continuous speech recognition",
   "original": "e97_2367",
   "page_count": 4,
   "order": 623,
   "p1": "2367",
   "pn": "2370",
   "abstract": [
    "A method of modelling accent-specific pronunciation variations is presented. Speech from an unseen accent group is phonetically transcribed such that pronunciation variations may be derived. These context-dependent variations are clustered in decision trees which are used as a model of the pronunciation variation associated with this new accent group. The trees are then used to build a new pronunciation dictionary for use during the recognition process. Experiments are presented, based on Wall Street Journal and WSJCAM0 corpora, for the recognition of American speakers using a British English recogniser. Speaker independent as well as speaker dependent adaptation scenarios are presented, giving up to 20% reduction in word error rate. A linguistic analysis of the pronunciation model is presented and finally the technique is combined with maximum likelihood linear regression, a well proven acoustic adaptation technique, yielding further improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-622"
  },
  "potamianos97b_eurospeech": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Sungbok",
     "Lee"
    ]
   ],
   "title": "Automatic speech recognition for children",
   "original": "e97_2371",
   "page_count": 4,
   "order": 624,
   "p1": "2371",
   "pn": "2374",
   "abstract": [
    "In this paper, the acoustic and linguistic characteristics of children speech are investigated in the context of automatic speech recognition. Acoustic variability is identified as a major hurdle in building high performance ASR applications for children. A simple speaker normalization algorithm combining frequency warping and spectral shaping introduced in [5] is shown to reduce acoustic variability and significantly improve recognition performance for children speakers (by 25{ 45%). Age-dependent acoustic modeling further reduces word error rate by 10%. Piece-wise linear and phoneme-dependent frequency warping algorithms are proposed for reducing acoustic mismatch between the children and adult acoustic spaces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-623"
  },
  "teixeira97_eurospeech": {
   "authors": [
    [
     "Carlos",
     "Teixeira"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Antonio",
     "Serralheiro"
    ]
   ],
   "title": "Recognition of non-native accents",
   "original": "e97_2375",
   "page_count": 4,
   "order": 625,
   "p1": "2375",
   "pn": "2378",
   "abstract": [
    "This paper deals with the problem of non-native accents in speech recognition. Reference tests were performed using whole-word and sub-word models trained either with a native accent or a pool of native and non-native accents. The results seem to indicate that the use of phonetic transcriptions for each specific accent may improve recognition scores with sub-word models. A data-driven process is used to derive transcription lattices. The recognition scores thus obtained were encouraging.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-624"
  },
  "finke97_eurospeech": {
   "authors": [
    [
     "Michael",
     "Finke"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Speaking mode dependent pronunciation modeling in large vocabulary conversational speech recognition",
   "original": "e97_2379",
   "page_count": 4,
   "order": 626,
   "p1": "2379",
   "pn": "2382",
   "abstract": [
    "In spontaneous conversational speech there is a large amount of variability due to accents, speaking styles and speaking rates (also known as the speaking mode) [3]. Because current recognition systems usually use only a relatively small number of pronunciation variants for the words in their dictionaries, the amount of variability that can be modeled is limited. Increasing the number of variants per dictionary entry is the obvious solution. Unfortunately, this also means increasing the confusability between the dictionary entries, and thus often leads to an actual performance decrease. In this paper we present a framework for speaking mode dependent pronunciation modeling. The probability of encountering pronunciation variants is defined to be a function of the speaking style. The probability function is learned through decision trees from rule based generated pronunciation variants as observed on the Switchboard corpus. The framework is successfully applied to increase the performance of our state-of-the-art Janus Recognition Toolkit Switchboard recognizer significantly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-625"
  },
  "shriberg97_eurospeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Rebecca",
     "Bates"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "A prosody only decision-tree model for disfluency detection",
   "original": "e97_2383",
   "page_count": 4,
   "order": 627,
   "p1": "2383",
   "pn": "2386",
   "abstract": [
    "Speech disfluencies (filled pauses, repetitions, repairs, and false starts) are pervasive in spontaneous speech. The ability to detect and correct disfluencies automatically is important for effective natural language understanding, as well as to improve speech models in general. Previous approaches to disfluency detection have relied heavily on lexical information, which makes them less applicable when word recognition is unreliable. We have developed a disfluency detection method using decision tree classifiers that use only local and automatically extracted prosodic features. Because the model doesn't rely on lexical information, it is widely applicable even when word recognition is unreliable. The model performed significantly better than chance at detecting four disfluency types. It also outperformed a language model in the detection of false starts, given the correct transcription. Combining the prosody model with a specialized language model improved accuracy over either model alone for the detection of false starts. Results suggest that a prosody-only model can aid the automatic detection of disfluencies in spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-626"
  },
  "boughazale97_eurospeech": {
   "authors": [
    [
     "Sahar E.",
     "Bou-Ghazale"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A novel training approach for improving speech recognition under adverse stressful conditions",
   "original": "e97_2387",
   "page_count": 4,
   "order": 628,
   "p1": "2387",
   "pn": "2390",
   "abstract": [
    "This paper presents a new training approach for improving recognition of speech under emotional and environmental stress. The proposed approach consists of training a speech recognizer with synthetically generated speech under each stress condition using stress perturbation models previously formulated in [4, 1]. The perturbation models were previously formulated to statistically model the parameter variations under angry, loud, and Lombard effect and were employed in an analysis-synthesis scheme for generating stressed synthetic speech from isolated neutral speech. In this paper, two training approaches employing the synthetically generated stressed speech are presented consisting of : speaker-independent, and speaker-adaptive training methods. Both approaches outperform neutral trained recognizers when tested with angry, loud, and Lombard effect speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-627"
  },
  "fissore97_eurospeech": {
   "authors": [
    [
     "L.",
     "Fissore"
    ],
    [
     "Giorgio",
     "Micca"
    ],
    [
     "C.",
     "Vair"
    ]
   ],
   "title": "Methods for microphone equalization in speech recognition",
   "original": "e97_2415",
   "page_count": 4,
   "order": 629,
   "p1": "2415",
   "pn": "2418",
   "abstract": [
    "This paper presents a review of current research carried on at various laboratories aiming to increase the robustness of speech recognition systems to channel and microphone variations. A comparative analysis of several techniques, used in recent studies on microphone-independence, are discussed and compared: these include Cepstral High- Pass Filtering, Cepstral-Mean Normalization, Ratz algorithm and Bayesian learning. Also, some results obtained at CSELT labs using the methods above mentioned are reported, specifically addressing the issue of robustness of ASR systems to microphone variations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-628"
  },
  "nakamura97c_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Room acoustics and reverberation: impact on hands-free recognition",
   "original": "e97_2419",
   "page_count": 4,
   "order": 630,
   "p1": "2419",
   "pn": "2422",
   "abstract": [
    "Hands-free speech recognition is a very important issue for a natural human machine interface. The distant talking speech in real environments is distorted by noise and reverberation of the room. This paper introduces characteristics of the room acoustical distortion and their influences on speech recognition accuracy. Then the paper tries to give a prospect of the solution based on previous studies and our research efforts. Especially a microphone array based-method and a model adaptation method are discussed. The microphone array can reduce the influences of the acoustical distortion by beam-forming. On the other hand, the model adaptation method can estimate the acoustical transfer function and adapt the speech models against the distorted observation signals. Furthermore, this paper also addresses hands-free speech recognition by incorporating automatic lip reading.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-629"
  },
  "faucon97_eurospeech": {
   "authors": [
    [
     "Gerard",
     "Faucon"
    ],
    [
     "Regine Le",
     "Bouquin-Jeannes"
    ]
   ],
   "title": "Echo and noise reduction for hands-free terminals - state of the art -",
   "original": "e97_2423",
   "page_count": 4,
   "order": 631,
   "p1": "2423",
   "pn": "2426",
   "abstract": [
    "This paper deals with speech enhancement in hands-free telecommunication systems. We summarize and discuss recent results on methods combining the two major problems encountered in such systems - acoustic echo cancellation and noise reduction -. Single microphone and two-microphone approaches are addressed. Finally, we outline the limitations of the different techniques and propose some prospects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-630"
  },
  "haebumbach97_eurospeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Robust speech recognition for wireless networks and mobile telephony",
   "original": "e97_2427",
   "page_count": 4,
   "order": 632,
   "p1": "2427",
   "pn": "2430",
   "abstract": [
    "The increased popularity of mobile telephony introduces both challenges and opportunitites for automatic speech recognition. ASR offers ways to simplify the use of mobile phones, notably in hands- and eyes-busy situations. However, the acoustic environment can be severely degraded and the wireless network may add additional distortions to the speech signal. This paper gives an overview of the sources of degradation and attempts to robust speech recognition for mobile communications. Emphasis is placed on approaches which are suitable for implementation in mobile terminals. Two example applications are described which illustrate the robustness issues and design considerations typical of low-cost noisy speech recognition: voice-dialling in a GSM phone and hands-free digit recognition in the car.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-631"
  },
  "compernolle97_eurospeech": {
   "authors": [
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Speech recognition in the car from phone dialing to car navigation",
   "original": "e97_2431",
   "page_count": 4,
   "order": 633,
   "p1": "2431",
   "pn": "2434",
   "abstract": [
    "This paper focuses on the evolving demands for speech recognition in the car and its corresponding impact on algorithmic and technological development. Till today the major demand for speech recognition in the car was related to hands free operation of the telephone. This functionality could be provided in a satisfactory way with a word based system, at the same time allowing for more simplistic noise suppression algorithms. Fully new speech recognition systems are required today to be able to cope with the demands for voice control of car navigation systems. These systems require noise robust phoneme based large vocabulary recognition systems and a much more advanced user interface. The very large perplexity of a car navigation task requires inherent embodiment of a spelling recognizer. Hardware and software design for this new application must also be tackled from the point of view that it will be one, though central part of a fully integrated speech control inside the car.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-632"
  },
  "williams97b_eurospeech": {
   "authors": [
    [
     "Briony",
     "Williams"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "A keyvowel approach to the synthesis of regional accents of English",
   "original": "e97_2435",
   "page_count": 4,
   "order": 634,
   "p1": "2435",
   "pn": "2438",
   "abstract": [
    "Most English text-to-speech synthesisers offer one of only two accents: General American or RP. Developing a new accent is laborious, since it is not possible to choose one accent as a base form and systematically translate to others. We use the approach of Wells ([1]), categorising vowels in terms of  keywords that encode classes of words. Thus it is unnecessary to use a phonemic transcription in either the development or the execution of a synthesiser. The \"keyvowel\" system can be used throughout the synthesis system, avoiding the need to make accent-specific changes manually. The same linguistic resources can be re-used for each new accent. More fundamentally, the keyvowel system functions as a meta-accent that subsumes vowel-related information in all accents of English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-633"
  },
  "ferencz97_eurospeech": {
   "authors": [
    [
     "Attila",
     "Ferencz"
    ],
    [
     "Radu",
     "Arsinte"
    ],
    [
     "Istvan",
     "Nagy"
    ],
    [
     "Teodora",
     "Ratiu"
    ],
    [
     "Maria",
     "Ferencz"
    ],
    [
     "Gavril",
     "Toderean"
    ],
    [
     "Diana",
     "Zaiu"
    ],
    [
     "Tunde-Csilla",
     "Kovacs"
    ],
    [
     "Lajos",
     "Simon"
    ]
   ],
   "title": "Experimental implementation of pitch-synchronous synthesis methods for the ROMVOX text-to-speech system",
   "original": "e97_2439",
   "page_count": 4,
   "order": 635,
   "p1": "2439",
   "pn": "2442",
   "abstract": [
    "The LPC-MPE synthesis method is an alternative method used for obtaining a better quality of the generated vocal signal, that can be easily implemented in vocal signal coding-decoding systems. Using the method in text-to-speech systems is more difficult because of the modification that must be done on the synthesized vocal signal in order to superimpose prosodical effects. This paper presents our steps in this direction, some researches and experimental results obtained for adapting the system to the pitch-synchronous LPC-MPE method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-634"
  },
  "mobius97_eurospeech": {
   "authors": [
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Richard",
     "Sproat"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Joseph P.",
     "Olive"
    ]
   ],
   "title": "The bell labs German text-to-speech system: an overview",
   "original": "e97_2443",
   "page_count": 4,
   "order": 636,
   "p1": "2443",
   "pn": "2446",
   "abstract": [
    "In this paper we present an overview of the German version of the Bell Labs text-to-speech system, a high-quality concatenative synthesis system with extensive text analysis capabilities. We discuss problems of text analysis, and our solutions to these problems, including: the integration of text normalization tasks into linguistic text analysis; the capability to morphologically analyze compounds and unseen words; name analysis and pronunciation. We briefly describe the prosodic components of the text-to-speech system and their underlying duration and intonation models. Finally, the phonetically motivated structure of the acoustic inventory is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-635"
  },
  "fitt97_eurospeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ]
   ],
   "title": "The generation of regional pronunciations of English for speech synthesis",
   "original": "e97_2447",
   "page_count": 4,
   "order": 637,
   "p1": "2447",
   "pn": "2450",
   "abstract": [
    "Most speech synthesisers and recognisers for English currently use pronunciation lexicons in standard British or American accents, but as use of speech technology grows there will be more demand for the incorporation of regional accents. This paper describes the use of rules to transform existing lexicons of standard British and American pronunciations to a set of regional British and American accents. The paper briefly discusses some features describes of the regional accents in the project, and the framework used for generating pronunciations. Certain theoretical and practical problems are highlighted; for some of these, solutions are suggested, but it is shown that some difficulties cannot be resolved by automatic rules. However, although the method described cannot produce phonetic transcriptions with 100% accuracy, it is more accurate than using letter-to-sound rules, and faster than producing transcriptions by hand.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-636"
  },
  "pavlova97_eurospeech": {
   "authors": [
    [
     "Elena",
     "Pavlova"
    ],
    [
     "Yuri",
     "Pavlov"
    ],
    [
     "Richard",
     "Sproat"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Bell laboratories Russian text-to-speech system",
   "original": "e97_2451",
   "page_count": 4,
   "order": 638,
   "p1": "2451",
   "pn": "2454",
   "abstract": [
    "This paper describes the Bell Labs Russian text-to-speech system, a concatenative system with extensive text-analysis capabilities. The construction of Russian-specific modules will be discussed, including the text-analysis module, the acoustic inventory, the duration module, and the intonation module.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-637"
  },
  "bonafonte97_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Ignasi",
     "Esquerra"
    ],
    [
     "Albert",
     "Febrer"
    ],
    [
     "Francesc",
     "Vallverdu"
    ]
   ],
   "title": "A bilingual text-to-speech system in Spanish and catalan",
   "original": "e97_2455",
   "page_count": 4,
   "order": 639,
   "p1": "2455",
   "pn": "2458",
   "abstract": [
    "This paper summarises the text-to-speech system that has been developed during the last years in the Speech Group of the Universitat Politccnica de Catalunya (UPC). The paper emphasises the parts of the system which are language dependent: phonetic transcription, prosodic module, and synthesis units database. One particularity of the system is the fact of being bilingual, i.e., the system is able to speak either in Spanish or in Catalan. Some effort has been done to allow the reading of bilingual texts and to reduce the computational resources needed. In particular, the Spanish and Catalan speech databases are merged to reduce the memory requirements and the development effort. The system is being used by disabled people which suffer from oral disorders. In order to give variability to the voices some experiments have been done in voice transformation using the TD-PSOLA algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-638"
  },
  "cremelie97_eurospeech": {
   "authors": [
    [
     "Nick",
     "Cremelie"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Automatic rule-based generation of word pronunciation networks",
   "original": "e97_2459",
   "page_count": 4,
   "order": 640,
   "p1": "2459",
   "pn": "2462",
   "abstract": [
    "In this paper a method for generating word pronunciation networks for speech recognition is proposed. The networks incorporate different acceptable pronunciation variants for each word. These variants are determined by applying pronunciation rules to the standard pronunciation of the words. Instead of a manual search, an automatic learning procedure is used to compose a sensible set of rules. The learning algorithm compairs the standard pronunciation of each utterance in a training corpus with its auditory transcription (i.e. 'how should it be pronounced' versus 'how was it actually pronounced'). It is shown that the latter transcription can be constructed with the assistance of a speech recognizer. Experimental results on a Dutch database and on TIMIT demonstrate that the pronunciation networks reduce the word error rate significantly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-639"
  },
  "elvira97_eurospeech": {
   "authors": [
    [
     "Jose Maria",
     "Elvira"
    ],
    [
     "Juan Carlos",
     "Torrecilla"
    ],
    [
     "Javier",
     "Caminero"
    ]
   ],
   "title": "Creating user defined new vocabularies for voice dialing",
   "original": "e97_2463",
   "page_count": 4,
   "order": 641,
   "p1": "2463",
   "pn": "2466",
   "abstract": [
    "This paper introduces a new approach for generation of phonetic transcriptions for voice dialing applications. where on-line construction of user vocabularies is mandatory. The proposed method allows adaptive selection of new transcriptions requiring much less speech utterances for system training than other approaches. The new approach is compared to other classical approaches showing a clear improvement on performance and efficiency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-640"
  },
  "ravishankar97b_eurospeech": {
   "authors": [
    [
     "Mosur",
     "Ravishankar"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Automatic generation of context-dependent pronunciations",
   "original": "e97_2467",
   "page_count": 5,
   "order": 642,
   "p1": "2467",
   "pn": "2470",
   "abstract": [
    "We describe experiments in modelling the dynamics of fluent speech in which word pronunciations are modified by neighbouring context. Based on all-phone decoding of large Volumes of training data, we automatically derive new word pronunciation, and context-dependent transformation rules for phone sequences. In contrast to existing techniques, the rules can be applied even to words not in the training set, and across word boundaries, thus modelling context-dependent behavior. We use the technique on the Wall Street Journal (WSJ) training data and apply the new pronunciations and rules to WSJ and broadcast news tests. The changes correct a significant portion of the errors they could potentially correct. But the transformations introduce a comparable number of new errors, indicating that perhaps stronger constraints on the application of such rules are needed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-641"
  },
  "fukada97_eurospeech": {
   "authors": [
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Automatic generation of a pronunciation dictionary based on a pronunciation network",
   "original": "e97_2471",
   "page_count": 4,
   "order": 643,
   "p1": "2471",
   "pn": "2474",
   "abstract": [
    "In this paper, we propose a method for automatically generating a pronunciation dictionary based on a pronunciation neural network that can predict plausible pronunciations (alternative pronunciations) from the canonical pronunciation. This method can generate multiple forms of alternative pronunciations using the pronunciation network for words that only occur a few times in the database and even for unseen words. Experimental results on spontaneous speech show that the automatically-derived pronunciation dictionaries give consistently higher recognition rates and require less computational time for recognition than a conventional dictionary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-642"
  },
  "jost97_eurospeech": {
   "authors": [
    [
     "Uwe",
     "Jost"
    ],
    [
     "Henrik",
     "Heine"
    ],
    [
     "Gunnar",
     "Evermann"
    ]
   ],
   "title": "What is wrong with the lexicon - an attempt to model pronunciations probabilistically",
   "original": "e97_2475",
   "page_count": 4,
   "order": 644,
   "p1": "2475",
   "pn": "2478",
   "abstract": [
    "We motivate the integration of a probabilistic pronunciation model into a system for recognizing spontaneous speech and propose a possible architecture of such a model. In order to develop an environment for experiments, a simplified version employing constrained phone recognition and discrete syllable-size HMM subword units was implemented and evaluated. Although the results are still significantly worse than those achieved by our \"conventional\" word recognizer, they are encouraging given that the experimental system is only a coarse approximation of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-643"
  },
  "markey97_eurospeech": {
   "authors": [
    [
     "Kevin L.",
     "Markey"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Lexical tuning based on triphone confidence estimation",
   "original": "e97_2479",
   "page_count": 4,
   "order": 645,
   "p1": "2479",
   "pn": "2482",
   "abstract": [
    "We propose and test a practical means of finding poor pronunciations and missing variants for large lexicons. We do so by statistically assessing the confidence of each phone in each pronunciation and comparing it with the statistical distribution of the same confidence metric for corresponding phones over the entire training corpus. A phone is targeted for correction for each word in which its mean score is significantly less than the phone's mean score over the entire training corpus. Neighboring phones are also reviewed for their contribution to the target phone's poor score. Thus far, we have experimented with this technique by manually correcting the pronunciation. In experiments with Wall Street Journal and dictated physical examination corpora, word error rates were reduced commensurate with the number of dictionary entries whose pronunciations were corrected as result of this process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-644"
  },
  "berthommier97_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Georg",
     "Meyer"
    ]
   ],
   "title": "Improving of amplitude modulation maps for F0-dependent segregation of harmonic sounds",
   "original": "e97_2483",
   "page_count": 4,
   "order": 646,
   "p1": "2483",
   "pn": "2486",
   "abstract": [
    "The AM-map model [ 1 ] can. be improved by adding two supplementary integration stages: the pooled map and the identification map. The pooled map's representation corresponds to a systematic bottom-up grouping of the first harmonics extracted at the level of the primary AM map. The identitication map's representation corresponds to a classification of spectra segregated along the pitch axis. This labelling allows selection at the pooled map level of the two salient vowels according to the distribution of energy across the pitch axis. The selected labels are those associated with the higher peaks. During this selection stage, FOs are not given. Simulations show that the model is able to separate spectra according to FO differences. The model therefore predicts qualitatively (1) the ability of listeners to segregate concurrent vowels, and (2) the effects of vowels' duration and relative level on segregation performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-645"
  },
  "kortekaas97_eurospeech": {
   "authors": [
    [
     "Reinier",
     "Kortekaas"
    ],
    [
     "Armin",
     "Kohlrausch"
    ]
   ],
   "title": "Psychophysical evaluation of PSOLA: natural versus synthetic speech",
   "original": "e97_2487",
   "page_count": 5,
   "order": 647,
   "p1": "2487",
   "pn": "2490",
   "abstract": [
    "This paper presents the results of psychophysical experiments dealing with pitch-marker positioning within the Pitch Synchronous OverLap and Add (PSOLA) framework. Sustained natural vowels were PSOLA-modified in fundamental frequency. The experiments were aimed at determining the auditory sensitivity to (1) deterministic shifts of either all or single pitch markers within a sequence, and (2) random shifts of all pitch markers (\"jitter\"). As for deterministic shifts of all pitch markers, the results were in reasonable agreement with results obtained previously for synthetic formant signals. For deterministic shifts of single pitch markers, thresholds depended on position in the sequence. Detection thresholds for jittered shifts were comparable to thresholds for detecting jitter in pulse trains. The ranking of the thresholds for these three conditions indicated that the auditory system is more sensitive to dynamic (modulation) cues rather than to static (timbral) cues arising from shifts in pitch-marker positioning.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-646"
  },
  "lublinskaja97_eurospeech": {
   "authors": [
    [
     "Valentina V.",
     "Lublinskaja"
    ],
    [
     "Inna V.",
     "Koroleva"
    ],
    [
     "A.N.",
     "Kornev"
    ],
    [
     "Elena V.",
     "Iagounova"
    ]
   ],
   "title": "Perception of noised words by normal children and children with speech and language impairments",
   "original": "e97_2491",
   "page_count": 4,
   "order": 648,
   "p1": "2491",
   "pn": "2494",
   "abstract": [
    "The recognition of noised words by 4-7 years old children with normal speech (NS) and with speech and language impairments (SLI) was studied. It was shown that children in both groups have more mistakes and more long reaction time than adults. Moreover, SLI children had worse performance than NS. In both groups older ( 5 years) children recognized noised words better than the younger (< 5 years) ones. NS children perceived the words which were acquired at the early age with less mistakes than the words acquired at the older age. The relations between the development of speech perception, noise resistance and speech production are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-647"
  },
  "meyer97b_eurospeech": {
   "authors": [
    [
     "Georg F.",
     "Meyer"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Modelling the perception of simultaneous semi-vowels",
   "original": "e97_2495",
   "page_count": 4,
   "order": 649,
   "p1": "2495",
   "pn": "2498",
   "abstract": [
    "A model that is able to predict human performance in a simultaneous glide recognition task is described. The model combines a primitive, F0 guided, segregation stage and a schema driven stage with a heuristic that models whether listeners perceive a single or two simultaneous sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-648"
  },
  "perdigao97_eurospeech": {
   "authors": [
    [
     "Fernando S.",
     "Perdigao"
    ],
    [
     "Luis V.",
     "Sa"
    ]
   ],
   "title": "Properties of auditory model representations",
   "original": "e97_2499",
   "page_count": 4,
   "order": 650,
   "p1": "2499",
   "pn": "2502",
   "abstract": [
    "We address the problem of robustness of auditory models as front ends for speech recognition. Auditory models have been referred as superior front ends when speech is corrupted by noise or linear filtering, but there is not yet a deep understanding of its functioning. We analyze some commonly used auditory models and show that they present some interesting properties which are useful for robust speech recognition. In our view, the short-time adaptation provided by hair cell models is a key factor for this robustness. A disadvantage of auditory models is that the distributions of the obtained features are not well represented by gaussian pdfs. We discuss the problem of parameter transformation in order to use a standard recognizer based on CDHMMs with gaussian pdfs and present some digit recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-649"
  },
  "marta97_eurospeech": {
   "authors": [
    [
     "Eduardo Sa",
     "Marta"
    ],
    [
     "Luis Vieira de",
     "Sa"
    ]
   ],
   "title": "Impact of \"ascending sequence\" AI (auditory primary cortex) cells on stop consonant perception",
   "original": "e97_2503",
   "page_count": 4,
   "order": 651,
   "p1": "2503",
   "pn": "2506",
   "abstract": [
    "The existence of multiple information carriers for a single phonemic distinction is well evident in studies of auditory and visual information integration for speech perception. Given the highly non-homogeneous nature of the auditorily-represented information carriers, we are applying the same principle withinthe auditory domain. Based on psychophysical experiments we have hypothesized that firing of \"ascending sequence\" cells in the primary auditory cortex is a primary information carrier for LABIAL place in stop-consonant discrimination. Partial implementation of a fuzzy-logic model for the firing of these cells, combined with a model for one other, secondary, information carrier, has yielded 1% errors in discrimination of /p/ vs. /t/ or /k/ in a \"E-set\", Portuguese research CV database. Exactly the same partial model, applied to /b/ vs. /d/ discrimination in an American English spelled letters database (ISOLET-1) yielded just 5% errors, providing strong evidence for the role of these cells in stop consonant discrimination across languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-650"
  },
  "santen97c_eurospeech": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Combinatorial issues in text-to-speech synthesis",
   "original": "e97_2507",
   "page_count": 4,
   "order": 652,
   "p1": "2507",
   "pn": "2510",
   "abstract": [
    "Enhanced storage capacities and new learning algorithms have increased the role of text and speech training data bases in the construction of text-to-speech systems. It has become apparent, however, that not always learning algorithms are available that have strong generalization capabilities - the ability to generalize from cases seen in the training data base to new cases encountered during TTS operation. This makes it important to measure and understand the degree of coverage of the input domain of a text-to-speech system (usually, the entire language) by a given training data base. The goal of this paper is to investigate the feasibility of coverage in several domains of interest for TTS. It is shown that, as a result of the combinatorics of language, coverage is typically quite disappointing. This puts a premium on the generalization capability of learning algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-651"
  },
  "boeffard97_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Boeffard"
    ],
    [
     "F.",
     "Emerard"
    ]
   ],
   "title": "Application-dependent prosodic models for text-to-speech synthesis and automatic design of learning database corpus using genetic algorithm",
   "original": "e97_2511",
   "page_count": 4,
   "order": 653,
   "p1": "2511",
   "pn": "2514",
   "abstract": [
    "The quality improvement of a Text-To-Speech synthesis system is usually considered as the arduous task of converting any text into speech. This paper is related to the work led at CNET in building application-oriented text-to-speech systems. For a majority of vocal services, the delivered messages have a strong syntactic constraint and use a limited vocabulary. We consider that, with our system, the most hopeful improvements in the overall quality of the speech synthesis signal are linked to the linguistic and prosodic processing. Discarding here segmental problems of the synthetic speech signal, the actual prosodic patterns are judged as too monotonous to allow a great diversity of vocal services. Thus, the actual effort deals with the development of automatic systems to adapt the parameters of statistical prosodic models to a specific speaker's voice under the constraint of a limited amount of different syntactic structures. This work presents an automatic system to build \"optimal\" training databases used to learn the models' parameters. The formulation of the problem is defined as a set covering problem and is solved using genetic algorithms. Both an objective and a subjective evaluation show the usefulness of this approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-652"
  },
  "lopezgonzalo97_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lopez-Gonzalo"
    ],
    [
     "Jose M.",
     "Rodriguez-Garcia"
    ],
    [
     "Luis",
     "Hernandez-Gomez"
    ],
    [
     "Juan M.",
     "Villar"
    ]
   ],
   "title": "Automatic corpus-based training of rules for prosodic generation in text-to-speech",
   "original": "e97_2515",
   "page_count": 4,
   "order": 654,
   "p1": "2515",
   "pn": "2518",
   "abstract": [
    "In this paper, we discuss a methodology for automatic prosodic modeling in Text-to-Speech (TTS) systems. The proposed methodology can be seen as a data-driven strategy to train prosodic rules from the automatic analysis of a specific text and its related speech material. Therefore, our corpus-based training procedure is based on an automatic linguistic analysis of the text and on an acoustic analysis of the speech using automatic speech recognition techniques. Together with the automatic derivation of prosodic rules, our method can be easily extended to obtain specific grammar categories suitable for accurate prosodic modeling of specific tasks. Evaluation results over two different applications and speaker styles, reveal that the proposed automatic prosodic generation procedure is able to provide a noticeable increase in naturalness when adapting TTS system to a new speaker and a new speaking style.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-653"
  },
  "kim97f_eurospeech": {
   "authors": [
    [
     "Eun-Kyoung",
     "Kim"
    ],
    [
     "Sangho",
     "Lee"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Hidden Markov model based voice conversion using dynamic characteristics of speaker",
   "original": "e97_2519",
   "page_count": 4,
   "order": 655,
   "p1": "2519",
   "pn": "2522",
   "abstract": [
    "This paper proposes a new voice conversion technique based on hidden Markov model (HMM) for modeling of speaker's dynamic characteristics. The basic idea of this technique is to use state transition probability as speaker's dynamic characteristics and have conversion rule at each state of HMM. A couple of methods is developed for creating state-dependent conversion rule. One uses source speaker's spectral dynamics and the other uses target speaker's. The experimental results showed that the proposed methods have better performance than conventional VQ-method in both objective and subjective tests. The comparison of our two methods showed that the method using target speaker's dynamics is superior in listening test and produces more natural sound.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-654"
  },
  "yoshimura97_eurospeech": {
   "authors": [
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Speaker interpolation in HMM-based speech synthesis system",
   "original": "e97_2523",
   "page_count": 4,
   "order": 656,
   "p1": "2523",
   "pn": "2526",
   "abstract": [
    "This paper describes an approach tovoice characteristics conversion for HMM-based text-to-speech synthesis system using speaker interpolation. An HMM interpolation technique is derived from a probabilistic distance measure for HMMs, and used to synthesize speech with untrained speaker's characteristics by interpolating HMM parameters among some representative speakers' HMM sets. The results of subjective experiments show that we can gradually change the characteristics of synthesized speech from one's to the other's by changing the interpolation ratio.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-655"
  },
  "darsinos97_eurospeech": {
   "authors": [
    [
     "Vassilios",
     "Darsinos"
    ],
    [
     "Dimitrios",
     "Galanis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Designing a speaker adaptable formant-based text-to-speech system",
   "original": "e97_2527",
   "page_count": 4,
   "order": 657,
   "p1": "2527",
   "pn": "2530",
   "abstract": [
    "First results of the efforts to build a formant Text- to-Speech system, capable to change its characteristics and imitate a specific speaker's voice, are presented. The designing procedure is based on the automatic analysis of phonetically labelled utterances of the speaker, for the automatic extraction of formant values, voice source characteristics and coarticulation rules. All these parameters are necessary to control the synthesizer. The results of preliminary listening tests are encouraging, indicating that the system can serve as an efficient tool for the automatic analysis of speaker voice characteristics and speaker imitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-656"
  },
  "maragos97_eurospeech": {
   "authors": [
    [
     "Petros",
     "Maragos"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "On using fractal features of speech sounds in automatic speech recognition",
   "original": "e97_2531",
   "page_count": 4,
   "order": 658,
   "p1": "2531",
   "pn": "2534",
   "abstract": [
    "The dynamics of air ow during speech production may often result into some small or large degree of turbulence. In this paper, we quantify the geometry of speech turbulence as reflected in the fragmentation of the time signal by using fractal models. We describe an efficient algorithm for estimating the short-time fractal dimension of speech signals based on multiscale morphological filtering and discuss its potential for phonetic classification. We also report experimental results on using the short- time fractal dimension of speech signals at multiple scales as additional features in an automatic speech recognition system using hidden Markov models, which provides a modest improvement in speech recognition performance. dimensions of speech segments as additional features in an automatic speech recognition system based on hidden Markov models (HMMs) and found them to offer a modest improvement to the speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-657"
  },
  "richards97_eurospeech": {
   "authors": [
    [
     "Hywel B.",
     "Richards"
    ],
    [
     "John S.",
     "Bridle"
    ],
    [
     "Melvyn J.",
     "Hunt"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Dynamic constraint weighting in the context of articulatory parameter estimation",
   "original": "e97_2535",
   "page_count": 4,
   "order": 659,
   "p1": "2535",
   "pn": "2538",
   "abstract": [
    "This paper describes a cross-validation method to determine the appropriate weight with which dynamic constraints should be applied when estimating vocal tract shapes from speech. This data-dependent method can estimate the weighting without the need for separate prior knowledge of the source and noise statistics. The principles are first demonstrated on a simple one-dimensional system analogous to speech production. As the data here is synthetic, the statistics are known, and so the success of the method can be objectively assessed. Next, the same principles are extended to real speech to improve estimation of vocal tract shape trajectories.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-658"
  },
  "lee97e_eurospeech": {
   "authors": [
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Donald G.",
     "Childers"
    ]
   ],
   "title": "Estimation of vocal tract front cavity resonance in unvoiced fricative speech",
   "original": "e97_2539",
   "page_count": 4,
   "order": 660,
   "p1": "2539",
   "pn": "2542",
   "abstract": [
    "The purpose of this paper is to study the effect of the front cavity resonance and the vocal tract area function on the quality of synthesized unvoiced speech. From prior experiments, it has been determined that unvoiced speech is highly related to the vocal tract front cavity resonance. The noise source is located near the vocal tract constriction and the front cavity serves as a spectral shaping filter. An algorithm is proposed to estimate front cavity resonances, from which effective length of the vocal tract front cavity can be calculated. The parameters are used to construct a simple vocal tract area function. Unvoiced speech is generated using an articulatory synthesizer. And effects of the front cavity length, back cavity shape on the perception of unvoiced fricatives are investigated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-659"
  },
  "teixeira97b_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Teixeira"
    ],
    [
     "Francisco",
     "Vaz"
    ],
    [
     "Jose Carlos",
     "Principe"
    ]
   ],
   "title": "A software tool to study portuguese vowels",
   "original": "e97_2543",
   "page_count": 4,
   "order": 661,
   "p1": "2543",
   "pn": "2546",
   "abstract": [
    "We are developing a software system to help the study of Portuguese Vowel Production. This tool is an articulatory synthesizer with a graphical user interface. The synthesizer is composed of a saggittal articulatory model derived from Mermelstein model and a frequency domain simulation of the electric analog of the acoustic tube. User can easily define the nasal tract configuration. System includes optimization by simulated annealing to perform acoustic-to-articulatory mapping. In this paper we present the system being developed, its current state and future perspectives. Preliminary experiments with Portuguese Vowels gave good results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-660"
  },
  "schoentgen97_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Sorin",
     "Ciocea"
    ]
   ],
   "title": "Post-synchronization via formant-to-area mapping of asynchronously recorded speech signals and area functions",
   "original": "e97_2547",
   "page_count": 4,
   "order": 662,
   "p1": "2547",
   "pn": "2550",
   "abstract": [
    "The article presents a method of post- synchronization which is the match, by means of formant-to-area mapping, of an area function model to a measured area function. The objective of post- synchronization is to compute a model which is as near as possible to a measured area function and whose eigenfrequencies are identical to the corresponding measured formant frequencies. Different types of acoustic models and constraints are examined. Results show that the best map is obtained in the case of a lossless acoustic model, corrected for lip radiation and wall vibration losses, and the minimization of the Euclidean distance between geometrically fitted and formant-to-area mapped area function models. The differences between measured and mapped area functions are gauged by means of the dynamic length warping distance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-661"
  },
  "yu97_eurospeech": {
   "authors": [
    [
     "Zhenli L.",
     "Yu"
    ],
    [
     "P.C.",
     "Ching"
    ]
   ],
   "title": "Geometrically and acoustically optimized codebook for unique mapping from formants to vocal-tract shape",
   "original": "e97_2551",
   "page_count": 4,
   "order": 663,
   "p1": "2551",
   "pn": "2554",
   "abstract": [
    "A method to generate a codebook with distributed formant targets and unique geometric-acoustic mapping from formants to vocal-tract shape by direct acoustic calculation is proposed. Geometric and acoustic constraints are applied to both vocal-tract model parameters and calculated acoustic features to eliminate unacceptable values from the initial codebook which usually has an extremely large codebook size. The vocaltract length is used as an additional parameter to model the vocal-tract. Restriction on the vocaltract length based on some measured data is employed. A geometric and acoustic optimization scheme is devised to cluster the constrained codebook into an uniquely mapped codebook with reduced size. The codebook generated by this method is precise and robust and provides a satisfactory solution to the inverse speech production problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-662"
  },
  "riedi97_eurospeech": {
   "authors": [
    [
     "Marcel",
     "Riedi"
    ]
   ],
   "title": "Modeling segmental duration with multivariate adaptive regression splines",
   "original": "e97_2627",
   "page_count": 4,
   "order": 664,
   "p1": "2627",
   "pn": "2630",
   "abstract": [
    "The application of \"Multivariate Adaptive Regression Splines\" (MARS) to the problem of modeling duration of a set of segments used in a text-to-speech system for German is presented. MARS is a technique to estimate general functions of high-dimensional arguments given sparse data. It automatically selects the parameters and the structure of the model based on data available. The result is a model with a correlation coefficient between observed and predicted durations of a test set of . Besides highly accurate predicting durations, a MARS model also allows interpretation of its structure, demonstrated in this study by analyses of factor importance and interactions of the MARS model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-663"
  },
  "malfrere97_eurospeech": {
   "authors": [
    [
     "Fabrice",
     "Malfrere"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "High-quality speech synthesis for phonetic speech segmentation",
   "original": "e97_2631",
   "page_count": 4,
   "order": 665,
   "p1": "2631",
   "pn": "2634",
   "abstract": [
    "This paper presents an original technique for solving the phonetic segmentation problem. It is based on the use of a speech synthesizer for the alignment of a text on its corresponding speech signal. A high-quality digital speech synthesizer is used to create a synthetic reference speech pattern used in the alignment process. This approach has the great advantage on other approaches that no training stage (hence no labeled database) is needed. The system has been mainly evaluated on French read utterances. Other evaluations have been made on other languages like English, German, Romanian and Spanish. Following these experiments, the system seems to be a powerful tool for the automatic constitution of large phonetically and prosodically labeled speech databases. The availability of such corpora will be a key point for the development of improved speech synthesis and recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-664"
  },
  "campbell97b_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Yoshiharu",
     "Itoh"
    ],
    [
     "Wen",
     "Ding"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Factors affecting perceived quality and intelligibility in the CHATR concatenative speech synthesiser",
   "original": "e97_2635",
   "page_count": 4,
   "order": 666,
   "p1": "2635",
   "pn": "2638",
   "abstract": [
    "In order to eliminate trial-and-error in the process of selecting a good speech database as a voice source for concatenative speech synthesis, and to determine the acoustic and prosodic characteristics that best predict 'appeal' or perceived 'quality' in the synthesised speech, we performed tests to evaluate listener preferences over a range of different synthesised voices. We found that variation of fundamental frequency in the source database, and open-quotient of the glottis as measured by joint-estimation (ARX) were the best correlates. To our surprise, there was very little correlation between the scores for 'intelligibility' and those for 'naturalness' in the test data, but the former showed a close correlation with durational characteristics, and the latter with pitch and loudness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-665"
  },
  "neukirchen97_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Reduced lexicon trees for decoding in a MMIi-connectionist/HMM speech recognition system",
   "original": "e97_2639",
   "page_count": 4,
   "order": 667,
   "p1": "2639",
   "pn": "2642",
   "abstract": [
    "The presented work deals with the experimental identification of parts in a tree based decoder lexicon, that are more important for decoding efficiency compared to less important lexicon parts. Three different methods for constructing only the most important nodes in a set of tree lexicon copies are presented: building large trees; tree cutting; lexicon node removal. This leads to dramatic reduction of memory requirements while retaining the original recognition performance. In addition a reduction of the active decoding search space can be observed that leads to improved recognition speed. Although the presented methods can be generally applied to any HMM speech recognizer, experiments are performed in the hybrid MMI-connectionist/HMM system framework on the speaker independent 5k WSJ database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-666"
  },
  "veronis97_eurospeech": {
   "authors": [
    [
     "Jean",
     "Veronis"
    ],
    [
     "Philippe Di",
     "Cristo"
    ],
    [
     "Fabienne",
     "Courtois"
    ],
    [
     "Benoit",
     "Lagrue"
    ]
   ],
   "title": "A stochastic model of intonation for French text-to-speech synthesis",
   "original": "e97_2643",
   "page_count": 4,
   "order": 668,
   "p1": "2643",
   "pn": "2646",
   "abstract": [
    "This paper presents a stochastic model of French intonation contours for use in text-to-speech synthesis. The model has two modules, a linguistic module that generates prosodic labels from text, and a phonetic module that generates an F0 curve from the prosodic labels. This model differs from previous work in the prosodic labels used, which can be automatically derived from the training corpus. This feature makes it possible to use large corpora or several corpora of different speech styles, in addition to making it easy to adapt to new languages. The present paper focuses on the linguistic module, which does not require full syntactic analysis of the text but simply relies on a part-of-speech tagging technique. The results were validated by means of a perception test which showed that listeners did not perceive a significant difference in quality between the sentences synthesized with the original F0 curve (from a recording), and those synthesized with the model-generated curve. The proposed model thus appears to capture a large part of the grammatical information needed to generate F0.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-667"
  },
  "sanderman97_eurospeech": {
   "authors": [
    [
     "Angelien A.",
     "Sanderman"
    ],
    [
     "Renè",
     "Collier"
    ]
   ],
   "title": "Phonetic rules for a phonetic-to-speech system",
   "original": "e97_2647",
   "page_count": 4,
   "order": 669,
   "p1": "2647",
   "pn": "2650",
   "abstract": [
    "In our previous research we investigated the demarcative function of prosody at the sentence level and the importance of this information for listeners in terms of perception, acceptability and ease of comprehension. In this paper we investigate if the results obtained for utterances spoken in isolation can be generalised to utterances spoken in context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-668"
  },
  "santen97d_eurospeech": {
   "authors": [
    [
     "Jan van",
     "Santen"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Evelyne",
     "Tzoukermann"
    ],
    [
     "Michael",
     "Tanenblatt"
    ]
   ],
   "title": "Multi-lingual duration modeling",
   "original": "e97_2651",
   "page_count": 4,
   "order": 670,
   "p1": "2651",
   "pn": "2654",
   "abstract": [
    "Controlling timing in text-to-speech synthesis systems is complicated, because there are many contextual factors that affect timing; moreover, which factors matter and what their precise effects are varies among languages. We describe here a language-independent approach for duration control. At run time, a language-independent timing module accesses language-specific tables. These tables specify which sub-classes of the feature space (i.e., all combinations of context and phone identity) are homogeneous in the specific sense that the same factors have similar effects on the cases in a sub-class. Within a sub-class, durations are modeled by simple arithmetic models such as multiplicative, additive, or - more generally - sums-of-products models. Exploratory statistical methods (supervised) and parameter estimation techniques (unsupervised) are used for table construction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-669"
  },
  "barbosa97_eurospeech": {
   "authors": [
    [
     "Plinio A.",
     "Barbosa"
    ]
   ],
   "title": "A model of segment (and pause) duration generation for Brazilian Portuguese text-to-speech synthesis",
   "original": "e97_2655",
   "page_count": 4,
   "order": 671,
   "p1": "2655",
   "pn": "2658",
   "abstract": [
    "This work presents and evaluates a model of segmental duration generation for Brazilian Portuguese where the notion of macrorhythmic unit is the starting point to drastically simplify duration assignment and to allow pause insertion as an integrated procedure of generation. This model is preferred to random assignment with the same error distribution. Some aspects of rhythm phonetics and phonology are also discussed that constitute a first step to the understanding of the prosodic component of the language under study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-670"
  },
  "halber97_eurospeech": {
   "authors": [
    [
     "Ariane",
     "Halber"
    ],
    [
     "David",
     "Roussel"
    ]
   ],
   "title": "Parsing strategy for spoken language interfaces with a lexicalized tree grammar",
   "original": "e97_2659",
   "page_count": 4,
   "order": 672,
   "p1": "2659",
   "pn": "2662",
   "abstract": [
    "Our work addresses the integration of speech recognition and natural language processing for spoken dialogue systems. To deal with recognition errors, we investigate two repairing strategies integrated in a parsing based on a Lexicalized Tree Grammar. The first strategy takes its root in the recognition hypothesis, the other in the linguistic expectations. We expose a formal framework to express the grammar, to describe the repairing strategies and to foresee further strategies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-671"
  },
  "amtrup97_eurospeech": {
   "authors": [
    [
     "Jan W.",
     "Amtrup"
    ],
    [
     "Henrik",
     "Heine"
    ],
    [
     "Uwe",
     "Jost"
    ]
   ],
   "title": "What's in a word graph evaluation and enhancement of word lattices?",
   "original": "e97_2663",
   "page_count": 4,
   "order": 673,
   "p1": "2663",
   "pn": "2666",
   "abstract": [
    "During the last few years, word graphs have been gaining increasing interest within the speech community as the primary interface between speech recognizers and language processing modules. Both development and evaluation of graph-producing speech decoders require generally accepted measures of word graph quality. While the notion of recognition accuracy can easily be extended to word graphs, a meaningful measure of word graph size has not yet surfaced. We argue, that the number of derivation steps a theoretical parser would need to process all unique subpaths in a graph could provide a measure that is both application oriented enough to be meaningful and general enough to allow a useful comparison of word recognizers across different applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-672"
  },
  "tillmann97_eurospeech": {
   "authors": [
    [
     "C.",
     "Tillmann"
    ],
    [
     "S.",
     "Vogel"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "A.",
     "Zubiaga"
    ],
    [
     "H.",
     "Sawaf"
    ]
   ],
   "title": "Accelerated DP based search for statistical translation",
   "original": "e97_2667",
   "page_count": 4,
   "order": 674,
   "p1": "2667",
   "pn": "2670",
   "abstract": [
    "In this paper, we describe a fast search algorithm for statistical translation based on dynamic programming (DP) and present experimental results. The approach is based on the assumption that the word alignment is monotone with respect to the word order in both languages. To reduce the search effort for this approach, we introduce two methods: an acceleration technique to eficiently compute the dynamic programming recursion equation and a beam search strategy as used in speech recognition. The experimental tests carried out on the Verbmobil corpus showed that the search space, measured by the number of translation hypotheses, is reduced by a factor of about 230 without affecting the translation performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-673"
  },
  "fujisawa97_eurospeech": {
   "authors": [
    [
     "Ken",
     "Fujisawa"
    ],
    [
     "Toshio",
     "Hirai"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Use of pitch pattern improvement in the CHATR speech synthesis system",
   "original": "e97_2671",
   "page_count": 4,
   "order": 675,
   "p1": "2671",
   "pn": "2674",
   "abstract": [
    "A corpus-based concatenative speech synthesis system using no signal processing can produce intelligible synthetic speech maintaining original voice characteristics, but it can sometimes be difficult to realize natural prosody. In such a concatenative system, it is very important to select appropriate waveform segments that are naturally close to the target prosody. This paper describes some approaches to unit selection for improving the prosody, especially intonation of such synthetic speech. If the unit selection measures for the fundamental frequency (F0) are insuficient, the concatenative system may produce speech having a discontinuous F0 pattern. Our proposed solution to this problem is to add extra measures for selecting units that form a smoother, more continuous F0 contour. Through subjective experiments, we confirmed that each of these measures effectively improved intonation naturalness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-674"
  },
  "corrigan97_eurospeech": {
   "authors": [
    [
     "G.",
     "Corrigan"
    ],
    [
     "N.",
     "Massey"
    ],
    [
     "O.",
     "Karaali"
    ]
   ],
   "title": "Generating segment durations in a text-zo-speech system: a hybrid rule-based/neural network approach",
   "original": "e97_2675",
   "page_count": 4,
   "order": 676,
   "p1": "2675",
   "pn": "2678",
   "abstract": [
    "A combination of a neural network with rule firing information from a rule-based system is used to generate segment durations for a text-to-speech system. The system shows a slight improvement in performance over a neural network system without the rule firing information. Synthesized speech using segment durations was accepted by listeners as having about the same quality as speech generated using segment durations extracted from natural speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-675"
  },
  "ishikawa97_eurospeech": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Takashi",
     "Ebihara"
    ]
   ],
   "title": "On the global FO shape model using a transition network for Japanese text-to-speech systems",
   "original": "e97_2679",
   "page_count": 4,
   "order": 677,
   "p1": "2679",
   "pn": "2682",
   "abstract": [
    "In this paper, we describe a model of fundamental frequency control. In general, a two stage model which consists of a global model and a local model is used as a FO control method for Japanese text-to-speech systems. We propose a model which is represented by transition network as a global model that generates parameters of a local pitch model from linguistic parameters of a sentence. In the proposed model, syntactic analysis and generation of FO parameters are integrated, and the nodes of a network represent the linguistic and prosodic state of a sentence. The parameters of a local model is generated when taking transition. We also propose a training method of the network. The prediction results showed our model can predict the phrasal accent parameters with satisfactory high accuracy. We also describe the model can be applied prediction of pause position.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-676"
  },
  "colas97_eurospeech": {
   "authors": [
    [
     "José",
     "Colás"
    ],
    [
     "Juan M.",
     "Montero"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "José M.",
     "Pardo"
    ]
   ],
   "title": "An alternative and flexible approach in robust information retrieval systems",
   "original": "e97_2683",
   "page_count": 4,
   "order": 678,
   "p1": "2683",
   "pn": "2686",
   "abstract": [
    "In this paper, we present a flexible architecture to implement a robust information retrieval system based on domain and linguistic modelling by means of a set of conceptual probabilistic and non-probabilistic grammars. It allows certain complexity in the functionality of the application, such as applying non-SQL functions to the results of SQL queries in order to retrieve information not explicitly included in the database, or translating certain natural spoken sentences that would produce difficult embedded queries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-677"
  },
  "horiguchi97_eurospeech": {
   "authors": [
    [
     "Keiko",
     "Horiguchi"
    ],
    [
     "Alexander",
     "Franz"
    ]
   ],
   "title": "A probabilistic approach to analogical speech translation",
   "original": "e97_2687",
   "page_count": 4,
   "order": 679,
   "p1": "2687",
   "pn": "2690",
   "abstract": [
    "Previous work on speech-to-speech translation has suffered from problems of brittleness and low quality (rule-based approaches), or from excessive data requirements and linguistic ineciency (analogical or example-based approaches). In this paper, we present a probabilistic approach to analogical speech translation, and describe its integration with linguistic processing. The evaluation results show that this approach results in high-accuracy translations in limited domains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-678"
  },
  "caraty97_eurospeech": {
   "authors": [
    [
     "Marie-José",
     "Caraty"
    ],
    [
     "Claude",
     "Montacié"
    ],
    [
     "Fabrice",
     "Lefèvre"
    ]
   ],
   "title": "Dynamic lexicon for a very large vocabulary vocal dictation",
   "original": "e97_2691",
   "page_count": 4,
   "order": 680,
   "p1": "2691",
   "pn": "2694",
   "abstract": [
    "For very large vocabulary vocal dictation systems, we present a decoding strategy useful to reduce the lexical decoding cost. For each test-utterance, a sub-lexicon is selected from a very large recognition vocabulary. Such a recognition sub-lexicon is called Dynamic Lexicon (DL). Various algorithms of DL selection are developed and tested in terms of coverage rate of textual corpus. From these experiments, we describe a DL constitution we choose to use in D-DAL, our HMM-based recognizer competing for the first campaign of french vocal dictation supported by AUPELF. The contribution made by this original DL is a posteriori confirmed through the AUPELF-B1 test-dictation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-679"
  },
  "segarra97_eurospeech": {
   "authors": [
    [
     "E.",
     "Segarra"
    ],
    [
     "L.",
     "Hurtado"
    ]
   ],
   "title": "Construction of language models using the morphic generator grammatical inference (MGGI) methodology",
   "original": "e97_2695",
   "page_count": 4,
   "order": 681,
   "p1": "2695",
   "pn": "2698",
   "abstract": [
    "Over the last few years, some alternatives to N-gram language models, which are based on stochastic regular grammars, have been proposed. These grammars are estimated from data through Grammatical Inference algorithms. In particular, the Morphic Generator Grammatical Inference (MGGI) methodology has been applied to tasks of written natural language queries to databases. As for N-gram models, language models obtained through this methodology require the use of smoothing techniques. This work incorporates a version of the well-known Back-Off smoothing method to the MGGI language models to solve the estimation problem of unseen events in the training corpus, and shows the behaviour of the smoothed MGGI models in two tasks of written sentences. The results illustrate that the smoothed MGGI model works better than the standard smoothed bigram model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-680"
  },
  "zhang97_eurospeech": {
   "authors": [
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "An integrated language modeling with n-gram model and WA model for speech recognition",
   "original": "e97_2699",
   "page_count": 4,
   "order": 682,
   "p1": "2699",
   "pn": "2702",
   "abstract": [
    "As to traditional n-gram model, smaller n value is an inherent defect for estimating language probabilities in speech recognition, simply because that estimation could not be executed over farther word association but by means of short sequential word correlated information. This has an strong effect on the performance of speech recognition. This paper introduces an integrated language modeling with n-gram model and word association model (abbreviated as WA model). This model integrated two kind of joint probabilities, traditional n-gram probability and word association probability, to estimate actual output probability. WA model are based on a combined probability estimation of orderly word association without distant and strict sequential limitation. In addition, two kinds of local linguistic constraints have also been incorporated into n-gram estimation for smoothing date sparse and adjusting special language unit score locally. A substantial improvement for the performance of Chinese phonetic-to-text transcription in speech recognition has been obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-681"
  },
  "wang97c_eurospeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Statistical analysis of dialogue structure",
   "original": "e97_2703",
   "page_count": 4,
   "order": 683,
   "p1": "2703",
   "pn": "2706",
   "abstract": [
    "We introduce a statistical model for dialogues. We describe a dynamic programming algorithm that can be used to bracket a dialogue into segments and label each segment with its speech act. We evaluate the performance of the model. We also use this model for language modelling and get perplexity reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-682"
  },
  "clarkson97_eurospeech": {
   "authors": [
    [
     "Philip",
     "Clarkson"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Statistical language modeling using the CMU-cambridge toolkit",
   "original": "e97_2707",
   "page_count": 4,
   "order": 684,
   "p1": "2707",
   "pn": "2710",
   "abstract": [
    "The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models. It is currently in use in over 40 academic, government and industrial laboratories in over 12 countries. This paper presents a new version of the toolkit. We outline the conventional language modeling technology, as implemented in the toolkit, and describe the extra efficiency and functionality that the new toolkit provides as compared tï previous software for this task. Finally, we give an example of the use of the toolkit in constructing and testing a simple language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-683"
  },
  "adda97_eurospeech": {
   "authors": [
    [
     "Gilles",
     "Adda"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Text normalization and speech recognition in French",
   "original": "e97_2711",
   "page_count": 4,
   "order": 685,
   "p1": "2711",
   "pn": "2714",
   "abstract": [
    "In this paper we present a quantitative investigation into the impact of text normalization on lexica and language models for speech recognition in French. The text normalization process defines what is considered to be a word by the recognition system. Depending on this definition we can measure different lexical coverages and language model perplexities, both of which are closely related to the speech recognition accuracies obtained on read news-paper texts. Different text normalizations of up to 185M words of newspaper texts are presented along with corresponding lexical coverage and perplexity measures. Some normalizations were found to be necessary to achieve good lexical coverage, while others were more or less equivalent in this regard. The choice of normalization to create language models for use in the recognition experiments with read newspaper texts was based on these findings. Our best system configuration obtained a 11.2% word error rate in the AUPELF 'French-speaking' speech recognizer evaluation test held in February 1997.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-684"
  },
  "damnati97_eurospeech": {
   "authors": [
    [
     "G.",
     "Damnati"
    ],
    [
     "J.",
     "Simonin"
    ]
   ],
   "title": "A novel tree-based clustering algorithm for statistical language modeling",
   "original": "e97_2715",
   "page_count": 4,
   "order": 686,
   "p1": "2715",
   "pn": "2718",
   "abstract": [
    "In this paper, a new method to cluster words into classes is proposed in order to define a statistical language model. The purpose of this algorithm is to decrease the computational cost of the clustering task while not degrading speech recognition performance. The algorithm provides a bottom-up hierarchical clustering using the reciprocal neighbours method. This technique consists in merging several pairs of classes within a single iteration. Experiments on a spontaneous speech corpus are presented. Results are given both in terms of perplexity and word recognition error rate. We obtain a large reduction in the number of iterations necessary to build a classification tree and thus a CPU time reduction in building the model as well as a reduction in both perplexity and word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-685"
  },
  "matsunaga97_eurospeech": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Variable-length language modeling integrating global constraints",
   "original": "e97_2719",
   "page_count": 4,
   "order": 687,
   "p1": "2719",
   "pn": "2722",
   "abstract": [
    "This paper proposes a novel variable-length class- based language model that integrates local and global constraints. In this model, the classes are iteratively recreated by grouping consecutive words and by splitting initial part-of speech (POS) clusters into finer clusters (word-classes). The main characteristic of this modeling is that these operations of grouping and splitting is carried out selectively, taking into account global constraints between noncontiguous words on the basis of a minimum entropy criterion. To capture the global constraints, the model takes into account the sequences of the function words and of the content words, which are expected to respectively represent the syntactic and semantic relationships between words. Experiments showed that the perplexity of the proposed model for the test corpus is lower than that of conventional models and that this model requires a small number of statistical parameters, showing the model's effectiveness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-686"
  },
  "smaili97_eurospeech": {
   "authors": [
    [
     "K.",
     "Smaili"
    ],
    [
     "I.",
     "Zitouni"
    ],
    [
     "F.",
     "Charpillet"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "An hybrid language model for a continuous dictation prototype",
   "original": "e97_2723",
   "page_count": 4,
   "order": 688,
   "p1": "2723",
   "pn": "2726",
   "abstract": [
    "This paper describes the combination of a stochastic language model and a formal grammar modelled such as a unification grammar. The stochastic model is trained over 42 million words extracted from Le monde newspaper. The stochastic model is based on smoothed 3-gram and 3-class. The 3-class model is represented by a Markov chain made up of four states. Several experiments have been done to state which values are the best for specific training and test corpus. Experiments indicate that the unification grammar reduce strongly the number of hypothesis (sentences) produced by the stochastic model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-687"
  },
  "perennou97_eurospeech": {
   "authors": [
    [
     "Guy",
     "Pérennou"
    ],
    [
     "L.",
     "Pousse"
    ]
   ],
   "title": "Dealing with pronunciation variants at the language model level for the continuous automatic speech recognition of French",
   "original": "e97_2727",
   "page_count": 4,
   "order": 689,
   "p1": "2727",
   "pn": "2730",
   "abstract": [
    "In this paper, we describe three approaches of continuous speech recognition. Two of them (referred to as (W,P) and (W',P) models) take into account pronunciation variants of words. They allow to handle (very common) phonological french phenomena like liaisons or mute-e elision. The (W',P) model introduces the phonotypical level as defined in the MHAT Model [4,5]. Comparing (W,P) and (W',P) models show a significant improvement in recognition accuracy when a contextual language model is introduced at this phonotypical level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-688"
  },
  "schukattalamazzini97_eurospeech": {
   "authors": [
    [
     "Ernst Günter",
     "Schukat-Talamazzini"
    ],
    [
     "Florian",
     "Gallwit"
    ],
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Volker",
     "Warnke"
    ]
   ],
   "title": "Rational interpolation of maximum likelihood predictors in stochastic language modeling",
   "original": "e97_2731",
   "page_count": 4,
   "order": 690,
   "p1": "2731",
   "pn": "2734",
   "abstract": [
    "In our paper, we address the problem of estimating stochastic language models based on n-gram statistics. We present a novel approach, rational interpolation, for the combination of a competing set of conditional n-gram word probability predictors, which consistently outperforms the traditional linea,r interpolation scheme. The superiority of rational interpolation is substantiated by experimental results from language modeling, speech recognition, dialog act classiflcation, and language identiflcation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-689"
  },
  "ito97_eurospeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Hideyuki",
     "Saitoh"
    ],
    [
     "Masaharu",
     "Katoh"
    ],
    [
     "Masaki",
     "Kohda"
    ]
   ],
   "title": "N-gram language model adaptation using small corpus for spoken dialog recognition",
   "original": "e97_2735",
   "page_count": 4,
   "order": 691,
   "p1": "2735",
   "pn": "2738",
   "abstract": [
    "This paper describes an N-gram language model adaptation technique. As an N-gram model requires a large size sample corpus for probability estimation, it is difficult to utilize N-gram model for a specific small task. In this paper, N-gram task adaptation is proposed using large corpus of the general task (TI text) and small corpus of the specific task (AD text). A simple weighting is employed to mix TI and AD text. In addition to mix two texts, the effect of vocabulary is also investigated. The experimental results show that adapted N-gram model with proper vocabulary size has significantly lower perplexity than the task independent models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-690"
  },
  "siu97b_eurospeech": {
   "authors": [
    [
     "Manhung",
     "Siu"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Variable n-gram language modeling and extensions for conversational speech",
   "original": "e97_2739",
   "page_count": 4,
   "order": 692,
   "p1": "2739",
   "pn": "2742",
   "abstract": [
    "Recent progress in variable n-gram language modeling provides an efficient representation of n-gram models and makes training of higher order n-grams possible. In this paper, we apply the variable n-gram design algorithm to conversational speech, extending the algorithm to learn skips and classes in context to handle conversational speech characteristics such as repetitions and dis uency markers. We show that using the extended variable n-gram, we can build a language model that uses fewer parameters for longer context and improves the test perplexity and recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-691"
  },
  "geutner97_eurospeech": {
   "authors": [
    [
     "Petra",
     "Geutner"
    ]
   ],
   "title": "Fuzzy class rescoring: a part-of-speech language model",
   "original": "e97_2743",
   "page_count": 4,
   "order": 693,
   "p1": "2743",
   "pn": "2746",
   "abstract": [
    "Current speech recognition systems usually use word-based trigram language models. More elaborate models are applied to word lattices or N best lists in a rescoring pass following the acoustic decoding process. In this paper we consider techniques for dealing with class-based language models in the lattice rescoring framework of our JANUS large vocabulary speech recognizer. We demonstrate how tointerpolate with a Part-of-Speech (POS) tag-based language model as example of a class-based model, where a word can be member of many different classes. Here the actual class membership of a word in the lattice becomes a hidden event of the A-algorithm used for rescoring. A forward type of algorithm is defined as extension of the lattice rescorer to handle these hidden events in a mathematically sound fashion. Applying the mixture of viterbi and forward kind of rescoring procedure to the German Spontaneous Scheduling Task (GSST) yields some improvement inword accuracy. Above all, the rescoring procedure enables usage of any fuzzy/stochastic class definition for recognition units that might be determined through automatic clustering algorithms in the future.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-692"
  },
  "nagai97_eurospeech": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Yasushi",
     "Ishikawa"
    ]
   ],
   "title": "Speech understanding based on integrating concepts by conceptual dependency",
   "original": "e97_2747",
   "page_count": 4,
   "order": 694,
   "p1": "2747",
   "pn": "2750",
   "abstract": [
    "We have proposed a concept-driven semantic interpretation method for a spoken dialogue system that robustly understands various expressions uttered by a naive user. The method is now being improved for practical application. Domain knowledge is important for this improvement. The system must also have portability. This paper discusses the generalization of the semantic interpretation method, and proposes a method that integrates concepts using general linguistic knowledge of conceptual dependency. Speech understanding for various utterances about Kamakura sightseeing with a 1000-word vocabulary was empirically evaluated. The results show that this method can achieve a satisfactory understanding rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-693"
  },
  "brugnara97_eurospeech": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Dynamic language models for interactive speech applications",
   "original": "e97_2751",
   "page_count": 4,
   "order": 695,
   "p1": "2751",
   "pn": "2754",
   "abstract": [
    "This work proposes the use of hierarchical LMs as an effective method both for efficiently dealing with context- dependent LMs in a dialogue system and for increasing the robustness of LM estimation and adaptation. Starting from basic LMs that express elementary semantic units, concepts, or data-types, sentence level LMs are recursively built. The resulting LMs may be a combination of grammars, word classes, and statistical LMs. Moreover, these LMs can be efficiently compiled into probabilistic recursive transition networks. A speech decoding algorithm directly exploits the recursive representation and produces the most probable parse tree matching the speech signal. The proposed approach has been implemented for a data-entry task which covers structured data, e.g. numbers, dates, and proper names, as well as free texts. In this task, the active LMmust continuously change according to the current status, the active form, and the data entered so far. Finally, while the hierarchical approach results very convenient to cope with this task, it also looks very general and can give advantages in other applications, e.g. dictation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-694"
  },
  "demetriou97_eurospeech": {
   "authors": [
    [
     "George",
     "Demetriou"
    ],
    [
     "Eric",
     "Atwell"
    ],
    [
     "Clive",
     "Souter"
    ]
   ],
   "title": "Large-scale lexical semantics for speech recognition support",
   "original": "e97_2755",
   "page_count": 4,
   "order": 696,
   "p1": "2755",
   "pn": "2758",
   "abstract": [
    "This paper presents a study on the use of wide-coverage semantic knowledge for large vocabulary (theoretically unrestricted) domain-independent speech recognition. A machine readable dictionary was used to provide the semantic information about the words and a semantic model was developed based on the conceptual association between words as computed directly from the textual representations of their meanings. The findings of our research suggest that the model is capable of capturing phenomena of semantic associativity or connectivity between words in texts and considerably reducing the semantic ambiguity in natural language. The model can cover both short and long-distance semantic relationships between words and has shown signs of robustness across various text genres. Experiments with simulated speech recognition hypotheses indicate that the model can efficiently be used to reduce the word error rates when applied to word lattices or N-best sentence hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-695"
  },
  "tsukada97_eurospeech": {
   "authors": [
    [
     "Hajime",
     "Tsukada"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Integration of grammar and statistical language constraints for partial word-sequence recognition",
   "original": "e97_2759",
   "page_count": 4,
   "order": 697,
   "p1": "2759",
   "pn": "2762",
   "abstract": [
    "This paper proposes a novel spontaneous speech recognition approach to obtain not a whole utterance but reliably recognized partial segments of an utterance to achieve robust speech understanding. Our method obtains reliably recognized partial segments of an utterance by using both grammatical and n-gram based statistical language constraints cooperatively, and uses a robust parsing technique to apply the grammatical constraints. Through an experiment, it has been confirmed that the proposed method can recognize partial segments of an utterance with a higher reliability than conventional continuous speech recognition methods using an n-gram based statistical language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-696"
  },
  "taylor97b_eurospeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Stephen",
     "Isard"
    ],
    [
     "Helen",
     "Wright"
    ],
    [
     "Jacqueline",
     "Kowtko"
    ]
   ],
   "title": "Using intonation to constrain language models in speech recognition",
   "original": "e97_2763",
   "page_count": 4,
   "order": 698,
   "p1": "2763",
   "pn": "2766",
   "abstract": [
    "This paper describes a method for using intonation to reduce word error rate in a speech recognition system designed to recognise spontaneous dialogue speech. We use a form of dialogue analysis based on the theory of conversational games. Different move types under this analysis conform to different language models. Different move types are also characterised by different into-national tunes. Our overall recognition strategy is first to predict from intonation the type of game move that a test utterance represents, and then to use a bigram language model for that type of move during recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-697"
  },
  "heeman97_eurospeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "James F.",
     "Allen"
    ]
   ],
   "title": "Incorporating POS tagging into language modeling",
   "original": "e97_2767",
   "page_count": 4,
   "order": 699,
   "p1": "2767",
   "pn": "2770",
   "abstract": [
    "Language models for speech recognition tend to concentrate solely on recognizing the words that were spoken. In this paper, we redefine the speech recognition problem so that its goal is to find both the best sequence of words and their syntactic role (part-of-speech) in the utterance. This is a necessary first step towards tightening the interaction between speech recognition and natural language understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-698"
  },
  "uhrik97_eurospeech": {
   "authors": [
    [
     "C.",
     "Uhrik"
    ],
    [
     "W.",
     "Ward"
    ]
   ],
   "title": "Confidence metrics based on n-gram language model backoff behaviors",
   "original": "e97_2771",
   "page_count": 5,
   "order": 700,
   "p1": "2771",
   "pn": "2774",
   "abstract": [
    "We report results from using language model confidence measures based on the degree of backoff used in a trigram language model. Both utterance-level and word-level confidence metrics proved useful for a dialog manager to identify out-of-domain utterances. The metric assigns successively lower confidence as the language model estimate is backed off to a bigram or unigram. It also bases its estimates on sequences of backoff degree. Experimental results with utterances from the domain of medical records management showed that the distributions of the confidence metric for in-domain and out-of-domain utterances are separated. Use of the corresponding word-level confidence metric shows similar encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-699"
  },
  "chelba97_eurospeech": {
   "authors": [
    [
     "Ciprian",
     "Chelba"
    ],
    [
     "David",
     "Engle"
    ],
    [
     "Frederick",
     "Jelinek"
    ],
    [
     "Victor",
     "Jimenez"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Lidia",
     "Mangu"
    ],
    [
     "Harry",
     "Printz"
    ],
    [
     "Eric",
     "Ristad"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Dekai",
     "Wu"
    ]
   ],
   "title": "Structure and performance of a dependency language model",
   "original": "e97_2775",
   "page_count": 4,
   "order": 701,
   "p1": "2775",
   "pn": "2778",
   "abstract": [
    "We present a maximum entropy language model that incorporates both syntax and semantics via a dependency grammar. Such a grammar expresses the relations between words by a directed graph. Because the edges of this graph may connect words that are arbitrarily far apart in a sentence, this technique can incorporate the predictive power of words that lie outside of bigram or trigram range. We have built several simple dependency models, as we call them, and tested them in a speech recognition experiment. We report experimental results for these models here, including one that has a small but statistically significant advantage (p<.02) over a bigram language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-700"
  },
  "stolcke97b_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Modeling linguistic segment and turn boundaries for n-best rescoring of spontaneous speech",
   "original": "e97_2779",
   "page_count": 4,
   "order": 702,
   "p1": "2779",
   "pn": "2782",
   "abstract": [
    "Language modeling, especially for spontaneous speech, often suffers from a mismatch of utterance segmentations between training and test conditions. In particular, training often uses linguistically-based segments, whereas testing occurs on acoustically determined segments, resulting in degraded performance. We present an N-best rescoring algorithm that removes the effect of segmentation mismatch. Furthermore, we show that explicit language modeling of hidden linguistic segment boundaries is improved by including turn-boundary events in the model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-701"
  },
  "kenne97_eurospeech": {
   "authors": [
    [
     "P. E.",
     "Kenne"
    ],
    [
     "Mary",
     "O'Kane"
    ]
   ],
   "title": "Hybrid language models: is simpler better?",
   "original": "e97_2783",
   "page_count": 4,
   "order": 703,
   "p1": "2783",
   "pn": "2786",
   "abstract": [
    "The use of several n-gram and hybrid language models with and without cache is examined in the context of producing court transcripts. Language models with cache (in which words which have recently been uttered are preferred) have seen considerable use. The suitability of cache models (with fixed size cache) in the production of court transcripts is not clear. A decrease in perplexity and an improvement in the word error rate is observed with some of the models when using a cache, however, performance deteriorates with increasing cache size.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-702"
  },
  "brants97_eurospeech": {
   "authors": [
    [
     "Thorsten",
     "Brants"
    ]
   ],
   "title": "Internal and external tagsets in part-of-speech tagging",
   "original": "e97_2787",
   "page_count": 4,
   "order": 704,
   "p1": "2787",
   "pn": "2790",
   "abstract": [
    "We present an approach to statistical part-of-speech tagging that uses two different tagsets, one for its internal and one for its external representation. The internal tagset is used in the underlying Markov model, while the external tagset constitutes the output of the tagger. The internal tagset can be modified and optimized to increase tagging accuracy (with respect to the external tagset). We evaluate this approach inan experiment and show that it performs significantly better than approaches using only one tagset.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-703"
  },
  "varin97_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Varin"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "A probabilistic model of double-vowel segregation",
   "original": "e97_2791",
   "page_count": 4,
   "order": 705,
   "p1": "2791",
   "pn": "2794",
   "abstract": [
    "The decomposition principle was first proposed by Varga and Moore [] and applied to Automatic Speech Recognition (ASR) in noise. We show a new adaptation of this principle to model the schema-based streaming process which was inferred after psychoacoustical studies []. We address here the classical problem of double vowel segregation. The signal decomposition is allowed by an internal and statistical model of vowel spectra. We apply this decomposition model able to reconstruct the spectra of superimposed signals after identification of only the dominant or of both members of the pair. Three stages are invoked. The first one is a module performing identification when the input is a mixture of interfering signals. Prior identification of the dominant spectra prevents combinatorial reconstruction. The second step is an evaluation of the mixture coefficient also based on an internal representation of spectra. Finally, the reconstruction of spectra is probabilistic, by the way of likelihood maximisation. It uses labels and mixture coefficient. This is tested on a large database of synthetic vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-704"
  },
  "houshang97_eurospeech": {
   "authors": [
    [
     "Habibzadeh V.",
     "Houshang"
    ],
    [
     "Kitazawa",
     "Shigeyoshi"
    ]
   ],
   "title": "Stimulus signal estimation from auditory-neural transduction inverse processing",
   "original": "e97_2795",
   "page_count": 4,
   "order": 706,
   "p1": "2795",
   "pn": "2798",
   "abstract": [
    "Auditory models reverse processing techniques would have very useful applications in speech perception and auditory models evaluation. This paper examines how we can be benefit an Inner Hair Cell (IHC) model as a compression and envelope detection section, in the cochlear model inverse processing. Our proposed inversion method, combines the reverse of the Meddis's auditory neural transduction model with Lyon's cochlear model to estimate the input signal to the inner ear from its auditory nerve firings, with the acceptable quality. Since this method uses neural firings or cleft contents as an input and re-generates the original acoustic stimulus, it is useful with any system generating auditory neural firings. For example, using this method, we are able to estimate the stimulus signal of the Nucleus Cochlear Implant systems to investigate the transferred speech quality without using the real patients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-705"
  },
  "tadj97b_eurospeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "FDVQ based keyword spotter which incorporates a semi-supervised learning for primary processing",
   "original": "e97_2799",
   "page_count": 4,
   "order": 707,
   "p1": "2799",
   "pn": "2802",
   "abstract": [
    "In this paper, we present a novel hybrid keyword spotting system that combines supervised and semi-supervised competitive learning algorithms. The first stage is a S-SOM (Semi-supervised Self- Organizing Map) module which is specifically designed for discrimination between keywords (KWs) and non-keywords (NKWs). The second stage is an FDVQ (Fuzzy Dynamic Vector Quantization) module which consists of discriminating between KWs detected by the first stage processing. The experiment on Switchboard database has show an improvement of about 6% on the accuracy of the system comparing to our best keyword-spotter one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-706"
  },
  "lublinskaja97b_eurospeech": {
   "authors": [
    [
     "V. V.",
     "Lublinskaja"
    ],
    [
     "Christian",
     "Sappok"
    ]
   ],
   "title": "The initial time Span of auditory processing used for speaker attribution of the speech signal",
   "original": "e97_2803",
   "page_count": 4,
   "order": 708,
   "p1": "2803",
   "pn": "2806",
   "abstract": [
    "Research on the temporal organisation of speech perception is focussed mostly on the linguistic categories of the input. What is the role of non-grammatical categories for this processes? What kind of mechanisms integrate both kinds of features within the online process of perception? Individual voice qualities and the position of the sentence within the text were chosen to test the time interval where decisions as to speaker belongingness are made. The results favour a model with a relatively fixed time span within which a familiar voice or a deviation from an inherent context expectancy are detected.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-707"
  },
  "strom97b_eurospeech": {
   "authors": [
    [
     "Nikko",
     "Ström"
    ]
   ],
   "title": "Sparse connection and pruning in large dynamic artificial neural networks",
   "original": "e97_2807",
   "page_count": 4,
   "order": 709,
   "p1": "2807",
   "pn": "2810",
   "abstract": [
    "This paper presents new methods for training large neural networks for phoneme probability estimation. A combination of the time-delay architecture and the recurrent network architecture is used to capture the important dynamic information of the speech signal. Motivated by the fact that the number of connections in fully connected recurrent networks grows super-linear with the number of hidden units, schemes for sparse connection and connection pruning are explored. It is found that sparsely connected networks outperform their fully connected counterparts with an equal or smaller number of connections. The networks are evaluated in a hybrid HMM/ANN system for phoneme recognition on the TIMIT database. The achieved phoneme error-rate, 28.3%, for the standard 39 phoneme set on the core test-set of the TIMIT database is not far from the lowest reported. All training and simulation software used is made freely available by the author, making reproduction of the results feasible.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-708"
  },
  "teodorescu97_eurospeech": {
   "authors": [
    [
     "Roxana",
     "Teodorescu"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Ioannis",
     "Dologlou"
    ]
   ],
   "title": "A modular initialization scheme for better speech recognition performance using hybrid systems of MLPs/HMMs",
   "original": "e97_2811",
   "page_count": 4,
   "order": 710,
   "p1": "2811",
   "pn": "2814",
   "abstract": [
    "This paper proposes a novel modular initialization scheme of Multilayer Perceptrons (MLPs) trained for phoneme classification. Small MLPs are trained to discriminate between a phoneme and all the others. In the next step they are merged using our novel initialization scheme in broad classes and trained further. In the last step we merge the broad phonetic MLPs using the same scheme to generate the final phonetic MLP. Experiments done on a Dutch language isolated word database showed that the scheme gives faster and better estimates of Bayesian a posteriori probabilities compared to random initialization. Moreover, given its modularity, the method offers the possibility to deal with high dimensional problems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-709"
  },
  "chernigovskaya97_eurospeech": {
   "authors": [
    [
     "Tatiana V.",
     "Chernigovskaya"
    ]
   ],
   "title": "Lateralization for auditory perception of foreign words",
   "original": "e97_2815",
   "page_count": 4,
   "order": 711,
   "p1": "2815",
   "pn": "2818",
   "abstract": [
    "This paper presents the experimental study of cerebral hemispheric engagement in auditory recognition of words depending on a set of linguistic factors. Words were native and foreign to the subjects. Listeners were normal right-handed adults with symmetrical hearing, native speakers of Russian; English was acquired as a second language at school. The stimuli were linguistically balanced lists of natural Russian and English words presented monaurally, white noise being contralateral masking. The data show strong overall left hemispheric advantage. The most significant factor for both hemispheres appeared to be 'frequency of usage' (contrary to 'word length'- characterizing the perception of native words). The second important factor was 'consonant ratio' for the RH and 'word length' for the LH.'Part of speech' was shown to be of minimal importance for both the hemispheres, 'stress position' -slightly more significant.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-710"
  },
  "kosarev97_eurospeech": {
   "authors": [
    [
     "Yuri",
     "Kosarev"
    ],
    [
     "Pavel",
     "Jarov"
    ],
    [
     "Alexander",
     "Osipov"
    ]
   ],
   "title": "The structural weighted sets method for continuous speech and text recognition",
   "original": "e97_2819",
   "page_count": 4,
   "order": 712,
   "p1": "2819",
   "pn": "2822",
   "abstract": [
    "In known approaches to speech recognition based on Dynamic Programming (DP) or Hidden Markov Modelling (HMM) time sequences of elements (feature vectors, sounds, letters, etc.) as objects of evaluating or matching are used directly. Both of these approaches have the same demerit: they both can be realised only in the course of the recurrent sequential process and can't be realised in parallel. In addition, the complexity of them are relatively high. In proposed below Structural Weighted Sets (SWS) method such sequence are reflected first into some structure as a set from relations between its elements and then a recognition is reduced to matching corresponding sets. So in this case a words matching can be realised as a finding an intersection of two sets and evaluating its relative weight. The possibility to carry out a processing in parallel is arisen. The results of simulation are represented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-711"
  },
  "sumner97_eurospeech": {
   "authors": [
    [
     "C. J.",
     "Sumner"
    ],
    [
     "D. F.",
     "Gillies"
    ]
   ],
   "title": "Lateral inhibitory networks for auditory processing",
   "original": "e97_2823",
   "page_count": 4,
   "order": 713,
   "p1": "2823",
   "pn": "2826",
   "abstract": [
    "A neural-network model is described that produces a rate-place representation from auditory nerve output that is of considerably higher frequency resolution than that from a standard auditory peripheral model. The neural circuits used are called Lateral Inhibitory Networks. They have long been known to be responsible for early spatio-temporal processing in the visual system. Here we investigate the use of such networks for early auditory processing. We describe the analytical basis, problems with various variants of the model, and show some initial results yielded by the research.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-712"
  },
  "reetz97_eurospeech": {
   "authors": [
    [
     "Henning",
     "Reetz"
    ]
   ],
   "title": "Missing fundamentals: a problem of auditory or mental processing?",
   "original": "e97_2827",
   "page_count": 4,
   "order": 714,
   "p1": "2827",
   "pn": "2830",
   "abstract": [
    "Subjects were presented with signal pairs with different musical intervals. Signals were sine tones, complex tones with a fundamental, and complex tones without a fundamental. Subjects had to decide, which signal pairs form a specific musical interval. Reaction times indicate that the perception of the 'missing fundamental' is a sort of musical processing and not necessarily a part of normal auditory processing in pitch perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-713"
  },
  "freitag97_eurospeech": {
   "authors": [
    [
     "F.",
     "Freitag"
    ],
    [
     "E.",
     "Monte"
    ],
    [
     "J.",
     "Salavedra"
    ]
   ],
   "title": "Predictive neural networks applied to phoneme recognition",
   "original": "e97_2831",
   "page_count": 4,
   "order": 715,
   "p1": "2831",
   "pn": "2834",
   "abstract": [
    "In this paper a phoneme recognition system based on predictive neural networks is proposed. Neural networks are used to predict observation vectors of speech frames. The obtained prediction error is used for phoneme recognition as 1) distortion measure on the frame level and 2) as feature, which is statistically modeled by the Rayleigh distribution. Continuous speech phoneme recognition experiments are performed different settings of the system are evaluated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-714"
  },
  "suhardi97_eurospeech": {
   "authors": [
    [
     "_",
     "Suhardi"
    ],
    [
     "Klaus",
     "Fellbaum"
    ]
   ],
   "title": "Empirical comparison of two multilayer perceptron-based keyword speech recognition algorithms",
   "original": "e97_2835",
   "page_count": 4,
   "order": 716,
   "p1": "2835",
   "pn": "2838",
   "abstract": [
    "In this paper, an empirical comparison of two multilayer perceptron (MLP)-based techniques for key- word speech recognition (wordspotting) is described. The techniques are the predictive neural model (PNM)-based wordspotting, in which the MLP is applied as a speech pattern predictor to compute a local distance between the acoustic vector and the phone model, and the hybrid HMM/MLP-based wordspotting, where the MLP is used as a state (phone) probability estimator given acoustic vectors. The comparison was performed with the same database. According to our experiments, the hybrid HMM/MLP-based technique excels the PNM-based techniques (~6.2 %).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-715"
  },
  "fukada97b_eurospeech": {
   "authors": [
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Sophie",
     "Aveline"
    ],
    [
     "Mike",
     "Schuster"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Segment boundary estimation using recurrent neural networks",
   "original": "e97_2839",
   "page_count": 4,
   "order": 717,
   "p1": "2839",
   "pn": "2842",
   "abstract": [
    "This paper describes a segment (e.g. phoneme) boundary estimation method based on recurrent neural networks (RNNs). The proposed method only requires acoustic observations to accurately estimate segment boundaries. Experimental results show that the proposed method can estimate segment boundaries significantly better than an HMM based method. Furthermore, we incorporate the RNN based segment boundary estimator into the HMM based and segment based recognition systems. As a result, the segment boundary estimates give useful information for reducing computational complexity and improving recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-716"
  },
  "schuster97_eurospeech": {
   "authors": [
    [
     "Mike",
     "Schuster"
    ]
   ],
   "title": "Incorporation of HMM output constraints in hybrid NN/HMM systems during training",
   "original": "e97_2843",
   "page_count": 4,
   "order": 718,
   "p1": "2843",
   "pn": "2846",
   "abstract": [
    "This paper describes a method to incorporate the HMM output constraints in frame based hybrid NN/HMM systems during training. While usually the NN parameters are adjusted to maximize the cross-entropy between the frame target probabilities and the network predictions assuming statistically independent outputs in time, in the approach described here the full likelihood of the utterance(s) using also the HMM output constraints, like for conventional HMM systems, is maximized. This is achieved by maximizing the state occupation probabilities after a forward/backward pass using the scaled likelihoods coming from the network. Making a simplifying approximation for the derivative for the back-propagation through the forward/backward pass, tests show that the proposed method gives consistently higher string (phoneme) recognition rates than the conventional approach that aims at maximizing cross-entropy at the frame level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-717"
  },
  "babkina97_eurospeech": {
   "authors": [
    [
     "Ludmila",
     "Babkina"
    ],
    [
     "Sergey",
     "Koval"
    ],
    [
     "Alexander",
     "Molchanov"
    ]
   ],
   "title": "Principles of the hearing periphery functioning in new methods of pitch detection and speech enhancement",
   "original": "e97_2847",
   "page_count": 4,
   "order": 719,
   "p1": "2847",
   "pn": "2850",
   "abstract": [
    "Spent researches show that one of mechanisms of human auditory system ensuring high noise resistance of vocal speech sounds recognition is an electromechanical envelope feedback, effecting in structures of inner ear in man. Digital modeling of hearing system peripheral section with a similar multichannel envelopes feedback has shown to be useful for pitch determination of vowels in noisy environment. The offered model provides robust pitch detection for signal/noise relation up to -12 - -14 dB. In number of cases such a noiseproof feature is better than for other existing methods and systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-718"
  },
  "meunier97b_eurospeech": {
   "authors": [
    [
     "Christine",
     "Meunier"
    ],
    [
     "Alain",
     "Content"
    ],
    [
     "Uli H.",
     "Frauenfelder"
    ],
    [
     "Ruth",
     "Kearns"
    ]
   ],
   "title": "The locus of the syllable effect: prelexical or lexical?",
   "original": "e97_2851",
   "page_count": 4,
   "order": 720,
   "p1": "2851",
   "pn": "2854",
   "abstract": [
    "The claim that the syllable constitutes a basic perceptual unit in French is commonly accepted. It is based in part on the syllable effect [1] obtained with words. The present study extends these syllable detection experiments to pseudowords. Four experiments failed to replicate the syllable effect observed on words. Detection responses in pseudowords are made as soon as sufficient information becomes available in the signal. The different pattern of results obtained with words and pseudowords suggests that the syllable effect is post-lexical rather than pre-lexical.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-719"
  },
  "lickley97_eurospeech": {
   "authors": [
    [
     "Robin J.",
     "Lickley"
    ],
    [
     "Ellen G.",
     "Bard"
    ]
   ],
   "title": "On not remembering disfluencies",
   "original": "e97_2855",
   "page_count": 4,
   "order": 721,
   "p1": "2855",
   "pn": "2858",
   "abstract": [
    "Disfluencies - repetitions and reformulations mid-sentence in normal spontaneous speech - are problematic for both psychological and computational models of speech understanding. Much effort is being applied to finding ways of adapting computational systems to detect and delete disfluencies. The input to such systems is usually an accurate transcription. We present results of an experiment in which human listeners are asked to give verbatim transcriptions of disfluent and fluent utterances. These suggest that listeners are seldom able to identify all the words \"deleted\" in disfluencies. While all types suffer, identification rates for repetitions are even worse than for other types. We attribute the results to difficulties in recall or coding for recall items which can not be identified with certainty. This inability seems to make human speech recognition more robust than current computational models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-720"
  },
  "andringa97_eurospeech": {
   "authors": [
    [
     "T.",
     "Andringa"
    ]
   ],
   "title": "Using an auditory model and leaky autocorrelators to tune in to speech",
   "original": "e97_2859",
   "page_count": 4,
   "order": 722,
   "p1": "2859",
   "pn": "2862",
   "abstract": [
    "This paper introduces a method to esitimate the spectrum of voiced speech in noise, based on an estimate of the fundamental frequency. The method uses the output of an auditory model that imitates the mechanics of the basilar membrane. The output of the segments of the model is used as an input to a set of leaky autocorrelator units (as simple neuron models) sensitive to a certain periodicity (delay). If a noisy vowel is presented to the system, the units sensitive to the fundamental period of that vowel respond most actively. The activity of the responding autocorrelator units as a function of segment number is a direct measure of the spectrum of the vowel. This technique is very robust and can, like humans, estimate the existence of a vowel in a SNR of -10 dB aperiodic speech-noise and formant frequencies in -3 to -6 dB. With this technique it is possible to split a mixture of sound sources in auditory entities (percepts) on the basis of pitch.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1997-721"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "rossi97_eurospeech",
    "zue97_eurospeech",
    "santen97_eurospeech",
    "junqua97_eurospeech",
    "bellegarda97_eurospeech",
    "lippmann97_eurospeech"
   ]
  },
  {
   "title": "Acoustic Modelling",
   "papers": [
    "dupont97_eurospeech",
    "wakita97_eurospeech",
    "chesta97_eurospeech",
    "schluter97_eurospeech",
    "lee97_eurospeech",
    "chengalvarayan97_eurospeech",
    "holter97_eurospeech",
    "beyerlein97_eurospeech",
    "hanna97_eurospeech",
    "jones97_eurospeech",
    "willett97_eurospeech",
    "beulen97_eurospeech",
    "duchateau97_eurospeech",
    "blomberg97_eurospeech",
    "pfau97_eurospeech",
    "goldberger97_eurospeech",
    "chang97_eurospeech",
    "hauenstein97_eurospeech",
    "hariharan97_eurospeech",
    "rodriguez97_eurospeech",
    "marino97_eurospeech",
    "kojima97_eurospeech",
    "rogina97_eurospeech",
    "roweis97_eurospeech",
    "tsopanoglou97_eurospeech",
    "cerisara97_eurospeech",
    "bitar97_eurospeech"
   ]
  },
  {
   "title": "Dynamic Articulatory Measurements",
   "papers": [
    "candille97_eurospeech",
    "stone97_eurospeech",
    "parlangeau97_eurospeech",
    "vilkman97_eurospeech",
    "demolin97_eurospeech",
    "badin97_eurospeech"
   ]
  },
  {
   "title": "Language Identification",
   "papers": [
    "zissman97_eurospeech",
    "corredorardoy97_eurospeech",
    "parris97_eurospeech",
    "kwan97_eurospeech",
    "andersen97_eurospeech",
    "navratil97_eurospeech"
   ]
  },
  {
   "title": "Neural Networks for Speech and Language Processing",
   "papers": [
    "rahim97_eurospeech",
    "rottland97_eurospeech",
    "moudenc97_eurospeech",
    "kurimo97_eurospeech",
    "castano97_eurospeech",
    "pican97_eurospeech"
   ]
  },
  {
   "title": "Training Techniques; Efficient Decoding in ASR",
   "papers": [
    "shinoda97_eurospeech",
    "modi97_eurospeech",
    "bocchieri97_eurospeech",
    "nock97_eurospeech",
    "kaltenmeier97_eurospeech",
    "girardi97_eurospeech",
    "mcdermott97_eurospeech",
    "rivlin97_eurospeech",
    "mohri97_eurospeech",
    "phillips97_eurospeech",
    "ortmanns97_eurospeech",
    "demuynck97_eurospeech",
    "padmanabhan97_eurospeech",
    "ravishankar97_eurospeech",
    "hovell97_eurospeech",
    "novak97_eurospeech",
    "stolcke97_eurospeech",
    "nguyen97_eurospeech",
    "iwasaki97_eurospeech"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "romano97_eurospeech",
    "vereecken97_eurospeech",
    "ode97_eurospeech",
    "mixdorff97_eurospeech",
    "tournemire97_eurospeech",
    "ni97_eurospeech",
    "brindopke97_eurospeech",
    "casacuberta97_eurospeech",
    "warnke97_eurospeech",
    "donzel97_eurospeech",
    "bruce97_eurospeech",
    "morlec97_eurospeech",
    "gibbon97_eurospeech",
    "elsner97_eurospeech",
    "bosch97_eurospeech",
    "buder97_eurospeech",
    "strangert97_eurospeech",
    "nakai97_eurospeech",
    "kim97_eurospeech",
    "dimperio97_eurospeech"
   ]
  },
  {
   "title": "Keyword and Topic Spotting",
   "papers": [
    "lin97_eurospeech",
    "junkawitsch97_eurospeech",
    "lau97_eurospeech",
    "bartkova97_eurospeech",
    "yamashita97_eurospeech",
    "noth97_eurospeech"
   ]
  },
  {
   "title": "Robustness in Recognition and Signal Processing",
   "papers": [
    "paliwal97_eurospeech",
    "zeljkovic97_eurospeech",
    "shozakai97_eurospeech",
    "torre97_eurospeech",
    "brendborg97_eurospeech",
    "abrash97_eurospeech",
    "hwang97_eurospeech",
    "kanedera97_eurospeech",
    "hong97_eurospeech",
    "rahim97b_eurospeech",
    "schless97_eurospeech",
    "gerven97_eurospeech",
    "doukas97_eurospeech",
    "taniguchi97_eurospeech",
    "avendano97_eurospeech",
    "martinez97_eurospeech",
    "dociofernandez97_eurospeech",
    "agaiby97_eurospeech",
    "kim97b_eurospeech",
    "chu97_eurospeech",
    "maes97_eurospeech",
    "kim97c_eurospeech",
    "gouvea97_eurospeech",
    "westphal97_eurospeech",
    "huerta97_eurospeech",
    "puel97_eurospeech",
    "chiang97_eurospeech"
   ]
  },
  {
   "title": "Modelling of Prosody",
   "papers": [
    "malliopoulos97_eurospeech",
    "pirker97_eurospeech",
    "hirose97_eurospeech",
    "fotinea97_eurospeech",
    "son97_eurospeech",
    "gros97_eurospeech"
   ]
  },
  {
   "title": "Microphone Arrays for Speech Enhancement",
   "papers": [
    "dorbecker97_eurospeech",
    "inoue97_eurospeech",
    "akagi97_eurospeech",
    "mahmoudi97_eurospeech",
    "nagata97_eurospeech",
    "giuliani97_eurospeech"
   ]
  },
  {
   "title": "Multilingual Recognition",
   "papers": [
    "wang97_eurospeech",
    "bonaventura97_eurospeech",
    "weng97_eurospeech",
    "billa97_eurospeech",
    "schultz97_eurospeech",
    "schultz97b_eurospeech"
   ]
  },
  {
   "title": "Language Specific Speech Analysis",
   "papers": [
    "pompinomarschall97_eurospeech",
    "thomas97_eurospeech",
    "geumann97_eurospeech",
    "wood97_eurospeech",
    "anderson97_eurospeech",
    "espywilson97_eurospeech"
   ]
  },
  {
   "title": "Feature Estimation, Pitch, and Prosody",
   "papers": [
    "hansen97_eurospeech",
    "halberstadt97_eurospeech",
    "milner97_eurospeech",
    "vuuren97_eurospeech",
    "nicholson97_eurospeech",
    "hernando97_eurospeech",
    "gaillard97_eurospeech",
    "simonin97_eurospeech",
    "fant97_eurospeech",
    "delopoulos97_eurospeech",
    "ohno97_eurospeech",
    "fujisaki97_eurospeech",
    "martinez97b_eurospeech",
    "lee97b_eurospeech",
    "traunmuller97_eurospeech",
    "brondsted97_eurospeech",
    "micallef97_eurospeech",
    "koval97_eurospeech",
    "hu97_eurospeech",
    "malayath97_eurospeech",
    "chengalvarayan97b_eurospeech",
    "beet97_eurospeech",
    "bajic97_eurospeech",
    "tambakas97_eurospeech",
    "noad97_eurospeech",
    "shimodaira97_eurospeech",
    "agranovski97_eurospeech",
    "montacie97_eurospeech",
    "doval97_eurospeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "vrecken97_eurospeech",
    "zolfaghari97_eurospeech",
    "chennoukh97_eurospeech",
    "mudugamuwa97_eurospeech",
    "takahashi97_eurospeech",
    "ismail97_eurospeech",
    "villette97_eurospeech",
    "chang97b_eurospeech",
    "vu97_eurospeech",
    "ramasubramanian97_eurospeech",
    "lemma97_eurospeech",
    "xydeas97_eurospeech",
    "choi97_eurospeech",
    "cho97_eurospeech",
    "kovesi97_eurospeech",
    "vepyek97_eurospeech",
    "gortz97_eurospeech",
    "matmti97_eurospeech",
    "ribeiro97_eurospeech",
    "baudoin97_eurospeech",
    "ghaemmaghami97_eurospeech",
    "torres97_eurospeech",
    "botinis97_eurospeech",
    "han97_eurospeech",
    "fingscheidt97_eurospeech",
    "seymour97_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis Techniques",
   "papers": [
    "ding97_eurospeech",
    "abe97_eurospeech",
    "banga97_eurospeech",
    "hwang97b_eurospeech",
    "santen97b_eurospeech",
    "gimenezdelosgalanes97_eurospeech",
    "karaali97_eurospeech",
    "hogberg97_eurospeech",
    "king97_eurospeech",
    "trancoso97_eurospeech",
    "rietveld97_eurospeech",
    "angelini97_eurospeech",
    "keller97_eurospeech",
    "fries97_eurospeech",
    "edgington97_eurospeech",
    "teixeiradejesus97_eurospeech",
    "black97_eurospeech",
    "jiang97_eurospeech",
    "bae97_eurospeech",
    "stylianou97_eurospeech"
   ]
  },
  {
   "title": "Technology for S&L Acquisition, Speech Processing Tools",
   "papers": [
    "sabah97_eurospeech",
    "zhou97_eurospeech",
    "geller97_eurospeech",
    "mccandless97_eurospeech",
    "witt97_eurospeech",
    "kilian97_eurospeech",
    "kitaazawa97_eurospeech",
    "kim97d_eurospeech",
    "ronen97_eurospeech",
    "alvarez97_eurospeech",
    "kawai97_eurospeech",
    "nouza97_eurospeech",
    "choi97b_eurospeech",
    "delmonte97_eurospeech",
    "kaspar97_eurospeech",
    "akbar97_eurospeech",
    "ehsani97_eurospeech",
    "goddeau97_eurospeech",
    "schalkwyk97_eurospeech",
    "ferenczi97_eurospeech",
    "laine97_eurospeech",
    "minematsu97_eurospeech",
    "taniguchi97b_eurospeech",
    "sutton97_eurospeech",
    "cucchiarini97_eurospeech",
    "holzrichter97_eurospeech",
    "epps97_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology",
   "papers": [
    "cavalcantealbano97_eurospeech",
    "kvale97_eurospeech",
    "moraes97_eurospeech",
    "steriopolo97_eurospeech",
    "rueber97_eurospeech",
    "zu97_eurospeech",
    "draxler97_eurospeech",
    "kavitskaya97_eurospeech",
    "seong97_eurospeech",
    "heid97_eurospeech",
    "batliner97_eurospeech",
    "petek97_eurospeech",
    "aguilar97_eurospeech",
    "caputo97_eurospeech",
    "magnocaldognetto97_eurospeech",
    "refice97_eurospeech",
    "moosmuller97_eurospeech",
    "hoskins97_eurospeech",
    "turk97_eurospeech",
    "bessac97_eurospeech",
    "demolin97b_eurospeech",
    "hernaez97_eurospeech",
    "oboyle97_eurospeech",
    "berrah97_eurospeech",
    "moen97_eurospeech",
    "ladefoged97_eurospeech",
    "wang97b_eurospeech",
    "ueyama97_eurospeech"
   ]
  },
  {
   "title": "Confidence Measures in ASR",
   "papers": [
    "fetter97_eurospeech",
    "chase97_eurospeech",
    "bergen97_eurospeech",
    "bernstein97_eurospeech",
    "kemp97_eurospeech",
    "siu97_eurospeech"
   ]
  },
  {
   "title": "Speaker and Language Identification",
   "papers": [
    "hussain97_eurospeech",
    "li97_eurospeech",
    "sarma97_eurospeech",
    "sokolov97_eurospeech",
    "lindberg97_eurospeech",
    "schmidt97_eurospeech"
   ]
  },
  {
   "title": "Perception of Prosody",
   "papers": [
    "rietveld97b_eurospeech",
    "caspers97_eurospeech",
    "mersdorf97_eurospeech",
    "auberge97_eurospeech",
    "heldner97_eurospeech",
    "house97_eurospeech"
   ]
  },
  {
   "title": "Applications of Speech Technology",
   "papers": [
    "lau97b_eurospeech",
    "carey97_eurospeech",
    "jones97b_eurospeech",
    "hemphill97_eurospeech",
    "verhelst97_eurospeech",
    "robertribes97_eurospeech",
    "moller97_eurospeech",
    "hatzis97_eurospeech",
    "lin97b_eurospeech",
    "leonardi97_eurospeech",
    "dubois97_eurospeech",
    "rubio97_eurospeech",
    "lopezcozar97_eurospeech",
    "shields97_eurospeech",
    "piroth97_eurospeech",
    "huttunen97_eurospeech",
    "ciocea97_eurospeech",
    "lobanov97_eurospeech",
    "ackermann97_eurospeech",
    "karjalainen97_eurospeech",
    "alarotu97_eurospeech",
    "ehrlich97_eurospeech",
    "anthony97_eurospeech",
    "falavigna97_eurospeech",
    "menendezpidal97_eurospeech",
    "torre97b_eurospeech",
    "os97_eurospeech",
    "julia97_eurospeech"
   ]
  },
  {
   "title": "Spontaneous Speech Recognition",
   "papers": [
    "gauvain97_eurospeech",
    "alleva97_eurospeech",
    "matsuok97_eurospeech",
    "cettolo97_eurospeech",
    "bauche97_eurospeech",
    "kubala97_eurospeech"
   ]
  },
  {
   "title": "Language Specific Segmental Features",
   "papers": [
    "cambierlangeveld97_eurospeech",
    "meunier97_eurospeech",
    "riele97_eurospeech",
    "fougeron97_eurospeech",
    "bruyninckx97_eurospeech",
    "gros97b_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "li97b_eurospeech",
    "he97_eurospeech",
    "reynolds97_eurospeech",
    "kimball97_eurospeech",
    "bimbot97_eurospeech",
    "kuitert97_eurospeech",
    "rosenberg97_eurospeech",
    "olsen97_eurospeech",
    "ariyaeeinia97_eurospeech",
    "jauquet97_eurospeech",
    "bimbot97b_eurospeech",
    "sonmez97_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Linguistic Analysis",
   "papers": [
    "campbell97_eurospeech",
    "bechet97_eurospeech",
    "gilifivela97_eurospeech",
    "krenn97_eurospeech",
    "black97b_eurospeech",
    "widera97_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis and Modelling",
   "papers": [
    "kuwabara97_eurospeech",
    "narayanan97_eurospeech",
    "arai97_eurospeech",
    "esposito97_eurospeech",
    "bolfanstosic97_eurospeech",
    "kipp97_eurospeech",
    "downey97_eurospeech",
    "hauenstein97b_eurospeech",
    "botha97_eurospeech",
    "vaxelaire97_eurospeech",
    "eriksson97_eurospeech",
    "strik97_eurospeech",
    "fosnot97_eurospeech",
    "wrench97_eurospeech",
    "gutierrezarriola97_eurospeech",
    "savariaux97_eurospeech",
    "jovanovic97_eurospeech",
    "furukawa97_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Design and Applications",
   "papers": [
    "barnett97_eurospeech",
    "seide97_eurospeech",
    "larsen97_eurospeech",
    "hugunin97_eurospeech",
    "denecke97_eurospeech",
    "issar97_eurospeech",
    "flammia97_eurospeech",
    "pieraccini97_eurospeech",
    "abella97_eurospeech",
    "levin97_eurospeech",
    "boros97_eurospeech",
    "maier97_eurospeech"
   ]
  },
  {
   "title": "Speech Production Modelling",
   "papers": [
    "arslan97_eurospeech",
    "mokbel97_eurospeech",
    "potamianos97_eurospeech",
    "mawass97_eurospeech",
    "claes97_eurospeech",
    "dobrisek97_eurospeech"
   ]
  },
  {
   "title": "Speech Enhancement and Noise Mitigation",
   "papers": [
    "haulick97_eurospeech",
    "yegnanarayana97_eurospeech",
    "gustafsson97_eurospeech",
    "lee97c_eurospeech",
    "yoma97_eurospeech",
    "tsoukalas97_eurospeech",
    "girin97_eurospeech",
    "engelsberg97_eurospeech",
    "chien97_eurospeech",
    "lawrence97_eurospeech",
    "langmann97_eurospeech",
    "delphinpoulat97_eurospeech",
    "miksic97_eurospeech",
    "tolba97_eurospeech",
    "unoki97_eurospeech",
    "sika97_eurospeech",
    "abdallah97_eurospeech",
    "moreno97_eurospeech",
    "cole97_eurospeech",
    "jones97c_eurospeech",
    "hussain97b_eurospeech",
    "kobatake97_eurospeech",
    "tibrewala97_eurospeech",
    "masgrau97_eurospeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "wright97_eurospeech",
    "minker97_eurospeech",
    "riley97_eurospeech",
    "lieske97_eurospeech",
    "papineni97_eurospeech",
    "amengual97_eurospeech"
   ]
  },
  {
   "title": "Language Model Adaptation",
   "papers": [
    "gotoh97_eurospeech",
    "martin97_eurospeech",
    "bellegarda97b_eurospeech",
    "schwartz97_eurospeech",
    "popovici97_eurospeech",
    "lin97c_eurospeech"
   ]
  },
  {
   "title": "Prosody and Speech Recognition/Understanding",
   "papers": [
    "langlais97_eurospeech",
    "ozeki97_eurospeech",
    "chung97_eurospeech",
    "strom97_eurospeech",
    "heuven97_eurospeech",
    "munteanu97_eurospeech"
   ]
  },
  {
   "title": "Wideband Speech Coding",
   "papers": [
    "kataoka97_eurospeech",
    "lynch97_eurospeech",
    "amodio97_eurospeech",
    "guimaraes97_eurospeech",
    "chan97_eurospeech",
    "salavedra97_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition in Adverse Environments CSR and Error Analysis",
   "papers": [
    "hung97_eurospeech",
    "hung97b_eurospeech",
    "komori97_eurospeech",
    "endo97_eurospeech",
    "han97b_eurospeech",
    "yamamoto97_eurospeech",
    "mauuary97_eurospeech",
    "chen97_eurospeech",
    "spina97_eurospeech",
    "ng97_eurospeech",
    "gong97_eurospeech",
    "neto97_eurospeech",
    "chase97b_eurospeech",
    "nakamura97_eurospeech",
    "watson97_eurospeech",
    "muthusamy97_eurospeech",
    "meyer97_eurospeech",
    "kobayashi97_eurospeech",
    "nokas97_eurospeech",
    "guan97_eurospeech",
    "vaich97_eurospeech"
   ]
  },
  {
   "title": "Multimodal Speech Processing, Emerging Techniques and Applications",
   "papers": [
    "raptis97_eurospeech",
    "jourlin97_eurospeech",
    "ng97b_eurospeech",
    "teissier97_eurospeech",
    "renevey97_eurospeech",
    "mokbel97b_eurospeech",
    "nakamura97b_eurospeech",
    "depambour97_eurospeech",
    "paping97_eurospeech",
    "martino97_eurospeech",
    "sarukkai97_eurospeech",
    "nakatoh97_eurospeech",
    "rogers97_eurospeech",
    "beskow97_eurospeech",
    "robbe97_eurospeech",
    "suhm97_eurospeech",
    "reveret97_eurospeech",
    "goff97_eurospeech",
    "adjoudani97_eurospeech",
    "fujisaki97b_eurospeech",
    "yardimci97_eurospeech",
    "burnham97_eurospeech"
   ]
  },
  {
   "title": "Databases, Tools and Evaluations",
   "papers": [
    "schiel97_eurospeech",
    "draxler97b_eurospeech",
    "engberg97_eurospeech",
    "eskenazi97_eurospeech",
    "feldes97_eurospeech",
    "neto97b_eurospeech",
    "nord97_eurospeech",
    "rothkrantz97_eurospeech",
    "bensaber97_eurospeech",
    "kim97e_eurospeech",
    "dutton97_eurospeech",
    "mella97_eurospeech",
    "grocholewski97_eurospeech",
    "muller97_eurospeech",
    "hansen97b_eurospeech",
    "taylor97_eurospeech",
    "itahashi97_eurospeech",
    "chatzi97_eurospeech"
   ]
  },
  {
   "title": "Speaker Adaptation I",
   "papers": [
    "huo97_eurospeech",
    "aubert97_eurospeech",
    "illina97_eurospeech",
    "digalakis97_eurospeech",
    "kannan97_eurospeech",
    "heck97_eurospeech"
   ]
  },
  {
   "title": "Assessment Methods",
   "papers": [
    "martin97b_eurospeech",
    "klaus97_eurospeech",
    "pallett97_eurospeech",
    "fraser97_eurospeech",
    "bernsen97_eurospeech",
    "leeuwen97_eurospeech"
   ]
  },
  {
   "title": "Education for Language and Speech Communication",
   "papers": [
    "huckvale97_eurospeech",
    "bloothooft97_eurospeech",
    "sjolander97_eurospeech",
    "beck97_eurospeech",
    "green97_eurospeech",
    "hazan97_eurospeech"
   ]
  },
  {
   "title": "Hybrid Systems for ASR",
   "papers": [
    "tu97_eurospeech",
    "dupont97b_eurospeech",
    "hennebert97_eurospeech",
    "williams97_eurospeech",
    "cook97_eurospeech",
    "fritsch97_eurospeech"
   ]
  },
  {
   "title": "Topic and Dialogue Dependent Language Modelling",
   "papers": [
    "witschel97_eurospeech",
    "kneser97_eurospeech",
    "iyer97_eurospeech",
    "rao97_eurospeech",
    "ries97_eurospeech",
    "seymore97_eurospeech"
   ]
  },
  {
   "title": "Lipreading",
   "papers": [
    "luettin97_eurospeech",
    "goldenthal97_eurospeech",
    "rogozan97_eurospeech",
    "beskow97b_eurospeech",
    "stiefelhagen97_eurospeech",
    "reveret97b_eurospeech"
   ]
  },
  {
   "title": "Articulatory Modelling",
   "papers": [
    "mathieu97_eurospeech",
    "payan97_eurospeech",
    "sanguineti97_eurospeech",
    "mohammad97_eurospeech",
    "vatikiotisbateson97_eurospeech",
    "loevenbruck97_eurospeech"
   ]
  },
  {
   "title": "Front-Ends and Adaptation to Acoustics Speaker Adaptation",
   "papers": [
    "komori97b_eurospeech",
    "yamada97_eurospeech",
    "hazen97_eurospeech",
    "yamaguchi97_eurospeech",
    "zeljkovic97b_eurospeech",
    "afify97_eurospeech",
    "diakoloukas97_eurospeech",
    "gales97_eurospeech",
    "fontaine97_eurospeech",
    "tchorz97_eurospeech",
    "morgan97_eurospeech",
    "holmes97_eurospeech",
    "zhan97_eurospeech",
    "gao97_eurospeech",
    "lincoln97_eurospeech",
    "welling97_eurospeech",
    "logan97_eurospeech",
    "nitta97_eurospeech",
    "chen97b_eurospeech"
   ]
  },
  {
   "title": "Speech Perception",
   "papers": [
    "ainsworth97_eurospeech",
    "schiller97_eurospeech",
    "morris97_eurospeech",
    "barker97_eurospeech",
    "pallier97_eurospeech",
    "son97b_eurospeech",
    "bonneau97_eurospeech",
    "li97c_eurospeech",
    "skljarov97_eurospeech",
    "lugt97_eurospeech",
    "jansens97_eurospeech",
    "pallier97b_eurospeech",
    "tokuma97_eurospeech",
    "koster97_eurospeech",
    "behne97_eurospeech",
    "neagu97_eurospeech",
    "kitagawa97_eurospeech",
    "pitermann97_eurospeech",
    "plauche97_eurospeech",
    "dumay97_eurospeech",
    "zhu97_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Linguistic Structures, Modelling and Evaluation",
   "papers": [
    "lin97d_eurospeech",
    "kamm97_eurospeech",
    "pouteau97_eurospeech",
    "lee97d_eurospeech",
    "dahlback97_eurospeech",
    "walker97_eurospeech",
    "byron97_eurospeech",
    "zue97b_eurospeech",
    "alexandersson97_eurospeech",
    "reithinger97_eurospeech",
    "pernel97_eurospeech",
    "suzuki97_eurospeech",
    "churcher97_eurospeech",
    "niimi97_eurospeech",
    "mathieu97b_eurospeech",
    "thanopoulos97_eurospeech",
    "mellor97_eurospeech",
    "hockey97_eurospeech",
    "moller97b_eurospeech",
    "gustafson97_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition and Language Identification",
   "papers": [
    "yuo97_eurospeech",
    "dersch97_eurospeech",
    "vergin97_eurospeech",
    "afify97b_eurospeech",
    "tadj97_eurospeech",
    "gravier97_eurospeech",
    "auckenthaler97_eurospeech",
    "slomka97_eurospeech",
    "lavner97_eurospeech",
    "rodriguezlinares97_eurospeech",
    "steeneken97_eurospeech",
    "kumpf97_eurospeech",
    "liu97_eurospeech",
    "goddijn97_eurospeech",
    "eberman97_eurospeech",
    "hume97_eurospeech",
    "chengalvarayan97c_eurospeech",
    "fakotakis97_eurospeech",
    "koolwaaij97_eurospeech",
    "koolwaaij97b_eurospeech",
    "montacie97b_eurospeech",
    "hernando97b_eurospeech"
   ]
  },
  {
   "title": "Style and Accent Recognition",
   "papers": [
    "humphries97_eurospeech",
    "potamianos97b_eurospeech",
    "teixeira97_eurospeech",
    "finke97_eurospeech",
    "shriberg97_eurospeech",
    "boughazale97_eurospeech"
   ]
  },
  {
   "title": "Towards Robust ASR for Car and Telephone Applications",
   "papers": [
    "fissore97_eurospeech",
    "nakamura97c_eurospeech",
    "faucon97_eurospeech",
    "haebumbach97_eurospeech",
    "compernolle97_eurospeech"
   ]
  },
  {
   "title": "Language-Specific Systems",
   "papers": [
    "williams97b_eurospeech",
    "ferencz97_eurospeech",
    "mobius97_eurospeech",
    "fitt97_eurospeech",
    "pavlova97_eurospeech",
    "bonafonte97_eurospeech"
   ]
  },
  {
   "title": "Pronunciation Models",
   "papers": [
    "cremelie97_eurospeech",
    "elvira97_eurospeech",
    "ravishankar97b_eurospeech",
    "fukada97_eurospeech",
    "jost97_eurospeech",
    "markey97_eurospeech"
   ]
  },
  {
   "title": "Auditory Modelling and Psychoacoustics",
   "papers": [
    "berthommier97_eurospeech",
    "kortekaas97_eurospeech",
    "lublinskaja97_eurospeech",
    "meyer97b_eurospeech",
    "perdigao97_eurospeech",
    "marta97_eurospeech"
   ]
  },
  {
   "title": "Voice Conversion and Data Driven F0-Models",
   "papers": [
    "santen97c_eurospeech",
    "boeffard97_eurospeech",
    "lopezgonzalo97_eurospeech",
    "kim97f_eurospeech",
    "yoshimura97_eurospeech",
    "darsinos97_eurospeech"
   ]
  },
  {
   "title": "Vocal Tract Analysis",
   "papers": [
    "maragos97_eurospeech",
    "richards97_eurospeech",
    "lee97e_eurospeech",
    "teixeira97b_eurospeech",
    "schoentgen97_eurospeech",
    "yu97_eurospeech"
   ]
  },
  {
   "title": "F0 and Duration Modelling, Spoken language processing",
   "papers": [
    "riedi97_eurospeech",
    "malfrere97_eurospeech",
    "campbell97b_eurospeech",
    "neukirchen97_eurospeech",
    "veronis97_eurospeech",
    "sanderman97_eurospeech",
    "santen97d_eurospeech",
    "barbosa97_eurospeech",
    "halber97_eurospeech",
    "amtrup97_eurospeech",
    "tillmann97_eurospeech",
    "fujisawa97_eurospeech",
    "corrigan97_eurospeech",
    "ishikawa97_eurospeech",
    "colas97_eurospeech",
    "horiguchi97_eurospeech",
    "caraty97_eurospeech"
   ]
  },
  {
   "title": "Language Modelling",
   "papers": [
    "segarra97_eurospeech",
    "zhang97_eurospeech",
    "wang97c_eurospeech",
    "clarkson97_eurospeech",
    "adda97_eurospeech",
    "damnati97_eurospeech",
    "matsunaga97_eurospeech",
    "smaili97_eurospeech",
    "perennou97_eurospeech",
    "schukattalamazzini97_eurospeech",
    "ito97_eurospeech",
    "siu97b_eurospeech",
    "geutner97_eurospeech",
    "nagai97_eurospeech",
    "brugnara97_eurospeech",
    "demetriou97_eurospeech",
    "tsukada97_eurospeech",
    "taylor97b_eurospeech",
    "heeman97_eurospeech",
    "uhrik97_eurospeech",
    "chelba97_eurospeech",
    "stolcke97b_eurospeech",
    "kenne97_eurospeech",
    "brants97_eurospeech"
   ]
  },
  {
   "title": "Auditory Modelling and Psychoacoustics, Neural Networks for Speech Processing and Recognition",
   "papers": [
    "varin97_eurospeech",
    "houshang97_eurospeech",
    "tadj97b_eurospeech",
    "lublinskaja97b_eurospeech",
    "strom97b_eurospeech",
    "teodorescu97_eurospeech",
    "chernigovskaya97_eurospeech",
    "kosarev97_eurospeech",
    "sumner97_eurospeech",
    "reetz97_eurospeech",
    "freitag97_eurospeech",
    "suhardi97_eurospeech",
    "fukada97b_eurospeech",
    "schuster97_eurospeech",
    "babkina97_eurospeech",
    "meunier97b_eurospeech",
    "lickley97_eurospeech",
    "andringa97_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1997"
}