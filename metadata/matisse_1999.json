{
 "title": "MATISSE - Method and Tool Innovations for Speech Science Education",
 "location": "University College, London, UK",
 "startDate": "16/4/1999",
 "endDate": "17/4/1999",
 "conf": "MATISSE",
 "year": "1999",
 "name": "matisse_1999",
 "series": "",
 "SIG": "",
 "title1": "MATISSE - Method and Tool Innovations for Speech Science Education",
 "date": "16-17 April 1999",
 "booklet": "matisse_1999.pdf",
 "papers": {
  "berkovitz99_matisse": {
   "authors": [
    [
     "Robert",
     "Berkovitz"
    ]
   ],
   "title": "Design, development and evaluation of computer-assisted learning for speech science education",
   "original": "mati_009",
   "page_count": 8,
   "order": 1,
   "p1": "9",
   "pn": "16",
   "abstract": [
    "From 1994-1997, members of the staff at Sensimetrics Corporation and a group of academic consultants created a series of computer-based instructional units in speech science. The resulting CD-ROM [1] was released in 1997 as Speech Production and Perception I. Participants in the project had to devise models used to explain and demonstrate the subject matter. In addition, academic researchers needed to learn to communicate their ideas and understand those of programmers and graphic designers who had no prior knowledge of speech science. The project was financed by grants from the U. S. National Institutes of Health. Despite our planning efforts, the work took more time than we expected, and cost more money than had been provided by our grant. However, Speech Production and Perception I has been well-received by instructors, is increasingly adopted for class use at universities, and is now being distributed internationally. Work on this course has led to development of computer-based teaching materials for other subjects, for which we have retained the staff and further applied the capabilities developed during this project.\n",
    ""
   ]
  },
  "cole99_matisse": {
   "authors": [
    [
     "Ron",
     "Cole"
    ],
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Jacques de",
     "Villiers"
    ],
    [
     "Brian",
     "Rundle"
    ],
    [
     "Khaldoun",
     "Shobaki"
    ],
    [
     "Johan",
     "Wouters"
    ],
    [
     "Michael",
     "Cohen"
    ],
    [
     "Jonas",
     "Baskow"
    ],
    [
     "Patrick",
     "Stone"
    ],
    [
     "Pamela",
     "Connors"
    ],
    [
     "Alice",
     "Tarachow"
    ],
    [
     "Daniel",
     "Solcher"
    ]
   ],
   "title": "New tools for interactive speech and language training: Using animated conversational agents in theclassroom of profoundly deaf children",
   "original": "mati_045",
   "page_count": 8,
   "order": 2,
   "p1": "45",
   "pn": "52",
   "abstract": [
    "This article describes our experienees with an animated conversational agent being used in daily classroom activities with profoundly deaf children at the Tucker Maxon Oral School in Portland Oregon. We first articulate some reasons why animated conversational agents can revolutionize learning and language training by providing a more effective mode of human computer interaction. We then describe the capabilities of our animated agent, Baldi, and the software environment used to design and run interactive media systems. We then describe applications designed by teachers and students that illustrate ways in which students in three different classrooms converse and interact with Baldi. We conclude with a brief look at the next generation of animated conversational agents.\n",
    ""
   ]
  },
  "roach99_matisse": {
   "authors": [
    [
     "Peter",
     "Roach"
    ]
   ],
   "title": "Developments in speech sciences curricula",
   "original": "mati_097",
   "page_count": 4,
   "order": 3,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "Training students in phonetics or speech science involves much more than simply putting on a course of lectures, It requires careful planning of practical work, lecture-based teaching and small-group activitias. The main components of this teaching are reviewed, and their place in a curriculum are discussed.\n",
    ""
   ]
  },
  "carsonberndsen99_matisse": {
   "authors": [
    [
     "Julie",
     "Carson-Berndsen"
    ],
    [
     "Dafydd",
     "Gibbon"
    ]
   ],
   "title": "Interactive Phonetics, virtually",
   "original": "mati_017",
   "page_count": 4,
   "order": 4,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "We present a set of phonetics teaching resources as modules in a more generic framework for web-based tutoring in the areas of phonetics, spoken language resources, and multimedia communication. These tools have been developed initially to supplement more traditional classroom teaching materials but experience has shown that students used these tools independently in their own study time, and indeed became interested in developing their own tool applications to help them understand the subject more thoroughly.\n",
    "Currently the toolkit consists of standalone interactive modules and lecture notes, including interactive Java and CGI tools for waveform generation and display, database access, phonetic alphabets, stylised articulatory phonetic animations with and without underlying phonological models, development and testing of syllable models construction of web-based hyperlexica and basic techniques for visual and speech synthesis.\n",
    "Our main emphasis has been on interactivity and widespread access for the student. The toolkit offers more flexibility than current CD-based tutorials in that it does not use canned examples (in the sense of stored video sequences, files and predefined frequency and amplitude specifications for waveform display) but rather calculates the examples on-the-fly, and is easily updated, maintained and extended in the web context, and permits additional interaction between student and tutor (chat lounge, email). Our presentation will demonstrate the underlying motivation, the computational linguistic and phonetic modelling and the actual use of these tools as phonetic teaching resources.\n",
    ""
   ]
  },
  "wrigley99_matisse": {
   "authors": [
    [
     "Stuart",
     "Wrigley"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Guy",
     "Brown"
    ]
   ],
   "title": "Interactive learning in speech and hearing",
   "original": "mati_021",
   "page_count": 4,
   "order": 5,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "As part of an ongoing action research study, we have appraised our traditional techniques for the teaching of speech and computational hearing and found them to be inadequate in some respects. In particular, they offer little opportunity for students to experiment interactively with concepts in these fields. As a result, students cannot reflect upon their own experiences nor learn from them. In this paper, we discuss a more interactive approach based on software applications which allow the student to explore the parameter space of the phenomena under discussion. Several examples (chosen from a range of more than twenty MATLAB applications) which illustrate the philosophical and pedagogical elements of this ongoing project are discussed. These include animations of basilar membrane motion, explorations of concurrent vowel perception, interactive visualisations of cepstral processing and computer-based audiometry training.\n",
    ""
   ]
  },
  "drygajlo99_matisse": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Guy",
     "Delafontaine"
    ]
   ],
   "title": "Using Java to develop interactive learning work-bench for speech analysis basics on theWorld-Wide Web",
   "original": "mati_025",
   "page_count": 4,
   "order": 6,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "Platform-independent, interactive, on-line laboratories accessible on the Internet, can increase the efficiency of student self-study in the speech analysis domain which is inherently multimedia in nature, involving both sound and vision. In the context of distance learning on the World-Wide Web, we argue that Java is a natural language to develop interactive educational material for such laboratories that can be shared and distributed widely. The interactive and computational capabilities of Java are demonstrated through modular speech analysis laboratory (JavaSpeechLab) based on Java application and applet. The JavaSpeechLab features a graphical user-friendly interface and can be used on the World-Wide Web through a Java-enabled Web browser. JavaSpeechLab is an excellent Computer-Assisted Learning (CAL) work-bench because of its conceptual ease. In this software laboratory the common \"frame-window\" environment is provided for all types of analysis from the simplest in the time domain (e.g. short-time energy or zero-crossing rate) through the classical one using short-term Fourier transform to the most complex in the time-spectral domain based on multi-resolution wavelet packet transforms which use different windows and frames at different frequency subbands. Desirable features, capabilities, deficiencies, and current implementations in the Signal Processing Laboratory (LTS) of the Swiss Federal Institute of Technology Lausanne (EPFL) of this CAL system for learning speech analysis techniques are discussed.\n",
    ""
   ]
  },
  "beck99_matisse": {
   "authors": [
    [
     "Janet M.",
     "Beck"
    ],
    [
     "Alistair M.",
     "Lawson"
    ],
    [
     "Ineke",
     "Mennen"
    ]
   ],
   "title": "InterPHACE: Internetworked links for PHonetic Analysis in ClinicalEducation",
   "original": "mati_029",
   "page_count": 4,
   "order": 7,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "This collaborative project addresses two key issues. The first is the need for Speech and Language Therapy (SALT) students to develop skills in analysis of a wide range of speech disorders and to apply these to clinical practice. The second is the lack of opportunities for utilising instrumental speech analysis techniques in many SALT clinics.\n",
    "",
    "",
    "Computerised links between Queen Margaret College and a variety of Health Care Trust clinics have been established which allow SALT clinic sessions to be monitored and speech data collection controlled remotely from QMC. Speech data is transmitted to the QMC Speech Laboratory for analysis and compilation of results. QMC staff and students are then able to discuss the results by teleconference with the SALT responsible for each client_s management, thus learning about the impact of analysis results on diagnosis and management. A data-base of clinical speech material is also being developed as a teaching/learning resource.\n",
    "",
    "",
    "A novel form of curricular design and delivery is thus being developed, which allows college staff and students access to speech from disordered clients in SALT clinics, and allows students to be actively involved in the process of clinical speech analysis of a range of current cases. Evaluation of the educational value of this project is ongoing, but it should enhance both undergraduate education in clinical phonetics and continuing professional development.\n",
    ""
   ]
  },
  "korkko99_matisse": {
   "authors": [
    [
     "Pentti",
     "Körkkö"
    ],
    [
     "Kerttu",
     "Huttunen"
    ],
    [
     "Martti",
     "Sorri"
    ]
   ],
   "title": "HI-SIM v1.0: Hearing impairment simulation on a CD-ROM",
   "original": "mati_033",
   "page_count": 4,
   "order": 8,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "Traditional classroom techniques lacking the advantages offered by multimedia demonstrations often fall short of the goal of furnishing the students with an adequate \"ears-on\" awareness of the problems of the hearing-impaired. As a first step towards virtual hearing impairment, a CD-ROM with speech material simulating a selection of common types of hearing impairment was produced. The material simulating hearing impairment was produced using digital signal processing, i.e. digital filtering and mixing of speech and noise. The CD-ROM, intended as an interactive educational tool, allows students and others interested in the effects of hearing defects a virtual experience of hearing impairments such as hearing loss due to chronic otitis media, noise exposure and aging. User-selectable options available in the simulation include grade of hearing impairment, audiometric configuration, type and level of background noise, reverberation time and listening distance. Clickable menus and buttons are used for selecting the simulation options. Word recognition scores can be computed for standard Finnish audiometric material.\n",
    ""
   ]
  },
  "lybergahlander99_matisse": {
   "authors": [
    [
     "Viveka",
     "Lyberg Åhlander"
    ],
    [
     "Eva",
     "Falk Nilsson"
    ],
    [
     "Eva",
     "Wigforss"
    ],
    [
     "Roland",
     "Rydell"
    ]
   ],
   "title": "The Project for the Development of Multimedia Methods in Logopedics and Phoniatrics(PUMP)",
   "original": "mati_037",
   "page_count": 4,
   "order": 9,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "PUMP is a web-based multimedia application, meant as an interactive support in several subject areas of the logopedic education. This educational programme is well suited for the use of multimedia pedagogics since the students need to be trained in various skills with dimensions of both auditive and visual perception, areas that can very well be better integrated by computer-based training than by traditional methods. We have started with the area of voice pathology and treatment. The programme in logopedics at Lund University is based on Problem Based Learning (PBL). Therefore one of the self-test parts in the application consists of problem based cases. This report is the first step in an evaluation of the PUMP application as a whole. This first step describes the ongoing evaluation and assessment which the students perform when working with PUMP. Through the evaluation it is possible to continuously improve and adapt the application. The report also describes how the students experience the usability of the application and how that correlates with the expectations of the developers. The main ideas and the content of the application will also be presented.\n",
    ""
   ]
  },
  "wigforss99_matisse": {
   "authors": [
    [
     "Eva",
     "Wigforss"
    ]
   ],
   "title": "European Virtual University - ODL Voice Course",
   "original": "mati_041",
   "page_count": 4,
   "order": 10,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "This paper presents an experimental Socrates ODL pilot project (EVU-Voice Course). The aim of the project is to implement new information technologies (e.g mulitimedia in Internet/Intranet) in the conventional educational programmes in Logopedics (Speech and Language Pathology and Therapy). The project has been developed partly through 8 partners' collaboration within Erasmus and the Socrates Tematic Network in Speech Communication Sciences, and partly through a special grant from the Swedish National Agency for Higher Education. The partners in the project are from 8 university departments in Europe; namly in Ireland, Finland, Spain, UK and Sweden.\n",
    "",
    "",
    "The author will present the \" knowledge-node\" concept used in the project. The node is a metaconstruction/demonstrator for a virtual arena on Internet/Intranet were the users can access academically quality checked information. Through the EVU-Voice Course we intend to create a virtual course space where students (clinical staff, teachers and patients), independently of space and time, can take a real part in life-long-learning. Thus, the main objectives are to promote a) cooperation and b) collaborative learning beteween European institutions (universities, hospitals and schools).The main activities during 1999 will be to adapt, implement and evaluate an interactive web-based Voice Course in the partner university departments. EVU-Voice will be recognized within the ECTS framework and it will serve as a model for improving the vocational training of logpedists in Europe.\n",
    "",
    "",
    "EVU-voice course URL http://www.ldc.lu.se/logopedi/EVU-Voice/\n",
    ""
   ]
  },
  "hazan99_matisse": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Wim van",
     "Dommelen"
    ]
   ],
   "title": "Curriculum development in phonetics education",
   "original": "mati_101",
   "page_count": 4,
   "order": 11,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "The study of Phonetics is characterised by its multidisciplinary orientation as it can be included in different specialisations such as language learning, speech and language therapy, speech technology and linguistics. Although historically phonetics was considered as belonging to the humanities, it has now strong connections with science and engineering. Until recently, the field of phonetics education was rather fragmented as there were few links across these specialisations.\n",
    "A survey of the field of phonetics education was done by the Phonetics Working Group of the Thematic Network in Human Communication Sciences. The primary aim of this working group was to collect information about the contents of current phonetics curricula across different areas of specialisation. The information was collected from responses to two questionnaires in 1997 and 1998 from 98 and 40 European institutes, respectively. This information was used as a point of departure in the formulation of guidelines regarding elements of phonetics knowledge which are relevant to students of phonetics from different specialisations. A further aim of the Phonetics working group was to determine the needs for further development of teaching material and best practice. Our presentation will highlight both the methodology and the outcomes of the two surveys and, especially, discuss aspects concerning curriculum development.\n",
    ""
   ]
  },
  "bryndal99_matisse": {
   "authors": [
    [
     "Malgorzata",
     "Bryndal"
    ]
   ],
   "title": "Phonetics teaching objectives in Polish universities",
   "original": "mati_105",
   "page_count": 4,
   "order": 12,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "The purpose of this paper is to outline the objectives of teaching phonetics in Polish universities and to relate them to the proposals advocated in \"The Landscape of Future Education in Speech Communication Sciences. Book 2 - Proposals\". Although the data presented in this paper have been collected from one university, the Adam Mickiewicz University in Poznañ, and thus the paper may be interpreted as a case study, it can be ascertained that the trends of phonetics teaching recognised in the examined university reflect the tendencies present in all Polish higher education institutions.\n",
    "The first section of the paper is dedicated to the issue of the importance of incorporating phonetics in curricula of certain academic courses, as well as to time, staff and facilities limitations arising whilst designing the curricula. Further, the structures of phonetics syllabuses are analysed and the main phonetics teaching objectives are established for respective courses. The contents of the syllabuses are compared with the study elements of phonetics curriculum proposed for different specialisations by the Socrates TN Phonetics working group. Subsequently, the paper provides an insight into the teaching resources and facilities available, used in teaching phonetics in Polish universities.\n",
    ""
   ]
  },
  "giurgiu99_matisse": {
   "authors": [
    [
     "Mircea",
     "Giurgiu"
    ]
   ],
   "title": "Teaching digital speech processing for telecommunications",
   "original": "mati_109",
   "page_count": 4,
   "order": 13,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "The communication focuses on the educational environment for teaching \"Digital Speech Processing\" at undergraduate, graduate and PhD levels in the Department of Telecommunications from Technical University of Cluj-Napoca, Romania. Emphasis will be done to reveal particular successes and drawbacks in order to set up and to develop a modern curriculum during the last four years. This educational environment has been set up to cover special issues on speech processing, but from an engineering applicative point of view: that of telecommunications. The teaching infrastructure is tailored to the local conditions, but it can be adopted as an overall model for other universities. The conclusions will refer to an critic analysis of education in the above mentioned field in our department, possibilities to improve teaching resources and teaching methods, but also proposals to move from \"classical\" education to an open and continuous one based on electronic information interchange.\n",
    ""
   ]
  },
  "mctear99_matisse": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ]
   ],
   "title": "Using the CSLU toolkit for practicals in spoken dialogue technology",
   "original": "mati_113",
   "page_count": 4,
   "order": 14,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "Spoken language interaction with computers has become a practical possibility as a result of recent technological developments in the speech sciences. The development of a spoken dialogue system is a complex process involving the integration of the various components of spoken language technology, including speech recognition, natural language processing, dialogue modelling, and speech synthesis. Various toolkits and authoring environments have been produced that provide assistance with this process. This paper reports on the use of CSLUs RAD (Rapid Application Developer) to provide practical experience for undergraduate students in the specification and development of spoken dialogue systems. Students of linguistics and speech and language therapy were able to develop simple state-based dialogues as well as sub-dialogues and repair dialogues as well as creating their own pronunciation models of the words to be recognised by simply altering the automatically generated Worldbet labels or by using the visualisation tools provided with the toolkit. Students with a computing background were able to develop more complex interactions with external data as well as more complex dialogue models. The paper reports on recent additions to the toolkit that enable robust parsing of the input as well as more flexible dialogue management.\n",
    ""
   ]
  },
  "kacic99_matisse": {
   "authors": [
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Laboratory course on speech processing using KHOROS development environment",
   "original": "mati_117",
   "page_count": 4,
   "order": 15,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "A laboratory course on speech processing that covers fundamentals of speech preprocessing, feature extraction, classification, automatic speech and speaker recognition was developed. The course is given in the fifth year of undergraduate study of electronics. It was designed using the KHOROS software integration and development environment, where a new speech processing toolbox was developed. The aim of the course is to give students hands-on experience in selected topics of speech processing. It provides an interactive environment that allows student to observe the underlying processing step by step and to display results in frequency or time domain at each stage. Each topic is divided into sessions and for each session different networks of glyphs (defined in Cantata) were developed, where the network performs a selected speech processing task (e.g., f0 analysis, feature extraction, speech recognition, ...). A glyph is a visual presentation of a program that performs one of the speech processing algorithms (e.g., preemphasis, filtering, DTW,...). The parameters of an algorithm can be controlled by opening the parameter list of the glyph, thus enabling student easy control of the whole process. The students can process prerecorded speech or record their own voice. The networks of glyphs can also be easily changed or new one designed what gives students a lot of possibilities for experimenting.\n",
    "The software currently runs on Hewlett Packard workstations under the HP-UX 10.20 operating system, but it can be easily ported to other platforms supported by the KHOROS as long as there is support for audio devices.\n",
    ""
   ]
  },
  "nouza99_matisse": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Teaching and Learning through Visualised Speech Processing Experiments",
   "original": "mati_121",
   "page_count": 4,
   "order": 16,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "At Eurospeech97 we presented system VISPER that had been developed as a teaching and learning tool for introductory courses in speech science and technology. The system is aimed at explaining basic speech recognition paradigms and procedures like, for example, signal processing, feature extraction, word distance measurement, hidden Markov modelling. Learning and understanding these topics is supported by VISPERs graphic environment that allows for visualisation and animation of the essential procedures. A large choice of options and settings makes the system an educational workbench enabling students to learn from experiments.\n",
    "In the paper we describe our experience with using the VISPER in university (MSc and PhD) courses in speech processing. We show that the scope of the experiments provided by the system is quite large, starting from a simple DTW based speaker-dependent word recognition task and ending with, e.g., a real-time continuous HMM speaker-independent classifier. After several introductory lectures and seminars the students may start their individual investigations. Is one feature (e.g. energy) and one template good enough for a 5-word recogniser? How does a DTW path look like for a pair of same words and different words? How is a difference between single-mixture and multi-mixture continuous HMMs? These are typical questions that the students ask. In our courses they try to find the answers by themselves. Simply by starting the VISPER, entering their own vocabularies, recording their own data, selecting arbitrary feature subsets, choosing algorithms and running their own visualised and animated experiments.\n",
    ""
   ]
  },
  "garzone99_matisse": {
   "authors": [
    [
     "Giuliana",
     "Garzone"
    ],
    [
     "Francesca",
     "Santulli"
    ]
   ],
   "title": "Using hypertexts/hypermedia in the teaching of phonetics and phonology",
   "original": "mati_125",
   "page_count": 4,
   "order": 17,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "The paper aims at introducing some options for the use of hypertexts/hypermedia in the teaching of phonetics and phonology. The authors illustrate and comment upon their experience in designing and authoring software applications for the teaching of phonetics and phonology to suit the needs of different groups of learners. In particular, they will present a hypermedia programme designed for Italian students of General Linguistics in faculties where foreign languages are the main specialisation area (Faculty of Modern Languages; Faculty of Interpreting and Translation). The programme deals with general phonetic theory, mainly on an articulatory basis, and presents a description of the sound systems of four European languages (English, French, German, Italian) based on the recognised pronunciation standard for each of them, the main language of reference being Italian, the mother tongue for most of our end users. At the same time, for each point the programme offers auditory reference and exemplification; in this way, the acquisition of theoretical notions is brought into relation to the students experience in language learning, thus also contributing to the improvement of their performance in foreign language pronunciation, an essential feature in a programme designed for undergraduates majoring in Modern Languages. A similar application specifically aimed at the teaching of English phonetics is under development, being still at an experimental stage.\n",
    ""
   ]
  },
  "delmonte99_matisse": {
   "authors": [
    [
     "Rodolfo",
     "Delmonte"
    ]
   ],
   "title": "A prosodic Module for self-learning activities",
   "original": "mati_129",
   "page_count": 4,
   "order": 18,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "We created an application specialized in prosodic tutoring, called the Prosodic Module(PM). The PM is composed of two different sets of Learning Activities, the first one dealing with prosodic problems at word syllabic level, the second one dealing with prosodic problems at phonological phrase and utterance level. The PM is able to detect significant deviations from a masters word/ phrase/ utterance and offers visual aids and a written diagnosis of the problem as well as indications on how to overcome and correct the error. This is achieved by means of a comparison between the two signals, the master and the student one; elements of comparison are constituted by the acoustic correlates of well-known prosodic elements such as intonational contour, sentence accent and word stress, duration at syllable, word and sentence level.\n",
    "We argue that the use of Automatic Speech Recognition as Teaching Aid should be targeted to narrowly focussed spoken exercises, for intermediate or higher level students, disallowing open-ended dialogues, in order to ensure consistency of evaluation. In addition, we support the conjoined use of ASR technology and prosodic tools to gauge Goodness of Pronunciation for linguistically consistent feedback.\n",
    "We are currently engaged in the creation of a syllable database for the detection of varieties of prosodic knowledge in Italian students learning English, structured according to different levels of interlingua. The database is used in the automatic segmentation and in the assessment phase. In particular, deviations from mean acoustic values at syllable level are estimated both on the database and on masters measurements\n",
    ""
   ]
  },
  "fiandino99_matisse": {
   "authors": [
    [
     "Christine",
     "Fiandino"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "Annie",
     "Rouxeville"
    ]
   ],
   "title": "Criteria for designing interactive exercisesto learn French intonation",
   "original": "mati_133",
   "page_count": 4,
   "order": 19,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Intonation is an important aspect of spoken language which often does not receive the attention it deserves. We present results of a survey on the teaching of French intonation by French teachers in several English Universities. We compare the conventional way of teaching intonation with the potential of methods based on computer-aided learning.\n",
    "We link intonation, interaction and interactivity : intonation is often assessed without being taught. When it is taught, interaction with the teacher is favoured, but this is more acceptable to extrovert learners. With interactivity, the use of a computer benefits the more reserved students, who otherwise tend to underperform.\n",
    "After a critical analysis of existing IT methods for learning French intonation, we describe the steps to set up a new IT method using the world-wide web language HTML together with the choice of criteria for setting up learning units.\n",
    ""
   ]
  },
  "larson99_matisse": {
   "authors": [
    [
     "Håkan",
     "Larson"
    ]
   ],
   "title": "Experiences of large scale implementation of speech analyzing tools in learning Swedish assecond language",
   "original": "mati_137",
   "page_count": 4,
   "order": 20,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "Speech analysis tools have been used on a large scale in adults and children learning Swedish as second language. Several thousands of students have had the opportunity to compare their own pitch curves with those of native speakers. The results vary between students but are in general quite positive. The influence of education, first language, age and other personal factors influence the way the tools are being used and the results. Experiences indicate that by using his intuition and the combination of visual and audio feedback even poorly educated learners can benefit from these tools.\n",
    "Different ways to combine the computer work with other training methods will be described. The actual tools will be demonstrated.\n",
    ""
   ]
  },
  "sjolander99_matisse": {
   "authors": [
    [
     "Kåre",
     "Sjölander"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "Web-based educational tools for speech technology",
   "original": "mati_141",
   "page_count": 4,
   "order": 21,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "The speech group at KTH has developed a number of speech technology tools for use in education of undergraduate students or researchers in the speech field. Many of these tools have been limited to a certain computer environment and the need for teacher guidance. During the last year we have started development work on a toolkit for spoken language technology that can be used over the Internet. The aim is to free the students from the need of using a particular computer at a particular time and place.\n",
    "We have created a speech technology toolkit that serves as a basis in the creation spoken language systems. This toolkit is partly based on the software technology in our existing spoken dialogue system Waxholm. We have used the speech toolkit to build three web-based educational systems.\n",
    "We have developed an interactive tool for working with parametric speech synthesis. The tool facilitates editing of parameter tracks, and it provides real-time feedback of the synthesised speech. It serves as an interface to KTHs multilingual rule based synthesis system, and can be used to control a formant synthesiser as well as a 3-D \"talking head\". The tool has been used in research and education during the past two years.\n",
    "In our courses on speech technology we have an introductory section on basic phonetics and speech analysis. For this section we have developed a set of exercises in which students analyze their own speech in various ways. These exercises are accessed through web pages, in which simple speech analysis tools have been embedded as small applications (applets) dedicated to the task at hand.\n",
    "We have created an integrated lab environment for dialog systems that has been used in the courses on spoken language technology given at Masters level at three Universities in Sweden. In this environment, students are presented with a simple spoken dialogue application for searching in the web-based Yellow pages on selected topics using speech, presently in the Swedish language. The system is initialized with knowledge about streets, restaurants, hotels, museums and similar services. The aim of this work has been to put a fully functioning spoken dialogue system into the hands of the students as an instructional aid. They can test it themselves and are able to examine the system in detail. They are shown how to extend and develop the functionality. In this way, we hope to increase their understanding of the problems and issues involved and to spur their interest for this technology and its possibilities. The complete dialog system with speech recognition and synthesis as well as an interactive map can be used inside a web-browser using our plug-ins for audio and speech recognition.\n",
    ""
   ]
  },
  "bowerman99_matisse": {
   "authors": [
    [
     "Chris",
     "Bowerman"
    ],
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Mike",
     "Rosner"
    ],
    [
     "Mark",
     "Tatham"
    ],
    [
     "Maria",
     "Wolters"
    ]
   ],
   "title": "Tutorial design for web-based teaching and learning",
   "original": "mati_145",
   "page_count": 4,
   "order": 22,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "The Computer Aided Learning working group of the SOCRATES thematic network in Speech Communication Science have investigated how the Internet is being used and could be used for the provision of self-study materials. In this paper we report on our findings and make recommendations that should be useful to any current or potential author of tutorial materials designed for use on the Web.\n",
    "Our survey of existing tutorial components [Huckvale et al, 1997] highlighted the fact that much educational material on the Web was poorly designed from the students point of view. We found material that was no more than a text-book presentation of ideas, material that did not encourage the student to discover new concepts, material that did not challenge the understanding of the student, and material that did not make the best use of the medium.\n",
    "Our recommendations to authors of tutorials include: choose topics of appropriate size, design tutorials for re-use, eliminate pre-requisites where possible, take care in the use of language, design for open-learning, use interactive means of assessment, and make appropriate use of the medium. We provide a checklist which authors can use to assess the likely effectiveness of their tutorials and which can help improve the quality and utility of the final result. We give some Internet links to tutorials, which can serve as models of good design and good use of the medium.\n",
    "Reference: Huckvale,M., et al, \"Opportunities for Computer Aided Instruction in Phonetics and Speech Communication Provided by the Internet\", Proc. Eurospeech 1997, Rhodes, Greece.\n",
    ""
   ]
  },
  "uhlir99_matisse": {
   "authors": [
    [
     "Jan",
     "Uhlir"
    ]
   ],
   "title": "The set of exercises in digital speech processing",
   "original": "mati_053",
   "page_count": 4,
   "order": 23,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "Engineers who study electronics and communications technology are frequently confronted with algorithms and techniques concerning speech signals. We have prepared the study course, which covers fundamentals of phonetics and speech processing. In the first lectures, students learn the basics of speech communication. Auditory and articulation models are described. The next theme is devoted to the speech processing techniques. Typical parameters of the signal are introduced. The principles of text to speech synthesis are presented in the third part of course. Finally, automatic speech recognition and its algorithms are studied in the last part of course.\n",
    "The experimental works, which follow the lectures, will be presented in the contribution. Experiments are made in Matlab. We have worked out the set of M-files, which guides the student through the main topics of lectures. All M-files are prepared as simple and open programs. Students can change constants and modify the calculations. The results are visualised and in proper cases demonstrated by audio output.\n",
    "Until now, the set of M-files covers following topics:\n",
    "Quantization, quantization noise, nonlinear quantizers, DPCM, sigma-delta quantizer\n",
    "Predictive quantizer, short term parameters (LPC)\n",
    "Short term spectra, FFT, LPC, cepstra\n",
    "Formant structure of phones, voiced vs. unvoiced, pitch period\n",
    "Spectral distance measure, DTW algorithms\n",
    "Selected examples will be presented in the contribution.\n",
    ""
   ]
  },
  "fellbaum99_matisse": {
   "authors": [
    [
     "Klaus",
     "Fellbaum"
    ],
    [
     "Jörg",
     "Richter"
    ]
   ],
   "title": "A tutorial on human speech production using interactive modules",
   "original": "mati_057",
   "page_count": 4,
   "order": 24,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The paper presents a tutorial, based on HTML and Java applets, which describes the human speech production.\n",
    "The excitation and articulation, are realised (simulated) by a technical system, known as Linear Predictive Vocoder (LPC vocoder). Its principle and analogy to the human speech organs is explained. The main emphasis is put on the control parameters of the system - above all the pitch frequency and the vocal tract parameters (prediction coefficients) and the effect when they are manipulated. These manipulations are visually presented as changes of the time signal, pitch sequence and the spectrum and they are also audible.\n",
    "As a speciality, the student can record his or her own voice and can manipulate the above mentioned characteristics.\n",
    "There is no need for a special equipment, an Internet-connected PC completed with a standard microphone and a Soundblaster board is sufficient.\n",
    "The core components of the tutorial were firstly introduced during a conference in Bergen (September 1998), but the tutorial, which will be presented and demonstrated here, is a completely revised and extended version.\n",
    ""
   ]
  },
  "hoffmann99_matisse": {
   "authors": [
    [
     "Rüdiger",
     "Hoffmann"
    ],
    [
     "Ulrich",
     "Kordon"
    ],
    [
     "Steffen",
     "Kürbis"
    ],
    [
     "Bettina",
     "Ketzmerick"
    ],
    [
     "Klaus",
     "Fellbaum"
    ]
   ],
   "title": "An interactive course on speech synthesis",
   "original": "mati_061",
   "page_count": 4,
   "order": 25,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "We are presenting an interactive course on speech synthesis which is designed to support the education in speech communication. In the basic section, the fundamental principles of speech synthesis are explained. From this section, the user may select different branches which are explaining special topics in more detail. Two special sections are worked out until now: The first one is devoted to the crucial problem of\n",
    "correct segmentation of the speech elements used for the concatenative synthesis. The user may select his own diphone segments from a given speech data base. The quality of the segments may be evaluated acoustically, and hints are given to avoid errors in cutting. Thus, the user will learn how to select the segments with good quality. The second special section offers a complete TTS system to the user for experimental purposes. The user may type any text, and he may observe how the system processes the text from the first linguistic preprocessing until the acoustic synthesis. The course is written in HTML and is designed for Internet application.\n",
    ""
   ]
  },
  "bonneau99_matisse": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Jacqueline",
     "Vaissière"
    ]
   ],
   "title": "Hypertext atlas of speech sounds",
   "original": "mati_065",
   "page_count": 4,
   "order": 26,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "We present a project concerning a method devoted to the learning of the phonetics of a given language. This system is written in hypertext and uses the function of a speech signal editor (Snorri). The learning concerns the pronunciation of vowels and of consonants in a reduced context, of syllables and words (study of coarticulatory phenomena), of prosody as well as an initiation to spontaneous speech.\n",
    "Our method relies upon the comparison between the users pronunciation and the standard one (the \"target\"). To this purpose, the user has at his disposal: the play-back of his utterances and that of the target, their spectrographic representations with formant trackings, the projection of their formant values in the vocalic space, for consonants, the values of the main acoustic cues.\n",
    "Snorri also displays the variation of the fundamental frequency (intonation) along the time. The energy curve and the durations of any part of the sentence is also available.\n",
    "We will constitute a database containing coarticulatory and spontaneous speech phenomena. The user will be allowed to visualise and play back these phenomenons at will.\n",
    "A book, written in hypertext, will explain to the user the main phonetic notions and the learning method.\n",
    ""
   ]
  },
  "kopecek99_matisse": {
   "authors": [
    [
     "Ivan",
     "Kopecek"
    ]
   ],
   "title": "Speech sciences and visually impaired students",
   "original": "mati_069",
   "page_count": 4,
   "order": 27,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "In the paper we discuss some problems of teaching visually impaired students speech science in the context of information technologies based on speech synthesis and recognition as well as on some methods of artificial intelligence, especially in relation with the software developed at the Faculty of Informatics, Masaryk University Brno. We will briefly describe the speech oriented hypertext system AUDIS developed for blind users and the programming system DIALOG which uses speech interface and is oriented to analysing and programming natural language problems and algorithms (speech, text, dialogue). Both systems are used for computer-aided teaching visually impaired students at our faculty and they are also developed in co-operation with them.\n",
    ""
   ]
  },
  "carmell99_matisse": {
   "authors": [
    [
     "Tim",
     "Carmell"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "An interactive course in spectrogram reading",
   "original": "mati_073",
   "page_count": 4,
   "order": 28,
   "p1": "73",
   "pn": "86",
   "abstract": [
    "This paper describes a computer-based spectrogram reading course in which the principal teaching tool is SpeechView, a component of the CSLU Toolkit.\n",
    "Already existing components of the Toolkit bring new capabilities to the course. A student can record an utterance, then type in the words uttered. A phonetic transcription is automatically time-aligned to the spectrogram and other user-specified displays. The speech corresponding to any labeled segmentphone or wordcan be listened to. The segments can also be presented visually using an animated face that can be made transparent to view an accurate tongue in relation to the palate, teeth and gums.\n",
    "In addition, the Toolkit authoring tools provide great flexibility in designing interactive exercises for learning about acoustic and articulatory phonetics. For example, course notes and questions are connected to the spectrogram displays and allow the students to visualize specific areas of interest in the spectrogram in conjunction with other modes of instruction.\n",
    "Using these interactive teaching tools, students progress from phonemes, through common syllable forms, to single words and finally to simple phrases. It is our experience that the use of the CSLU Toolkit has greatly enhanced the teaching of spectrogram reading.\n",
    ""
   ]
  },
  "abberton99_matisse": {
   "authors": [
    [
     "Evelyn",
     "Abberton"
    ],
    [
     "Adrian",
     "Fourcin"
    ],
    [
     "Xinghui",
     "Hu"
    ],
    [
     "Colin",
     "Bootle"
    ],
    [
     "David",
     "Miller"
    ]
   ],
   "title": "Speech pattern element analysis and display - 'Lx Speech Studio'",
   "original": "mati_077",
   "page_count": 4,
   "order": 29,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Both teaching and research in Speech Sciences differ radically from comparable work in the physical sciences because of the need to link different levels of representation - and analysis. Spectrographic and waveform presentations are useful first tools but the vital phonetic information which they encapsulate is not readily accessible. The present work is aimed at meeting three speech science teaching objectives:\n",
    "to show how quite simple phonetically relevant features can be derived in real-time from combinations of acoustic and physiological data provide a basic PC workstation giving a platform for the analysis, display and measurement of single and combined speech pattern element sets which are directly related to: pitch; loudness; regularity; frication; and timbre discuss particular examples (taken from English, French, Chinese, Sekgalagadi and speech pathology in the demonstrations).\n",
    "Special attention is given to establishing links between auditory, articulatory and physical levels of description and to provide for normalisation in display and quantification.\n",
    ""
   ]
  },
  "howard99_matisse": {
   "authors": [
    [
     "Sara",
     "Howard"
    ],
    [
     "Mick",
     "Perkins"
    ],
    [
     "Paul",
     "Martland"
    ]
   ],
   "title": "An integrated multi-media package for teaching clinical phonetics andlinguistics",
   "original": "mati_081",
   "page_count": 3,
   "order": 30,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "VISUAL-CLP (VIdeo SUpported Active Learning in Clinical Linguistics and Phonetics) is a student-centred, interactive, multi-media tutorial in clinical linguistics and phonetics. The tutorial is based on digitised video- and audiotaped speech data of a child with a complex communication disorder. Students independently analyse the data from different disciplinary perspectives, and at different levels of complexity, resulting in their own integration of information. The final version will include modules on phonetics, phonology, grammar, semantics and pragmatics; assessment and remediation; case history; psychometric and linguistic assessment results; theoretical background; and an annotated bibliography. All modules are interlinked enabling students to create their own profile of the client using whatever information they feel is most important. The complete tutorial package is provided on CD ROM. It enables students to work individually or in small groups, at their own pace, to develop and integrate a range of skills in clinical linguistic and phonetic transcription and analysis, using data rich enough to support increasing levels of complexity. The tutorial is designed to break down the tasks of analysis into the appropriate levels of detail and to prompt the student - should they so wish - to attempt the appropriate type of analysis, which may then be compared with the tutor's model.\n",
    ""
   ]
  },
  "cutugno99_matisse": {
   "authors": [
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Cécile",
     "Fougeron"
    ]
   ],
   "title": "A tutorial on models of speech perception",
   "original": "mati_085",
   "page_count": 4,
   "order": 31,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Learning material on speech perception has appeared in a recent survey to be less documented than other topic in phonetics. This tutorial was built in order to fill, at least in part, this lack. We aim to provide a comprehensive overview of some models of speech perception process to both basic and advanced students. The topics covered in the tutorial include a broad description of the mechanisms involved in the speech perception process; difficulties that listeners have to overcome and that models have to account for; a comparative description of 5 of the most famous models: Fant's auditory model, Motor Theory, Analysis by Synthesis, Quantal Nature of Speech, Hyper- and hypo-speech models. The description and comparison of the models are based on their main assumptions, historical background, and supporting experiments and results. This computer based tutorial rely on an interactive learning strategy based on multimedia presentations, hypertext, pop-ups, and animated diagrams. This work is freely available on World Wide Web, at the Elsnet site (http://www.elsnet.org).\n",
    ""
   ]
  },
  "laprie99_matisse": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Snorri, a software for speech sciences",
   "original": "mati_089",
   "page_count": 4,
   "order": 32,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "Using tools for investigating speech signals is an invaluable help to teach phonetics and more generally speech sciences. For several years we have undertaken the development of the software Snorri which is for both speech scientists as a research tool and lecturers in phonetics as an illustration tool.\n",
    "Unlike other softwares dedicated to signal processing, Snorri is intended specifically to speech. It consists of five types of tools:\n",
    "to edit speech signals (e.g. to create stimuli for perception experiments). The spectrogram is recomputed after each command (cut, paste, filtering, damping) so that the acoustic consequences can be easily evaluated. to annotate phonetically or orthographically speech signals. The set of phonetic symbols is fully parametrised so that it is possible to annotate any language. Snorri offers tools to explore annotated corpora automatically. to analyse speech with a large set of spectral analyses and monitor spectral peaks along time. to study prosody. Besides pitch calculation it is possible to synthesise new signals by modifying the F0 curve and/or the speech rate. to generate parameters for Klatts synthesiser (e.g. to accurately control which are the acoustic cues introduced in a speech signal). A user friendly graphic interface and copy synthesis tools allows the user to generate files for Klatts synthesiser easily.\n",
    "In the context of speech sciences Snorri can therefore be exploited for many purposes, among them, investigating acoustic cues of speech sounds and prosody in order to evaluate their perceptive importance.\n",
    ""
   ]
  },
  "draxler99_matisse": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "WWWSigTranscribe - Annotation via the WWW",
   "original": "mati_093",
   "page_count": 4,
   "order": 33,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "WWWSigTranscribe is a tool for annotating single and multichannel speech signals via the WWW. It was originally developed for the SpeechDat(II) project for an orthographic annotation of telephone speech, and has been extended to handle standard (WAV, AIFF, au) as well as generic binary audio file formats.\n",
    "The major motivation for providing annotation via the WWW is\n",
    "platform independence and world wide access to speech data, and central storage of annotations and descriptive data.\n",
    "This means that e.g. students can access their labs data from home. Annotations, lexicons, and other descriptive data are held on the speech server; updates e.g. of the lexicon are thus immediately available to the annotators.\n",
    "WWWSigTranscribed is implemented in Java in a clean object-oriented design. It features a signal display window and an editing pane with editing buttons. These editing buttons speed up the annotation process by performing common conversions. Syntactical correctness of the annotation is checked, and a dictionary lookup is performed prior to saving the annotation to a DBMS.\n",
    "Due to its object-oriented design, WWWSigTranscribe can easily be adapted to new file formats or languages. Because it is implemented in pure Java it supports Unicode - i.e. IPA symbols or ideographic languages, and can make use of the standard interface to relational DBMSs.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "berkovitz99_matisse",
    "cole99_matisse",
    "roach99_matisse"
   ]
  },
  {
   "title": "Oral Sessions",
   "papers": [
    "carsonberndsen99_matisse",
    "wrigley99_matisse",
    "drygajlo99_matisse",
    "beck99_matisse",
    "korkko99_matisse",
    "lybergahlander99_matisse",
    "wigforss99_matisse",
    "hazan99_matisse",
    "bryndal99_matisse",
    "giurgiu99_matisse",
    "mctear99_matisse",
    "kacic99_matisse",
    "nouza99_matisse",
    "garzone99_matisse",
    "delmonte99_matisse",
    "fiandino99_matisse",
    "larson99_matisse",
    "sjolander99_matisse",
    "bowerman99_matisse"
   ]
  },
  {
   "title": "Demonstration/Poster Sessions",
   "papers": [
    "uhlir99_matisse",
    "fellbaum99_matisse",
    "hoffmann99_matisse",
    "bonneau99_matisse",
    "kopecek99_matisse",
    "carmell99_matisse",
    "abberton99_matisse",
    "howard99_matisse",
    "cutugno99_matisse",
    "laprie99_matisse",
    "draxler99_matisse"
   ]
  }
 ]
}