{
 "title": "Workshop on Speech, Music and Mind (SMM 2019)",
 "location": "Vienna, Austria",
 "startDate": "14/9/2019",
 "endDate": "14/9/2019",
 "URL": "http://smm19.ifs.tuwien.ac.at/",
 "chair": "Chairs: Venkata S Viraraghavan, Alexander Schindler, João P Cabral, Gauri Deshpande and Sachin Patel",
 "conf": "SMM",
 "year": "2019",
 "name": "smm_2019",
 "series": "SMM",
 "SIG": "",
 "title1": "Workshop on Speech, Music and Mind",
 "title2": "(SMM 2019)",
 "date": "14 September 2019",
 "booklet": "smm_2019.pdf",
 "papers": {
  "narayanan19_smm": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Understanding affective expressions and experiences through behavioral machine intelligence",
   "original": "abs1",
   "page_count": 0,
   "order": 1,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿Behavioral signals in the audio and visual modalities available in speech, spoken language and body language offer a window into decoding not just what one is doing but how one is thinking and feeling. At the simplest level, this could entail determining who is talking to whom about what and how using automated audio and video analysis of verbal and nonverbal behavior. Computational modeling can also target more complex, higher level constructs, like the expression and processing of emotions. Behavioral signals combined with physiological signals such as heart rate, respiration and skin conductance offer further possibilities for understanding the dynamic cognitive and affective states in context. Machine intelligence could also help detect, analyze and model deviation from what is deemed typical. This talk will focus on multimodal bio-behavioral sensing, signal processing and machine learning approaches to computationally understand aspects of human affective expressions and experiences. It will draw upon specific case studies to illustrate the multimodal nature of the problem in the context of both vocal encoding of emotions in speech and song, as well as processing of these cues by humans."
   ]
  },
  "burgoyne19_smm": {
   "authors": [
    [
     "John Ashley",
     "Burgoyne"
    ]
   ],
   "title": "Everyday Features for Everyday Listening",
   "original": "abs2",
   "page_count": 0,
   "order": 10,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿You are sitting on a commuter train. How many passengers are wearing headphones? What are they listening to? What else are they doing? Most importantly, amid the cornucopia of distractions, what exactly are they hearing?\n",
    "Much research in music cognition pits ‘musicians’, variously defined, against non-musicians. Recently, especially since the appearance of reliable measurement instruments for musicality in the general population (e.g., Müllensiefen et al., 2014), there has been growing interest in the space in between. Moreover, the ubiquity of smartphones has greatly enhanced the ability of techniques like gamification or Sloboda’s ‘experience sampling’ to reach this general population outside of a psychology lab.\n",
    "Music information retrieval (MIR) – and signal processing research more generally – can provide the last ingredients to understand what is happening between our commuters’ earbuds: everyday features for studying everyday listening. Since Aucouturier and Bigand’s 2012 manifesto on the poor interpretability of traditional DSP measures, clever dimensionality reduction paired with feature sets like those from the FANTASTIC (Müllensiefen and Frieler, 2006) or CATCHY (Van Balen et al., 2015) toolboxes have sought a middle ground.\n",
    "This talk will present several uses of everyday features from the CATCHY toolbox for studying everyday listening, most notably a discussion of the Hooked on Music series of experiments (Burgoyne et al., 2013) and a recent user study of thumbnailing at a national music service. In conclusion, it will outline some areas where MIR expertise can go further than just recommendation to learn about and engage with listeners during their daily musical activities.\n"
   ]
  },
  "ishi19_smm": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Takayuki",
     "Kanda"
    ]
   ],
   "title": "Prosodic and voice quality analyses of loud speech: differences of hot anger and far-directed speech",
   "original": "1",
   "page_count": 5,
   "order": 2,
   "p1": 1,
   "pn": 5,
   "abstract": [
    "﻿In this study, we analyzed the differences in acoustic-prosodic and voice quality features of loud speech in two situations: hot anger (aggressive/frenzy speech) and far-directed speech (i.e., speech addressed to a person in a far distance). Analysis results indicated that both types are accompanied by louder power and higher pitch, while differences were observed in the intonation: far-directed voices tend to have large power and high pitch over the whole utterance, while angry speech has more pitch movements in a larger pitch range. Regarding voice quality, both types tend to be tenser (higher vocal effort), but angry speech tends to be more pressed, with local appearance of harsh voices (with irregularities in the vocal fold vibrations)."
   ],
   "doi": "10.21437/SMM.2019-1"
  },
  "gauder19_smm": {
   "authors": [
    [
     "Lara",
     "Gauder"
    ],
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Pablo",
     "Riera"
    ],
    [
     "Silvina",
     "Brussino"
    ]
   ],
   "title": "A protocol for collecting speech  data with varying degrees of trust",
   "original": "2",
   "page_count": 5,
   "order": 3,
   "p1": 6,
   "pn": 10,
   "abstract": [
    "﻿This paper describes a novel experimental setup for collecting speech data from subjects induced to have different degrees of trust in the skills of a conversational agent. The protocol consists of an interactive session where the subject is asked to respond to a series of factual questions with the help of a virtual assistant. In order to induce subjects to either trust or distrust the agent’s skills, they are first informed that the agent was previously rated by other users as being either good or bad; subsequently, the agent answers the subjects’ questions consistently to its alleged abilities. These interactions will be speech-based, with subjects and agents communicating verbally, which will allow for the recording of speech produced under different trust conditions. Ultimately, the resulting dataset will be used\nto study the feasibility of automatically predicting the degree of trust from speech. This paper describes a preliminary experiment using a text-only version of the protocol in Argentine Spanish. The results show that the protocol effectively succeeds in influencing subjects into the desired mental state of either trusting or distrusting the agent’s skills. We are currently beginning the collection of the speech dataset, which will be made publicly available once ready."
   ],
   "doi": "10.21437/SMM.2019-2"
  },
  "riera19_smm": {
   "authors": [
    [
     "Pablo",
     "Riera"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Lara",
     "Gauder"
    ]
   ],
   "title": "No Sample Left Behind: Towards a Comprehensive Evaluation of Speech Emotion Recognition Systems",
   "original": "3",
   "page_count": 5,
   "order": 4,
   "p1": 11,
   "pn": 15,
   "abstract": [
    "﻿Human agreement for the task of labeling speech utterances with emotion information is usually low, especially for natural speech, where emotions could be ambiguous or subtle. For this reason, datasets of emotional speech are generally labeled\nby several human annotators. The common practice in speech emotion recognition (SER) literature is to summarize the multiple labels provided by the annotators for a sample into a single one by choosing the majority label. The problem with this approach is that a significant proportion of samples may not be assigned a majority label. These samples are usually ignored for system evaluation, along with any samples initially labeled by the annotators as being from emotions other than the emotions of interest for the specific dataset. This implies that the estimation of emotion recognition performance is incomplete. We do not know how the system will behave when presented with those ambiguous samples, which will certainly appear in practice. In this paper, we analyze the effects that these samples have in system performance and propose different ways to use the multiple labels available from the annotators during evaluation and to assess system performance without discarding any samples."
   ],
   "doi": "10.21437/SMM.2019-3"
  },
  "v19_smm": {
   "authors": [
    [
     "Vishnu Vidyadhara Raju",
     "V"
    ],
    [
     "Krishna",
     "Gurugubelli"
    ],
    [
     "Mirishkar Sai",
     "Ganesh"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Towards Feature-space Emotional Speech Adaptation for TDNN based Telugu ASR systems",
   "original": "4",
   "page_count": 5,
   "order": 5,
   "p1": 16,
   "pn": 20,
   "abstract": [
    "﻿The unavailability of speech corpora is one of the critical barriers for building a large vocabulary naturalistic Telugu automatic speech recognition (ASR) system. Hence, an effort is put towards the collection of both neutral and emotional\nspeech samples created as Telugu naturalistic emotional speech corpus(IIIT-H TNESC). In this work, we investigate the feature-space adaptation approach to compensate the acoustic mismatch between neutral and emotional speech by using auxiliary features. The features derived from the maximum likelihood linear regression (fMLLR) of GMM models are used to perform the feature-space adaptation. The effectiveness of this adaptation is studied on deep neural network (DNN), time-delay neural network (TDNN) and combined TDNN with Long short-term memory (TDNN-LSTM) based acoustic models. Experimental\nresults show that the feature-space adaptation approach has improved the performance of baseline by an average word error rate of 15.8%"
   ],
   "doi": "10.21437/SMM.2019-4"
  },
  "gupta19_smm": {
   "authors": [
    [
     "Kaajal",
     "Gupta"
    ],
    [
     "Anzar",
     "Zulfiqar"
    ],
    [
     "Pushpa",
     "Ramu"
    ],
    [
     "Tilak",
     "Purohit"
    ],
    [
     "V.",
     "Ramasubramanian"
    ]
   ],
   "title": "Detection of emotional states of OCD patients in an exposure-response prevention therapy scenario",
   "original": "5",
   "page_count": 5,
   "order": 6,
   "p1": 21,
   "pn": 25,
   "abstract": [
    "﻿We address the problem of detection of emotional states of\nobsessive-compulsive disorder (OCD) patients in an exposureresponse\nprevention (ERP) therapy protocol scenario. Here, it is required to identify the emotional levels of a patient at a granular level needed for successful progression of the therapy, and one of the major hurdles in this is the so called alexithymia (subclinical inability to identify emotions in the self). Alternately,\nwe propose estimating the emotional state of an OCD patient automatically from raw speech signal, elicited under a\nsituation-based emotion entry to an on-line therapy aid. Towards this, we propose a novel multi-temporal CNN architecture\nfor end-to-end ‘speech emotion recognition’ (SER) from raw speech signal. The proposed architecture allows for multiple\ntime-frequency resolutions with multiple filter banks having different time-frequency resolutions to create feature-maps\n(ranging from very narrow-band to very wide-band spectrographic maps in steps of fine time-frequency resolutions). On\nSER task, we show 2-8% absolute enhancement in accuracy for the multi-temporal cases (e.g. 3, 6 branches) over the conventional single-temporal CNNs. As a position paper, we identify further work as fine-granular emotion detection of the OCD emotional states via a valence-arousal-dominance detection to derive the ‘degree’ of emotion of an OCD patient."
   ],
   "doi": "10.21437/SMM.2019-5"
  },
  "k19_smm": {
   "authors": [
    [
     "Punnoose A",
     "K"
    ]
   ],
   "title": "New Features for Speech Activity Detection",
   "original": "6",
   "page_count": 5,
   "order": 7,
   "p1": 26,
   "pn": 30,
   "abstract": [
    "﻿This paper discusses two new features for speech activity detection(SAD), using a multi-layer perceptron(mlp) trained to predict phoneme from acoustic features. The first feature is based on the difference between speech and noise histogram of certain phonemes. A scoring mechanism is formulated to score the softmax probabilities of the frames of a phoneme. The second feature is based on the correlation between softmax probabilities of the edge frames for certain phoneme transitions. A probabilistic approach is formulated to score the phoneme transition. Relevant datasets are used to prove the robustness of the proposed features in terms of speech activity detection."
   ],
   "doi": "10.21437/SMM.2019-6"
  },
  "cabral19_smm": {
   "authors": [
    [
     "João P.",
     "Cabral"
    ],
    [
     "Alexsandro R.",
     "Meireles"
    ]
   ],
   "title": "Transformation of voice quality in singing using glottal source features",
   "original": "7",
   "page_count": 5,
   "order": 8,
   "p1": 31,
   "pn": 35,
   "abstract": [
    "﻿Glottal activity information can be very important in several speech processing applications, such as in speech therapy,\nvoice disorder diagnosis, voice transformation and text-to-speech synthesis. However, the use of algorithms for estimating\nglottal parameters from the speech signal is very limited in those applications because of problems with robustness and accuracy. In singing synthesis, the glottal source representation is also very important because it is closely related with the emotions and singing style. This paper proposes a robust method to estimate the voice quality parameters of the glottal source by using both the electroglottographic (EGG) signal and the acoustic recordings of singing voice for five vowels in three different voice qualities: modal, breathy and creaky. The analysis of the resulting measurements permitted to confirm that voice quality parameters of the glottal source are correlated with the type of voice. Moreover, another experiment was conducted to show that it is possible to transform the modal singing voice into breathy and creaky by using an analysis-synthesis method that incorporates a glottal source model."
   ],
   "doi": "10.21437/SMM.2019-7"
  },
  "deshpande19_smm": {
   "authors": [
    [
     "Gauri",
     "Deshpande"
    ],
    [
     "Venkata Subramanian",
     "Viraraghavan"
    ],
    [
     "Rahul",
     "Gavas"
    ]
   ],
   "title": "A Successive Difference Feature for Detecting Emotional Valence from Speech",
   "original": "8",
   "page_count": 5,
   "order": 9,
   "p1": 36,
   "pn": 40,
   "abstract": [
    "﻿Many features have been proposed for detecting emotions from speech. Their detection performance is influenced by\nthe change in contextual parameters such as background noise, speaker variability, expressions, demographics and so on. In\nthis paper, we use a recent, time-domain feature extraction technique for detecting emotional-valence. We report the performance of the time-domain features on data in three different contexts: the Fearless-Steps challenge data for Sentiment Detection with spontaneous emotions, an Indian-demography corpus with induced emotions, and the Berlin database of acted emotions (EmoDB). Data is pre-processed according to the environment they are captured in, but the feature extraction that follows is identical. With these features, a RandomForest Classifier yields an accuracy of 71% on the development set of the challenge data, and 74% on the evaluation set, a significant improvement on the published baseline result of 49%. The same features provide an accuracy of 75% on the Indian-demography corpus and 100% accuracy in classifying happy, sad and neutral emotions from EmoDB, again with RandomForest Classifier. These results are better than those obtained with other prevalent techniques such as Long Short Term Memory (LSTM) with spectrograms, and the RandomForest classifier with the widely accepted features, OpenSMILE and Mel-Frequency Cepstral Coefficients (MFCCs)."
   ],
   "doi": "10.21437/SMM.2019-8"
  },
  "viraraghavan19_smm": {
   "authors": [
    [
     "Venkata",
     "Viraraghavan"
    ],
    [
     "Rahul",
     "Gavas"
    ],
    [
     "Hema",
     "Murthy"
    ],
    [
     "R",
     "Aravind"
    ]
   ],
   "title": "Visualizing Carnatic music as projectile motion in a uniform gravitational field",
   "original": "9",
   "page_count": 5,
   "order": 11,
   "p1": 41,
   "pn": 45,
   "abstract": [
    "﻿There are several examples of visualization of Western classical music, many of which use the score as a reference. By\ncontrast, a standard descriptive notation that serves the same purpose for Indian music is not available. In this paper, we\ndraw upon recent transcription approaches for Carnatic music (CM) and propose an automated visualization procedure. It is\nformalized by an analogy between the pitch movement and projectile motion in a uniform gravitational field. During constant-pitch notes (CPNs), the projectile moves horizontally on ledges at constant speed. Outside CPNs, reflectors are placed at the points where the pitch curve changes its direction. The positions of reflectors and ledges are quantized, and the motion between them is under the influence of gravity. We then suggest that the non-uniform scaling of CPNs, silence, and transients (i.e. music outside CPNs) in CM matches the analogy. Finally, the projectile motion equations are used to interpolate pitch curves for CM synthesis. Perceptual tests rate it on par with existing interpolation schemes. In addition, experts felt that visualization is needed to correct errors in descriptive notation."
   ],
   "doi": "10.21437/SMM.2019-9"
  },
  "greer19_smm": {
   "authors": [
    [
     "Timothy",
     "Greer"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Using Shared Vector Representations of Words and Chords in Music for Genre Classification",
   "original": "10",
   "page_count": 5,
   "order": 12,
   "p1": 46,
   "pn": 50,
   "abstract": [
    "﻿With so much music readily available for consumption today, it has never been more important to study music perception. In this paper, we represent lyrics and chords in a shared vector space using a phrase-aligned lyrics-and-chords corpus and show that models that use these shared representations can predict musical genre of songs—a perceptual construct of music listening—better than models that do not use these representations. This work adds to our understanding of how lyrics and chords interact with one another in music and has applications in multimodal perception and music information retrieval.\n"
   ],
   "doi": "10.21437/SMM.2019-10"
  },
  "cantisani19_smm": {
   "authors": [
    [
     "Giorgia",
     "Cantisani"
    ],
    [
     "Gabriel",
     "Trégoat"
    ],
    [
     "Slim",
     "Essid"
    ],
    [
     "Gaël",
     "Richard"
    ]
   ],
   "title": "MAD-EEG: an EEG dataset for decoding auditory attention to a target instrument in polyphonic music",
   "original": "11",
   "page_count": 5,
   "order": 13,
   "p1": 51,
   "pn": 55,
   "abstract": [
    "﻿We present MAD-EEG, a new, freely available dataset for studying EEG-based auditory attention decoding considering the challenging case of subjects attending to a target instrument in polyphonic music. The dataset represents the first music-related EEG dataset of its kind, enabling, in particular, studies on single-trial EEG-based attention decoding, while also opening the path for research on other EEG-based music analysis tasks. MAD-EEG has so far collected 20-channel EEG signals\nrecorded from 8 subjects listening to solo, duo and trio music excerpts and attending to one pre-specified instrument. The\nproposed experimental setting differs from the ones previously considered as the stimuli are polyphonic and are played to the subject using speakers instead of headphones. The stimuli were designed considering variations in terms of number and type of instruments in the mixture, spatial rendering, music genre and melody that is played. Preliminary results obtained with a state-of-the-art stimulus reconstruction algorithm commonly used for speech stimuli show that the audio representation reconstructed from the EEG response is more correlated with that of the attended source than with the one of the unattended source, proving the dataset to be suitable for such kind of studies."
   ],
   "doi": "10.21437/SMM.2019-11"
  },
  "agarwal19_smm": {
   "authors": [
    [
     "Rajat",
     "Agarwal"
    ],
    [
     "Ravinder",
     "Singh"
    ],
    [
     "Suvi",
     "Saarikallio"
    ],
    [
     "Katrina",
     "McFerran"
    ],
    [
     "Vinoo",
     "Alluri"
    ]
   ],
   "title": "Mining Mental States using Music Associations",
   "original": "12",
   "page_count": 4,
   "order": 14,
   "p1": 56,
   "pn": 59,
   "abstract": [
    "﻿Owing to the stigmatization of mental illnesses such as depression in India [1], there is a need for indirect unsuspecting ways to identify risk for depression and provide timely intervention. Healthy-Unhealthy Music scale (HUMS) [2] is one such assessment tool developed on Australian population that uses music engagement as an indicator of anxiety levels and potential high-risk for depression as assessed by Kessler’s Psychological Distress Scale (K10). The current study aims to ascertain its validity in an Indian setting followed by applying machine learning approaches to predict mental well-being from music associations. A diverse group comprising Indian adult population was assessed using HUMS and mental well-being and proneness to depression measures. HUMS structure investigated via Exploratory factor analyses, and concurrent validity tested with correlations to depression risk and wellbeing revealed high external validity and applicability of HUMS in Indian adult population. Furthermore, very low in-sample error for models like Support Vector Machines (SVM) with nonlinear kernels suggests an underlying pattern between HUMS responses and K10 score. Finally, a two-class model resulted in out of sample accuracy of 81%. To conclude, HUMS demonstrates high generalizability and hence applicability in Indian adult population and potential for employing ML models to capture the underlying pattern."
   ],
   "doi": "10.21437/SMM.2019-12"
  },
  "rudenko19_smm": {
   "authors": [
    [
     "Svetlana",
     "Rudenko"
    ],
    [
     "João P.",
     "Cabral"
    ]
   ],
   "title": "Synaesthesia: How can it be used to enhance the audio-visual perception of music and multisensory design in digitally enhanced environments?",
   "original": "13",
   "page_count": 5,
   "order": 15,
   "p1": 60,
   "pn": 64,
   "abstract": [
    "﻿Synaesthesia is a type of sensory cross-activation related to a specific wiring of the brain which has additional neuronal\npathways between different cortexes, creating various sensory experiences, such as people “hearing colours” or “seeing sounds”. There is a great potential of applying the knowledge about synaesthesia in different fields. For example, it could lead to innovative developments of educational programs for the young age cross-modal perception, digital applications for children with autism, and technological solutions to train plasticity of ageing brain. Synaesthetes very often benefit from their condition, especially in production of creative works, transposing experience from one sensory modality to another. For example, mixing sounds to create colour palette, as Vincent van Gogh, or composing modes of limited transposition with very clear view of colours, as Olivier Messiaen. Both had the chromaesthesia type of synaesthesia. This paper reviews important achievements in modelling synaesthetes perceptual experiences, with a focus on audio-visual perceptions, such as 4D digital visualisation of musical texture/musical-space synaesthesia, Cognitive Musicology visuals for classical music based on cross-modal associations and composing for animated Art. We also discuss the implications of the cross-modal sensory output on visual digital applications. Our assumption is that applications of cross-modal perception models based on synaesthesia can contribute to more engaging experiences to users, including the possibility to stimulate plasticity of the brain through multisensory design and digitally enhanced environments in Augmented Reality (AR) and Virtual Reality (VR)."
   ],
   "doi": "10.21437/SMM.2019-13"
  }
 },
 "sessions": [
  {
   "title": "Forenoon keynote speech",
   "papers": [
    "narayanan19_smm"
   ]
  },
  {
   "title": "Detecting mental states from speech",
   "papers": [
    "ishi19_smm",
    "gauder19_smm",
    "riera19_smm",
    "v19_smm",
    "gupta19_smm",
    "k19_smm",
    "cabral19_smm",
    "deshpande19_smm"
   ]
  },
  {
   "title": "Afternoon keynote speech",
   "papers": [
    "burgoyne19_smm"
   ]
  },
  {
   "title": "Effect of music and audio on mental states",
   "papers": [
    "viraraghavan19_smm",
    "greer19_smm",
    "cantisani19_smm",
    "agarwal19_smm",
    "rudenko19_smm"
   ]
  }
 ],
 "doi": "10.21437/SMM.2019"
}