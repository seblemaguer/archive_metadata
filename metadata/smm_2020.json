{
 "title": "Workshop on Speech, Music and Mind (SMM 2020)",
 "location": "Nanjing, China",
 "startDate": "23/10/2020",
 "endDate": "23/10/2020",
 "URL": "http://www.csmcw-csmt.cn/SMM20/index.html",
 "chair": "Chairs: Venkata S Viraraghavan, Saho Xi, Alexander Schindler, João P Cabral, Gauri Deshpande and Sachin Patel",
 "conf": "SMM",
 "year": "2020",
 "name": "smm_2020",
 "series": "SMM",
 "SIG": "",
 "title1": "Workshop on Speech, Music and Mind",
 "title2": "(SMM 2020)",
 "date": "23 October 2020",
 "booklet": "smm_2020.pdf",
 "papers": {
  "li20_smm": {
   "authors": [
    [
     "Haifeng",
     "Li"
    ]
   ],
   "title": "Cognitive mechanism of auditory emotion and inspiration for emotion recognition",
   "original": "abs1",
   "page_count": 0,
   "order": 1,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿Emotion plays an important role in human mental life. It is a conscious mental experience reflecting the personal significance of internal and external events. Music is one of the oldest art forms for human beings to express their thoughts and feelings. And speech is another principal conveyer to express one’s emotion during human social communication. The ability to identify vocal expressions of emotion or attitude in speech is one of the basic cognitive functions of human beings. With the development of artificial intelligence and neurocomputing, Multi-channel EEG provides population measures of neurons that allow us to uncover the complex cognitive processes of emotional information integration and processing. In this talk, first, several novel signal processing methods for investigating the emotional cognitive process are introduced and the brain mechanisms of emotion perception in the music and speech are presented. Then, an enhanced model of emotion recognition inspired by  the cognitive principles are shown."
   ]
  },
  "busso20_smm": {
   "authors": [
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Multimodal emotion recognition: Understanding the production process before modeling multimodal behaviors",
   "original": "abs2",
   "page_count": 0,
   "order": 4,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿The verbal and non-verbal channels of human communication are internally and intricately connected. As a result, gestures and speech present high levels of correlation and coordination. This relationship is greatly affected by the linguistic and emotional content of the message being communicated. The interplay is observed across the different communication channels such as various aspects of speech, facial expressions, and movements of the hands, head and body. For example, facial expressions and prosodic speech tend to have a stronger emotional modulation when the vocal tract is physically constrained by the articulation to convey other linguistic communicative goals. This interplay also affects gestures and speech events observed across individuals. Psycholinguistic studies on human communication have shown that during human interaction individuals tend to adapt their behaviors mimicking the spoken style, gestures and expressions of their conversational partners. This synchronization pattern is referred to as entrainment. We study the presence of entrainment at the emotion level in cross-modality settings and its implications on multimodal emotion recognition systems. The analysis explores the relationship between acoustic features of the speaker and facial expressions of the interlocutor during dyadic interactions. The analysis shows a strong mutual influence in their expressive behaviors. The seminar will discuss the clear implications of these results for audiovisual emotion recognition, and other areas of affective computing. "
   ]
  },
  "kadali20_smm": {
   "authors": [
    [
     "Devi Bhavani",
     "Kadali"
    ],
    [
     "Vinay Kumar",
     "Mittal"
    ]
   ],
   "title": "Studies on Paralinguistic Sounds, Emotional Speech and Expressive Voices",
   "original": "3",
   "page_count": 5,
   "order": 5,
   "p1": 11,
   "pn": 15,
   "abstract": [
    "﻿Human speech consists of both Verbal and Nonverbal sounds. Nonverbal sounds such as paralinguistics and expressive voices, along with emotional speech, play an important role in human communication. Nonverbal speech sounds not only convey important non-linguistic information, but also carry linguistic content in some cases. For example, emotional audio-visual signals carry linguistic information along with the emotions that convey the physical and mental state of a human being. Several studies have explored different signal processing methods and features for analysing the paralinguistic sounds and expressive voices. Different approaches for emotion classification have also been examined. Few studies have also attempted developing automated systems for emotion recognition However, much research work is still required for a range of diverse practical applications. Hence, in this paper, different databases, signal processing methods, features extracted, classifiers used for analyses of paralinguistic sounds and expressive voices, performance evaluation results and diverse applications are reviewed. The objective of this paper is to present the diverse information related to this domain in a concise manner and at a single place, that can be quite helpful to the beginners as well as experts in this domain. This paper should also help further research work in this direction."
   ],
   "doi": "10.21437/SMM.2020-3"
  },
  "pandharipande20_smm": {
   "authors": [
    [
     "Meghna",
     "Pandharipande"
    ],
    [
     "Upasana",
     "Tiwari"
    ],
    [
     "Rupayan",
     "Chakraborty"
    ],
    [
     "Sunil Kumar",
     "Kopparapu"
    ]
   ],
   "title": "Ranking Contact Center Conversations using Dynamic Programming based Pattern Matching",
   "original": "4",
   "page_count": 5,
   "order": 6,
   "p1": 16,
   "pn": 20,
   "abstract": [
    "﻿Any customer initiated inbound call is not only essential for the service agent to respond, but also to resolve the problem. Although agents are trained not to be expressive and provide solution timely and in best possible ways, but the course of the conversation actually plays an important role in deciding the outcome of the call, whether it ends with a positive or negative note. Timely identifying dissatisfied customer based on the automatic analysis of the conversation to measure the positivity of a call, can aid to improve customer satisfaction index and the call handling capabilities of the service desk. In this paper, to automatically analyze the call conversation, we propose a system that extract non-linguistic features, create patterns using multi-dimensional representations, followed by a dynamic algorithm to find the similarity measures. Further, we fuse the information regarding the trend in the variation of the affective content throughout the conversation to get a final score that quantifies the positiveness in a call. Finally, ranking contact center calls in the decreasing order of the positivity measure and evaluate the system using a ranking agreement metric."
   ],
   "doi": "10.21437/SMM.2020-4"
  },
  "surana20_smm": {
   "authors": [
    [
     "Aayush",
     "Surana"
    ],
    [
     "Yash",
     "Goyal"
    ],
    [
     "Vinoo",
     "Alluri"
    ]
   ],
   "title": "Static and Dynamic Measures of Active Music Listening as Indicators of Depression Risk",
   "original": "1",
   "page_count": 5,
   "order": 2,
   "p1": 1,
   "pn": 5,
   "abstract": [
    "﻿Music, an integral part of our lives, which is not only a source of entertainment but plays an important role in mental well-being by impacting moods, emotions and other affective states. Music preferences and listening strategies have been shown to be associated with the psychological well-being of listeners including internalized symptomatology and depression. However, till date no studies exist that examine time-varying music consumption, in terms of acoustic content, and its association with users’ well-being. In the current study, we aim at unearthing static and dynamic patterns prevalent in active listening behavior of individuals which may be used as indicators of risk for depression. Mental well-being scores and listening histories of 541 Last.fm users were examined. Static and dynamic acoustic and emotion-related features were extracted from each user’s listening history and correlated with their mental well-being scores. Results revealed that individuals with greater depression risk resort to higher dependency on music with greater repetitiveness in their listening activity. Furthermore, the affinity of depressed individuals towards music that can be perceived as sad was found to be resistant to change over time. This study has large implications for future work in the area of assessing mental illness risk by exploiting digital footprints of users via online music streaming platforms."
   ],
   "doi": "10.21437/SMM.2020-1"
  },
  "sharon20_smm": {
   "authors": [
    [
     "Rini A",
     "Sharon"
    ],
    [
     "Hema A",
     "Murthy"
    ]
   ],
   "title": "Correlation based Multi-phasal models for improved imagined speech EEG recognition",
   "original": "5",
   "page_count": 5,
   "order": 7,
   "p1": 21,
   "pn": 25,
   "abstract": [
    "﻿Translation of imagined speech electroencephalogram(EEG) into human understandable commands greatly facilitates the design of naturalistic brain computer interfaces. To achieve improved imagined speech unit classification, this work aims to profit from the parallel information contained in multi-phasal EEG data recorded while speaking, imagining and performing articulatory movements corresponding to specific speech units. A bi-phase common representation learning module using neural networks is designed to model the correlation and reproducibility between an analysis phase and a support phase. The trained Correlation Network is then employed to extract discriminative features of the analysis phase. These features are further classified into five binary phonological categories using machine learning models such as Gaussian mixture based hidden Markov model and deep neural networks. The proposed approach further handles the non-availability of multi-phasal data during decoding. Topographic visualizations along with result-based inferences suggest that the multi-phasal correlation modelling approach proposed in the paper enhances imagined speech EEG recognition performance."
   ],
   "doi": "10.21437/SMM.2020-5"
  },
  "viraraghavan20_smm": {
   "authors": [
    [
     "Venkata S",
     "Viraraghavan"
    ],
    [
     "Rahul D",
     "Gavas"
    ],
    [
     "Ramesh K",
     "Ramakrishnan"
    ]
   ],
   "title": "Role of emotion words in detecting emotional valence from speech",
   "original": "6",
   "page_count": 5,
   "order": 8,
   "p1": 26,
   "pn": 30,
   "abstract": [
    "﻿An important task in several wellness applications is detection of emotional valence from speech. Two types of features of speech signals are used to detect valence: acoustic features and text features. Acoustic features are derived from short frames of speech, while text features are derived from the text transcription. In this paper, we investigate the effect of text on acoustic features. Some studies show that acoustic features of phones carry specific emotion information. We also observe that emotion words and the emotional valence of the spoken sentence need not always match (e.g. the usage of ‘not happy’). We thus propose that acoustic features of speech segments carrying emotion words must be treated differently from other segments that do not carry such words. In this paper, we propose that all speech segments carrying emotion words are excluded from the training set. Standard emotion words from a language, words from Plutchik’s wheel of emotion, and their synonyms are considered. We report performance results on the the Elderly Emotion Sub-Challenge corpus of the Computational Paralinguistics Challenge 2020. We show that exclusion of emotional words show significant improvements for both OpenSMILE (p < 0.05) and OpenXBoAW features (p < 0.01)."
   ],
   "doi": "10.21437/SMM.2020-6"
  },
  "viraraghavan20b_smm": {
   "authors": [
    [
     "Venkata S",
     "Viraraghavan"
    ],
    [
     "Vasundhara",
     "Agrawal"
    ],
    [
     "Rahul Dasharath",
     "Gavas"
    ],
    [
     "Tince",
     "Varghese"
    ],
    [
     "Varnika",
     "Naik"
    ],
    [
     "Mayuri",
     "Duggirala"
    ],
    [
     "Ramesh K",
     "Ramakrishnan"
    ]
   ],
   "title": "Grammar-constrained music generation for unconstrained doodling",
   "original": "2",
   "page_count": 5,
   "order": 3,
   "p1": 6,
   "pn": 10,
   "abstract": [
    "﻿Music as well as art both have been long recognized for their therapeutic benefits. Music not only helps in enhancing general wellbeing but also in combating several mental health issues. Similarly, art therapy enables individuals to cope with their emotional conflicts and allows self-expression. In art therapy, doodling has especially been recognized for its various benefits ranging from catharsis to improvement in memory. In this paper we propose a system that amalgamates music and doodling. The paper presents the mapping of the unconstrained doodling patterns received as user input to generate pleasant music using certain constraints. The paper explains the implementation of these constraints in the form of the musical grammar from the genre of Hindustani and Carnatic music. Initial feedback indicates that such genre-based constraints are clearly needed for the generated audio to sound musical."
   ],
   "doi": "10.21437/SMM.2020-2"
  }
 },
 "sessions": [
  {
   "title": "Keynote speech A",
   "papers": [
    "li20_smm"
   ]
  },
  {
   "title": "Session A: Effect of music and audio on mental states",
   "papers": [
    "surana20_smm",
    "viraraghavan20b_smm"
   ]
  },
  {
   "title": "Keynote speech B",
   "papers": [
    "busso20_smm"
   ]
  },
  {
   "title": "Session B: Detecting mental states from speech",
   "papers": [
    "kadali20_smm",
    "pandharipande20_smm",
    "sharon20_smm",
    "viraraghavan20_smm"
   ]
  }
 ],
 "doi": "10.21437/SMM.2020"
}