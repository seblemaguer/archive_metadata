{
 "location": "St. Jorioz, France",
 "startDate": "4/9/2003",
 "endDate": "7/9/2003",
 "conf": "AVSP",
 "year": "2003",
 "name": "avsp_2003",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "4-7 September 2003",
 "papers": {
  "fogassi03_avsp": {
   "authors": [
    [
     "Leonardo",
     "Fogassi"
    ]
   ],
   "title": "Evolution of language from action understanding",
   "original": "av03_001",
   "page_count": 2,
   "order": 1,
   "p1": "1",
   "pn": "2",
   "abstract": [
    "In my presentation I will try to provide hypotheses and neurophysiological evidences about the possible evolution of language from a neural motor system involved in action recognition.\n",
    "There are different theories about language evolution. Some authors claim that language evolved from monkey vocalization, others from gestural communication. Others even denies a direct evolution of language from some monkey precursor, considering the former as a completely new acquisition of the human gender, with characteristics that are completely different from any other animal cognitive function.\n",
    "A number of comparative anatomical and neurophysiological data seem to support a gradual evolution of language from gestural communication.\n",
    "Vocalization could at first glance appear as a very good candidate for evolution of verbal communication. However it must be noted that vocalization in monkeys is, in general, a function that an individual uses for sending some important messages to its group (for example about the presence of a specific predator). Furthermore, vocalization is under the neural control of the primitive mesial cingulate circuit, that is mainly involved in emotional behavior.\n",
    "Differently from these forms of communication, that between two individuals is less linked to emotional messages, often conveying a relational content. This content is often expressed with gestures.\n",
    "Where in the cortex are located the neural circuits involved in gesture production? Gestures can be considered belonging to the field of goal-directed actions, because, although often devoid of an object target, they are associated to a meaning. Actions are coded in several areas of premotor cortex, and, more extensively, in the parieto-frontal circuits formed by interconnected specific premotor and parietal areas. Coding of goal-directed actions at the single neuron level was studied in more detail in the ventral premotor cortex of the macaque monkey, that is in area F4 and F5. Area F4 is mainly involved in axial and proximal actions toward spatial targets. Area F5 code different types of hand and mouth actions such as grasping, manipulating, holding, tearing. In area F5 there is also a class of visuomotor neurons that become active not only when the monkey performs hand actions, but also when it observes another individual making similar actions. The interest of these neurons consists in the fact that, by matching action observation with action execution, they allow understanding of actions made by others. This capacity is not simply limited to a recognition of motor patterns, but it extends also to the goal of the observed action. The observation/execution matching system represented by mirror neurons could be crucial for an inter-individual communication system originally based on gestures. A communicative gesture made by an actor (the sender) retrieves in the observer (the receiver) the neural circuit encoding the motor representation of same gesture. This allows the receiver to understand the message of the observer and, perhaps, to begin a response (see Rizzolatti and Arbib, TINS 21: 188- 194,1998).\n",
    "Recently in the monkey we discovered some new categories of mirror neurons that could explain the transition between a basic action understanding neural system to a system endowed with features typical of language.\n",
    "First of all, we found that in area F5, beyond hand mirror neurons, there are also mouth mirror neurons (Ferrari et al. Eur. J. Neurosci. 17: 1703-1714, 2003). Most of these neurons activate during observation and execution of mouth ingestive action, such as biting, tearing with the teeth, sucking, etc. A small percent of mouth mirror neurons respond also to the observation of mouth communicative actions belonging to the monkey repertoire. All of these actions have affiliative meaning. Interestingly, neurons responding to the observation of mouth communicative actions are also active during the execution of mouth ingestive actions motorically similar to the observed ones. These results suggest that in ventral premotor cortex the action understanding system starts to evolve in a oro-facial communicative system that is not linked anymore to emotional behavior, as that represented in the mesial cortex, but to affiliative behavior. This evolution occurs in a region endowed with the apparatus controlling the execution of ingestive behavior. The visuo-motor properties of mouth mirror neurons suggest that the structures of ingestive behavior could have been taken up in order to be used for communicative behavior. This transformation is in line with the proposal by Mc Neilage (Behav. Brain Sci. 21: 499-546, 1998) of a derivation, in the lateral cortical system, of the syllabic frame from the cyclic mandibular open-close alternation.\n",
    "Second, another category of hand-related mirror neurons was described, that become active when monkeys not only observe, but also hear the sound of an action (audio-visual mirror neurons) (Kohler et al. Science 297:846-848, 2002). The response of these neurons are specific for the type of action. For example, they respond to peanut breaking when the action is only observed or only heard or both heard and observed, and do not respond to the vision and sound of another action, for example paper tearing. Thus it appears that in these neurons action understanding can occur also through the acoustic channel. This result has two important implications: a) the acoustic input to a motor area allows the individual to retrieve the action representation present in this area, thus accessing to action meaning. Note that this is probably the process occurring during listening to spoken language; b) mirror neurons responding to the observation and to the sound of actions belong to the category of hand mirror neurons. This could be an important element for language evolution, because the association between a gesture and a sound could have occurred, in phylogenesis, before in hand-related neurons than in those related to mouth actions. This would be explained by the larger variety of gestures present in the monkey brachio-manual motor repertoire in respect to mouth gestures (see Rizzolatti and Arbib, TINS 21: 188-194, 1998). The coupling between oro-facial gestures and sounds (not vocalization) could have been a subsequent acquisition.\n",
    "The discovery in area F5 of mouth communicative mirror neurons and of audio-visual mirror neurons is in good agreement with the proposed homology between F5 and Brocas area. Area 44 (part of Broca) and area F5 are both dysgranular. They both have a mouth and hand representation (many brain imaging experiments demonstrate involvement of Brocas area in hand movements, beyond its classical role in speech). They both respond to the observation of hand and mouth actions (see Rizzolatti et al. Nat. Rev. Neurosci. 2:661- 670, 2001). Recently it was demonstrated that Brocas area is activated both when subjects observe biting action and when they observe other individuals performing silent speech. Another support for the derivation of Brocas area from F5 is the recently described left asymmetry present in the ventral premotor cortex of apes (Cantalupo and Hopkins, Nature 414: 505, 2001). Thus Brocas area could be the result of the evolution of a neural system initially able to produce and understand hand and mouth actions. Part of this system would have generated a population of neurons with communicative properties and another sensitive to meaningful acoustic stimuli. The process through which the oro-facial system achieved the properties to emit meaningful sound has to be investigated.\n",
    "s\n",
    "Rizzolatti G. and Arbib M.A. (1998) Language within our grasp. TINS 21: 188-194. Ferrari P.F., Gallese V., Rizzolatti G. and Fogassi, L. (2003) Mirror neurons responding to the observation of ingestive and communicative mouth actions. Eur. J. Neurosci. 17: 1703-1714. Mac Neilage P.F. (1998) The frame/content theory of evolution of speech production. Behav. Brain Sci. 21: 499-546. Kohler E., Keysers C., Umiltà M.A., Fogassi L., Gallese V. and Rizzolatti G. (2002) Hearing sounds, understanding actions: action representation in mirror neurons. Science 297:846-848. Rizzolatti G., Fogassi L. and Gallese V. (2001) Neurophysiological mechanisms underlying the understanding and imitation of action. Nat. Rev. Neurosci. 2:661-670. Cantalupo C. and Hopkins W.D (2001) Asymmetric Broca's area in great apes. Nature 414: 505.\n",
    ""
   ]
  },
  "lebib03_avsp": {
   "authors": [
    [
     "Riadh",
     "Lebib"
    ],
    [
     "David",
     "Papo"
    ],
    [
     "Abdel",
     "Douiri"
    ],
    [
     "Stella de",
     "Bode"
    ],
    [
     "Pierre-Marie",
     "Baudonniere"
    ]
   ],
   "title": "Early processing of visual speech information modulates the subsequent processing of auditory speech input at a pre-attentive level: Evidence from event-related brain potential data",
   "original": "av03_003",
   "page_count": 6,
   "order": 2,
   "p1": "3",
   "pn": "8",
   "abstract": [
    "In the present study, we examined the degree of attenuation/augmentation  of the sensory response to congruent or non-congruent audiovisual speech  stimuli as reflected by the P50 amplitudes in normal volunteers. We also  analyse the early response to mouth movement onset, and dipole modeling  was conducted to study the specific brain structures activated during  early visual speech processing. The specific aims of the study were  a) to show that speech processing starts as soon as lip movements  occurred, b) to provide data that the brain can detect changes  in incoming bimodal speech stimuli at either a pre-attentive or a  very early attentive stage of information processing as reflected  in the P50 component, and c) to confirm that this early detection  is dependent upon the congruency status and the discriminability  level of audiovisual speech input. Finally, the goal of this project  was to generalize the concept of sensory gating with \"real-life\" stimuli, that is semantically-relevant and multimodal.\n",
    ""
   ]
  },
  "kim03_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Testing the cuing hypothesis for the AV speech detection advantage",
   "original": "av03_009",
   "page_count": 4,
   "order": 3,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "Seeing the moving face of the talker permits better detection of speech in noise compared to not seeing their face. We report on an experiment that examined the basis of this AV facilitation effect. This work follows up research by [1] and [2] that developed a procedure for demonstrating an AV speech detection effect and [3] that showed that this facilitation occurred regardless of whether participants knew the language of test. In the current experiment we tested to see if AV facilitation occurred because participants were cued to when to pay attention by relatively simply properties of the visual speech of the talker (e.g., when the talkers mouth opened wide). This cuing idea was tested for two types of auditory and visual information that altered the naturalness of speech but maintained many simply cues. The first alteration was to present the AV stimuli backwards, e.g., (both speech and vision played timereversed). The second used a computer-generated face (Baldi) with synthesised speech. We also tested with a human talker with time-forward presentation. Our findings indicated that AV facilitation only occurred for the time forward human talker presentation; we discuss these results with respect to different types of Audio-Visual cuing.\n",
    "s\n",
    "Grant, K.W. / Seitz, P. (2000). The use of visible speech cues for improving auditory detection of spoken sentences. Journal of the Acoustical Society of America, 108, 1197-1208. Grant, K.W. (2001). The effect of speechreading on masked detection thresholds for filtered speech. Journal of the Acoustical Society of America, 109, 2272-2275. Kim, J. & Davis, C. (2003). Hearing foreign voices: does knowing what is said affect masked visual speech detection? Perception, 32, 111-120.\n",
    ""
   ]
  },
  "bernstein03_avsp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Sumiko",
     "Takayanagi"
    ],
    [
     "Edward T. jr.",
     "Auer"
    ]
   ],
   "title": "Enhanced auditory detection with av speech: perceptual evidence for speech and non-speech mechanisms",
   "original": "av03_013",
   "page_count": 5,
   "order": 4,
   "p1": "13",
   "pn": "17",
   "abstract": [
    "Speech in a noisy or reverberant environment is more detectable and  more intelligible when the listener can see the talker. How to explain  these perceptual phenomena is a fundamental problem for AV speech  research. We have undertaken a series of behavioral and  electrophysiological experiments to investigate the perceptual  and neural bases for enhanced auditory speech detection in noise  with AV stimuli. We hypothesize that the enhancement effect arises  due to at least two neurophysiologically distinct mechanisms, one  in no way specialized for speech and the other specific to speech  stimuli. Here we report results of a perceptual experiment in  which an auditory /ba/ token was presented adaptively to obtain  its 71% detection threshold [1] in white noise. Participants were  tested in three conditions, auditory-only speech, audiovisual  speech, and auditory speech with a visual dynamic Lissajous  figure. The Lissajous figure was a control for many of the  complex visual features of speech. Evidence was obtained for two  separate sources of AV detection enhancement: Detection thresholds  were highest for the auditory-only speech, lower for the auditory  speech with the Lissajous figure, and lowest for the audiovisual  speech. Our Discussion section outlines the implications and  limitations of the current results for explaining the AV speech  detection enhancement effect.\n",
    ""
   ]
  },
  "schwartz03_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "Auditory syllabic identification enhanced by non-informative visible speech",
   "original": "av03_019",
   "page_count": 6,
   "order": 5,
   "p1": "19",
   "pn": "24",
   "abstract": [
    "Recent experiments show that seeing lip movements may improve the detection of speech sounds embedded in noise. We show here that the \"speech detection\" benefit may result in a \"speech identification\" benefit different from lipreading per se. The experimental trick consists in dubbing the same lip gesture on a number of visually similar but auditorily different configurations, e.g. [y u ty tu ky ku dy du gy gu] in French. The visual stimulus does not enable to identify the syllable, but it provides a temporal cue improving the audio identification of these stimuli embedded in a large level of cocktail-party noise, and particularly the identification of plosive voicing. Replacing the visual speech cue (the lip rounding gesture) by a nonspeech one with the same temporal pattern (a red bar on a black background, increasing and decreasing in synchrony with the lips) removes the benefit.\n",
    ""
   ]
  },
  "conrey03_avsp": {
   "authors": [
    [
     "Brianna L.",
     "Conrey"
    ],
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Audiovisual asynchrony detection for speech and nonspeech signals",
   "original": "av03_025",
   "page_count": 6,
   "order": 6,
   "p1": "25",
   "pn": "30",
   "abstract": [
    "This study investigated the \"intersensory temporal synchrony window\" [1] for audiovisual (AV) signals. A speeded asynchrony detection task was used to measure each participants temporal synchrony window for speech and nonspeech signals over an 800-ms range of AV asynchronies. Across three sets of stimuli, the video-leading threshold for asynchrony detection was larger than the audio-leading threshold, replicating previous findings reported in the literature. Although the audio-leading threshold did not differ for any of the stimulus sets, the video-leading threshold was significantly larger for the point-light display (PLD) condition than for either the full-face (FF) or nonspeech (NS) conditions. In addition, a small but reliable phonotactic effect of visual intelligibility was found for the FF condition. High visual intelligibility words produced larger video-leading thresholds than low visual intelligibility words. Relationships with recent neurophysiological data on multisensory enhancement and convergence are discussed.\n",
    "",
    "",
    "Lewkowicz, D.J., Perception of auditory-visual temporal synchrony in human infants. Journal of Experimental Psychology: Human Perception and Performance, 1996. 22: p. 1094-1106.\n",
    ""
   ]
  },
  "grant03_avsp": {
   "authors": [
    [
     "Ken W.",
     "Grant"
    ],
    [
     "Virginie van",
     "Wassenhove"
    ],
    [
     "David",
     "Poeppel"
    ]
   ],
   "title": "Discrimination of auditory-visual synchrony",
   "original": "av03_031",
   "page_count": 5,
   "order": 7,
   "p1": "31",
   "pn": "35",
   "abstract": [
    "Discrimination thresholds for temporal synchrony in auditory-visual sentence materials were obtained on a group of normal-hearing subjects. Thresholds were determined using an adaptive tracking procedure which controlled the degree of audio delay, both positive and negative in separate tracks, relative to a video image of a female speaker. Four different auditory filter conditions, as well as a broadband speech condition, were evaluated in order to determine whether discrimination thresholds were dependent on the spectral content of the acoustic speech signal. Consistent with previous studies of auditory-visual speech recognition which showed a broad, asymmetrical range of temporal synchrony (audio delays roughly between -40 ms and +240 ms) for which intelligibility was basically unaffected, synchrony discrimination thresholds also showed a broad, asymmetrical pattern of similar magnitude (audio delays roughly between -45 ms and 200 ms). No differences in synchrony thresholds were observed for the different filtered bands of speech, or for broadband speech. These results suggest a fairly tight coupling between a subject's ability to detect cross-modal asynchrony and the intelligibility of auditory-visual speech materials.\n",
    ""
   ]
  },
  "wassenhove03_avsp": {
   "authors": [
    [
     "Virginie van",
     "Wassenhove"
    ],
    [
     "Ken W.",
     "Grant"
    ],
    [
     "David",
     "Poeppel"
    ]
   ],
   "title": "Electrophysiology of auditory-visual speech integration",
   "original": "av03_037",
   "page_count": 6,
   "order": 8,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "Abstract\n",
    "Twenty-six native English speakers identified auditory (A), visual (V), and congruent and incongruent auditory-visual (AV) syllables while undergoing electroencephalography (EEC) in three experiments. In Experiment 1, unimodal (A, V) and bimodal (AV) stimuli were presented in separate blocks. In Experiment 2 the same stimuli were pseudo-randomized in the same blocks, providing a replication of Experiment 1 while testing the effect of participants' expectancy on the AV condition. In Experiment 3, NcGurk fusion (audio /pa/ dubbed onto visual /ka/, eliciting the percept /ta/) and combination (audio /ka/ dubbed onto visual /pa/) stimuli were tested under visnal attention.\n",
    "EEG recordings show early effects of visual influence on auditory evoked-related potentials (P1/N1/P2 complex). Specifically, a robust amplitude reduction of the Nl/P2 complex was observed (Experiments 1 and 2) that could not be solely accounted for by attentional effects (Experiment 3) The N1/P2 reduction was accompanied by a temporal facilitation (approximting ~2O ms) of the P1/N1 and N1/P2 transitions in AV conditions. Additionally, incongruient syllables showed a different profile from congruent AV /ta/ over a large latency range (~5O to 350 ms post-auditory onset), which was influenced by the accuracy of identification of the visual stinmli presented unimodally.\n",
    "Our results suggest that (i) auditory processing is modulated early on by visual speech inputs, in agreement with an early locus of AV speech interaction, (ii) natural precedence of visual kinematics facilitates auditory speech processing in the time domain, and (iii) the degree of  temporal gain is a function of the saliency of visual speech inputs.\n",
    ""
   ]
  },
  "sekiyama03_avsp": {
   "authors": [
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Denis",
     "Burnham"
    ],
    [
     "Helen",
     "Tam"
    ],
    [
     "Dogu",
     "Erdener"
    ]
   ],
   "title": "Auditory-visual speech perception development in Japanese and English speakers",
   "original": "av03_043",
   "page_count": 5,
   "order": 9,
   "p1": "43",
   "pn": "47",
   "abstract": [
    "Development of auditory-visual speech perception was investigated in a cross-linguistic developmental framework, using the McGurk effect. Stimuli consisting of /ba/, /da/, and /ga/ utterances were presented to participants who were asked to make syllable identifications on audiovisual (congruent and discrepant), audio-only, and video-only presentations at various signal-to-noise levels. The results of Experiment 1 with 24 adult native speakers of English and 24 of Japanese supported previous reports of a weaker visual influence for Japanese participants. Experiment 2 was a short version of Experiment1 in which 16 Japanese and 14 English language 6-year-olds, and new groups of 24 Japanese and 24 English adults were tested. The results showed that the degree of visual influence was low but statistically equivalent for Japanese and English language 6-year-olds, and that there was a significant increase in visual influence over age for the English but not the Japanese language groups. Nevertheless, both the Japanese and English language groups showed an increase in speechreading performance (in the visual-only condition). It appears that the developmental increase of speechreading performance is related to the increase of the size of the visual influence in the English language participants, whereas such a straightforward relationship is not the case for the Japanese participants.\n",
    ""
   ]
  },
  "mohammed03_avsp": {
   "authors": [
    [
     "Tara Ellis",
     "Mohammed"
    ],
    [
     "Mairéad",
     "MacSweeney"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Developing the TAS: Individual differences in silent speechreading, reading and phonological awareness in deaf and hearing speechreaders",
   "original": "av03_049",
   "page_count": 6,
   "order": 10,
   "p1": "49",
   "pn": "54",
   "abstract": [
    "We report new developments in the TAS: the Test of Adult Speechreading, a test designed to be appropriate to the needs of both deaf and hearing users. Profoundly prelingually deaf people outperform hearing users on this test. Subtests of TAS showed different patterns of inter-correlation in deaf and hearing people, suggesting different underlying factors accounting for individual variability in speechreading which depended on hearing status. Although reading was poorer in deaf than hearing groups, it correlated with phonological awareness and with TAS scores. Phonological awareness may have performed a mediating role in the relationship between reading and silent speechreading in deaf participants. There was no relationship between reading and TAS in the hearing users.\n",
    ""
   ]
  },
  "bergeson03_avsp": {
   "authors": [
    [
     "Tonya R.",
     "Bergeson"
    ],
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Jeffrey T.",
     "Reynolds"
    ]
   ],
   "title": "Perception of point light displays of speech by normal-hearing adults and deaf adults with cochlear implants",
   "original": "av03_055",
   "page_count": 6,
   "order": 11,
   "p1": "55",
   "pn": "60",
   "abstract": [
    "Normal-hearing (NH) adults display audiovisual enhancement when degraded auditory input (e.g., words, sentences) is paired with point -light displays (PLDs) of speech, which isolate the kinematic properties of a speakers face [10]. Do deaf adults who use cochlear implants (CIs) benefit in the same way? Does feedback influence NH adults performance? In the present study, we investigated audiovisual (AV) word recognition using PLDs of speech in postlingually deaf adults with CIs and NH adults. Both groups displayed evidence of AV enhancement with PLDs. Moreover, NH participants Visual-alone performance improved over time with Auditory-alone and AV feedback. These results suggest that NH and CI adults were sensitive to the kinematic properties in speech represented in the PLDs, and they were able to use kinematics to improve their word recognition performance even with highly degraded visual displays of speech. In addition, NH adults were able to use temporal cues from Auditory-alone and AV feedback to improve their word recognition performance with point-light visual displays of speech.\n",
    ""
   ]
  },
  "cathiard03_avsp": {
   "authors": [
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Séverine",
     "Gedzelman"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ]
   ],
   "title": "Visual and auditory perception of epenthetic glides",
   "original": "av03_061",
   "page_count": 6,
   "order": 12,
   "p1": "61",
   "pn": "66",
   "abstract": [
    "The purpose of this contribution is to improve our knowledge about the time course of visual and auditory perception with regard to the representation of sound types as different in their phenomenological format as vowels and glides. Our results on the perception of Vowel-to-Vowel gesture via the production of epenthetic glides in between - according to our 2-COMP-Vowel Model - allow us to conceive of the timevarying vs. stationary representational issue as linked to the underlying control for moving phases: (i) while the stationary (plateau) or climax phase of a vowel can be shown as truly representative for this segment, (ii) and whereas motion in the time-varying on-gliding phase can be shown to be informative only when shape is incomplete, (iii) motion in the off-gliding phase of the same vowel can reveal itself as misleading, in the sense that it could prime an erroneous candidate for the following vowel, up to the end of the transitional glide in V-to-V. Thus, contrary to the dynamic specification theory for vowels, the moving on-gliding and off-gliding phases can be, respectively, less informative than stationary parts, as shown before, and even truly misleading as shown here. (iiii) Moreover the same off-gliding phase can be recovered as a true controlled glide under certain language-based constraints (as exemplified by the power-effect). Finally in line with our caveat against the claim that \"all is dynamics in speech\", we will briefly mention recent computational modeling and neural data which support our hypothesis, especially the snapshot neuronal computation which fits with recent brain imaging data of stilled and moving speaking mouths.\n",
    ""
   ]
  },
  "vroomen03_avsp": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Mirjam",
     "Keetels"
    ],
    [
     "Sabine van",
     "Linden"
    ],
    [
     "Béatrice de",
     "Gelder"
    ],
    [
     "Paul",
     "Bertelson"
    ]
   ],
   "title": "Selective adaptation and recalibration of auditory speech by lipread information: Dissipation",
   "original": "av03_067",
   "page_count": 4,
   "order": 13,
   "p1": "67",
   "pn": "70",
   "abstract": [
    "Recently, we have shown that visual speech can recalibrate auditory speech identification. When an ambiguous sound intermediate between /aba/ and /ada/ was dubbed onto a face articulating /aba/ (or /ada/), then the proportion of /aba/ responses increased in subsequent unimodal auditory sound identification trials. In contrast, when an unambiguous /aba/ sound was dubbed onto the face articulating /aba/, then the proportion of /aba/ responses decreased, revealing selective adaptation. Here we show that recalibration and selective adaptation not only differ in the direction of their after-effects, but also that they dissipate at a different rates, confirming that the effects are caused by different brain mechanisms.\n",
    ""
   ]
  },
  "ojanen03_avsp": {
   "authors": [
    [
     "Ville",
     "Ojanen"
    ],
    [
     "Jyrki",
     "Tuomainen"
    ],
    [
     "Mikko",
     "Sams"
    ]
   ],
   "title": "Effect of audiovisual primes on identification of auditory target syllables",
   "original": "av03_071",
   "page_count": 5,
   "order": 14,
   "p1": "71",
   "pn": "75",
   "abstract": [
    "We studied the representations underlying audiovisual integration using a priming paradigm. Audiovisual primes, preceding auditory targets, were either incongruent (auditory /ba/ & visual /va/) or congruent (auditory /va/ & visual /va/, auditory /ba/ & visual /ba/). The targets were /ba/ or /va/. The intensity of the prime´s auditory component was either 50 dB or 60 dB. Identification speed of the target /ba/ was strongly affected by the nature of the prime. The effect of the incongruent audiovisual prime depended on the intensity of its acoustic component. Our results suggest that processing of visible articulatory movements influence auditory speech processing.\n",
    ""
   ]
  },
  "schwartz03b_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Why the FLMP should not be applied to McGurk data ...or how to better compare models in the Bayesian framework",
   "original": "av03_077",
   "page_count": 6,
   "order": 15,
   "p1": "77",
   "pn": "82",
   "abstract": [
    "We come back on two criticisms addressed to the \"Fuzzy-Logical Model of Perception\" (FLMP): its assumed ability to fit any data, and the choice of a \"fitting all\" procedure rather than a \"predicting AV from A and V\" approach. We confirm the FLMP ability to fit random data in the \"McGurk region\", that is in all conditions including conflicting stimuli, thanks to the \"0/0 trick\". This is associated to a high instability of the rmse value in the optimal region. Then we introduce a Bayesian approach replacing fit by global likelihood. In this sounder framework, rmse instabilities decrease likelihood, hence the 0/0 trick is problematic for the FLMP. We conclude by indicating when the FLMP can be used in sound conditions: apart from the \"McGurk\" region.\n",
    ""
   ]
  },
  "massaro03_avsp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Model Selection in AVSP: Some old and not so old news",
   "original": "av03_083",
   "page_count": 6,
   "order": 16,
   "p1": "83",
   "pn": "88",
   "abstract": [
    "We reiterate a paradigm of inquiry in auditory visual speech processing, focusing on appropriate experimental procedures and methods of model selection. Several methods of model selection find evidence in support of the fuzzy logical model of perception (FLMP). We caution investigators to not limit themselves to simply testing the classic McGurk effect because its outcomes cannot distinguish among alternative interpretations.\n",
    ""
   ]
  },
  "berthommier03_avsp": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "A phonetically neutral model of the low-level audiovisual interaction",
   "original": "av03_089",
   "page_count": 6,
   "order": 17,
   "p1": "89",
   "pn": "94",
   "abstract": [
    "The improvement of detectability by visible speech cues found by  Grant and Seitz (JASA, 108:1197-1208, 2000) has been related to the  degree of correlation between acoustic envelopes and visible movements.  This suggests that the audio and visual signals could interact early  during the audio-visual perceptual process on the basis of audio  envelope cues. On the other hand, acoustic-visual correlations were  previously reported by Yehia et al.  (Speech Communication, 26(1):23-43, 1998). Taking into account these  two main facts, the problem of extraction of the redundant audio-visual  components is revisited: The video parametrization of natural images  and three types of audio parameters are tested together, leading to new  and realistic applications in video synthesis and audiovisual speech  enhancement. Consistently with Grant and Seitz prediction, the 4-subbands  envelope energy features are found to be optimal for encoding the  redundant components available for the enhancement task. The computational  model of audio-visual interaction which is proposed is based on the  product, in the audio pathway, between the time-aligned audio  envelopes and video-predicted envelopes. This interaction scheme  is shown to be phonetically neutral, so that it will not bias the  phonetic identification. Then, the low-level stage which is  described is compatible with a late integration process, and this is  a potential front-end for speech recognition applications.\n",
    ""
   ]
  },
  "potamianos03_avsp": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ],
    [
     "Sabine",
     "Deligne"
    ]
   ],
   "title": "Joint audio-visual speech processing for recognition and enhancement",
   "original": "av03_095",
   "page_count": 10,
   "order": 18,
   "p1": "95",
   "pn": "104",
   "abstract": [
    "Visual speech information present in the speakers mouth region has long been viewed as a source for improving the robustness and naturalness of human-computer-interfaces (HCI). Such information can be particularly crucial in realistic HCI environments, where the acoustic channel is corrupted, and as a result, the performance of traditional automatic speech recognition (ASR) systems falls below usability levels. In this paper, we review two general approaches that utilize visual speech to improve ASR in acoustically challenging environments: One directly combines features extracted from the acoustic and visual channels, aiming at superior recognition performance of the resulting audio-visual ASR system. The other seeks to eliminate the noise present in the acoustic features, aiming at their audio-visual based enhancement, and thus resulting in improved speech recognition. We present a number of techniques recently introduced in the literature for bimodal ASR and enhancement, and we study their performance using a suitable audio-visual database. Among the methods considered, our recognition experiments demonstrate that decision based combination of audio and visual features significantly outperforms simpler feature based integration methods for audio-visual ASR. For audio feature enhancement, a non-linear technique is more successful than a regression-based approach. As expected, bimodal ASR and enhancement outperform their audio-only counterparts.\n",
    ""
   ]
  },
  "odisio03_avsp": {
   "authors": [
    [
     "Matthias",
     "Odisio"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Shape and appearance models of talking faces for model-based tracking",
   "original": "av03_105",
   "page_count": 6,
   "order": 19,
   "p1": "105",
   "pn": "110",
   "abstract": [
    "This paper presents a system that can recover and track the 3D speech movements of a speakers face for each image of a monocular sequence. A speaker-specific face model is used for tracking: model parameters are extracted from each image by an analysis-by-synthesis loop. To handle both the individual specificities of the speakers articulation and the complexity of the facial deformations during speech, speaker-specific models of the face geometry and appearance are built from real data. The geometric model is linearly controlled by only seven articulatory parameters. Appearance is seen either as a classical texture map or through local appearance of a relevant subset of 3D points. We compare several appearance models: they are either constant or depend linearly on the articulatory parameters. We evaluate these different appearance models with ground truth data.\n",
    ""
   ]
  },
  "guitarteperez03_avsp": {
   "authors": [
    [
     "Jesús F.",
     "Guitarte Pérez"
    ],
    [
     "Klaus",
     "Lukas"
    ],
    [
     "Alejandro F.",
     "Frangi"
    ]
   ],
   "title": "Low resource lip finding and tracking algorithm for embedded devices",
   "original": "av03_111",
   "page_count": 6,
   "order": 20,
   "p1": "111",
   "pn": "116",
   "abstract": [
    "One of the best challenges in Lip Reading is to apply this technology in embedded devices. In current solutions the high use of resources, especially in reference to visual processing, makes the implementation and integration into a small device very difficult.\n",
    "In this article a new and efficient algorithm for detection and tracking of lips is presented. Lip Finding and Tracking are customary first steps in visual processing for Lip Reading. In our approach Lips are found among a small number of blobs, which should fulfill geometric constraints. The proposed algorithm runs on an ARM920T embedded device using on average less than 4 MHz1 (2,7% of CPU load). This algorithm shows promising results in a realistic environment accomplishing successful lip finding and tracking in 94.2% of more than 4900 image frames.\n",
    ""
   ]
  },
  "yoshinaga03_avsp": {
   "authors": [
    [
     "Tomoaki",
     "Yoshinaga"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Audio-visual speech recognition using lip movement extracted from side-face images",
   "original": "av03_117",
   "page_count": 4,
   "order": 21,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "This paper proposes an audio-visual speech recognition method using lip movement extracted from side-face images to attempt to increase noise-robustness in mobile environments. Although most previous bimodal speech recognition methods use frontal face (lip) images, these methods are not easy for users since they need to hold a device with a camera in front of their face when talking. Our proposed method capturing lip movement using a small camera installed in a handset is more natural, easy and convenient. This method also effectively avoids a decrease of signal-to-noise ratio (SNR) of input speech. Visual features are extracted by optical-flow analysis and combined with audio features in the framework of HMM-based recognition. Phone HMMs are built by the multi-stream HMM technique. Experiments conducted using Japanese connected digit speech contaminated with white noise in various SNR conditions show effectiveness of the proposed method. Recognition accuracy is improved by using the visual information in all SNR conditions, and the best improvement is approximately 6% at 5dB SNR.\n",
    ""
   ]
  },
  "shdaifat03_avsp": {
   "authors": [
    [
     "I.",
     "Shdaifat"
    ],
    [
     "R.",
     "Grigat"
    ],
    [
     "Detlev",
     "Langmann"
    ]
   ],
   "title": "A System for Automatic Lip Reading",
   "original": "av03_121",
   "page_count": 6,
   "order": 22,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "In this paper, we present our approach of face and lip detection, lip modeling, and tracking. A new lip model based on B´ezier Curves is used to capture the dynamics of the lips effi- ciently. The model is defined only through few points which are modeled using the Active Shape Model (ASM). Accurate detection of lip details is implemented using multiple independent feature templates. The method detects tracks and model the lips online and robustly. The results of the lip detection are presented based on our collected data for German language. We also describe also some of the current and future work on audio video integration.\n",
    ""
   ]
  },
  "scanlon03_avsp": {
   "authors": [
    [
     "Patricia",
     "Scanlon"
    ],
    [
     "Richard",
     "Reilly"
    ],
    [
     "Philip de",
     "Chazal"
    ]
   ],
   "title": "Visual feature analysis for automatic speechreading",
   "original": "av03_127",
   "page_count": 6,
   "order": 23,
   "p1": "127",
   "pn": "132",
   "abstract": [
    "This paper proposes a novel method of visual feature extraction for  automatic speechreading. While current methods of extracting delta  or difference features involves computing the difference between  adjacent frames, this method proposed provides information on how  the visual features evolve over a time period longer than the time  period between adjacent frames, the time period being relative to  the length of the utterance. These new features provide a visual  memory capability for improved system performance. Good visual  discrimination is achieved by maintaining a base level of detail  in the features. A frame rate of 30 frames per second provides  rapid visual recognition of speech. The combination of the novel  visual memory features, good visual discrimination and rapid  visual recognition of speech movements is shown to improve visual  speech recognition. Using this method an isolated word accuracy  of 28.1% for a vocabulary 78 words over a database of 10 speakers  was achieved.\n",
    ""
   ]
  },
  "goecke03_avsp": {
   "authors": [
    [
     "Roland",
     "Goecke"
    ],
    [
     "Bruce",
     "Millar"
    ]
   ],
   "title": "Statistical analysis of the relationship between audio and video speech parameters for Australian English",
   "original": "av03_133",
   "page_count": 6,
   "order": 24,
   "p1": "133",
   "pn": "138",
   "abstract": [
    "After decades of research, automatic speech processing has become more and more viable in recent years. Audio-video speech recognition has been shown to improve the recognition rate in noise-degraded environments. However, which audio and video speech parameters to choose for an optimal system and how they are related is still an open research issue. Here we present a number of statistical analyses that aim at increasing our understanding of such audio-video relationships. In particular, we look at the canonical correlation analysis and the coinertia analysis which investigate the relationship of linear combinations of parameters. The analyses are performed on Australian English as an example.\n",
    ""
   ]
  },
  "girin03_avsp": {
   "authors": [
    [
     "Laurent",
     "Girin"
    ]
   ],
   "title": "Pure audio McGurk effect",
   "original": "av03_139",
   "page_count": 6,
   "order": 25,
   "p1": "139",
   "pn": "144",
   "abstract": [
    "In the experiments described in this paper, [aba] and [aga] speech sequences are combined in such a way that an [ada] stimuli is obtained, referring to the well-known McGurk effect. But contrary to the standard experiments, where audio [aba] and visual [aga] stimuli are combined, only audio signals are considered here, resulting in what is called a pure audio McGurk effect. The processing consists in modelling the audio signals by the classical linear prediction model and then linearly combining the [aba] and [aga] sequence LP filters that model the contribution of the vocal tract before resynthesis. The relation of the experiments with the problem of data representation and the nature of the audio-visual integration space is discussed.\n",
    ""
   ]
  },
  "sodoyer03_avsp": {
   "authors": [
    [
     "David",
     "Sodoyer"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Christian",
     "Jutten"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Further experiments on audio-visual speech source separation",
   "original": "av03_145",
   "page_count": 6,
   "order": 26,
   "p1": "145",
   "pn": "150",
   "abstract": [
    "Looking at the speakers face seems useful to better hear a speech  signal and extract it from competing sources before identification.  This might result in elaborating new speech enhancement or extraction  techniques exploiting the audio-visual coherence of speech stimuli.  In this paper, we present a set of experiments on a novel algorithm  plugging audio-visual coherence estimated by statistical tools, on  classical blind source separation algorithms. We show in the case of  additive mixtures that this algorithm performs better than classical  blind tools both when there are as many sensors as sources, and when  there are less sensors than sources. Audiovisual coherence enables  to focus on the speech source to extract. It may also be used at the  output of a classical source separation algorithm, to select the  \"best\" sensor in reference to a target source.\n",
    ""
   ]
  },
  "shi03_avsp": {
   "authors": [
    [
     "Rui P.",
     "Shi"
    ],
    [
     "Johann",
     "Adelhardt"
    ],
    [
     "Viktor",
     "Zeißler"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Carmen",
     "Frank"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Using speech and gesture to explore user states in multimodal dialogue systems",
   "original": "av03_151",
   "page_count": 6,
   "order": 27,
   "p1": "151",
   "pn": "156",
   "abstract": [
    "Modern dialogue systems should interpret the users behavior and mind in the same way as human beings do. That means in a multimodal manner, where communication is not limited to verbal utterances, as is the case for most state-of-the-art dialogue systems, several modalities are involved, e.g., speech, gesture, and facial expression. The design of a dialogue system must adapt its concept to multimodal interaction and all these different modalities have to be combined in the dialogue system. This paper describes the recognition of a users internal state of mind using a prosody classifier based on artificial neural networks combined with a discrete Hidden Markov Model (HMM) for gesture analysis. Our experiments show that both input modalities can be used to identify the users internal state. We show that an improvement of up to 70% can be achieved when fusing both modalities.\n",
    ""
   ]
  },
  "nakadai03_avsp": {
   "authors": [
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Daisuke",
     "Matsuura"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Hiroshi",
     "Tsujino"
    ]
   ],
   "title": "Improvement of three simultaneous speech recognition by using AV integration and scattering theory for humanoid",
   "original": "av03_157",
   "page_count": 6,
   "order": 28,
   "p1": "157",
   "pn": "162",
   "abstract": [
    "This paper presents improvement of recognition of three simultaneous speeches for a humanoid robot with a pair of microphones. In such situations, sound separation and automatic speech recognition (ASR) of the separated speech are difficult, because the number of simultaneous talkers exceeds that of its microphones, the signal-to-noise ratio is quite low (around -3 dB) and noise is not stable due to interfering voices. To improve recognition of three simultaneous speeches, two key ideas are introduced  acoustical modeling of robot head by scattering theory and two-layered audio-visual integration in both name and location, that is, speech and face recognition, and speech and face localization. Sound sources are separated in real-time by an active direction-pass filter (ADPF), which extracts sounds from a specified direction by using interaural phase/intensity difference estimated by scattering theory. Since features of sounds separated by ADPF vary according to the sound direction, multiple Direction- and Speaker-dependent (DS-dependent) acoustic models are used. The system integrates ASR results by using the sound direction and speaker information by face recognition as well as confidence measure of ASR results to select the best one. The resulting system shows around 10% improvement on average against recognition of three simultaneous speeches, where three talkers were located 1 meter from the humanoid and apart from each other by 0 to 90 degrees at 10-degree intervals.\n",
    ""
   ]
  },
  "heckmann03_avsp": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Kristian",
     "Kroschel"
    ]
   ],
   "title": "Effects of image distortions on audio-visual speech recognition",
   "original": "av03_163",
   "page_count": 6,
   "order": 29,
   "p1": "163",
   "pn": "168",
   "abstract": [
    "Audio-visual speech recognition leads to significant improvements compared to pure audio recognition especially when the audio signal is corrupted by noise. This has been reproduced by many researchers. Little research has been done on the behavior of audio-visual recognition with additional degradations of the video signal, however. In this article we investigate the consequences of different types of image degradations, namely white noise, a JPEG compression, and errors in the localization of the mouth region, on the audio-visual recognition process. The first question we address is how the noise in the video stream in- fluences the recognition scores. Therefore we added noise to both, the audio and video signal at different SNR levels. The second question is how the adaptation of the fusion parameter, controlling the contribution of the audio and video stream to the recognition, is affected by the additional noise in the video stream. We compare the results we obtain when we adapt the fusion parameter to the noise in the audio and video stream to those we get when it is only adapted to the noise in the audio stream and hence a clean video stream is assumed. For the second type of tests we use an automatic adaptation of the fusion parameter based on the entropy of the a-posteriori probabilities from the audio stream.\n",
    ""
   ]
  },
  "zelezny03_avsp": {
   "authors": [
    [
     "Milos",
     "Zelezný"
    ],
    [
     "Petr",
     "Císar"
    ]
   ],
   "title": "Czech audio-visual speech corpus of a car driver for in-vehicle audio-visual speech recognition",
   "original": "av03_169",
   "page_count": 5,
   "order": 30,
   "p1": "169",
   "pn": "173",
   "abstract": [
    "This paper presents the design of an audio-visual speech corpus for in-vehicle audio-visual speech recognition. Throughout the world, there exist several audio-visual speech corpora. There are also several (audio-only) speech corpora for in-vehicle recognition. So far, we have not found an audiovisual speech corpus for in-vehicle speech recognition. And, we have not found any audio-visual speech corpora for the Czech language either. Since our aim is to design an audio-visual speech recognizer for in-vehicle recognition, the first thing we had to do was to design, collect, and process the Czech invehicle audio-visual speech corpora.\n",
    "The purpose of in-vehicle speech recognition is usually its utilization for command control of car features, which does not involve driver's hands. Thus, in real deployment, it will be the driver, whose speech will be recognized. Although it is more demanding than to collect the speech of a passenger, we decided to collect the driver's speech for training purposes. This is probably not so important for audio-only speech corpus, but for our purpose we need to collect speech in real conditions, i.e. conditions that include head movements caused by the fact that the driver has to pay attention to the traffic situation.\n",
    ""
   ]
  },
  "huang03_avsp": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ]
   ],
   "title": "Improving audio-visual speech recognition with an infrared headset",
   "original": "av03_175",
   "page_count": 4,
   "order": 31,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "Visual speech is known to improve accuracy and noise robustness of automatic speech recognizers. However, almost all audio-visual ASR systems require tracking frontal facial features for visual information extraction, a computationally intensive and error-prone process. In this paper, we consider a specially designed infrared headset to capture audio-visual data, that consistently focuses on the speakers mouth region, thus eliminating the need for face tracking. We conduct small-vocabulary recognition experiments on such data, benchmarking their ASR performance against traditional frontal, fullface videos, collected both at an ideal studio-like environment and at a more challenging office domain. By using the infrared headset, we report a dramatic improvement in visual-only ASR that amounts to a relative 30% and 54% word error rate reduction, compared to the studio and  office data, respectively. Furthermore, when combining the visual modality with the acoustic signal, the resulting relative ASR gain with respect to audio-only performance is significantly higher for the infrared headset data.\n",
    ""
   ]
  },
  "leybaert03_avsp": {
   "authors": [
    [
     "Jacqueline",
     "Leybaert"
    ]
   ],
   "title": "The role of Cued Speech in language processing by deaf children : An overview",
   "original": "av03_179",
   "page_count": 8,
   "order": 32,
   "p1": "179",
   "pn": "186",
   "abstract": [
    "Cued Speech (CS) is a system of manual cues that combined to the lipread information conveys visually the traditionally-spoken languages at the phonemic level. It i s argued here that exposure to CS allows deaf children to develop accurate phonological representations which can support rhyme judgment, phonological short-term memory, use of grapheme-to-phoneme correspondences in reading and spelling. Strong differences systematically appear between those children who have been exposed early and late to CS, suggesting that early exposure induces the development of left hemisphere specialization for language processing. The question of contribution of CS to linguistic development of children fitted with cochlear implant is discussed.\n",
    ""
   ]
  },
  "theobald03_avsp": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "J. Andrew",
     "Bangham"
    ],
    [
     "Iain",
     "Matthews"
    ],
    [
     "Gavin",
     "Cawley"
    ]
   ],
   "title": "Evaluation of a talking head based on appearance models",
   "original": "av03_187",
   "page_count": 6,
   "order": 33,
   "p1": "187",
   "pn": "192",
   "abstract": [
    "In this paper we describe how 2D appearance models can be applied to the problem of creating a near-videorealistic talking head. A speech corpus of a talker uttering a set of phonetically balanced training sentences is analysed using a generative model of the human face. Segments of original parameter trajectories corresponding to the synthesis unit are extracted from a codebook, normalised, blended, concatenated and smoothed before being applied to the model to give natural, realistic animations of novel utterances. We also present some early results of subjective tests conducted to determine the realism of the synthesiser.\n",
    ""
   ]
  },
  "vignali03_avsp": {
   "authors": [
    [
     "Guillaume",
     "Vignali"
    ],
    [
     "Harold",
     "Hill"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Linking the structure and perception of 3D faces: Gender, ethnicity, and expressive posture",
   "original": "av03_193",
   "page_count": 6,
   "order": 34,
   "p1": "193",
   "pn": "198",
   "abstract": [
    "This paper reports a statistical study of human face shape whose overall goal is to identify and characterize salient components of facial structure for human perception and communicative behavior. A large database of 3D faces has been constructed and is being analyzed for differences in ethnicity, sex, and posture. For each of more than 300 faces varying in race/ethnicity (Japanese vs. Caucasian) and sex, nine postures (smiling, producing vowels, etc.) were recorded. Principal Components Analysis (PCA) and Linear Discriminant Analysis (LDA) were used to reduce the dimensionality of the data and to provide simple, yet reliable reconstruction of any face from components that correspond to the sex, ethnicity, and posture of the face. Thus, it appears that any face can be reconstructed from a small set of linear and intuitively salient components. Psychophysical tests con\u0002rmed that the shape is suf\u0002cient to estimate sex and ethnicity. Subjects were asked to judge the sex and ethnicity of (a) natural faces and (b) faces synthesized by randomly combining Principal Component coef\u0002cients within the database. Subjects successfully discriminated ethnicity and sex independently of posture, verifying that different combinations of components are required and in differing amounts. Finally, implications of these results on animation and face recognition are discussed, incorporating results of studies currently underway that examine the \"face print\" residue of the sex-ethnicity factor analysis.\n",
    ""
   ]
  },
  "frydrych03_avsp": {
   "authors": [
    [
     "Michael",
     "Frydrych"
    ],
    [
     "Jari",
     "Kätsyri"
    ],
    [
     "Martin",
     "Dobsík"
    ],
    [
     "Mikko",
     "Sams"
    ]
   ],
   "title": "Toolkit for animation of Finnish talking head",
   "original": "av03_199",
   "page_count": 6,
   "order": 35,
   "p1": "199",
   "pn": "204",
   "abstract": [
    "We have designed and implemented a toolkit for real-time animation of virtual 3D talking head, Artificial Person. Synchronized auditory and visual speech are automatically produced from input text, which can be enriched by user definable commands to perform specific gestures, as for example head nodding or facial expressions. The toolkit is extensible through external configuration files, so new actions can be quickly added. We have configured the Artificial Person to display facial expressions and phoneme articulations. Modular design of the toolkit allows to easily replace parts of the system or to detach and run them on different computers. Artificial Person can be used to produce controllable stimuli in psychophysical and neurophysiological research on audio-visual perception of speech and emotions.\n",
    ""
   ]
  },
  "siciliano03_avsp": {
   "authors": [
    [
     "Catherine",
     "Siciliano"
    ],
    [
     "Andrew",
     "Faulkner"
    ],
    [
     "Geoff",
     "Williams"
    ]
   ],
   "title": "Lipreadability of a synthetic talking face in normal hearing and hearing-impaired listeners",
   "original": "av03_205",
   "page_count": 4,
   "order": 36,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "The Synface project is developing a synthetic talking face to aid the hearing-impaired in telephone conversation. This report investigates the gain in intelligibility from the synthetic talking head when controlled by hand-annotated speech in both 12 normal hearing (NH) and 13 hearing-impaired (HI) listeners (average hearing loss 86 dB). For NH listeners, audio from everyday sentences was degraded to simulate the information losses that arise in severe-to-profound hearing impairment. For the HI group, audio was filtered to simulate telephone speech. Auditory signals were presented alone, with the synthetic face, and with a video of the original talker. Purely auditory intelligibility was low for the NH group. With the addition of the synthetic face, average intelligibility increased by 22%. The HI group had a large variation in intelligibility in the purely auditory condition. They showed a 22% improvement with the addition of the synthetic face. For both groups, intelligibility with the synthetic face was significantly lower than with the natural face. However, the improvement with the synthetic face is sufficient to be useful in everyday communication. Questionnaire responses from the HI group indicated strong interest in the Synface system.\n",
    ""
   ]
  },
  "magnocaldognetto03_avsp": {
   "authors": [
    [
     "Emanuela",
     "Magno-Caldognetto"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Graziano",
     "Tisato"
    ],
    [
     "Federica",
     "Cavicchio"
    ]
   ],
   "title": "Coproduction of speech and emotions: visual and acoustic modifications of some phonetic labial targets#",
   "original": "av03_209",
   "page_count": 6,
   "order": 37,
   "p1": "209",
   "pn": "214",
   "abstract": [
    "This paper concerns the bimodal transmission of emotive speech and describes how the expression of joy, surprise, sadness, disgust, anger, and fear, leads to visual and acoustic target modifications in some Italian phonemes. Current knowledge on the audio-visual transmission of emotive speech traditionally concerns global prosodic and intonational characteristics of speech and facial configurations. In this research we intend to integrate this approach with the analysis of the interaction between labial configurations, peculiar to each emotion, and the articulatory lip movements defined by phonetic-phonological rules, specific to the vowels and consonants /a/, /b/, /v/. Moreover, we present the correlations between articulatory data and the spectral features of the co-produced acoustic signal.\n",
    ""
   ]
  },
  "fagel03_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Caroline",
     "Clemens"
    ]
   ],
   "title": "Two articulation models for audiovisual speech synthesis - description and determination",
   "original": "av03_215",
   "page_count": 6,
   "order": 38,
   "p1": "215",
   "pn": "220",
   "abstract": [
    "The authors present two visual articulation models for speech synthesis and methods to obtain them from measured data. The visual articulation models are used to control visible articulator movements described by six motion parameters: one for the up-down movement of the lower jaw, three for the lips and two for the tongue (see section 2.1 for details). To obtain the data, a female speaker was measured with the 2Darticulograph AG100 [1] and simultaneously filmed. The first visual articulation model is a hybrid data and rule based model that selects and combines most similar viseme patterns (section 2.3.). It is retrieved more or less directly from the measurements. The second model (section 2.4.) is rule based, following the dominance principal suggested by Löfqvist [2][3]. The parameter values for the second model are derived from the first one. Both models are integrated into MASSY, the Modular Audiovisual Speech SYnthesizer [4].\n",
    "s AG 100 - Electromagnetic Articulography, http://www.articulograph.de. A. Löfqvist, Speech as Audible Gestures, in W. J. Hardcastle, A. Marchal (eds.), Speech Production and Speech Modeling, Dodrecht: Kluwer Academic Publishers, 1990. M. M. Cohen, D. W. Massaro, Modeling Co-articulation in Synthetic Visual Speech, in N. Magnenat Thalmann, D. Thalmann (eds.), Models and Techniques in Computer Animation, pp. 139-156, Tokyo: Springer- Verlag, 1993. S. Fagel, \"MASSY - a Prototypic Implementation of the Modular Audiovisual Speech SYnthesizer\", Proceedings of the 15th International Conference on Phonetic Science (to appear), Barcelona, 2003.\n",
    ""
   ]
  },
  "bevacqua03_avsp": {
   "authors": [
    [
     "Elisabetta",
     "Bevacqua"
    ],
    [
     "Cathérine",
     "Palachaud"
    ]
   ],
   "title": "Triphone-based coarticulation model",
   "original": "av03_221",
   "page_count": 6,
   "order": 39,
   "p1": "221",
   "pn": "226",
   "abstract": [
    "Our model of lip movements is based on real data (symmetric triphone 'VCV') from a speaker on which was applied passive markers. Target positions of vowels and consonants have been extracted from the data. Coarticulation is simulated by modifying the target points associated to consonants depending on the vocalic context using a logistic function. Coarticulation rules are then applied to each facial parameter to simulate muscular tension. Our model of lip movements is applied on a 3D facial model compliant with MPEG-4 standard.\n",
    ""
   ]
  },
  "attina03_avsp": {
   "authors": [
    [
     "V.",
     "Attina"
    ],
    [
     "D.",
     "Beautemps"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Matthias",
     "Odisio"
    ]
   ],
   "title": "Toward an audiovisual synthesizer for Cued Speech: Rules for CV French syllables",
   "original": "av03_227",
   "page_count": 6,
   "order": 40,
   "p1": "227",
   "pn": "232",
   "abstract": [
    "Manual Cued Speech is an effective method used to enhance speech perception for hearing-impaired people. Thanks to this system, a speaker can clarify what has been said with the help of hand gestures. Seeing manual cues associated to lip shapes allows the cue receiver to identify speech elements unambiguously. A large amount of work has been devoted to Cued Speech effectiveness in visual identification, in the access to complete phonological representations and in language acquisition or reading and writing learning. No work aimed at investigating the temporal organization of Cued Speech production, i.e. the co-articulation of Cued Speech articulators. In this framework, the present paper presents an investigation of the temporal organization of hand cue presentation in relation to lip motion and the corresponding acoustic patterns in order to specify the nature of the syllabic structure of Cued Speech. Data reveal a clear advance of the hand on the sound and lip motion. Temporal coordination rules for French Cued Speech gestures are derived and an audiovisual synthesizer generating CV sequences in Cued Speech and based on these principles is presented.\n",
    ""
   ]
  },
  "nordstrand03_avsp": {
   "authors": [
    [
     "Magnus",
     "Nordstrand"
    ],
    [
     "Gunilla",
     "Svanfeldt"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Measurements of articulatory variation and communicative signals in expressive speech",
   "original": "av03_233",
   "page_count": 5,
   "order": 41,
   "p1": "233",
   "pn": "238",
   "abstract": [
    "This paper describes a method for acquiring data for facial movement analysis and implementation in an animated talking head. We will also show preliminary data on how a number of articulatory and facial parameters for some Swedish vowels vary under the influence of expressiveness in speech and gestures. Primarily we have been concerned in expressive gestures and emotions conveying information that is intended to make the animated agent more \"human-like\" as described in the objectives of the PF-Star project.\n",
    ""
   ]
  },
  "katsyri03_avsp": {
   "authors": [
    [
     "Jari",
     "Kätsyri"
    ],
    [
     "Vasily",
     "Klucharev"
    ],
    [
     "Michael",
     "Frydrych"
    ],
    [
     "Mikko",
     "Sams"
    ]
   ],
   "title": "Identification of synthetic and natural emotional facial expressions",
   "original": "av03_239",
   "page_count": 5,
   "order": 42,
   "p1": "239",
   "pn": "243",
   "abstract": [
    "Identification of emotional expressions of a Talking Head (TH) were evaluated and compared to that of natural faces. In addition, the effect of static (pictures) and dynamic (video sequences) stimuli was studied. Natural stimuli consisted of six basic emotional expressions (anger, disgust, fear, happiness, sadness and surprise). Two expression sets were selected from both Ekman-Friesen facial affect pictures [1] and Cohn-Kanade database [2]. In addition, two new natural expression sets were recorded in our laboratory. Synthetic expressions were created by our new TH [3], both with and without facial texture. Preliminary results indicate that the TH expressions, except fear, were identified as expected. Overall level of identification of TH stimuli was below those of natural ones. Of all used stimuli, happiness was identified the best and fear the worst. Natural static and dynamic expressions were identified equally well. However, the dynamic expressions of the TH were identified significantly more accurately than the static ones.\n",
    "s Ekman, P. and Friesen, W., Pictures of Facial Affect, Consulting Psychologists Press, Palo Alto, CA, 1978. Kanade, T., Cohn, J. F., Tian, Y., \"Comprehensive Database for Facial Expression Analysis\", 4th IEEE Int. Conference on Automatic Face and Gesture Recognition Proc., 2000, p 46 - 53. Frydrych, M., Kätsyri, J., Dobsík, M., Sams, M., ``Toolkit for animation of Finnish Talking Head, AVSP 2003.\n",
    ""
   ]
  },
  "dohen03_avsp": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audiovisual perception of contrastive focus in French",
   "original": "av03_245",
   "page_count": 6,
   "order": 43,
   "p1": "245",
   "pn": "250",
   "abstract": [
    "The purpose of this study is to determine whether the visual modality  is useful for the perception of prosody. An audio-visual corpus was  recorded from a male native French speaker. The sentences had a  subject-verb-object (SVO) syntactic structure. Four contrastive  focus conditions were studied: focus on each phrase (S, V or O)  and no focus. Normal and reiterant modes were recorded. We first  measured fundamental frequency (F0), duration and intensity to  validate the corpus. Then, lip aperture and jaw opening were  extracted from the video data. The articulatory analysis enabled  us to suggest a set of possible visual cues to focus. These cues  are a) large jaw opening gestures and high opening velocities on  all the syllables of the focused phrase; b) long initial lip closure  and c) hypo-articulation (reduced jaw opening and duration) of the  following phrases. A perception test to see if subjects could  perceive focus through the visual modality alone was developed.  It showed that a) contrastive focus was well perceived visually  for reiterant speech; b) no training was necessary and c) subject  focus was slightly easier to identify than the other focus  conditions. We also found that the presence and salience of the  visual cues enhances perception.\n",
    ""
   ]
  },
  "cerrato03_avsp": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ],
    [
     "Mustapha",
     "Skhiri"
    ]
   ],
   "title": "A method for the analysis and measurement of communicative head movements in human dialogues",
   "original": "av03_251",
   "page_count": 6,
   "order": 44,
   "p1": "251",
   "pn": "256",
   "abstract": [
    "The aim of this study is twofold: explore how people use specific gestures to serve important dialogue functions and show evidence that it is possible to measure and quantify their extent. The paper focuses both on the presentation of the method used for the recording, coding and measurement of gestures and on the discussion on the obtained results.\n",
    "Gestures, mainly head movements, were selected from naturally elicited dialogues ad-hoc recorded in lab-environment. The recordings consisted both of audio-video data and data measurements obtained with a motion tracking system.\n",
    "Most of the analyzed head movements are produced to give feedback and the results show that it is possible to identify a specific pattern for a specific movement and that movements can be easily measured and their extent can be quantified. The results obtained with our method might eventually be implemented to improve the naturalness in animated talking heads.\n",
    ""
   ]
  },
  "shiller03_avsp": {
   "authors": [
    [
     "Douglas M.",
     "Shiller"
    ],
    [
     "Christian",
     "Kroos"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "K. G.",
     "Munhall"
    ]
   ],
   "title": "Exploring the spatial frequency requirements of audio-visual speech using superimposed facial motion",
   "original": "av03_257",
   "page_count": 1,
   "order": 45,
   "p1": "257",
   "pn": "",
   "abstract": [
    "While visually complex stimuli such as human faces contain information across a wide range of spatial frequencies, information related to specific perceptual judgements may be concentrated in distinct spatial frequency bands. For example, previous work on static face perception has shown that face recognition relies primarily on low spatial frequency information while other tasks, such as identifying facial expressions, may require higher spatial frequencies. An innovative approach to identifying such spatial frequency biases has been the use of hybrid visual stimuli: stimuli that involve the overlap of two distinct images, one of which has been spatially filtered to remove high spatial-frequency information (i.e., low-pass filtered) and another which has been filtered to remove low-frequency information (i.e., high-pass filtered) (Schyns and Oliva, 1999). By placing these two spatial-frequency portions of the image in direct competition with each other, the use of hybrid stimuli allows for the identification of spatial frequency bands that are preferentially processed by the visual system and not merely sufficient for the task.\n",
    "In this paper, we have used a similar technique to explore the range of spatial frequencies that are involved in the processing of audio-visual speech. We produced dynamic hybrid stimuli in which two video sequences of a talker producing different VCV utterances (such as 'aba' and 'aga') were spatially low- and highpass filtered at a number of cutoff frequencies (ranging from 2.75 to 44 cycles/face) and then combined. The resulting hybrids were presented to subjects with a single audio signal that was congruent with one of the two utterances. Thus, subjects were presented with two visual alternatives for audio-visual integration, one of which would produce the McGurk effect. In a separate condition, subjects were presented with individual lowpass or high-pass filtered video sequences on their own (i.e., half of the hybrid stimuli) with either congruent or incongruent audio.\n",
    "Results suggest that visual information sufficient for speech perception is represented across a broad range of spatial frequencies. The McGurk effect was observed for nearly all of the individual low- and high-pass filtered stimuli. The hybrid stimuli, however, revealed a clear low spatial frequency bias in the processing of visual speech information. These results will be discussed in the context of the information requirements of face-to-face\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Psycho - Neurophysiology",
   "papers": [
    "fogassi03_avsp",
    "lebib03_avsp",
    "kim03_avsp",
    "bernstein03_avsp",
    "schwartz03_avsp",
    "conrey03_avsp",
    "grant03_avsp",
    "wassenhove03_avsp",
    "sekiyama03_avsp",
    "mohammed03_avsp",
    "bergeson03_avsp",
    "cathiard03_avsp",
    "vroomen03_avsp",
    "ojanen03_avsp",
    "schwartz03b_avsp",
    "massaro03_avsp",
    "berthommier03_avsp"
   ]
  },
  {
   "title": "Analysis and Recognition",
   "papers": [
    "potamianos03_avsp",
    "odisio03_avsp",
    "guitarteperez03_avsp",
    "yoshinaga03_avsp",
    "shdaifat03_avsp",
    "scanlon03_avsp",
    "goecke03_avsp",
    "girin03_avsp",
    "sodoyer03_avsp",
    "shi03_avsp",
    "nakadai03_avsp",
    "heckmann03_avsp",
    "zelezny03_avsp",
    "huang03_avsp"
   ]
  },
  {
   "title": "Talking Faces, Gestures, and Expressions",
   "papers": [
    "leybaert03_avsp",
    "theobald03_avsp",
    "vignali03_avsp",
    "frydrych03_avsp",
    "siciliano03_avsp",
    "magnocaldognetto03_avsp",
    "fagel03_avsp",
    "bevacqua03_avsp",
    "attina03_avsp",
    "nordstrand03_avsp",
    "katsyri03_avsp",
    "dohen03_avsp",
    "cerrato03_avsp"
   ]
  },
  {
   "title": "Complementary Material",
   "papers": [
    "shiller03_avsp"
   ]
  }
 ]
}