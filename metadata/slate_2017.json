{
 "title": "7th ISCA Workshop on Speech and Language Technology in Education (SLaTE 2017)",
 "location": "Stockholm, Sweden",
 "startDate": "25/8/2017",
 "endDate": "26/8/2017",
 "URL": "http://www.slate2017.org",
 "chair": "Chairs: Olov Engwall and José Davis Lopes",
 "conf": "SLaTE",
 "year": "2017",
 "name": "slate_2017",
 "series": "SLaTE",
 "SIG": "SLaTE",
 "title1": "7th ISCA Workshop on Speech and Language Technology in Education",
 "title2": "(SLaTE 2017)",
 "date": "25-26 August 2017",
 "booklet": "slate_2017.pdf",
 "papers": {
  "caines17_slate": {
   "authors": [
    [
     "Andrew",
     "Caines"
    ]
   ],
   "title": "Spoken CALL Shared Task system description",
   "original": "14",
   "page_count": 6,
   "order": 14,
   "p1": 79,
   "pn": 84,
   "abstract": [
    "We describe the systems we applied to the Spoken CALL Shared Task text-processing track in which an 'accept' or 'reject' label had to be applied to transcribed responses to a given 'prompt' stimulus. The data come from a language tutoring system for Swiss-German learners of English. In our system we attempt to capture the grammaticality and semantic dimensions of language assessment: dimensions which are explicitly labelled in the training corpus. With the training data we achieved accuracies and differential scores well above the baseline. However, our system performed less well on the test data and we discuss ways to improve performance.\n"
   ],
   "doi": "10.21437/SLaTE.2017-14"
  },
  "pilan17_slate": {
   "authors": [
    [
     "Ildiko",
     "Pilan"
    ],
    [
     "David",
     "Alfter"
    ],
    [
     "Elena",
     "Volodina"
    ]
   ],
   "title": "Lärka: an online platform where language learning meets natural language processing",
   "original": "abs1",
   "page_count": 1,
   "order": 33,
   "p1": 187,
   "pn": 187,
   "abstract": [
    "We present Lärka, an Intelligent Computer Assisted Language Learning (ICALL) platform developed at Språkbanken. Lärka is an openly available web-based tool that builds on a variety of existing language resources such as corpora, lexical resources and language technology tools. This makes the platform flexible and a valuable source of additional learning material (e.g. via corpus-based exercises) and a support tool for both teachers and learners of Swedish.\n"
   ]
  },
  "wang17_slate": {
   "authors": [
    [
     "Chuan",
     "Wang"
    ],
    [
     "Ruobing",
     "Li"
    ],
    [
     "Hui",
     "Lin"
    ]
   ],
   "title": "Deep Context Model for Grammatical Error Correction",
   "original": "29",
   "page_count": 5,
   "order": 29,
   "p1": 167,
   "pn": 171,
   "abstract": [
    "In this paper, we propose a deep context model based on recurrent neural networks (RNN) for grammatical error correction. For a specific error type, we treat the error correction task as a classification problem where the grammatical context representation is learnt from native text data that are largely available. Compared with traditional classifier methods, our model does not require sophisticated feature engineering which usually requires linguistic knowledge and may not cover all context patterns.Experiments on CoNLL-2014 shared task show that our approach significantly outperforms the state-of-the-art classifier and machine translation approaches for grammatical error correction.\n"
   ],
   "doi": "10.21437/SLaTE.2017-29"
  },
  "duan17_slate": {
   "authors": [
    [
     "Richeng",
     "Duan"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Masatake",
     "Dantsuji"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ]
   ],
   "title": "Transfer Learning based Non-native Acoustic Modeling for Pronunciation Error Detection",
   "original": "8",
   "page_count": 5,
   "order": 8,
   "p1": 42,
   "pn": 46,
   "abstract": [
    "The scarcity of large-scale non-native corpora and human annotations are two fundamental challenges in the development of computer-assisted pronunciation training (CAPT) systems. We explored several transfer learning based methods to detect the pronunciation errors without using non-native training data. Effects were confirmed in the Mandarin Chinese pronunciation error detection of Japanese speakers. In this paper, we investigate the generality of the methods through application to an English speech data of Japanese speakers. We also evaluate on a non-native phone recognition experiment, which is necessary but challenging in advanced CAPT systems. Experimental results show that transfer learning based acoustic modeling methods can not only be ported to a new target language but also effective in a recognition task.\n"
   ],
   "doi": "10.21437/SLaTE.2017-8"
  },
  "khalifa17_slate": {
   "authors": [
    [
     "Albara",
     "Khalifa"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Measuring Effect of Repetitive Queries and Implicit Learning with Joining-in-type Robot Assisted Language Learning System",
   "original": "3",
   "page_count": 5,
   "order": 3,
   "p1": 13,
   "pn": 17,
   "abstract": [
    "Computer assisted language learning (CALL) becomes more motivating for learners and realistic through introduction of humanoid robots. A robot assisted language learning (RALL) system is expected to provide an immersive environment for a second language (L2) learner to prepare for real face-to-face human communication. We are developing a joining-in-type RALL system using two humanoid robots, one playing the role of a teacher and the other playing the role of an advanced peer learner. The interaction between the two robots and learner is designed to smoothly switch between two learning modes, that is, tutoring and implicit learning, for effective language learning. In this paper, we measured the effect of repetitive queries and implicit learning quantitatively with 37 learners divided into two groups with and without interaction for implicit learning. The experimental results showed that the repetitive queries of specific expressions consistently improved the correct use of the expressions, and the improvement was significantly greater when the peer learner robot presented an answer for implicit learning compared with when there was no assistance from the robot.\n"
   ],
   "doi": "10.21437/SLaTE.2017-3"
  },
  "rayner17_slate": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Irene",
     "Strasly"
    ],
    [
     "Nikolaos",
     "Tsourakis"
    ],
    [
     "Johanna",
     "Gerlach"
    ],
    [
     "Pierrette",
     "Bouillon"
    ]
   ],
   "title": "Menusigne: A Serious Game for Learning Sign Language Grammar",
   "original": "32",
   "page_count": 6,
   "order": 32,
   "p1": 181,
   "pn": 186,
   "abstract": [
    "We present Menusigne, a serious game designed to help beginner students learn basic sign language grammar. At the first level, the game uses a generation grammar and a signing avatar to let the student create signed utterances from menu-based patterns; at higher levels, the game presents avatar-generated or human-produced signed utterances, and the student uses the menus to indicate the meaning. The intention is to introduce the students to the principles of sign language grammar, and the game in particular emphasises the crucial role played by non-manual (non-hand) movements. We describe an initial course for teaching basic Langue de Signes Franaise (French sign language) to French students.\n"
   ],
   "doi": "10.21437/SLaTE.2017-32"
  },
  "baur17_slate": {
   "authors": [
    [
     "Claudia",
     "Baur"
    ],
    [
     "Cathy",
     "Chua"
    ],
    [
     "Johanna",
     "Gerlach"
    ],
    [
     "Manny",
     "Rayner"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Xizi",
     "Wei"
    ]
   ],
   "title": "Overview of the 2017 Spoken CALL Shared Task",
   "original": "13",
   "page_count": 8,
   "order": 13,
   "p1": 71,
   "pn": 78,
   "abstract": [
    "We present an overview of a first shared task for spoken CALL. Groups competed on a prompt-response task using English-language data collected, using an online CALL game, from Swiss German teens in their second and third years of learning English. Each item consists of a written German prompt and an audio file containing a spoken reply. The task is to accept linguistically correct responses and reject linguistically incorrect ones, to match a gold standard; scoring was performed using a metric defined as the ratio of the relative rejection rates on incorrect and correct responses. The task received twenty entries from nine different groups. We present the task itself, the results, a tentative analysis of what makes items challenging, correlations between different possible metrics, and suggestions for a continuation.\n"
   ],
   "doi": "10.21437/SLaTE.2017-13"
  },
  "yang17_slate": {
   "authors": [
    [
     "Seung Hee",
     "Yang"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Linguistic Factors Affecting Evaluation of L2 Korean Speech Proficiency",
   "original": "10",
   "page_count": 6,
   "order": 10,
   "p1": 53,
   "pn": 58,
   "abstract": [
    "Much research attention has been directed to identify how native speakers perceive non-native speakers' oral proficiency. To investigate the generalizability of previous findings, this study examined segmental, phonological, accentual, and temporal correlates of native speakers' evaluation of L2 Korean proficiency produced by learners with various levels and nationalities. Our experiment results show that proficiency ratings by native speakers significantly correlate not only with rate of speech, but also with the segmental accuracies. The influence of segmental errors has the highest correlation with the proficiency of L2 Korean speech. We further verified the validity of this finding across different L1 backgrounds. Although phonological accuracy was expected to be highly correlated with the proficiency score, it was the least influential measure. Another new finding in this study is that the role of pitch and accent has been underemphasized so far in the non-native Korean speech perception studies. This work will serve as the groundwork for the development of automatic assessment module in Korean CAPT system.\n"
   ],
   "doi": "10.21437/SLaTE.2017-10"
  },
  "laarmannquante17_slate": {
   "authors": [
    [
     "Ronja",
     "Laarmann-Quante"
    ]
   ],
   "title": "Towards a Tool for Automatic Spelling Error Analysis and Feedback Generation for Freely Written German Texts Produced by Primary School Children",
   "original": "7",
   "page_count": 6,
   "order": 7,
   "p1": 36,
   "pn": 41,
   "abstract": [
    "This paper proposes a tool for the automatic analysis of spelling errors in freely written German texts. It is based on automatic annotations of spelling errors that comprise various levels, such as linguistic properties of the target word (phonemes, syllables, morphemes) and error-related properties such as error categories which mark whether the misspelling changes the pronunciation of a word or whether the correct spelling can be derived from a related word form. These can be used to create an application that could, for example, help teachers analyze their students' orthographic skills and give feedback with little manual effort. For the future, it could also be implemented as an automatic tutoring system for children in which case the surface has to be child-oriented and should present error analysis as a kind of game. While the paper presents the capabilities of a first prototype, the concrete implementation for real-world use is open for discussion with experts on orthography instruction.\n"
   ],
   "doi": "10.21437/SLaTE.2017-7"
  },
  "ryu17_slate": {
   "authors": [
    [
     "Hyuksu",
     "Ryu"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Mispronunciation Diagnosis of L2 English at Articulatory Level Using Articulatory Goodness-Of-Pronunciation Features",
   "original": "12",
   "page_count": 6,
   "order": 12,
   "p1": 65,
   "pn": 70,
   "abstract": [
    "This paper proposes a method to provide an articulatory diagnosis of English produced by Korean learners using articulatory Goodness-Of-Pronunciation (aGOP) features, which are based on the distinctive feature theory in phonology. Previous studies on mispronunciation diagnosis have mainly dealt with pronunciation errors at phone-level. They inform learners of which phone is recognized as a diagnosis, when the corresponding segment is realized as a mispronunciation. However, to provide learners more effective corrective feedback, diagnosis had better be performed at articulatory-level, such as place and manner of articulation, rather than at phone-level. This study aims to provide automatic articulatory diagnosis using articulation-based confidence scores. At first, the speech of learners is forced-aligned and recognized to compute the GOP and aGOPs. When the forced-aligned segment is a consonant, articulatory diagnosis is conducted in three articulatory categories: voicing, place of articulation, and manner of articulation. Otherwise, diagnosis is performed in terms of rounding, height, and backness corresponding to articulatory characteristics of vowels. Experimental results show that F1 scores for voicing, place, and manner corresponding to consonants are 0.828, 0.754, and 0.781, respectively, whereas F1 scores for rounding, height, and backness corresponding to vowels are 0.843, 0.782, and 0.824, respectively. These results indicate that the proposed method yields effective articulatory diagnosis.\n"
   ],
   "doi": "10.21437/SLaTE.2017-12"
  },
  "axtmann17_slate": {
   "authors": [
    [
     "Nico",
     "Axtmann"
    ],
    [
     "Carolina",
     "Mehmet"
    ],
    [
     "Kay",
     "Berkling"
    ]
   ],
   "title": "The CSU-K Rule-Based Pipeline System for Spoken CALL Shared Task",
   "original": "15",
   "page_count": 6,
   "order": 15,
   "p1": 85,
   "pn": 90,
   "abstract": [
    "This paper presents the set-up and results regarding the Cooperative State University submitted system for the shared spoken CALL ESL task. The data was collected from Swiss teenage students using a speech-enabled online tool for English conversation practice. The tasks consisted of training data of a German text prompt with the associated audio file containing an English language response by the students. Problems therefore consisted of recognizing children's speech with foreign accent, grammatical and vocabulary problems and a number of false starts due to language learning issues. The task is to create software that will decide whether each response is appropriate (accept) or inappropriate (reject) in the context of the prompt. Results are reported through a single D-value that is computed specifically for this task. The contribution of this paper is a detailed analysis of a va- riety of changes to the baseline system and the analysis of their contribution to the overall performance. The paper reports the official result on the shared task test files but also goes beyond the originally submitted system.\n"
   ],
   "doi": "10.21437/SLaTE.2017-15"
  },
  "hommel17_slate": {
   "authors": [
    [
     "Marlisa",
     "Hommel"
    ]
   ],
   "title": "Speech perception training as a serious game in the EFL classroom",
   "original": "31",
   "page_count": 4,
   "order": 31,
   "p1": 177,
   "pn": 180,
   "abstract": [
    "Speech perception training improves listeners' perception, but it could also significantly improve production. The benefits of perception training are interesting for the English as a foreign language (EFL) classroom, where this tool is currently not being used. This study looks at students' perception and production performance on a pretest and, following perception training sessions, on a posttest and retention test. Results show that perception training improved students' perception and production in the classroom context.\n"
   ],
   "doi": "10.21437/SLaTE.2017-31"
  },
  "sabu17_slate": {
   "authors": [
    [
     "Kamini",
     "Sabu"
    ],
    [
     "Prakhar",
     "Swarup"
    ],
    [
     "Hitesh",
     "Tulsiani"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Automatic Assessment of Children's L2 Reading for Accuracy and Fluency",
   "original": "21",
   "page_count": 6,
   "order": 21,
   "p1": 121,
   "pn": 126,
   "abstract": [
    "This project targets using state-of-the-art in automatic speech recognition technology, coupled with new work in predicting the relevant prosody ratings, to build an oral reading assessment tool. A reliable automatic system can prove invaluable in helping children acquire basic reading skills apart from facilitating the monitoring of literacy programs at large scale. In the present work, we target middle-school learners of English as a second language in a rural Indian setting. We present the design and observed characteristics of our field-collected oral reading dataset to outline the research challenges faced. Recently proposed solutions to the training of robust acoustic models in the face of limited task specific data are evaluated for the prediction of the child's word decoding accuracy and for achieved word-level alignments for prosody scoring. A language model is designed to exploit the known text and observed reading errors while being flexible enough to adapt to new reading material without further training. Based on a scoring rubric proposed by a national mission on literacy assessment in India, we present an automatic system that detects reading miscues and computes fluency indicators at the sentence level which are then correlated with finerained subjective ratings by an expert.\n"
   ],
   "doi": "10.21437/SLaTE.2017-21"
  },
  "yeung17_slate": {
   "authors": [
    [
     "Gary",
     "Yeung"
    ],
    [
     "Amber",
     "Afshan"
    ],
    [
     "Kaan Ege",
     "Ozgun"
    ],
    [
     "Canton",
     "Kaewtip"
    ],
    [
     "Steven M.",
     "Lulich"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Predicting Clinical Evaluations of Children’s Speech with Limited Data Using Exemplar Word Template References",
   "original": "28",
   "page_count": 6,
   "order": 28,
   "p1": 161,
   "pn": 166,
   "abstract": [
    "The need for automated speech pathology diagnostic tools for children has increased in recent years. Such tools can help speech pathologists identify speech disorders in children at an early age. This paper introduces an approach to automated clinical evaluations of children's speech using limited data. A database of ten normally developing first-grade children administered the Goldman-Fristoe Test of Articulation, 3rd Edition (GFTA-3) was recorded. Graduate clinicians evaluated the pronunciation of the rhotic sounds by evaluating words in the GFTA-3 containing the letter r. The rhotic sounds were specifically chosen due to their late acquisition in children. Experiments were performed attempting to predict the results of the clinical evaluations. Five children, judged to have proper rhotic pronunciations, were chosen as exemplar templates for the experiment. The remaining children, used for evaluation, were aligned in time to match the five templates using dynamic time warping, and the difference between a test child's r and a template child's r was measured using the cosine distance. Multiple linear regression on the difference scores was shown to be effective at producing predictions that were well-correlated with human clinical evaluations. Several sublists of words with rhotic sounds were used to evaluate the regression, and the sublist containing words with the most mispronunciations performed best. Further discussion includes how much each individual template contributed to the regression and how consistent the clinicians were at scoring children's speech production.\n"
   ],
   "doi": "10.21437/SLaTE.2017-28"
  },
  "ahmad17_slate": {
   "authors": [
    [
     "Muneeb",
     "Ahmad"
    ],
    [
     "Omar",
     "Mubin"
    ],
    [
     "Suleman",
     "Shahid"
    ],
    [
     "Joanne",
     "Orlando"
    ]
   ],
   "title": "Emotion and Memory Model for a Robotic Tutor as a Social Partner in a Learning Environment",
   "original": "4",
   "page_count": 7,
   "order": 4,
   "p1": 18,
   "pn": 24,
   "abstract": [
    "In this paper, we present an emotion and memory model for a social robot. The model allowed the robot to create a memory account of a child's emotional events over 4 individual sessions. The robot then adapted its behaviour based on the developed memory. We tested our model through using the NAO robot. The robot was programmed to teach vocabulary to children during the popular game 'Snakes and Ladders'. We conducted a between-subject study with 24 children at a primary school to check the validity of our model. We also evaluated the impact of robot's positive, negative, and neutral emotional feedback of the NAO robot on children vocabulary learning. Three groups of children (8/group) interacted with the robot for four different times during three weeks. Our results showed that the condition where the robot displayed positive emotional responses had a significant effect on the child's learning performance as compared to the two other conditions: negative feedback and neutral feedback. In addition, our model helped children in improving their vocabulary.\n"
   ],
   "doi": "10.21437/SLaTE.2017-4"
  },
  "minematsu17_slate": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Daisuke",
     "Saito"
    ]
   ],
   "title": "New Features and Effectiveness of Suzuki-kun, the First and Only Prosodic Reading Tutor of Tokyo Japanese",
   "original": "abs2",
   "page_count": 1,
   "order": 34,
   "p1": 188,
   "pn": 188,
   "abstract": [
    "We have updated Suzuki-kun, which is the prosodic reading tutor of Japanese that we developed as one feature of OJAD (Online Japanese Accent Dictionary). The original Suzuki-kun can visualize the prosodic and hierarchical structure of any given sentences to read them aloud in Tokyo Japanese and provide a synthetic speech sample based on the visualized prosody. The added new features are 1) generation of multiple speakers' voices, 2) speaking rate control, and 3) generation of dialogue speech among these speakers. Further, experimental results of examining the effects of Suzuki-kun on the naturalness of learners' spoken Japanese are described. It was found that visualized prosody is significantly more effective than auditory prosody. As of late August in 2017, 111 tutorial workshops of OJAD will have been given in 29 countries. In demonstration, teachers' reports will also be shown as well as Suzuki-kun's new features.\n"
   ]
  },
  "godde17_slate": {
   "authors": [
    [
     "Erika",
     "Godde"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "David",
     "Escudero"
    ],
    [
     "Marie-Line",
     "Bosse"
    ],
    [
     "Maryse",
     "Bianco"
    ],
    [
     "Coriandre",
     "Vilain"
    ]
   ],
   "title": "Improving fluency of young readers: introducing a Karaoke to learn how to breathe during a Reading-while-Listening task",
   "original": "22",
   "page_count": 5,
   "order": 22,
   "p1": 127,
   "pn": 131,
   "abstract": [
    "We evaluate here an original method for implicitly providing breathing guidance to young readers during a Reading-while-Listening task. Close-shadowing is supplemented by a Karaoke system that enlightens not only the word -- as it is jointly spelled by the pre-recorded expert reader and the learner -- but also the current and next breath groups. We compare here the impact of two enlightening conditions -- i.e. word-only vs. word and breath group -- on fluency and comprehension.\n"
   ],
   "doi": "10.21437/SLaTE.2017-22"
  },
  "wang17b_slate": {
   "authors": [
    [
     "Xinhao",
     "Wang"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "Empirical Evaluation of the Communicative Effectiveness of an Automatic Speech-to-Speech Translation System",
   "original": "26",
   "page_count": 6,
   "order": 26,
   "p1": 150,
   "pn": 155,
   "abstract": [
    "In this study we evaluate the ability of a state-of-the-art speech-to-speech machine translation system, Skype Translator, to preserve the communicative effectiveness of its input speech. Specifically, we elicited Spanish spoken responses from 24 native speakers of Spanish in the following two scenarios: academic speaking (in the context of 6 questions from a standardized assessment of academic speaking proficiency) and daily communication (in the context of 6 map-based directions tasks). Skype Translator was then used to translate these Spanish responses into English, and expert human raters subsequently provided speaking proficiency scores using both holistic and analytic scoring rubrics for both the original Spanish responses and the corresponding automatically translated English versions. The results indicate that the average holistic scores for the academic speaking task decreased from 3.5 (for the original Spanish responses) to 1.9 (for the English translations) on a 4-point scale, and the average task completion scores for the directions tasks decreased from 3.4 to 2.0. While these results indicate that automated speech-to-speech machine translation does not yet fully meeting the communicative demands of these speaking tasks, they also indicate how far the technology has improved in recent years and can serve as a benchmark for tracking further improvements in the future.\n"
   ],
   "doi": "10.21437/SLaTE.2017-26"
  },
  "tejedorgarcia17_slate": {
   "authors": [
    [
     "Cristian",
     "Tejedor-García"
    ],
    [
     "David",
     "Escudero"
    ],
    [
     "César",
     "González-Ferreras"
    ],
    [
     "Enrique",
     "Cámara-Arenas"
    ],
    [
     "Valentín",
     "Cardeñoso-Payo"
    ]
   ],
   "title": "Evaluating the Efficiency of Synthetic Voice for Providing Corrective Feedback in a Pronunciation Training Tool Based on Minimal Pairs",
   "original": "5",
   "page_count": 5,
   "order": 5,
   "p1": 25,
   "pn": 29,
   "abstract": [
    "Feedback is an important concern in Computer-Assisted Pronunciation Training (CAPT), inasmuch as it bears on a system's capability to correct users' input and promote improved L2 pronunciation performance in the target language. In this paper, we test the use of synthetic voice as a corrective feedback resource. A group of students used a CAPT tool for carrying out a battery of minimal-pair discrimination-production tasks; to those who failed in production routines, the system offered the possibility of undergoing extra training by using synthetic voice as a model in a round of exposure exercises. Participants who made use of this resource significantly outperformed those who directly repeated the previously failed exercise. Results suggest that the Text-To-Speech systems offered by current operating systems (Android in our case) must be considered a relevant feedback resource in pronunciation training, especially when combined with efficient teaching methods.\n"
   ],
   "doi": "10.21437/SLaTE.2017-5"
  },
  "evanini17_slate": {
   "authors": [
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Matthew",
     "Mulholland"
    ],
    [
     "Eugene",
     "Tsuprun"
    ],
    [
     "Yao",
     "Qian"
    ]
   ],
   "title": "Using an Automated Content Scoring Engine for Spoken CALL Responses: The ETS submission for the Spoken CALL Challenge",
   "original": "17",
   "page_count": 6,
   "order": 17,
   "p1": 97,
   "pn": 102,
   "abstract": [
    "In this study we investigate the performance of an automated content scoring engine for accepting or rejecting learner responses in a spoken CALL application. Specifically, we employed a system based on word and character n-gram features in a support vector classification framework that was originally designed for scoring content in written texts and augmented its feature set with additional features from the following categories: prompt bias, text-to-text similarity to reference responses, and automatically detected grammatical errors. This system achieved a D score of 4.353 (compared to a baseline score of 1.694) on the test set consisting of Kaldi ASR output in the 2017 Spoken CALL Challenge. In this paper we also provide an analysis of the impact of the size and nature of the training data set (human transcriptions vs. ASR output) on the model's performance and present the results of feature ablation experiments to demonstrate which of the additional features are most helpful.\n"
   ],
   "doi": "10.21437/SLaTE.2017-17"
  },
  "wang17c_slate": {
   "authors": [
    [
     "Xinhao",
     "Wang"
    ],
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Klaus",
     "Zechner"
    ],
    [
     "Matthew",
     "Mulholland"
    ]
   ],
   "title": "Modeling Discourse Coherence for the Automated Scoring of Spontaneous Spoken Responses",
   "original": "23",
   "page_count": 6,
   "order": 23,
   "p1": 132,
   "pn": 137,
   "abstract": [
    "This study describes an approach for modeling the discourse coherence of spontaneous spoken responses in the context of automated assessment of non-native speech. Although the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spontaneous spoken language, little prior research has been done to assess a speaker's coherence in the context of automated speech scoring. To address this, we first present a corpus of spoken responses drawn from an assessment of English proficiency that has been annotated for discourse coherence. When adding these discourse annotations as features to an automated speech scoring system, the accuracy in predicting human proficiency scores is improved by 7.8% relative, thus demonstrating the effectiveness of including coherence information in the task of automated scoring of spontaneous speech. We further investigate the use of two different sets of features to automatically model the coherence quality of spontaneous speech, including a set of features originally designed to measure text complexity and a set of surface-based features describing the speaker's use of nouns, pronouns, conjunctions, and discourse connectives in the spoken response. Additional experiments demonstrate that an automated speech scoring system can benefit from coherence scores that are generated automatically using these feature sets.\n"
   ],
   "doi": "10.21437/SLaTE.2017-23"
  },
  "sloan17_slate": {
   "authors": [
    [
     "John",
     "Sloan"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Was it something I said? Facial Expressions in Language Learning",
   "original": "1",
   "page_count": 6,
   "order": 1,
   "p1": 1,
   "pn": 6,
   "abstract": [
    "This paper describes an experiment to evaluate facial expressions of an animated avatar as a means of providing feedback to non-native English learners on language production. The aim of the study was to ascertain whether native and non-native English speakers interpret and respond differently to facial expressions and whether such expressions have a role to play as feedback in emerging language learning technologies. Native English speakers and non-native English learners took part in an experiment where facial expressions were presented as a response to their textual input sentences and were asked for their interpretation of the change in expression (or otherwise). Furthermore, it was investigated to what extent non-native learners subsequently altered their language behaviour in line with their perceived interpretation of the expression. The majority of non-native learners attributed a change in facial expression, where the avatar looked away, to errors in language production in the preceding sentence and they reduced the syntactic complexity of the following sentence accordingly. The results underpin the potential of facial expressions as a feedback mechanism for language learning and the insights will now be deployed in an effective and engaging personalised e-learning language platform.\n"
   ],
   "doi": "10.21437/SLaTE.2017-1"
  },
  "yue17_slate": {
   "authors": [
    [
     "Junwei",
     "Yue"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Yutaka",
     "Yamauchi"
    ],
    [
     "Kayoko",
     "Ito"
    ]
   ],
   "title": "Development and Maintenance of Practical and In-service Systems for Recording Shadowing Utterances and Their Assessment",
   "original": "abs3",
   "page_count": 1,
   "order": 35,
   "p1": 189,
   "pn": 189,
   "abstract": [
    "We demonstrate two systems, network-based and standalone, for collecting shadowing utterances and their automatic assessment. Since June 2016, these systems have been used in real English classes at several universities in Japan. The following features are highlighted: 1) To avoid pop noises from a speaker and reduce babble noises from other surrounding students, an ear-hook microphone is used with a USB audio device, 2) A network-based system and a standalone system were developed separately because network traffic not rarely causes technical errors when recording, 3) For beginners, easy-to-understand illustrations are prepared for them to get accustomed rapidly to shadowing practices, 4) DNN-based GOP calculation is run and its score is fed back to learners, and 5) To motivate them, the GOP score distribution over the learners is also fed back for them to compare their own scores with others'. In demonstration, each feature will be exhibited and explained in detail.\n"
   ]
  },
  "lopes17_slate": {
   "authors": [
    [
     "José",
     "Lopes"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "A First Visit to the Robot Language Café",
   "original": "2",
   "page_count": 6,
   "order": 2,
   "p1": 7,
   "pn": 12,
   "abstract": [
    "We present an exploratory study on using a social robot in a conversational setting to practice a second language. The practice is carried out within a so called language café, with two second language learners and one native moderator a human or a robot engaging in a social conversation of a small talk type. We compare the interactions with the human and robot moderators and perform a qualitative analysis of the potentials of a social robot as a conversational partner for language learning. Interactions with the robot are carried out in a wizard-of-Oz setting, in which the native moderator who leads the corresponding human moderator session controls the robot. The observations of the video recorded sessions and the subject questionnaires suggest that the appropriate learner level for this type of practice is elementary (A1 to A2), for whom the structured, slightly repetetive interaction pattern was perceived as beneficial. We identify both some key features that are appreciated by the learners and technological parts that need further development.\n"
   ],
   "doi": "10.21437/SLaTE.2017-2"
  },
  "gilmartin17_slate": {
   "authors": [
    [
     "Emer",
     "Gilmartin"
    ],
    [
     "Jaebok",
     "Kim"
    ],
    [
     "Alpha",
     "Diallo"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Neasa Ni",
     "Chiarain"
    ],
    [
     "Ketong",
     "Su"
    ],
    [
     "Yuyun",
     "Huang"
    ],
    [
     "Benjamin",
     "Cowan"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "CARAMILLA - Speech Mediated Language Learning Modules for Refugee and High School Learners of English and Irish",
   "original": "24",
   "page_count": 6,
   "order": 24,
   "p1": 138,
   "pn": 143,
   "abstract": [
    "In the development of Computer-Assisted Language Learning (CALL) modules there is a growing emphasis on the use of evolving technological developments including text-to-speech technology (TTS) and automatic speech recognition (ASR). These technologies allow for greater spoken interaction between the learner and the computer, which is particularly useful when native speaker input is not readily available and to elicit learner speech in a quasi-natural environment. They also allow for greater learner autonomy, and facilitate the creation of modules based on effective learning activities at all levels to address needs for accuracy as well as fluency. The present paper reports on two modules which integrate speech technology, (1) a spoken dictogloss, or text reconstruction, module and (2) a pronunciation module. These modules have been implemented in JAVA as part of CARAMILLA, a spoken dialogue-based language learning tool, which is based on an earlier prototype, MILLA. The dictogloss module is being evaluated with two distinct learner groups -- adult refugees learning English while living in the host country and learners of Irish, a minority endangered language. The pronunciation module is also being evaluated with the adult refugee group.\n"
   ],
   "doi": "10.21437/SLaTE.2017-24"
  },
  "malinin17_slate": {
   "authors": [
    [
     "Andrey",
     "Malinin"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Yu",
     "Wang"
    ],
    [
     "Mark",
     "Gales"
    ]
   ],
   "title": "An attention based model for off-topic spontaneous spoken response detection: An Initial Study",
   "original": "25",
   "page_count": 6,
   "order": 25,
   "p1": 144,
   "pn": 149,
   "abstract": [
    "Automatic spoken language assessment systems are gaining popularity due to the rising demand for English second language learning. Current systems primarily assess fluency and pronunciation, rather than semantic content and relevance of a candidate's response to a prompt. However, to increase reliability and robustness, relevance assessment and off-topic response detection are desirable, particularly for spontaneous spoken responses to open-ended prompts. Previously proposed approaches usually require prompt-response pairs for all prompts. This limits flexibility as example responses are required whenever a new test prompt is introduced. This paper presents a initial study of an attention based neural model which assesses the relevance of prompt-response pairs without the need to see them in training. This model uses a bidirectional Recurrent Neural Network (BiRNN) embedding of the prompt to compute attention over the hidden states of a BiRNN embedding of the response. The resulting fixed-length embedding is fed into a binary classifier to predict relevance of the response. Due to a lack of off-topic responses, negative examples for both training and evaluation are created by randomly shuffling prompts and responses. On spontaneous spoken data this system is able to assess relevance to both seen and unseen prompts.\n"
   ],
   "doi": "10.21437/SLaTE.2017-25"
  },
  "artuso17_slate": {
   "authors": [
    [
     "Stefano",
     "Artuso"
    ],
    [
     "Luca",
     "Cristoforetti"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Nadia",
     "Mana"
    ],
    [
     "Gianluca",
     "Schiavo"
    ]
   ],
   "title": "A System for Asessing Children Readings as School",
   "original": "20",
   "page_count": 6,
   "order": 20,
   "p1": 115,
   "pn": 120,
   "abstract": [
    "In this paper we describe a system for analyzing the reading errors made by children of the primary school. To assess the reading skills of children in terms of reading accuracy and speed, a standard reading achievement test, defined by educational psychologists and named \"Prove MT\" (MT reading test), is used in all Italian primary schools. This test is based on a set of texts specific for different ages, from 7 to 10 years. At present during the test children are asked to read aloud short stories, while teachers manually write down the reading errors on a sheet and then compute a total score based on several measures, such as duration of the whole reading, number of read syllables per second, number and type of errors, etc. The system we have developed is aimed to support the teachers in this task by automatically detecting the reading errors and estimating the needed measures. To do this we use an automatic speech-to-text transcription system that employs a a language model trained over the texts containing the stories to read. In addition, we embed in the language model an error model that allows to take into account typical reading errors, mostly consisting in pronunciation errors, substitutions of syllables or words, word truncation, etc. To evaluate the performance of our system we collected 55 audio recordings, uttered by 8-13 years old children reading 4 different stories.\n"
   ],
   "doi": "10.21437/SLaTE.2017-20"
  },
  "rouhe17_slate": {
   "authors": [
    [
     "Aku",
     "Rouhe"
    ],
    [
     "Reima",
     "Karhila"
    ],
    [
     "Heini",
     "Kallio"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "A pipeline for automatic assessment of foreign language pronunciation",
   "original": "abs4",
   "page_count": 1,
   "order": 36,
   "p1": 190,
   "pn": 190,
   "abstract": [
    "We illustrate our prototype pipeline for automatic analysis and assessment of foreign language speech. It uses automatic speech recognition as a preprocessing step for phonetic analysis and predicts the grade that human experts would give for the utterance. The work is applied in two educational research projects: DigiTala for L2 Swedish for Finnish-speaking high school students, and SIAK ”Say it again, kid!” for L2 English for Finnish-speaking 6-9 year old children.\n"
   ]
  },
  "qian17_slate": {
   "authors": [
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Xizi",
     "Wei"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Martin",
     "Russell"
    ]
   ],
   "title": "The University of Birmingham 2017 SLaTE CALL Shared Task Systems",
   "original": "16",
   "page_count": 6,
   "order": 16,
   "p1": 91,
   "pn": 96,
   "abstract": [
    "This paper describes the system developed by the University of Birmingham for the SLaTE CALL Shared Task on grammatical and linguistic assessment of English spoken by German-speaking Swiss teenagers. Our work focused on automatic speech recognition (ASR) but we also improved the text-processing component of the system. Several approaches to training a DNN-HMM ASR system using the AMI and the German PF-STAR corpus, plus a limited amount of Shared Task data, are described. In cross-validation evaluations on the initial Shared Task data, our final ASR system achieved a word-error-rate (WER) of 9.27%, compared with 14% for the official baseline Shared Task DNN-HMM system. For text processing we expanded the baseline template-based grammar to include additional correct response patterns from the original Shared Task transcriptions. Finally, we fused the outputs of several systems at the text processing stage using linear logistic regression. Our best single and fused systems submitted to the challenge achieved scores of 4.71 and 4.766, respectively, on the final test set.\n"
   ],
   "doi": "10.21437/SLaTE.2017-16"
  },
  "oh17_slate": {
   "authors": [
    [
     "Yoo Rhee",
     "Oh"
    ],
    [
     "Hyung-Bae",
     "Jeon"
    ],
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Byung Ok",
     "Kang"
    ],
    [
     "Yun-Kyung",
     "Lee"
    ],
    [
     "Jeon-Gue",
     "Park"
    ],
    [
     "Yun-Keun",
     "Lee"
    ]
   ],
   "title": "Deep-Learning Based Automatic Spontaneous Speech Assessment in a Data-Driven Approach for the 2017 SLaTE CALL Shared Challenge",
   "original": "18",
   "page_count": 6,
   "order": 18,
   "p1": 103,
   "pn": 108,
   "abstract": [
    "This paper presents a deep-learning based assessment method of a spoken computer-assisted language learning (CALL) for a non-native child speaker, which is performed in a data-driven approach rather than in a rule-based approach. Especially, we focus on the spoken CALL assessment of the 2017 SLaTE challenge. To this end, the proposed method consists of four main steps: speech recognition, meaning feature extraction, grammar feature extraction, and deep-learning based assessment. At first, speech recognition is performed on an input speech by using three automatic speech recognition (ASR) systems. Second, twenty-seven meaning features are extracted from the recognized texts via the three ASRs by using language models (LMs), sentence-embedding models, and word-embedding models. Third, twenty-two grammar features are extracted from the recognized text via one ASR system by using linear-order LMs and hierarchical-order LMs. Fourth, the extracted forty-nine features are fed into a full-connected deep neural network (DNN) based model for the classification of acceptance or rejection. Finally, an assessment is performed by comparing the probability of a output unit of the DNN-based classifier with a pre-defined threshold. For the experiments of a spoken CALL assessment, we use English spoken utterances by Swiss German teenagers. It is shown from the experiments that the D score is 4.37 for the spoken CALL assessment system employing the proposed method.\n"
   ],
   "doi": "10.21437/SLaTE.2017-18"
  },
  "magooda17_slate": {
   "authors": [
    [
     "Ahmed",
     "Magooda"
    ],
    [
     "Diane",
     "Litman"
    ]
   ],
   "title": "Syntactic and semantic features for human like judgement in spoken CALL",
   "original": "19",
   "page_count": 6,
   "order": 19,
   "p1": 109,
   "pn": 114,
   "abstract": [
    "Educational applications of Natural Language Processing (NLP) and Automatic Speech Recognition (ASR) have included providing learners with helpful and accurate feedback. In this paper we present a system that takes a first step towards providing feedback during spoken Computer-Assisted Language Learning (spokenCALL). We propose a machine learning based approach that combines syntactic and semantic features in order to accept or reject a textual response given a provided prompt. Our approach was evaluated as part of the SpokenCALL shared task, ranking third place among the submitted systems and outperforming the provided baselines.\n"
   ],
   "doi": "10.21437/SLaTE.2017-19"
  },
  "chiarain17_slate": {
   "authors": [
    [
     "Neasa Ní",
     "Chiaráin"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ]
   ],
   "title": "Effects of Educational Context on Learners’ Ratings of a Synthetic Voice",
   "original": "9",
   "page_count": 6,
   "order": 9,
   "p1": 47,
   "pn": 52,
   "abstract": [
    "Studies suggest that the context in which synthetic voices are evaluated has a significant bearing on the evaluators' assessments of the voices. The present study is based on learners' evaluations of an Irish synthetic voice which is used in a Computer-Assisted Language Learning (CALL) context. Previous studies have allowed a number of extraneous variables, such as different voices being used or pitch or speech rate manipulations, to enter the study. The present study involves a single synthetic Irish HTS voice being evaluated in three different contexts, one of which is highly interactive, a second less so and a third which does not require the evaluator to perform any specific task. Results show that the voice was rated more favourably when presented in a highly interactive context. This finding has significant implications for the way we interpret the results of evaluations of synthetic voices.\n"
   ],
   "doi": "10.21437/SLaTE.2017-9"
  },
  "kyriakopoulos17_slate": {
   "authors": [
    [
     "Konstantinos",
     "Kyriakopoulos"
    ],
    [
     "Mark",
     "Gales"
    ],
    [
     "Kate",
     "Knill"
    ]
   ],
   "title": "Automatic Characterisation of the Pronunciation of Non-native English Speakers using Phone Distance Features",
   "original": "11",
   "page_count": 6,
   "order": 11,
   "p1": 59,
   "pn": 64,
   "abstract": [
    "The distances between and relative movements of phones in acoustic space in language learners have been shown to be indicative of the speaker's proficiency, in a way that is compact and independent of bias-inducing voice qualities. Typically these features are based on known transcriptions, \"read aloud\" style tasks. This paper examines the information that can be extracted about speakers from phone distance features (PDFs) when the transcription is unknown. Here, phone distances are obtained by measuring the relative entropy between a distribution trained on the speaker's manner of pronunciation of each of the phones of the English language and distributions trained on each of the other phones. These features are extracted from untranscribed audio and so rely on automatic speech recognition (ASR) output. The ASR can have high word error rates, as spontaneous, non-native speech is being recognised. Two forms of speaker characterisation are examined using these features: first, the use of PDFs to predict the speaker's proficiency and second, their use in classifying the mother tongue (L1) of the speaker. For both tasks, recorded answers to sections of the BULATS English Speaking test were used. Using only PDFs for predicting the grade within a Gaussian Process based grader showed performance comparable to using a range of standard fluency style features. This indicates the robustness of PDFs to errors in the ASR output. Additionally, the same PDF features can detect with high accuracy the L1 of the speakers from among 21 L1s using a deep neural network based classifier. Experiments on South American Spanish show that it is further possible to discriminate between the speakers' countries of origin.\n"
   ],
   "doi": "10.21437/SLaTE.2017-11"
  },
  "yamauchi17_slate": {
   "authors": [
    [
     "Yutaka",
     "Yamauchi"
    ],
    [
     "Junwei",
     "Yue"
    ],
    [
     "Kayoko",
     "Ito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Investigation of teacher-selected sentences and machine-suggested sentences in terms of correlation between human ratings and GOP-based machine scores",
   "original": "6",
   "page_count": 6,
   "order": 6,
   "p1": 30,
   "pn": 35,
   "abstract": [
    "This study investigated relationships between teacher-selected stimulus sentences and machine-suggested ones in terms of the correlation between human ratings and GOP-based machine scores. In assessing shadowed speeches consisting of 55 sentences recorded by 125 Japanese learners of English, it was examined which sentence combinations out of the 55 sentences could maximize the correlation between automatic scores and human ratings. A veteran teacher selected 10 sentences based on criteria such as sentence length, grammar, pronunciation, and prosodic features. The shadowed speeches of the 10 sentences were manually rated by two native speakers of English focusing on pronunciation, prosody and lexical access (whether shadowing is done adequately or not after each word or phrase is identified). The same shadowed utterances were automatically assessed by the DNN-based GOP procedure. A significantly high correlation (r=0.738, p<.01) was found between manual ratings and automatic scores. Then three sentences were selected out of the original 55 sentences by greedy search so that correlation between their DNN-GOP scores and manual ratings of the selected 10 sentences could be maximized, and the top-ranked combinations of three sentences were listed up. The number of shared sentences between teacher-selected sentences and machine-suggested ones was much smaller than expected, and thus the teacher's strategies in selecting stimulus sentences turned out to be inappropriate in terms of maximizing correlation. Examining the sentence sets suggested by the machine revealed that some particular sentences frequently appeared in the sets and seemed to have something to do with maximizing correlation. Hence, the present study compared the 10 sentences selected by the teacher and his selection criteria with the sentence sets suggested by the machine and discussed what kind of sentence should be chosen to improve reliability in automatic assessment.\n"
   ],
   "doi": "10.21437/SLaTE.2017-6"
  },
  "mirzaei17_slate": {
   "authors": [
    [
     "Maryam Sadat",
     "Mirzaei"
    ],
    [
     "Kourosh",
     "Meshgi"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Detecting listening difficulty for second language learners using Automatic Speech Recognition errors",
   "original": "27",
   "page_count": 5,
   "order": 27,
   "p1": 156,
   "pn": 160,
   "abstract": [
    "This paper introduces a new approach to detect difficulties in speech for the second language (L2) listeners using automatic speech recognition (ASR) systems. In this study, the ASR systems are viewed as a model to predict L2 learners' listening difficulties and the ASR erroneous cases were analyzed to find useful categories of errors that can epitomize language learners' transcription mistakes. Annotation of the errors revealed the usefulness of several categories of ASR errors in predicting learners' listening difficulties when watching TED videos. Experiments with L2 learners confirmed that these categories lead to listening problems for a majority of the learners. One application to make use of these errors can be found in partial and synchronized caption (PSC), in which only difficult words are selected and shown to facilitate listening. Findings of the experiments attest that adding these errors to PSC improves learners' comprehension.\n"
   ],
   "doi": "10.21437/SLaTE.2017-27"
  },
  "bernstein17_slate": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Jian",
     "Cheng"
    ],
    [
     "Jennifer",
     "Balogh"
    ],
    [
     "Elizabeth",
     "Rosenfeld"
    ]
   ],
   "title": "Studies of a Self-Administered Oral Reading Assessment",
   "original": "30",
   "page_count": 5,
   "order": 30,
   "p1": 172,
   "pn": 176,
   "abstract": [
    "Reading assessments are most useful when they inform instruction. We present a mobile-device app, Moby.Read, that presents a short self-administered test of oral reading fluency. Children read text passages aloud. Spoken responses are scored automatically on the device and displayed for teacher review. First, we report a usability study and a preliminary validation study with 99 children. Usability results are positive and score accuracy is high. Accurate automatic reading assessment enables new analyses of reading performance (e.g. rate trends over time and passage) that have not previously been available to guide individual instruction. We present early results of sub-passage and cross-passage results that hold diagnostic promise and discuss their potential to guide reading instruction.\n"
   ],
   "doi": "10.21437/SLaTE.2017-30"
  }
 },
 "sessions": [
  {
   "title": "Robot-assisted language learning",
   "papers": [
    "sloan17_slate",
    "lopes17_slate",
    "khalifa17_slate",
    "ahmad17_slate"
   ]
  },
  {
   "title": "Poster session 1",
   "papers": [
    "tejedorgarcia17_slate",
    "yamauchi17_slate",
    "laarmannquante17_slate",
    "duan17_slate",
    "chiarain17_slate"
   ]
  },
  {
   "title": "Pronunciation assessment",
   "papers": [
    "yang17_slate",
    "kyriakopoulos17_slate",
    "ryu17_slate"
   ]
  },
  {
   "title": "CALL shared task",
   "papers": [
    "baur17_slate",
    "caines17_slate",
    "axtmann17_slate",
    "qian17_slate",
    "evanini17_slate",
    "oh17_slate",
    "magooda17_slate"
   ]
  },
  {
   "title": "Reading assessment",
   "papers": [
    "artuso17_slate",
    "sabu17_slate",
    "godde17_slate"
   ]
  },
  {
   "title": "Discourse and spontaneous speech",
   "papers": [
    "wang17c_slate",
    "gilmartin17_slate",
    "malinin17_slate"
   ]
  },
  {
   "title": "Poster session 2",
   "papers": [
    "wang17b_slate",
    "mirzaei17_slate",
    "yeung17_slate",
    "wang17_slate",
    "bernstein17_slate"
   ]
  },
  {
   "title": "Serious games",
   "papers": [
    "hommel17_slate",
    "rayner17_slate"
   ]
  },
  {
   "title": "Demo papers",
   "papers": [
    "pilan17_slate",
    "minematsu17_slate",
    "yue17_slate",
    "rouhe17_slate"
   ]
  }
 ],
 "doi": "10.21437/SLaTE.2017"
}