{
 "series": "SIGUL",
 "title": "2nd Annual Meeting of the ELRA/ISCA SIG on Under-resourced Languages (SIGUL 2023)",
 "location": "Dublin, Ireland",
 "startDate": "18/08/2023",
 "endDate": "20/08/2023",
 "URL": "https://sigul-2023.ilc.cnr.it",
 "chair": "Chairs: Sakriani Sakti, Claudia Soria and Maite Melero",
 "intro": "intro.pdf",
 "conf": "SIGUL",
 "year": "2023",
 "name": "sigul_2023",
 "SIG": "SIGUL",
 "title1": "2nd Annual Meeting of the ELRA/ISCA SIG on Under-resourced Languages",
 "title2": "(SIGUL 2023)",
 "booklet": "sigul_2023.pdf",
 "date": "18-20 August 2023",
 "papers": {
  "panigrahi23_sigul": {
   "authors": [
    [
     "Subhashish",
     "Panigrahi"
    ]
   ],
   "title": "Reclaiming Our Voices: Imagining Community-led AI/ML Practices",
   "original": "Panigrahi",
   "page_count": 3,
   "order": 1,
   "p1": 1,
   "pn": 3,
   "abstract": [
    "This paper critically examines technology's role in preserving endangered languages and its relationship with social justice. The concepts of \"Slow Language Models\" and \"Small Large-Language Models\" are introduced, emphasizing community-led language preservation efforts. The paper highlights the limitations of current large language models, focusing on their text-centric bias and issues of transparency and representation in AI development. The Santali community serves as a case study, illustrating successful community-driven language preservation in the digital age. The research advocates for \"Slow Language Models\" and \"Small Large-Language Models\" as collaborative approaches, challenging the rush for technological solutions and calling for ethical, community-centered practices in AI and machine learning development."
   ],
   "doi": "10.21437/SIGUL.2023-1"
  },
  "cristea23_sigul": {
   "authors": [
    [
     "Dan",
     "Cristea"
    ],
    [
     "Petru",
     "Rebeja"
    ]
   ],
   "title": "Short-Cutting Manual Acquisition in Deep-Learning Deciphering of Old Documents ",
   "original": "1_Cristea",
   "page_count": 5,
   "order": 2,
   "p1": 4,
   "pn": 8,
   "abstract": [
    "We present an approach to ease the effort of acquiring annotation data intended to train a technology for automatic transcription and transliteration of old documents. The research is motivated by the interest to decipher in the Latin script Romanian documents written in Cyrillic along the centuries XVIth-XIXth. The whole enterprise is briefly described, then the attention concentrates on an alignment algorithm that reuses manual transcripts for the benefit of automatically acquiring training data to enhance neural network models. The approach is easily reproducible for other languages."
   ],
   "doi": "10.21437/SIGUL.2023-2"
  },
  "alam23_sigul": {
   "authors": [
    [
     "Meesum",
     "Alam"
    ],
    [
     "Alexandra",
     "O’Neil"
    ],
    [
     "Daniel",
     "Swanson"
    ],
    [
     "Francis",
     "Tyers"
    ]
   ],
   "title": "A Finite-State Morphological Analyzer for Saraiki ",
   "original": "2_Alam",
   "page_count": 5,
   "order": 3,
   "p1": 9,
   "pn": 13,
   "abstract": [
    "Saraiki (also Sirariki) (skr) is the first language of almost 25 million speakers in Pakistan and nearly one million speakers in India. Our study documents the process of creating an Apertium module for Saraiki and contributes to future efforts to generate computational resources for Saraiki. Apertium is chosen for the development of a Saraiki morphological analyzer since the platform has shown to adequately handle morphological complexity. In discussing the process of creating an analyzer for Saraiki, we detail our implementation by discussing our treatment of Saraiki morphology in regard to gender, number, and case marking for nouns and adjectives, verb categorizations (basic stem forms, direct causatives, and indirect causatives), and cases of ambiguity in nominal gender inflections."
   ],
   "doi": "10.21437/SIGUL.2023-3"
  },
  "kumar23_sigul": {
   "authors": [
    [
     "Ritesh",
     "Kumar"
    ],
    [
     "Meiraba",
     "Takhellambam"
    ],
    [
     "Bornini",
     "Lahiri"
    ],
    [
     "Amalesh",
     "Gope"
    ],
    [
     "Shyam",
     "Ratan"
    ],
    [
     "Neerav",
     "Mathur"
    ],
    [
     "Siddharth",
     "Singh"
    ]
   ],
   "title": "Collecting Speech Data for Endangered and Under-resourced Indian Languages",
   "original": "3_Kumar",
   "page_count": 5,
   "order": 4,
   "p1": 14,
   "pn": 18,
   "abstract": [
    "The preparation of speech corpora for languages un(der)represented on the web largely depends on the manual methods of data collection and processing from different sources. The methods used in field linguistics and documentary linguistics for collecting data from the speech communities provide a valuable set of resources and methodologies for such data collection but these methods were not developed and optimised for large-scale data collection. However, this limitation could be overcome by combining linguistic field methods with crowdsourcing for data collection. In this paper, we discuss two such ongoing projects - SpeeD-TB and SpeeD-IA - in which we are experimenting with different methods and developing software and other infrastructure to rapidly collect speech data in six Tibeto-Burman - Toto, Chokri, Nyishi, Kok Borok, Bodo and Meitei - and four Indo-Aryan - Awadhi, Bhojpuri, Braj and Magahi - languages in India. Till now we have collected over 40 hours of speech data in these languages and over the period of the next year, we plan to collect a total of approximately 1,200 hours of speech data."
   ],
   "doi": "10.21437/SIGUL.2023-4"
  },
  "welby23_sigul": {
   "authors": [
    [
     "Pauline",
     "Welby"
    ],
    [
     "Brigitte",
     "Bigi"
    ],
    [
     "Antoine",
     "Corral"
    ],
    [
     "Fabrice",
     "Wacalie"
    ],
    [
     "Guillaume",
     "Wattelez"
    ]
   ],
   "title": "A Visit to the Cliffs of Jokin: A Role for Phonetizers in Second Language Pronunciation and Word Learning, with an Example from the Languages of New Caledonia ",
   "original": "4_Welby",
   "page_count": 5,
   "order": 5,
   "p1": 19,
   "pn": 23,
   "abstract": [
    "In this position paper, we argue for a role for overt phonetizers in second language learning. Phonetization or letter-to-sound conversion is often used simply as a module of text-to-speech synthesis (TTS) or to create pronunciations for dictionaries. Based on evidence of the overwhelming influence of orthographic input on second language pronunciation and word learning, we argue that on their own (or coupled with TTS), phonetizers can be effective support tools for two broad groups: 1.language learners and instructors, and 2.non-specialized users. We address the issues involved and give the example of a multilingual phonetizer under development in New Caledonia, a special status collectivity of France in the South Pacific. Encountering words and names in one of the almost 30 languages of the indigenous Kanak people of New Caledonia is an everyday experience, for example, on class lists, road signs and in news articles. Pronouncing these words is often a challenge, since each of the languages has its own phonology and its own orthography. We discuss the motivation behind the phonetizer, challenges in its development, and potential applications, many of which are common to other endangered or vulnerable and under-resourced languages."
   ],
   "doi": "10.21437/SIGUL.2023-5"
  },
  "vegarodriguez23_sigul": {
   "authors": [
    [
     "Jenifer",
     "Vega Rodriguez"
    ],
    [
     "Nathalie",
     "Vallée"
    ],
    [
     "Thiago",
     "Chacon"
    ],
    [
     "Christophe",
     "Savariaux"
    ],
    [
     "Silvain",
     "Gerber"
    ]
   ],
   "title": "An Intra- and Inter-Dialectal Study of Korebaju Vowels",
   "original": "5_Vega Rodriguez",
   "page_count": 5,
   "order": 6,
   "p1": 24,
   "pn": 28,
   "abstract": [
    "Korebaju (ISO 639-3 coe) [́kóˀrèbàjɨ] is an endangered ́ Tukanoan language spoken in the foothills of the Colombian Amazon. Two fieldworks carried out between 2021 and 2023 on a sample of 24 native speakers (12 females and 12 males) from two different varieties: Tama and Korebaju, located in two different villages, provide new data for improving intra- and inter-dialectal phonetic-phonological description of Korebaju. This acoustic study focuses on the vowel system of each of these two varieties and is the first part of an ongoing project on vowel nasalization and glottalization in Korebaju. The acoustic and statistical analyses indicate that there are no significant interdialectal differences between vowels. However, differences between generations of the same sex and of the same variety have been evidenced in our analyses. These results also suggest that Korebaju speakers' perception of stronger glottalization in the Tama variant refers to a morphological distinction. However, this hypothesis is still being analyzed considering the language’s tonal system, nasalization, and morphology."
   ],
   "doi": "10.21437/SIGUL.2023-6"
  },
  "guillaume23_sigul": {
   "authors": [
    [
     "Séverine",
     "Guillaume"
    ],
    [
     "Guillaume",
     "Wisniewski"
    ],
    [
     "Alexis",
     "Michaud"
    ]
   ],
   "title": "From ‘Snippet-lects’ to Doculects and Dialects: Leveraging Neural Representations of Speech for Placing Audio Signals in a Language Landscape ",
   "original": "6_Guillaume",
   "page_count": 5,
   "order": 7,
   "p1": 29,
   "pn": 33,
   "abstract": [
    "XLSR-53, a multilingual model of speech, builds a vector representation from audio, which allows for a range of computational treatments. The experiments reported here use this neural representation to estimate the degree of closeness between audio files, ultimately aiming to extract relevant linguistic properties. We use max-pooling to aggregate the neural representations from a ‘snippet-lect’ (the speech in a 5-second audio snippet) to a ‘doculect’ (the speech in a given resource), then to dialects and languages. We use data from corpora of 11 dialects belonging to 5 less-studied languages. Similarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. The findings suggest that (i) dialect/language can emerge among the various parameters characterizing audio files and (ii) estimates of overall phonetic/phonological closeness can be obtained for a little- resourced or fully unknown language. The findings help shed light on the type of information captured by neural representations of speech and how it can be extracted from these representations."
   ],
   "doi": "10.21437/SIGUL.2023-7"
  },
  "fadte23_sigul": {
   "authors": [
    [
     "Swapnil",
     "Fadte"
    ],
    [
     "Edna",
     "Vaz Fernandes"
    ],
    [
     "Hanumant",
     "Redkar"
    ],
    [
     "Teja",
     "Kundaikar"
    ],
    [
     "Ramdas",
     "Karmali"
    ],
    [
     "Jyoti D.",
     "Pawar"
    ]
   ],
   "title": "Konkani Integer Phonetic Transcription System ",
   "original": "7_Fadte",
   "page_count": 5,
   "order": 8,
   "p1": 34,
   "pn": 38,
   "abstract": [
    "This paper describes an ongoing work on the Phonetic Dictionary for Konkani language. In this work, we have build a re- source that would phonetically transcribe Konkani Integers and generate their written form in the Devanagari script. The algorithm developed in this work takes an integer as an input and generates its written form in the Devanagari script, along with its phonetic transcription in the International Phonetic Alphabet (IPA). Our algorithm is a rule-based system in which phonetic transcriptions of numerals are created using rules from the avail- able literature and for some cases we have proposed new forms for the numerals. The algorithm has been made robust enough to automatically give a written form of any Konkani numeral in the Devanagari script, along with its equivalent IPA transcription. This work is the first step towards providing an open-source phonetic dictionary for Konkani language. We have tried to keep the phonetic transcriptions as much as closer to their natural pronunciations. This is done for the purpose of capturing the general tendency of the language. So, for example, while the number ’8’ आठ [aúh] is written with an aspirated retroflex consonant ठ [úh], the final consonant [úh] is heard without aspiration in the actual speech. This loss of aspiration at word final position generally happens across all the consonants of the language, in the Konkani varieties spoken in Goa."
   ],
   "doi": "10.21437/SIGUL.2023-8"
  },
  "williams23_sigul": {
   "authors": [
    [
     "Aiden",
     "Williams"
    ],
    [
     "Andrea",
     "Demarco"
    ],
    [
     "Claudia",
     "Borg"
    ]
   ],
   "title": "The Applicability of Wav2Vec2 and Whisper for Low-Resource Maltese ASR ",
   "original": "8_Williams",
   "page_count": 5,
   "order": 9,
   "p1": 39,
   "pn": 43,
   "abstract": [
    "Maltese is a low-resource language with limited digital tools, including automatic speech recognition. With very limited datasets of Maltese speech available, a recent project, MASRI, developed further speech datasets and produced an initial prototype trained using the Jasper architecture. The best system achieved 55.05% WER on the MASRI test set. Our work builds upon this, producing a further two-and-a- half-hour annotated speech corpus from a domain in which no data was previously available (Parliament of Malta). Moreover, we experiment with existing pre-trained self-supervised models (Wav2Vec2.0 and Whisper) and further fine-tune these models on Maltese annotated data. A total of 30 Maltese ASR models are trained and evaluated using the WER and the CER. The results indicate that the performance of the models scales with the quantity of data, although not linearly. The best model achieves state-of-the-art results of 8.53% WER and 1.93% CER on a test set extracted from the CommonVoice project and 24.98% WER and 8.37% CER on the MASRI test set."
   ],
   "doi": "10.21437/SIGUL.2023-9"
  },
  "rouhe23_sigul": {
   "authors": [
    [
     "Aku",
     "Rouhe"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Speech Recognition System Improvements for North Sa ́mi Speaker-dependent and Speaker Independent Tasks",
   "original": "9_Rouhe",
   "page_count": 4,
   "order": 10,
   "p1": 44,
   "pn": 47,
   "abstract": [
    "We are working on North Sa ́mi, an under-resourced language, for which we have less than ten hours of transcribed speech in total. Previously, we applied wav2vec 2.0 pretrained large Transformer models to this data. However, error rates were still high. Here, we present a series of system improvements to these models, yielding minor performance improvements. We also experiment with a slightly larger text corpus, which provides a further minor performance improvement. Nonetheless, we conclude that more transcribed speech is needed, at least so that standard size development and test sets can be created."
   ],
   "doi": "10.21437/SIGUL.2023-10"
  },
  "hiovainasikainen23_sigul": {
   "authors": [
    [
     "Katri",
     "Hiovain-Asikainen"
    ],
    [
     "Javier",
     "de la Rosa"
    ]
   ],
   "title": "Developing TTS and ASR for Lule and North Sámi languages ",
   "original": "10_Hiovain-Asikainen",
   "page_count": 5,
   "order": 11,
   "p1": 48,
   "pn": 52,
   "abstract": [
    "Recent innovations in speech technology have made high quality TTS and ASR available even for extremely low-resource languages. This paper presents our updated work-in-progress report of an open-source speech technology project for two indigenous Sámi languages that are minority languages in Norway, Sweden and Finland.\nAt this stage, we have designed and collected text and speech corpora for training the first neural text-to-speech (TTS) for Lule Sámi. We will update the previous North Sámi TTS by collecting additional materials and by training a new model using state-of-the-art technologies.\nWe also describe our first experiments with developing ASR for North Sámi and discuss the next steps to be taken in our project."
   ],
   "doi": "10.21437/SIGUL.2023-11"
  },
  "nguyen23_sigul": {
   "authors": [
    [
     "Luan Thanh",
     "Nguyen"
    ],
    [
     "Sakriani",
     "Sakti"
    ]
   ],
   "title": "VGSAlign: Bilingual Speech Alignment of Unpaired and Untranscribed Languages using Self-Supervised Visually Grounded Speech Models",
   "original": "11_Nguyen",
   "page_count": 5,
   "order": 12,
   "p1": 53,
   "pn": 57,
   "abstract": [
    "Direct neural speech-to-speech translation (S2ST) systems enable translating speech from source to target languages without the need for text transcription. However, these systems are mostly trained using supervised learning that relies on a massive amount of parallel source-target speech data, which is often unavailable. This paper proposes a bilingual speech alignment approach called VGSAlign, as the initial solution for obtaining paired data from unknown, untranscribed, and unpaired speech data. Here, we assume the speech has auxiliary input from the visual modality that describes the semantic information. The approach then leverages the ability (1) to discover spoken words in multiple languages from the correspondences between speech segments and part of images based on self-supervised visually grounded speech models and (2) to find the visually grounded semantically equivalent between the spoken discovery of speech segments of source and target languages. By learning the representations of speech and images, VGSAlign shows the potential to achieve bilingual speech alignment based on visual representation. Furthermore, experimental results show that the proposed approach could work effectively with unknown, untranscribed, and unpaired speech without being trained on any supervised tasks."
   ],
   "doi": "10.21437/SIGUL.2023-12"
  },
  "mihajlik23_sigul": {
   "authors": [
    [
     "Péter",
     "Mihajlik"
    ],
    [
     "Máté Soma",
     "Kádár"
    ],
    [
     "Gergely",
     "Dobsinszki"
    ],
    [
     "Yan",
     "Meng"
    ],
    [
     "Meng",
     "Kedalai"
    ],
    [
     "Julian",
     "Linke"
    ],
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Katalin",
     "Mády"
    ]
   ],
   "title": "What Kind of Multi- or Cross-lingual Pre-training is the most Effective for a Spontaneous, Less-resourced ASR Task?",
   "original": "12_Mihajlik",
   "page_count": 5,
   "order": 13,
   "p1": 58,
   "pn": 62,
   "abstract": [
    "Most languages are under-resourced for Automatic Speech Recognition (ASR), and most relevant tasks are related to the transcription of spontaneous speech. The application of cross- or multi-lingual pre-training is inevitable, however, the selection of the best pre-trained model or data/method is not straightforward. In this paper, we introduce a case study for Hungarian, targeting good quality spontaneous speech while monitoring the ASR performance of read speech. Transformer/conformer- based end-to-end neural models with supervised cross-lingual, self-supervised cross- and (massively) multi-lingual and weakly supervised multi-lingual pre-training are fine-tuned and evaluated. Surprisingly, a relatively small-scale trilingual (SSL pre-trained) model won the competition by a large margin over very large-scale models trained on more Hungarian data. The results revealed that the composition of pre-training data in terms of language and speech style was essential, bigger size or higher number of languages did not necessarily come with improvement, and no transcription was required in the pre-training for the best performance."
   ],
   "doi": "10.21437/SIGUL.2023-13"
  },
  "lonergan23_sigul": {
   "authors": [
    [
     "Liam",
     "Lonergan"
    ],
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Neasa",
     "Ní Chiaráin"
    ],
    [
     "Christer",
     "Gobl"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Towards Spoken Dialect Identification of Irish ",
   "original": "13_Lonergan",
   "page_count": 5,
   "order": 14,
   "p1": 63,
   "pn": 67,
   "abstract": [
    "The Irish language is rich in its diversity of dialects and accents. This compounds the difficulty of creating a speech recognition system for the low-resource language, as such a system must contend with a high degree of variability with limited corpora. A recent study investigating dialect bias in Irish ASR found that balanced training corpora gave rise to unequal dialect performance, with performance for the Ulster dialect being consistently worse than for the Connacht or Munster dialects. Motivated by this, the present experiments investigate spoken dialect identification of Irish, with a view to incorporating such a system into the speech recognition pipeline. Two acoustic classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a text-based classifier using a pretrained Irish- language BERT model. The ECAPA-TDNN, particularly a model pretrained for language identification on the VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was further improved to 76% by fusing the model’s outputs with the text-based model. The Ulster dialect was most accurately identified, with an accuracy of 94%, however the model struggled to disambiguate between the Connacht and Munster dialects, suggesting a more nuanced approach may be necessary to robustly distinguish between the dialects of Irish."
   ],
   "doi": "10.21437/SIGUL.2023-14"
  },
  "gutscher23_sigul": {
   "authors": [
    [
     "Lorenz",
     "Gutscher"
    ],
    [
     "Michael",
     "Pucher"
    ],
    [
     "Víctor",
     "Garcia"
    ]
   ],
   "title": "Neural Speech Synthesis for Austrian Dialects with Standard German Grapheme-to-Phoneme Conversion and Dialect Embeddings ",
   "original": "14_Gutscher",
   "page_count": 5,
   "order": 15,
   "p1": 68,
   "pn": 72,
   "abstract": [
    "For languages where extensive audio data and text transcriptions are available, text-to-speech (TTS) systems have showcased the ability to generate speech that closely resembles natural human speech. However, the development of TTS systems for dialects and language varieties poses challenges such as limited data availability and strong regional variations. This paper presents a TTS system tailored for under-resourced language varieties spoken in Austrian regions. The system is built upon the FastSpeech 2 architecture and includes modifications to incorporate dialect embeddings for training and inference. It is demonstrated that employing dialect embeddings and a standard German grapheme-to-phoneme conversion is effective in modeling language varieties and provides means to shift a person’s spoken variety from one to another. This allows for the generation of regional standards for dialect speakers or the generation of dialect speech with the voice of a standard speaker. The findings unveil new possibilities and applications in other multilingual contexts where shared characteristics within the language or dialect embedding space can be leveraged."
   ],
   "doi": "10.21437/SIGUL.2023-15"
  },
  "khadka23_sigul": {
   "authors": [
    [
     "Supriya",
     "Khadka"
    ],
    [
     "Ranju",
     "G.C."
    ],
    [
     "Prabin",
     "Paudel"
    ],
    [
     "Rahul",
     "Shah"
    ],
    [
     "Basanta",
     "Joshi"
    ]
   ],
   "title": "Nepali Text-to-Speech Synthesis using Tacotron2 for Melspectrogram Generation",
   "original": "15_Khadka",
   "page_count": 5,
   "order": 16,
   "p1": 73,
   "pn": 77,
   "abstract": [
    "The paper proposes a method for generating high-quality synthesized Nepali speech from the text using the Tacotron2 model for melspectrogram generation. The speech synthesis process involves two phases: melspectrogram generation and vocoder output. The Nepali text is preprocessed and tokenized before being fed into a Tacotron2 model for generating melspec- trograms. The Tacotron2 model is trained on a publicly available OpenSLR dataset for the Nepali language and finetuned on a new dataset created by the authors. Through fine-tuning, the model is refined to improve its performance and adapt it to language-specific characteristics. Further, incremental learning is employed to continually update the model with new data, ensuring its ability to generalize and adapt to evolving contexts. The melspectrograms are then sent to HiFiGAN and WaveGlow vocoders, which produce the synthesized speech. Finally, post- processing techniques are applied to further refine the generated output, enhancing its naturalness. The synthesized speech was qualitatively evaluated to obtain a Mean Opinion Score of 4.03 for naturalness, which stands as the highest among all previous Nepali Text to Speech tasks conducted to date."
   ],
   "doi": "10.21437/SIGUL.2023-16"
  },
  "tran23_sigul": {
   "authors": [
    [
     "Tu Dinh",
     "Tran"
    ],
    [
     "Sakriani",
     "Sakti"
    ]
   ],
   "title": "Low-Resource Japanese-English Speech-to-Text Translation Leveraging Speech-Text Unified-model Representation Learning ",
   "original": "16_Tran",
   "page_count": 5,
   "order": 17,
   "p1": 78,
   "pn": 82,
   "abstract": [
    "Speech-to-text translation (S2TT) has made it critically important to overcome language barriers. Several multilingual datasets have been introduced recently to expand the coverage of multilingual S2TT systems. However, most research works only focus on increasing the number of languages covered. Unfortunately, many of those languages were covered with only a few hours of training data resulting in a low translation performance. This paper proposes utilizing a unified speech-text representation learning framework to overcome the shortage of parallel speech-text datasets in the S2TT system. Although the approach can be utilized for any language pair, we focus on the Japanese-English S2TT task and evaluate it on the publicly available CoVoST 2 dataset. In addition, we also evaluate the S2TT system on our new Japanese-English dataset with sentence ambiguities in which the same spoken utterances can have different translation meanings depending on different prosodic features. We achieve competitive results compared with other state-of-the-art models in CoVoST 2 dataset and provide significant improvement in the more challenging case of our new dataset."
   ],
   "doi": "10.21437/SIGUL.2023-17"
  },
  "lamyeemui23_sigul": {
   "authors": [
    [
     "Léa-Marie",
     "Lam-Yee-Mui"
    ],
    [
     "Waad",
     "Ben Kheder"
    ],
    [
     "Viet-Bac",
     "Le"
    ],
    [
     "Claude",
     "Barras"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Multilingual Models with Language Embeddings for Low-resource Speech Recognition",
   "original": "17_Lam-Yee-Mui",
   "page_count": 5,
   "order": 18,
   "p1": 83,
   "pn": 87,
   "abstract": [
    "",
    "Speech recognition for low-resource languages remains challenging and can be addressed with techniques such as multi- lingual modeling and transfer learning. In this work, we explore several solutions to the multilingual training problem: training monolingual models with multilingual features, adapting a multilingual model with transfer learning and using language em- beddings as additional features. To develop practical solutions we focus our work on medium size hybrid ASR models. The multilingual models are trained on 270 hours of iARPA Babel data from 25 languages, and results are reported on 4 Babel languages for the Limited Language Pack (LLP) condition. The results show that adapting a multilingual acoustic model with language embeddings is an effective solution, outperforming the baseline monolingual models, and providing comparable results to models based on state-of-the-art XLSR-53 features but with the advantage of needing 15 times fewer parameters."
   ],
   "doi": "10.21437/SIGUL.2023-18"
  },
  "vanderwesthuizen23_sigul": {
   "authors": [
    [
     "Ewald",
     "van der Westhuizen"
    ],
    [
     "Marco",
     "Ribeiro"
    ],
    [
     "Joshua Jansen",
     "van Vüren"
    ],
    [
     "Paula",
     "Hidalgo-Sanchis"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "Low-resource ASR-free Keyword Spotting Using Listen-and-confirm",
   "original": "18_van der Westhuizen",
   "page_count": 5,
   "order": 19,
   "p1": 88,
   "pn": 92,
   "abstract": [
    "We present listen-and-confirm (LAC), a human-in-the-loop approach for improving extremely low-resource convolutional neural network-based (CNN) keyword spotting (KWS). LAC interactively presents short audio segments, detected by the KWS, to a human evaluator who confirms whether or not the keyword is present. These LAC responses are used to adjust the CNN training targets and then obtain an improved KWS. Experiments were conducted in English, for controlled experimentation, and Bambara, a severely under-resourced Malian language reflecting the true operational setting in which the KWS is currently used for humanitarian support. Relative improvements in mean precision of 38.18 and 21.62%, respectively, for English and Bambara were achieved for an audio query-by-example task after incorporating feedback from 50 LAC evaluations per keyword type. As a key finding, we show that LAC improves keyword spotting performance even when the human evaluator is completely unfamiliar with the target language. Therefore, LAC can be used to support rapid KWS development in a completely new language."
   ],
   "doi": "10.21437/SIGUL.2023-19"
  },
  "markl23_sigul": {
   "authors": [
    [
     "Nina",
     "Markl"
    ],
    [
     "Electra",
     "Wallington"
    ],
    [
     "Ondrej",
     "Klejch"
    ],
    [
     "Thomas",
     "Reitmaier"
    ],
    [
     "Gavin",
     "Bailey"
    ],
    [
     "Jennifer",
     "Pearson"
    ],
    [
     "Matt",
     "Jones"
    ],
    [
     "Simon",
     "Robinson"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Automatic Transcription and (De)Standardisation ",
   "original": "19_Markl",
   "page_count": 5,
   "order": 20,
   "p1": 93,
   "pn": 97,
   "abstract": [
    "In this paper we illustrate the gap between real language use and the language use assumed in ASR development through the example of isiXhosa in Langa, South Africa. Understanding speech and writing practices in context is particularly important when developing speech technologies for minoritised and under-resourced languages, and their communities."
   ],
   "doi": "10.21437/SIGUL.2023-20"
  },
  "prys23_sigul": {
   "authors": [
    [
     "Delyth",
     "Prys"
    ]
   ],
   "title": "New Challenges, Old Problems: AI and Under-resourced Languages",
   "original": "Prys",
   "page_count": 5,
   "order": 21,
   "p1": 98,
   "pn": 102,
   "abstract": [
    "Advances in language technologies for under-resourced languages include more focused strategies and policies for individual languages as well as progress in the technologies themselves. Looking especially at Welsh and other Celtic languages we consider the importance of economic regeneration hand in hand with language revitalization. We also consider how greater multilingual cooperation and new research on cultural appropriateness of language models could benefit them and other languages, as part of a wider European effort to put an end to digital language extinction."
   ],
   "doi": "10.21437/SIGUL.2023-21"
  },
  "jouitteau23_sigul": {
   "authors": [
    [
     "Mélanie",
     "Jouitteau"
    ]
   ],
   "title": "Community Internally-driven Corpus Buildings. Three Examples from the Breton Ecosystem ",
   "original": "20_Jouitteau",
   "page_count": 5,
   "order": 22,
   "p1": 103,
   "pn": 107,
   "abstract": [
    "This paper is a position paper concerning corpus-building strategies in minoritized languages in the Global North. It draws attention to the structure of the non-technical community of speakers, and concretely addresses how their needs can inform the design of technical solutions. Celtic Breton is taken as a case study for its relatively small speaker community, which is rather well-connected to modern technical infrastructures, and is bilingual with a non-English language (French). I report on three different community internal initiatives that have the potential to facilitate the growth of NLP-ready corpora in FAIR practices (Findability, Accessibility, Interoperability, Reusability). These initiatives follow a careful analysis of the Breton NLP situation both inside and outside of academia, and take advantage of pre-existing dynamics. They are integrated to the speaking community, both on small and larger scales. They have in common the goal of creating an environment that fosters virtuous circles, in which various actors help each other. It is the interactions between these actors that create quality- enriched corpora usable for NLP, once some low-cost technical solutions are provided. This work aims at providing an estimate of the community’s internal potential to grow its own pool of resources, provided the right NLP resource gathering tools and ecosystem design. Some projects reported here are in the early stages of conception, while others build on decade-long society/research interfaces for the building of resources. All call for feedback from both NLP researchers and the speaking communities, contributing to building bridges and fruitful collaborations between these two groups."
   ],
   "doi": "10.21437/SIGUL.2023-22"
  },
  "huang23_sigul": {
   "authors": [
    [
     "Junfan",
     "Huang"
    ],
    [
     "Beatrice",
     "Alex"
    ],
    [
     "Michael",
     "Bauer"
    ],
    [
     "David",
     "Salvador-Jasin"
    ],
    [
     "Yuchao",
     "Liang"
    ],
    [
     "Robert",
     "Thomas"
    ],
    [
     "William",
     "Lamb"
    ]
   ],
   "title": "A Transformer-Based Orthographic Standardiser for Scottish Gaelic ",
   "original": "21_Huang",
   "page_count": 5,
   "order": 23,
   "p1": 108,
   "pn": 112,
   "abstract": [
    "The transition from rule-based to neural-based architectures has made it more difficult for low-resource languages like Scottish Gaelic to participate in modern language technologies. The performance of deep-learning approaches correlates with the availability of training data, and low-resource languages have limited data reserves by definition. Historical and non-standard orthographic texts could be used to supplement training data, but manual conversion of these texts is expensive and time-consuming. This paper describes the development of a neural-based orthographic standardisation system for Scottish Gaelic and compares it to an earlier rule-based system. The best performance yielded a precision of 93.92, a recall of 92.20 and a word error rate of 11.01. This was obtained using a transformer- based mixed teacher model which was trained with augmented data."
   ],
   "doi": "10.21437/SIGUL.2023-23"
  },
  "vangberg23_sigul": {
   "authors": [
    [
     "Preben",
     "Vangberg"
    ],
    [
     "Leena Sarah",
     "Farhat"
    ],
    [
     "Dewi Bryn",
     "Jones"
    ],
    [
     "Sean",
     "Kinahan"
    ]
   ],
   "title": "Developing Live Welsh Speech Recognition Models for a Commercial Product - a case study ",
   "original": "22_Vangberg",
   "page_count": 3,
   "order": 24,
   "p1": 113,
   "pn": 115,
   "abstract": [
    "",
    "This paper reports on the work undertaken to develop Welsh speech recognition within a wider pilot project for adding a Welsh speech-to-translated English text capability into Haia Communication Ltd’s (Haia)1 innovative hybrid events platform.\nA hybrid event system is a platform that allows for the integration of physical and virtual elements in an event. It combines the advantages of both physical and virtual event experiences, allowing participants to interact and engage from a physical venue as well as remotely via digital methods. In comparison to platforms such as Microsoft Teams, Zoom, and other video conferencing options, hybrid event systems provide more complete capabilities including multi-camera setups, live streaming capabilities, virtual attendee interaction tools, interactive Q&A sessions, virtual exhibitor booths, networking capabilities, and analytics to gauge attendee engagement and satisfaction. Hybrid events conducted by Welsh public sector organisations require facilitating bilingual Welsh and English language interactions, which at present are provided through human interpretation.\nConsequently, a key requirement by the Haia platform for facilitating bilingual interactions via technology is support for accurate and efficient online or live automatic speech recognition (ASR)."
   ],
   "doi": "10.21437/SIGUL.2023-24"
  },
  "nichasaide23_sigul": {
   "authors": [
    [
     "Ailbhe",
     "Ní Chasaide"
    ],
    [
     "Neasa",
     "Ní Chiaráin"
    ],
    [
     "Harald",
     "Berthelsen"
    ],
    [
     "Andrew",
     "Murphy"
    ],
    [
     "Liam",
     "Lonergan"
    ],
    [
     "John",
     "Sloan"
    ],
    [
     "Christoph",
     "Wendler"
    ],
    [
     "Connor",
     "McCabe"
    ],
    [
     "Emily",
     "Barnes"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "The Language Communities as Active Partners in Technology Provisions: the Irish ABAIR Experience",
   "original": "23_NiChasaide",
   "page_count": 5,
   "order": 25,
   "p1": 116,
   "pn": 120,
   "abstract": [
    "",
    "The impact of speech and language technology for the endangered language depends on the extent to which the language community engages with its development. In this paper, the range of speech technologies and applications developed for Irish in the ABAIR initiative in Trinity College Dublin is presented, along with reflections on the many ways in which the language community has come to play an increasingly central role. Community involvement and buy-in is essential for all aspects – not only for the development of core technologies, such as speech synthesis and recognition systems – but to prompt development directions, to determine priorities for the most important and urgently needed applications and to collaborate in their provision. In order for technology to achieve its potential for the endangered language, developers need a knowledge of the language and an understanding of the socio- linguistic context in which the technologies will be used."
   ],
   "doi": "10.21437/SIGUL.2023-25"
  },
  "murphy23_sigul": {
   "authors": [
    [
     "Andy",
     "Murphy"
    ],
    [
     "Liam",
     "Lonergan"
    ],
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Harald",
     "Berthelsen"
    ],
    [
     "Christoph",
     "Wendler"
    ],
    [
     "Neasa",
     "Ní Chiaráin"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "ABAIR & ÉIST: a Demonstration of Speech Technologies for Irish ",
   "original": "24_Murphy",
   "page_count": 3,
   "order": 26,
   "p1": 121,
   "pn": 123,
   "abstract": [
    "",
    "The ABAIR initiative [1, 2, 3, 4] has been providing speech technology, and its applications, for Irish language communities for many years. For Irish, as an endangered language, it is particularly important to implement speech technology, not only to offer speakers a resource for language learning and accessibility, but to maintain and preserve the very language itself. The ABAIR initiative has developed linguistic resources, text- to-speech (TTS) systems, automatic speech recognition (ASR), and many applications that utilise these technologies directly. This paper demonstrates and discusses the TTS and ASR technologies created, and provides a glimpse at the applications being developed in parallel.\n"
   ],
   "doi": "10.21437/SIGUL.2023-26"
  },
  "watson23_sigul": {
   "authors": [
    [
     "Catherine I.",
     "Watson"
    ],
    [
     "Piata",
     "Allen"
    ],
    [
     "Peter J.",
     "Keegan"
    ],
    [
     "Keoni",
     "Mahelona"
    ],
    [
     "Peter-Lucas",
     "Jones"
    ]
   ],
   "title": "Towards Automatic Marking of Pepeha: a Formulaic Māori Language Speech",
   "original": "25_Watson",
   "page_count": 5,
   "order": 27,
   "p1": 124,
   "pn": 128,
   "abstract": [
    "",
    "This study looks at the assessment of the pronunciation of te reo Māori (Māori language) within a short formulaic speech, known as a pepeha. It is a comparison between marks awarded to the pehepa by trained markers, and scores awarded to the pepeha by Arero, a speech recognition platform which is purpose-build to assess te reo Māori. Pepeha recordings of 304 people were analysed. The study found that there were many similarities between the two assessment methods and they were correlated, albeit weakly. It is argued that the results suggest automatic marking of pepeha is feasible. The next step is to understand acceptable phonetic variation in the pepeha pronunciation via phonetic analysis of a large number of the pepeha recordings."
   ],
   "doi": "10.21437/SIGUL.2023-27"
  },
  "barnes23_sigul": {
   "authors": [
    [
     "Emily",
     "Barnes"
    ],
    [
     "Julia",
     "Cummins"
    ],
    [
     "Rian",
     "Errity"
    ],
    [
     "Oisín",
     "Morrin"
    ],
    [
     "Harald",
     "Berthelsen"
    ],
    [
     "Christoph",
     "Wendler"
    ],
    [
     "Andy",
     "Murphy"
    ],
    [
     "Helen",
     "Husca"
    ],
    [
     "Neasa",
     "Ní Chiaráin"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Geabaire, the First Irish AAC System: Voice as a Vehicle for Change ",
   "original": "26_Barnes",
   "page_count": 5,
   "order": 28,
   "p1": 129,
   "pn": 133,
   "abstract": [
    "",
    "This paper introduces Geabaire, an Irish language Alternative and Augmentative Communication (AAC) system which is currently under development in the Phonetics and Speech Laboratory, Trinity College Dublin. This high-tech AAC system provides a vehicle for users who are non-speaking or minimally-speaking to converse with the world. Geabaire is one step in the movement towards the provision of access to the same technologies in Irish as in English, with the overarching goal of improving support for children who are neurodivergent or who have special educational needs who wish to speak Irish in a social or educational context. The development involves a community-driven, multidisciplinary team and involves co- design with autistic people, parents, teachers and speech and language therapists. This paper describes three initial approaches made – (i) the adaptation of an existing English system, (ii) efforts towards commercial collaboration and (iii) building a prototype within an existing open-source system – each of which provided lessons for future development. Based on these lessons, we embarked on building our own custom- made, native system called Geabaire. The challenges of development in a minority language – even a well-resourced one – are discussed and plans for future development are described."
   ],
   "doi": "10.21437/SIGUL.2023-28"
  }
 },
 "sessions": [
  {
   "title": "Keynote",
   "papers": [
    "panigrahi23_sigul"
   ]
  },
  {
   "title": "Session 1: NLP",
   "papers": [
    "cristea23_sigul",
    "alam23_sigul"
   ]
  },
  {
   "title": "Session 2: Resources, Pronunciation & Dialect",
   "papers": [
    "kumar23_sigul",
    "welby23_sigul",
    "vegarodriguez23_sigul",
    "guillaume23_sigul"
   ]
  },
  {
   "title": "Poster Session",
   "papers": [
    "fadte23_sigul",
    "williams23_sigul",
    "rouhe23_sigul",
    "hiovainasikainen23_sigul",
    "nguyen23_sigul"
   ]
  },
  {
   "title": "Session 3: Language Technologies 1",
   "papers": [
    "mihajlik23_sigul",
    "lonergan23_sigul",
    "gutscher23_sigul",
    "khadka23_sigul",
    "tran23_sigul"
   ]
  },
  {
   "title": "Session 4: Language Technologies 2",
   "papers": [
    "lamyeemui23_sigul",
    "vanderwesthuizen23_sigul",
    "markl23_sigul"
   ]
  },
  {
   "title": "Special Session on Celtic Languages, Keynote",
   "papers": [
    "prys23_sigul"
   ]
  },
  {
   "title": "Special Session on Celtic Languages",
   "papers": [
    "jouitteau23_sigul",
    "huang23_sigul",
    "vangberg23_sigul",
    "nichasaide23_sigul",
    "murphy23_sigul"
   ]
  },
  {
   "title": "Joint SIGUL-SLaTE Session",
   "papers": [
    "watson23_sigul",
    "barnes23_sigul"
   ]
  }
 ],
 "doi": "10.21437/SIGUL.2023"
}