{
 "title": "First International Conference on Spoken Language Processing (ICSLP 1990)",
 "location": "Kobe, Japan",
 "startDate": "18/11/1990",
 "endDate": "22/11/1990",
 "conf": "ICSLP",
 "year": "1990",
 "name": "icslp_1990",
 "series": "ICSLP",
 "SIG": "",
 "title1": "First International Conference on Spoken Language Processing",
 "title2": "(ICSLP 1990)",
 "date": "18-22 November 1990",
 "papers": {
  "kohno90_icslp": {
   "authors": [
    [
     "Morio",
     "Kohno"
    ],
    [
     "Tomoko",
     "Tanioka"
    ]
   ],
   "title": "The nature of timing control in language",
   "original": "i90_0001",
   "page_count": 4,
   "order": 1,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "This paper will first propose the universal timing measure and then will show how language specific rhythmic constraints are brought about from this universal timing instrument. Argument will be built up on the neuropsychological facts which were found by five experiments using as subjects a patient with infarction involving the forebrain commissural fibers, children with age variety from one year and half to nine years old as well as normal adults.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-1"
  },
  "beckman90_icslp": {
   "authors": [
    [
     "Mary E.",
     "Beckman"
    ],
    [
     "Maria G.",
     "Swora"
    ],
    [
     "Jane",
     "Rauschenberg"
    ],
    [
     "Kenneth de",
     "Jong"
    ]
   ],
   "title": "Stress shift, stress clash, and polysyllabic shortening in a prosodically annotated discourse",
   "original": "i90_0005",
   "page_count": 4,
   "order": 2,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "This paper reports on tests of three different stress-timing effects in a corpus of spontaneous utterances, elicited in a task which prompted naive speakers to produce many tokens of relevant target phrases. Several independent transcriptions of stress and intonation patterns were obtained. The intonation markings were used to control for phrase boundaries in interpreting duration measurements for polysyllabic shortening and stress clash effects. The first, but not the second is robustly demonstrated. The intonation patterns and stress transcriptions were also used to understand stress shift and to show where it will occur. The results show the intricate link between intonation and rhythm, making it crucial to obtain judgments of accent and intonational phrasing in experiments on speech rhythms in languages like English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-2"
  },
  "campbell90_icslp": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ]
   ],
   "title": "Evidence for a syllable-based model of speech timing",
   "original": "i90_0009",
   "page_count": 4,
   "order": 3,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "To test whether a measure of lengthening can be found which applies to segments uniformly within the syllable, instead of differentially with respect to segment type or position in the syllable, the durations of each segment in the SCRIBE 200-sentence phonetically balanced database were converted to standard normal form by subtracting the means and expressing the residuals in terms of their standard deviations to yield a mean of zero and a variance of 1 for each phoneme distribution. These normalised values were taken to represent the amount of lengthening or compression undergone by each segment relative to its elasticity. This paper explores the extent to which lengthening or compression measured in this way applies within the syllable, and shows that stress-induced lengthening may be differentiated from intonation-phrase-final lengthening, which is in turn different from the phonetically-motivated lengthening that is undergone by vowels preceeding voiced plosives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-3"
  },
  "price90_icslp": {
   "authors": [
    [
     "Patti J.",
     "Price"
    ],
    [
     "C. W.",
     "Wightman"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "John",
     "Bear"
    ]
   ],
   "title": "The use of relative duration in syntactic disambiguation",
   "original": "i90_0013",
   "page_count": 4,
   "order": 4,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "We describe the modification of a grammar to take advantage of prosodic information automatically extracted from speech. The work includes (1) the development of an integer break index representation of prosodic phrase boundary information, (2) the automatic detection of prosodic phrase breaks using a hidden Markov model on relative duration of phonetic segments, and (3) the integration of the prosodic phrase break information in SRI's Spoken Language System to rule out alternative parses in otherwise syntactically ambiguous sentences. Automatically detected phrase break indices had a correlation of greater than 0.8 with hand-labeled data for speaker-dependent and independent models; and in a subset of sentences with preposition ambiguities, the number of parses was reduced by 25% with a simple grammar modification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-4"
  },
  "kaiki90_icslp": {
   "authors": [
    [
     "Nobuyoshi",
     "Kaiki"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Statistical analysis for segmental duration rules in Japanese speech synthesis",
   "original": "i90_0017",
   "page_count": 4,
   "order": 5,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "In this paper, duration control factors are statistically analyzed using Japanese speech data uttered by four speakers. According to previous studies, the important factors are phoneme category, neighboring phonemes, position in breath group and mora count of breath group. In addition to the above factors, we introduce several new control factors. They are position in phrase, mora count of phrase, content / function word category, pre- and post-adjacent phonemes, and temporal compensation caused by geminated consonants. Using these statistically significant factors, a vowel duration model is proposed for Japanese speech synthesis. The duration prediction experiments using this model showed that the root mean square errors between predicted duration and observed duration were 15.30ms (19.6% of the average length) for vowels in the training set, and 15.84ms (19.9% of the average length) for vowels in the testing set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-5"
  },
  "koopmansvanbeinum90_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "Spectro-temporal reduction and expansion in spontaneous speech and read text: the role of focus words",
   "original": "i90_0021",
   "page_count": 4,
   "order": 6,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "This study is part of a larger project in which we investigate the communicatively important aspects in spontaneous speech as compared to read texts. To start with we first concentrated on spectro-temporal and intonational aspects of focus words (i.e. words bearing the highest load of semantic information) in spontaneous speech and in the same texts, re-read after ortho- graphic transcription, against the background of some global acoustic parameters of both speech styles. Use is made of speech material produced by one professional male speaker (model for the diphone-based component of the Dutch national speech synthesis program). Although measurements on local acoustic parameters are still in progress, first results on some global background parameters reveal hardly any overall durational difference between spontaneous speech and the same text read aloud, but large differences in mean fundamental frequency and fundamental frequency range. Results of global and local acoustic parameters together will be used to establish the role of focus words, within read as well as in spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-6"
  },
  "naganomadsen90_icslp": {
   "authors": [
    [
     "Yasuko",
     "Nagano-Madsen"
    ]
   ],
   "title": "Perception of mora in the three dialects of Japanese",
   "original": "i90_0025",
   "page_count": 4,
   "order": 7,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "One naturally spoken token of the words beru and beeru was edited by LPC synthesis to produce stimuli varying in the timing of F0 fall. These stimuli were presented in two durational contexts to listeners representing the three varieties of Japanese who were asked to judge which of the words the stimuli sounded like. The results showed that there was a significant effect of F0 pattern on the judgement made by the Standard and Osaka group whereas no such effect was observed for the accentless group. The durational context played no role for the former group but increased accuracy in judgement for the latter group. The finding that F0 pattern influenced the perception of vowel mora in the Standard/Osaka group gives support to the traditional description that it is the mora, not the syllable, that is the unit of tone assignment in these varieties of Japanese, However, it does not support the definition of the Japanese mora as fundamentally a durational phenomena and nothing else.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-7"
  },
  "wang90_icslp": {
   "authors": [
    [
     "Shihua",
     "Wang"
    ],
    [
     "Erdal",
     "Paksoy"
    ],
    [
     "Allen",
     "Gersho"
    ]
   ],
   "title": "Performance of nonlinear prediction of speech",
   "original": "i90_0029",
   "page_count": 4,
   "order": 8,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "A novel method for nonlinear prediction of speech is introduced which does not require a parametric model of the predictor. The observable past is vector quantized and a nonlinear prediction is obtained by a table lookup, addressed by the index of the quantized input vector. The table is designed with speech training data. Experimental results for a moving-average process confirm that nearly optimal nonlinear prediction is achievable. Results for speech show that the performance depends on both the size of the vector quantizer codebook and the size of the training set. The method is applied to DPCM and some useful performance gain is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-8"
  },
  "wang90b_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Quan fen",
     "Guan"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A method for robust GARMA analysis of speech",
   "original": "i90_0033",
   "page_count": 4,
   "order": 9,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "Conventional linear prediction of speech suffers from oversimplification in both vocal tract modeling and voice source modeling. We have presented a six-parameter model for the glottal source and a method for simultaneous estimation of voice source and vocal tract parameters based on an ARMA model combined with the glottal model (henceforth the GARMA method ). This paper describes further effort to refine our previous proposal. Taking into account the non-Gaussian nature of the innovation for voiced speech sounds, the robust signal processing techniques are introduced as an effective means of separating the voice source from the vocal tract transfer function. By minimizing the sum of appropriately weighted errors rather than the sum of squared errors, the proposed method is more robust against glottal excitation as well as random noise interferences. Therefore higher synthesized speech quality can be obtained by the robust GARMA method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-9"
  },
  "tokuda90_icslp": {
   "authors": [
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tahao",
     "Kobayashi"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Generalized cepstral analysis of speech - unified approach to LPC and cepstral method",
   "original": "i90_0037",
   "page_count": 4,
   "order": 10,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "This paper describes a spectral estimation method based on the generalized cepstral representation. The model spectrum represented by the generalized cepstrum varies from the all-pole spectrum to that represented by the cepstrum according to the value of the parameter 7 in the range of [-1,0]. We apply the criterion used in the unbiased estimation of log spectrum to the spectral model. As a result, the proposed method includes linear prediction and the unbiased estimation of log spectrum as the special cases. To solve the non-linear minimization problem involved in the method, we give an iterative algorithm whose convergence is guaranteed. The stability of the model solution is guaranteed. We also show some results of natural speech analysis to discuss the optimal value of 7 in the sense of minimizing the prediction error.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-10"
  },
  "dix90_icslp": {
   "authors": [
    [
     "P. J.",
     "Dix"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "E. J. M. van",
     "Mierlo"
    ]
   ],
   "title": "A geometrical argument for imposing an additional constraint on temporal decomposition",
   "original": "i90_0041",
   "page_count": 4,
   "order": 11,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "Temporal decomposition (TD), an analysis procedure introduced by Atal in 1983, yields a linear approximation of speech parameters in terms of a series of time-overlapping interpolation functions and an associated series of data vectors. Essentially, TD is based on a linear model of the effects of coarticulation, the coarticulation effects being modelled by the overlap of neighboring interpolation functions. This method does not make use of any specific phonetic knowledge and can be applied to any time-sequence of data vectors. In this paper we will discuss some of the constraints which must be imposed on such a linear approximation and describe a new and improved method for constructing the interpolation functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-11"
  },
  "funaki90_icslp": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ],
    [
     "Yukio",
     "Mitome"
    ]
   ],
   "title": "A speech analysis method based on a glottal source model",
   "original": "i90_0045",
   "page_count": 4,
   "order": 12,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "The speech analysis method based on a glottal source model is important for efficiently constructing a rule-based speech synthesis system, since it is easy to control parameters, it can obtain smooth spectral trajectories and it can reduce memory capacity to store the parameters. This paper proposes a new speech analysis method based on the glottal source model. The proposed method uses glottal source and vocal tract inverse filterings in the frequency domain, and the model parameters are calculated so as to minimize inverse filtered error. The method can significantly reduce the computation amount and can assure vocal tract filter stability. Speech analysis synthesis experiments were carried out. The experimental results show that the speech spectrum is estimated more accurately and the estimated pole trajectory is smoother in the proposed method than in the conventional one. Further, synthetic speech is natural and intelligible. The results indicate that the proposed method is suitable for a rule-based speech synthesis system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-12"
  },
  "lee90_icslp": {
   "authors": [
    [
     "Ki Y.",
     "Lee"
    ],
    [
     "Inhyok",
     "Cha"
    ],
    [
     "Eckho",
     "Song"
    ],
    [
     "Souguil",
     "Ann"
    ]
   ],
   "title": "An improved method for multipulse speech analysis",
   "original": "i90_0049",
   "page_count": 4,
   "order": 13,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "In an effort to improve existing multipulse LP speech synthesis methods, we propose in this paper a new multipulse synthesis method which is devised to modify in two aspects the exisiting multipulse algorithms. First, in order to lessen the influence of the pitch periodicity of voiced speech upon conventional estimation of vocal tract, sample selective LP analysis is applied to estimate the vocal tract transfer function and thus is used as the synthesis filter of MPLPC method. Conventional residual signal is exploited so as to sort out from the analysis set those speech samples which have seemingly been influenced by the pitch bias effect. Second, instead of searching, for the pulses on every point in the frame, a new method, called candidate locations restricted searching (CLRS), is devised so that computational loads for the multipulse searching could be lessened.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-13"
  },
  "chang90_icslp": {
   "authors": [
    [
     "Lu",
     "Chang"
    ],
    [
     "M. M.",
     "Bayoumi"
    ]
   ],
   "title": "New results on theory of hidden Markov models",
   "original": "i90_0053",
   "page_count": 4,
   "order": 14,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "This paper describes an effort to extend the theory of hidden Markov models (HMM). It is rather revealing to find out that the scaling factors are a conditional probability of observing the current symbol given all the past observed symbols. And the single symbol emission probability is independent of time if the initial state distribution is the limit distribution of the corresponding Markov chain. To make the real-time implementation of reestimation possible, we derive the forward evaluation of the backward probabilities. Binomial distribution is introduced to model the state transitions of HMM. This increases not only the flexibility of the model but also the modelling power of HMM. Simulation results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-14"
  },
  "scherer90_icslp": {
   "authors": [
    [
     "Ronald C.",
     "Scherer"
    ],
    [
     "Chwen-geng",
     "Guo"
    ]
   ],
   "title": "Laryngeal modeling: translaryngeal pressure for a model with many glottal shapes",
   "original": "i90_0057",
   "page_count": 4,
   "order": 15,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "Voice quality is dependent on the shape and amplitude of the laryngeal airflow signal. Physiological models of the airflow need laryngeal pressure-flow-geometry information. A comprehensive translaryngeal pressure-flow equation is offered. A model of the larynx 7.5 times life size was used with 63 combinations of glottal angles and diameters. The translaryngeal pressure-flow data were nondimensionalized into a pressure coefficient P* and Reynolds number Re such that P* = (A1/Re) + A2. A1 and A2 were empirically determined from the data, and were structured on Poiseuille, diffuser, and optimum pressure recovery considerations. For pressure drops ranging between 3 and 50 cm H2O, the average mean difference between the equation predictions and the empirical data was 4.45% (sd 3.38%). The results can be used in phonatory models or speech synthesis schemes for which volume velocity is dependent upon subglottal pressure and glottal configuration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-15"
  },
  "kiritani90_icslp": {
   "authors": [
    [
     "Shigeru",
     "Kiritani"
    ],
    [
     "Hiroshi",
     "Imagawa"
    ],
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "Vocal cord vibration and voice source characteristics - observations by a high-speed digital image recording -",
   "original": "i90_0061",
   "page_count": 4,
   "order": 16,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "By using a newly developed high-speed digital Image recording system, the relationship between vocal cord vibration and voice source charateristics has been investigated. The system employs either a fiberscope or a solid type endoscope combined with an image sensor and a digital image memory. Video signals from the image sensor are A/D converted and stored in the image memory. Since the system does not have any mechanical unit producing noise sounds, simultaneous recording of vocal cord vibration and speech signal can. be performed easily. Fiberscope system made it possible to observe vocal cord vibration during running speech. Specifically, pattern of vocal cord vibration at the release of the consonants were analysed. Solid endoscope system has been used for observing sustained phonation of pathological voice. It has brighter image than fiberscope system and thus, has better resolution. Many cases of rough voice show asymmetric and/or asynchronous movements between the right and left vocal cords, and between the anterior and posterior to parts of the vocal cords. These movements appear to be related the periodical fluctuations in the vibratory pattern which produce corresponding fluctuations in the speech waveform.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-16"
  },
  "cranen90_icslp": {
   "authors": [
    [
     "Bert",
     "Cranen"
    ]
   ],
   "title": "Interpretation of EGG and glottal flow by means of a parametrical glottal geometry model",
   "original": "i90_0065",
   "page_count": 4,
   "order": 17,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "We examined simultaneously measured EGG and glottal flow waveforms of 20 male subjects and found characteristic details which we tried to interpret in terms of a parametrical model of the glottal geometry [1,2], We came to the conclusion that it is possible to simulate either EGG or glottal flow adequately, but that it is impossible to find model settings which are adequate for simulating both flow and EGG waveforms simultaneously. The simulated glottal flow waveforms showed the same kind of details as observed in real measurements, i.e. a main flow pulse followed by a smaller one while the dip in between is a function of abduction. We could explain the non-negligible flow after the lower parts of the vocal folds have come into contact, by assuming that it consists of two components. One flow component arises when glottal leakage is combined with vertical phasing [3,4], The other component was identified as squish flow.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-17"
  },
  "karlsson90_icslp": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ]
   ],
   "title": "Voice source dynamics for female speakers",
   "original": "i90_0069",
   "page_count": 4,
   "order": 18,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "Dynamic variations of the voice source in ordinary speech have been studied by means of inverse filtering of the sound pressure wave. The inverse filtered voice pulses have been matched by the LF-model to achieve a parametric description. Special attention was given to the spectrum of the pulses. The speech material consisted of sentences and vowels uttered by three female speakers judged to differ in voice quality. The voices were judged to be normal and their quality ranged from somewhat tight and sonorous to thin. The variation of the voice source parameters with place and manner of articulation will be discussed. The length of the closing time in the voice pulse is shown to vary with vowel height; a more open vowel shows a shorter closing time. This tendency seem to be true for the three different voices studied even though the range can vary with voice type. Consonants have been studied, especially concerning the influence of spectral zeros and the transition between consonants and vowels. Variations of the different glottal parameters within sentences have also been studied in relation to voice type.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-18"
  },
  "koizumi90_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Shuji",
     "Taniguchi"
    ]
   ],
   "title": "A novel model of pathological vocal cords and its application to the diagnosis of vocal cord polyp",
   "original": "i90_0073",
   "page_count": 4,
   "order": 19,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "The present paper deals with a new noninvasive method of diagnosing vocal cord polyp through hoarse voice analysis. A noteworthy feature of this method is that it enables us not only to discriminate hoarse voices caused by pathological vocal cords with a single polyp from hoarse voices due to other laryngeal diseases but also to estimate polyp features such as the mass and dimension of polyp through the use of a novel model of pathological vocal cords which has been devised to simulate the subtle movement of the vocal cords with a single polyp. A syn- thetic hoarse voice produced with a hoarse voice synthesizer is compared with a natural hoarse voice caused by the vocal cord polyp in terms of a distance measure and the polyp features are estimated by minimizing the distance measure. Some estimates of polyp dimension that have been obtained by applying this procedure to hoarse voices are found to compare favorably with actual polyp dimensions, demonstrating that the procedure is effective for the diagnosis of vocal cord polyp.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-19"
  },
  "iijima90_icslp": {
   "authors": [
    [
     "Hirohisa",
     "Iijima"
    ],
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Nobuo",
     "Nagai"
    ]
   ],
   "title": "Glottal flow analysis based on a finite element simulation of a two-dimensional unsteady viscous fluid",
   "original": "i90_0077",
   "page_count": 4,
   "order": 20,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Unsteady viscous flows in two-dimensional static models of the glottis are numerically analyzed using a finite element method, and from those results, the physical characteristics of the glottal flow are considered. Since the glottal flow is sensitively affected by the shape of the vocal cord muscles, we make representative experiments on diverging, converging and uniform shaped glottal models in the case of wide diameter, and compare the flow fields between these models. After we briefly note the computing method for simulating the glottal flow numerically, equivalent impedance of the glottal flow is identified from the results of the finite element simulation. After that, pressure distributions along the vocal cord surfaces are discussed. The pressure distributions are almost accordant with Bernoulli's theorem and with the losses due to viscosity and nonlinearity. Furthermore, time-series of supraglottal pressure are shown and they are analyzed in the frequency domain. The supraglottal pressure would appear to be related to the rate of generation of the vortices, and to affect the acoustical propagation of the vocal tract.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-20"
  },
  "kasuya90_icslp": {
   "authors": [
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Yuji",
     "Ando"
    ],
    [
     "Lu",
     "Jinlin"
    ],
    [
     "Osamu",
     "Komuro"
    ]
   ],
   "title": "A voice source model for synthesizing speech with various voice quality variations",
   "original": "i90_0081",
   "page_count": 4,
   "order": 21,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "An acoustic voice source model is presented to generate the voicing waveform and laryngeal noise signal for synthesizing a natural-sounding speech with various voice quality variations. The model incorporates the fluctuation of fundamental periods. Special emphasis is placed on the acoustic measurements of the fluctuations and laryngeal noise from speech signals and on the perceptual salience of these quantities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-21"
  },
  "nichasaide90_icslp": {
   "authors": [
    [
     "Ailbhe",
     "Ni Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Linguistic and paralinguistic variation in the voice source",
   "original": "i90_0085",
   "page_count": 4,
   "order": 22,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "This paper presents an overview of past results and ongoing work by the authors on voice source variation. The LF-model is used to quantify voice source parameters and ultimately as the basis for resynthesis. First of all, the nature and directionality of coarticulatory effects of voiced/voiceless segments are looked at across languages. Results show some striking cross-language differences, and some of these can be closely linked to the temporal coordination of laryngeal and supralaryngeal gestures. Voice source variations due to prosodic context (degree of stress) are also dealt with briefly. Concerning paralinguistic variation, ongoing work is presented on different voice qualities as produced by a single speaker. In conclusion, some implications for voice source rules in speech synthesis are mentioned.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-22"
  },
  "alku90_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Glottal-LPC based coding of telephone band vowels with simple all-pole excitation",
   "original": "i90_0089",
   "page_count": 4,
   "order": 23,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "The coding of telephone band vowels has been studied in this research by using a speech production model that consists of three separated parts: the glottal excitation, the vocal tract and the lip radiation effect. The processes were identified and computed using the AIF-method. In the coding procedure the obtained glottal excitation was replaced by impulse responses of low order LPC-filters. The difference between the time domain characteristics of the original and the coded excitation is extensive but their frequency domain curves are similar. The vocal tract and the lip radiation effects were coded as an eigth order LPC-filter and a differentiator, respectively. The results of this study serve as a promising basis for further studies in order to create a low bit rate speech coder that is based on a more reliable modeling of speech production in comparison to conventional LPC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-23"
  },
  "yeldener90_icslp": {
   "authors": [
    [
     "Suat",
     "Yeldener"
    ],
    [
     "Ahmet M.",
     "Kondoz"
    ],
    [
     "Barry G.",
     "Evans"
    ]
   ],
   "title": "Sine wave excited linear predictive coding of speech",
   "original": "i90_0093",
   "page_count": 4,
   "order": 24,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "The choice an algorithm of speech coding is very important to achieve high quality speech at low bit rates. Speech can be modeled using LPC and Sinusoidal Transform Coding (STC). In LPC, it leads to CELP type coders [l][2]. In CELP, during vector quantization of the excitation, all components are matched as a single vector. This produces background noise and hence roughness below 4.8 kbits/s. In STC [3], on the other hand, the model parameters (phase and frequency) are very sensitive to quantization errors. This affects the performance of this system under channel errors even though it produces high quality speech at low bit rates. In our previous work, we used sine wave components to represent the CELP excitation [4] and LPC residual waveform [6] which both are capable of synthesizing speech without the artifacts common to model-based speech system. In this paper, we present the sine wave excited linear prediction (SWELP) speech model which has been found to be robust in the presence of quantization noise in speech. These characteristics make the model particularly useful in the development of high quality speech coding system at low bit rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-24"
  },
  "miyano90_icslp": {
   "authors": [
    [
     "Toshiki",
     "Miyano"
    ],
    [
     "Kazunori",
     "Ozawa"
    ]
   ],
   "title": "Improvement on 8 kb/s CELP using learned codebook: LCELP",
   "original": "i90_0097",
   "page_count": 4,
   "order": 25,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "This paper proposes 8kb/s LCELP (Learned Code Excited LPC Coding). In order to improve conventional CELP speech quality with relatively low computational complexity, LCELP uses a two-stage vector quantizer with learned and random code-books. The advantages of using both the learned codebook and the random codebook are that synthetic quality is improved by the learned codebook and that robustness for any speech is enhanced by the random codebook. The learned codebook is designed using a speech database, and the random codebook is designed using white Gaussian signals. In order to allocate more bits to the learned and the random codebooks, LSP parameters are efficiently quantized by using a vector-scalar quantization (VQ-SQ) technique. Computer simulation results show that 8 kb/s LCELP achieves an average of 14.5 dB segmental SNR, which is 0.9 dB higher than that for the conventional CELP [2]. Informal listening tests show that LCELP speech quality is high and close to 56 kb/s ...-law PCM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-25"
  },
  "saoudi90_icslp": {
   "authors": [
    [
     "S.",
     "Saoudi"
    ],
    [
     "J. M.",
     "Boucher"
    ],
    [
     "A. Le",
     "Guyader"
    ]
   ],
   "title": "Optimal scalar quantization of the LSP and the LAR for speech coding",
   "original": "i90_0101",
   "page_count": 4,
   "order": 26,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "The Line Spectrum Pairs (LSP) provide an efficient representation of the synthesis filter used in Linear Predictive Coding (LPC) of speech. In this paper, several algorithms are studied for the quantization of these parameters including a new non uniform adaptive quantization forward method. The performance of the quantization methods of the LSP parameters is studied for a total number B of bits per frame varing from 10 to 40. The optimum bit allocation for each LSP is obtained by the Fox-Makhoul procedure. The proposed method for quantizing the LSP parameters is tested in a CELP coder running at 8 Kbits/s and compared to the optimum quantization of the LAR (Log Area Ratios) coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-26"
  },
  "takahashi90_icslp": {
   "authors": [
    [
     "Shinya",
     "Takahashi"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "4.8 kbps speech coding using frame synchronous time domain compression (FS-TDC)",
   "original": "i90_0105",
   "page_count": 4,
   "order": 27,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "In this paper, we propose a low bit rate (4.0-4.8kbps) speech coding algorithm for digital mobile communication. The coding algorithm is based on a frame synchronous time domain compression (FS-TDC) of LPC residual signal which is optirnaly combined with stochastic coding algorithm. Like other TDC schemes, FS-TDC basically uses pitch periodicity of speech, but different from others, its compression and expansion process are both completed within a frame, and the compression ratio is large and fixed. In evaluation tests, the sochastic coders with FS-TDC shows better speech quality than an usual system, and shows good robustness for channel error if parameters are protected by optimized error protection scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-27"
  },
  "tasaki90_icslp": {
   "authors": [
    [
     "Hirohisa",
     "Tasaki"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Time-domain flexible matrix quantization for very-low-rate speech coding",
   "original": "i90_0109",
   "page_count": 4,
   "order": 28,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "The block quantizers such as a matrix quantizer and a segment quantizer are used in low-bit-rate speech coding systems. It has been demonstrated that block quantizers have good benefits in bit rate when the codeword label is transmitted with duration time information. Although, if we send speech information in fixed bit rate, the quantizer must have some fixed boundaries between quantized matrices, and this limitation induces a great deal of degradation on quantization performance. To solve this problem, we propose a new spectral quantization technique called the time-domain flexible matrix quantization (TFMQ) which based on the fact that listeners are insensitive to the degradation by a small amount of time-warping transformation. Subjective and objective quality evaluations on TFMQ have been performed for 40 short Japanese sentences talked by one male speaker. The results have shown that TFMQ has superoir perceptual performance than conventional matrix quantizers. And it have been shown that the improvement is mainly brougt by the time-warping transformation in the manner of minimizing the matching distortion, and the slight difference in the time-domain structure is absorbed efficiently by this transformation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-28"
  },
  "taniguchi90_icslp": {
   "authors": [
    [
     "Tomohiko",
     "Taniguchi"
    ],
    [
     "Mark",
     "Johnson"
    ],
    [
     "Yasuji",
     "Ohta"
    ]
   ],
   "title": "Multi-vector pitch-orthogonal LPC: quality speech with low complexity at rates between 4 and 8 kbps",
   "original": "i90_0113",
   "page_count": 4,
   "order": 29,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This paper will present the Pitch-Orthogonal LPC speech coding technique. Pitch-Orthogonal LPC codes the residual in a vector excitation speech coder (VXC ) using the projection of the target vector into each of a series of \"codevector planes,\" where each plane is defined by a pair of vectors, the code and the pitch, which have been transformed to make their optimum gain functions independent. We will show how, using the \"backward-transform\" structure, POLPC can be used to take advantage of the low complexity offered by sparse, lattice, sparse-delta, and other efficient codebook structures in a way that standard simultaneous gain optimization VXC (SGOC) can't. Finally, we will show how this same backward-transform structure can be used with multi-stage coding to create low-complexity, jointly optimized multi-stage VXC speech coders, extending the dimension of coding accuracy by extending the codevector plane into a higher-dimensional hyperplane. Simulation results will be given which show that backward-transform POLPC provides quality identical to that of SGOC, at a computational complexity which is less than that of SGOC by a factor roughly proportional to the number of codevectors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-29"
  },
  "shoham90_icslp": {
   "authors": [
    [
     "Yair",
     "Shoham"
    ],
    [
     "Erik",
     "Ordentlich"
    ]
   ],
   "title": "Low-delay code-excited linear-predictive coding of wideband speech at 32 kbps",
   "original": "i90_0117",
   "page_count": 4,
   "order": 30,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "The prospect of multi-channel/multi-user speech communication via the emerging ISDN has raised a lot of interest in advanced coding algorithms for 7KHz wideband speech. The capability of 32Kb/s wideband speech coding will support sustained high-quality stereo (or bilingual) voice transmission for audio-video teleconferencing over a basic rate 64Kb/s ISDN channel. In addition to the high-quality requirement, network applications are likely to dictate the use of algorithms with a very short coding delay. This paper reports on the use of the well known Codebook Excited Linear Predictive (CELP) [1Â»3] algorithm for 32Kb/s low-delay (LD-CELP) coding of wideband speech. The main problem associated with wideband coding, namely, spectral noise weighting, is discussed. The paper proposes an enhanced noise weighting technique and demonstrates its efficiency via subjective listening tests. In these tests, involving 20 listeners and 8 test sentences, the average ratings for the proposed 32Kb/s LD-CELP was essentially equal to that of the 64Kb/s standard (G.722) CCITT wideband coder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-30"
  },
  "unno90_icslp": {
   "authors": [
    [
     "Yoshihiro",
     "Unno"
    ],
    [
     "Makio",
     "Nakamura"
    ],
    [
     "Toshifumi",
     "Sato"
    ],
    [
     "Toshiki",
     "Miyano"
    ],
    [
     "Kazunori",
     "Ozawa"
    ]
   ],
   "title": "11.2 kb/s LCELP speech codec for digital cellular radio",
   "original": "i90_0121",
   "page_count": 4,
   "order": 31,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "A 11.2 kb/s speech codec for digital cellular radio has been developed. The speech coder uses an LCELP (Learned Code Excited LPC Coding) speech coding algorithm operating at 8 kb/s. In LCELP, the excitation signal is efficiently quantized by a two-stage vector quantizer with a learned codebook and a stochastic codebook. LCELP can provide excellent speech quality in the absence of channel errors. In order to maintain good speech quality in the presence of transmission bit errors, 3.2 kb/s of forward error correction and detection coding is applied. Further, improved speech interpolation and extrapolation techniques have been developed. The speech codec has been implemented on a real time hardware using two 16 bit fixed-point DSP chips. The speech quality of the codec is good even under bit errors, and it is equivalent to 40 kb/s ...-law PCM at 1 % bit error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-31"
  },
  "ohya90_icslp": {
   "authors": [
    [
     "Tomoyuki",
     "Ohya"
    ],
    [
     "Hirohito",
     "Suda"
    ],
    [
     "Toshio",
     "Miki"
    ],
    [
     "Shinji",
     "Uebayashi"
    ],
    [
     "Takehiro",
     "Moriya"
    ]
   ],
   "title": "Revised TC-WVQ speech coder for mobile communication system",
   "original": "i90_0125",
   "page_count": 4,
   "order": 32,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "An enhanced version of Transform Coding with a Weighted Vector Quantization (TC-WVQ) speech coder combined with Bit Selective Forward Error Correction (BS-FEC) at 11.2 kbit/s is presented. In this version, speech quality, channel error robustness, computational complexity, and the memory requirements are improved. A real-time operating codec (8.2 MIPS in total) can produce high quality speech in an error free channel. This codec also produces intelligible speech even in a channel of 3% burst error.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-32"
  },
  "noda90_icslp": {
   "authors": [
    [
     "Hideki",
     "Noda"
    ],
    [
     "Masuzo",
     "Yanagida"
    ]
   ],
   "title": "Extraction of phoneme-dependent individuality using HMM-based segmentation for text-independent speaker recognition",
   "original": "i90_0129",
   "page_count": 4,
   "order": 33,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "This paper describes a new method for text-independent speaker recognition which exploits phoneme-dependent voice individuality without direct phoneme recognition. This method uses a segmentation technique based on the Hidden Markov Model (HMM). Appropriate segmentations are expected to be carried out through parameter estimation of models, given enough amount of utterances with their phonetic transcriptions. Segmentations being completed, the difference between feature vectors of reference and input which belong to the same phoneme is used as the dissimilarity measure for speaker recognition. Speaker verification performance has been evaluated by experiments using 20 word utterances of 177 male speakers. In a experiment 95.4% verification rate is achieved using the proposed method, whereas 89.3% by a well-known VQ method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-33"
  },
  "eatock90_icslp": {
   "authors": [
    [
     "J.",
     "Eatock"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "Automatically focusing on good discriminating speech segments in speaker recognition",
   "original": "i90_0133",
   "page_count": 4,
   "order": 34,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "It is recognised that some parts of speech contain more speaker discriminating information than others. The aim here is to automatically identify useful speech segments in order to improve speaker recognition performance. Initially we present a brief overview of our earlier papers on this subject, in which the speech classification idea is first introduced. We show the need for speaker-dependent classification and further examine the use of hierarchical classifiers to improve recognition performance. Finally a majority voting system is introduced, which uses person-specific classifiers in order to automatically reject utterances containing little speaker specific information. We show that this scheme can be used to reduce the identification error rate from 11.7% to 3.2% by the automatic rejection of 38.5% of the test utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-34"
  },
  "matsui90_icslp": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Text-independent speaker recognition using vocal tract and pitch information",
   "original": "i90_0137",
   "page_count": 4,
   "order": 35,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "This paper proposes a new text-independent speaker recognition method based on vector quantization (VQ) using vocal tract and pitch information. The purpose of this research is to create a speaker recognition system robust against the temporal variations of feature parameters. This paper introduces several feature parameters related to both vocal tract and pitch information extracted from spoken vowels, words, and sentences. Interspeaker variability is enhanced, and intraspeaker variability is reduced, by using a new normalization method, Talker Variability Normalization (TVN). A new distance measure, the Distortion-Intersection Measure (DIM), is defined by the size and similarity of the intersection between test vectors and VQ codebook vectors. This proposed method, evaluated using a nine-talker database recorded over three years, achieves 99.0% speaker identification and 98.7% speaker verification accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-35"
  },
  "rosenberg90_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Maureen A.",
     "McGee"
    ]
   ],
   "title": "Experiments in automatic talker verification using sub-word unit hidden Markov models",
   "original": "i90_0141",
   "page_count": 4,
   "order": 36,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "A talker verification system based on characterizing talker utterances as sequences of sub-word units represented by Hidden Markov Models (HMM's) has been implemented and tested. Two types of subword units have been studied, phone-like units (PLU's) and acoustic segment units (ASU's). PLU's are based on phonetic transcriptions of spoken utterances and ASU's are extracted directly from the acoustic signal without use of any linguistic knowledge. The ASU representation has the advantage of not requiring transcriptions of training utterances. Verification performance has been evaluated on a 20-talker database of isolated digit utterances and a 20-talker database of continously spoken sentences drawn from a 1000-word vocabulary. In the isolated digit experiments the verification equal-error rate is approximately 7 to 8% for 1-digit test utterances (approximately 0.5 sec in duration) and 1% or less for 7-digit test utterances (approximately 3.5 sec in duration) with only small differences in performance between PLU- and ASU-based representations. In the continuously spoken sentences experiments using ASU's the best verification performance is 1.7% equal-error rate for 5 second test trials. This is obtained using 64 ASU models trained from 90 seconds of speech. In addition, a technique for updating models, using data from current test utterances, has been devised and implemented. Using this adaptation technique for isolated digits, the error rate falls to 6% for 1-digit utterances and less than 0.5% for 7-digit utterances. The experiments show that excellent verification performance can be obtained with sub-word units represented by HMM's. The techniques can be readily expended from small vocabularies and isolated words to large vocabularies and connected sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-36"
  },
  "koo90_icslp": {
   "authors": [
    [
     "M. W.",
     "Koo"
    ],
    [
     "Chong Kwan",
     "Un"
    ],
    [
     "Hwang Soo",
     "Lee"
    ],
    [
     "J. M.",
     "Koo"
    ],
    [
     "H. R.",
     "Kim"
    ]
   ],
   "title": "A comparative study of speaker adaptation methods for HMM-based speech recognition",
   "original": "i90_0145",
   "page_count": 4,
   "order": 37,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "In this paper, we compare the performances of speaker adaptation algorithms which consist of two stages of processing for an HMM-based speech recognition system. We compare three kinds of VQ adaptation methods which may be used in the first stage to reduce the total distortion error for a new speaker; label prototype adaptation, adaptation with a codebook from adaptation speech itself, and adaptation with a mapped codebook. We then compare the performance of four kinds of HMM parameter adaptation methods which may be used in the second stage to transform HMM parameters for a new speaker; adaptation by the Viterbi algorithm, that by the DTW algorithm, that by the iterative alignment algorithm, and that by the fuzzy mapped codebook algorithm. The results show that adaptation based on the mapped codebook and the DTW algorithm yields the highest accuracy in an HMM-based speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-37"
  },
  "hattori90_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Hattori"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speaker weighted training of HMM using multiple reference speakers",
   "original": "i90_0149",
   "page_count": 4,
   "order": 38,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "This paper proposes a new speaker adaptation method using speaker weights for multiple reference speaker training. The speaker weights are calculated to reflect the similarity of each reference speaker's dynamic features to an input speaker. They are used to have the similarities affect to hidden Markov models. The evaluation experiments are carried out through the /b,d,g,m,n,N/ phoneme recognition task using 8 speakers. Average recognition rates are 68.0%, 66.4%, and 65.6% respectively for three test sets which have different speech styles, that is, word utterances, phrase-by-phrase utterances and continuous utterances. These are 1.6%, 6.7%, and 8.2% respectively higher than the supplemented HMM rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-38"
  },
  "kubala90_icslp": {
   "authors": [
    [
     "Francis",
     "Kubala"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Improved speaker adaptation using multiple reference speakers",
   "original": "i90_0153",
   "page_count": 4,
   "order": 39,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "In this paper we describe a method of rapid speaker adaptation that uses speech from multiple reference speakers to improve performance for large vocabulary continuous speech recognition. This method is an extension of our previous work in which we estimated a speaker transformation between a single reference speaker and the new (target) speaker based on a small sample of speech of each speaker. The transformation is applied to the parameters of a speaker-dependent (SD) phonetic hidden Markov model (HMM) made for the reference speaker to make an adapted model for the target. In the present work, we estimate multiple independent transformations between a set of reference speakers and a single target speaker and then combine the resulting adapted models. We have tested this approach on the DARPA 1000-word Resource Management continuous speech corpus using the standard word-pair grammar of perplexity 60. We used 30 minutes of speech from each of 11 reference speakers to train the reference HMMs. Using 2 minutes of adaptation speech from the target speakers, the average recognition performance is 4.1% word error. This error rate is nearly 40% less than that achieved by adaptation from a single reference speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-39"
  },
  "abe90_icslp": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Statistical study on voice individuality conversion across different languages",
   "original": "i90_0157",
   "page_count": 4,
   "order": 40,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "In this paper we discuss spectrum differences between different languages. This research is motivated by a \"cross-language voice conversion\". The goal of cross-language voice conversion is to preserve the individuality of a speaker's speech when that speaker's utterances are translated and used to synthesize speech in another language. To investigate the spectrum difference caused by language differences, the speech uttered by a bilingual speaker is analyzed. Experimental results are as follows: (l)the size of spectrum space is smaller in the inter-language case (between English and Japanese) than in the inter-speaker case, (2)the overlap of inter-language spectrum space is larger than the overlap of inter-speaker, (3)the unique spectra in English are /r/,/se/,/s/,/f/, and the unique spectra in Japanese are /i/,/u/,/N/, (4)although there is a critical boundary between the unique spectra in English and Japanese spectrum space, Japanese spectrum space contains some spectra which are close to the unique spectra in English, (5) judging from listening tests, the dynamic characteristics also play an important role to characterize a language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-40"
  },
  "matsumoto90_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Hirowo",
     "Inoue"
    ]
   ],
   "title": "A minimum distortion spectral mapping applied to voice quality conversion",
   "original": "i90_0161",
   "page_count": 4,
   "order": 41,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "This paper presents both supervised and unsupervised spectral mapping methods between speakers, and an application to voice conversion based on PSE analysis-synthesis system. In both mapping methods, a spectrum of one speaker is converted to that of another speaker by interpolating the estimated speaker difference vectors at given points in the spectral space. In the supervised method, these speaker difference vectors are estimated minimizing the spectral distortion along the DTW path between the mapped and target spectral sequences for training samples. In the unsupervised method, after mapping the spectral codebook of the target speaker onto the source speaker based on a minimum fuzzy objective function, the source spectra are converted by a fuzzy mapping using the mapped and target code-books. A voice conversion experiment showed that, in male-to-male conversion, both methods attained an average correct score of 84 % in speaker discrimination for the converted voices, and that, in male-to-female conversion, an average correct score of 70 % was obtained for the supervised method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-41"
  },
  "barney90_icslp": {
   "authors": [
    [
     "Anna M.",
     "Barney"
    ],
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "David W.",
     "Thomas"
    ]
   ],
   "title": "Airflow measurement in a dynamic mechanical model of the vocal folds",
   "original": "i90_0165",
   "page_count": 4,
   "order": 42,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "A dynamic mechanical model of the vocal folds and tract has been constructed in order to investigate the effect of periodically interrupting a flow of air along a tube. Hotwire anemometer measurements of the fluid movement within the tract have been made. Behaviour close to the shutters is found to vary greatly across the tract cross-section, having the form of a jet surrounded by a vortex ring which widens as it develops to fill the entire tract cross-section by approximately 4 cm downstream of the shutters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-42"
  },
  "estill90_icslp": {
   "authors": [
    [
     "Jo",
     "Estill"
    ],
    [
     "Noriko",
     "Kobayashi"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Yuki",
     "Kakita"
    ]
   ],
   "title": "A study on respiratory and glottal controls in six western singing qualities: airflow and intensity measurement of professional singing",
   "original": "i90_0169",
   "page_count": 4,
   "order": 43,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "The purpose of this study is twofold: (1) to examine respiratory controls when voice quality was specified, (2) to rate the relative efficiency of each of the voice qualities recorded. One female professional singer, who can consistently control six qualities was the subject of the experiment. Multiple repetitions were made of each quality at four fundamental frequencies: 192, 294, 392, and 587 Hz, as airflow, sound pressure levels, and electroglottographic measurements were recorded. In most of the qualities, air flow rate and acoustic intensity showed high correlation. Glottal efficiency, computed from these aerodynamic and acoustic measures, divided the six singing qualities into two groups: four loud qualities and two soft quantities. In the loud group, glottal efficiency increased with higher frequencies. The soft qualities, on the other hand, showed maximum efficiency in the middle frequency range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-43"
  },
  "imaizumi90_icslp": {
   "authors": [
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Hiroshi",
     "Imagawa"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "A model of dynamic characteristics of the voice source and formant trajectories",
   "original": "i90_0173",
   "page_count": 4,
   "order": 44,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "This paper describes a model of the voice source and of formant trajectories which can be used in developing a high-fidelity speech synthesizer. A polynomial model was used to generate the glottal source. Formant trajectories are modelled as the sum of two kinds of functions: one represents vowel-to-vowel transitions and the other represents the effects of surrounding consonants upon the formants. The intelligibility and fidelity were tested for the speech synthesized based on the model at slow and fast speaking rates. Compared to speech obtained by an analysis-synthesis method, the model slightly improved the intelligibility of vowels at both speaking rates, and of consonants at slow rate. For consonants at fast rate, the model made the intelligibility decrease by 6%. The polynomial model of the glottal source could reproduce to some extent delicate voice quality differences in vowels uttered at various pitch and loudness. It was found that this model is useful as a high-fidelity synthesizer with variable speaking rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-44"
  },
  "nakajima90_icslp": {
   "authors": [
    [
     "Takayuki",
     "Nakajima"
    ],
    [
     "Hiroshi",
     "Ohmura"
    ]
   ],
   "title": "Pole-zero structure based on two-source vocal tract model, PSE inspection of continuous speech vowel part",
   "original": "i90_0177",
   "page_count": 4,
   "order": 45,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "In this paper, at first, Japanese continuous speech vowel parts PSE (power spectrum envelope) are ob- served, and it is pointed out that there is a great difference between the continuous speech vowel PSE and the.PSE of a vowel uttered in isolation. In order to explain the phenomena, a two-source vocal tract model is proposed. In the model, the vocal tract is expressed as a non-uniform single tube including the under glottal portion. Giving the vowel vocal tract area function corresponding to the 5 vowels of Japanese and two other parameters: the source position and the power ratio, vocal tract pole-zero transfer functions are calculated and an articulatory/acoustic nomogram is constructed. As the results of the inspection if there is a correspondence between the PSE of real speech and one of the nomogram's frequency transfer characteristics in pole-zeroes, the authors present the following hypothesis: \"The continuous speech vowel part PSE has the kind of pole-zero structure generated by the two-source vocal tract model\".\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-45"
  },
  "wang90c_icslp": {
   "authors": [
    [
     "Gang",
     "Wang"
    ],
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Nobuo",
     "Nagai"
    ]
   ],
   "title": "Evaluation of speech synthesis using an ARMA estimation and excitation sources",
   "original": "i90_0181",
   "page_count": 4,
   "order": 46,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "It is important how to estimate the coefficients of speech synthesis filter by using a suitable estimation method and determine an adequate excitation input to the synthesis filter to improve the sound quality in speech synthesis. In this paper, we evaluate the sound quality of ARMA speech synthesis with multipulse excitation by signal-to-noise ratio. The ARMA parameters of the synthesis filter are estimated by an adaptive algorithm which smoothes the estimated parameters through modeled time-varying ARMA parameters. The results show that the SNR have been improved in the ARMA synthesis that compared with the AR synthesis by using the same adaptive estimation method. Furthermore, the results suggest the importance of selecting the coefficients of the synthesis filter as the most adequate parameter set in a frame of speech from the estimated parameters by adaptive estimation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-46"
  },
  "iwata90_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Yukio",
     "Mitome"
    ],
    [
     "Jun",
     "Kametani"
    ],
    [
     "Minoru",
     "Akamatsu"
    ],
    [
     "Seimitsu",
     "Tomotake"
    ],
    [
     "Kazunori",
     "Ozawa"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "A rule-based speech synthesizer using pitch controlled residual wave excitation method",
   "original": "i90_0185",
   "page_count": 4,
   "order": 47,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "A Japanese text-to-speech conversion system has been developed, which can generate highly intelligible and natural synthetic speech from an arbitrary text written in Kanji characters (Chinese ideographs) by concatenating CV (C: consonant, V: vowel) and VC speech units. The system consists of a text analysis system and a speech synthesizer, constructed on compact hardware for a personal computer. To generate high quality synthetic speech, a pitch controlled residual wave excitation method is proposed, which uses residual waves as excitation signals for a synthesis filter in all portions of each speech unit. To realize natural rhythms, a phoneme duration rule has been created, based on statistical analysis of a large speech database. Evaluation experiments for the synthesizer were carried out. Results for the 100 syllable articulation test show an 88.8% accuracy rate and results for the 1,000 phonetically balanced word intelligibility test show a 97.4% accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-47"
  },
  "itoh90_icslp": {
   "authors": [
    [
     "Kenzo",
     "Itoh"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Tetsuya",
     "Nomura"
    ],
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "Phoneme segment concatenation and excitation control based on spectral distortion criterion for speech synthesis",
   "original": "i90_0189",
   "page_count": 4,
   "order": 48,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "This paper proposes two new methods based on spectral distortion criteria that produce high quality speech synthesis. One is a phoneme segment selection method using an objective continuity measure, and the other is an excitation signal extraction method for pitch and duration control. The continuity measure is expressed using continuity of the LPC spectrum envelopes. When this measure is used for optimum selection, natural sounding synthetic speech is produced without any smoothing technique. For pitch and duration control, an automatic excitation signal extraction method is proposed that also uses the spectral distortion criteria between original and synthetic speech based on residual excited LPC vocoder. When this new pitch and duration control method is used, the average LPC cepstrum distortion (CD) is decreased from 1.90 dB to 1.01 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-48"
  },
  "pearson90_icslp": {
   "authors": [
    [
     "Stephen D.",
     "Pearson"
    ],
    [
     "Hector R.",
     "Javkin"
    ],
    [
     "Kenji",
     "Matsui"
    ],
    [
     "Takahiro",
     "Kamai"
    ]
   ],
   "title": "Text-to-speech synthesis using a natural voice source",
   "original": "i90_0193",
   "page_count": 4,
   "order": 49,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "Our aim is to improve text-to-speech in its naturalness and its ability to model individual speakers. This paper describes various methods for using inverse-filtered waveforms from natural speech as a voice source in a text-to-speech system. One method uses a repeating loop, and controls pitch by interpolating samples in the waveform. Another method creates a source waveform of the desired pitch by concatenating single pulses from a collection of pulses. Listening tests were carried out to compare these methods with each other and with more traditional voice source generation techniques. The results indicate that these \"natural glottal source\" methods can substantially improve the quality of text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-49"
  },
  "alku90b_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Erkki",
     "Vilkman"
    ],
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "A comparison of egg and a new automatic inverse filtering method in phonation change from breathy to normal",
   "original": "i90_0197",
   "page_count": 4,
   "order": 50,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "The analysis of the glottal excitation in the case when phonation slides from breathy to normal has been studied in this paper. The performance of a developed analysis tool, the AIF-method, has been compared to the results given by the EGG-signal. The AIF-method yielded reliable glottal pulse forms for all the speech material that was studied. Even in the case of very breathy phonation the wave form given by the AIF-method was reliable whereas the information given by the EGG-analysis was not relevant. The correspondence between the EGG-signal and the result obtained by the AIF-method concerning the instant of closure and the maximum opening of the glottal cycle was evident.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-50"
  },
  "kim90_icslp": {
   "authors": [
    [
     "Ki Chul",
     "Kim"
    ],
    [
     "Hyun Soo",
     "Yoon"
    ],
    [
     "Jung Wan",
     "Cho"
    ]
   ],
   "title": "Enhanced parametric representation using binarized spectrum",
   "original": "i90_0201",
   "page_count": 4,
   "order": 51,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "This paper describes an enhanced parametric representation for all-pole models of speech useful for speech recognition. The enhanced parametric representation, mcl-frequency peak coefficients, utilizes the concepts of spectral peak weighting and mel-scale integration in the frequency domain. The speaker-dependent syllable recognition results show that mcl-frequency peak coefficients with the Euclidian distance measure offers improved performance in comparison to LPC and mel-cepstrum with root power sum distance measure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-51"
  },
  "asai90_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Asai"
    ],
    [
     "Shigeru",
     "Chiba"
    ]
   ],
   "title": "Voiced-unvoiced classification using weighted distance measures",
   "original": "i90_0205",
   "page_count": 4,
   "order": 52,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "Voiced-Unvoiced Classifications of Japanese speech are discussed. Voiced-unvoiced classification of short frames using the criteiria of weighted distance is proposed by Atal [1]. We adopted these methods to the Japanese speech, and reached the conclusion that the bias terms of determinants of the weights are significant for Japanese speech. We propose several mothods with regard to this. We tried several modifications of the Atal's method, and got considerably better results than direct application to Japanese speech data. In these methods it is assumed that the distributions of these parameters have normal distributions, where the Bays-like likelihood ratio tests are efficient. However, when the distributions of the parameters in the unvoiced class are checked precisely, these distributions apparently have two peaks. This breaks the assumption for the Bays-like likelihood ratio tests. The relation between the methods of classifiction and the fact that unvoiced parameters are distributed in two peaks is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-52"
  },
  "miki90_icslp": {
   "authors": [
    [
     "Kei",
     "Miki"
    ]
   ],
   "title": "Phoneme recognition using a hierarchical time spectrum pattern",
   "original": "i90_0209",
   "page_count": 4,
   "order": 53,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "This paper describes a new vector-quantization-based phoneme recognition method which uses the hierarchical time spectrum pattern (TSP). The phonetic feature is discussed by a mutual information and a posteriori probability between vector quantization codes and phoneme label codes. The TSP of Mel-scaled LPC cepstra and the power-change pattern (PCP) are used as acoustic parameters. Input speech is firstly vector-quantized by the PCP codebook. Secondly it is vector-quantized by the TSP of which the codebooks are classified by the PCP-VQ code. Hierarchical TSP-VQ improves performance of phoneme classification compared with only the TSP-VQ. A frame-label matching experiment on a speaker-independent condition with the JEIDA Japanese speech database of connected 4-digit uttered by 16 males and 16 females, shows 79.4% of recognition accuracy using the method. The experimental result indicates that the proposed method is highly effective.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-53"
  },
  "sato90_icslp": {
   "authors": [
    [
     "Susumu",
     "Sato"
    ],
    [
     "Takeshi",
     "Fukabayashi"
    ]
   ],
   "title": "Recognition of plosive using mixed features by fisher's linear discriminant",
   "original": "i90_0213",
   "page_count": 4,
   "order": 54,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "We investigated to use mixed features obtained with K-L (Karhunen-Loeve) transform and Fisher's linear discriminant (discriminant analysis) for the recognition of plosives. In the experiment of speaker-independent plosive recognition using the mixed features obtained from LPC cepstrum and PARCOR coefficients, the recognition rate improved in the rate of 3% as compared with that using the features from LPC cepstrum coefficients. Also, in the experiment using the mixed features obtained from spectrum and PARCOR coefficients, the recognition rate improved in the rate of 3% as compared with that using the features from spectrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-54"
  },
  "ando90_icslp": {
   "authors": [
    [
     "Akio",
     "Ando"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Clustering algorithms to minimize recognition error function and their applications to the vowel template learninig",
   "original": "i90_0217",
   "page_count": 4,
   "order": 55,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "Clustering is one of the most prevalent methods to construct multi-templates from training data. However, most of clustering algorithms proposed in the literature aim at minimizing not the recognition error, but the square error distortion measure. This paper describes a new clustering algorithm which optimally classifies training data into clusters in such a way that it minimizes a recognition error function. Optimization is accomplished by the simulated annealing technique. The new algorithm is compared with the LBG clustering algorithm and the LVQ2 algorithm in vowel template learning experiments, and confirmed to yield the best results. This paper also investigates a possibility of using an iterative improvement method as an optimization technique in the new algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-55"
  },
  "wang90d_icslp": {
   "authors": [
    [
     "Changfu",
     "Wang"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Chinese four tone recognition based on the model for process of generating F0 contours of sentences",
   "original": "i90_0221",
   "page_count": 4,
   "order": 56,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "This paper proposes a new method of recognizing the four tones, the method is based on the functional model for the process of generating F0 contours of sentences, so our method is different from proposed others, it is applied to the four tone recognition not only for monosyllables, but also for disyllables, phrases and even sentences. This method is adaptable to every speaker with a simple training procedure using only \"MA\" four tones. It can recognize the lexical tones using a few parameters. The recognition accuracy is over 99% for monosyllables, 93% for disyllables and idioms of four-syllable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-56"
  },
  "kim90b_icslp": {
   "authors": [
    [
     "N. S.",
     "Kim"
    ],
    [
     "Chong Kwan",
     "Un"
    ]
   ],
   "title": "Generalized training of hidden Markov model parameters for speech recognition",
   "original": "i90_0225",
   "page_count": 4,
   "order": 57,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "In this paper, we investigate training algorithms of hidden Markov model(HMM) parameters for speech recognition. Here the speech recognition problem is thought of as multicategory hypotheses testing based on the HMM method. The criterion used in a training algorithm is classified into two classes; single and multicategory criterion. We study the maximum likelihood estimation(MLE) and unification estimation(UE) methods which use single category criterion, and the maximum mutual information estimation(MMIE) and corrective estimation(CE) methods which use multicategory criterion. Also, we propose a hybrid method of training and a method of variable weighting to each category. In addition, we compare the performances of these training methods by a confusable-phoneme recognition experiment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-57"
  },
  "kawahara90_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Toru",
     "Ogawa"
    ],
    [
     "Shigeyoshi",
     "Kitazawa"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Phoneme recognition by combining Bayesian linear discriminations of selected pairs of classes",
   "original": "i90_0229",
   "page_count": 4,
   "order": 58,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "A new phoneme recognition method based on Bayesian linear discrimination (BLD) is presented. The conventional BLD lowers the performance as the number of classes to be discriminated becomes larger. To overcome this defect, we propose the pair-wise discrimination method which combines BLDs of the pairs of the classes. A given sample is recognized as the class which is supported or is not denied by most pairs. This method realizes high accuracy recognition but needs much computation and storage. Therefore, an algorithm to select effective pairs for overall discrimination is discussed. To measure the effectiveness of each pair, we count the occurrence of the first candidate and the second candidate combination pairs for all the samples and eliminate those pairs whose occurrence is rare. Thus we could reduce the necessary pairs for discrimination to about one fourth within 1% accuracy decrease.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-58"
  },
  "atkins90_icslp": {
   "authors": [
    [
     "S.",
     "Atkins"
    ],
    [
     "P.",
     "Kenne"
    ],
    [
     "D.",
     "Landy"
    ],
    [
     "S.",
     "Nulsen"
    ],
    [
     "M.",
     "O'Kane"
    ]
   ],
   "title": "WAL - a speech recognition programming language",
   "original": "i90_0233",
   "page_count": 4,
   "order": 59,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "The Wave Analysis Language (WAL) is a functional programming language which is primarily designed to allow rapid writing, testing and modification of speech-to-phonetics recognition rules. WAL has an extremely simple production-rule syntax which is easy for non-programmers to learn and use. The simplicity and innate modularity of the WAL syntax is accompanied by a rich semantic expressive capacity for capturing and combining those features of a speech waveform and various associated signal processing derivations that indicate the presence of whatever phoneme of phonetic class is under consideration.\n",
    "The result of the recognition rule firings can be examined interactively through a flexible signal processing package into which WAL has been embedded. Rules can also be batched to run over a labelled speech database with the rules being run in inverse mode to provide an explanation for mis-firings. Output from WAL can also be produced in the form of a string of phonetic labels which can be considered either with or without time alignment to the original speech waveform. This phonetic string can be passed to other components of a speech recognition or understanding system. Thus WAL is used both as a recognition rule development tool and as an integral component of a large speech recognition/understanding architecture. Rules in WAL can be combined explicitly using the various temporal reasoning primitives provided ('and', 'or', 'not', 'then', 'before', 'after') or implicitly using a context-tree mechanism which allows for hierarchical overwriting. Rules can be either categorical or probabilistic. WAL includes the notion of 'rubber-templates' i.e. a set of primitives for describing shapes and their deformations. This idea, together with the ability to perform temporal reasoning, provides a powerful extension to picture languages and their use in speech recognition. The original version of the language was implemented in Prolog and has now been reimplemented in C. It and the associated signal processing package are now maintained both under MS-DOS and Unix.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-59"
  },
  "rossi90_icslp": {
   "authors": [
    [
     "Mario",
     "Rossi"
    ]
   ],
   "title": "Automatic segmentation: why and what segments?",
   "original": "i90_0237",
   "page_count": 4,
   "order": 60,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "I present and discuss the SAPHO (Segmentation by Acoustico-Phonetic knowledge) model implemented in the AWk language under the Unix system on a MASSCOMP computer. The system is devised as a speaker independent ASS (automatic speech segmentation), by previous recognition of the phonetic articulation manner. In all the ASR systems the phonetic knowledge is at least implicitely used. It has to be referred to explicitely. Following the Level Building procedure SAPHO supplies a hierarchized set of acoustic properties and segments, and phonetic properties and segments which fit the phonetic parsing of the acoustic wave. The amenability of this system is entailed by its modularity which allows a possible further architecture as distributed tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-60"
  },
  "makino90_icslp": {
   "authors": [
    [
     "Shozo",
     "Makino"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Mitsuru",
     "Endo"
    ],
    [
     "Ken'iti",
     "Kido"
    ]
   ],
   "title": "A Japanese text dictation system based on phoneme recognition using a modified LVQ2 method",
   "original": "i90_0241",
   "page_count": 4,
   "order": 61,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "A Japanese text dictation system has been developed based on phoneme recognition and a dependency grammar. The phoneme recognition is carried out using a modified LVQ2 method proposed. A linguistic processor is composed of a Bunsetsu-unit(Japanese Phrase) spotting processor and a syntactic processor with semantic constraints. In the Bunsetsu-unit spotting processor, using a syntax driven continuous DP matching algorithm the Bunsetsu-units are spotted from a recognized phoneme sequence and then a Bunsetsu lattice is generated. In the syntactic processor, the Bunsetsu lattice is parsed based on the dependency grammar. The dependency grammar is expressed as the correspondence between a FEATURE marker in a modifier-Bunsetsu and a SLOT-FILLER marker in a head-Bunsetsu. Recognition scores of the Bunsetsu-unit and phoneme were 73.2% and 86.1% for 113 sentences uttered by each of two male speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-61"
  },
  "mizuta90_icslp": {
   "authors": [
    [
     "Shinobu",
     "Mizuta"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "An optimal discriminative training method for continuous mixture density HMMs",
   "original": "i90_0245",
   "page_count": 4,
   "order": 62,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "In this paper, we describe a training method for continuous mixture density HMM parameters, called optimal discriminative training. Conventional maximum likelihood estimation method for HMM training has a problem that the recognition performance is not considered in the training procedure. To solve the problem, a corrective training method has been already-proposed, but this method is applied to discrete HMMs, so the trained HMMs cannot avoid undesirable effects of VQ distortion. In this paper, the optimal discriminative training method (ODT) is described, appling the the basic concept of the corrective training to continuous mixture density HMMs, better recognition performance can be obtained as avoiding the VQ distortion. Prom word recognition experiments, we discuss the way to optimize each parameters of this training method, and by using the optimum value of parameters, we show the effectiveness of this method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-62"
  },
  "datta90_icslp": {
   "authors": [
    [
     "S.",
     "Datta"
    ],
    [
     "M.",
     "Al-Zabibi"
    ]
   ],
   "title": "Discrimination of words in a large vocabulary speech recognition system",
   "original": "i90_0249",
   "page_count": 4,
   "order": 63,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "Word discrimination according to broad phonetic classes is presented in this paper. Different Phonetic classification strategies are used to describe large vocabulary lexicons. The phonetic description in these strategies varies from 2 to 17 phonetic classes. The statistical results show that about 83% of the 10,000 test Arabic words can be uniquely represented by using 7 broad phonetic classes for consonants and six classes for vowels. In this case, the maximum number of words having the same phonetic labelling is 6. The paper summarises the results of ten different phonetic classification schemes and discusses their implication for a large vocabulary speech recognition system. Distributions of vowels and consonant classes are also presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-63"
  },
  "koo90b_icslp": {
   "authors": [
    [
     "J. M.",
     "Koo"
    ],
    [
     "Chong Kwan",
     "Un"
    ],
    [
     "Hwang Soo",
     "Lee"
    ],
    [
     "H. R.",
     "Kim"
    ],
    [
     "M. W.",
     "Koo"
    ]
   ],
   "title": "A recognition time reduction algorithm for large-vocabulary speech recognition",
   "original": "i90_0253",
   "page_count": 4,
   "order": 64,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "We propose an efficient pre-classification algorithm extracting candidate words to reduce the recognition time in a large-vocabulary recognition system and also propose the use of spectral and temporal smoothing of the observation probability to improve its classification performance. The proposed algorithm computes the coarse likelihood score for each word in a lexicon using the observation probabilities of speech spectra and duration information of recognition units. With the proposed approach we could reduce the computational amount by 74% with slight degradation of recognition accuracy in a 1160-word recognition system based on the phoneme-level HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-64"
  },
  "kim90c_icslp": {
   "authors": [
    [
     "Hyung Soon",
     "Kim"
    ],
    [
     "Chong Kwan",
     "Un"
    ]
   ],
   "title": "Speech recognition method based on the dual processing nature of speech perception",
   "original": "i90_0257",
   "page_count": 4,
   "order": 65,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In this paper, we propose a speech recognition method based on the dual processing nature of speech perception, where vowel-like sounds and nonvowel-like sounds are processed differently. For this purpose, vowel-like portions and nonvowel-like portions of the speech signal are segmented first, and different distance measures are applied to each of them. In order to segment input utterance into vowel-like and nonvowel-like intervals, we also propose a segmentation algorithm based on a set of descriptive features derived from the auditory filter bank output. According to our recognition experiments, a reduction of over one third in recognition errors is possible with the proposed method in comparison with the conventional method. This method is particularly useful when a broad phonetic classifier is employed as a front-end stage for reducing the number of candidate words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-65"
  },
  "shinoda90_icslp": {
   "authors": [
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Ken-ichi",
     "Iso"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Speaker adaptation for demi-syllable based speech recognition using continuous HMM",
   "original": "i90_0261",
   "page_count": 4,
   "order": 66,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "A novel speaker adaptation method, which is applied to the demi-syllable based speech recognition system using continuous density HMM, is proposed. In this method, mean vectors of HMM Gaussian pdfs for a standard speaker are adapted to those for a new speaker with a small amount of training data. Supervised speaker adaptation is first employed, and for the recognition units which are not adapted in the supervised adaptation, unsupervised speaker adaptation is performed. The effectiveness of the proposed method was confirmed by large vocabulary word recognition experiments. Using 50 word utterances for speaker adaptation, the recognition rates were improved by 14.4 %, on an average.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-66"
  },
  "skinner90_icslp": {
   "authors": [
    [
     "Toby",
     "Skinner"
    ]
   ],
   "title": "Speech signal processing on a neurocomputer",
   "original": "i90_0265",
   "page_count": 4,
   "order": 67,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Neural networks offer the potential to solve many classification problems, such as those associated with speech signals. Adaptive Solutions is developing a powerful neurocomputer that is capable of on-chip learning. The speed, programmability, and flexibility of the architecture enables the neurocomputer to efficiently perform feature extraction as well as classification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-67"
  },
  "ono90_icslp": {
   "authors": [
    [
     "Shigeru",
     "Ono"
    ]
   ],
   "title": "Syllable structure parsing for continuous speech recognition",
   "original": "i90_0269",
   "page_count": 4,
   "order": 68,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper describes a scheme to deal with allophonic and coarticulatory variations for phoneme-based continuous speech recognition and a probabilistic algorithm to parse syllable structure from acoustic speech realizations. In the scheme, phonological objects are represented in terms of \"syllable features \"-syllable positions- and \"phoneme features\" - distinctive features-, and they are organized within hierarchical structures. The constituent features of the structures are associated with the acoustic realizations through probabilistic measure. In the algorithm, syllable structure is parsed from the acoustic realizations by applying the acoustic-phonological constraints and the collocational restrictions involved in the internal constituent features. Performance results for 15 test sentences spoken by 5 male speakers that phonemes are recognized at 90.5% accuracy, and syllable structure is parsed at 79.7% accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-68"
  },
  "tsuboi90_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Tsuboi"
    ],
    [
     "Hiroshi",
     "Kanazawa"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ]
   ],
   "title": "An accelerator for high-speed spoken word-spotting and noise immunity learning system",
   "original": "i90_0273",
   "page_count": 4,
   "order": 69,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "An accelerator utilizing four digital signal processors (DSPs) has been developed to facilitate real-time speech recognition. The accelerator has been implemented in a real-time robust speaker-independent word recognition system. This system employs word-spotting based on Noise Immunity Learning to avoid word boundary detection errors and to increase recognition accuracy in noisy environments. The accelerator board, including four DSPs with shared memory, has 132 MFLOPS peak performance. Since more than 90% of the computational load is inner product calculation, the DSPs share a vocabulary for the purpose of load-balancing. The architecture of the accelerator consists of off-the-shelf components connected in such a way for improved performance in the application. The speed of a single accelerator was shown to be approximately 20 times faster than that of a current high speed workstation with 32 MFLOPS peak performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-69"
  },
  "sharrif90_icslp": {
   "authors": [
    [
     "Zainul Abidin Md.",
     "Sharrif"
    ],
    [
     "Masuri",
     "Othman"
    ],
    [
     "Mohammad Ibrahim AKB",
     "Maiden"
    ]
   ],
   "title": "Recognition of standard malaysian language pronunciation",
   "original": "i90_0277",
   "page_count": 4,
   "order": 70,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "In line with the Malaysian Government policy of implementing standard Malay language pronunciation in schools and institutions of higher learning, research on the recognition of spoken Malay language is currently being pursued at the National University of Malaysia.The purpose of this research is to develop a computerised system which can recognize objectively standard Malay language. Uttered Malay words are sampled and broken into phonemes. The voiced and unvoiced phonemes are stored using different parameters. The zero-crossing rate and average magnitude of the signal are used to build the template for the unvoiced phonemes, while the normalized time sampled signal over a selected pitch period and the duration of the phonemes are being used as parameters for the voiced template.In the recognition part, Dynamic Time Warping (DTW) algorithm is used to compare between the voiced template and the input voiced samples. The recognition is done by comparing the sample within the pitch period for the voiced part. However in the case of unvoiced recognition, it is done through the computation of zero-crossing and average magnitude. A database of Malay words is used to compare the recognised words with all the words in the database. A decision as to whether the sound of the uttered word is in accordance with the standard Malay language is achieved through this comparison.Work is actively being undertaken to port the developed system from the IBM PC-AT to the TMS320-based system [1].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-70"
  },
  "djoudi90_icslp": {
   "authors": [
    [
     "M.",
     "Djoudi"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "The SAPHA acoustic-phonetic decoder system for standard Arabic",
   "original": "i90_0281",
   "page_count": 4,
   "order": 71,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "We present in this paper the SAPHA system is developed for the purpose of the acoustic-phonetic decoding of standard Arabic. First of all, we give a description of the general architecture of the system and the various modules which compose it. Afterwards, we develop the principal stages of the Arabic decoder, namely : - segmentation of the speech signal into large phonetic classes. - automatic extraction of the parameters that are pertinent to phonetic recognition of standard Arabic. - multispeaker identification of phonemes in continuous speech by using an expert system based on production rules. The results obtained in broad phonetic classfication and phoneme labelling for three male speakers are also presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-71"
  },
  "bodden90_icslp": {
   "authors": [
    [
     "Markus",
     "Bodden"
    ]
   ],
   "title": "A concept for a cocktail-party-processor",
   "original": "i90_0285",
   "page_count": 4,
   "order": 72,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "This paper proposes a speech enhancement method that has no restrictions with regard to sound-field conditions or signal characteristics. It is specially designed to model the Cocktail-Party-Effect, that is, to suppress interfering speech signals, which is the most challenging problem concerning speech enhancement. This important aim is achieved by involving the knowledge of binaural signal processing of the human auditory system. Combining tools that simulate binaural signal processing with a powerful, adapted noise cancelling algorithm leads to a new concept for a unique system. The individual steps of the processing scheme are described and promising, preliminary results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-72"
  },
  "usagawa90_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Usagawa"
    ],
    [
     "Yuji",
     "Morita"
    ],
    [
     "Masanao",
     "Ebata"
    ]
   ],
   "title": "Remote control system using speech-reduction of known noise",
   "original": "i90_0289",
   "page_count": 4,
   "order": 73,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "Surrounding noise seriously affects the performance of speech recognition which can operate under usual noise environment. When we try to make an audio equipment or television set which can be controlled by speech, the sound radiated by the equipment itself also affects the performance of speech recognition. In this study, we propose the method to reduce the latter type noise, a priori known noise. The exponential step normalized LMS algorithm is used for the adaptation of FIR digital filter. Also the method of speech candidate detection is presented using the characteristics of adaptive filter. The experiments are carried out under usual room condition with the several broadcasted sound as noise. The total reduction of noise varies from 15dB to 25dB due to the difference of noise type. There is the correlation between the spectrum of noise and the obtained filter coefficients on adaptive filter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-73"
  },
  "takizawa90_icslp": {
   "authors": [
    [
     "Yumi",
     "Takizawa"
    ],
    [
     "Masahiro",
     "Hamada"
    ]
   ],
   "title": "Lombard speech recognition by formant-frequency-shifted LPC cepstrum",
   "original": "i90_0293",
   "page_count": 4,
   "order": 74,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "To perform speech recognition in a noisy environ- ment, it is important to minimize the influences of both additive noise and the Lombard effect. Data analysis clearly showed a trend of formant frequency shift in some specific frequency bands due to the Lombard effect. This paper reports a method of modifying the LPC cepstrum to compensate for the formant frequency shift, which had usually been left uncorrected. The compensated value can be obtained by multiplication of the formant frequency shift value and the partial differential of the LPC cepstrum with respect to the formant-frequency. To validate the effect of the compensation method proposed herein, the following items were studied. (1) Comparison to the usual method (Compensation of spectral tilt). (2) Comparison to the weighted cepstrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-74"
  },
  "matsumoto90b_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Hirokazu",
     "Mitsui"
    ]
   ],
   "title": "A robust distance measure based on group delay difference weighted by power spectra",
   "original": "i90_0297",
   "page_count": 4,
   "order": 75,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "This paper proposes a weighted group delay distance measure (WGD) for noisy speech recognition. The WGD distance measure approximates an absolute group delay spectral distance weighted by the power spectra, which is sensitive to lower formant peaks and insensitive to the global spectral slope variation. The recognition performance of WGD was evaluated through speaker dependent and independent word recognition tests in additive white Gaussian and low-pass filtered noise. The results showed that WGD not only attains significant gains on robustness to additive noise over WLR and RPS, but also maintains the same robustness to interspeaker variation as WLR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-75"
  },
  "yegnanarayana90_icslp": {
   "authors": [
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "Hema A.",
     "Murthy"
    ],
    [
     "V. R.",
     "Ramachandran"
    ]
   ],
   "title": "Speech enhancement using group delay functions",
   "original": "i90_0301",
   "page_count": 4,
   "order": 76,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "A method of processing noisy speech to enhance spectral features corresponding to the vocal tract system is presented. A new method of extracting pitch from noisy speech data is also presented. These methods depend on processing the Fourier transform phase through group delay functions. The basic idea used in these methods is that the features of noise and an all-pole system are distinct in the group delay function. To enhance the spectral features of the vocal tract system a modified group delay function is derived by suppressing the features corresponding to noise from the standard group delay function. Similar ideas are used to extract the periodic component corresponding to pitch from the magnitude spectrum. Performance of these methods is demonstrated for noisy speech data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-76"
  },
  "wang90e_icslp": {
   "authors": [
    [
     "Hong",
     "Wang"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Recovery of reverberated speech using multi-microphone sub-band envelope estimation",
   "original": "i90_0305",
   "page_count": 4,
   "order": 77,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "A new approach of recovering acoustically reverberated signals using Multi-microphone Sub-Band Envelope Estimation (M-SBEE) is proposed. The transfer functions between sound source and microphones have different zero distributions in the z-plane. This method takes the advantage of the differences of the zero distributions by chosing the best behaved microphone for each sub-band, and reconstruct the full band signal using the best microphone in each sub-band. The recovered speech signals using 7 microphones with 256 sub-bands are perfect when the reverberation time(RT) is less than 4.3s, is good enough for RT<0.85s, and is reasonably good for even RT=1.07s. The effects of the number of micro-phones and the size of the microphone array are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-77"
  },
  "marchal90_icslp": {
   "authors": [
    [
     "Alain",
     "Marchal"
    ],
    [
     "Marie-HÃ©lÃ¨ne",
     "Casanova"
    ],
    [
     "P.",
     "Gavarry"
    ],
    [
     "M.",
     "Avon"
    ]
   ],
   "title": "DISPE: a divers' speech data-base",
   "original": "i90_0309",
   "page_count": 4,
   "order": 78,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "Gas mixture and pressure modify the spectral characterictics of divers' speech. Additionally, constraints imposed on jaw movements by wearing a facial mask affect the speech production process. The auditory feed-back loop is equally concerned. Furthermore, underwater adverse working conditions are characterised by noise from different sources.\n",
    "As a result, divers' speech is poorly intelligible and communications between divers and surface control need to be enhanced. This is clearly true for both security and task efficiency reasons. To this end, \"voice unscramblers\" are being used. However, the technological state of commercially available equipment is dated and the quality of speech remains insufficient.\n",
    "To help with the design, testing and qualification (NORM) of new communication devices, a bilingual (French-English) Data-base is currently being set up. It consists of phonetically balanced lists of 200 words read by 17 divers under sea and in chambers at operational levels from the surface to -300m. These recordings will be edited, labelled and stored for further distribution on a CD-ROM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-78"
  },
  "carlson90_icslp": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "BjÃ¶rn",
     "GranstrÃ¶m"
    ],
    [
     "Sheri",
     "Hunnicutt"
    ]
   ],
   "title": "Lexical components in rule-based speech systems",
   "original": "i90_0313",
   "page_count": 4,
   "order": 79,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "Research involving lexical components of both text-to-speech and recognition systems is of vital importance for their total performance. Lexicons vary in content dependent on the theory behind the systems and their proposed use. For a number of years, we have been working with lexica for different European languages as part of our multi-language text-to-speech project. However, the need for more detailed lexical descriptions is increasing in our projects. We have chosen to base our current efforts on a morphological component, developed at the University of Helsinki. This component, based on many different kinds of corpora, has a very good coverage of running text. The morph dictionary has been transcribed with the help of the KTH text-to-speech system. A partial morphological analysis of native names has proven to be efficient ito predict the pronunciation of names in a telephone directory. We will also report on a project where we have applied knowledge about lexical structure to facilitate lexical search for motorically disabled or non-speaking computer users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-79"
  },
  "ceder90_icslp": {
   "authors": [
    [
     "Ken",
     "Ceder"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "The integration of linguistic levels in a text-to-speech conversion system",
   "original": "i90_0317",
   "page_count": 4,
   "order": 80,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "An interactive environment, implemented in Prolog, has been designed for developing applications and conducting research in text-to-speech conversion. A special notational formalism has been devised for relating linguistic knowledge to the acoustic parameters which signal prosodic information. In the present paper it is shown how this formalism can be used for the implementation of some models of Swedish prosody as e.g., the power-law model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-80"
  },
  "shimizu90_icslp": {
   "authors": [
    [
     "Tohru",
     "Shimizu"
    ],
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "The linguistic processing module for Japanese text-to-speech system",
   "original": "i90_0321",
   "page_count": 4,
   "order": 81,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "A linguistic processing module for Japanese text-to-speech system for use on personal computers has been developed. The linguistic processing module analyzes Japanese sentences and outputs information for the rule-based speech synthesizer. This module is implemented as software for 16-bit personal computers, running under the operating system MS-DOS. The linguistic processing module has the following two characteristics. (1) Both a morphological analysis module and a Kana-Kanji conversion module are available to analyze input texts, and the editing module can modify the output of these two modules. (2) The syntactic analysis which is introduced in morphological analysis decreases the number of word candidates, and consequently the system has high performance for selection of correct words. The result of the evaluation tests shows that the accuracy of the morphological analysis is 99.0% for the sample articles of a popular economical newspaper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-81"
  },
  "yamaguchi90_icslp": {
   "authors": [
    [
     "Yukiko",
     "Yamaguchi"
    ],
    [
     "Tatsuro",
     "Matsumoto"
    ]
   ],
   "title": "A neural network approach to multi-language text-to-speech system",
   "original": "i90_0325",
   "page_count": 4,
   "order": 82,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "This paper describes a neural network approach to phrase/clause boundary detection and letter-to-phoneme conversion that depend largely on the target language specification. The phrase/clause boundary detection network is provided with the parts of speech of three consecutive words and determines whether there is a phrase/clause boundary between the first and second words. The letter-to-phoneme conversion network converts input letters to distinctive features of the phonemes. The performance evaluation for English shows that the phrase/clause boundary detection network, trained with 500 sentences and tested with another 500 sentences, correctly detected the phrase/clause boundaries with 95% accuracy, and the letter-to-phoneme conversion network, trained with 1000 words and tested with another 1000 words, converted letters to phoneme correctly with 85% accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-82"
  },
  "fujisaki90_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Yasuharu",
     "Asano"
    ]
   ],
   "title": "Proposal and evaluation of a new type of terminal analog speech synthesizer",
   "original": "i90_0329",
   "page_count": 4,
   "order": 83,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "Because of simplification in the realization of both the vocal tract transfer functions and the excitation sources, certain limitations exist in the quality of speech synthesized by conventional terminal analog speech synthesizers. In order to realize high quality synthesis of speech, a new type of terminal analog synthesizer has been developed consisting of four paths of cascade connection of pole/zero filters and three types of source waveform generators. The four separate paths simulate the vocal tract transfer functions of the four different speech categories: the vowels and vowel-like sounds, the nasal murmur and the buzz bar, the frication and the plosion. Configuration of each path has been decided based on the results of analysis of natural utterances. The generators produce voicing source waveforms, white Gaussian noise waveforms, and impulse-like waveforms. The quality of synthesized speech, especially stop consonants, has indicated the advantage of the proposed synthesizer over the conventional ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-83"
  },
  "malsheen90_icslp": {
   "authors": [
    [
     "Bathsheba J.",
     "Malsheen"
    ],
    [
     "Mariscela",
     "Amador-Hernandez"
    ]
   ],
   "title": "The interrelationship of intelligibility and naturalness in text-to-speech",
   "original": "i90_0333",
   "page_count": 4,
   "order": 84,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "Although high-quality synthesis-by-rule systems produce consonants which are easily and readily distinguishable in controlled tests, these same segments often sound unnatural in running speech. Many have attributed this lack of segmental naturalness to rules which produce \"overcued\" or \"overarticulated\" consonants. In human speech, however, it has been found that consonantal differences are often cued by the quality of adjacent vowel allophones. This paper will discuss how both the naturalness and intelligibility of consonants can be enhanced by incorporating a large number of vowel allophones into a text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-84"
  },
  "hirokawa90_icslp": {
   "authors": [
    [
     "Tomohisa",
     "Hirokawa"
    ],
    [
     "Kazuo",
     "Hakoda"
    ]
   ],
   "title": "Segment selection and pitch modification for high quality speech synthesis using waveform segments",
   "original": "i90_0337",
   "page_count": 4,
   "order": 85,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "We propose a new method for speech synthesis that concatenates waveforms selected from a waveform dictionary. The method uses a modified PSOLA technique to alter the pitch of waveforms selected from a dictionary. The limits of acceptable pitch shifts are determined by preference tests. To make segment selection more accurate, we introduce a new factor which considers the spectral continuity across voiced phoneme boundaries. The average spectral difference is reduced from 5.4dB to 2.7dB and the synthesized voice is more fluent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-85"
  },
  "takeda90_icslp": {
   "authors": [
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Katsuo",
     "Abe"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "On the unit search criteria and algorithms for speech synthesis using non-uniform units",
   "original": "i90_0341",
   "page_count": 4,
   "order": 86,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "A selective use of non-uniform synthesis units for speech synthesis-by-rule is discussed focusing on an optimal unit selection method. In this paper, we propose two algorithms for unit selection. The first one uses one total measure reflecting contextual similarities and adequacy of unit concatenation. The second one combines top down control for concatenation points and bottom up search for the appropriate speech template. The high quality of both selection methods, compared to the conventional method using fixed units, is confirmed by both subjective and objective tests. Furthermore, the results of intelligibility tests are analyzed aiming at designing a quantitative measure to evaluate unit suitability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-86"
  },
  "shirai90_icslp": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Y.",
     "Sato"
    ],
    [
     "K.",
     "Hashimoto"
    ]
   ],
   "title": "Speech synthesis using superposition of sinusoidal waves generated by synchronized oscillators",
   "original": "i90_0345",
   "page_count": 4,
   "order": 87,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "In this paper, a new speech synthesis method is proposed. The voiced speech has the line spectrum structure and is represented by the superposition of sinusoidal waves which are generated by a group of mutually synchronized oscillators. This method has some features as follows. (1) Voiced and unvoiced sounds can be generated in a same framework to operate sinusoidal oscillators in parallel. (2) Since the phase and power information of each sinusoidal wave can be easily controlled, if necessary, periodic waveforms in the voiced sounds can be precisely reproduced in the time domain. (3) The pitch frequency and phoneme duration can be easily changed without much degradation of original sound quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-87"
  },
  "rainton90_icslp": {
   "authors": [
    [
     "David",
     "Rainton"
    ],
    [
     "S. J.",
     "Young"
    ]
   ],
   "title": "Time-frequency spectral analysis of speech",
   "original": "i90_0349",
   "page_count": 4,
   "order": 88,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "In recent years there has been a growing interest amongst the speech research community into the use of spectral estimators which circumvent the traditional quasi-stationary assumption and provide greater time-frequency (t-f) resolution than conventional spectral estimators, such as the short time Fourier power spectra (STFPS). One distribution in particular, the Wigner distribution (WD), has attracted considerable interest. However, experimental studies have indicated that, despite its improved t-f resolution, employing the WD as the front end of a speech recognition system actually reduces recognition performance; only by explicitly re-introducing t-f smoothing into the WD are recognition rates improved. By re-formulating the spectral estimation problem in terms of a bias variance optimisation task, we provide an explanation for these previous experimental findings.\n",
    "A practical adaptive smoothing algorithm is introduced, which attempts to match the degree of smoothing introduced into the WD with the time varying quasi-stationary regions within the speech waveform. The recognition performance of the resulting adaptively smoothed estimator is found to be comparable to that of conventional interbank estimators, yet the average temporal sampling rate of the resulting spectral vectors is reduced by around a factor of ten.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-88"
  },
  "coile90_icslp": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Inductive learning of grapheme-to-phoneme rules",
   "original": "i90_0765",
   "page_count": 4,
   "order": 89,
   "p1": "765",
   "pn": "768",
   "abstract": [
    "This paper describes a system for the inductive learning of grapheme-to-phoneme rules. As input, the system only needs a list of words; each word in its orthographic and phonemic form. In a first step, the correspondence between graphemes and phonemes is established for each word. This is done with the technique of Hidden Markov Models. Next, the actual learning process is started. This iterative induction process creates an ordered list of pronunciation rules for each letter of the alphabet. The proposed methods were evaluated for Dutch.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-89"
  },
  "yamashita90_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Hiroyuki",
     "Fujiwara"
    ],
    [
     "Yasuo",
     "Nomura"
    ],
    [
     "Nobuyoshi",
     "Kaiki"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "A support environment based on rule interpreter for synthesis by rule",
   "original": "i90_0769",
   "page_count": 4,
   "order": 90,
   "p1": "769",
   "pn": "772",
   "abstract": [
    "SSRI (Speech Synthesis system using Rule Interpreter) is a support environment for the experts to develop rule bases of speech synthesis. In SSRI, the synthesis rules are stored in SSRB (Speech Synthesis Rule Base) and separated from RI (Rule Interpreter) which generates the parameter transition contours. In general, management of the fired rules for each text can reduce the time of speech re-synthesis from the text which has ever converted to speech by RI. RM (Rule Manager) keeps the history of rule invocation and restores it after the modification of SSRB. And, RM divides the synthesis rules into groups to accelerate speech synthesis from a new text. The history data and the rule group drastically reduce the turn-around time of synthesis rule development. SSRI also provides the user with several utilities of graphically displaying the parameter transition contours, listing the fired rules for the text, saving the synthesized speech wave and so on. SSRI is implemented in C-Prolog and C on the UNIX engineering workstation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-90"
  },
  "lee90b_icslp": {
   "authors": [
    [
     "Jung-Chul",
     "Lee"
    ],
    [
     "Yong-Ju",
     "Lee"
    ],
    [
     "Hee-il",
     "Han"
    ],
    [
     "Eung-Bae",
     "Kim"
    ],
    [
     "Chang-Joo",
     "Kim"
    ],
    [
     "Kyung-Tae",
     "Kim"
    ]
   ],
   "title": "Speech synthesis using demisyllables for Korean: a preliminary system",
   "original": "i90_0773",
   "page_count": 4,
   "order": 91,
   "p1": "773",
   "pn": "776",
   "abstract": [
    "This paper describes a preliminary version of the text-to-speech system for Korean, which is referred to as \"Geul-Sori\". Input letters include Korean characters, numerals, and punctation marks. As a synthetic unit, we use the demisyllables and the control parameters of prosody are generated by rule. Geul-Sori was implemented on IBM-PC/AT using the TMS320C25 DSP chip. Geul-Sori is well operated, but yet poor in naturalness of speech. We are now developing many kinds of rules to achieve more naturalness and more intelligibility of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-91"
  },
  "ahn90_icslp": {
   "authors": [
    [
     "Seung-Kwon",
     "Ahn"
    ],
    [
     "Koeng-Mo",
     "Sung"
    ]
   ],
   "title": "The rules in a Korean text-to-speech system",
   "original": "i90_0777",
   "page_count": 4,
   "order": 92,
   "p1": "777",
   "pn": "780",
   "abstract": [
    "The rules for a Korean text-to-speech system based on the formant synthesis method are proposed. This paper describes all the aspect of the synthesis rules for Korean language. The rules are required for two parts. Firstly, for a linguistic process which converts the input text into phonetic unit. Secondly, for a synthesis process which generates and concatenates the synthesis unit to form a data package for spoken output message. All these rules have been implemented on a personal computer with a digital signal processing board which is designed for real time processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-92"
  },
  "liu90_icslp": {
   "authors": [
    [
     "Chi-Shi",
     "Liu"
    ],
    [
     "Wern-Jun",
     "Wang"
    ],
    [
     "Shiow-Min",
     "Yu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Mandarin speech synthesis by the unit of coarticulatory demi-syllable",
   "original": "i90_0781",
   "page_count": 4,
   "order": 93,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "In this paper, we will discuss the selection of synthesis unit for Mandarin speech synthesis. To choose the synthesis unit which could be easily segmented and concatenated, and produce natural and intelligent speech, we proposed the synthesis unit of Coarticulatory Demi-syllable segmented from di-syllable words. We found that spectral continuity of concatenating demisyllable still kept good. By subjectively listening, testers felt that the Coarticulatory Demi-syllable produced more natural and intelligent synthetic speech than monosyllable, but less than synthesis-by-analysis speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-93"
  },
  "teranishi90_icslp": {
   "authors": [
    [
     "Ryunen",
     "Teranishi"
    ]
   ],
   "title": "A study on various prosody styles in Japanese speech synthesizable with the text-to-speech system",
   "original": "i90_0785",
   "page_count": 4,
   "order": 94,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "This paper describes an original text-to-speech system developed in my laboratory, for the purpose of research study in the field of speech synthesis by rule, especially of synthesizing various prosody in Japanese. After many trials of the synthesis experiment with this system, it is found that most of prosody styles and the variations which appear in the text reading mode speech are synthesized, but prosody styles in another speech mode, viz. conversation mode are hardly synthesized, and only acted conversation style can be simulated. According to such results, the author discusses the synthesis possibility of various prosody styles, and he remarks that the speech prosody style of the text-reading mode (including the acted conversation style) is easier to be simulate with a simpler rule system, because the speech to be simulated is already regularized when the original speech turned to the literal text and turned again from letters to speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-94"
  },
  "kamanaka90_icslp": {
   "authors": [
    [
     "Hiroki",
     "Kamanaka"
    ],
    [
     "Takashi",
     "Yazu"
    ],
    [
     "Keiichi",
     "Chihara"
    ],
    [
     "Makoto",
     "Morito"
    ]
   ],
   "title": "Japanese text-to-speech conversion system",
   "original": "i90_0789",
   "page_count": 4,
   "order": 95,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "This paper describes a newly developed Japanese text-to-speech conversion system. The phonetic and prosodic information for speech synthesis is extracted from the input text by the Japanese text analyzer so that Kanji and Kana texts can be input directly to the system. The synthetic speech is generated by concatenating speech synthesis units which contain symmetrical waveforms. As synthesis units, VCV syllables are used as well as CV syllables and symmetrical waveforms have been obtained by PSE analysis. Therefore, the synthetic speech sounds smooth and natural. The system has been built on a compact board which can be housed in a PC slot.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-95"
  },
  "ishikawa90_icslp": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Neural network based concatenation method of synthesis units for synthesis by rule",
   "original": "i90_0793",
   "page_count": 4,
   "order": 96,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "In this paper, we describe a neural network based concatenation method of synthesis units for synthesis by rule. In proposed method, two types of multilayer perceptions are used. One is neural network for phoneme recognition, another is for production of spectrum. A recognition network performs mapping a spectrum to a vector of which elements show similarities to each phoneme ( phonetic vector ), and a spectral production network performs inverse transformation of a recognition network. At boundary of synthesis units, two phonetic vectors are calculated using a recognition network, and interpolation between these vectors are performed, then the spectra of interpolation segment are generated by a spectral production network. We provide multiple sets of neural networks for vowels and consonants, and these are trained based on back-propagation algorithm. Using the proposed method, we obtained satisfactory results in realizing coarticulation, and synthetic speech is very natural.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-96"
  },
  "higuchi90_icslp": {
   "authors": [
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Tohru",
     "Shimizu"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Improvement of the synthetic speech quality of the formant-type speech synthesizer and its subjective evaluation",
   "original": "i90_0797",
   "page_count": 4,
   "order": 97,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "The authors have recently improved the synthetic speech quality of the Japanese speech synthesizer, which was developed for a special-purpose word processor named \"Pasokon Talk\" three years ago. The peculiarities of this system were using phonemes as synthesis units and generating all acoustic parameters based on production rules.\n",
    "The major differences between the previous and current systems concern: (1) the method for control of the voice fundamental frequency contour, especially the phrase component of the generation model for the voice fundamental frequency contour proposed by Fujisaki, (2) the method for control of the formant frequencies and formant bandwidths, and (3) the characteristics of the voicing source.\n",
    "In order to verify the improvement of synthetic speech quality quantitatively, (1) intelligibility tests of Japanese syllables and (2) opinion tests of naturalness have been performed. The results of comparative subjective evaluation tests show that the synthetic speech of the current system has an almost equal intelligibility and a much better grade of naturalness in comparison to that of the previous system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-97"
  },
  "galas90_icslp": {
   "authors": [
    [
     "Thierry",
     "Galas"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "A parametric model of speech signals: application to high quality speech synthesis by spectral and prosodic modifications",
   "original": "i90_0801",
   "page_count": 4,
   "order": 98,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "We propose here a new parametric model and its application to speech synthesis. In our source-filter model, the source is described by spectro temporal events. The filter combine an all-pole filter model for the vocal tract and a de-emphasis filter corresponding to the lip radiation and glottal spectrum slope. Source events are singular or belong to a continuum or pseudo-continuum of events. Examples of singular events are the burst of noise at release of plosive or isolated glottal pulses. Pseudo-continua of events are quasi periodic glottal pulses with their intrinsic irregularities with possible superimposed fricative noise, or pure noise signals as in unvoiced fricatives. Our model allows for a precise and perceptually satisfying description of speech signal and simultaneously provides more flexibility for prosodic modifications. We present an analysis method according to our model using any spectral estimation technique such as AR or homomorphic estimations. We also present an overlap-add synthesis method using the analysis data. We show that our method can be interpreted in terms of frequency domain spectral interpolation as an ARMA model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-98"
  },
  "hamagami90_icslp": {
   "authors": [
    [
     "Tomoki",
     "Hamagami"
    ],
    [
     "Shinichiro",
     "Hashimoto"
    ]
   ],
   "title": "The improved source model for high-quality synthetic speech sound",
   "original": "i90_0805",
   "page_count": 4,
   "order": 99,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "We describe a new speech production model for improving the quality of synthetic vowel speech. The strength of this model is that the source model has a continuous harmonic structure in the time domain and the frequency domain. This report provides a comparative study of the ordinary source model, such as impulse and Rosenberg one, and the new model. The listening experiments have confirmed that our model produces high quality synthetic speech which is better as compared with synthetic ones using ordinary models. Thus, we understand that the really continuous harmonic structure in speech spectrum significantly contributes to synthetic speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-99"
  },
  "hakoda90_icslp": {
   "authors": [
    [
     "Kazuo",
     "Hakoda"
    ],
    [
     "Shin-ya",
     "Nakajima"
    ],
    [
     "Tomohisa",
     "Hirokawa"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "A new Japanese text-to-speech synthesizer based on COC synthesis method",
   "original": "i90_0809",
   "page_count": 4,
   "order": 100,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "This paper describes a new Japanese text-to-speech synthesizer that produces far more natural and intelligible speech than existing synthesizers by using the new Context Oriented Clustering(COC) method. The COC method automatically generates speech unit variations from natural speech database. Preference tests show that the intelligibility of COC synthesized speech is better than that of the conventional dyad based method. A new LSP synthesizer which produces a wide frequency band of output speech is developed. The synthesizer is implemented with a general purpose Digital Signal Processor(DSP). Optimum design parameters, such as LSP order, parameter quantization bits are decided on the basis of spectral distortion and preference tests results. This synthesizer is constructed on a single PC board to permit easy installation in personal computers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-100"
  },
  "asher90_icslp": {
   "authors": [
    [
     "G. M.",
     "Asher"
    ],
    [
     "K. M.",
     "Curtis"
    ],
    [
     "J.",
     "Andrews"
    ],
    [
     "J.",
     "Burniston"
    ]
   ],
   "title": "A parallel multialgorithmic approach for an accurate and fast English text to speech transcriber",
   "original": "i90_0813",
   "page_count": 4,
   "order": 101,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "The paper describes a Transputer-based English text-to-speech system which runs in quasi-real time, is efficient in data memory, non-demanding in labour and is capable of text to speech transcription of high accuracy. The system described is multialgorithmic in that three algorithms representing different approaches to transcription are executed concurrently. Intercommunications and arbitration between algorithms result in sound phonemes which are of acceptable accuracy with respect to character vowel, length and intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-101"
  },
  "curtis90_icslp": {
   "authors": [
    [
     "K. M.",
     "Curtis"
    ],
    [
     "G. M.",
     "Asher"
    ],
    [
     "S. E.",
     "Pack"
    ],
    [
     "J.",
     "Andrews"
    ]
   ],
   "title": "A highly programmable formant speech synthesiser utilising parallel processors",
   "original": "i90_0817",
   "page_count": 4,
   "order": 102,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "Speech synthesisers capable of reproducing high quality male speech from parametric information already exist. The parallel formant synthesiser developed by the Joint Speech Research Unit is recognised as one of the best of these with a real-time, five formant digital signal processor implementation being commercially available. Current research into copy synthesis of female speech shows that the inclusion of further formants is required for the synthesis of realistic female speech. The inherently sequential nature of a digital signal processor implementation limits this extension to the parallel formant synthesiser. The advent of parallel processors and the availability of the Transputer however makes possible a truly parallel highly versatile implementation of a real-time synthesiser, capable of multi-gender speech synthesis. This paper describes the development of such a synthesiser having seven formant resonators. The design of the synthesiser utilised a new technique to overcome the communications \"bottleneck\" associated with the Transputer due to it only having four links. The lack of Transputer links would have proved problematic when performing the resonator data communications and during the passing of the control parameters for each of the resonators of the parallel formant synthesis model. This optimisation technique is also briefly described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-102"
  },
  "maeda90_icslp": {
   "authors": [
    [
     "Kris",
     "Maeda"
    ],
    [
     "Yasuki",
     "Yamashita"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ]
   ],
   "title": "Enhancement of human-computer interaction through the synthesis of nonverbal expressions",
   "original": "i90_0821",
   "page_count": 4,
   "order": 103,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "This paper addresses the enhancement of human-computer interaction through the synthesis of nonverbal expressions. As a means of improving the naturalness of synthesized speech for the realization of human-computer discourse, study has been focused on the effects of emotion and intention on prosodic features of human speech. Experiments were carried out to observe the time-variant behavior of the fundamental frequency, amplitude, and formant frequencies of nonverbal expressions relative to the intentions and emotional states of the speakers. From the observed data, fundamental relationships were determined and stated in the form of constraints on parameters for a formant synthesizer. Fuzzy inference was used to process these constraints. The method supports the linguistic declaration of constraints in the form of conditionals, and it enables fine-tuning through the adjustment of fuzzy set membership functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-103"
  },
  "campbell90b_icslp": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ],
    [
     "Stephen D.",
     "Isard"
    ],
    [
     "Alex I. C.",
     "Monaghan"
    ],
    [
     "J.",
     "Verhoeven"
    ]
   ],
   "title": "Duration, pitch and diphones in the CSTR TTS system",
   "original": "i90_0825",
   "page_count": 4,
   "order": 104,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "This paper describes the prosodic processing and wave-form generation components of the text-to-speech system being developed at Edinburgh University's Centre for Speech Technology Research. Intonation is specified as a sequence of minimal descriptors whose locations are given in terms of syntactically-determined prosodic domains. A pitch contour is computed by converting the descriptors into a sequence of abstract targets whose absolute values depend on a specific speaker model. Duration is determined first at the level of the syllable by a neural network, then accommodated at the segment level according to the distributions observed in a phonetically balanced database. The output waveform is generated by LPC resynthesis of diphone units. Three methods of diphone segmentation are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-104"
  },
  "chen90_icslp": {
   "authors": [
    [
     "Sin-Horng",
     "Chen"
    ],
    [
     "Su-Min",
     "Lee"
    ],
    [
     "Saga",
     "Chang"
    ]
   ],
   "title": "A Chinese fundamental frequency synthesizer based on a statistical model",
   "original": "i90_0829",
   "page_count": 4,
   "order": 105,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "A novel statistical method is proposed in this paper for synthesizing fundamental frequency (F0-) contour of natural Mandarin speech. By taking advantage of simple tone structure, a statistical model is defined to describe the dependence of F0-contour patterns of monosyllables on phonetical features extracted from input texts. In the training, parameters of the model are empirically estimated from a set of sentential utterances. Phonological rules for synthesis are then automatically extracted from the training utterances and implicitly included in the model. In the test, based on the model, the best sequence of F0-contour pattern is estimated using a Viterbi search for an input sentence. Performance of this method was evaluated by simulation using nine repeats of utterances of 112 declarative sentences spoken bv a single speaker. Experiment results show that 77.56% of synthesized F0-contour patterns of monosyllables coincide with the VQ-quantized versions of the original natural speech. Naturality of the synthesized speech is confirmed by an informal listening test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-105"
  },
  "avesani90_icslp": {
   "authors": [
    [
     "Cinzia",
     "Avesani"
    ]
   ],
   "title": "A contribution to the synthesis of Italian intonation",
   "original": "i90_0833",
   "page_count": 4,
   "order": 106,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "A first approximation of a model for the automatic synthesis of Italian intonation is proposed. In line with Pierrehumbert's theory of intonational description [1] fundamental frequency contours of Italian are modelled as sequences of abstract tonal elements aligned with the text. Different levels of prosodic phrasing (accent units, intermediate and intonational phrases) are taken into account in the intonational specification of a text string. The theoretical assumptions as well as the two stages (\"text-to-phonology\" and \"phonology-to-phonetics\") of the model's implementation in the text-to-speech system are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-106"
  },
  "iwata90b_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Yukio",
     "Mitome"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Pause rule for Japanese text-to-speech conversion using pause insertion probability",
   "original": "i90_0837",
   "page_count": 4,
   "order": 107,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "A pause rule for Japanese text-to-speech conversion technique is proposed, which can determine natural pause locations. In order to insert several pauses at appropriate bunsetsu boundaries (which resemble \"phrase\" boundaries in English), the probabilities (pause insertion probabilities) that words are followed or preceded by pauses are used. The pause insertion probabilities are obtained by statistically analyzing a large number of sentence utterances. It was found that the probabilities differ from each other, according to the parts of speech for the words adjacent to the pauses. By the rule, adequate pauses are inserted at the bunsetsu boundaries whose pause insertion probabilities are high. An evaluation experiment for the rule was carried out, using 200 sentences. The result indicates that the pause locations, determined by the rule, are as natural, in 93% of the sentences, as those determined by humans. The rule is adopted by a Japanese text-to-speech conversion system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-107"
  },
  "fujisaki90b_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Pierre",
     "Halle"
    ],
    [
     "Haitao",
     "Lei"
    ]
   ],
   "title": "Analysis and modeling of tonal features in polysyllabic words and sentences of the standard Chinese",
   "original": "i90_0841",
   "page_count": 4,
   "order": 108,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "While the tonal characteristics of Chinese have been qualitatively described in traditional phonetics, a more quantitative analysis requires a mathematical model. We present such a model for the fundamental frequency contours of the Standard Chinese, based on an extension of a model that has already been proved to be applicable to Japanese. This model allows one to interpret a given F0 contour in terms of tone commands and phrase commands, and to analyze various tonal phenomena in quantitative terms. Based on the analysis of a number of words and short sentences using this model, several rules are derived to express various sandhi phenomena found in connected utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-108"
  },
  "yamamura90_icslp": {
   "authors": [
    [
     "Akira",
     "Yamamura"
    ],
    [
     "Hiroharu",
     "Kunizawa"
    ],
    [
     "Noboru",
     "Ueji"
    ],
    [
     "Hiroshi",
     "Itoyama"
    ],
    [
     "Osamu",
     "Kakusho"
    ]
   ],
   "title": "Voice response unit embedded in factory automation systems",
   "original": "i90_0845",
   "page_count": 4,
   "order": 109,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "We describe a voice response unit embedded in FA systems and a Japanese voice synthesis technique using synthesis-by-rule, where prosodic information such as duration, stress, and pitch are combined with phonemic information such as CV syllables (Japanese syllables). The unit has a compact and user-extensible speech database and yields natural sounding synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-109"
  },
  "wothke90_icslp": {
   "authors": [
    [
     "Klaus",
     "Wothke"
    ]
   ],
   "title": "Tetos - a text-to-speech system for German",
   "original": "i90_0849",
   "page_count": 4,
   "order": 110,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "A text-to-speech system was developed which reads aloud arbitrary German texts. It converts an orthographic input text into synthetic speech in 3 main steps: At first the words of the text are preprocessed and abbreviations, special characters, and digits are replaced by their full orthographic correlates. In a second step the preprocessed words are phonetically transcribed by means of about 1,370 letter-to-phone rules. The rate of incorrect transcriptions is near 2% of the running words of a text. Finally the phone symbols in the phonetic transcriptions are mapped on control codes for a commercial speech synthesizer. The control codes are sent to the synthesizer where they trigger the production of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-110"
  },
  "divay90_icslp": {
   "authors": [
    [
     "Michel",
     "Divay"
    ]
   ],
   "title": "A written text processing expert system for text to phoneme conversion",
   "original": "i90_0853",
   "page_count": 4,
   "order": 111,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "Converting a word or a written text from characters to phonemes is one of the first steps done in text-to-speech synthesis. The first part of this process is a text normalization replacing numbers, abbreviations and acronyms, by their full text equivalents. The second step is grapheme to phoneme conversion. Phonetics can also be used to search a file using a phonetic index when the spelling of the name (client file for example) or of the word (look-up in a dictionary) is not certain. In the last case, phonetics can be used to correct spelling mistakes in a text or to find a word in a dictionary without knowing how to write it. A specialized programming language has been designed to write the rules. This allows the rules to be easily defined, revised and expanded independandly of the program (compiler and interpreter). For French, the lastest version consists of approximately 500 rules and is used in different products (synthesizers, CDROM). A different set of rules could be defined for proper names or for other languages: English, Spanish, etc.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-111"
  },
  "yamaguchi90b_icslp": {
   "authors": [
    [
     "Mikio",
     "Yamaguchi"
    ]
   ],
   "title": "Trial production of a module for speech synthesis by rule",
   "original": "i90_0857",
   "page_count": 4,
   "order": 112,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "A module-type speech synthesizer was developed for rule-synthesis of Japanese, in order to find practical applications of rule-synthesis devices. The module can be built into any machine and can be used almost anywhere. Input to the module consists of katakana, accent marks, punctuation marks, and modification marks. The modification marks represent accent sandhi, between words on both sides. The size of the module is 66x46x15 mm and it weighs 59 grams. All necessary hardware and software components are included in the module. The electrical interface is compatible with common CPU buses. Syllable articulation is 63% comprehensible. It uses at most 160 mA from two 5V power supplies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-112"
  },
  "shirai90b_icslp": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "N.",
     "Hosaka"
    ],
    [
     "E.",
     "Kitagawa"
    ],
    [
     "T.",
     "Endou"
    ]
   ],
   "title": "Speaker adaptable phoneme recognition selecting reliable acoustic features based on mutual information",
   "original": "i90_0353",
   "page_count": 4,
   "order": 113,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "In this paper, a statistical method to recognize phoneme in continuous speech is presented. Three aspects of the system are discussed. The first problem is speaker adaptation to improve the recognition rate. The second is about the effective calculation of phoneme likelihood, especially for consonants in various phoneme environments. The third is an algorithm to modify a label and to get final phoneme decision from a frame label using a duration and phoneme sequence rule. In the frame level, with a set of 100 words, the correct recognition rate of all phoneme categories reaches to 80.42% in multi-speaker experiment (6 males) and 74.76% in completely speaker independent experiment. And we will show the results obtained from ATR data base with a set of 3271 words of single speaker data in which 1370 words are used for training and 1901 words are used for recognition test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-113"
  },
  "montacie90_icslp": {
   "authors": [
    [
     "C.",
     "Montacie"
    ],
    [
     "M.-J.",
     "Caraty"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Experiments in the use of an automatic learning system for acoustic-phonetic decoding",
   "original": "i90_0357",
   "page_count": 4,
   "order": 114,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "Results are reported of experiments in the use of Charade, an Automatic Learning System, to classify phonetic macro-classes. A preliminary evaluation of the Charade system is carried out on a reference database of continuous speech and compared to an usual classifier (i.e., Hamming Distance Nearest Neighbor) and a neural net based technique (i.e., Modified Hopfied Net). Preliminary results of classification of phonetic macro-classes can be summarized as follows : For a given reasonable error rate, Charade classifier gives the lowest rejection rate. An important advantage of Charade lies in the ability to analyse and interpret the production rules. For instance, a rule can be interpreted in terms of cues relevant to features.\n",
    "Spectral masks of vowel, drawn from analysis of the most frequent clustering rules found for each of them, are shown to be coherent with phonetic knowledge.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-114"
  },
  "sagayama90_icslp": {
   "authors": [
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Shigeru",
     "Honrna"
    ]
   ],
   "title": "Estimation of unknown context using a phoneme environment clustering algorithm",
   "original": "i90_0361",
   "page_count": 4,
   "order": 115,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "This paper discusses the problem of unknown contexts in the training data set, which often arise in the context dependent approach to speech modeling for speech recognition, and gives a solution by a phoneme environment clustering (PEC) algorithm. A context (phoneme environment)-dependent phoneme model approach is supposed to be very helpful both in speech recognition and speech synthesis. However, it introduces a very difficult problem, namely that the variety of contexts is often too large to obtain from the training data and, even if this is possible, the number of samples for each context may be too small to train its HMM phonetic model. PEC is one possible solution to the problem. It is a general framework for handling phoneme context (or, more generally, phoneme environments) which allows arbitrary number of clusters of phoneme varieties or \"acoustic allophones\" in minimization of some total distortion measure. Since it is based on successive binary division of an abstract space, it inherently has the ability to interpolate the unknown contexts. It is described how contextual 'holes' and 'gaps' are interpolated. This ability is proved through pattern prediction experiments and evaluation by a separability measure. This scheme is tested by an HMM-based phoneme recognition experiment on 5240-words, half of which were used for training and the other half for testing. The results have shown that the recognition error rate is reduced from 9.4% by 25 phoneme models to 3.1% using the PEC with 256 clusters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-115"
  },
  "laprie90_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "Phonetic triplets in knowledge based approach of acoustic-phonetic decoding",
   "original": "i90_0365",
   "page_count": 4,
   "order": 116,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "We propose an original knowledge based approach of acoustic-phonetic decoding of continuous speech relying on the use of triplets: a phone with its phonetic context. A triplet is made up of two description levels: an acoustic description in terms of acoustic 'events (formant, burst ...) and an expert component which indicates the significant acoustic correlates to recognize a triplet. The matching process is firstly performed at the acoustic level, results are then weighted with the help of the expert component. As triplet are phone prototypes it is possible to extract acoustic relations between two reference triplets. These relations must be simultaneously satisfied between two triplet instances of the unknown sentence and two reference triplets proposed as solution. Relaxation techniques enable to implement this idea in order to increase consistency of the global solution.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-116"
  },
  "ariki90_icslp": {
   "authors": [
    [
     "Y.",
     "Ariki"
    ],
    [
     "A. M.",
     "Sutherland"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Optimisation of English phoneme recognition based on HMM",
   "original": "i90_0369",
   "page_count": 4,
   "order": 117,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "This paper describes methods to improve the performance of English phoneme recognition from a linguistic view points. The methods include exploiting time duration information in hidden Markov model (HMM), intrinsic feature space for vowel and consonant recognition respectively, and finally integration of the vowel/consonant recognition results for detail phoneme recognition. As a first step toward the application of these methods, a real time continuous speech recognition system is developed whose software is running on transputer boards plugged into IBM PC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-117"
  },
  "franco90_icslp": {
   "authors": [
    [
     "Horacio",
     "Franco"
    ],
    [
     "Antonio",
     "Serralheiro"
    ]
   ],
   "title": "A new discriminative training algorithm for hidden Markov models",
   "original": "i90_0373",
   "page_count": 4,
   "order": 118,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "In this work we present a new HMM training procedure which aims explicitly at minimizing the recognition error and increase the discrimination between competing phonetic classes. We propose to minimize a function of the model parameters and the training data which can be interpreted as a temporal integration of a \"frame recognition error\". An iterative algorithm is proposed in which the parameters of the probability distributions associated with each state of the HMM are modified with the objective of reducing this error function. Experimental evaluation of the algorithm showed improved recognition performance relative to the values obtained with maximum likelihood training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-118"
  },
  "hirata90_icslp": {
   "authors": [
    [
     "Yoshimitsu",
     "Hirata"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Speaker adaptation of continuous parameter HMM",
   "original": "i90_0377",
   "page_count": 4,
   "order": 119,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "As an speaker adaptation method of continuous parameter HMM, we adapted mean vectors which are a part of parameters of multi-dimensional normal distributions. We regard a set of mean vectors belonging to each HMM as a codebook. The unsupervised adaptation algorithm modifies the mean vectors by using vector-quantization error-vectors for a test speaker. For two persons, 23 Japanese phoneme recognition accuracy was improved from 62% of non-adaptation into 73% after unsupervised adaptation and into 82% after supervised adaptation, respectively. Also, we describe adaptation results through multi-speaker mode HMM and comparison between the improvement by speaker adaptation and the degradation by vector-quantization of input vectors on speaker dependent mode HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-119"
  },
  "hirahara90_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Hirahara"
    ],
    [
     "Hitoshi",
     "Iwamida"
    ]
   ],
   "title": "Auditory spectrograms in HMM phoneme recognition",
   "original": "i90_0381",
   "page_count": 4,
   "order": 120,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "Several auditory spectrograms based on the adaptive Q cochlear filter and its relatives are compared in speaker dependent HMM phoneme recognition tests using clean speech, as well as speech degraded by adding pink noise. These spectrograms are created using a filter banks an inner hair cell (IHC) model and a lateral inhibition (LINH) circuit, in different combinations. Eight different filter banks of three different types of filter are prepared: (1) a simple band pass filter with Qb=4.5 and 30, (2) a conventional fixed Q cochlear filter with Qb-4.5 and 30, and (3) an adaptive Q cochlear filter with feedback/feedforward control with short/long adaptation time constant. Each filter bank is composed of 55 channel filters spaced by 1/3 Bark and spanning the frequency range from 1 to 18.7 Bark. The IHC model involves a saturated half wave rectifier and a short term adaptation circuit. The recognition task is to classify input tokens into 18 phoneme categories using 5,788 training tokens and 5,773 testing tokens. Results are as follows; (1) The adaptive Q cochlear filter with LINH gives better recognition performances than the other types of filter banks in all training/testing conditions. (2) The LINH effectively improves recognition performance. (3) The IHC model produces no benefit even for the noisy data set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-120"
  },
  "nooteboom90_icslp": {
   "authors": [
    [
     "Sieb G.",
     "Nooteboom"
    ],
    [
     "P.",
     "Scharpff"
    ],
    [
     "Vincent J. Van",
     "Heuven"
    ]
   ],
   "title": "Effects of several pausing strategies on the recognizability of words in synthetic speech",
   "original": "i90_0385",
   "page_count": 3,
   "order": 121,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "This paper is concerned with the effects of different pausing strategies on word recognizability in less than optimal, connected synthetic speech. The main hypothesis behind the experiments to be described is that human recognition of poor quality speech can be improved by inserting well formed speech pauses at appropriate positions within sentences. This hypothesis was tested with fifteen syntactically and semantically well formed Dutch sentences, each 36 words and 68. syllables long, and realized by diphone concatenation, with appropriate synthetic sentence melodies. There were four stimulus conditions, viz.: 1) No pauses at all, 2) five pauses at syntactically motivated positions, 3) five pauses immediately preceding informative content words, and 4) five pauses at fixed intervals of six words each. Realizations of all versions of each sentence were given the same overall duration. Each sentence was listened to by 1O listeners in a blocked design involving 4O listeners in total. Each listener had a form giving away the function words of each sentence and was asked to fill in the content words. Main results are the following: a) Syntactically motivated pause positions lead to higher recognition scores than any of the other conditions. b) Words immediately preceding pauses profit much more from syntactically motivated pauses and suffer much more from ill placed pauses in their recognition scores than any other words. c) Monosyllables profit and suffer much more from well placed and ill placed pauses respectively than longer words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-121"
  },
  "kitahara90_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Kitahara"
    ],
    [
     "Yoh'ichi",
     "Tohkura"
    ]
   ],
   "title": "The role of temporal structure of speech in word perception and spoken language understanding",
   "original": "i90_0389",
   "page_count": 4,
   "order": 122,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "Speech has two aspects: one is phonetic feature and the other is prosodic feature. The role of prosody in the cognitive process of spoken language has been studied. Prosody consists of three kinds of prosodic features, namely pitch, amplitude and temporal structures. Among them, this study focuses on temporal structure, including pausing and phoneme duration. Perceptual experiments based on word detection tests have been performed. The stimuli were excitation source signals composed of pulse trains and white noise that included no spectral information. The experimental results suggest that a continuous rhythm of spoken language, which consists of not only pausing information but also phonemic duration, plays an important role in word detection. Still more, pausing information, including the silence just before the burst in a word-initial plosive, contributes to phrase or word segmentation. On spoken language understanding, these temporal cues assist in the word recognition, combined with the context information in various levels of speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-122"
  },
  "goodman90_icslp": {
   "authors": [
    [
     "Judith C.",
     "Goodman"
    ],
    [
     "Howard C.",
     "Nusbaum"
    ],
    [
     "Lisa",
     "Lee"
    ],
    [
     "Kevin",
     "Broihier"
    ]
   ],
   "title": "The effects of syntactic and discourse variables on the segmental intelligibility of speech",
   "original": "i90_0393",
   "page_count": 4,
   "order": 123,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "Spoken words depend on their linguistic context to be identified in fluent speech. We examined whether contextual dependence varied with the level of information conveyed by a word. In particular, we compared the intelligibility of closed-class vs. open-class words and of given vs. new discourse information. If speakers articulate most clearly words that provide the greatest information, then open-class words and new discourse information should be less dependent on context. Words were excised from conversational speech and presented to listeners for identification. Closed-class words were accurately identified significantly less often than open-class words and open-class words expressing given information were identified significantly less often than those expressing new information. Results can be attributed to the clarity of articulation of each segment in a word rather than to greater coarticulation between word boundaries for the less informative words. Furthermore, results cannot be attributed solely to focal stress. The results are interpreted as driven by a speaker's notions about what is functionally communicative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-123"
  },
  "amano90_icslp": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "Lexical and coarticulatory effects on phoneme monitoring before and after a word identification point in spoken Japanese words",
   "original": "i90_0397",
   "page_count": 4,
   "order": 124,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "Experiments were conducted to investigate lexical and coarticulatory effects on phoneme processing in relation to the word recognition point. Real Japanese words and non-words all containing /k/ at various positions were presented to six subjects as naturally spoken or as synthesized by the concatenation of moras. Reaction times for /k/ in the words and for lexical decisions of the words were individually measured. A recognition point for each word was estimated from the time required to make a lexical decision. The results showed that the reaction times for /k/ in real words were the same for nonwords before the point preceding the word recognition point by about 300 ms. However, after this point, the reaction times for /k/ in real words was shorter than in nonwords. The results were the same for naturally-spoken words and concatenated words. Therefore, this facilitatory effect on phoneme processing is due to the use of lexical information and not coarticulatory information, and this lexical facilitation is effective even before a word is fully recognized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-124"
  },
  "pisoni90_icslp": {
   "authors": [
    [
     "David B.",
     "Pisoni"
    ],
    [
     "Ellen E.",
     "Garber"
    ]
   ],
   "title": "Lexical memory in visual and auditory modalities: the case for a common mental lexicon",
   "original": "i90_0401",
   "page_count": 4,
   "order": 125,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "This paper reports the results of a study designed to measure differences in familiarity for spoken and written words. Two sets of 450 English words were randomly selected from a computerized version of Webster's pocket dictionary. Four groups of subjects were presented with both lists of words for familiarity judgements. The first group (W), saw all the words on each list presented visually; the second group (AA) heard all the words; the third and fourth groups (AV, VA) received one list visually and another list auditorily. Subjects rated the familiarity of each word using a seven-point scale. Correlations of the familiarity scores across both lists and modalities were very high. The mean ratings were not significantly different for visual and auditory groups. The absence of modality differences suggests that familiarity effects occur late in the processing system where information from the input modality converges on a common lexical store in long-term memory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-125"
  },
  "ohala90_icslp": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ],
    [
     "Elizabeth E.",
     "Shriberg"
    ]
   ],
   "title": "Hypercorrection in speech perception",
   "original": "i90_0405",
   "page_count": 4,
   "order": 126,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "Listeners hearing distorted speech may be able to \"correct\" it if they have enough information about the nature of the distortion. We present evidence that this ability to correct a distorted speech signal is bought at a slight cost, namely, hyper-correction. We presented American English listeners with brief (85 msec) samples of 11 vowels that had been 1 kHz low-passed filtered. When presented in a way that gave them no chance to learn the filter characteristics they made 33.3% correct identifications. Many of the errors involved front vowels (those having a high Formant 2) being confused with back vowels (which often have F2 and F1 fused at 1 kHz or less). When the samples were preceded by a redundant precursor sentence filtered in the same way as the vowel samples, listeners raised their correct identifications to 50.1% but there was an increase in \"hyper-correction\" errors, i.e., where back vowels were identified as front vowels. This may provide clues as to how listeners \"correct\" distorted signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-126"
  },
  "nusbaum90_icslp": {
   "authors": [
    [
     "Howard C.",
     "Nusbaum"
    ]
   ],
   "title": "The role of learning and attention in speech perception",
   "original": "i90_0409",
   "page_count": 4,
   "order": 127,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "Over 30 years of research has shown that speech perception requires attention and effort. In addition, more recent studies have shown that speech recognition is not carried out by a fixed perceptual system: Perceptual learning of speech takes place even in adult listeners. In spite of these findings, most theories of speech perception do not incorporate mechanisms of learning or attention. Perhaps the absence of these mechanisms may explain why theories of speech perception have been unable to provide a sufficient account of our ability to recognize the linguistic structure of spoken language. Recent research suggests that a linkage between perceptual learning and attention may provide the basis for a new, cognitive theory of speech perception that can explain our ability to understand spoken language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-127"
  },
  "massaro90_icslp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Michael M.",
     "Cohen"
    ]
   ],
   "title": "The joint influence of stimulus information and context in speech perception",
   "original": "i90_0413",
   "page_count": 4,
   "order": 128,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "Empirical results in speech perception indicate that the joint influence of stimulus and contextual information is consistent with the independence of these sources of information and inconsistent with interactive processing. Thus, the results appear to be inconsistent with an interactive activation and competition (IAC) model [1], and consistent with the fuzzy logical model of perception (FLMP) [2] [3]. In order to overcome its empirical shortcomings, McClelland (in press) modified the interactive activation to be stochastic rather than deterministic and to use a best one wins (BOW) decision rule. When tested against real data and contrasted with the FLMP, however, the new stochastic IAC (SIAC) model gives a poorer description of the joint influence of stimulus information and context in perception. Interactive activation is both inconsistent with empirical results and not necessary to describe the joint influence of stimulus information and context in language perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-128"
  },
  "fujisaki90c_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Influence of context and knowledge on the perception of continuous speech",
   "original": "i90_0417",
   "page_count": 4,
   "order": 129,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "While it is generally assumed that human speech perception starts with the identification of the smallest units, i.e., phones, followed by lexical access, the great variability found in the acoustic characteristics of continuous speech on the one hand and the apparent ease of human listeners in coping with the variability on the other call for re-examination of the conventional view. This paper describes a few experiments conducted to examine the influence of context and knowledge on the units of recognition as well as that of familiarity on the ease of lexical access. A tentative model is then presented for the human processes of spoken language perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-129"
  },
  "foldvik90_icslp": {
   "authors": [
    [
     "A. K.",
     "Foldvik"
    ],
    [
     "O.",
     "Husby"
    ],
    [
     "J.",
     "Kvaerness"
    ],
    [
     "I. C.",
     "Nordli"
    ],
    [
     "P. A.",
     "Rinck"
    ]
   ],
   "title": "MRI (magnetic resonance imaging) film of articulatory movements",
   "original": "i90_0421",
   "page_count": 2,
   "order": 130,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "This paper reports on the use of Magnetic Resonance Imaging (MRI) for filming articulatory movements and discusses advantages and shortcomings of the technique for speech research. A short MRI film will be shown after the presentation of the paper.\n",
    ""
   ]
  },
  "matsumura90_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Atsushi",
     "Sugiura"
    ]
   ],
   "title": "Modeling of 3-dimensional vocal tract shapes obtained by magnetic resonance imaging for speech synthesis",
   "original": "i90_0425",
   "page_count": 4,
   "order": 131,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "Three-dimensional vocal tract shapes have been studied using the magnetic resonance (MR) imaging. MR Images of 24 horizontal sections from the larynx to the nasal cavity at 0.6-cm intervals, were measured during steady-state productions of Japanese vowels. The measurement time was 123 second. Configurations of the area interior to the horizontal vocal tract were extracted manually from the horizontal MR images by using digitizer, and three-dimensional vocal tract shapes were obtained from the 24 areas of horizontal vocal tract. Curvature functions of the mid-sagittal tongue shape and cross-sectional areas interior to the vocal tract, were estimated from the three-dimensional vocal tract data. Based on the observation, a three-dimensional vocal tract model that estimate the vocal tract area function from 2 positions on frontal tongue surface, was proposed for natural speech synthesis. The vocal tract model consist of articulatory models for the midsagittal tongue shape and for cross-sectional area interior to the vocal tract. Vowels were synthesized by model parameters adaptation of the 2 positions on frontal tongue shape. Time courses of the formant frequency for the synthesized vowels, agreed with one for the subjects' original productions. The results indicate the usefulness of the proposed models based on the observation of the three-dimensional vocal tract shape.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-130"
  },
  "kaburagi90_icslp": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Ultrasonic measurement of tongue motion",
   "original": "i90_0429",
   "page_count": 4,
   "order": 132,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "An ultrasonic method for monitoring tongue motion is presented. Employing pulsed-echo and pulsed-transmission techniques, this method can visualize lateral tongue shape and monitor the position of a specific point on the tongue surface. Measurement accuracy is investigated with respect to the distance between the transducer and the receiver, and with respect to the receiver's tilt angle. The method is applied to measure tongue motions during continuous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-131"
  },
  "motoki90_icslp": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Nobuo",
     "Nagai"
    ]
   ],
   "title": "Measurement of sound wave characteristics in the vocal tract",
   "original": "i90_0433",
   "page_count": 4,
   "order": 133,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "In this paper the characteristics of sound pressure distribution in the vocal tract are described on the basis of acoustic measurement. The measurement was performed with plaster replicas of the oral cavity. The validity of plane wave propagation is examined from the measured spatial distribution of the amplitude and phase of sound pressure for pure tone. It is shown that at certain frequencies, there exist points where sound pressure is absolutely zero, with the phase spatially circulating around them. A simple model is considered to explain this phenomenon. And up to about 4kHz except at these certain frequencies, the wave front is almost 1-dimensional though an amplitude gradient can be seen in the vertical direction. This paper also presents the characteristics of particle trajectories and of sound intensity fields.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-132"
  },
  "suzuki90_icslp": {
   "authors": [
    [
     "Hisayoshi",
     "Suzuki"
    ],
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Jiauwu",
     "Dang"
    ],
    [
     "Chengxiang",
     "Lu"
    ]
   ],
   "title": "Speech production model involving subglottal structure and oral-nasal coupling through closed velum",
   "original": "i90_0437",
   "page_count": 4,
   "order": 134,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "This paper describes a speech production model considering the factors such as velum impedance, vocal tract volume inflation, and subglottal system. Based on our measurements of acoustic waveforms and mechanical vibrations at several points of vocal organ, we propose a speech production model having oral-nasal coupling through velum even when it closes. The velum is assumed to be composed of two vibrating plates connected each other by a spring and a mechanical resistance. A syllable /bi/ is synthesized and examined, as an example, by the model considering the velum leakage as well as small inflation of vocal tract volume by inner air pressure at just before mouth opening. As to the speech production model which has both of subglottal structure and supraglottal one, it is shown that the wave-form of glottal flow is changed, and synthesized vowels have zeros in their spectra because of an interaction between sub- and supra-glottal structures through small opening area due to incomplete vocal cords closure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-133"
  },
  "sonoda90_icslp": {
   "authors": [
    [
     "Yorinobu",
     "Sonoda"
    ],
    [
     "Keisuke",
     "Mori"
    ],
    [
     "Tetsuaki",
     "Kuriyama"
    ]
   ],
   "title": "Articulatory characteristics of lip shape during the production of Japanese",
   "original": "i90_0441",
   "page_count": 4,
   "order": 135,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "Articulatory movements of the lips have been measured for study of lip articulations and their dynamics when vowels and bilabial stops were produced in various phonetic environments. Image data of lip movements were recorded by a high-speed video recorder system at a frame rate of 5 ms, for two male subjects. Sampled images with gray-levels were processed to detect contours of upper and lower lips, which were approximated by a 4-th degree polynomial function with five coefficients. These coefficients, as well as geometric configuration parameters (mouth\" opening area, vertical and horizontal width of mouth, etc.), were adopted as characteristic parameters describing lip articulatory behaviors. Various shapes of lip contour were taken in the combination of individual polynomial coefficient. The effect of phonetic environments on lip shape and individual differences were observed in their combination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-134"
  },
  "kusakawa90_icslp": {
   "authors": [
    [
     "Naoki",
     "Kusakawa"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Yuki",
     "Kakita"
    ]
   ],
   "title": "Sequential control model of speech articulation in producing word utterance",
   "original": "i90_0445",
   "page_count": 4,
   "order": 136,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "Speech articulation is executed by motor commands to various speech muscles. The subsystem of motor programming autonomically outputs a sequence of commands according to intended signals for articulatory target. The present study examined such a function of motor program for coordinated speech gestures by speculating motor commands from electro-myographic (EMG) data. A waveform separation technique was applied to extract component commands from ensemble average EMG wave-forms. The data used in this study contained six tongue muscles and two other muscles, which were recorded during production of /apVp/ word utterances. The motor score was obtained from the results, which demonstrated temporal and hierarchical organization of sequential control on motor commands.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-135"
  },
  "simada90_icslp": {
   "authors": [
    [
     "Zyun'ici B.",
     "Simada"
    ],
    [
     "Satoshi",
     "Horiguchi"
    ],
    [
     "Seiji",
     "Niimi"
    ],
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "Sternohyoid muscle activity and pitch control at the onset of utterances",
   "original": "i90_0449",
   "page_count": 4,
   "order": 137,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "Sternohyoid activity involving accentual contrast was examined in a male speaker of Tokyo Japanese. The contrast of interest was in the initial position of sentence frames; thus, unaccented vs. accented pairs of two-mora meaningful words were compared in terms of pitch movements and electro- myographic activity. The utterance types all showed such activity preceding the audio onset. For words with accent, this activity became still for a short period and was followed by consistent reactivation; moreover, the mean level of this pre-audio activity was lower for one type of utterances with initial accent. On the other hand, words with no accent showed a different timing for the offset of the pre-audio activity. These findings lend support to the role played by the sternohyoid muscle in pitch lowering, and specifically in the accentual distinction in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-136"
  },
  "azuma90_icslp": {
   "authors": [
    [
     "Junichi",
     "Azuma"
    ],
    [
     "Yoshimasa",
     "Tsukuma"
    ]
   ],
   "title": "Prosodic features marking the major syntactic boundary of Japanese: a study on syntactically ambiguous sentences of the kinki dialect",
   "original": "i90_0453",
   "page_count": 3,
   "order": 138,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "This paper investigates which of the two prosodic features, pause or F0, has a greater effect on the comprehension of a syntactically ambiguous Japanese sentence of the Kinki dialect. The results of the experiment, where subjects judged the meaning of PARCOR-synthesized syntactically ambiguous sentences, showed that insertion of a pause at a certain point, which was supposed to cause the listeners' judgment for the other meaning of the sentence, does not greatly affect the listeners' judgment. On the other hand, it was found that manipulation of F0 can completely change the listeners' judgment of the stimulus sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-137"
  },
  "wang90f_icslp": {
   "authors": [
    [
     "H. D.",
     "Wang"
    ],
    [
     "GÃ©rard",
     "Bailly"
    ],
    [
     "D.",
     "Tuffelli"
    ]
   ],
   "title": "Automatic segmentation and alignment of continuous speech based on temporal decomposition model",
   "original": "i90_0457",
   "page_count": 4,
   "order": 139,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "The speaker independence and the context modelling are the key problems in automatic segmentation and alignment of continuous speech, which are connected with the segmental concept of speech. In this paper, a new approach is presented: a robust speaker-independent algorithm for this task. It aligns a phonetic transcription with a phoneme nucleus detector using the temporal decomposition (TD) paradigm. The algorithm performs this task in 3 stages: a) Predetection of phoneme nuclei centers candidates using an adaptive detection window; b) Time-alignment of the corresponding phonetic transcription using a TD model based Dynamic Time Warping (TD-DTW) procedure; c) Adjustment of these output nuclei centers and phoneme boundaries detection based also on the TD model. A new temporal decomposition technique was developed also. This algorithm has been trained using 200 sentences pronounced by one speaker and tested using 50 sentences pronounced by 7 speakers. On the test corpus, 86% of the phonemes nuclei centers candidates fall into one manual segment alone. 94% of the final nuclei centers match the manual segmentation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-138"
  },
  "hahn90_icslp": {
   "authors": [
    [
     "Hee-Il",
     "Hahn"
    ],
    [
     "Minsoo",
     "Hahn"
    ]
   ],
   "title": "Voiced/unvoiced/silence classification of spoken Korean",
   "original": "i90_0461",
   "page_count": 4,
   "order": 140,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "In this paper, we presented two techniques for the automatic voiced/unvoiced/ silence classification of spoken Korean which is essential for the high quality speech synthesis and for the speech recognition system taking advantage of the acoustic-phonetic information. The database in this study is composed of five sentences spoken by 5 male and 5 female speakers. Each sentence was uttered twice by each speaker in a sound-treated room. (Almost all kinds of Korean unvoiced sounds are contained in these sentences.) One classification technique is based on the Neural Network utilizing the spectral and the time domain features such as spectral slope, energy, zero-crossing rate, and the autocorrelation coefficient at unit sample delay. The other adopts the conventional pattern classification technique, and uses almost the same features as above. Final classification accuracy of 96.2 % is achieved for both methods. Finally, the results are compared and possible future extensions are briefly discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-139"
  },
  "angderi90_icslp": {
   "authors": [
    [
     "E.",
     "Angderi"
    ],
    [
     "M.",
     "Barsotti"
    ],
    [
     "L.",
     "Mazzei"
    ],
    [
     "L.",
     "Vttrano"
    ],
    [
     "R.",
     "Volpentesta"
    ]
   ],
   "title": "Vocal pauses in teaching: statistical analysis and applications",
   "original": "i90_0465",
   "page_count": 4,
   "order": 141,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "The distribution of pauses in universitary lesson is presented. The aim of this study is the definition and specification of a system for voice-data interpolation on switched telephone networks. The results obtained shove to the realization of a transmission protocol and a hardware architecture achieving the target. This paper gives an overview of the studied approach; some suggestions for different applications and further improvements are also provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-140"
  },
  "kadambe90_icslp": {
   "authors": [
    [
     "Shubha",
     "Kadambe"
    ],
    [
     "Gloria F.",
     "Boudreaux-Bartels"
    ]
   ],
   "title": "A pitch detector based on event detection using the dyadic wavelet tranform",
   "original": "i90_0469",
   "page_count": 4,
   "order": 142,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "Several pitch detectors[l-9], which have been developed so far, are not always suitable for both low pitched and high pitched speakers and are not robust' to noise. In this paper, we describe an event based pitch detector which overcomes the above mentioned problems. We estimate the pitch period by detecting the glottal closure, which we label here as an event, using a time-scale representation viz., the Discrete Wavelet Transform(DWT) and by measuring the time interval between two such events. We illustrate the applicability of this pitch detector to a wide range of pitch periods i.e. for both low pitched and high pitched speakers and its robustness to noise with various examples. We then highlight the merits and demerits of this method by comparing it with existing pitch detection methods. The results of this study indicate that the algorithm works well for noise free and noisy vowels as well as voiced consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-141"
  },
  "fujisaki90d_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Shigenobu",
     "Seto"
    ]
   ],
   "title": "Proposal and evaluation of a new scheme for reliable pitch extraction of speech",
   "original": "i90_0473",
   "page_count": 4,
   "order": 143,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "Analysis using short frame length is necessary in order to realize correct tracking of time-varying features of quasi-periodic signals such as speech. However, when the frame length is reduced for the analysis of rapidly changing signal characteristics, the analysis results are strongly affected by the position of the frame and sometimes may lead to gross errors. In the pitch extraction schemes using the conventional definition of the short-time autocorrelation function, the value of its peak indicating the fundamental period varies with the frame position. In order to reduce these variations, we present a new definition for the normalized shorttime autocorrelation function that does not require the selection of frame length. A new scheme for pitch extraction of speech is proposed that assures high accuracy of results without adjusting the frame length for each speaker. The validity of the proposed scheme is confirmed by the experiments using speech materials recorded by both male and female radio announcers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-142"
  },
  "sugiyama90_icslp": {
   "authors": [
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "Spectral interpolation using distortion geodesic lines",
   "original": "i90_0477",
   "page_count": 4,
   "order": 144,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "In this paper, we propose a spectral interpolation method using a distortion geodesic line, which is defined as the curve with minimal accumulated distortion. We apply the distortion geodesic line to interpolation of two given spectra (vectors). The first part of this paper describes the definition of the distortion geodesic line. It is shown that a geodesic line is characterized by the Riemannian metric which is introduced as a bilinear form of the second partial derivatives of a given distortion measure. The second part describes an inequality for the WLR measure on several interpolating curves. This inequality guarantees that the accumulated WLR distortion value for any two given spectra, F0 and F1, on the correlation interpolation curve, is always smaller than the direct WLR value dWLR(F0, F1). This property is easily extended to a category of several distortion measures. The third part describes an application of the distortion geodesic line to spectral interpolation, and numerically shows that the interpolation line on the correlation parameter is the best of several kinds of LPC based parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-143"
  },
  "yogo90_icslp": {
   "authors": [
    [
     "Hirofumi",
     "Yogo"
    ],
    [
     "Naoki",
     "Inagaki"
    ]
   ],
   "title": "Adaptive speech processing using an accelerated stochastic approximation method",
   "original": "i90_0481",
   "page_count": 4,
   "order": 145,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "This paper presents a speech parameter estimation based on adaptive digital filtration(ADF). Accelerated stochastic approximation methodology with improved convergence property and parameter estimation accuracy is used as an adaptive algorithm for ADF. ADF has a performance which converges within 25 iterations even if the input signal has a considerablly higher autocorrelation;for example,a correlation coefficient of 0.99. Parameter estimation,therefore, within a one pitch period is easily performed,and the method can process both male and female speech without any restrictions. This paper describes the application of a method to parameter estimation for stop consonants and vowels. The identification of the stop consonants by neural network is described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-144"
  },
  "fujisaki90e_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Noboru",
     "Takahashi"
    ]
   ],
   "title": "Manifestation of linguistic and para-linguistic information in the voice fundamental frequency contours of spoken Japanese",
   "original": "i90_0485",
   "page_count": 4,
   "order": 146,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "Prosodic features of spoken language play an important role in the transmission of linguistic information concerning word meaning, sentence structure and discourse structure but also in the transmission of para- and non-linguistic information such as speaker's intention/emotion, idiosyncrasy, and naturalness. In this paper, we first define the units of prosody of the spoken Japanese on the basis of analysis of voice fundamental frequency contours of a large number of spoken sentences, and then clarify the relationship between linguistic and para-linguistic information and the components of the F0 contour. While the phrase components convey mainly the syntactic information the accent components convey the information on accent type, syntactic structure, and discourse structure. Para-linguistic information is mainly conveyed by an additional accent component at the sentence end.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-145"
  },
  "bruce90_icslp": {
   "authors": [
    [
     "GÃ¶sta",
     "Bruce"
    ],
    [
     "Paul",
     "Touati"
    ]
   ],
   "title": "Analysis and synthesis of dialogue prosody",
   "original": "i90_0489",
   "page_count": 4,
   "order": 147,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "The object of study is the prosody of spontaneous dialogue. The focus is on the methodology that we are developing, and the exemplification is from Swedish. We have been conducting three types of analysis: analysis of dialogue structure, auditory (prosodic) analysis, and acoustic-phonetic analysis. Dialogue structure analysis concerns textual, interactive and turn taking aspects. The auditory analysis takes the form of a transcription encoding five prosodic features: prominence, phrasing, pitch range, boundary tones and pausing. The acoustic-phonetic analysis is centered around pitch, particularly the use of overall pitch range for the expression of textual coherence and boundary of a dialogue. For the modelling of dialogue prosody we have been using rule synthesis. Our preliminary testing shows that variation in pitch range is a potentially important means for signalling textual aspects of a dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-146"
  },
  "takeda90b_icslp": {
   "authors": [
    [
     "Shoichi",
     "Takeda"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Analysis of prosodic features of prominence in spoken Japanese sentences",
   "original": "i90_0493",
   "page_count": 4,
   "order": 148,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "This paper focuses on a partial emphasis process in sentences known as \"prominence\". Prominence is considered important in achieving rule-based synthetic speech that is easily understood. In general, prominence in Japanese sentences may be divided into two types, default and intended. Intended prominence can be further classified depending on where focus is placed. Features of Japanese sentences which include both types of prominence are analyzed in terms of prosody. The analysis results show that the most dominant form of prominence is enhancement of fundamental frequency and increasing of power. Specifically, this analysis concerning F0 contours confirms that a syntactic unit or a part of it, as an object of placed focus, is mapped into one of several categories of prosodic words and variation types. Finally, listening test results are briefly presented to show the effectiveness of the rules proposed based on the above analysis results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-147"
  },
  "daly90_icslp": {
   "authors": [
    [
     "Nancy A.",
     "Daly"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Acoustic, perceptual, and linguistic analyses of intonation contours in human/machine dialogues",
   "original": "i90_0497",
   "page_count": 4,
   "order": 149,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "This paper describes our research directed towards the quantification and use of prosodic cues in the intonation contours for different types of queries found in human/machine problem-solving dialogues. We ask three fundamental questions: First, what factors determine intonation encoding for queries? Second, how do these factors interact? Third, what are the implications for speech understanding? Our analysis is based on a corpus of spontaneous speech, containing several thousand sentences, collected in conjunction with the development of the MIT voyager urban exploration and navigation system, under simulated human/machine dialogues. In our corpus, we found that over 90% of the WH-questions, such as Where is MIT, have low final boundary tones. For the YES-NO questions, such as Is there a bank near Harvard, on the other hand, only about 64% were found to have high final boundary tones. Our results, based on classification and regression tree analyses (CART), indicate that, while syntactic structure is the most important factor in predicting intonation contours, other factors such as the sentence's main verb and the speaker's sex are also important. We performed perceptual experiments in which subjects were asked to rate the appropriateness of a simple yes-no answer on a 10-point scale. Our results confirm that listeners vary their judgments of YES-NO appropriateness based on factors other than final boundary tone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-148"
  },
  "kubozono90_icslp": {
   "authors": [
    [
     "Haruo",
     "Kubozono"
    ]
   ],
   "title": "The role of the mora in speech production of Japanese",
   "original": "i90_0501",
   "page_count": 4,
   "order": 150,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "This paper discusses the reality and relevance of the prosodic unit 'mora' in Japanese in a cross-linguistic perspective. The main lines of evidence to be discussed are taken from the following three linguistic phenomena: (a) word formation, (b) speech errors, and (c) stuttering. Careful analyses of these phenomena in Japanese reveal that the mora plays an important role as a unit of phonological length, i.e. as a unit for measuring phonological distance in various types of linguistic phenomena. This finding contrasts with the situation in English where essentially the same role is played by the syllable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-149"
  },
  "tsukuma90_icslp": {
   "authors": [
    [
     "Yoshimasa",
     "Tsukuma"
    ],
    [
     "Junichi",
     "Azuma"
    ]
   ],
   "title": "Prosodic features determining the comprehension of syntactically ambiguous sentences in Mandarin Chinese",
   "original": "i90_0505",
   "page_count": 4,
   "order": 151,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "This paper investigates the prosodic features that can disambiguate polysemous sentences in Mandarin Chinese and confirms the results of the acoustic investigation by means of a series of perceptual experiments using synthesized speech stimuli. Judging from the results of both the acoustic and perceptual experiments, it is concluded that a pause and stretched word duration before a syntactic boundary are the two primary prosodic cues to disambiguate polysemous sentences in Mandarin Chinese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-150"
  },
  "huber90_icslp": {
   "authors": [
    [
     "Dieter",
     "Huber"
    ]
   ],
   "title": "Prosodic transfer in spoken language interpretation",
   "original": "i90_0509",
   "page_count": 4,
   "order": 152,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "This paper presents a unified approach to the description and classification of prosodic phenomena in continuous speech, and evaluates its applicability to interpreting telephony for a limited transfer task between equivalent samples of Japanese and English dialogue. An algorithm is proposed which uses the F0 tracings of connected speech dialogue as input and performs speaker independent segmentation into prosodically defined information units. The time- alignment of these units with linguistic structure is established separately for each language, which permits both monolingual classification and bilingual comparison of the prosodic data. A tentative set of transfer roles for the \"translation\" of prosodic features between Japanese and English is introduced, and directions for further research are indicated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-151"
  },
  "sugito90_icslp": {
   "authors": [
    [
     "Miyoko",
     "Sugito"
    ]
   ],
   "title": "On the role of pauses in production and perception of discourse",
   "original": "i90_0513",
   "page_count": 4,
   "order": 153,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "This paper reveals that there is not always a significant correlation between the durations of pauses and utterances in discourse, and that the duration of pause is mainly controlled by syntactic factors, with physiological factors playing only a secondary role. Listening tests were conducted using material collected from three different genres of speech, both with pauses intact and with pauses removed. The results indicate that pauses are indispensable for listeners as a rehearsal time for the short-term memory and therefore play an important role not only for the speaker, but also for the listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-152"
  },
  "maekawa90_icslp": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Production and perception of the accent in the consecutively devoiced syllables in tokyo Japanese",
   "original": "i90_0517",
   "page_count": 4,
   "order": 154,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "What happens to the phonological pitch accent of Tokyo Japanese when the accented syllable and all other syllables in the word are devoiced by the effect of phonetical vowel devoicing rule? The acoustic analysis and perception tests reveals that the accentedness of the devoiced syllable is perceived by the elevated pitch at the beginning of the voiced syllable immediately following the sequence of the devoiced syllables. The exact location of the accent in the devoiced syllables, however, can't be identified correctly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-153"
  },
  "gurgen90_icslp": {
   "authors": [
    [
     "Fikret S.",
     "Gurgen"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Line spectrum pair frequency - based distance measures for speech recognition",
   "original": "i90_0521",
   "page_count": 4,
   "order": 155,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "In the present study, the performance of the line spectrum pair (LSP) frequencies representation for speech recognition is investigated. Various distance measures such as Euclidean, inverse variance weighted Euclidean, and Mel-scale-like weighted distance measures based on the LSP frequencies are used for speaker-independent isolated word recognition experiments with a Dynamic Time Warping (DTW) system. Transitional LSP frequency parameters defined by regression coefficients of LSP frequencies are also introduced. The transitional and the instantaneous parameters and distances are linearly combined for better recognition performance. The cepstral distance measures, and transitional and instantaneous cepstral parameters and distances are used for the comparison of the performances. The linear combination of the instantaneous and the transitional parameters for LSP representation is found to be the best among the all distances used in the experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-154"
  },
  "shimodaira90_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Yoshio",
     "Horiuchi"
    ],
    [
     "Masayuki",
     "Kimura"
    ]
   ],
   "title": "Speaker independent isolated word recognition using local and global structural features",
   "original": "i90_0525",
   "page_count": 4,
   "order": 156,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "A speaker-independent isolated word recognition system is described which is based on the use of intra-word local and global structural features. The local features are incorporated into the system with using matrix quantization of segment patterns that are obtained by dividing a speech word pattern into partial patterns of equal frame length of about 4. Then global features are incorporated by making use of the correlations between segments. On a data base with a vocabulary of 212 words spoken by 20 speakers (10 male and 10 female), the system shows higher performance in recognition accuracy and processing speed than the system using whole-word template based dynamic time warping (DTW) algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-155"
  },
  "gurlekian90_icslp": {
   "authors": [
    [
     "Jorge A.",
     "Gurlekian"
    ],
    [
     "Horacio E.",
     "Franco"
    ],
    [
     "Miguel",
     "Santagada"
    ]
   ],
   "title": "Speaker independent recognition of isolated Spanish digits",
   "original": "i90_0529",
   "page_count": 4,
   "order": 157,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "This paper presents an approximation to the recognition of Spanish digits based on a phonetic approach using probabilistic functions of Markov chains. The way to incorporate the phonetic knowledge to the markovian modelling resides on the strong sensitivity of the models to the initial conditions. Continuous PDF's, associated with each state allowed to represent the segments of least variability with greater weight than the others, and in the recognition stage assign more discriminative power to the phonetic relevant segments. The material under study consisted of the ten Spanish digits recorded from two utterances of 28 native male speakers, for a total of 560 words to recognize. The recognition accuracy of 99.64% confirm that the use of phonetic knowledge in the training step of the system allowed to define high discrimination states which favored the recognition. This type of training allowed to obtain an attractive relation of performance vs. complexity of the model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-156"
  },
  "sugi90_icslp": {
   "authors": [
    [
     "Nobuo",
     "Sugi"
    ],
    [
     "Jun'ichi",
     "Iwasaki"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ],
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Akira",
     "Fukumine"
    ],
    [
     "Akira",
     "Nakayama"
    ]
   ],
   "title": "Speaker independent word recognition system based on the structured transition network of phonetic segments",
   "original": "i90_0533",
   "page_count": 4,
   "order": 158,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "This paper proposes a new word-recognition method based on the Structured Transition Networks (STN) with phonetic segments. Phonetic segments are multiple phonological units which consist of about 600 acoustic/phonetic structures of 32~96 msec duration. The STNs are state transition networks composed of a main path which represents a standard speech pattern and branches which represent distorted patterns. A flexible representation of speech fluctuation using these branches realizes a high rejection performance. The network design with the acoustic/phonetic knowledge requires a smaller amount of training data than do other statistical approaches. An evaluation of 16 spoken words uttered by 10 unknown speakers has achieved a recognition rate of 93.1%, and a rejection rate of 92.5% for the utterances outside the vocabulary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-157"
  },
  "imamura90_icslp": {
   "authors": [
    [
     "Akihiro",
     "Imamura"
    ],
    [
     "Yoshitake",
     "Suzuki"
    ]
   ],
   "title": "Speaker-independent word spotting and a transputer-based implementation",
   "original": "i90_0537",
   "page_count": 4,
   "order": 159,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "This paper describes an HMM-based speaker-independent word spotting system and its Transputer-based implementation. The candidates of word end-points and the corresponding likelihood scores are computed with the continuous Viterbi decoding algorithm. To prune unreasonable candidates, a new duration control method, a threshold logic for the likelihood scores and a new local peak detection method are proposed. An efficient parallel processing scheme for the word spotting system is carried out by using a tree structure of Transputers. In each frame period, the spectral feature vector from the speech analyzer is broadcasted from the root Transputer (Processing Master: PM) to the node Transputers (Processing Element : PE). Each PE performs the continuous Viterbi decoding and the pruning of candidates in parallel, and the spotting results are returned to PM. With 8 PEs in a tree structure, 72 words can be processed within a 12msec frame period. Word detection experiments, using the 10 Japanese digits spoken over a noisy telephone network, yield a word detection accuracy of 97%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-158"
  },
  "kim90d_icslp": {
   "authors": [
    [
     "Jin Yul",
     "Kim"
    ],
    [
     "Yun Seok",
     "Cho"
    ],
    [
     "Soon Young",
     "Yoon"
    ],
    [
     "Hwang Soo",
     "Lee"
    ],
    [
     "Chong Kwan",
     "Un"
    ]
   ],
   "title": "An efficient viterbi scoring architecture for HMM-based isolated word recognition systems",
   "original": "i90_0541",
   "page_count": 4,
   "order": 160,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "In this paper, we propose a dedicated architecture for the Viterbi scoring in hidden Markov model(HMM)-based real-time isolated word recognition systems. Since, in HMM, most states are connected to only three or fewer preceding states, the state transition matrix is very sparse and upper diagonal. Using this property of HMM, we design an efficient Viterbi scoring architecture. The proposed architecture is constructed using either of two types of processing elements (PEs); one can process efficiently most HMM topologies that are being used in practice with a very simple hardware structure, and the other is designed to cover almost all of the possible HMM topologies at an additional hardware cost. Each PE processes, one trellis stage. The PEs can be cascaded to process several consecutive trellis stages in pipeline, thus the memory I/O bandwidth requirement is relaxed significantly. The proposed PEs are implemented in the digit-serial fashion. Thus, the hardware cost of the circuit and the pin count for off-chip I/O is kept to a minimum. Also, since the operators are pipelined fully at digit-level, higher throughput can be obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-159"
  },
  "matsuoka90_icslp": {
   "authors": [
    [
     "Tatsuo",
     "Matsuoka"
    ]
   ],
   "title": "Word spotting using context-dependent phoneme-based HMMs",
   "original": "i90_0545",
   "page_count": 4,
   "order": 161,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "This paper proposes a new clustering method for context-dependent phoneme HMMs. This clustering method uses triphone context as far as training samples are sufficient, and automatically selects biphone and uniphone contexts if only a few training samples are given. Using this clustering method, context-dependent models were created and tested in phoneme recognition experiments and word spotting experiments. Compared with the context-independent models, the context-dependent models achieved 7.6% higher phoneme recognition accuracy and 7.0% higher word spotting accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-160"
  },
  "vittorelli90_icslp": {
   "authors": [
    [
     "V.",
     "Vittorelli"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Roberto",
     "Billi"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "E.",
     "Vivalda"
    ]
   ],
   "title": "POLYGLOT: multilingual speech recognition and synthesis",
   "original": "i90_0549",
   "page_count": 4,
   "order": 162,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "This paper details the project configuration and technical basis for ESPRIT project 2104 (POLYGLOT) which concerns automatic recognition of isolated words in a large vocabulary system and automatic text-to-speech synthesis for unrestricted text in a range of European languages: Dutch, English, French, German, Greek, Italian and Spanish. The paper reports technical progress in terms of advances in isolated word recognition and text-to-speech synthesis. Experimental results for isolated word recognition (Italian language) with 7,000 and 30,000 word vocabulary are presented. Work on continuous speech recognition within the project is described in terms of a feasibility study.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-161"
  },
  "takahashi90b_icslp": {
   "authors": [
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Isolated word recognition using pitch pattern information",
   "original": "i90_0553",
   "page_count": 4,
   "order": 163,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "This paper describes a new technique for isolated word recognition that uses both pitch information and spectral information. Words with similar phonetic features tend to be misrecognized in conventional methods which use only spectral information, even if their phonemes are accented differently. Many phonetically-similar Japanese words are classified by pitch patterns. This paper introduces a measure of the pitch pattern distance. A pitch pattern template is produced by averaging pitch patterns obtained from a set of words which have the same accent pattern. A measure for word recognition is proposed, based on a combination of the pitch pattern distance and the phonetic likelihood. Speaker-dependent word recognition experiments were carried out using 216 Japanese words uttered by five male and five female speakers. The proposed measure reduces the recognition error rate by 40% compared with the conventional phonetic likelihood.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-162"
  },
  "kashino90_icslp": {
   "authors": [
    [
     "Makio",
     "Kashino"
    ]
   ],
   "title": "Distribution of perceptual cues for Japanese intervocalic stop consonants",
   "original": "i90_0557",
   "page_count": 4,
   "order": 164,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "Two experiments were conducted to examine the distribution of perceptual cues for Japanese intervocalic stop consonants, with a special interest in the perceptual role of the pre-closure portion for Japanese listeners. In Experiment 1, as predicted by Japanese phonological structure, Japanese listeners could not identify stop consonants correctly when only pre-closure portions of vowel-consonant-vowel(VCV) utterances were presented . However, in Experiment 2, despite the apparent ineffectiveness, the pre-closure portion demonstrated strong effects for place perception of intervocalic stop consonants. In natural VCV utterances, even when the release burst and most of the CV formant transitions were replaced by a louder white noise, nearly 90% of original consonants were clearly \"heard\" under the noise. On the other hand, without VC coarticulation, only about a half of original consonants were restored. Moreover, when a preclosure and a post-closure of different intervocalic stops were spliced together, the pre-closure dominated the consonant restoration as the noise duration increases. These results suggest that Japanese listeners can utilize cues for Japanese intervocalic stop consonants that occur not only after the closure, but which are also distributed in the region before and after it.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-163"
  },
  "datscheweit90_icslp": {
   "authors": [
    [
     "W.",
     "Datscheweit"
    ]
   ],
   "title": "Frication noise and formant-onset frequency as independent cues for the perception of /f/,/s/ and /// in vowel-fricative-vowel stimuli",
   "original": "i90_0561",
   "page_count": 4,
   "order": 165,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "The present study focusses on how the frication noise and the vocalic formant transitions work together to attain the percept of intervocalic fricatives. The three voiceless fricatives /f/,/s/ and /j/ were investigated within the three symmetric vowel contexts /a:/,/u:/ and /i:/, using synthetic vowel-fricative-vowel (VFV) stimuli from an LPC-based speech editor. Three different experiments have been carried out to measure the influence of the frication noise solely, the influence of the vocalic formant transitions and the combined influence of these two cues. The results indicate that both act as primary and independent cues for the perception of unvoiced fricatives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-164"
  },
  "tsuzaki90_icslp": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Jorge A.",
     "Gurlekian"
    ]
   ],
   "title": "Effects of different standards on the within-category discrimination of synthesized /ABA/ sequences: comparison between Japanese and Spanish",
   "original": "i90_0565",
   "page_count": 4,
   "order": 166,
   "p1": "565",
   "pn": "569",
   "abstract": [
    "To investigate the effects of the internal phonetic prototype on auditory discrimination, two groups of subjects with different phonological systems were tested for auditory discrimination of two different synthesized speech continua. One group was Japanese, and the other was Spanish. Each group received an AX discrimination task both with a typical Japanese /b/and a typical Spanish /b/ as the standard stimulus. The estimated dispersion width for a physically identical stimulus pair was smaller when the standard was a typical realization of the phoneme /b/ for each group than when it was atypical. Thus, even in a discrimination task within a category, the difference in the phonological system results in some behavioral differences. These behavioral differences are discussed in relation to a long term memory factor, or an internal prototype which may be involved in auditory information processing of speech sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-165"
  },
  "akagi90_icslp": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Contextual effect models and psycho acoustic evidence for the models",
   "original": "i90_0569",
   "page_count": 4,
   "order": 167,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "This paper presents two models of contextual effects which can cope with co-articulation problems, especially vowel neutralization. Model 1 predicts target spectral peaks in reduced vowels based on interactions between spectral peak pairs. Model 2 distorts a spectral pattern space to pull back reduced spectral patterns into each of their correct categories based on influences of preceding spectral patterns on phoneme boundaries in a spectral pattern space. To construct two models and to substantiate these models, two psychoacoustic experiments were carried out which measured the extent of phoneme boundary shift with (1) a single formant stimulus as a preceding anchor and (2) a vowel as a preceding anchor. The results from experiment 1 showed that the contextual effect between single formant stimuli should play an important role in phoneme neutralization recovery, and that the neutralization recovery model is formulated as the sum of the contextual effects resulting from interaction between spectral peaks. Additionally, a comparison of the results of the first and second experiments showed that a phoneme boundary shift with a vowel anchor can be considered the sum of a shift with a single formant anchor and a factor from the preceding stimulus. This factor was represented as a function of the distance between the preceding vowel anchor and the perceived vowel in a phoneme space.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-166"
  },
  "shigeno90_icslp": {
   "authors": [
    [
     "Sumi",
     "Shigeno"
    ]
   ],
   "title": "Vowel-contingent anchoring effects on the perception of stop consonants",
   "original": "i90_0573",
   "page_count": 4,
   "order": 168,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "The vowel-contingent effect in the anchoring paradigm was examined. Two experiments were conducted to investigate (1) the vowel-contingent effect in the anchoring effect upon the perception of voiced stop consonants and (2) the duration of vowel-contingent anchoring effects. Anchors were presented to fifteen subjects as a part of the stimulus series in Experiment 1 and to five subjects as a preceding stimulus to one of the stimulus series in Experiment 2. Two conditions of interstimulus interval between anchor and target were employed: 1.0 s and 3.0 s. The vowel-contingent anchoring effect was obtained in both of the presentation conditions. More effetcs were obtained when the acoustical similarity between the anchor and stimulus series increased. It was also found that the vowel-contingent anchoring effect decreased when the ISI was 3.0 s. The present results suggest that vowel-contingent anchoring effects occur at the auditory level irrespective of their phonetic code.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-167"
  },
  "massaro90b_icslp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Process and connectionist models of speech perception",
   "original": "i90_0577",
   "page_count": 4,
   "order": 169,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "Connectionist models offer an alternative frame-work to existing process models for the study of speech perception. The empirical domain consists of the integration of auditory and visual speech in bimodal speech perception. Two classes of connectionist models of speech perception have been evaluated and tested. The classes are interactive-activation models and feed- forward models. Empirical results indicate that while several sources of information simultaneously influence speech perception, these representations of the sources remain independent of one another. This independence is strong evidence against interactive activation in speech perception. Performance in speech identification experiments can be described adequately by a connectionist model with only input and output layers. Given the similarities between this model and a successful process model, we currently have two adequate theories of speech perception by human observers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-168"
  },
  "cutler90_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ],
    [
     "Brit van",
     "Ooyen"
    ]
   ],
   "title": "Vowels as phoneme detection targets",
   "original": "i90_0581",
   "page_count": 4,
   "order": 170,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "Phoneme detection experiments, in which listeners' response time to detect a phoneme target is measured, have typically used consonant targets. This paper reports two experiments in which subjects responded to vowels as phoneme detection targets. In the first experiment, targets occurred in real words, in the second in nonsense words. Response times were long by comparison with consonant targets, and error rates were high. Targets in initial syllables were responded to much more slowly than targets in second syllables. Full vowels were responded to faster and more accurately than reduced vowels in real words, but not in nonwords. Vowel duration correlated negatively with response time. We conclude that the process of phoneme detection in English is more difficult for vowels than for consonants, and vowels in words are relatively likely to be responded to on the basis of a lexical representation. We speculate that vowel detection may be less difficult in languages with sparser vowel distributions than English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-169"
  },
  "uosaki90_icslp": {
   "authors": [
    [
     "Noriko",
     "Uosaki"
    ],
    [
     "Morio",
     "Kohno"
    ]
   ],
   "title": "Perception of rhythm: a comparison between americans and Japanese",
   "original": "i90_0585",
   "page_count": 4,
   "order": 171,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "Three kinds of experiments were held to find out some answers to the following questions:(l)Do speakers of different language perceive rhythmic sequences differently? If so, is it due to the language difference? (2) Is the grouping effect consistent in different language groups? (3) Is there any difference in rhythmic perception between sounds with and without linguistic information? The results showed that (1) statistically significant difference was found only in pitch, but not in intensity nor duration, (2)difference in grouping effect among sequences of different frequency, pitch and duration existed only in Japanese subjects, but not in American subjects, (3)the types of stimuli with some linguistic information and without it did not produce any difference in rhythmic perception, (4)perception of rhythm by American subjects is deeply concerned with their native language but no particular relation was found in Japanese subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-170"
  },
  "sekimoto90_icslp": {
   "authors": [
    [
     "Sotaro",
     "Sekimoto"
    ]
   ],
   "title": "Perceptual frequency normalization of frequency compressed or expanded voiceless consonants",
   "original": "i90_0589",
   "page_count": 4,
   "order": 172,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "In order to clarify the characteristics of frequency normalization on voiceless consonants in which the frequency axes were compressed or expanded, perceptual experiments were carried out for voiceless fricatives and voiceless stops. For voiceless fricatives, a series of fricative noises that varied from the center frequency appropriate for /// to one appropriate for /s/ were synthesized as stimuli. These stimuli were subjected to a listening test to determine the phonetic categorical boundary between //a/ and /sa/ on the continuum of the noise frequency for various frequency-compression or expansion rates. For voiceless stops, noise part was simulated by either a single-pole noise or a multiple-pole noise. A series of noises that varied from /k/ to /t/ were synthesized and subjected to a identification test between /ka/ and /ta/. The results showed that the categorical boundaries between //a/ and /sa/ were invariant, but the categorical boundaries between /ka/ and /ta/ moved in relation to the frequency-compression or expansion rate for both noise conditions. These results implied that the frequency normalization was made on voiceless stops but not on voiceless fricatives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-171"
  },
  "hayashi90_icslp": {
   "authors": [
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Takehiko",
     "Harada"
    ],
    [
     "Hideaki",
     "Seki"
    ],
    [
     "Hiroshi",
     "Hosoi"
    ]
   ],
   "title": "Effects of temporal factors on the speech perception of the hearing impaired",
   "original": "i90_0593",
   "page_count": 4,
   "order": 173,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "The width of ear's temporal window and the values of VOT at the phoneme boundary between voiced versus unvoiced consonants in CV (/ba-pa/) and VCV (/aba-apa/) stimuli were measured for 7 normals and 6 sensori-neural hearing-impaired subjects. The measurements were made at the most comfortable level for each of the hearing-impaired subject, and at three levels, 40, 60 and 80 dB SPL for the normal hearing subjects. The temporal window was generally wider for the hearing-impaired subjects than for the normal subjects. The VOT phoneme boundary was longer for the VCV than for the CV contex-ts. The VOT phoneme boundary in the VCV context tended to correlate to the width of the temporal window. Based on these results, we concluded that the poor temporal resolution of the ear affects to some extent the VOT perception for the hearing-impaired subjects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-172"
  },
  "masaki90_icslp": {
   "authors": [
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Itaru F.",
     "Tatsumi"
    ],
    [
     "Sumiko",
     "Sasanuma"
    ]
   ],
   "title": "Analysis of temporal coordination between articulatory movements and pitch control in the realization of Japanese word accent by a patient with apraxia of speech",
   "original": "i90_0597",
   "page_count": 4,
   "order": 174,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "Characteristics of articulatory movements and. pitch control were investigated in the realization of Japanese word accent for a patient with apraxia of speech. Japanese meaningful words beginning with [ai] which have an upward pitch transition at the end of the initial mora were uttered at normal (5.0 mora/s) and slow (3.3 mora/s) speaking rates by a patient with apraxia of speech and five normal controls. Formant trajectories for the initial vowel sequence and pitch contour were extracted and analyzed based on the models proposed by Fujisaki [1], The results for the patient indicated marked delay of pitch control relative to articulatory movements for the slow speaking rate, and shortening of duration of pitch transition in proportion to the delay of initiation of pitch control. A positive correlation of amplitude of accent command with duration of pitch transition was observed for both normal subjects and the patient with apraxia of speech. The correlation indicates compensatory control of amplitude of accent command in order to produce sufficient upward pitch shift against the gradual fall of the voicing component of pitch contour.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-173"
  },
  "moore90_icslp": {
   "authors": [
    [
     "Brian C. J.",
     "Moore"
    ],
    [
     "Jeannette Seloover",
     "Johnson"
    ],
    [
     "Vincent",
     "Pluvinage"
    ],
    [
     "Teresa M.",
     "Clark"
    ]
   ],
   "title": "Multiband dynamic range compression sound processing for hearing impaired patients: effect on intelligibility of speech in background noise",
   "original": "i90_0601",
   "page_count": 4,
   "order": 175,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "This paper describes an evaluation of an in-the-ear hearing aid, the ReSound model ED2, which applies fast-acting compression independently in two frequency bands. This can compensate for the loudness recruitment typically associated with sensorineural hearing loss. The crossover frequency between the two bands, and the gain and compression ratio in each band are programmable to suit the individual patient. Speech reception thresholds (SRTs) in 12-talker babble were measured for six subjects with moderate sensorineural hearing loss under monaurally and binaurally aided conditions, with the speech and babble both coincident and spatially separated. The ED2 programmed as a two-band compressor gave better speech intelligibility (lower SRTs) than the ED2 programmed as a linear amplifier, and also than unaided listening. A significant advantage for binaural aiding was found, and this advantage was greater when the speech and noise were spatially separated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-174"
  },
  "mizutani90_icslp": {
   "authors": [
    [
     "Takao",
     "Mizutani"
    ],
    [
     "Kiyoshi",
     "Hashimoto"
    ],
    [
     "Masahiko",
     "Wakumoto"
    ],
    [
     "Ken-ich",
     "Michi"
    ],
    [
     "Hareo",
     "Hamada"
    ],
    [
     "Tanetoshi",
     "Miura"
    ]
   ],
   "title": "New graphical expression of the high-speed palatographic data in study of the articulatory behaviors of the tongue",
   "original": "i90_0605",
   "page_count": 4,
   "order": 176,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "Toward a goal of developing a speech training system, three methods of processing the high-speed palatographic data are proposed for better graphical expression of articulatry behaviors of the tongue. The first is the lateral shape for tongue-palate contact point, the second is the lateral loci for the tounge movement, and the third is the cumulative map for tongue-palate contact during a specified period of the utterance. A scrutiny of the results, obtained by these methods for the dental or dentalveolar stops and fricatives, elucidates the subtle differences of articulatry behaviors among these consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-175"
  },
  "kariyasu90_icslp": {
   "authors": [
    [
     "Makoto",
     "Kariyasu"
    ],
    [
     "Kukiko",
     "Maruyama"
    ]
   ],
   "title": "Aging in the rate and regularity of maximum syllable repetition under bite-block",
   "original": "i90_0609",
   "page_count": 4,
   "order": 177,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "Maximum syllable repetition task was performed for three age groups (young, middle, and elderly healthy adults) under different bite-block conditions (BB; None, 10mm, 20mm) to examine effects of age of speaker, size of BB, and CV syllable on the rate and regularity of articulatory movement. Maximum repetition rate (MRR) decreased as a function of age, while articulatory jitter (AJ) , syllable-to- syllable variations in duration which was applied as an index of the irregularity, showed no significant differences. The effect of bite-block was significant for both MRR and AJ. Larger size of BB was associated with smaller MRR and larger AJ. The reduction of MRR as a function of the size of BB was the largest for (ta) and the smallest for (ka).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-176"
  },
  "zhi90_icslp": {
   "authors": [
    [
     "Minje",
     "Zhi"
    ],
    [
     "Yong-Ju",
     "Lee"
    ]
   ],
   "title": "Vowel quantity contrast in Korean: production and perception",
   "original": "i90_0613",
   "page_count": 4,
   "order": 178,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "In the present study, the acoustic properties of the vowel quantity contrast in the Seoul variety of Korean were investigated. Acoustic measurements included the duration and fundamental frequency of the vowels, and the frequencies of the first two formants of a-vowels of test words. Perception tests were also carried out with manipulated stimuli. The purpose was to ascertain to what extent listeners from Seoul and Busan, where the tone contrast is substituted for the quantity contrast, could distinguish between two words contrasted in the vowel quantity in a frame with neither semantic nor syntactic cues. In the acoustic analysis it was found that the vowel quantity contrast in the Seoul variety is manifested primarily by differences in vowel duration and sencondarily by differences in formant structure. The perception experiment revealed that a majority of the Seoul and Busan listeners could discriminate between the words contrasting minimally by the quantity of the vowel in the Seoul variety. The positive effect of vowel duration on the perception of the quantity contrast was observed for the listeners from both Seoul and Busan.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-177"
  },
  "svantesson90_icslp": {
   "authors": [
    [
     "Jan-Olof",
     "Svantesson"
    ]
   ],
   "title": "Phonetic correlates of stress in mongolian",
   "original": "i90_0617",
   "page_count": 4,
   "order": 179,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "In this paper I give data from standard Khalkha Mongolian on some phonetic parameters associated with stress: duration, vowel quality (as measured by formant frequencies), intensity and fundamental frequency. The conclusion is that a unified phenomenon of word stress does not exist in Mongolian.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-178"
  },
  "iwata90c_icslp": {
   "authors": [
    [
     "Ray",
     "Iwata"
    ],
    [
     "Hajime",
     "Hirose"
    ],
    [
     "Seiji",
     "Niimi"
    ],
    [
     "Masayuki",
     "Sawashima"
    ],
    [
     "Satoshi",
     "Horiguchi"
    ]
   ],
   "title": "Syllable final stops LN east asian languages: southern Chinese, Thai and Korean",
   "original": "i90_0621",
   "page_count": 4,
   "order": 180,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "One of the remarkable characteristics for the languages spoken in East Asian continent, like Southern Chinese dialects, Thai, Vietnamese and Korean, regardless of their genealogical origins, is that the stops, like -p,-t,-k, are pronounced without their oral releases at the syllable final positions.\n",
    "Fiberoptic observations of the larynx on Southern Chinese and Thai revealed that final stops were pronounced with the glottis being closed, and particularly in the case of Southern Chinese, with the supraglottal structures being constricted. On the other hands in Korean final stops were characterized by a slight degree of glottal opening.\n",
    "Different laryngeal strategies are employed among East Asian languages in producing the final stops. It is suggested that this might reflect the difference in syllabicity and suprasegmental features among these languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-179"
  },
  "niimi90_icslp": {
   "authors": [
    [
     "Seiji",
     "Niimi"
    ],
    [
     "Qun",
     "Yan"
    ],
    [
     "Satoshi",
     "Horiguchi"
    ],
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "An electromyographic study on laryngeal adjustment for production of the light tone in Mandarin Chinese",
   "original": "i90_0625",
   "page_count": 4,
   "order": 181,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "Cricothyroid and sternohyoid muscle activities were examined in a male adult subject of Mandarin Chinese. The main interest of the present study is to know the neural organization for production of light tone. Syllables with light tone and syllables with tone 4 are selected for investigation. Since the latter tone type is falling as well as that of the light tone, the comparative study of the EMG patterns for production of these two tone types should give us some physiological correlates of the light tone production. Suppression of the cricothyroid muscle and activation of the sternohyoid muscle were observed in both tone types. However, the degree of suppression of the cricothyroid activity is less marked in the light tone than in the tone 4. Physiological interpretation is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-180"
  },
  "cui90_icslp": {
   "authors": [
    [
     "Jingxu",
     "Cui"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "A comparison of the articulation of the Chinese /i,l,l/ by Chinese and Japanese speakers",
   "original": "i90_0629",
   "page_count": 4,
   "order": 182,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "This paper compares the articulation features of the vowel phonemes /i,/,l/ of Standard Chinese by Chinese and Japanese speakers. The Chinese speech samples: were taken from a phonetic database of Chinese speech sounds [1], the Japanese materials were from sound recordings of Japanese students who were studying Standard Chinese at the Beijing Language Institute [2]. They were analyzed by the LPC method. The spectra of the vowels by Chinese and Japanese speakers were analysed. The articulation defects which appeared in the learners' spectra are discussed from the viewpoint of the influence of the mother language and the articulation methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-181"
  },
  "nakashima90_icslp": {
   "authors": [
    [
     "Hirotake",
     "Nakashima"
    ],
    [
     "Masao",
     "Yamaguchi"
    ]
   ],
   "title": "The durations of Japanese long vowels and geminated consonants uttered by indonesian",
   "original": "i90_0633",
   "page_count": 4,
   "order": 183,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "This research was conducted to measure the durations of Japanese long vowels and geminated consonants uttered by Indonesian and Japanese speakers, and compared with the values. The rusults showed that it is difficult even for the fluent speakers of Japanese to utter the long vowels and geminated consonants correctly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-182"
  },
  "saita90_icslp": {
   "authors": [
    [
     "Izumi",
     "Saita"
    ]
   ],
   "title": "On phrasing of Japanese language learners",
   "original": "i90_0637",
   "page_count": 4,
   "order": 184,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "The purpose of this research is to investigate how listening ability relates to speaking ability, especially in prosodic elements. An experiment on the mannars of \"phrasing\" has been given to two groups of foreign learners of Japanese language; beginners and advanced ones. The word \"phrasing\" here means how one expresses the boundary of phrases orally to make the mean ing c lear. The result of the experiment tells that beginners have difficulty in acquiring prosodic elements, except pause, only by listening. Advanced learners, on the contrary, show improvements in the use of prosodic elements after the listening training. This suggests the existence of a type of learners of Japanese who can not reproduce what he/she can recognize, at least in terms of \"phrasing\".\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-183"
  },
  "kawaimusicalinstruments90_icslp": {
   "authors": [
    [
     "Kawai Musical Instruments",
     "Kawai Musical Instruments"
    ]
   ],
   "title": "PROTS (pronunciation training system)",
   "original": "i90_0641",
   "page_count": 3,
   "order": 185,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "PROTS (from PROnunciation Training System) is an English pronunciation practice system which applies speech analysis technology to language learning, PROTS was developed by Kawai Musical Instruments Mfg. Co., Ltd. under the guidance of Professor Hisako Murakawa of the International Budo University. The system is composed of texts and laser disc software which form the basis for the systematic study of pronunciation, from single vocabulary items to dialogs, and an analyzer hardware system which subjects the student's pronunciation to computer voice analysis, allowing visual and aural checking of accuracy. Both the learning materials and the analyzer hardware are appropriate for use in language labs as well as for independent study. The system is organized so that it can be used effectively by students at all levels, from junior high school to college and private language schools. In this paper we would like to introduce the background and basic principles of the system, explain its design and function, and then present some examples of its use in the study of pronunciation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-184"
  },
  "shoham90b_icslp": {
   "authors": [
    [
     "Yair",
     "Shoham"
    ]
   ],
   "title": "Constrained-stochastic excitation coding of speech at 4.8 kb/s",
   "original": "i90_0645",
   "page_count": 4,
   "order": 186,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "This paper proposes a method for enhancing the performance of Codebook-Excited Linear Predictive (CELP) coders. It is based on the observation that the codebook-driven excitation in these coders is noisy and that the noisy component is not adequately filtered by the LPC filter. It is proposed to adaptively constrain the amount of the noisy excitation by linking its level to a performance index of the long-term (pitch-loop) sub-system. This operation reduces the noisy effects of the excitation, enhances the synthesized speech periodicity and hence, the perceptual quality of the coder. Listening test results are presented to demonstrate the subjective improvement of this coder over the basic CELP. The CSEC technique has been implemented in various AT&T coders at 4.8 to 8.0 Kbps, including low-delay CELP, with both stochastic and trained codebooks. Noticeable improvement in speech quality has been achieved. The technique has also been incorporated in the proposed federal standard PFS1016 4.8 Kbps coder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-185"
  },
  "hazu90_icslp": {
   "authors": [
    [
     "Fumie",
     "Hazu"
    ],
    [
     "Akihiko",
     "Sugiyama"
    ],
    [
     "Masahiro",
     "Iwadare"
    ],
    [
     "Takao",
     "Nishitani"
    ]
   ],
   "title": "Adaptive transform coding with an adaptive block size using a modified DCT",
   "original": "i90_0649",
   "page_count": 4,
   "order": 187,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "A Modified Discrete Cosine Transform (MDCT), which is effective for reducing block boundary noise, is applied to Adaptive Transform Coding with an Adaptive Block Size (ATC-ABS) reported by authors. As original MDCT is defined to have a fixed block size for realizing time domain aliasing cancellation, a new overlap window set is introduced to enable variable block size existence, in ATC-ABS. In addition to MDCT employment, a new adaptive bit assignment algorithm based on the auditory characteristic is also introduced to prevent distortion by deficiency of assigned quantization bit amount. In the results of paired comparison tests, the original signals and the reproduced signals by the proposed algorithm are preferred by more than 40% of the subjects, while the reproduced signal by ATC-ABS is preferred by less than 10% of the subjects. The proposed method is promising for digital recording media and for digital communications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-186"
  },
  "moriya90_icslp": {
   "authors": [
    [
     "Takehiro",
     "Moriya"
    ]
   ],
   "title": "Medium-delay 8 kbit/s speech coder based on conditional pitch prediction",
   "original": "i90_0653",
   "page_count": 4,
   "order": 188,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "A medium bit-rate (8 kbit/s), medium delay (10 msec one-way), and high-quality speech coder is designed. The coder uses a conditional pitch predictor in the framework of the backward adaptive CELP (Code Excited Linear Prediction) coder. This scheme transmits only 3 to 5 bits to select from the pitch period candidates pruned by backward pitch analysis. It also uses block-wise backward adaptive short-term LPC analysis and backward adaptive gain quantization. In coding experiments, Signal to Noise Ratio (SNR) and subjective quality were superior to schemes with conventional forward pitch prediction or without pitch prediction. Although the quality of the proposed coder is slightly inferior to that of the conventional forward CELP (more than 50 ms delay) at the same bit-rate, it can outperform conventional CELP if delayed decision of the excitation vector is introduced by paying a computational cost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-187"
  },
  "lee90c_icslp": {
   "authors": [
    [
     "Sung Ro",
     "Lee"
    ],
    [
     "Hwang Soo",
     "Lee"
    ],
    [
     "Chong Kwan",
     "Un"
    ]
   ],
   "title": "A low rate VQ speech coding algorithm with variable transmission frame length",
   "original": "i90_0657",
   "page_count": 4,
   "order": 189,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "In this paper, an efficient variable transmission frame length (VTFL) speech coding method is proposed and its performance is studied by computer simulation. The proposed speech coding method is based on the idea that the performance of speech coding would be improved by varying the transmission frame length according to the stationarity of input speech signal and vector quantizing the representative feature vector of the transmission frame. In the proposed speech coding method, the feature vector sequence consists of PARCOR coefficient vectors obtained by analyzing input speech signal sample-by-sample using the prewindowed recursive least square lattice algorithm. We take the segmentation from the input speech signal and then determine the representative PARCOR vector in each segment. By joining the consecutive segments of phonetically similar characteristics using the likelihood ratio distortion measure, we can obtain transmission frames of variable length. From the computer simulation results, the proposed VTFL speech coding method yields the reduction of total transmission bit rate while maintaining the good reproduced speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-188"
  },
  "iso90_icslp": {
   "authors": [
    [
     "Ken-ichi",
     "Iso"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Speech recognition using demi-syllable neural prediction model",
   "original": "i90_0661",
   "page_count": 4,
   "order": 190,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "The Neural Prediction Model (NPM) is a speech recognition model designed to consider the correlation between spectral and temporal structures in the speech patterns, which are important for accurate speech recognition. This paper presents an improvement in the model and its application to large vocabulary speech recognition, based on subword units. The improvement involves an introduction of \"backward prediction,\" which further improves the prediction accuracy of the original model with only \"forward prediction\". In application of the model to large vocabulary speech recognition, the demi-syllable unit is used as a subword recognition unit. Speaker dependent large vocabulary speech recognition experiments were carried out. The training data amount, necessary for model parameter estimation, and the input layer configuration for pattern predictors were examined. As the best result, a 94.8% recognition accuracy for a 5000 word test set was obtained and the effectiveness was confirmed for the proposed model improvement and the demi-syllable units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-189"
  },
  "bimbot90_icslp": {
   "authors": [
    [
     "FrÃ©dÃ©ric",
     "Bimbot"
    ],
    [
     "Gerard",
     "Chollet"
    ],
    [
     "Jean-Pierre",
     "Tubach"
    ]
   ],
   "title": "Phonetic features extraction using time-delay neural networks",
   "original": "i90_0665",
   "page_count": 4,
   "order": 191,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "A. Waibel introduced Time-Delay Neural Networks as a specific neural network architecture that is especially well adapted to the \"dynamic nature of speech\". We propose here to use low-dimensioned TDNNs for discriminating between phonetic features. We give evaluations of the different performances and we comment them. We also compare direct phoneme recognition scores using a sophisticated classical classifier on one hand, and a medium-size TDNN on the other hand. Extra results obtained after having split our corpus into vowels and consonants are also reported. Experiments are conducted on a set of 5270 phonemes extracted from natural continuous speech uttered by 1 male speaker. Nearly all scores on binary phonetic features range between 90 % and 99 %. More complex tasks provide results between 80 % and 90 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-190"
  },
  "nakamura90_icslp": {
   "authors": [
    [
     "Masami",
     "Nakamura"
    ],
    [
     "Shinichi",
     "Tamura"
    ]
   ],
   "title": "Vowel recognition by phoneme filter neural networks",
   "original": "i90_0669",
   "page_count": 4,
   "order": 192,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "This paper describes a vowel filter neural network (PFN) approach to vowel recognition. Most conventional speech recognition neural networks have a serious drawback: the network output values do not correspond to candidate likelihoods. The PFN is a multi-layer neural network with fewer hidden units than input units prepared for each of the phoneme categories. Each network is trained as identity mapping by speech data belonging to one phoneme category. In the recognition process, the similarity between the input data and output data is computed for each network. The results of the experiment to apply the Japanese vowel recognition task showed that the PFN recognition rates for the top 2 or more choices are higher than those of a conventional 3-layer neural network. It was also confirmed that the PFN outputs represented candidate likelihoods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-191"
  },
  "torkkola90_icslp": {
   "authors": [
    [
     "Kari",
     "Torkkola"
    ],
    [
     "Mikko",
     "Kokkonen"
    ]
   ],
   "title": "A comparison of two methods to transcribe speech into phonemes: a rule-based method vs. back-propagation",
   "original": "i90_0673",
   "page_count": 4,
   "order": 193,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "Two methods to transcribe spoken utterances into phonemes are described and compared. Both operate on symbolic input provided by using Kohonen's Self-Organizing Feature Maps to vector quantize speech frames into code sequences. The idea is to transform the code sequences deteriorated by coarticulation effects and by an unideal acoustic processor closer to ideal sequences. This facilitates almost trivial conversion into phonemes accomplished without any statistical models. One method called Dynamically Focusing Context (DFC) is based on context-sensitive transformation rules, and the other on a feed-forward network trained using error back-propagation. The network uses multiresolution input derived from the code sequence. The performance of these methods is almost equal: the DFC reduces the error rate about 30 per cents, and the latter method about 25 per cents. The training time required by the feed-forward network is, however, orders of magnitude longer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-192"
  },
  "takami90_icslp": {
   "authors": [
    [
     "Jun-Ichi",
     "Takami"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Phoneme recognition by pairwise discriminant TDNNs",
   "original": "i90_0677",
   "page_count": 4,
   "order": 194,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "In this paper, a phoneme recognition method using Pairwise Discriminant Time-Delay Neural Networks (PD-TDNNs) is proposed. In conventional approaches to phoneme recognition based on neural networks, it was found that the difference between training data and testing data degrades recognition performance. To overcome this problem, we developed a phoneme recognition method using PD-TDNNs. Each PD-TDNN has the ability to discriminate between two phoneme categories. In this method, phoneme candidates are selected by judging multiple pair discrimination scores, each of which is obtained from the PD-TDNN. We tested this method on a phoneme recognition task for /b,d,g,m,n,N/. Testing on continuous speech using the PD-TDNNs which were trained with the phoneme data in isolated word utterances, we obtained a first candidate recognition rate of 81.6%, and 96.7% for the cumulative recognition rate up to third candidates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-193"
  },
  "masai90_icslp": {
   "authors": [
    [
     "Yasuyuki",
     "Masai"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Speaker independent speech recognition based on neural networks of each category with embedded eigenvectors",
   "original": "i90_0681",
   "page_count": 4,
   "order": 195,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "This paper describes a speaker independent word recognition algorithm which is based on four layered neural networks with embedded eigenvectors. Eigenvectors in the Subspace Method (SM) are used as weights. In the SM, the accumulation of projection component values from an input pattern is used as a measure of similarity. In contrast to this, our proposed method utilizes each projection component value to achieve performance better than that of the SM. We propose the Subspace Training (SST) algorithm with the SM and the Decision Controlled Back Propagation Training (DCBPT) algorithm to reduce training times. Training and recognition experiments were performed using a 26 word vocabulary consisting of train station names. The error rate of the SM was 1.3%. The error rate was reduced to 0.7% using the neural networks combined with the SM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-194"
  },
  "aikawa90_icslp": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ],
    [
     "Alexander H.",
     "Waibel"
    ]
   ],
   "title": "Speech recognition using sub-phoneme recognition neural network",
   "original": "i90_0685",
   "page_count": 4,
   "order": 196,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "This paper proposes a new phoneme-based speech recognition approach using neural networks trained to recognize sub-phonemes. The sub-phoneme is an acoustic unit which is shorter than a phoneme. The sub-phoneme recognition neural networks exhibit a more precise firing pattern and smaller firing gaps around phoneme boundaries than conventional phoneme recognition neural networks. The word or sentence score is given by the normalized highest sum of the output neuron firing score, which is obtained by the Dynamic Time Warping (DTW) algorithm. A Time Delay Neural Network (TDNN) structure is employed for the sub-phoneme recognizer. The proposed method has been evaluated through word recognition using a continuous speech database. The results show that the recognition rate greatly improves when the sub-phoneme is introduced as a recognition unit. The best word recognition rate is obtained when a phoneme period is divided into front and rear sub-phonemes. The recognition rate is further improved by introducing a multiple entry word dictionary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-195"
  },
  "xu90_icslp": {
   "authors": [
    [
     "Li-Qun",
     "Xu"
    ],
    [
     "Tie-Cheng",
     "Yu"
    ],
    [
     "G. D.",
     "Tattersall"
    ]
   ],
   "title": "Speech recognition based on the integration of FSVQ and neural network",
   "original": "i90_0689",
   "page_count": 4,
   "order": 197,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "In this paper we have developed a novel technique to deal with the problem of feeding a temporal variable speech signal to Multi-Layered Perceptron (MLP), which generally only accept fixed-dimension input pattern, for speech recognition. Instead of using conventional linear or nonlinear interpolation methods, this method is based on the integration of an MLP with Finite-State Vector Quantizer (FSVQ) which is characterized by the ability to memorize the correlations between successive speech feature vectors. FSVQ is designed to map the variable length input pattern into an activation trace on a set of sub-codebooks, each of which corresponds to a 'frame' input unit of the MLP. Experiments show that for the multi-speaker English Alphabet E-set task it can achieve better performance than an MLP with a non linearly interpolated input of fixed dimension.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-196"
  },
  "sayegh90_icslp": {
   "authors": [
    [
     "Samir I.",
     "Sayegh"
    ]
   ],
   "title": "Fast text-to-speech learning",
   "original": "i90_0693",
   "page_count": 4,
   "order": 198,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "A fast text-to-speech learning technique is presented that takes into account the maximum information that is available to the reader for each word. This formulation is based on the connectionist Optimum Path Paradigm and is consistent with recent theories of \"natural\" phonology. It is capable of capturing allophonic variations in a variety of languages with the minimum needed retraining. The procedure is described and completed by the specification of graph traversal and learning of the W phoneme- transition weight matrix which is related to a Hebbian form of learning. Results for learning text-to-speech for several languages as well as implementation aspects are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-197"
  },
  "morgan90_icslp": {
   "authors": [
    [
     "Nelson",
     "Morgan"
    ],
    [
     "C.",
     "Wooters"
    ],
    [
     "HervÃ©",
     "Bourlard"
    ],
    [
     "Michael",
     "Cohen"
    ]
   ],
   "title": "Continuous speech recognition on the resource management database using connectionist probability estimation",
   "original": "i90_1337",
   "page_count": 4,
   "order": 199,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for a Hidden Markov Model (HMM) [1][2][3][4]. The advantage to this approach is the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distribution or statistical independence. In our earlier publications on this topic, a hybrid MLP/HMM continuous speech recognition algorithm was tested on the SPICOS German-language data base. In our recent work, we have shifted to the speaker-dependent portion of DARPA's English language Resource Management (RM) data base. Both consist of continuous utterances (sentences) and incorporate a lexicon of roughly 1000 words. Preliminary results appear to support the previously reported utility of MLP probability estimation for continuous speech recognition (at least, for the case of this simple form of HMM).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-198"
  },
  "tsuboka90_icslp": {
   "authors": [
    [
     "Eiichi",
     "Tsuboka"
    ],
    [
     "Yoshihiro",
     "Takada"
    ],
    [
     "Hisashi",
     "Wakita"
    ]
   ],
   "title": "Neural predictive hidden Markov model",
   "original": "i90_1341",
   "page_count": 4,
   "order": 200,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "This paper describes a. new type of hidden Markov model where a non-linear predictor composed of a neural network is defined at each state. The idea assumes that the sequence of frames is nonstationary and is a nonlinear autoregressive process whose parameters are controlled by a hidden Markov chain. The parameter estimation methods are shown and the relation of this model with some others is discussed. According to the experimental results, this model shows the best performance among them.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-199"
  },
  "minami90_icslp": {
   "authors": [
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Toskiyuki",
     "Hanazawa"
    ],
    [
     "Hitoshi",
     "Iwamida"
    ],
    [
     "Erik",
     "McDermott"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Shigeru",
     "Katagiri"
    ],
    [
     "Masaona",
     "Kagawa"
    ]
   ],
   "title": "On the robustness of HMM and ANN speech recognition algorithms",
   "original": "i90_1345",
   "page_count": 4,
   "order": 201,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "The robustness of HMM and ANN speech recognizers is studied in the speaking mode-independent situation where the recognizers are trained using phoneme tokens extracted from isolated word speech data and tested on phonemes taken from both isolated word speech data and continuous speech data. In this situation there is considerable variation in phoneme identity between training and testing data. We examined six recognizers: a discrete HMM, a continuous HMM, TDNN, Shift-tolerant LVQ2, an LVQ-HMMhybrid algorithm, and a Fuzzy LVQ-HMM hybrid algorithm. The experiment results mainly show the following two points: 1) the recognizers with great discriminative power on the isolated-mode testing data do not necessarily perform highly on the continuous-mode testing data, and 2) algorithms such as Shift-tolerant LVQ2 and Fuzzy LVQ-HMM (which integrates Fuzzy VQ into LVQ-HMM) may be able to achieve robust as well as accurate recognition- i.e. perform well on both continuous and isolated mode data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-200"
  },
  "sawai90_icslp": {
   "authors": [
    [
     "Hidefumi",
     "Sawai"
    ]
   ],
   "title": "The TDNN-LR large-vocabulary and continuous speech recognition system",
   "original": "i90_1349",
   "page_count": 4,
   "order": 202,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "This paper describes an integration of speech recognition and language processing. The speech recognition part consists of the Large Phonemic Time-Delay Neural Networks (TDNN) which can automatically spot all 24 Japanese phonemes by simply scanning among an input speech. The language processing part is made up of a predictive LR parser which predicts subsequent phonemes based on the currently processed phonemes. We call this 'hybrid' integrated recognition system 'TDNN-LR' method. The TDNN-LR recognition system provides large-vocabulary and continuous speech recognition. Two kinds of recognition experiments i.e., large-vocabulary isolated word recognition and continuous speech recognition were performed using the TDNN-LR method. Speaker-dependent recognition rates of 92.6% for the first choices and 97.6% for the top two choices were obtained for 5,240 Japanese common words, and rates of 65.1% for the first choices and 88.8% within the fifth choices were attained for phrase recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-201"
  },
  "bulot90_icslp": {
   "authors": [
    [
     "Remy",
     "Bulot"
    ],
    [
     "Henri",
     "Meloni"
    ],
    [
     "Pascal",
     "Nocera"
    ]
   ],
   "title": "Rule-driven neural networks for acoustic-phonetic decoding",
   "original": "i90_1353",
   "page_count": 4,
   "order": 203,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "We are presently developing an Acoustico-Phonetic Decoding system which uses a Prolog II rule base combined with various neural networks. The rules organize the learning process by choosing relevant examples from a data base of sound. Their essential role in recognition is to describe the structure of the sounds ; they select the input data for the networks from the signal, and interpret their output according to the context. Different strategies were used for localizing and identifying vowels, fricatives and occlusives (depending upon the acoustic features of each macro-class); several network architectures were tested in parallel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-202"
  },
  "poirier90_icslp": {
   "authors": [
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "Knowledge-based segmentation and feature maps for speech recognition",
   "original": "i90_1357",
   "page_count": 4,
   "order": 204,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "A system for multi-speaker letter recognition in the French alphabet is presented. The main characterictic of this system is to combine a knowledge-based segmentation with a connexionist classifier. The segmentation module detects, automatically for each letter, discriminant acoustic patterns. For the classification module, two different approachs are tested. One is unsupervised using Self-organizing Feature Maps ; while the other requires supervised training data using Linear Vector Quantization classifier. A problem called bord effect appears with the first classifier and mainly explains the recognition rate. Best results are obtained with a new version of the second classifier which improves the recognition rate of 10%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-203"
  },
  "fanty90_icslp": {
   "authors": [
    [
     "Mark",
     "Fanty"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "Speaker-independent English alphabet recognition: experiments with the e-set",
   "original": "i90_1361",
   "page_count": 4,
   "order": 205,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "As part of an effort to do high-accuracy speaker-independent recognition of the English alphabet, we focus our attention on the E-set, a very difficult subset of nine letters: B,C,D,E,G,P,T,V,Z. By adding knowledge-based features and finding the most suitable spectral representation, we are able to reduce the E-set error rate by 20% compared to a baseline system that uses features designed for the whole alphabet. The resulting E-set classifier was successfully added to the full-alphabet system, to be invoked whenever the first answer is any member of the E-set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-204"
  },
  "poddar90_icslp": {
   "authors": [
    [
     "Pinaki",
     "Poddar"
    ],
    [
     "P. V. S.",
     "Rao"
    ]
   ],
   "title": "Neural network based segmentation of continuous speech",
   "original": "i90_1365",
   "page_count": 4,
   "order": 206,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "This paper describes a technique to mark phonetic boundaries by locating points of maximal change from an acoustic criterion and refining them by means of phonetic knowledge encoded in a neural network architecture. A threshold was set so that no boundaries are revised at the cost of a few spurious segments. A three layer network was trained using the average spectrum of each segment and used to label segments in the testing phase. Segmentation accuracy was tested by comparison with manual segmentation by an expert phonetician. The consistency of the segmentation and labelling were tested by a recognition experiment on 40 Hindi words. Recognition scores were 84% (first choice only) and 98% (first three choices).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-205"
  },
  "takara90_icslp": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ],
    [
     "Motonori",
     "Tamaki"
    ]
   ],
   "title": "A normalization of coarticulation of connected vowels using neural network",
   "original": "i90_1369",
   "page_count": 4,
   "order": 207,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "In this paper, we report the results of automatic speech recognition experiments which used a model in which we regarded the recognition of observed phoneme as recall from association among the feature parameters of preceding, observed and following phonemes, and in which the neural network model was trained to learn correlations among their feature parameters. In these experiments, we adopted symmetrically connected three-vowel sets, Vi VqVj, as an example of continuous speech, with the central vowel, Fo, as the object to be recognized. Based on these experiments, it was shown that the proposed model, which utilized the contribution from the preceding and following phonemes, is useful for automatic continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-206"
  },
  "watanabe90_icslp": {
   "authors": [
    [
     "Tomio",
     "Watanabe"
    ],
    [
     "Masaki",
     "Kohda"
    ]
   ],
   "title": "Lip-reading of Japanese vowels using neural networks",
   "original": "i90_1373",
   "page_count": 4,
   "order": 208,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "This paper proposes mappings of supervisory signals in layered neural networks for lip-reading the five Japanese vowels with the aim of enhancing recognition. The feature parameters of the width P\\ and height Pi of the lip shape and the distance P3 between the top of the upper lip and the bottom of the jaw are selected. Mappings from the input vector with the three feature parameters to the desired output vectors with various supervisory signals are discussed on the basis of the similarity between the pentagonal distribution of the F1-F2 formant diagram and the P1-P2(P1-P3) diagram. As a result of speaker-dependent lip reading experiments using twenty test sets of five vowels, the recognition rates of the mappings of supervisory signals based on the spatial relationship between vowels are several percent higher than the rates of the mappings disregarding the relationship. Finally, a mapping for generating the desired relationship between vowels in the hidden layer is proposed, and the effectiveness of the mapping is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-207"
  },
  "lucke90_icslp": {
   "authors": [
    [
     "H.",
     "Lucke"
    ],
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "Application of the compositional representation to lexical access using neural networks",
   "original": "i90_1377",
   "page_count": 4,
   "order": 209,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "A new method of representing structured data in neural network classifiers, known as the Compositional Representation has been proposed [4, 5]. This paper gives a brief description of the method, (in particular where it differs from [4]) and applies it to a word recognition task of a isolated multi-speaker data base. The Compositional Representation represents structured data such as strings or lattices of arbitrary length in an n2 dimensional domain (n being a fixed parameter of the model), thus making it possible to use as input for a neural network with n2 input units. The representation can be trained using standard back-propagation. A training algorithm that allocates separate learning factors for each weight is proposed. It has been found that this method converges several orders of magnitude faster than the usual method with just one learning parameter plus momentum term.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-208"
  },
  "mobin90_icslp": {
   "authors": [
    [
     "Abdul",
     "Mobin"
    ],
    [
     "S. S.",
     "Agrawal"
    ],
    [
     "Anil",
     "Kumar"
    ],
    [
     "K. D.",
     "Pavate"
    ]
   ],
   "title": "A voice input-output system using isolated words",
   "original": "i90_1381",
   "page_count": 4,
   "order": 210,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "This paper describes the recognition and synthesis strategies employed in the design and development of a voice input-output system using isolated words. The criterion for the design of the system have been low cost and its suitability for applications in the areas such as control of elctronic/mechanical systems and aids for handicapped by voice commands particularly using isolated words (Abdul Mobin et. al [1][2][3][4][5] [6]). The system consists of a speech recognition and a synthesis unit which can be used either separately or in combination. The hardware and software of the system are based on a Z-.80A microprocessor. The. recognition unit accepts isolated words spoken by a person through a close talking microphone. The detected signal is passed through a specially designed signal conditioning circuitry to obtain a constant signal level. The speech waveform is then processed to obtain a shortterm Fourier spectrum in real time using a audio spectrum analyzer chip ASA-16. The signal obtained from the analyzer is sampled and digitized at the. rate of 200 frames per second. The data is checked for background noise, impulse sounds, amplitude levels, minimum and maximum word lengths etc. Special software has been developed for amplitude normalisation, word boundary detection, time normalisation, data compression and pattern matching etc. [1], [2] and [4]. The speech synthesis unit has been designed using a commercially available phoneme based speech synthesizer chip SC-02 which is also controlled by the same Z-80A microprocessor. Programmes have been developed to synthesize Hindi and English words by generating a proper sequence of phonemes and controlling parameters such as speech rate, pitch, pitch movement rate, amplitude, articulation rate, vocal tract filter response, phoneme duration etc.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-209"
  },
  "slamacazacu90_icslp": {
   "authors": [
    [
     "Tatiana",
     "Slama-Cazacu"
    ]
   ],
   "title": "A psycholinguistic model of first and second language learning",
   "original": "i90_1385",
   "page_count": 3,
   "order": 211,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "1. The author discusses first some models of language learning (MIL); the behaviouristic, the inneistic, the \"neoteny'. 2. The MLL presented by Slama-Cazacu (various works, 1961-1989) is based on her model of the act of communication, in the \"dynamic-contextual\" (DC) approach. It takes into consideration the concrete reality of communication, with all its components, included in contexts and viewed in their dynamicity. 3. The DC model used as a basis for the MLL is centered on concepts such as: a strong drive for communication, motivating the child to learn language; very early and predominant influence of the social environment; occurrence of language learning in a gradual and long process of development, etc. Though different in some basic respects, first and second language learning have also many common features. 4. Various facts will be offered as examples that validate this model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-210"
  },
  "zhao90_icslp": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ],
    [
     "Hisashi",
     "Wakita"
    ]
   ],
   "title": "Experiments with a speaker-independent continuous speech recognition system on the timit database",
   "original": "i90_0697",
   "page_count": 4,
   "order": 212,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "This paper describes a speaker-independent, continuous speech recognition system that we designed and implemented, and reports some of the major features of this system with experimental results on a subset of the TIMIT database. The system is based on hidden Markov modeling of phoneme-sized acoustic units using continuous mixture Gaussian densities. The mixture densities are generated using an algorithm which minimizes the average trace of the mixture components and makes use of the segmental structure of the speech signals. Methods of preparing a dictionary for decoding and controlling the perplexity of grammars are also elaborated. On a subset of TIMIT data-base with 443 words and a grammar perplexity of 49, the system achieved decoding rates of 87.4% sentence correct, 97.9% word correct and 97.5% word accuracy on the training set; on the test set, the rates are 72.4% sentence correct, 93.9% word correct and 92.4% word accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-211"
  },
  "weigel90_icslp": {
   "authors": [
    [
     "Walter",
     "Weigel"
    ]
   ],
   "title": "Continuous speech recognition with vowel-context-independent hidden-Markov-models for demisyllables",
   "original": "i90_0701",
   "page_count": 4,
   "order": 213,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "An acoustic-phonetic decoding stage of a recognition system for continuous speech is described, which uses demisyllables as processing units. The task is divided into an explicit localization of the syllable nuclei and their classification based on pat tern-matching techniques and loudness evaluation. The second part classifies the consonant clusters using vowel-context-independent Hidden-Markov-Models with discrete emission probabilities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-212"
  },
  "hayamizu90_icslp": {
   "authors": [
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Kai-Fu",
     "Lee"
    ],
    [
     "Hsiao-Wuen",
     "Hon"
    ]
   ],
   "title": "Description of acoustic variations by tree-based phone modeling",
   "original": "i90_0705",
   "page_count": 4,
   "order": 214,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper discusses the use of tree-based phone modeling to describe acoustic variations of speech, and its application to speech recognition system. There are many sources of variabilities that affect the realization of a phoneme: phonetic contexts, speakers, stress, speaking rates and so on. Explicit modeling with these sources of variabilities will give more accurate and more detailed phone models, but needs a large amount of speech data for training. Tree-based phone modeling is studied to solve this problem with three case studies: phone models with large VQ codebook sizes, decision tree clustering, and speaker-clustering. They are tested on speaker-independent continuous speech recognition experiments with a 991 word vocabulary. Tree-based phone modeling is shown to produce improvement in all three cases and to provide a good guide to provide trainability and generalizability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-213"
  },
  "soong90_icslp": {
   "authors": [
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Eng-Fong",
     "Huang"
    ]
   ],
   "title": "A tree-trellis based fast search for finding the n best sentence hypotheses in continuous speech recognition",
   "original": "i90_0709",
   "page_count": 4,
   "order": 215,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "In this paper a new, tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition is proposed. The search consists of a forward, time-synchronous, trellis search and a backward, time asynchronous, tree search. The well known Viterbi algorithm is used for finding the best hypothesis in a trellis and for recording the scores of all partial paths time synchronously. A backward A* algorithm based tree search is used to grow partial paths time asynchronously. Each partial path in the backward tree search is rank ordered in a stack by the corresponding full path score, which is computed by adding the partial path score with the best possible score of the remaining path obtained from the trellis path map. In each path growing cycle, the current best partial path, which is at the top of the stack, is extended by one arc (word). The new tree-trellis search is different from the traditional time synchronous Viterbi search in its ability for finding not just the best but the N best paths of different word content. The new search is also different from the A* algorithm, or the stack algorithm, in its capability for providing an exact, full path score estimate of any given partial (i.e., incomplete) path before its completion. When compared with the best candidate Viterbi search, the search complexities for finding the N best strings are rather low, i.e., only a fraction more computation is needed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-214"
  },
  "gabrieli90_icslp": {
   "authors": [
    [
     "F.",
     "Gabrieli"
    ],
    [
     "A.",
     "Dimundo"
    ],
    [
     "A.",
     "Rizzi"
    ],
    [
     "G.",
     "Colangelit"
    ],
    [
     "A.",
     "Stagni"
    ]
   ],
   "title": "Modeling vocabularies for a connected speech recognizer",
   "original": "i90_0713",
   "page_count": 4,
   "order": 216,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "This paper focuses on modeling vocal command languages for a connected speech recognition system. The structure for representing the vocabulary and the syntax are very complex to be directly created by a user, so an easy, interactive way has been implemented for modeling command language of applications, and an automatic, transparent procedure gets the task of creating structures required by training and recognition algorithms. In the latter phase, the two major problems the system has to solve are the elimination of recursive structures (very frequent specially in office automation applications), and the need to break some sequences of words in order to cope with keyboard- speech mixed inputs in applications which cannot be completely interfaced by voice. For completeness, a description of how recognition uses the syntax has been provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-215"
  },
  "kawabata90_icslp": {
   "authors": [
    [
     "Takeshi",
     "Kawabata"
    ],
    [
     "Toshiyuki",
     "Hanazawa"
    ],
    [
     "Katsunobu",
     "Itoh"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Japanese phonetic typewriter using HMM phone units and syllable trigrams",
   "original": "i90_0717",
   "page_count": 4,
   "order": 217,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "This paper describes a Japanese phonetic typewriter based on HMM phone units and syllable trigrams. Even though HMM methods have considerable ability to recognize speech, it is still difficult to recognize individual phones in continuous speech without lexical information. This paper reports on a phonetic typewriter to improve HMM phone recognition performance by incorporating syllable trigrams. HMM phone units are trained using an isolated word database, and their duration parameters are modified in relation to the speaking rate. The syllable trigram tables are made from a large text database of over 35,000 syllables. Phone sequence probabilities calculated from the trigrams are combined with HMM probabilities. Limiting the number of intermediate candidates using these probabilities leads to an accurate phonetic typewriter system without excessive computation time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-216"
  },
  "shigenaga90_icslp": {
   "authors": [
    [
     "Minoru",
     "Shigenaga"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ],
    [
     "Toshihiko",
     "Hanagata"
    ],
    [
     "Takehiro",
     "Yamaguchi"
    ],
    [
     "Ryouta",
     "Masuda"
    ]
   ],
   "title": "A large vocabulary continuous speech recognition system with high prediction capability",
   "original": "i90_0721",
   "page_count": 4,
   "order": 218,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "A large vocabulary (with 1018 words and 1379 kinds of inflectional endings) continuous speech recognition system with high predictability applicable to any task and aiming to have unsupervised speaker adaptation capability is described. Phoneme identification bases on various features. Speaker adaptation is done using reliablely identified phonemes. Using prosodic information, phrase boundaries are detected. The syntactic analyzer uses a syntactic state transition network and outputs their syntactic interpretations. The semantic analyser deals with meaning of each words, relationship between words, and extended case structures of predicates. Recognizing noun phrases first, predicates are predicted, and vice versa.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-217"
  },
  "kobayashi90_icslp": {
   "authors": [
    [
     "Yutaka",
     "Kobayashi"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "Evaluation of a speech understanding system - suskit-2",
   "original": "i90_0725",
   "page_count": 4,
   "order": 219,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "This paper describes an overview of our speech understanding system and reports on the recent results of the sentence recognition experiments. The system recognizes database queries in natural Japanese language spoken sentence by sentence. Based on a hierarchical architecture, the system predicts words strings in a top-down manner, however, the verifications proceeds irrelevantly to the word boundaries. Those phoneme strings bounded by the easily detectable phonemed are dynamically extracted from the predicted word string as verification templates. We carried out sentence recognition experiments using two different matching modules - the lattice matcher and the HMM matcher. The controller adopted a left-to-right time-synchronous beam-search strategy for searching likely sentences. The speech corpus consists of 159 sentences read by three Japanese male speakers. The task perplexity was 8.3. Using the speaker-dependent HMM parameters, the sentence recognition rates were 62.3~71.1 % for the lattice matcher and 83.0-92.5 % for the HMM matcher.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-218"
  },
  "price90b_icslp": {
   "authors": [
    [
     "Patti J.",
     "Price"
    ],
    [
     "Victor",
     "Abrash"
    ],
    [
     "Doug",
     "Appelt"
    ],
    [
     "John",
     "Bear"
    ],
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Bridget",
     "Bly"
    ],
    [
     "John",
     "Butzberger"
    ],
    [
     "Michael",
     "Cohen"
    ],
    [
     "Eric",
     "Jackson"
    ],
    [
     "Robert",
     "Moore"
    ],
    [
     "Doug",
     "Moran"
    ],
    [
     "Hy",
     "Murveit"
    ],
    [
     "Mitchel",
     "Weintraub"
    ]
   ],
   "title": "Spoken language system integration and development",
   "original": "i90_0729",
   "page_count": 4,
   "order": 220,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "SRI is developing a spoken language system (SLS) that should permit natural and efficient communication with an air travel information system. SLS development at SRI divides roughly into three areas: speech recognition, natural language processing, and human interface design.The paper presents an overview of SRI's development effort and an analysis of selected technical challenges in subparts of this effort, including the choice of initial domains for such technology, the architecture for the integration of the two technologies, the attributes of goal-directed spontaneous speech, and the evaluation of spoken language systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-219"
  },
  "menyuk90_icslp": {
   "authors": [
    [
     "Paula",
     "Menyuk"
    ]
   ],
   "title": "Relationship between speech perception and production in language acquisition",
   "original": "i90_0733",
   "page_count": 4,
   "order": 221,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "The relation between speech perception and production during the period of lexical acquisition is still a matter of much discussion but little resolution. There have been three principal models that have been proposed to explain possible relations between perception and production. These models, the data they account for and the questions they leave unanswered will be discussed. A fourth model will be presented which attempts to explain the relation between speech perception and production when taking into account other aspects of language, and all possible constraints on the process of mapping perceptions into productions. The model will be tested by examining how well it can account for different types of findings about young children's speech production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-220"
  },
  "meltzoff90_icslp": {
   "authors": [
    [
     "Andrew N.",
     "Meltzoff"
    ],
    [
     "Alison",
     "Gopnik"
    ]
   ],
   "title": "Relations between thought and language in infancy",
   "original": "i90_0736",
   "page_count": 5,
   "order": 222,
   "p1": "736",
   "pn": "740",
   "abstract": [
    "Reported here are developmental relations between language and thought in the one-word period. However, these relations are different from those discussed in classic developmental theories like Piaget's. Piaget proposed that deferred imitation emerged at about 18 months, in synchrony with other aspects of the \"semiotic function\" including spoken language. Our findings establish deferred imitation in 9-month-olds. Thus, the emergence of deferred imitation is not a cognitive correlate of language; it is exhibited before language is acquired. We develop the \"specificity hypothesis\" concerning the interrelation between certain aspects of early semantic and cognitive development. Data are adduced showing highly specific relations between: (a) categorization and the onset of the \"naming explosion,\" (b) object permanence and the use of words encoding disappearance, and (c) means-ends understanding and words encoding success and failure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-221"
  },
  "kohno90b_icslp": {
   "authors": [
    [
     "Movto",
     "Kohno"
    ]
   ],
   "title": "The role of rhythm in the first and second language aquisition",
   "original": "i90_0741",
   "page_count": 4,
   "order": 223,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "First, neuropsychological fundamentals of rhythm perception will be shown by the research in the mechanism of timing control of a patient with infarction involving the forebrain commissural fibers, children with age variety from one year and half to nine years old and a normal adult. The timing measures thus cleared up will be innate and universal. Then the unit by which listeners chunk the flow of utterance to carry on the work of listening, that is, so called 'perceptual sense unit' will be identified by the experiments in the effects of pausing on listening comprehension, and the nature of the unit will be explained from the viewpoint of the above-mentioned universal timing measures. On the basis of all these facts, the process of listening comprehension will be elucidated by making clear the mechanism of echoic memory which gives a very important suggestions to the process of language acquisition and learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-222"
  },
  "kuhl90_icslp": {
   "authors": [
    [
     "Patricia K.",
     "Kuhl"
    ]
   ],
   "title": "Towards a new theory of the development of speech perception",
   "original": "i90_0745",
   "page_count": 4,
   "order": 224,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "Among topics related to the acquisition of language, the acquisition of speech is particularly fascinating. Speech units - the consonants and vowels that make up words - are comprised of features. The perception of these features has been studied in very young infants, and this has provided a window on the acquisition of language from its very beginning. Infants display highly sophisticated speech perception skills, leading theorists to emphasize innate mechanisms in infant speech perception. Linguistic experience eventually plays a role, however, because adults from different language environments perceive the same speech sounds very differently. At what age does linguistic experience affect phonetic perception? The new cross-language studies reported here show that exposure to a specific language alters infants' perception of speech by 6 months of age. The fact that phonetic perception is affected by early linguistic input suggests a new model of the development of speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-223"
  },
  "kojima90_icslp": {
   "authors": [
    [
     "Shozo",
     "Kojima"
    ]
   ],
   "title": "Audition and speech perception in the chimpanzee",
   "original": "i90_0749",
   "page_count": 4,
   "order": 225,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "The basic auditory functions and the perception of vowel-like vocal sounds (grunts) were studied in the chimpanzee. The chimpanzee showed a W-shaped auditory sensitivity function. The chimpanzee was more sensitive to 1-kHz and 8-kHz than 2-kHz and 4-kHz tones. It was found that the first formant was more important than the second formant for the perception of these vocal sounds. The chimpanzee did not vocalize vowel-like sounds [i] and [el. The relationship between the perception of grunts and the auditory sensitivity function and between the perception and the production of grunts of the chimpanzee was discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-224"
  },
  "halle90_icslp": {
   "authors": [
    [
     "Pierre",
     "Halle"
    ],
    [
     "Benedicte de",
     "Boysson-Bardies"
    ]
   ],
   "title": "Prosodic and phonetic patterning of disyllables produced by Japanese versus French infants",
   "original": "i90_0753",
   "page_count": 4,
   "order": 226,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "As a subset of a large scale study of American, French, Japanese, and Swedish infants' vocal productions, designed for tracing the emergence of language-specific features, we present some results on Japanese and French children. From the first words on, the consonant repertoires and some phonotactic characteristics of infants' vocalizations reflect ambient language specificities, as appearing in sets of \"target\" words, familiar to infants in each language group. We also examine some prosodic aspects of the disyllables produced by 3 Japanese and 4 French children at the 25-word stage, a little before the phonological stage. F0 contour and syllabic durations in disyllables (words or babbling), are also clearly language- specific. For French infants, rising F0 contours and final syllable lengthening are the rule, whereas the opposite patterns are found for Japanese children.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-225"
  },
  "yamada90_icslp": {
   "authors": [
    [
     "Reiko A.",
     "Yamada"
    ],
    [
     "Yoh'ichi",
     "Tohkura"
    ]
   ],
   "title": "Perception and production of syllable-initial English /r/ and /l/ by native speakers of Japanese",
   "original": "i90_0757",
   "page_count": 4,
   "order": 227,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "This study investigated how native speakers of Japanese, who are midway toward acquiring English phonemes, perceive and produce American English /r/, /I/, and /w/ sounds. From the perceptual experiments using synthesized stimuli, the following results are obtained. Japanese listeners perceive /r/ and /I/ sounds non-categorically, and they use both F2 and F3 frequencies as cues to identify /r/ and /I/. In contrast, American listeners perceive these sounds categorically using F3 as the primary cue. This result is consistent with the result obtained from the acoustic analysis of the /r/ and /I/ sounds produced by native Japanese. F3 frequency is a main feature for American subjects to produce /r/ differently from /I/. Such acoustic features, i.e. formant frequencies of the consonant part, are not well differentiated into phoneme categories when Japanese speakers produce these phonemes. However, the tendency to use both F2 and F3 frequencies, even as production cues, is observed in the sounds by Japanese speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-226"
  },
  "mochizukisudo90_icslp": {
   "authors": [
    [
     "Michiko",
     "Mochizuki-Sudo"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "The perception of inter-stress-intervals in Japanese speakers of English",
   "original": "i90_0761",
   "page_count": 4,
   "order": 228,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "This study examines the perception of inter-stress-intervals by the Americans and the Japanese learners of English. Perception experiments were performed by using LPC analysis and synthesis of speech in which the durations of target stressed vowels were lengthened or shortened. We measured the discrimination of the stressed vowel durations as well as the influence of the durational change in a stressed vowel on the perception of the naturalness of temporal patterns. The results of the experiments demonstrated that non-proficient Japanese speakers of English are not less sensitive than the Americans in discriminating the durations of stressed vowels. However, their judgments of what is natural for a stressed vowel span a broader range of vowel durations than for the Americans. Moreover, a durational adjustment in the following unstressed syllable compensated for the lengthening of the stressed vowel in the case of the American listeners, but not in the case of the Japanese listeners. These results indicate that the ISI is a temporal unit in the perception of English for the Americans, but not for the non-proficient Japanese speakers, thus lending support for the traditional distinction between English versus Japanese as being stress-timed versus mora-timed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-227"
  },
  "berkley90_icslp": {
   "authors": [
    [
     "D. A.",
     "Berkley"
    ],
    [
     "James L.",
     "Flanagan"
    ]
   ],
   "title": "Integration of speech recognition, text-to-speech synthesis, and talker verification into a hands-free audio/image teleconferencing system (humanet)",
   "original": "i90_0861",
   "page_count": 4,
   "order": 229,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "This report describes the design and implementation of a digital teleconferencing system that integrates a number of speech technologies together with image and data facilities. The aim is to provide a variety of sophisticated communication features that are easy to learn and use. The system is called HuMaNet, for Human/Machine Network. The system is controlled totally and interactively hands-free by natural speech. The system combines the technologies of speech recognition, text synthesis, and talker verification with autodirective microphone arrays, image compression, data and hypertext management to provide high-quality audio and image conferencing over basic-rate ISDN (Integrated Services Digital Network). The present public-switched transport capacity provides \"2B+D\", or two 64 k bits/sec circuit-switched channels (2B), and one 16 k bits/sec packet-switched channel (D).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-228"
  },
  "velius90_icslp": {
   "authors": [
    [
     "G.",
     "Velius"
    ],
    [
     "C.",
     "Kamm"
    ],
    [
     "Mary-Jo",
     "Altom"
    ],
    [
     "T. C.",
     "Feustel"
    ],
    [
     "Marian J.",
     "Macchi"
    ],
    [
     "Murray F.",
     "Spiegel"
    ]
   ],
   "title": "Bellcore efforts in applying speech technology to telephone network services",
   "original": "i90_0865",
   "page_count": 4,
   "order": 230,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "Recent speech technology research at Bellcore has contributed to several telephone network applications that are now either being deployed or tested. These applications include Automated Alternate Billing Services (AABS) for collect and third party calls using automatic speech recognition; reducing operator connect time in Directory Assistance (DA) via speech compression; an Automated Customer Name and Address (ACNA) service using text-to-speech synthesis; and remote security services such as credit card validation and home-incarceration checks using Speaker Identity Verification (SIV). In this paper, we describe each of these applications and the speech technology and human-interface issues critical for successful deployment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-229"
  },
  "yato90_icslp": {
   "authors": [
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Kazuki",
     "Katagisi"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Extension number guidance system",
   "original": "i90_0869",
   "page_count": 4,
   "order": 231,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "The authors have developed a speaker independent word recognition system for the purpose of giving extension number guidance to our laboratories. The system consists of a workstation, AD converter, PBX interface and speech synthesizer. The vocabulary consists of the names of persons and laboratories, and its size is about 300. The recognition process is based on three methods: continuous DP matching, multi-templates and SPLIT methods. The system announces the results of recognition and guidance of the extension number to the user by using a Klatt type speech synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-230"
  },
  "sato90b_icslp": {
   "authors": [
    [
     "Hirokazu",
     "Sato"
    ]
   ],
   "title": "Japanese text-to-speech equipment: current applications and trends",
   "original": "i90_0873",
   "page_count": 4,
   "order": 232,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "This paper introduces the Japanese text-to-speech equipment developed by Nippon Telegraph and Telephone Corporation, and describes several current applications of text-to-speech systems such as ANSER. Besides the original role of generating speech from unrestricted texts, other application possibilities are discussed. Moreover, this paper discusses the importance of achieving low-priced equipment, improving the synthetic speech quality, and realizing the controllability of voice quality and speech style. The solution of these problems will expand the future application fields of text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-231"
  },
  "amadorhernandez90_icslp": {
   "authors": [
    [
     "Mariscela",
     "Amador-Hernandez"
    ],
    [
     "Bathsheba J.",
     "Malsheen"
    ]
   ],
   "title": "The synthesis of dialectal variation in English and Spanish",
   "original": "i90_0877",
   "page_count": 3,
   "order": 233,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "At present, users of text-to-speech products can select either male or female voice, or even a multitude of different languages for various applications. However, the next generation of text-to-speech converters will need to provide additional features, such as the capability of switching from one dialect of a language to another. This paper shows that if a given language, other dialects can easily be derived from the base dialect. This paper will provide a strategy for synthesizing examples of different dialects of English and Spanish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-232"
  },
  "saito90_icslp": {
   "authors": [
    [
     "Hiroyoshi",
     "Saito"
    ],
    [
     "Motoshi",
     "Kurihara"
    ],
    [
     "Ken-ichiro",
     "Kobayashi"
    ],
    [
     "Yoshiyuki",
     "Hara"
    ],
    [
     "Naritoshi",
     "Saito"
    ]
   ],
   "title": "A Japanese text-to-speech system for electronic mail",
   "original": "i90_0881",
   "page_count": 4,
   "order": 234,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "This paper describes the methods for a Japanese text-to-speech system the authors have developed and its application to a voice supply mechanism for electronic mail. This system analyzes an arbitrary text morphologically with a dictionary which has 100,000 entry words, their grammatical attributes, and their phonetic and accent information. This analysis separates the text into individual terms, and the phonetic and accent data for the individual terms are obtained from the dictionary. In the proposed system, rules play an important role in improving the speech in order to produce a high quality speech. These rules decide whether the phonetic symbols for each term are separated or unified, and whether a part of them are modified or not : to modify a vowel sound to a long sound, to make a breath sound, and to make a nasal sound, whether the accent position of each term is shifted or not, and how long the duration between individual terms is. The authors tried out this system in the field of electronic mail. If the mail receiver wishes to confirm a mail text when out of his or her office, the person can call the mail box by telephone and obtain a synthesized speech of the mail produced by the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-233"
  },
  "nitta90_icslp": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Nobuo",
     "Sugi"
    ]
   ],
   "title": "Issues concerning voice input applications",
   "original": "i90_0885",
   "page_count": 4,
   "order": 235,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "At present, many voice input systems are unable to consistently achieve accurate recognition in practical environments. This paper describes issues and some solutions concerning voice input applications from two different standpoints of a speech recognition researcher and an application designer. The authors point out that both the robustness in speech recognition and the well-designed user interface are particularly important for voice input applications to successfully incorporate speech recognition and to complete user's demands. Some practical applications, such as voice-activated telephones, ticket vending machines, and elevators, are also discussed, and two important items of the multimodality and well-designed prompts are emphasized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-234"
  },
  "tsuboi90b_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Tsuboi"
    ],
    [
     "Noboru",
     "Sugamura"
    ]
   ],
   "title": "A prototype for a speech-to-text transcription system",
   "original": "i90_0889",
   "page_count": 4,
   "order": 236,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "A prototype for a speech-to-text transcription system is described. This system recognizes continuous phrasal speech and transcribes it in Japanese text. This paper outlines methods for acoustic and linguistic processing, and describes the system configuration and results of performance evaluation tests. As a text is spoken phrase by phrase, it is recognized by a word-spotting method using a continuous dynamic programming technique. High frequency words and CVs in continuous phrasal speech are detected using established CV and word templates. The CV and word candidates are converted to phrase candidates using a word dictionary, inflection table, post positional word dictionary, compound word table, and phrase syntactic pattern table. Frequent phrase co-occurrence patterns are used to select feasible phrase candidates. A performance evaluation test is carried out for Japanese X-ray CT scanning reports. Conversion accuracies of 80% and 65% are obtained for normal and abnormal medical findings, at input speeds of 100 Chinese characters/minute and 50 Chinese characters/minute respectively. These input speeds equal those of a professional transcriber and a novice transcriber after 20 days of training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-235"
  },
  "hamada90_icslp": {
   "authors": [
    [
     "Masahiro",
     "Hamada"
    ],
    [
     "Yumi",
     "Takizawa"
    ],
    [
     "Takeshi",
     "Norimatsu"
    ]
   ],
   "title": "A noise robust speech recognition system",
   "original": "i90_0893",
   "page_count": 4,
   "order": 237,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "This paper describes issues in developing a noise robust speaker-dependent isolated word recognizer usedin a running automobile. A new definition of signal-to-noise ratio is proposed to cope with the low-pass type auto noise and frequency characteristics of human hearing, and to evaluate speech power level properly. Noise robustness is realized by the improvement of end-point detection and sharpening of spectral peaks. To minimize memory size and computational load, cepstrum coefficients of the matching template are quantized with the steps designed according to the inverse of cepstrum weighting function. Recognition and ADPCM recording-synthesis are executed in parallel in one DSP. Road tests revealed that the recognition rate for 20 words vocabulary exceeds 95% with 2 successive utterances and 40cm of microphone distance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-236"
  },
  "corazzat90_icslp": {
   "authors": [
    [
     "A.",
     "Corazzat"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "R.",
     "Gretter"
    ],
    [
     "G.",
     "Satta"
    ]
   ],
   "title": "Computation of probabilities for island-driven parsers",
   "original": "i90_0897",
   "page_count": 4,
   "order": 238,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "Language models for automatic speech recognition are used for computing probabilities of theories corresponding to partial interpretations of sentences. Algorithms have been developed for computing these probabilities when theories grow in a strictly left-to-right fashion. This paper introduces a new framework for the computation of probabilities of theories that contain a gap corresponding to an uninterpreted signal segment. Algorithms have been developed and their complexity is here derived. The use of these algorithms in an island-driven parser is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-237"
  },
  "su90_icslp": {
   "authors": [
    [
     "Keh-Yih",
     "Su"
    ],
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "A unified probabilistic score function for integrating speech and language information in spoken language processing",
   "original": "i90_0901",
   "page_count": 4,
   "order": 239,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "In this paper, a unified approach for integrating speech and language information in spoken language processing is proposed. This unified approach uses probabilistic score functions to characterize different levels of knowledge in a uniform way. By jointly considering the knowledge from different levels, from acoustics to semantics, the processing capability of speech and language processing are both enhanced with the information provided by the other modules. Besides, since the perplexity of this task is greatly reduced by this approach, the searching efficiency is also improved. After applying this proposed formulation to a Chinese Phonetic Typewriter task, great improvement in sentence accuracy, compared with the word lattice method, is observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-238"
  },
  "kita90_icslp": {
   "authors": [
    [
     "Kenji",
     "Kita"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Junko",
     "Hosaka"
    ],
    [
     "Terumasa",
     "Ehara"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "Continuous speech recognition using two-level LR parsing",
   "original": "i90_0905",
   "page_count": 4,
   "order": 240,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "This paper describes a continuous speech recognition system using two-level predictive LR parsing. ATR has already implemented a predictive LR parsing algorithm in an HMM-based speech recognition system for Japanese. However, up to now, this system has used only intra-phrase grammatical constraints. In Japanese, a sentence is composed of several phrases, thus two kinds of grammars, namely an intra-phrase grammar and an inter-phrase grammar, are sufficient for recognizing sentences. Two-level predictive LR parsing makes it possible to use not only intra-phrase grammatical constraints but also inter-phrase grammatical constraints during speech recognition. The system is applied to Japanese sentence recognition, and attains a word accuracy of 95.9% and a sentence accuracy of 84.7%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-239"
  },
  "saito90b_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Saito"
    ]
   ],
   "title": "Gap-filling LR parsing for noisy spoken input: towards interactive speech recognition",
   "original": "i90_0909",
   "page_count": 4,
   "order": 241,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "This paper introduces the new parsing technique \"gap-filling LR parsing.\" This technique enhances the robustness of the generalized LR parsing and is very useful in speech recognition in that the parser can skip the very noisy part of the input The unidentified portion is resolved by the re-utterance of that portion by the speaker, which is parsed very efficiently by using the parse record of the first utterance. Experiments in parsing Japanese spoken! input show the significant effectiveness of this interactive approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-240"
  },
  "bornerand90_icslp": {
   "authors": [
    [
     "S.",
     "Bornerand"
    ],
    [
     "Francoise",
     "Neel"
    ],
    [
     "G.",
     "Sabah"
    ]
   ],
   "title": "Semantic weights derived from syntax-directed understanding in DTW-based spoken language processing",
   "original": "i90_0913",
   "page_count": 4,
   "order": 242,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "This paper deals with the integration of different levels of knowledge in a continous speech recognition framework. A system has been developed to carry out recognition and understanding processes in parallel. We present a method which allows the system to dynamically compute weights, taking semantic knowledge into account during the recognition process, which is based on a dynamic time warping (IjTW) algorithm. The weights change during the course of an utterance recognition process, according to the interpretation of partial recognized sentences. Nested into the DTW algorithm, a natural language processor (NLP) computes semantic weights. It builds an interpretation using the joining operation of the conceptual graph model. The syntax describes the joining rules of conceptual graphs. Then, we show how a set of joining rules can be represented by a conceptual grammar from which a parser can be generated. The parsing process evaluates an interpretation of a partially recognized sentence according to a criterion using the overlapping rate of all the conceptual graphs which participate into the interpretation, in order to return a semantic weight.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-241"
  },
  "kitano90_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kitano"
    ],
    [
     "Tetsuya",
     "Higuchi"
    ],
    [
     "Masaru",
     "Tomita"
    ]
   ],
   "title": "Massively parallel spoken language processing using a parallel associative processor IXM2",
   "original": "i90_0917",
   "page_count": 4,
   "order": 243,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "This paper reports experimental results of a natural language processing scheme for spoken inputs implemented on a massively parallel machine IXM2. We have carried out performance tests on language models on IXM2 including a word-pair grammar and a memory-based parsing. Experimental results demonstrate that use of IXM2 as a language processing engine for certain language models is a promising approach in attaining a real-time response. Syntactic recognition completes in a few milli-seconds. Plus, response time grows only linearly to the length of the input sentences, attaining approximately of order o(ri).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-242"
  },
  "morimoto90_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Hitoshi",
     "Iida"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "Integration of speech recognition and language processing in spoken language translation system (SL-TRANS)",
   "original": "i90_0921",
   "page_count": 4,
   "order": 244,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "SL-TRANS is an experimental spoken language translation system from Japanese to English. It can recognize Japanese speech, translate it to English, and output a synthesized English voice. In this system, a new method of integrating speech recognition and language processing is adopted. This method is composed of three stages, predictive speech recognition using intra-phase grammar, candidate filtering using inter-phrase Kakariuke dependency, and, a unification based language analyzer to which sentence preference mechanism is added. In each stage, appropriate linguistic information is used in a step by step fashion in order to get the most plausible sentence. Some preliminary experiment results for the system are also reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-243"
  },
  "sakano90_icslp": {
   "authors": [
    [
     "Toshiya",
     "Sakano"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "Design principle of language model for speech recognition",
   "original": "i90_0925",
   "page_count": 4,
   "order": 245,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "In speech recognition, the recognition rate can be improved by using statistical information about linguistic texts. However, there is no criterion for selecting sample linguistic texts from those available. If unbalanced linguistic text samples are selected, the information extracted would not be suitable for a linguistic model. To solve this problem, we need to quantitatively analyze linguistic texts. In this paper, we introduce a method to quantitatively analyze linguistic texts, and propose a method for selecting linguistic texts. Moreover, we describe the possibility of developing a criterion for selecting test texts needed for system evaluation, and show the relationship between recognition system performance and the features of the sample text group used for the linguistic model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-244"
  },
  "matsunaga90_icslp": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Sentence speech recognition using semantic dependency analysis",
   "original": "i90_0929",
   "page_count": 4,
   "order": 246,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "This paper describes a sentence speech recognition system based on phoneme-based hidden Markov models (HMMs) and two grammatical constraints: a syntactic grammar of phrase structure and a semantic dependency grammar of sentence structure. A joint score, combining acoustic likelihood and linguistic certainty factors derived from phoneme based HMMs and two grammatical constraints, is maximized to obtain the optimal sentence recognition. A semantic analysis algorithm globally optimizes the joint score. This algorithm is based on two key techniques: most likely multi-phrase candidate-detection using the Viterbi algorithm, and breadth-first search for dependency parsing. Where the perplexity of the phrase syntax is 40, this system increases phrase recognition performance in the sentences by approximately 14%, showing the effectiveness of semantic dependency analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-245"
  },
  "lisker90_icslp": {
   "authors": [
    [
     "Leigh",
     "Lisker"
    ]
   ],
   "title": "Distinctive, redundant, predictable, neotssary, sufficffint accounting for English /bdg/-/ptk/",
   "original": "i90_0933",
   "page_count": 4,
   "order": 247,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "Traditionally phonologists have appealed to three independent phonetic features in separating English /bdg/ and /ptk/: V(oice), A(spiration), and F(orce of articulation). Moreover, these features were unequal,- one \"distinctive,\" the others \"predictable\" and/or \"redundant.\" Once F was distinctive; more recently V has been assigned this role. Moreover, where a distinctive feature once was the principal category marker, now it is not clear that a \"distinctive\" feature has special perceptual status. Experimental evidence suggests that no single property can be isolated as the perceptually distinctive one, nor does any seem so devoid of \"cue potential\" that it maintains its neutrality under all the conditions that experimental ingenuity and modern technical facilities can contrive.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-246"
  },
  "kassel90_icslp": {
   "authors": [
    [
     "Rob",
     "Kassel"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "An information theoretic approach to the study of phoneme collocational constraints",
   "original": "i90_0937",
   "page_count": 4,
   "order": 248,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "In this paper we describe a lexical study of phoneme collocational constraints using a metric motivated by information theory. We used a pairwise, hierarchical clustering technique to combine phonemes into classes using a normalized measure of mutual information. The result of this clustering process can be displayed as a dendrogram, from which one may select an arbitrary number of equivalence classes. We have conducted a number of experiments investigating phoneme collocational constraints within pairs and triplets. In many cases we found that phonemes are organized into classes that share certain phonological features. In fact, phonemes that have similar acoustic properties often exhibit similar collocational constraints. We also compared the constraining power of our phoneme classes with those chosen by a phonological criterion, and found ours to be more than competitive. Based on our results, we conclude that our information theoretic metric is particularly useful as a description of lexical constraining power.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-247"
  },
  "derwing90_icslp": {
   "authors": [
    [
     "Bruce L.",
     "Derwing"
    ],
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "Real-time effects of some intrasyllabic collocational constraints in English",
   "original": "i90_0941",
   "page_count": 3,
   "order": 249,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "This study sought experimental support for the following two proposed collocational constraints for English: (1) that syllable peaks consist of a vowel plus optional sonorant and (2) that the second consonant of a coda must be a coronal. In our experiment, subjects were trained to identify vowel or coda substitutions in words containing only post-vocalic obstruents, then were tested on their ability to identify the same substitutions in words where the first post-vocalic element was a nasal or /I/, which was sometimes treated as part of the nucleus (tense vs. lax vowel) and sometimes as part of the coda (coronal vs. other consonant or cluster). The following significant differences emerged, but only on the two coda tasks: (1) nasals were, in general, more coda-like ('C-sticky') than /I/, (2) nasals and /I/ together were more coda-like after tense vowels than after lax vowels, and (3) they were also more coda-like before single coronal consonants than elsewhere, as expected from the theory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-248"
  },
  "dalsgaard90_icslp": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "William",
     "Barry"
    ]
   ],
   "title": "Acoustic-phonetic features in the framework of neural-network multi-lingual label alignment",
   "original": "i90_0945",
   "page_count": 4,
   "order": 250,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "Results are presented from a multi-speaker, multi-lingual method of phonetic label alignment which is based on a combined application of a Self-Organising Neural Network and a Viterbi decoding and level-building technique constrained by an independently specified string of phonetic segments. The Neural Network is trained to convert vectors of cepstral coefficients into vectors of continuously valued acoustic-phonetic features, and to derive a multi-dimensional Gaussian probability density function for each phonemic unit. Multi-lingual application simply requires the definition of the features for each new language. The Viterbi decoding and Level-Building technique is applied to the task of performing label alignment on large speech corpora. The paper firstly presents results for Danish and English, with distributions for selected features and phonemes in the two languages to show the validity of the approach. Covariance analysis within a language allows a reduction of the features to a maximally discriminative set, and comparison across the languages points to the multi- lingual validity of the feature definitions. Secondly, results are given in a number of histograms showing the accuracy of the alignment settings for selected phoneme classes compared to corresponding settings from manually labelled test databases. The work has been developed in part under the ESPRIT project 'Speech Assessment Methodology' (SAM).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-249"
  },
  "hieronymus90_icslp": {
   "authors": [
    [
     "James L.",
     "Hieronymus"
    ]
   ],
   "title": "Preliminary study of vowel coarticulation in british English",
   "original": "i90_0949",
   "page_count": 4,
   "order": 251,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "Coarticulation in continuous speech causes vowel formant tracks to be affected by nearby phonemes. Generally continuous speech causes the vowel formant targets to be centralised relative to their isolated word counterparts. The present study uses data from one male talker in the CSTR/ATR speech database saying 200 phonetically rich sentences. By concentrating on one talker, coarticulation can be studied without the confounding effects of accents, speech habits and formant ranges from many talkers. While some of these effects could be normalised out of multitalker data, some residual variance remains with all the methods we have tried. The 21 vowels of RP British English have been studied, the 14 monothongal vowels, /ii, i, a, e, aa, uh, oo, o, u, u\", @@, @, 1/ and the 7 diphthongal vowels /ai, au, e@, ei, i@, oi, ou, u@/. Formant tracks were computed automatically and scanned for errors by hand. The formant frequency values at the right and left edges and the temporal center of the hand labelled vowel region are obtained. These values are plotted on scatter plots and analyzed using statistical methods. Generally the vowels are most effected by nearby semi-vowels /l, r, y, w/. No simple relationship between adjacent phoneme place of articulation and the vowel target change has been found. The data shows the presence of \"robust vowels\" which are not greatly effected by nearby semi-vowels. These vowels are not simply stressed vowels, but depend on duration and others factors being studied. Similar results were found in American English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-250"
  },
  "huang90_icslp": {
   "authors": [
    [
     "Caroline B.",
     "Huang"
    ]
   ],
   "title": "Effects of context, stress, and speech style on american vowels",
   "original": "i90_0953",
   "page_count": 4,
   "order": 252,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "In the present study, formant frequencies are measured in General American English vowels taken from a read story which is well-controlled with respect to consonant context, lexical stress, and speech style, including words in a carrier phrase, continuously read speech, and spontaneous speech. The vowels carry primary or secondary lexical stress. In total, the database consists of approximately 1500 vowel tokens from four speakers. Vowels from each speaker are analyzed separately. Results indicate that consonant context has a greater effect on formant frequencies of non-reduced vowels at the midpoint than lexical stress or speech style.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-251"
  },
  "djoudi90b_icslp": {
   "authors": [
    [
     "M.",
     "Djoudi"
    ],
    [
     "H.",
     "Aouizerat"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Phonetic study and recognition of standard Arabic emphatic consonants",
   "original": "i90_0957",
   "page_count": 4,
   "order": 253,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "In this paper we propose a phonetic study of emphatic consonants in standard Arabic, which is essentially based on the spectrographic examination of these consonants in various production contexts. This study allowed us to determine the pertinent parameters to the recognition of emphatic consonants in continuous speech. The recognition stage uses the SAPHA system, developed for the acoustic-phonetic decoding of standard Arabic and deals with the following points: - segmentation of the speech signal into large phonetic classes; - extraction of parameters necessary for the recognition of emphatic consonants; - phonetic identification of these consonants by using an expert system based on production rules. The results of the segmentation and of the recognition for three male speakers are given commented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-252"
  },
  "recasens90_icslp": {
   "authors": [
    [
     "Daniel",
     "Recasens"
    ],
    [
     "Edda",
     "Farnetani"
    ]
   ],
   "title": "Articulatory and acoustic properties of different allophones of /l/ in american English, catalan and Italian",
   "original": "i90_0961",
   "page_count": 4,
   "order": 254,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "This paper attempts to investigate whether velarization is a scalar articulatory feature and if an increase in degree of velarization conveys an increment in the degree of constraint upon tongue dorsum activity. Electropalatographic and acoustical data for [1] in different languages and contextual conditions allow isolating three degrees of velarization in the consonant. Precise articulatory and acoustic correlates are given for each velarization group. Moreover V-to-C coarticulatory effects in dorsopalatal contact were found to decrease with degree of velarization for [1] thus suggesting that changes in degree of velarization convey modifications in degree of articulatory constraint.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-253"
  },
  "suzuki90b_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Suzuki"
    ],
    [
     "Ghen",
     "Ohyama"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "In search of a method to improve the prosodic features of English spoken by Japanese",
   "original": "i90_0965",
   "page_count": 4,
   "order": 255,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "A series of studies were conducted on the prosodic features of the English spoken by Japanese to find out which features (i.e. the duration of each sound, the fundamental frequency change and the intensity change) should be altered and how they should be changed in order for their English to be judged more English-like. Various combinations of three prosodic features, in an English sentence uttered by a Japanese speaker in a typically Japanese fashion, were replaced with the same combinations of the prosodic features of the same English sentence read by an Englishman. Through a listening test it was found that the duration component and the fundamental frequency change played far more important roles than the intensity component in the acceptability judgment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-254"
  },
  "bond90_icslp": {
   "authors": [
    [
     "Zinny S.",
     "Bond"
    ],
    [
     "Thomas J.",
     "Moore"
    ]
   ],
   "title": "A note on loud and lombard speech",
   "original": "i90_0969",
   "page_count": 4,
   "order": 256,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "The purpose of this report is to compare speech which is loud as a consequence of noise exposure (Lombard speech) with speech which is loud deliberately. One male speaker was recorded in six speaking conditions: ambient noise, high noise, and intentionally loud speech, all three recorded with a boom microphone and while wearing an oxygen mask. Lombard speech and deliberately loud speech shared more similarities than differences and appear to result from the same speech production mechanisms. Both were produced with more effort so that energy and fundamental frequency increased. Both were produced with a wider mouth opening so that formants, particularly Fl, shifted. The oxygen mask minimized changes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-255"
  },
  "jekosch90_icslp": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "A weighted intelligibility measure for speech assessment",
   "original": "i90_0973",
   "page_count": 4,
   "order": 257,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "A closer look at intelligibility failures shows that some phoneme confusions are more probable than others, since stimulus and response have a larger number of phonetic features in common. Since they can partly also be found in natural speech, these failures indicate consequently not solely a quality loss of synthetic or degraded natural speech but, moreover, they can also be a mirror image of human speech in general. Consequently, for the assessment of, e.g., speech output systems, those confusions that can be observed frequently in natural speech as well have to be weighted less than those ones that occur very seldom.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-256"
  },
  "hayashi90b_icslp": {
   "authors": [
    [
     "Shinji",
     "Hayashi"
    ]
   ],
   "title": "Improvements in binaural articulation score by simulated localization using head-related transfer functions",
   "original": "i90_0977",
   "page_count": 4,
   "order": 258,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "Speech localization is simulated using the head-related transfer functions (HRTF) in order to clarify the binaural effects on speech intelligibility under the condition of noise disturbance. The HRTF filter coefficients are specifically calculated for the subject's own head and ears. The frequency response of the headphone is compensated for using an inverse filter calculated from the response at the subject's own ear canal entrance point. The phoneme articulation score under disturbance of noise is improved when the simulated distance in localization between the phoneme and noise is increased even though the total signal to noise ratio is maintained constant. This result shows the binaural effect in speech intelligibility under the noise disturbance, which is regarded as a part of the well-known cocktail party effect.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-257"
  },
  "silverman90_icslp": {
   "authors": [
    [
     "Kim",
     "Silverman"
    ],
    [
     "Sara",
     "Basson"
    ],
    [
     "Suzi",
     "Levas"
    ]
   ],
   "title": "Evaluating synthesiser performance: is segmental intelligibility enough?",
   "original": "i90_0981",
   "page_count": 4,
   "order": 259,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "Laboratory-based evaluations of synthetic speech often suggest that it is as intelligible as natural speech. Yet studies and experience in applied settings do not confirm this. We identify two issues underlying this discrepancy: (i) neither the speech material nor the listeners' task in typical evaluations sufficiently represent application conditions, and (ii) tests should measure the cognitive load accompanying a given intelligibility score. Our preliminary data for two commercial synthesisers underline both issues: the two synthesisers were ranked in one order on a segmental intelligibility test, but in the opposite order on a more application-like comprehension test. For the latter test, listeners' accuracy may be related to their performance speed. Results are related to the underlying issues, and two approaches to measurement of cognitive load are suggested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-258"
  },
  "maehara90_icslp": {
   "authors": [
    [
     "Fumio",
     "Maehara"
    ],
    [
     "Masamichi",
     "Nakagawa"
    ],
    [
     "Kunio",
     "Nobori"
    ],
    [
     "Toshiyuki",
     "Maeda"
    ],
    [
     "Tsutomu",
     "Mori"
    ],
    [
     "Makoto",
     "Fujimoto"
    ]
   ],
   "title": "Media conversion into language and voice for intelligent communication",
   "original": "i90_0985",
   "page_count": 4,
   "order": 260,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "An integrated management of multi-media, including voice, video, and document data would become essential in a next generation communication systems for computers and information networks. As the natural language is considered definitely advantageous to represent such multi-media data, an intelligent communication system wherein a character recognizer, translation device enabling communications between different languages, speech-rule synthesizer and an image retrieval system by which this system is systematically integrated, and a natural language processing algorithm for managing such system are described here in addition to reports on the results of evaluation and problems arisen therefrom when this system is practically applied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-259"
  },
  "carlson90b_icslp": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "BjÃ¶rn",
     "GranstrÃ¶m"
    ],
    [
     "Lennart",
     "Nord"
    ]
   ],
   "title": "Segmental intelligibility of synthetic and natural speech in real and nonsense words",
   "original": "i90_0989",
   "page_count": 4,
   "order": 261,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "We have been using the preliminary version of the Esprit/SAM test procedure for synthetic speech to evaluate an experimental version of the multilingual text-to-speech system under development at our department. The proposed segmental test battery includes: a) hearing tests of the subjects, b) the familiarisation to the special type of speech synthesizer by an introductory paragraph, c) lists of CV, VC and VCV stimuli according to the phonotactic structure of the individual language. Tests on natural speech have also been performed forming a baseline for the synthesis evaluation and at the same time indicating the subjects' ability to give unambiguous orthographic response to nonsense words. An interesting question in this context is the phonemic awareness of the listeners. The Swedish fricative allophone set is a good example, where difficulties in labelling has to be studied carefully. Results will be presented at the meeting and compared to data reported earlier. We will also present data on the intelligibility of monosyllabic words drawn from the most frequent 10 000 words in Swedish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-260"
  },
  "chan90_icslp": {
   "authors": [
    [
     "Chorkin",
     "Chan"
    ],
    [
     "Ren-hua",
     "Wang"
    ]
   ],
   "title": "The HKU-USTC speech corpus",
   "original": "i90_0993",
   "page_count": 4,
   "order": 262,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "A design of spoken Chinese corpus is proposed which consists of five sub-corpora Cl to C5. The design principles are (1) Mono-syllables are important not only for the recognition of isolated syllables but also for the recognition of connected spoken Chinese because they simplify the isolation of phonetic information in the training phase, (2) A corpus including all inter-syllable triphones captures all the immediate left- and right-context of phones at syllabic boundaries. This is a logical and practical compromise between exhausting all possible syllabic transitions and keeping the corpus building effort at a manageable level. Cl consists of 433 mono-syllables and 4 consonant clusters in each of its four versions. Each toned syllable exists in at least one of the four versions and the 433 mono-syllables in each version include all the syllables in one of the four regular tones (yin, yang, shang and qu) plus all the neutral-toned ones. C2 is a collection of 16 digit strings each ranging from 4 to 7 digits in length. These strings exhaust all the inter-digits triphones. C3 has 30 geographic names each of two to five syllables. C4 consists of 859 short phrases each of six to nine syllables long forming the bulk of the inter-syllable triphone collection. C5 is a catch-all sub-corpus composed of all the inter-syllable triphones which do not appear in C2 to C4. These triphones are very seldomly used in the Chinese language today and can be ignored in recognizer training for practical purposes. Keywords:-Speech database, isolated speech, connected speech, Putonghua, inter-syllable triphones, coarticulatiori between syllables\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-261"
  },
  "svendsen90_icslp": {
   "authors": [
    [
     "Torbjorn",
     "Svendsen"
    ],
    [
     "Knut",
     "Kvale"
    ]
   ],
   "title": "Automatic alignment of phonemic labels with continuous speech",
   "original": "i90_0997",
   "page_count": 4,
   "order": 263,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "Annotation of speech waveforms with phonemic labels and markers defining the position of the labels within the wave-form is desirable for many purposes. In this paper we propose a method for automatic label alignment which can ease the task. The algorithm contists of two stages. The first stage segment the speech waveform into segments which contain acoustically similar speech frames. The second stage use the acoustic segment boundaries as anchor points in a phoneme based HMM type segmentation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-262"
  },
  "tuffelli90_icslp": {
   "authors": [
    [
     "D.",
     "Tuffelli"
    ],
    [
     "H. D.",
     "Wang"
    ]
   ],
   "title": "TELS: a speech time-expansion labelling system",
   "original": "i90_1001",
   "page_count": 4,
   "order": 264,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "In this paper we present a system designed for the broad labelling of a large speech acoustic database. This system uses an original labelling strategy which is based on 3 characteristics: a time-expanded (10 times) speech sound output, an automatic scanning of the corresponding label strings and a speech visualization (using a moving cursor which is synchronized with the time-expanded speech sound output and the label strings scanning). By using an algorithm that is similar to the SOLA algorithm proposed by Roucos [1], the time-expansion technique produces a good quality 10 times expanded speech signal from the speech signal to be labelled. The time-expanded speech listening brings out a fine speech structure. This result is usually attained, with difficulty, by careful spectrogram reading in a classic manual labelling system (signal editors).The advantage of the automatic label strings scanning mechanism is that it enables the cursor to be moved by the system instead of by the operator. Eventually the operator labelling task is only confirmation of the current label. Moreover the time-expanded mechanism can be viewed as a kind of \" acoustic magnifying glass \" which brings some additional perceptual information on the signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-263"
  },
  "arai90_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Arai"
    ],
    [
     "Yolchi",
     "Yamashita"
    ],
    [
     "Tadahiro",
     "Kitahashi"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "A speech labeling system based on knowledge processing",
   "original": "i90_1005",
   "page_count": 4,
   "order": 265,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "Speech labeling is an indispensable task for construction of speech data bases. Currently, speech labeling is done manually by experts. Thus it takes too much time and labor to label large amount of speech data. In order to decrease such labor, it is necessary to construct an automatic labeling system. The authors have been developing a speech labeling system based on a generate-and-test architecture with segmentation knowledge. Performance evaluation shows that for closed data, 99.1% segments are identified within 30ms of the locations determined by a human expert and 95.5% segments are also identified correctly for open data with rejection rates of 6.7% and 8.9%, respectively. This paper describes the system organization and the results of performance evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-264"
  },
  "tillmann90_icslp": {
   "authors": [
    [
     "Hans G.",
     "Tillmann"
    ],
    [
     "Maximilian",
     "Hadersbeck"
    ],
    [
     "Hans Georg",
     "Piroth"
    ],
    [
     "Barbara",
     "Eisen"
    ]
   ],
   "title": "Development and experimental use of phonwork a new phonetic workbench",
   "original": "i90_1009",
   "page_count": 4,
   "order": 266,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "PHONWORK is a new phonetic workbench which has been developed at the phonetic institute of the University of Munich for the segmentation and labeling of a large corpus of utterances and can now be used for detailed inspection of all kinds of phonetic material. One of our main interests in developing this system has been in designing a very high functionality for the phonetic user. The result is a human interface allowing fast access to a variety of different tools for segmentation and labeling of speech data. This paper will first describe the use of PHONWORK in detail, and second present a preliminary analysis of the distribution of the durations of German vowels processed by means of PHONWORK.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-265"
  },
  "chimoto90_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Chimoto"
    ],
    [
     "Hideaki",
     "Shinchi"
    ],
    [
     "Hideki",
     "Hashimoto"
    ],
    [
     "Shinya",
     "Amano"
    ]
   ],
   "title": "A speech recognition research environment based on large-scale word and concept dictionaries",
   "original": "i90_1013",
   "page_count": 4,
   "order": 267,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "This paper describes a speech recognition research environment based on large-scale Japanese word and concept dictionaries. The dictionaries are currently under development at Japan Electronic Dictionary Research Institute (EDR). The research environment includes user-interactive capabilities, which enable efficient upgrade and revision of the dictionaries; and it has high-speed hardware, which supports real-time processing of speech input data. Within this environment, a combination of the two dictionaries will be applied to deal with the inherent ambiguities of speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-266"
  },
  "chigier90_icslp": {
   "authors": [
    [
     "Benjamin",
     "Chigier"
    ],
    [
     "Judith",
     "Spitz"
    ]
   ],
   "title": "Are laboratory databases appropriate for training and testing telephone speech recognizers?",
   "original": "i90_1017",
   "page_count": 4,
   "order": 268,
   "p1": "1017",
   "pn": "1021",
   "abstract": [
    "Automatic speech recognition systems are typically trained on speech data collected in the laboratory and then tested on a mutually exclusive subset of the same data. Results of these tests may significantly overestimate performance in the field. It could be that systems should be trained and/or tested on spontaneously-produced real user field data. The goal of this study was to evaluate the performance of a speaker independent isolated word telephone network speech recognition system when tested on laboratory vs. real user data under two training scenarios: 1. trained on laboratory and 2. trained on real user data. The results of this experiment suggest that real user speech databases are needed to achieve high accuracy speech recognition results in the field. In addition, it appears that a system trained on user data can be accurately tested with either real user or laboratory speech databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-267"
  },
  "danielsen90_icslp": {
   "authors": [
    [
     "Sven W.",
     "Danielsen"
    ]
   ],
   "title": "Standardisation of speech input assessment within the SAM esprit project",
   "original": "i90_1021",
   "page_count": 4,
   "order": 269,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "The paper presents the current status and the future directions of the Speech Input Assessment activities, which are planned within the European Community ESPRIT Project 2589 \"Multi-lingual Speech Input/Output Assessment, Methodology and Standardisation\" (SAM). The aim of the SAM Project is to provide standardised methodologies for assessment of speech input/output devices, since the availability of such agreed standards of performance is crucial to the effective development, sale and use of such systems. This paper is solely concentrated on the description of the hardware applied and the software packages developed for assessment and scoring of various classes of recognisers, and summarises the experiences gathered up to now. It concludes with a discussion on future directions for introducing reliable predictive assessment methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-268"
  },
  "irii90_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Irii"
    ],
    [
     "Kenzo",
     "Ito"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ]
   ],
   "title": "Multilingual speech data base for evaluating quality of digitized speech",
   "original": "i90_1025",
   "page_count": 4,
   "order": 270,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "This paper proposes a multilingual set of speech samples collected to standardize an artificial voice as a data base in order to evaluate the language and talker dependency of a digital coding algorithm. Speech recordings are made by telecommunication laboratories of telecommunication administration or operating companies participating in the International Telegraph and Telephone Consultative Committee (CCITT) under uniform conditions. The number of languages is 20. Each language is , composed of at least 16 short sentences and the duration of each sentence is about 8 seconds. To investigate the impartiality of the data base, the fundamental statistical speech characteristics of the speech samples are analyzed. It is confirmed that the results agree with those of previous researches taking into account the dispersions. The speech quality dependency on talker and language is investigated when this set of speech samples is coded and applied to three typical digital coding algorithms. This set of speech samples reduces the bias in the evaluation due to the limited speech samples. Speech samples are stored on CD-ROM and are publicly available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-269"
  },
  "wu90_icslp": {
   "authors": [
    [
     "Lizhong",
     "Wu"
    ],
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "The optimal gain sequence for fastest learning in connectionist vector quantiser design",
   "original": "i90_1029",
   "page_count": 4,
   "order": 271,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "Kohonen's self-organising algorithm has been widely used for the design of connectionist vector quantisers (CVQ). One of its features is that the weight update gain sequence ?/m) is a decreasing function of the number of iterations, and if incorrectly chosen can lead to very long training times. Here we derive the time-optimal gain sequence and demonstrate its efficacy for a number of cases. It is demonstrated that the new method is time optimal and that its performance tends to that of VQ with the LBG algorithm. Finally the CVQ for linear predictive data with respect to the Itakura distance measure is applied to a multipulse linear predictive speech coder using data from the TIMIT database. Comparisons are made of waveforms and rate distortion functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-270"
  },
  "robinson90_icslp": {
   "authors": [
    [
     "Tony",
     "Robinson"
    ],
    [
     "John",
     "Holdsworth"
    ],
    [
     "Roy",
     "Patterson"
    ],
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "A comparison of preprocessors for the cambridge recurrent error propagation network speech recognition system",
   "original": "i90_1033",
   "page_count": 4,
   "order": 272,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "This paper makes a comparison of several preprocessors for the task of speaker independent phoneme recognition from the TIMIT database using a recurrent error propagation network recogniser [l] The paper evaluates FFT, filterbank, auditory model and LPC based techniques in the spectral and cepstral domains and adds some simple features such as estimates of the degree of voicing, formant positions and amplitudes. The paper concludes that the features do not make a significant contribution and that the spectral domain representations, independent of their derivation, are better suited to this task. However, we find that the recogniser was relatively insensitive to preprocessor and changes in the architecture and training of the recogniser are more significant. The current, recognition rate on the TIMIT database of 61 symbols is 69.5% correct (64.0% including insertion errors) and on a reduced 39 symbol set the recognition rate is 76.1% correct (70.4%). This compares favourably with the results of other methods, such as Hidden Markov Models, on the same task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-271"
  },
  "allen90_icslp": {
   "authors": [
    [
     "R. B.",
     "Allen"
    ],
    [
     "C.",
     "Kamm"
    ],
    [
     "S. B.",
     "James"
    ]
   ],
   "title": "A recurrent neural network for word identification from phoneme sequences",
   "original": "i90_1037",
   "page_count": 4,
   "order": 273,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "A neural network architecture was designed for locating word boundaries and identifying words from phoneme sequences. This architecture was tested in three sets of studies. First, a highly redundant corpus with a restricted vocabulary was generated and the network was trained with a limited number of phonemic variations for the words in the corpus. Tests of network performance on a transfer set (i.e., sentences not used during training) yielded a very low error rate. In a second study, a network was trained to identify words from expert transcriptions of speech. On a transfer test, error rate for correct simultaneous identification of words and word boundaries was 31%. The third study used the output of a phoneme classifier as the input to the word and word boundary identification network. The error rate on a transfer test set was 49% for this task. Overall, these studies provide a first step at identifying words in connected discourse with a neural network. While the results are moderately encouraging, the identification of word boundaries was especially difficult when the input sequences contained many inserted and deleted phonemes. Although many issues, including scaling the model to larger corpora, are unresolved, many strategies for improving performance remain to be explored.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-272"
  },
  "depuydt90_icslp": {
   "authors": [
    [
     "Lieven",
     "Depuydt"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Luc Van",
     "Immerseel"
    ],
    [
     "Nico",
     "Weymaere"
    ]
   ],
   "title": "Improved broad phonetic classification and segmentation with a neural network and a new auditory model",
   "original": "i90_1041",
   "page_count": 4,
   "order": 274,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "We describe a broad phonetic classification and segmentation algorithm based on auditory modelling, neural networks and dynamic programming. As the basics are outlined in another paper [6], we here focuss on some new elements such as the introduction of auditory model features and durational constraints, and the use of different sets of broad phonetic classes. Furthermore, we have carried out experiments to test the robustness of our algorithm against new speakers and new vocabularies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-273"
  },
  "obara90_icslp": {
   "authors": [
    [
     "Kazuaki",
     "Obara"
    ],
    [
     "Hideyuki",
     "Takagi"
    ]
   ],
   "title": "Formant extraction model by neural networks and auditory model based on signal processing theory",
   "original": "i90_1045",
   "page_count": 4,
   "order": 275,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "This paper discribes a formant extraction model constructed by an auditory model for preprocessing and a neural networks (NN) that extracts formant. This auditory model consists of the first stage based on physiological foundings and the second stage based on a hypothetic function. The first stage is consisted of a basilar membrane model of 190ch critical band pass filter bank (CBF), hair cell model of half-wave (HW) rectification and satulating property, and synapse or axon model of low pass (LP) characteristics. The second stage has hypothetical function that has homomorphic processing. Finally, a NN for formant extraction and its experiment are described. This NN extract formant from tge output of auditory model. The resuls displayed in this paper shows that our formant extraction model performs well for not only supervised data but also unsupervised data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-274"
  },
  "kanedera90_icslp": {
   "authors": [
    [
     "Noboru",
     "Kanedera"
    ],
    [
     "Tetsuo",
     "Funada"
    ]
   ],
   "title": "/b,d,g/ recognition with elliptic discrimination neural units",
   "original": "i90_1049",
   "page_count": 4,
   "order": 276,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "Many researchers achieved high phoneme recognition rates by multi-layered neural networks with Linear Discrimination Neural units (LDN). However, it is difficult to analyze the functions of those LDN networks. In this paper, we propose a multi-layered neural network with Elliptic Discrimination Neural units (EDN) in order to interpret the functions of each unit in the network more easily. The center of the elliptic discrimination boundary of a neural unit corresponds to the typical point in a input space. The radii of the ellipse correspond to the extent of the input space, hence it becomes clear which components of the input space are important to each unit in the EDN network. For comparison between EDN and LDN, we carried out the recognition experiments of phonemes /b,d,g/ in 5240 tokens of a Japanese speech database. The back-propagation learning procedure was used to train each network. In the experiments, we obtained recognition rates of EDN network as high as that of LDN network. We also confirmed which components of the input are important to each unit in the EDN network. Thus we found some significant regions for recognition on the input spectrogram. These regions are localized both by time and in frequency, thus the network is robust for time variations of input vectors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-275"
  },
  "meng90_icslp": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A comparative study of acoustic representations of speech for vowel classification using multi-layer perceptrons",
   "original": "i90_1053",
   "page_count": 4,
   "order": 277,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "This paper describes a study comparing several signal representations for context-independent vowel classification. It forms the first step in our investigation for a distinctive-feature-based approach to phonetic recognition. Six different signal representations were investigated. They include the outputs of Seneff's Auditory Model (SAM), the mel-scale representations and the conventional Fourier Transform. To strive towards a fair and meaningful comparison, the mel-frequency niters were carefully designed to resemble the filters of SAM and the dimensionality of the feature vectors were constrained to be equal. The representations were compared on the basis of classifying 16 vowels in American English. Experiments with speech degraded by adding white noise were also conducted. Our results are based on over 22,000 vowel tokens excised from 2,750 sentences spoken by 550 speakers. The combined Synchronous and Mean Rate responses from SAM outperformed all the other representations with both undegraded and noisy speech, yielding top-choice accuracies of 66% and 54% respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-276"
  },
  "cho90_icslp": {
   "authors": [
    [
     "Yong Duk",
     "Cho"
    ],
    [
     "Ki Chul",
     "Kim"
    ],
    [
     "Hyun Soo",
     "Yoon"
    ],
    [
     "Seung Ryoul",
     "Maeng"
    ],
    [
     "Jung Wan",
     "Cho"
    ]
   ],
   "title": "Extended elman's recurrent neural network for syllable recognition",
   "original": "i90_1057",
   "page_count": 4,
   "order": 278,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "This paper describes an extended Elman's recurrent neural network adapted for speech recognition with input context buffers and analog target function. The input layer has context buffers to extract context sensitive features in the input. The analog target function in the output layer reflects the confidence level of the output for the current input in the context buffer. Speaker dependent recognition results for 10 syllables using cepstral coefficients show that the extended Elman's network is superior to the Elman's network as well as Multi-layer Perceptron. The recognition accuracy of the extended Elman's network is better than that of the cepstral distance measure and comparable to that of the weighted cepstral distance measure using dynamic time warping based template matching. Preliminary conclusion is that the input context buffers with time replicated scanning enhance the shift invariant capability of the recurrent neural network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-277"
  },
  "leung90_icslp": {
   "authors": [
    [
     "Hong C.",
     "Leung"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "Michael S.",
     "Phillips"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Detection and classification of phonemes using context-independent error back-propagation",
   "original": "i90_1061",
   "page_count": 4,
   "order": 279,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "Over the past few years, we have been investigating the problem of utilizing artificial neural networks for phonetic classification. In this paper, we will describe several extensions to our earlier work, utilizing a segment-based approach. We will formulate our segmental framework and report our study on the use of multi-layer perceptions for detection and classification of phonemes. Issues related to computational requirements and input representations will also be discussed. Our investigation is performed within a set of experiments that attempts to recognize 38 vowels and consonants in American English independent of speaker. When evaluated on the TIMIT database, our system achieves an accuracy of 56%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-278"
  },
  "chiba90_icslp": {
   "authors": [
    [
     "Shigeru",
     "Chiba"
    ],
    [
     "Kiyoshi",
     "Asai"
    ]
   ],
   "title": "A new method of consonant detection and classification using neural networks",
   "original": "i90_1065",
   "page_count": 4,
   "order": 280,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "We propose a new method of phoneme detection and classification using neural networks. Neural networks can automatically learn phoneme spotting and classification by this method if hand-labeled speech data are given. In this method, first, the candidate time positions of consonant in continuous speech were extracted using an acoustic cues which indicated the possibility of the consonant location. Then, the phoneme label corresponding to the running spectra around the candidate point was determined based on hand-labeled results for speech wave. These candidate points were divided into several groups depending on acoustic cues in order to reduce the charge of neural network learning. The neural networks were trained by the backpropagation learning procedure as those were able to label continuous speech similar to hand-labeled results when the running spectra around the consonant candidate points were given. The speaker independent consonant recognition experiment showed the recognition rate of about 80 % for the test data set. This result proved the effectiveness of this method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-279"
  },
  "kitazawa90_icslp": {
   "authors": [
    [
     "Shigeyoshi",
     "Kitazawa"
    ],
    [
     "Masahiro",
     "Serizawa"
    ]
   ],
   "title": "An artificial neural network for the burst point detection",
   "original": "i90_1069",
   "page_count": 4,
   "order": 281,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "We trained a three-layered neural network to discriminate between the left and right contexts around the burst point. The shifting window is the portion of the input speech which will serve as input to the network at a time. The detection window is the portion of the input speech where the searched burst locates. The outputs from our network distinguish three states, i. e. before or after the burst and outside the burst of the shifting window. The acoustic properties used was speech power time series of a 5-band me 1-sea 1ed LPC spectrum. The network consisted of 80 input units 30 hidden units and 3 output units. French voiced stop consonants /b,d,g/ served as input. Elimination of phantom burst point was attained. The decisions error was around 15~ms. Usually right decisions consistently proceeded the real burst point and then followed by left decisions. High frequency component was effective among 5 frequency bands.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-280"
  },
  "lefebvre90_icslp": {
   "authors": [
    [
     "Claude",
     "Lefebvre"
    ],
    [
     "Dariusz A.",
     "Zwierzynski"
    ]
   ],
   "title": "The use of discriminant neural networks in the integration of acoustic cues for voicing into a continuous-word recognition system",
   "original": "i90_1073",
   "page_count": 4,
   "order": 282,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "The performance of a small vocabulary speaker-dependent robust speech recogniser can be improved by adding more input features in the front-end. Our present speech recognition system employs both static & dynamic spectral representations which are combined with a linear discriminant analysis. We have done recognition experiments with CVC words, differing in their initial consonant phonemes only, e.g. peep vs beep and found that most of the errors are due to the system not distinguishing between voiceless/voiced stop consonants. There are a number of acoustic cues useful to improve distinction between voiceless/voiced plosives, specifically, the fundamental frequency at voicing onset and the Voice Onset Time (VOT). This paper reports on recognition experiments where both of these features are extracted from the speech signal and are combined with the other features using the linear discriminant network. The results from the experiments confirmed that the addition of these two input features improved the performance of the recogniser for confusable word-pairs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-281"
  },
  "yamaguchi90c_icslp": {
   "authors": [
    [
     "Kouichi",
     "Yamaguchi"
    ],
    [
     "Kenji",
     "Sakamoto"
    ],
    [
     "Toshio",
     "Akabane"
    ],
    [
     "Yoshiji",
     "Fujimoto"
    ]
   ],
   "title": "A neural network for speaker-independent isolated word recognition",
   "original": "i90_1077",
   "page_count": 4,
   "order": 283,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "This paper presents a new, speaker-independent word recognition system based on three kinds of multilayer neural networks hierarchically arranged. The bottom neural networks act as identifiers of acoustic, events and align time distortion. The middle neural networks output similarity measures for the input words. The top neural network is a classifier and outputs the recognition candidates. Speaker-independent recognition experiments using 28 isolated Japanese words were carried out using data uttered by 150 speakers (100 speakers for training and 50 speakers for testing). As a result, we obtained a 97.1% recognition accuracy and a 1.0% error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-282"
  },
  "itahashi90_icslp": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Recent speech database projects in Japan",
   "original": "i90_1081",
   "page_count": 4,
   "order": 284,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "This report first discusses storage media for speech databases and performance indexes for discrete and connected word recognition. Secondly, the report describes recent speech database projects in Japan in which the author has been involved. The JEIDA Japanese Common Speech Data Corpus was first reported on in 1986. It has been converted to DAT recently. The JEIDA Noise Database has been released to the public recently. It contains various kinds of environmental noise and standard noise for sound level calibration. The 'Spoken Language' project collected speech data including continuous speech spoken by 10 males and 10 females. The 'Spoken Japanese' project, started in 1989, attempts to collect various dialectal speech from all over Japan and create speech databases. A compact disc containing a fairy tale and weather forecast spoken by 20 dialect speakers has been produced.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-283"
  },
  "choi90_icslp": {
   "authors": [
    [
     "Joon-Hyuk",
     "Choi"
    ],
    [
     "Kyung-Tae",
     "Kim"
    ]
   ],
   "title": "Construction of a large Korean speech database and its management system in ETRI",
   "original": "i90_1085",
   "page_count": 4,
   "order": 285,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "A large size Korean speech database under construction at ETRI is introduced. We have three kinds of speech databases. They are 35 connected-4-digits,144 CV monosyllables which are segmented into phonemes(C+V)combination, and 445 phoneme-balanced words. The first two are collected from 10 male speakers and the last, from 4 male and 4 female speakers. For the easy application of the database in speech research, we proposed a 2-level acoustic-phonetic transcription to express almost all possible phonetic environments in Korean. And the transcriptions were carried out manually. For the effective management of these databases, a relational database management system, which helps fast access and easy manipulation, was used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-284"
  },
  "sagisaka90_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "M.",
     "Abel"
    ],
    [
     "Shigeru",
     "Katagiri"
    ],
    [
     "T.",
     "Umeda"
    ],
    [
     "H.",
     "Kuwabara"
    ]
   ],
   "title": "A large-scale Japanese speech database",
   "original": "i90_1089",
   "page_count": 4,
   "order": 286,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "This paper describes a large-scale Japanese speech database (JSDB) which is now under construction for the studies of speech recognition and synthesis. The database consists of (a) an isolated word JSDB, (b) an isolated sentence JSDB, (c) a mixed word and sentence JSDB and (d) a text JSDB. For multi-purpose use, five different transcriptions were made for almost all databases: phonetic symbols, acoustic events, allophonic variants, inseparable portions, and vowel centers. Moreover, in some of these databases, the grammatical information such as a part of speech, inflectional categories and phrase structure is given to the constituents in the corresponding sentences. These databases have been used for various research purposes in speech technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-285"
  },
  "ehara90_icslp": {
   "authors": [
    [
     "Terumasa",
     "Ehara"
    ],
    [
     "Kentaro",
     "Ogura"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "ATR dialogue database",
   "original": "i90_1093",
   "page_count": 4,
   "order": 287,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "Abstract We are constructing\" a dialogue database called the ATR Dialogue Database (ADD) as the basic data to study an automatic interpreting telephony system. ADD is a large structured database of dialogues collected from simulated telephone or keyboard conversations which are spontaneously spoken or typed in Japanese or English. The corpus collected in one language is manually translated/interpreted to the other language and the correspondences of these two corpora are made by several linguistic units. We compared telephone dialogues and keyboard dialogues in ADD. From the examination of the experiment results, we can conclude that, except for the following items, linguistic phenomena in telephone dialogues are almost the same of those in keyboard dialogues. The phenomena peculiar to telephone dialogues are the existence of interjections, restatements, fragmental sentences, long complex sentences, redundant expressions and indirect expressions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-286"
  },
  "gauvain90_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Design considerations and text selection for BREF, a large French read-speech corpus",
   "original": "i90_1097",
   "page_count": 4,
   "order": 288,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "BREF, a large read-speech corpus in French has been designed with several aims: to provide enough speech data to develop dictation machines, to provide data for evaluation of continuous speech recognition systems (both speaker-dependent and speaker-independent), and to provide a corpus of continuous speech to study phonological variations. This paper presents some of the design considerations of BREF, focusing on the text analysis and the selection of text materials. The texts to be read were selected from 5 million words of the French newspaper, Le Monde. In total, 11,000 texts were selected, with an emphasis on maximizing the number of distinct triphones. Separate text materials were selected for training and test corpora. The goal is to obtain about 10,000 words (approximately 60-70 min.) of speech from each of 100 speakers, from different French dialects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-287"
  },
  "tanaka90_icslp": {
   "authors": [
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Kozo",
     "Ohta"
    ]
   ],
   "title": "The ETL speech database for speech analysis and recognition research",
   "original": "i90_1101",
   "page_count": 4,
   "order": 289,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "This paper describes a speech database developed for speech analysis and recognition research at ETL. Its system design concept, system configuration and phonetic labeling are presented as well as database statistics. The features of the system are 1) utterance text generation from a statistic-phonological view, and 2) fine acoustic-phonetic labeling by semiautomatic techniques. An acoustically compact segment called APSeg is adopted for the labeling of speech samples and acoustic-phonetic variations are represented by directed networks. Some sorted results of the labeling are shown for a real speech database of a phonemically balanced word set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-288"
  },
  "soclof90_icslp": {
   "authors": [
    [
     "Michal",
     "Soclof"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Collection and analysis of spontaneous and read corpora for spoken language system development",
   "original": "i90_1105",
   "page_count": 4,
   "order": 290,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "As part of our effort in developing the MIT voyager system, we recently collected speech data from 50 male and 50 female subjects under a simulation mode, in which spoken sentences were typed into the computer for automatic natural language processing and response generation. Since a computer log of the spoken dialogue was maintained, we were able to ask the subjects to provide read versions of the sentences as well. Thus the corpus includes both a read version arid a spontaneous version of (approximately) the same sentence, modulo false starts and filled pauses in the spontaneous version. These corpora enable us to make a direct comparison between spontaneous and read speech. All in all, we were able to collect nearly 5000 spontaneous sentences, with an equal number of read sentences. All sentences were digitized and orthographically transcribed. In addition, time-aligned phonetic transcriptions were obtained for about 35% of the data. This paper documents the data collection process, and provides some linguistic and acoustic analyses of the collected data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-289"
  },
  "makino90b_icslp": {
   "authors": [
    [
     "Shozo",
     "Makino"
    ],
    [
     "Toshihiko",
     "Shirokaze"
    ],
    [
     "Ken'iti",
     "Kido"
    ]
   ],
   "title": "A distributed speech database with an automatic acquisition system of speech information",
   "original": "i90_1109",
   "page_count": 4,
   "order": 291,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "This paper describes a distributed speech database with an automatic acquisition system of speech information. The first feature is a distributed speech database composed of various types of storages(optical disk, magnetic disk, CD-ROM) and communication networks(Ethernet, GP-IB). This database contains 12 GB speech waveform data. The second feature is a virtual database. An unified dictionary administrates allocation of the original speech waveform data and the secondary data on various kinds of storages. It is not necessary for users to know the allocation of the data. The third feature is the automatic acquisition system of speech information based on a given phoneme string for an input speech. The fourth feature is a user-friendly database editor. The database is built up based on relational tables. The database editor has functions of data insertion, data deletion, and data display. Furthermore, frequently used speech analysis methods, a speech synthesizer and a speech recognizer are implemented to the database. Using those functions it is very easy for an user to carry out speech research without any knowledge concerning to the database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-290"
  },
  "millar90_icslp": {
   "authors": [
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "P.",
     "Dermody"
    ],
    [
     "M.",
     "Harrington"
    ],
    [
     "Julie",
     "Vonwiller"
    ]
   ],
   "title": "A national database of spoken language: concept, design, and implementation",
   "original": "i90_1281",
   "page_count": 4,
   "order": 292,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "A model is proposed for the building of a national resource of spoken language data in the form of a cluster of compatible databases. Each component of the cluster will have its own linguistic characteristics dependent on the primary purpose behind its collection. However each component corpus will have the same structure and the same standards of data description. The emphasis is on adequate description of the data rather than on conformity to a standard of recording conditions, data storage, or linguistic content. This paper outlines the rationale for such a database and proposes principles for the structuring of data storage, and for the description of important dimensions of such spoken language data. Some attention is also given to the management of such a data base within the speech and language technology community.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-291"
  },
  "castagneri90_icslp": {
   "authors": [
    [
     "Giuseppe",
     "Castagneri"
    ],
    [
     "Kyriaki",
     "Vagges"
    ]
   ],
   "title": "The Italian national database for speech recognition",
   "original": "i90_1285",
   "page_count": 3,
   "order": 293,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "The object of this project is to begin the collection of a large corpus of Italian speech. This activity will provide infrastructure support to Italian speech and language technology. A series of corpora supporting both general research in these areas and/or specific evaluation tasks will be collected. The first corpus is oriented toward CVCV utterances.\n",
    "The data were collected and organised in conformity with the standards currently being formulated by the Esprit Project 2589 \"SAM\".\n",
    "The corpora will be distributed on CD-ROM by the Italian Superior Institute of Telecommunications and will be widely available to the whole Italian speech and language technology community.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-292"
  },
  "pols90_icslp": {
   "authors": [
    [
     "Louis C.W.",
     "Pols"
    ]
   ],
   "title": "How useful are speech databases for rule synthesis development and assessment?",
   "original": "i90_1289",
   "page_count": 4,
   "order": 294,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "In Automatic Speech Recognition it is rather straightforward that large and diverse spoken language databases are required to train and to test speech input systems. For unlimited-text-to-speech rule-synthesis systems this is less apparent. These systems speak with one or few voices only, cannot just imitate natural speech, apply rules that are a mixture of database 'facts', designer's intuitions, and compensations for system insufficiencies, whereas subjective evaluation requires different texts all the time. Still, carefully designed, relatively small, annotated, spoken language databases can be an indispensable source of information and can be of some value as a standard for comparison. Such databases should be appropriately 'documented' in terms of orthographic input, grammatical categories, prosodic labeling, detailed acoustic-phonetic labeling, and voice characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-293"
  },
  "hardcastle90_icslp": {
   "authors": [
    [
     "William J.",
     "Hardcastle"
    ],
    [
     "Alain",
     "Marchal"
    ]
   ],
   "title": "Eur-accor: a multi-lingual articulatory and acoustic database",
   "original": "i90_1293",
   "page_count": 4,
   "order": 295,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "A large number of digitized databases of speech items have been compiled recently for various purposes: for training material; benchmark evaluation of speech products; diagnostic evaluation of speech recognizers etc. (e.g. ESPRIT/SAM EUROM.O [1];DARPA/TIMIT [2]). EUR-ACCOR is unique in that it consists of both articulatory and acoustic data for seven different European languages - English, French, German, Italian, Catalan, Irish Gaelic and Swedish. This database is currently being compiled as part of the ACCOR project (\"Articulatory/acoustic correlations in coarticulation processes: a cross-language investigation\") within the Basic Research Action framework of ESPRIT II (Action 3279). We will describe in this paper the main features of this multi-lingual articulatory and acoustic database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-294"
  },
  "juang90_icslp": {
   "authors": [
    [
     "B. H.",
     "Juang"
    ]
   ],
   "title": "Recent developments in speech recognition under adverse conditions",
   "original": "i90_1113",
   "page_count": 4,
   "order": 296,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "In this paper, we review several promising methods that were proposed in the past few years to deal with the problem of speech recognition in adverse conditions. We discuss these methods in six categories: signal enhancement preprocessing, special transducer arrangements, noise masking, stress compensation, robust distortion measures, and novel speech representations. We explain each categorical approach and provide a digest of the performance improvements each method is able to achieve. This type of information is helpful in making a technical decision for the actual recognizer design to be deployed in adverse environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-295"
  },
  "hanson90_icslp": {
   "authors": [
    [
     "Brian A.",
     "Hanson"
    ],
    [
     "Ted H.",
     "Applebaum"
    ]
   ],
   "title": "Features for noise-robust speaker-independent word recognition",
   "original": "i90_1117",
   "page_count": 4,
   "order": 297,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "Effects such as additive noise and noise-induced changes in vocal effort (Lombard effect) can cause significant loss of performance for recognizers trained on normal (non-noisy, non-Lombard) speech. In earlier work, improvements to recognition rate over a \"standard\" speech representation consisting of cepstral coefficients and their first time-derivative (calculated over a 50 msec interval) were demonstrated on the English digits vocabulary by lengthening the interval over which the first derivative is calculated and incorporating a second derivative feature. The current paper extends this work by considering recognition of a much more confusable vocabulary. The recognition results are analyzed for each proposed change in the speech representation, examined by confusable subsets of the vocabulary and contrasted with previous results. Most of the earlier findings for the digits vocabulary were confirmed for the confusable vocabulary. Additionally, it was found that adding a third derivative feature further enhances performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-296"
  },
  "acero90_icslp": {
   "authors": [
    [
     "Alejandro",
     "Acero"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Acoustical pre-processing for robust spoken language systems",
   "original": "i90_1121",
   "page_count": 4,
   "order": 298,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "In this paper we discuss several issues that concern the development of spoken language systems that are robust to changes in the acoustical environment. We describe the benefit of joint compensation for differences in noise level and spectral tilt between close-talking and desk-top microphones, as opposed to independent compensation. For Sphinx, the CMU continuous-speech speaker-independent recognition system, cepstral processing offers the advantages of easier integration, greater computationally efficiency and greater accuracy compared to processing in the spectral domain. We also present algorithms that adapt to new environments by estimating noise level and spectral tilt directly from the input speech, without the need for environment-specific training data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-297"
  },
  "hansen90_icslp": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Oscar N.",
     "Bria"
    ]
   ],
   "title": "Lombard effect compensation for robust automatic speech recognition in noise",
   "original": "i90_1125",
   "page_count": 4,
   "order": 299,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "This paper addresses the problem of automatic speech recognition under Lombard and noise conditions. The main contributions include the statistical analysis of vocal tract and speech parameters under Lombard effect, and the formulation of a new speech recognition system which employs adaptive noise suppression and Lombard effect compensation front-end processors. The effects on formant location, bandwidth, and mel-cepstral parameters from noise and Lombard effect are presented. These parameters vary greatly, with significant variations across all phonemes for spectral tilt. Approximately half of all mel-cepstral parameters result in statistically significant variation from neutral. The significance of parameter variation between noisefree and noisy Lombard conditions shifts, suggesting the need for an alternate compensation for noise-free and noisy Lombard speech. A new recognition algorithm employing noise adaptive boundary detection, noise suppression, and voiced/unvoiced Lombard compensation is presented. Observed shift in mean cepstral values from neutral can be modeled using an exponential tilt, as suggested by Chen [3], but that the exponential form appears to differ for each phoneme class. A new Lombard effect compensator is formulated which allows varying degrees of compensation to be placed on voiced/unvoiced speech sections. Preliminary recognition results suggest that separate compensation of voiced and unvoiced speech sections improves recognition performance by as much as 10% over no compensation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-298"
  },
  "kitamura90_icslp": {
   "authors": [
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Etsuro",
     "Hayahara"
    ],
    [
     "Yasuhiko",
     "Simazciki"
    ]
   ],
   "title": "Speaker-independent word recogniton in noisy environments using dynamic and averaged spectral features based on a two-dimensional mel-cepstrum",
   "original": "i90_1129",
   "page_count": 4,
   "order": 300,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "This paper describes a speaker-independent word recognition method in noisy environments using dynamic and averaged spectral features based on a two-dimensional mel-cepstrum (TDMC). A TDMC is defined as the two-dimensional Fourier transform of mel-frequency scaled logarithm spectra in the frequency and time domains, and it consists of averaged and dynamic features of the two-dimensional mel-log spectrum in the analyzed interval. This method uses distance measures based on averaged and dynamic spectral features of the TDMC of the analyzed word. Furthermore, one of noise-added reference pattern sets is used to improve this method. Speaker-independent word recognition experiments, in white noise and colored noise, for 10 Japanese digits uttered by ten male speakers show the effectiveness of this method. By using a reference pattern set of 20 dB this method gives the recognition error rate lower than 5% and 2% for white-noise-added speech and colored-noise-added speech of 20 and 10 dB SNR, respectively. This method gives better recognition rates than a standard one using a one-dimensional mel-cepstrum representing instantaneous spectral features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-299"
  },
  "noll90_icslp": {
   "authors": [
    [
     "A.",
     "Noll"
    ]
   ],
   "title": "Problems of speech recognition in mobile environments",
   "original": "i90_1133",
   "page_count": 4,
   "order": 301,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "For speech recognition in mobile environments a number of severe problems have to be solved. This paper gives an overview of several approaches dealing with the basic problem of noise handling in speech recognition in relation to the characteristica of noise in the mobile environment. Using a qualitative description of the noise characteristica, it can be seen that most of the currently investigated noise-handling strategies are just capable of handling parts of the phenomena of \"mobile noise\". From the analysis of these algorithms a general architecture of a noise resistent speech recognition strategy for further investigation is derived.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-300"
  },
  "fissore90_icslp": {
   "authors": [
    [
     "L.",
     "Fissore"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "M.",
     "Codogno"
    ],
    [
     "G.",
     "Venuti"
    ]
   ],
   "title": "HMM modeling for voice-activated mobile-radio system",
   "original": "i90_1137",
   "page_count": 4,
   "order": 302,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "This paper deals with the development of an isolated-word speech recognition system operating in the car environment. The problems related to the limited size of training data and to the different operating conditions between training and testing are addressed, and the performance of the system reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-301"
  },
  "nakadai90_icslp": {
   "authors": [
    [
     "Yoshio",
     "Nakadai"
    ],
    [
     "Noboru",
     "Sugamura"
    ]
   ],
   "title": "A speech recognition method for noise environments using dual inputs",
   "original": "i90_1141",
   "page_count": 4,
   "order": 303,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "This paper describes an improved speaker-dependent isolated word recognition algorithm that overcomes the effect of variable noise environments. The noisy speech is received by dual inputs; primary and reference. Speech endpoints are detected by the power ratio of these two inputs. The noise power spectrum that overlaps the primary speech input is offset by the reference input noise spectrum where the spectrum is restricted to values lower than a mean noise spectrum. The Staggered Array DP algorithm is used for pattern matching and the inverse-variance weighted distance measure of LPC cepstrum and its' regression coefficients are applied as a spectral distance measure between the input pattern and the reference templates. These algorithms are applied to noisy speech uttered in an actual passenger car. Tests confirm the superiority of this approach over the conventional algorithms that use single speech input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-302"
  },
  "morii90_icslp": {
   "authors": [
    [
     "Shuji",
     "Morii"
    ],
    [
     "Toshiyuki",
     "Morii"
    ],
    [
     "Masakatsu",
     "Hoshimi"
    ],
    [
     "Shoji",
     "Hiraoka"
    ],
    [
     "Taisuke",
     "Watanabe"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ]
   ],
   "title": "Noise robustness in speaker independent speech recognition",
   "original": "i90_1145",
   "page_count": 4,
   "order": 304,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "This paper describes methods for improving the accuracy of our speaker independent speech recognition system in a noisy environment. Phoneme templates were selected from several varieties of phoneme templates, which were generated in typical noise environments, to simulate actual background noise. If none of these templates suits the noise environment, a template adaptation is performed by adding the estimated spectrum of the background noise to the mean vector of each phoneme template. A speech period detector and segmentation algorithm were also improved with respect to changes in environment noise. These approaches were applied to a hardware system and evaluated. Average phoneme recognition score is 71.6% by use of the template adaptation method. Input speech data was achieved by adding a noise to clean speech. Signal-to-noise ratio is 20dB. In a actual noisy environment(70-85dBA), an average word recognition score is 97%, using a twenty four word vocabulary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-303"
  },
  "gyoutoku90_icslp": {
   "authors": [
    [
     "Kaoru",
     "Gyoutoku"
    ],
    [
     "Hidefumi",
     "Kobatake"
    ]
   ],
   "title": "Maximum likelihood estimation of speech waveform under nonstationary noise environments",
   "original": "i90_1149",
   "page_count": 4,
   "order": 305,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper describes a study on noisy speech processing. The background noise is assumed to be a sum of a stationary noise and isolated nonstationary noises, and it is also assumed that there are no overlapping between speech segments and nonstationary noises. The noise processing system described in this paper consists of three subsystems. The first subsystem is to detect nonstationary segments buried in the stationary noise. The second one is for the speech/nonspeech discrimination of the detected nonstationary segments. If they are identified as speech, the third subsystem begins to work for speech enhancement. This paper gives a detailed discussion on the third subsystem, which is based on the maximum likelihood estimation method. Experimental results show that the improvement in signal-to-noise ratio by the proposed method is more than 13dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-304"
  },
  "hardcastle90b_icslp": {
   "authors": [
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "Electropalatography in phonetic research and in speech training",
   "original": "i90_1153",
   "page_count": 4,
   "order": 306,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "Electropalatography (EPG) is an instrumental technique which displays and records spatio-temporal details of tongue contacts with the hard palate during continuous speech. It has been useful in experimental phonetic research, particularly in identifying lingual co-articulatory effects in different languages, and in describing the precise articulatory characteristics of complex lingual fricative sounds such as /s/. In speech pathology also, the technique has had important clinical applications. For example, as a remedial tool, it can provide an immediate, real-time display of palatal contacts which can be used to modify abnormal articulatory gestures associated with certain types of speech disorders. A similar application yet to be fully explored is in pronunciation teaching. This paper describes a pilot experiment in which two Japanese learners of English are taught to produce an acceptable /r/,/l/ contrast with the aid of EPG.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-305"
  },
  "rost90_icslp": {
   "authors": [
    [
     "Michael",
     "Rost"
    ]
   ],
   "title": "Teaching spoken language: a genre-based approach",
   "original": "i90_1157",
   "page_count": 4,
   "order": 307,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "This paper proposes a genre-based approach for the teaching of spoken language. This approach is based on an integration of social and cognitive dimensions in language learning. The specific approach is focused on identification of discourse genres, expert modeling of speech production, and development of language learner awareness of task stress and multiple levels of discourse processing. Narratives are used as an example of a genre which lends itself to the approach outlined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-306"
  },
  "yoshida90_icslp": {
   "authors": [
    [
     "Kazue",
     "Yoshida"
    ]
   ],
   "title": "Interaction between native and nonnative speakers in team teaching",
   "original": "i90_1161",
   "page_count": 4,
   "order": 308,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "This study examines the interaction of native speakers and nonnative speakers. Six hypotheses are tested. Three of them deal with the frequencies of temporal marking, topic-continuing and topic-initiating move-es, and uninverted, wh, yes/no and tag questions. The other three hypotheses are concerned with the frequencies of nine categories of negotiation. All hypotheses were supported except two categories of one hypothesis. To test these hypotheses,eight native speakers' speech that the data were from two separate periods of collection was video-recorded, transcribed and analyzed. Analysis was done on teacher-student interaction, in team teaching in., junior high schools and senior high schools and on. English classes in a university. The study showed the same results as in studies of second language learners' FTD, but some differences were recognized in the details between EFL(team teaching) and ESL.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-307"
  },
  "nikolarea90_icslp": {
   "authors": [
    [
     "Ekaterini",
     "Nikolarea"
    ]
   ],
   "title": "Contrastive phonetics of English, French and modern Greek in language teaching and interpreting",
   "original": "i90_1165",
   "page_count": 3,
   "order": 309,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "Based on the assumption that language is auditorily based and phonemes are auditorily perceived elements, this paper proposes a microlinguistic contrastive analysis of the vowel and consonant systems in English, French and Modern Greek. Consequently, the point of the discussion will be the three types of physical reality of these languages (articulatory, acoustic, auditory) and the functional difference in each language. This paper will also examine the implicational value of the notions of 'transfer' and 'interference' as a source for further experimental studies into the predictability of the English, French and Greek learner's difficulties and their bidirectional pedagogical potentiality: language teaching and interpreting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-308"
  },
  "nagano90_icslp": {
   "authors": [
    [
     "Keiko",
     "Nagano"
    ],
    [
     "Kazunori",
     "Ozawa"
    ]
   ],
   "title": "English speech training using voice conversion",
   "original": "i90_1169",
   "page_count": 4,
   "order": 310,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "This paper proposes an English prosody training method using voice conversion technique. The unique point of this proposed method is that voice converted synthetic speech is used to train English prosody pronunciation. The synthetic speech is produced by converting important prosodic parameters in the student's speech into corresponding native English speaker's speech, while the student's voice characteristics except prosody are preserved. By using the proposed method, pronunciation problem can be easily found out, and training efficiency is improved. The comparative evaluations for training efficiency of the proposed training method and the conventional method show that the proposed method, using the voice converted synthetic speech, is more effective in English prosody training than the conventional method, using the native English speaker's original speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-309"
  },
  "saeki90_icslp": {
   "authors": [
    [
     "Namie",
     "Saeki"
    ]
   ],
   "title": "Contrastive analysis of american English and Japanese pronunciation",
   "original": "i90_1173",
   "page_count": 3,
   "order": 311,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "In the present paper, I will discuss the difficult areas for native speakers of Japanese in learning the pronunciation of English consonants. The difficulties that Japanese speakers encounter when they learn English pronunciation involve the pronunciation of individual phonological segments, which is the segmental component, and word stress, sentence stress, and rhythm, which are considered as suprasegmental components. This paper is limited to the segmental component, especially to English consonants and syllable structure. The subject is examined, first, on the basis of a contrastive analysis of consonants of American English and Japanese, and second, on the basis of how the English segments are perceived and adapted by Japanese speakers according to properties of the system of Japanese phonology, in order to obtain admissible representations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-310"
  },
  "rahimpour90_icslp": {
   "authors": [
    [
     "Massoud",
     "Rahimpour"
    ]
   ],
   "title": "Oral communicative approaches in spoken language processing",
   "original": "i90_1177",
   "page_count": 4,
   "order": 312,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "The purpose of this paper is to consider the problem of English learners, who, after several years of study are unable to use language properly, especially in spoken language. Speech mastery is of great importance and a strong shift of focus of attention is felt necessary. In this paper, special emphasis will be laid on the oral communicative approaches and some recommendations would be offered for the betterment of spoken language education.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-311"
  },
  "murakawa90_icslp": {
   "authors": [
    [
     "Hisako",
     "Murakawa"
    ]
   ],
   "title": "Teaching English pronunciation to Japanese university students: the voiceless fricative /s/ sound",
   "original": "i90_1181",
   "page_count": 4,
   "order": 313,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "It is generally believed that an adult has more difficulty mastering the pronunciation of a second language than does a child. Various studies, from psychological and methodological viewpoints, have found evidence that the child acquires the phonetics of speech much faster than does the adult. Various writers have discussed techniques and methods of teaching classroom. However, in Japan, very few researchers have attempted to verify scientifically whether or not the application of phonetic information in teaching English pronunciation will enhance adult performance. The purpose of this study is to determine whether or not the phonetic method, used with laser discs which show the articulation of English pronunciation, produces any significant differences from using the audio-lingual method only. The target English voiceless fricative I si sound is chosen. Althought the /s/ sound exists in the Japanese language, the English fricative /s/ requires a much stronger breath stream than the Japanese /s/. It is, therefore, one of the most difficult English sounds for Japanese to produce. Learning the correct way of breathing and articulation will facilitate the production of this significant /s/ sound. Subjects for the experimental group were from the junior class of International Budo University majoring in budo (martial ways). The control group are juniors from IBU majoring in physical education. Both groups are in the highest class of English proficiency of the four instructional levels. An identical measurement test is given before and after the training session. A sound analyzer, one of the devices of PROTS(Pronunciation Training System) is used. All spoken data from both Japanese groups are compared with those of native speakers of American English. The results will be observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-312"
  },
  "bernstein90_icslp": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Michael",
     "Cohen"
    ],
    [
     "Hy",
     "Murveit"
    ],
    [
     "Dimitry",
     "Rtischev"
    ],
    [
     "Mitchel",
     "Weintraub"
    ]
   ],
   "title": "Automatic evaluation and training in English pronunciation",
   "original": "i90_1185",
   "page_count": 4,
   "order": 314,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "SRI is developing a system that uses real time speech recognition to diagnose, evaluate and provide training in spoken English. The paper first describes the methods and results of a study of the feasibility of automatically grading the performance of Japanese students when reading English aloud. Utterances recorded from Japanese speakers were independently rated by expert listeners. Speech grading software was developed from a speaker independent hidden-Markov-model speech recognition system. The automatic grading procedure first aligned the speech with a model and then compared the segments of the speech signal with models of those segments that have been developed from a database of speech from native speakers of English. The evaluation study showed that ratings of speech quality by experts are very reliable and automatic grades correlate well (r > 0.8) with those expert ratings. SRI is now extending this technology and integrating it in a spoken-language training system. This effort involves (1) porting SRI's DECIPHER speech recognition system to a microcomputer platform, and (2) extending the speech-evaluation software to more exactly diagnose a learner's pronunciation deficits and lead the learner through an appropriate regimen of exercises.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-313"
  },
  "abe90b_icslp": {
   "authors": [
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Vocabulary independent phrase recognition with a linear phonetic context model",
   "original": "i90_1189",
   "page_count": 4,
   "order": 315,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "This paper describes a continuous speech recognition system using phonemes as basic units in both of acoustic and linguistic processings. Acoustically the phoneme is difficult to be identified since it is strongly varied by the phonemic contexts. In order to cope with the contextual variability of the phonemes, we use a linear model called Linear Phonetic-Context Model(LPCM), which represents acoustical features as the sum of context-independent and context-dependent components. Incorporating with the LPCM we design a phoneme-based phrase recognition algorithm which accepts speech input of an arbitrary string of phonemes. The algorithm obtains plural recognition candidates using an end-point spotting method along with a beam-search technique. The language model bases on task-independent statistics of phoneme strings, and gives a probability of a phrase not existing in the corpus avoiding the null probability problem as in the simple N-gram model. In experiments to recognize 336 phrases extracted from 50 sentences spoken by a male speaker, we obtained a phoneme recognition rate of 95.0% and a phrase recognition rate of 67.9% without limiting a vocabulary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-314"
  },
  "ariki90b_icslp": {
   "authors": [
    [
     "Y.",
     "Ariki"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Phoneme probability presentation of continuous speech",
   "original": "i90_1193",
   "page_count": 4,
   "order": 316,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "This paper describes a new presentation of continuous speech in terms of the probability of all phoneme types as a function of time. The presentation is called a phoneme probability presentation (PPP) and can be used for phoneme segmentation or phoneme lattice production. As a technique to produce the PPP, we have employed hidden Markov models (HMM) with time duration information. This information is essential to produce the PPP and is effective in English phoneme recognition. With this information the HMMs of all the phoneme types can compute their probability in parallel and in time synchronism. The PPP can serve as phoneme filters which can produce phoneme probability from continuous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-315"
  },
  "ye90_icslp": {
   "authors": [
    [
     "Haiyan",
     "Ye"
    ],
    [
     "Jean",
     "Caelen"
    ]
   ],
   "title": "Duration constraints for the speech input interface in the MULTIWORKS project",
   "original": "i90_1197",
   "page_count": 4,
   "order": 317,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "We present here an early version of MULTIWORKS Speech Input Interface without syntax rules. We have shown that the duration constraints is very useful even whole-word Hidden Markov Model (HMM) is used in the case of connected word recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-316"
  },
  "zhiping90_icslp": {
   "authors": [
    [
     "Hu",
     "Zhi-ping"
    ],
    [
     "Imai",
     "Satoshi"
    ]
   ],
   "title": "Chinese continuous speech recognition system using the state transition models both of phonemes and words",
   "original": "i90_1201",
   "page_count": 4,
   "order": 318,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "This paper describes a Chinese continuous speech recognition system in which we use a new statistical model, i.e. the State Transition Model(STM) to express phoneme and word in Chinese. The acoustic-phonetic process of this system can be divided into two levels. The first level is phoneme level process which is carried out before linguistic process and provides the fundamental information of the linguistic process. The second level is word level process which is carried out after the linguistic process and provides the information of coarticulation of the consonant and the vowel in a word. For linguistic process, 12 kinds of Chinese sentence patterns most in use, have been registered in the form of the context-free grammars. Semantic analysis is carried out at the same time with the syntactic analysis. Experiments of phoneme, word and sentence recognition have been done to evaluate the performance of this system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-317"
  },
  "goldstein90_icslp": {
   "authors": [
    [
     "Jade",
     "Goldstein"
    ],
    [
     "Akio",
     "Amano"
    ],
    [
     "Hideki",
     "Murayama"
    ],
    [
     "Mariko",
     "Izawa"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "A new training method for multi-phone speech units for use in a hidden Markov model speech recognition system",
   "original": "i90_1205",
   "page_count": 4,
   "order": 319,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "This paper describes preliminary results for a new method of training multi-phone units for discrete hidden Markov model speech recognition systems. The context sensitive, potentially poorly trained multi-phone units are combined with smaller speech units by a weighting scheme favoring well-trained data. We tested this method for the Japanese language, using the multi-phone disyllable (VCV pattern in Japanese) unit and the tripartite disyllable unit. A tripartite disyllable is composed of smaller speech units, a single consonant phone (in the case of Japanese) surrounded by two vowel demiphones (context sensitive half phones). For speaker-dependent isolated-word recognition, and training on data from three recording sessions of the same continuous speech training set, we obtained an average recognition performance of 96.6% for the merged system. This is a 9.1% improvement in the recognition rate over the standard disyllable system, and 0.8% over the tripartite disyllable system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-318"
  },
  "ueda90_icslp": {
   "authors": [
    [
     "Yoshio",
     "Ueda"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Diction for phoneme/syllable/word-category and identification of language using HMM",
   "original": "i90_1209",
   "page_count": 4,
   "order": 320,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "Natural languages were modeled popularly by Markov models. In this paper, natural languages were modeled by HMM (Hidden Markov Model). And the identification of language and the prediction for phoneme/syllable/word-category were performed using HMM. The results show that the HMM had a performance better than the first order Markov model (bigram) and almost the same as the second order Markov model (trigram) on the entropy. From the results, we believe that HMM is not useful only speech recognition but also natural language processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-319"
  },
  "otsuki90_icslp": {
   "authors": [
    [
     "Takashi",
     "Otsuki"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Toshio",
     "Sone"
    ],
    [
     "Ken'iti",
     "Kido"
    ]
   ],
   "title": "Performance evaluation in speech recognition system using transition probability between linguistic units",
   "original": "i90_1213",
   "page_count": 4,
   "order": 321,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "This paper describes performance evaluation in speech recognition system which uses transition probability between linguistic units. The lower limit of word recognition score is predicted based on phoneme recognition score and number of word pairs with short distance in a vocabulary defined by linguistic information. But it is difficult to calculate it when transition probability is used as linguistic information. We propose new algorithm to calculate it when bigram or trigram of linguistic units is used. Using this algorithm, we carry out performance prediction in speech recognition which uses bigram or trigram. Recognition score for word with 5 phonemes is more than 26% using bigram, more than 71% using trigram and more than 95% using a dictionary when phoneme recognition score is 90%, where bigram and trigram of phonemes are estimated from the 5,317 Japanese popular words. Recognition score of sentence composed of 11 words is more than 4.3% using bigram, on the other hand, more than 67% using trigram, when word recognition score is 80%, where bigram and trigram are estimated from 136 sentences represented with 18 kinds of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-320"
  },
  "murase90_icslp": {
   "authors": [
    [
     "Isao",
     "Murase"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Sentence recognition method using word cooccurrence probability and its evaluation",
   "original": "i90_1217",
   "page_count": 4,
   "order": 322,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "In this paper, we describe the sentence recognition method using word cooccurrence probability, and compare it with the method using CFG(Context-Free Grammar). Since the aim of this study is to investigate whether the word cooccurrence probability (bigram or trigram) is able to represent the corresponding context-free grammar or not, we calculated it from CFG, By comparing the results using the bigram method with those using the method of CFG, we realized that it is insufficiency to represent CFG with only word cooccurrence probabilities from both perplexity and sentence recognition accuracy. Therefore, to improve the bigram we have extended the bigram model to a quasi-trigram model (subclass-word-word). From the experimental results using this quasi-trigram we realized that the quasi-trigram model can represent the corresponding CFG better than the corresponding bigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-321"
  },
  "lu90_icslp": {
   "authors": [
    [
     "Yanghai",
     "Lu"
    ],
    [
     "Beiqian",
     "Dai"
    ]
   ],
   "title": "A knowledge-based understanding system for the Chinese spoken language",
   "original": "i90_1221",
   "page_count": 4,
   "order": 323,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "This paper describes a Chinese spoken language understanding system USTC-2. Various kinds of knowledge such as acoustic-phonetics, vocabulary, syntax and semantics are represented and utilized in the system. It's constructed as an expert system based on frame representation capable of metting the needs of various tasks. In USTC-2 system, segmentation of input speech is carried out utilizing the energy contour, LPC variance and syllable length. Recognition is performed by a matching algorithm which constrains the search on the basis of morphological knowledge concerning both the part of word-class and and constituent syllales of a given word . The use of morpohological knowledge not only reduces the amount of computation but also increases the correct rate of syllable recognition in spoken Chinese sentences. An analysis method starting from KEYWORDS was proposed in speech understanding. This method combines syntastic analysis and semantic analysis together and has strong ability of correcting errors. The system discriminates homonymous syllables and homonymous words in each processing step.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-322"
  },
  "komatsu90_icslp": {
   "authors": [
    [
     "Akio",
     "Komatsu"
    ],
    [
     "Eiji",
     "Oohira"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Conversational speech understanding based on cooperative problem solving",
   "original": "i90_1225",
   "page_count": 4,
   "order": 324,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "Natural conversational speech is so ambiguous that a system for understanding it requires the cooperation of many knowledge sources. We propose here a general framework for cooperative problem solving based on the blackboard model and a TMS (truth maintenance system) with an enhanced proving function. In this framework, a reasonably consistent interpretation is automatically kept on the blackboard, while each knowledge source performs its own inference and puts the results on the blackboard. Based on the proposed cooperative problem solving framework, we established a model for a system which can understand conversational speech through the cooperation of independent knowledge sources. The feasibility and validity of our basic framework were comfirmed by computer simulation experiments with several conversational speeches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-323"
  },
  "okada90_icslp": {
   "authors": [
    [
     "Michio",
     "Okada"
    ]
   ],
   "title": "A one-pass search algorithm for continuous speech recognition directed by context-free phrase structure grammar",
   "original": "i90_1229",
   "page_count": 4,
   "order": 325,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "This paper presents an efficient parsing algorithm for integrating the search problems both in speech and language processing in general use for speech understanding systems. The parsing algorithm we propose is regarded as an extension of the finite-state-network-directed, one-pass search algorithm to the one directed by a context-free grammar with retention of the time-synchronous procedure. Our attempt is to embed an active chart parser in the one-pass search algorithm for top-down prediction. Extended formalization of the chart parser makes it possible to incrementally generate both new vertices and multiple lexical edges based on a data-driven beam search technique. Moreover, some extensions of the algorithm are discussed. This paper shows that the one-pass search algorithm proposed can be extended to a time-synchronous Viterbi-style beam search procedure that is guaranteed to find the N most likely sentences under the constraints of a unification-based phrase structure grammar.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-324"
  },
  "carlo90_icslp": {
   "authors": [
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Rino",
     "Falcone"
    ]
   ],
   "title": "A blackboard architecture for a word hypothesizer and a chart parser interaction in an ASR system",
   "original": "i90_1233",
   "page_count": 4,
   "order": 326,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In this work, we present our recent efforts to find an effective framework for the interaction between lexical hypothesization modules and syntactical parsers of an Automatic Continuous Speech Recognition. In this perspective the main problem is the uncertainty processing. We consider the chart a suitable blackboard to represent and to work on the unperfectly recognized linguistic data and structures. Our approach to the uncertainty processing is based on the asynchronous interaction between several modules in which we implement the basic functionalities of the word hypothesization and of the syntactical parsing with different running modalities (top-down and bottom-up knowledge activation, left-to-rigth and bidirectional input processing, breadth-first and depth-first search, best-first and beam use of the scoring). We describe the developed modules and the interaction preliminary framework then we present a brief discussion on the use of the system. Some ideas for the further activity are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-325"
  },
  "mousel90_icslp": {
   "authors": [
    [
     "P.",
     "Mousel"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ],
    [
     "A.",
     "Roussanaly"
    ]
   ],
   "title": "Heuristic search problems in a natural language task oriented spoken man-machine dialogue system",
   "original": "i90_1237",
   "page_count": 4,
   "order": 327,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "The paper first presents the dialogue system DIAL now being developed at the Centre de Recherche en Informatique de Nancy (CRIN). It then focusses on heuristic search problems related to the components in charge of parsing and interpretation, the syntactic-semantic component (SYNSEM) and the pragmatic component (DIALOG). After briefly outlining the operation of these components, we discuss two major types of heuristic search strategies in these components, selection strategies and propagation strategies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-326"
  },
  "kitano90b_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kitano"
    ]
   ],
   "title": "The making of a speech-to-speech translation system: some findings from the dmdialog project",
   "original": "i90_1241",
   "page_count": 4,
   "order": 328,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "In this paper, we discuss some important problems which we encountered in developing the DmDialog real-time speech-to-speech translation system. DMDlALOG is a real-time Japanese-English speech-to-speech dialog translation system that accepts speaker-independent continuous speech input and produces audio output of translated sentences. It has been publicly demonstrated at the Center for Machine Translation at Carnegie Mellon University since March 1989. Major problems are seen in the areas of (1) integration of linguistic and statistical nautral language processing, (2) use of discourse knowledge to improve speech recognition rates, and (3) attainment of simultaneity in translation. We chose these problems to discuss here because they are very difficult and unique to speech-to-speech translation systems; they are not encountered in text-based machine translaton systems. We describe our approaches to these problems and describe problems left unresolved, so that this paper can serve as a basis for further discussions on these issues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-327"
  },
  "lokenkim90_icslp": {
   "authors": [
    [
     "K. H.",
     "Loken-Kim"
    ],
    [
     "Yasuhiro",
     "Nara"
    ],
    [
     "Shinta",
     "Kimura"
    ]
   ],
   "title": "Using high level knowledge sources as a means of recovering DLL-formed Japanese sentences distorted by ambient noise",
   "original": "i90_1245",
   "page_count": 3,
   "order": 329,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "In this paper, the authors present the performance of a large vocabulary Japanese spoken sentence recognition system tested under several noise levels (S/N (Signal to Noise Ratio): lOdB, 20dB, 25dB, 30dB, ), and discuss the ability of the system to recover input sentences. The result of the performance evaluation was that the system recovered over 90% of the test sentences when the noise level was low ( Â°Â°, 30dB), but recovered only 50% of the test sentences when the noise level reached to an S/N of 20dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-328"
  },
  "baekgaard90_icslp": {
   "authors": [
    [
     "Anders",
     "Baekgaard"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "Tools for designing dialogues in speech understanding interfaces",
   "original": "i90_1249",
   "page_count": 4,
   "order": 330,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "The ESPRIT project 'Integration of Speech Understanding Interfaces ' (SUNSTAR) concentrates on the design of a Dialogue System and the integration of existing off-the-shelf speech input and output devices into a number of practical applications. The communication between the speech I/O devices and a specific application is managed by a Dialogue System, which consists of a number of modules the aim of which are to guide speech activated I/O messages between the user and the application in a user-friendly and meaningful way, to forward user requests to the application, and to send back responses from the application to the user. Furthermore, the Dialogue System is supervising the state of functionality of the entire man-machine system. This paper focuses its attention on the development of the individual modules of the Dialogue System, which consists of the Dialogue Description Language including a common Dialogue Design Tool and an Interpretation and Control Module. The Dialogue Design Tool is an essential part of the Dialogue System. It is used by the Dialogue Developer for assistance during the specification of a specific application dialogue. The SUNSTAR project has partners from industry, telecommunication organisations and universities from five European countries, and the project is focused on establishing a number of practical speech I/O activated applications for the professional and public domains.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-329"
  },
  "takizawa90b_icslp": {
   "authors": [
    [
     "Osamu",
     "Takizawa"
    ],
    [
     "Masuzo",
     "Yanagida"
    ]
   ],
   "title": "A method for expressing associative relations using fuzzy concepts -aiming at advanced speech recognition-",
   "original": "i90_1253",
   "page_count": 4,
   "order": 331,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "A frame-based association mechanism is proposed aiming at advanced speech recognition. The mechanism is based on two knowledge bases. One is a hierarchical dictionary and the other, an association index file. In the association index file \"Association Confidence Factor (ACF)\" is introduced to express a directional weight of each link from a source concept to a target concept using a membership function. As it is impossible to assign ACFs on every links among all concepts, the number of links to which ACFs are assigned is limited, so it is required to provide means to give appropriate ACF values for the rest of the links on demand employing available ACFs. A set of formulae is proposed for that purpose. In addition, psychological experiments were conducted to investigate human association activity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-330"
  },
  "tubach90_icslp": {
   "authors": [
    [
     "Jean-Pierre",
     "Tubach"
    ],
    [
     "Raymond",
     "Descout"
    ],
    [
     "Pierre",
     "Isabelle"
    ]
   ],
   "title": "Bilingual speech interface for a bidirectional machine translation system",
   "original": "i90_1257",
   "page_count": 4,
   "order": 332,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "This paper describes the work carried out at CWARC (Canadian Workplace Automation Research Centre) (Voice Technology Group (VTG), in co-operation with the Machine Translation Group (MTG)). It investigates the use of commercially available speech technology devices for advanced human-computer interaction applications, a very important matter nowadays.\n",
    "The MTG at CWARC has developed a bi-directional machine translation system, working between French and English, for meat and cattle market reports issued by Agriculture Canada. An initial speech input/output interface (IRMA) was designed for this system by the VTG, and was demonstrated successfully at the Expotec exhibition in Montreal over the summer of 1989.\n",
    "The purpose of this work is to provide CRITTER with a more advanced speech input interface than that integrated in IRMA (continuous speech, multi-speaker or high-quality), using the VECSYS and XCOM MEDIA50 recognition boards. The initial project objectives dealt only with French, but the satisfactory performance of the study and its initial results showed that it was possible and desirable for English to also be included.\n",
    "The result was a bi-directional (French English) translation system with speech input, a \"first\" to the best of our knowledge.\n",
    "This project offers interesting ergonomic potential for the translators' workstation project, since users can enter either the source language text for machine-aided translation by CRITTER, or the results of their \"human\" translation in the target language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-331"
  },
  "laprie90b_icslp": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Optimum spectral peak track interpretation in terms of formants",
   "original": "i90_1261",
   "page_count": 4,
   "order": 333,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "An original approach to interpret spectral peak tracks in terms of formants is described. That enables to process results of algorithms which extract peak tracks rather than formant tracks. Our approach relies on the following optimality criterion: the best track combination for formant interpretation is the one which explains more energy in terms of formants than any other combination and which satisfies constraints placed upon formants (Fl F2 and F2 F3 compatibility). The algorithm is made up of two distinct steps: - a local formant interpretation in order to determine at each instant the best interpretation, - a global formant interpretation guided by results of the preceding step. The assessment of the global interpretation enables to choose the best formant tracking parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-332"
  },
  "thierry90_icslp": {
   "authors": [
    [
     "Spriet",
     "Thierry"
    ]
   ],
   "title": "A speech understanding system",
   "original": "i90_1265",
   "page_count": 4,
   "order": 334,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "The study we present here is a basic structure for a continuous speech recognition system. Our research concentrated on the general analysis algorithm and communication between the different linguistic levels. These modules and the algoritm are driven by a supervisor which processes information provied by the modules as theorical constraints, to reduce the branching ratio. These constraints are marked (phonetic, lexical, or syntactic) but have a meaning only for the module which produces them. The separation of the information sources and the processing operations enables the system to adapt rapidly to new applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-333"
  },
  "hangai90_icslp": {
   "authors": [
    [
     "Seiichiro",
     "Hangai"
    ],
    [
     "Kazvhiro",
     "Miyauchi"
    ]
   ],
   "title": "Speaker based on multipulse excitation and UPC vocal-tract model",
   "original": "i90_1269",
   "page_count": 4,
   "order": 335,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "In automatic speaker identification, the reduction of dimensions of template is a key to realize a quick identification and to save storage. In this study, we extract some glottal wave parameters which does not seem to be susceptible to mimicry and combine them with some LPC vocal tract parameters to make a smaller sized template. In the extraction of feature parameters of glottal wave, the multipulse excitation model is adopted under modifying the pulse search algorithm. According to experimental results obtained from 30 speakers' 5 vowels, 94% identification rate with a template of 10 feature parameters (8 vocal tract and 2 glottal parameters) and 99% with a template of 36 feature parameters (14 vocal tract and 22 glottal parameters) are obtained respectively. In comparison with the number of parameters to get same identification rate only with a formants' template, the number of feature parameters is reduced by 5 in case of getting 94% identification rate and 7 in case of getting 99% identification rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-334"
  },
  "jou90_icslp": {
   "authors": [
    [
     "I-Chang",
     "Jou"
    ],
    [
     "Su-Ling",
     "Lee"
    ],
    [
     "Min-Tau",
     "Lin"
    ],
    [
     "Chih-Yuan",
     "Tseng"
    ],
    [
     "Shih-Shien",
     "Yu"
    ],
    [
     "Yuh-Juain",
     "Tsay"
    ]
   ],
   "title": "A neural network based speaker verification system",
   "original": "i90_1273",
   "page_count": 4,
   "order": 336,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "This paper proposes a neural network based speaker verification system. Different with the determined way of traditional method, the neural net support the ability of automatically constructing the separation space from the data we give. In this paper We just utilize the clustering power of neural net and the statistics of the speech's features onto the speaker verification system. A speech input is segmented into small frames and we takes their linear predicted coefficients (LPC) as their feature maps. Then a three layer perceptron is used as training and recalling machine. After training, the results of recalling test has shown that it can get very good performance (inside test 98.5% and outside test 98.3%)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-335"
  },
  "yin90_icslp": {
   "authors": [
    [
     "Hujun",
     "Yin"
    ],
    [
     "Tong",
     "Zhou"
    ]
   ],
   "title": "Speaker recognition using static and dynamic CEPSTRAL feature by a learning neural network",
   "original": "i90_1277",
   "page_count": 4,
   "order": 337,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "We have applied a Multi-layer learning neural network to speaker recognition. A set of LPC-based cepstral coefficients and their orthogonal polynomials were chosen as static and dynamic spectral feature of speaker's utterance. A number of experiments have been conducted to assess the performance of the network of both text-dependent and text-independent speaker recognition tasks. The results describe the relations between the recognition accuracy and the number of hidden units, the number of training set presentations, and the number of speaker database. In a critical condition, the network achieved a high recognition rate of 98.5 percent correct and 95.2 percent correct in text-dependent and text-independent tests respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-336"
  },
  "osaka90_icslp": {
   "authors": [
    [
     "Naotoshi",
     "Osaka"
    ]
   ],
   "title": "Conversational turn-taking model using PETRI net",
   "original": "i90_1297",
   "page_count": 4,
   "order": 338,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "This paper deals with a newly proposed identification problem of the turn-taking (surface level exchange) pattern of spontaneous conversation using only the power patterns of two conversants. Some necessary concepts of turn-taking are first defined corresponding to the power pattern. A new algorithm, HSPN (Hidden Stochastic Petri Net) is introduced to identify Conversation Unit (CU), that is, turn-taking patterns. HSPN is equivalent to a multi channel coupled Hidden Markov Model (HMM), which incorporates synchronization of inter channel states. It is transformed into an ordinary Markov model using HSPN. A high CU correct rate is obtained for spontaneous conversation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-337"
  },
  "yamamoto90_icslp": {
   "authors": [
    [
     "Tetsuya",
     "Yamamoto"
    ],
    [
     "Yoshikazu",
     "Ohta"
    ],
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Dialog management system mascots in speech understanding system",
   "original": "i90_1301",
   "page_count": 4,
   "order": 339,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "Many kinds of expert systems have been developed in various fields to date. To realize interaction through spoken Japanese between a user and such an expert system, we are currently developing a dialog management system called MASCOTS in addition to the speech understanding system and the speech synthesizer. MASCOTS manages the complex flow of dialog using two stacks and plan information. It sends useful information tailored in appropriate forms to the expert system and helps the language processing system by predicting the next user utterance. This paper describes the architecture of MASCOTS focusing on exchanges of the information with the language processing system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-338"
  },
  "oviatt90_icslp": {
   "authors": [
    [
     "Sharon L.",
     "Oviatt"
    ],
    [
     "Philip R.",
     "Cohen"
    ],
    [
     "Ann M.",
     "Podlozny"
    ]
   ],
   "title": "Spoken language in interpreted telephone dialogues",
   "original": "i90_1305",
   "page_count": 4,
   "order": 340,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "This research outlines the predominant dialogue and performance characteristics of 3-person interpreted telephone speech during service-oriented dialogues, in comparison with those of 2-person noninterpreted dialogues. An empirical study was conducted in which 12 native English speakers each made one telephone call through an experienced telephone interpreter to a Japanese confederate who did not speak English, and a second call to a Japanese confederate fluent in English. In total, 24 dialogues were collected, each one containing two successfully completed service tasks, or 48 tasks total. This paper reports on comparisons performed between 3-person interpreted and 2-person noninterpreted speech, based on the same pool of tasks and English subjects. The unique characteristics of interpreted telephone dialogues are outlined, including structural and referential features, miscommunications and other performance characteristics, confirmatory language, and linguistic indirection. In addition, a.n analysis is presented of interpreters' strategic management of turn shifts and of the content and sequencing of information passed among speakers. The long-term goal of this exploratory research is the modelling of human speech, and the specification of preliminary target requirements for future automatic systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-339"
  },
  "morimoto90b_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ]
   ],
   "title": "Linguistic knowledge for spoken dialogue processing",
   "original": "i90_1309",
   "page_count": 4,
   "order": 341,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "In this paper, we discuss what kinds of linguistic knowledge are important and how they contribute to improvement of speech recognition accuracy by analyzing the actual results from a Japanese speech recognition experiment. Linguistic knowledge is categorized into four groups: syntax, semantics, pragmatics and contextual information. Through analysis, qualitative and quantitative evaluation of their abilities are made.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-340"
  },
  "hoge90_icslp": {
   "authors": [
    [
     "Harald",
     "HÃ¶ge"
    ]
   ],
   "title": "SPICOS II - a speech understanding dialogue system",
   "original": "i90_1313",
   "page_count": 4,
   "order": 342,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "SPICOS II represents a speaker adaptive speech understanding dialogue system allowing data base requests. The system consists of a dialogue manager, components for acoustic and linguistic analysis and components for data base access and answer generation. The acoustic analysis is based on a speaker adaptive articulatory feature vector and speaker-independent HMM-phoneme models. The linguistic analysis is carried out using an augmented phrase structure grammar and a formal-logic semantic representation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-341"
  },
  "zue90_icslp": {
   "authors": [
    [
     "Victor W.",
     "Zue"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "Dave",
     "Goddeau"
    ],
    [
     "David",
     "Goodine"
    ],
    [
     "Hong C.",
     "Leung"
    ],
    [
     "Michael K.",
     "McCandless"
    ],
    [
     "Michael S.",
     "Phillips"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Dave",
     "Whitney"
    ]
   ],
   "title": "Recent progress on the MIT VOYAGER spoken language system",
   "original": "i90_1317",
   "page_count": 4,
   "order": 343,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "The MIT voyager speech understanding system, described in some detail in [1], is an urban exploration system that interacts with the user through spoken dialogue and graphics. This paper describes some recent changes made in the Voyager system, and attempts to assess the effectiveness of these developments. Two key improvements are a tighter integration of the speech and natural language components and a pipelined hardware implementation leading to a speed-up in processing time from approximately 12 times real time to approximately 3 times real time. We have also made a number of incremental improvements in the word-pair grammar, pronunciation networks, and the back-end capabilities. Finally, we demonstrate substantial performance improvements, mainly as a consequence of tighter integration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-342"
  },
  "koopmansvanbeinum90b_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ]
   ],
   "title": "The source-filter model of speech production applied to early speech development",
   "original": "i90_1321",
   "page_count": 4,
   "order": 344,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "In order to understand the speech developmental process in infants from birth onwards, and design an instrument suitable to relate early infant sound productions to basic elements of adult speech, we tried to map the sensorimotoric development of the sound producing mechanism and possibly trace any systematics within the process. Subsequently we intend to describe the acoustic aspects of the sound productions within well defined motoric stages.\n",
    "A method to study this developmental process was designed, based on the source-filter model of speech production. Within the framework of the anatomical and physiological capacities of the infant, source activities were described in aspects of phonatory development. Filter activities were represented by developmental aspects of articulatory movements. A breath group was used as a segmentation unit.\n",
    "This method resulted in six clearly recognizable, sensorimotoric developmental stages in the infant's sound productions in the first year of life. Based on these results we tried to mark within the sound productions of one infant in his first year of life, each of the stages I-V by a number of acoustic features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-343"
  },
  "miura90_icslp": {
   "authors": [
    [
     "Ichiro",
     "Miura"
    ]
   ],
   "title": "The acquisition of Japanese long consonants, syllabic nasals, and long vowels",
   "original": "i90_1325",
   "page_count": 4,
   "order": 345,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "Kindergarten children of ages three through five as well as adults pronounced six Japanese nouns including long consonats, syllabic nasals, and long vowels. Their utterances were recorded and segmental durations were measured based mainly on the spectrographic analysis. The results showed that a strong developmental change is observed in the acquisition of long consonants and syllabic nasals whereas a weak developmental change is observed in the acquisition of long vowels. They also showed that no effect of word accent is observed in the acquisition of these sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-344"
  },
  "shimura90_icslp": {
   "authors": [
    [
     "Yoko",
     "Shimura"
    ],
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Kozue",
     "Saito"
    ],
    [
     "Tamiko",
     "Ichijama"
    ],
    [
     "Jan",
     "Gauffin"
    ],
    [
     "Pierre",
     "Halle"
    ],
    [
     "Itsuro",
     "Yamanouchi"
    ]
   ],
   "title": "Infants' vocalization observed in verbal communication: acoustic analysis",
   "original": "i90_1329",
   "page_count": 4,
   "order": 346,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "The vocal behaviour of 26 young infants was recorded in order to throw some light on speech acquisition processes. So far, acoustic analyses were carried out for 4 infants aged from 2 to 17 months. Infants were addressed by either their mother, adults unfamiliar to them, or voices presented to them through a loudspeaker. Results were as follows. (1) Even very young infants were able to produce a wide spectrum of speech-like sounds, for example with abrupt F0 changes, or with various voice qualities, characterized by the richness of subharmonic components and of noise components. (2) Mothers tended to use a wider and higher F0 range while speaking to their child than while speaking to adults. (3) Vocal behaviour of infants change according to how and by whom they were spoken to. They produced longer vocalizations in response to their mother's incentive speech, and they did so more often than with other adults.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-345"
  },
  "masuko90_icslp": {
   "authors": [
    [
     "Yukie",
     "Masuko"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Perception of mora sounds in Japanese by non-native speakers of Japanese",
   "original": "i90_1333",
   "page_count": 4,
   "order": 347,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "The listening test was performed on Japanese mora sounds with learners of Japanese. Their native languages were Chinese, Thai, Indonesian and Korean. Word accent pattern, vowel length, voiceless consonant length, nasal consonants and the nasalized vowel were examined. The results showed that word accent pattern was identified better by the subjects of tone languages than the other groups of subjects. Vowel length was identified better than voiceless consonant length, but the difference from their native languages were not found. According to nasals, two types of errors (confusions and omissions & additions) were examined. Compared to the Indonesian subjects, the Chinese and the Thai subjects made more errors of omission & addition, while the Korean subjects made more errors of both types. It is supposed that the monosyllable structure of Chinese and Thai have relation to the errors of omission & addition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-346"
  },
  "fant90_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ]
   ],
   "title": "The speech code. segmental and prosodic features",
   "original": "i90_1389",
   "page_count": 9,
   "order": 348,
   "p1": "1389",
   "pn": "1398",
   "abstract": [
    "This is a review of basic issues in speech analysis with an emphasis on work carried out at the KTH in Stockholm. The concept of the speech code as a knowledge base for speech research and applications is outlined. In a narrow sense the speech code may be substantiated by programs and rules for text-to-speech conversion, in a wider sense also covering speaker type and speaking style and other aspects of a message not inherent in a text. For adequate descriptions we need the support of extensive speech analysis, articulatory modelling and perceptual critera. This is exemplified by a discussion of labial and velar stops in a back vowel context. One aspect of the interaction between prosodic and segmental features is the degree of articulatory and associated spectral contrast which becomes enhanced with increasing stress. A brief review is made of stress correlates based on objective and subjective measures with comments are made on language differences. Interstress intervals do not display physical isochrony but some readers plan their pauses to integrate in rhythmical synchrony with a perceived local average stress rate, The average pause-to-speech ratio is a speaker specific characteristic.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-347"
  },
  "pisoni90b_icslp": {
   "authors": [
    [
     "David B.",
     "Pisoni"
    ]
   ],
   "title": "Effects of talker variability on speech perception: implications for current research and theory",
   "original": "i90_1399",
   "page_count": 9,
   "order": 349,
   "p1": "1399",
   "pn": "1408",
   "abstract": [
    "This paper summarizes recent findings on the effects of talker variability on speech perception. The results of several experiments demonstrate that detailed information about a talker is encoded into multi-layered representations in long-term memory. These representations appear to be much more detailed than the canonical symbolic representations of speech that linguists have traditionally assumed. The representations also appear to be highly context-dependent and apparently preserve a great deal of information in the speech signal. The usefulness of these representations for optimal strategies of lexical access is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1990-348"
  },
  "itakura90_icslp": {
   "authors": [
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Early developments of LPC speech coding techniques",
   "original": "i90_1409",
   "page_count": 1,
   "order": 350,
   "p1": "1409",
   "pn": "1410",
   "abstract": [
    "Since the invention of a speech analysis-synthesis system based on the maximum likelihood(ML) spectrum estimation in 1966 by Shuzo Saito and the author, research and development of efficient speech coding methods and their applications to computer and communication systems have been actively conducted in Japan. Our interest in LPC speech coding goes back to 1965 when I was looking for new research project for the graduate study at Nagoya University, after finishing the master degree on time series analysis of the bio-medical signals. At that time, an excellent book \"Speech Analysis Synthesis and Perception\" by Jim Flanagan was just printed and a review paper on vocoders by Manfred Schroeder was published in a short time later. Previous efforts to encode speech signal were well summarized in these documents. After examining these, I was convinced that the most fundamental problem in speech processing to achieve efficient speech coding and speech recognition, is to extract the spectral information from speech waveform as accurately and efficiently as possible. Accordingly, I tried to attack the problem from the view point of a statistical analysis of stationary random processes. The first outcome was the maximum likelihood (ML) spectral estimation of the all-pole spectrum. This method was motivated by the profound results on the hypothesis testing in time series analysis by Peter Whittle. The second outcome was the partial autocorrelation (PARCOR) analysis. The statistial concept of the partial autocorrelation was introduced to speech analysis, and the resulting lattice structure for the analysis and synthesis filter has now been widely used for general speech processing. The third development is the line spectrum pair(LSP) representation of speech spectrum. The concept of LSP was motivated by the work of Ya. L. Geronimus on the orthogonal polynomials on a circle (1948).\n",
    "In this talk, I would like to present a historical retrospective of the early developments of LPC speech analysis and coding techniques and their impacts on the recent developments of new generation speech coding methods and speech recognition.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Temporal Control in the Spoken Language",
   "papers": [
    "kohno90_icslp",
    "beckman90_icslp",
    "campbell90_icslp",
    "price90_icslp",
    "kaiki90_icslp",
    "koopmansvanbeinum90_icslp",
    "naganomadsen90_icslp"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "wang90_icslp",
    "wang90b_icslp",
    "tokuda90_icslp",
    "dix90_icslp",
    "funaki90_icslp",
    "lee90_icslp",
    "chang90_icslp"
   ]
  },
  {
   "title": "Voice Source Dynamics; Facts and Models",
   "papers": [
    "scherer90_icslp",
    "kiritani90_icslp",
    "cranen90_icslp",
    "karlsson90_icslp",
    "koizumi90_icslp",
    "iijima90_icslp",
    "kasuya90_icslp",
    "nichasaide90_icslp"
   ]
  },
  {
   "title": "Speech Coding and Transmission",
   "papers": [
    "alku90_icslp",
    "yeldener90_icslp",
    "miyano90_icslp",
    "saoudi90_icslp",
    "takahashi90_icslp",
    "tasaki90_icslp",
    "taniguchi90_icslp",
    "shoham90_icslp",
    "unno90_icslp",
    "ohya90_icslp"
   ]
  },
  {
   "title": "Extraction and Processing of Voice Individuality",
   "papers": [
    "noda90_icslp",
    "eatock90_icslp",
    "matsui90_icslp",
    "rosenberg90_icslp",
    "koo90_icslp",
    "hattori90_icslp",
    "kubala90_icslp",
    "abe90_icslp",
    "matsumoto90_icslp"
   ]
  },
  {
   "title": "Voice Source Characteristics and Synthesis",
   "papers": [
    "barney90_icslp",
    "estill90_icslp",
    "imaizumi90_icslp",
    "nakajima90_icslp",
    "wang90c_icslp",
    "iwata90_icslp",
    "itoh90_icslp",
    "pearson90_icslp",
    "alku90b_icslp"
   ]
  },
  {
   "title": "Speech Recognition and Enhancement",
   "papers": [
    "kim90_icslp",
    "asai90_icslp",
    "miki90_icslp",
    "sato90_icslp",
    "ando90_icslp",
    "wang90d_icslp",
    "kim90b_icslp",
    "kawahara90_icslp",
    "atkins90_icslp",
    "rossi90_icslp",
    "makino90_icslp",
    "mizuta90_icslp",
    "datta90_icslp",
    "koo90b_icslp",
    "kim90c_icslp",
    "shinoda90_icslp",
    "skinner90_icslp",
    "ono90_icslp",
    "tsuboi90_icslp",
    "sharrif90_icslp",
    "djoudi90_icslp",
    "bodden90_icslp",
    "usagawa90_icslp",
    "takizawa90_icslp",
    "matsumoto90b_icslp",
    "yegnanarayana90_icslp",
    "wang90e_icslp",
    "marchal90_icslp"
   ]
  },
  {
   "title": "Synthesis of Spoken Language",
   "papers": [
    "carlson90_icslp",
    "ceder90_icslp",
    "shimizu90_icslp",
    "yamaguchi90_icslp",
    "fujisaki90_icslp",
    "malsheen90_icslp",
    "hirokawa90_icslp",
    "takeda90_icslp",
    "shirai90_icslp",
    "rainton90_icslp",
    "coile90_icslp",
    "yamashita90_icslp",
    "lee90b_icslp",
    "ahn90_icslp",
    "liu90_icslp",
    "teranishi90_icslp",
    "kamanaka90_icslp",
    "ishikawa90_icslp",
    "higuchi90_icslp",
    "galas90_icslp",
    "hamagami90_icslp",
    "hakoda90_icslp",
    "asher90_icslp",
    "curtis90_icslp",
    "maeda90_icslp",
    "campbell90b_icslp",
    "chen90_icslp",
    "avesani90_icslp",
    "iwata90b_icslp",
    "fujisaki90b_icslp",
    "yamamura90_icslp",
    "wothke90_icslp",
    "divay90_icslp",
    "yamaguchi90b_icslp"
   ]
  },
  {
   "title": "Phoneme Recognition",
   "papers": [
    "shirai90b_icslp",
    "montacie90_icslp",
    "sagayama90_icslp",
    "laprie90_icslp",
    "ariki90_icslp",
    "franco90_icslp",
    "hirata90_icslp",
    "hirahara90_icslp"
   ]
  },
  {
   "title": "Recent Progress in Speech Perception Research",
   "papers": [
    "nooteboom90_icslp",
    "kitahara90_icslp",
    "goodman90_icslp",
    "amano90_icslp",
    "pisoni90_icslp",
    "ohala90_icslp",
    "nusbaum90_icslp",
    "massaro90_icslp",
    "fujisaki90c_icslp"
   ]
  },
  {
   "title": "Speech Production, Prosody and Analysis",
   "papers": [
    "foldvik90_icslp",
    "matsumura90_icslp",
    "kaburagi90_icslp",
    "motoki90_icslp",
    "suzuki90_icslp",
    "sonoda90_icslp",
    "kusakawa90_icslp",
    "simada90_icslp",
    "azuma90_icslp",
    "wang90f_icslp",
    "hahn90_icslp",
    "angderi90_icslp",
    "kadambe90_icslp",
    "fujisaki90d_icslp",
    "sugiyama90_icslp",
    "yogo90_icslp"
   ]
  },
  {
   "title": "The Role of Prosody in Production and Perception of Spoken Language",
   "papers": [
    "fujisaki90e_icslp",
    "bruce90_icslp",
    "takeda90b_icslp",
    "daly90_icslp",
    "kubozono90_icslp",
    "tsukuma90_icslp",
    "huber90_icslp",
    "sugito90_icslp",
    "maekawa90_icslp"
   ]
  },
  {
   "title": "Word Recognition",
   "papers": [
    "gurgen90_icslp",
    "shimodaira90_icslp",
    "gurlekian90_icslp",
    "sugi90_icslp",
    "imamura90_icslp",
    "kim90d_icslp",
    "matsuoka90_icslp",
    "vittorelli90_icslp",
    "takahashi90b_icslp"
   ]
  },
  {
   "title": "Perception of Spoken Language",
   "papers": [
    "kashino90_icslp",
    "datscheweit90_icslp",
    "tsuzaki90_icslp",
    "akagi90_icslp",
    "shigeno90_icslp",
    "massaro90b_icslp",
    "cutler90_icslp",
    "uosaki90_icslp",
    "sekimoto90_icslp"
   ]
  },
  {
   "title": "Perception, Impairments/Aids, Phonetics in Language Teaching and Speech Coding",
   "papers": [
    "hayashi90_icslp",
    "masaki90_icslp",
    "moore90_icslp",
    "mizutani90_icslp",
    "kariyasu90_icslp",
    "zhi90_icslp",
    "svantesson90_icslp",
    "iwata90c_icslp",
    "niimi90_icslp",
    "cui90_icslp",
    "nakashima90_icslp",
    "saita90_icslp",
    "kawaimusicalinstruments90_icslp",
    "shoham90b_icslp",
    "hazu90_icslp",
    "moriya90_icslp",
    "lee90c_icslp"
   ]
  },
  {
   "title": "Neural Networks for Speech Processing I, II",
   "papers": [
    "iso90_icslp",
    "bimbot90_icslp",
    "nakamura90_icslp",
    "torkkola90_icslp",
    "takami90_icslp",
    "masai90_icslp",
    "aikawa90_icslp",
    "xu90_icslp",
    "sayegh90_icslp",
    "morgan90_icslp",
    "tsuboka90_icslp",
    "minami90_icslp",
    "sawai90_icslp",
    "bulot90_icslp",
    "poirier90_icslp",
    "fanty90_icslp",
    "poddar90_icslp",
    "takara90_icslp",
    "watanabe90_icslp",
    "lucke90_icslp",
    "mobin90_icslp",
    "slamacazacu90_icslp"
   ]
  },
  {
   "title": "Continuous Speech Recognition",
   "papers": [
    "zhao90_icslp",
    "weigel90_icslp",
    "hayamizu90_icslp",
    "soong90_icslp",
    "gabrieli90_icslp",
    "kawabata90_icslp",
    "shigenaga90_icslp",
    "kobayashi90_icslp",
    "price90b_icslp"
   ]
  },
  {
   "title": "Modeling of First and Second Language Acquisition",
   "papers": [
    "menyuk90_icslp",
    "meltzoff90_icslp",
    "kohno90b_icslp",
    "kuhl90_icslp",
    "kojima90_icslp",
    "halle90_icslp",
    "yamada90_icslp",
    "mochizukisudo90_icslp"
   ]
  },
  {
   "title": "Application of Speech Recognition / Synthesis Technologies",
   "papers": [
    "berkley90_icslp",
    "velius90_icslp",
    "yato90_icslp",
    "sato90b_icslp",
    "amadorhernandez90_icslp",
    "saito90_icslp",
    "nitta90_icslp",
    "tsuboi90b_icslp",
    "hamada90_icslp"
   ]
  },
  {
   "title": "Language Modeling",
   "papers": [
    "corazzat90_icslp",
    "su90_icslp",
    "kita90_icslp",
    "saito90b_icslp",
    "bornerand90_icslp",
    "kitano90_icslp",
    "morimoto90_icslp",
    "sakano90_icslp",
    "matsunaga90_icslp"
   ]
  },
  {
   "title": "Phonetics and Phonology",
   "papers": [
    "lisker90_icslp",
    "kassel90_icslp",
    "derwing90_icslp",
    "dalsgaard90_icslp",
    "hieronymus90_icslp",
    "huang90_icslp",
    "djoudi90b_icslp",
    "recasens90_icslp",
    "suzuki90b_icslp"
   ]
  },
  {
   "title": "Assessment / Human Factors, Database and Neural Networks",
   "papers": [
    "bond90_icslp",
    "jekosch90_icslp",
    "hayashi90b_icslp",
    "silverman90_icslp",
    "maehara90_icslp",
    "carlson90b_icslp",
    "chan90_icslp",
    "svendsen90_icslp",
    "tuffelli90_icslp",
    "arai90_icslp",
    "tillmann90_icslp",
    "chimoto90_icslp",
    "chigier90_icslp",
    "danielsen90_icslp",
    "irii90_icslp",
    "wu90_icslp",
    "robinson90_icslp",
    "allen90_icslp",
    "depuydt90_icslp",
    "obara90_icslp",
    "kanedera90_icslp",
    "meng90_icslp",
    "cho90_icslp",
    "leung90_icslp",
    "chiba90_icslp",
    "kitazawa90_icslp",
    "lefebvre90_icslp",
    "yamaguchi90c_icslp"
   ]
  },
  {
   "title": "Speech I/O Assessment and Database I, II",
   "papers": [
    "itahashi90_icslp",
    "choi90_icslp",
    "sagisaka90_icslp",
    "ehara90_icslp",
    "gauvain90_icslp",
    "tanaka90_icslp",
    "soclof90_icslp",
    "makino90b_icslp",
    "millar90_icslp",
    "castagneri90_icslp",
    "pols90_icslp",
    "hardcastle90_icslp"
   ]
  },
  {
   "title": "Speech Recognition in Noisy Environments",
   "papers": [
    "juang90_icslp",
    "hanson90_icslp",
    "acero90_icslp",
    "hansen90_icslp",
    "kitamura90_icslp",
    "noll90_icslp",
    "fissore90_icslp",
    "nakadai90_icslp",
    "morii90_icslp",
    "gyoutoku90_icslp"
   ]
  },
  {
   "title": "Foreign Language Teaching",
   "papers": [
    "hardcastle90b_icslp",
    "rost90_icslp",
    "yoshida90_icslp",
    "nikolarea90_icslp",
    "nagano90_icslp",
    "saeki90_icslp",
    "rahimpour90_icslp",
    "murakawa90_icslp",
    "bernstein90_icslp"
   ]
  },
  {
   "title": "Continuous Speech Recognition and Speaker Recognition",
   "papers": [
    "abe90b_icslp",
    "ariki90b_icslp",
    "ye90_icslp",
    "zhiping90_icslp",
    "goldstein90_icslp",
    "ueda90_icslp",
    "otsuki90_icslp",
    "murase90_icslp",
    "lu90_icslp",
    "komatsu90_icslp",
    "okada90_icslp",
    "carlo90_icslp",
    "mousel90_icslp",
    "kitano90b_icslp",
    "lokenkim90_icslp",
    "baekgaard90_icslp",
    "takizawa90b_icslp",
    "tubach90_icslp",
    "laprie90b_icslp",
    "thierry90_icslp",
    "hangai90_icslp",
    "jou90_icslp",
    "yin90_icslp"
   ]
  },
  {
   "title": "Dialogue Modeling and Processing",
   "papers": [
    "osaka90_icslp",
    "yamamoto90_icslp",
    "oviatt90_icslp",
    "morimoto90b_icslp",
    "hoge90_icslp",
    "zue90_icslp"
   ]
  },
  {
   "title": "Language Acquisition",
   "papers": [
    "koopmansvanbeinum90b_icslp",
    "miura90_icslp",
    "shimura90_icslp",
    "masuko90_icslp"
   ]
  },
  {
   "title": "Plenary Lectures",
   "papers": [
    "fant90_icslp",
    "pisoni90b_icslp",
    "itakura90_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.1990"
}