{
 "location": "The Chinese University of Hong Kong, Hong Kong",
 "startDate": "15/12/2004",
 "endDate": "18/12/2004",
 "original_url": "http://www.isca-speech.org/archive_open/iscslp2004/index.html",
 "original_title": "Int'l Symp. on Chinese Spoken Language Proc.",
 "logo": "top_right.jpg",
 "conf": "ISCSLP",
 "year": "2004",
 "name": "iscslp_2004",
 "series": "ISCSLP",
 "SIG": "CSLP",
 "title": "International Symposium on Chinese Spoken Language Processing",
 "title1": "International Symposium on Chinese Spoken Language Processing",
 "date": "15-18 December 2004",
 "papers": {
  "huang04_iscslp": {
   "authors": [
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Enabling Natural Computing",
   "original": "keynote1",
   "page_count": 1,
   "order": 1,
   "p1": "0",
   "pn": "",
   "abstract": [
    "We are entering the third generation of computer human interface. In contrast to the first and second generations where human users must learn the arcane command languages or graphical icons to operate computers in the ways the computers are designed, the third generation interface will allow the users to express their intents naturally by shifting the burden of understanding what it takes to interact from the human to the computer. Natural computing will be mainstream in the near future that could dramatically improve the quality of our daily lives. Spoken language technologies play a central role for natural computing. Spoken language is the modality that can offer a consistent means of interaction for a variety of computer form factors across a wide range of hands free, eyes free environments. Technology advancements in this area have made impressive progresses that the prevalence of spoken language interface is no longer a question of “whether” but “when”. In this talk, I'll summarize the recent progress of the industry and academia in brining natural computing to the mass market.  ABOUT THE SPEAKER  Dr. Xuedong Huang is General Manager responsible for Microsoft's new incubation initiatives. Xuedong (XD) Huang joined Microsoft Research as a Senior Researcher to lead the formation of Microsoft's Speech Technology Group in 1993. From 2000 to 2004, he served as General Manager of Microsoft's Speech Platforms Group responsible for research, development, marketing, and business development of Microsoft speech technologies and products. He led Microsoft developing and marketing Microsoft's speech technologies and products, including the award-winning Speech Server 2004. Prior to joining Microsoft, he was on the faculty of Carnegie Mellon's School of Computer Sciences and directed the effort in developing CMU's Sphinx-II speech recognition system. He is an affiliate Professor at University of Washington. He has published more than 100 journal and conference papers and is a frequent keynote speaker in numerous industry conventions. He has co-authored two books: Hidden Markov Models for Speech Recognition (Edinburgh University Press 1990) and Spoken Language Processing (Prentice Hall 2001). Huang's professional awards include: National Education Commission of China's 1987 Science and Technology Progress Award, IEEE Signal Processing Society's 1992 Paper Award, Allen Newell Research Excellence Medal, and Top Ten Leaders in the speech industry award from SpeechTek. Huang holds a doctorate in Electrical Engineering from University of Edinburgh, a master's in Computer Sciences from Tsinghua University, and a bachelor's in Computer Sciences from Hunan University. Huang is a Fellow of the IEEE.\n"
   ]
  },
  "juang04_iscslp": {
   "authors": [
    [
     "Biing-Hwang (Fred)",
     "Juang"
    ]
   ],
   "title": "Speech Research in Telecommunications: A Bell-Centric View",
   "original": "keynote2",
   "page_count": 1,
   "order": 2,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Speech research aiming at developing technologies to enhance telecommunications has produced many remarkable results in the past five decades. In various branches of the field, many breakthrough technologies were brought about due to courageous paradigm shifts advocated by few. In this talk, we highlight the progress in speech processing technologies, particularly from a historical perspective as seen from Bell Laboratories, and point out these paradigm shifts in hope to inspire more technical breakthroughs.  ABOUT THE SPEAKER  Professor Biing-Hwang (Fred) Juang received his Ph.D. from University of California, Santa Barbara in 1981. He had worked at Speech Communications Research Laboratory (SCRL) and Signal Technology, Inc. (STI) on a number of Government-sponsored research projects. Notable accomplishments during the period include development of vector quantization for voice applications, voice coders at extremely low bit rates, 800 bps and around 300 bps, and robust vocoders for use in satellite communications. He was also a co-Principal Investigator for the project on co-channel separation of speech signals. He subsequently joined the Acoustics Research Department of Bell Laboratories, working in the area of speech enhancement, coding and recognition. Prof. Juang became Department Head/Director of Acoustics and Speech Research at Bell Labs in 1996, and Director of Multimedia Technologies Research at Avaya Labs (a spin-off of Bell Labs) in 2001. In the past few years, he and his group developed a speech server for applications such as AT&T's advanced 800 calls and the Moviefone, the Perceptual Audio Coder (PAC) for digital audio broadcasting in North America (in both terrestrial and satellite systems), and a world-first real-time full-duplex hands-free stereo teleconferencing system. Prof. Juang joined Georgia Institute of Technology in 2002 as Motorola Foundation Chair Professor in the School of Electrical and Computer Engineering. He is also an Eminent Scholar of Georgia Research Alliance of the State of Georgia. Prof. Juang has published extensively, including the book “Fundamentals of Speech Recognition”, co-authored with L.R. Rabiner, and holds about twenty patents. He has served as Editor-in-Chief for the IEEE Transactions on Speech and Audio Processing, and a number of positions in the IEEE Signal Processing Society, including the current Chair of its Fellow Reference Committee. He is currently on the IEEE Proceedings Editorial Board. Prof. Juang has received a number of technical awards, notable among which are several Best Paper awards in the area of speech communications and processing, the Technical Achievement Award from the Signal Processing Society, and the IEEE Third Millennium Medal. He is a Fellow of the IEEE, a Fellow of Bell Laboratories, and a member of the National Academy of Engineering of the United States.\n"
   ]
  },
  "wang04_iscslp": {
   "authors": [
    [
     "William Shi-Yuan",
     "Wang"
    ]
   ],
   "title": "Spoken Language Processing: People versus Machines",
   "original": "keynote3",
   "page_count": 1,
   "order": 3,
   "p1": "0",
   "pn": "",
   "abstract": [
    "A fundamental challenge we must meet for computers to eventually process spoken language as effectively as humans is to capture the immensely rich fund of information we have in our heads that is NOT in the speech signal. This information is what gives us the ability to supply acoustic cues when these are degraded or missing, or to zero in on one speaker amid a chorus of other voices. While the powerful statistical methods currently used in speech recognition and synthesis have brought some success and useful applications, future progress will depend crucially on a deeper knowledge and greater use of this information. Some of this information is applicable to all languages, and some of it is specific to individual language types. In my discussion, special attention will be given to the processing of spoken Chinese.  ABOUT THE SPEAKER  William S.-Y. Wang is Research Professor at the Chinese University of Hong Kong, in the Department of Electronic Engineering and in the Department of Linguistics and Modern Languages. He is an Academician and Research Fellow at Academia Sinica, Taiwan. He is also Professor of Linguistics Emeritus of the University of California at Berkeley, where he taught for some 30 years. His work has spanned broad areas of language study, including language evolution and language engineering. His publications have appeared in American Scientist, Nature, Proceedings of the National Academy of Sciences (USA), Scientific American, as well as in many professional journals. He was a Guggenheim Fellow, and twice a Fellow at the Center for Advanced Study in the Behavioral Sciences, Stanford, California. In recent years, he has been collaborating with biologists and computer scientists in a common search for the origins of language, and for patterns in language differentiation and language endangerment.\n"
   ]
  },
  "chou04_iscslp": {
   "authors": [
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "Minimum Classification Error Rate Pattern Recognition Approach for Speech and Language Processing",
   "original": "tutorial1",
   "page_count": 1,
   "order": 4,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Minimum classification error (MCE) rate pattern recognition approach is a fast moving research area and broadly applied to pattern recognition problems in speech and language processing. In this talk, we will give an overview of the basic MCE classifier design algorithms as well as the more advanced extensions of the MCE approach. We differentiate the classifier design by way of distribution estimation and by way of the discriminant function methods according to the minimum classification error rate paradigm. We study the practical issues in system implementation and highlight the application perspectives of applying MCE classifier design to practical speech and language processing systems.  ABOUT THE SPEAKER  Dr. Wu Chou is the technical manager of dialogue system research group at Avaya Labs Research, Avaya Inc. He received M.S. degree in mathematics in 1986, M.S. degree in electrical engineering in 1987, the M.S. degree in statistics in 1988, and the Ph.D. degree in electrical engineering in June 1990, all from Stanford University, California, USA, respectively. He joined the speech research department of AT&T Bell Laboratories in July 1990. Since joining Bell Labs, he has been working extensively in the areas of speech recognition and understanding, intelligent dialogue systems for multimodal/multimedia communication and interaction, natural language call routing, acoustic modeling, large vocabulary speech recognition, speaker adaptation, vector quantization, acoustic assisted image coding and animation, signal processing and oversampled sigma-delta modulation. Since October 2000, he moved to Avaya Labs Research leading a group on dialogue systems and multimodal/multimedia communication. He was an associate editor for IEEE Transactions on Speech and Audio Processing, the general chair of 2002 IEEE International Conference on Computer, Communication and Networking (IC3N'02), editor and the group chair of EMMA (Extensible Multimodal Annotation), a standard of W3C (World Wide Web Consortium). He received several honors including Bell Labs President's Gold Award in 1997. He authored or co-authored more than 80 technical papers, three book chapters, one edited book and holds 18 US and International patents.\n"
   ]
  },
  "kuo04_iscslp": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ]
   ],
   "title": "Maximum Entropy Modeling for Speech Recognition",
   "original": "tutorial2",
   "page_count": 1,
   "order": 5,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Maximum entropy (maxent) models have become very popular in natural language processing. This tutorial will begin with a basic introduction of the maximum entropy principle, cover the popular algorithms for training maxent models, and describe how maxent models have been used in language modeling and (more recently) acoustic modeling for speech recognition. Some comparisons with other discriminative modeling methods will be made. A substantial amount of time will be devoted to the details of a new framework for acoustic modeling using maximum entropy direct models, including practical issues of implementation and usage. Traditional statistical models for speech recognition have all been based on a Bayesian framework using generative models such as Hidden Markov Models (HMMs). The new framework is based on maximum entropy direct modeling, where the probability of a state or word sequence given an observation sequence is computed directly from the model. In contrast to HMMs, features can be asynchronous and overlapping, and need not be statistically independent. This model therefore allows for the potential combination of many different types of features.  Results from a specific kind of direct model, the maximum entropy Markov model (MEMM), will be presented. Even with conventional acoustic features, the approach already shows promising results for phone level decoding. The MEMM significantly outperforms traditional HMMs in word error rate when used as stand-alone acoustic models. Combining the MEMM scores with HMM and language model scores show modest improvements over the best HMM speech recognizer. This tutorial will give a sense of some exciting possibilities for future research in using maximum entropy models for acoustic modeling.  ABOUT THE SPEAKER  Hong-Kwang Jeff Kuo received the S.B. degree in Computer Science and the S.M. degree in Electrical Engineering and Computer Science in 1992, and the Ph.D. degree in Electrical and Medical Engineering in 1998, all from the Massachusetts Institute of Technology. In 1998, he joined Bell Laboratories as a Member of Technical Staff in Murray Hill, New Jersey, where he worked on research in speech recognition and spoken dialogue systems. In 2002, he joined the IBM T.J. Watson Research Center as a Research Staff Member. Dr. Kuo has published papers in journals and international conferences and workshops on many topics, including discriminative training for natural language call routing, language and pronunciation modeling, natural spoken language parsing and understanding, spoken dialogue systems, and models of normal and pathological speech production.\n"
   ]
  },
  "hwang04_iscslp": {
   "authors": [
    [
     "MeiYuh",
     "Hwang"
    ],
    [
     "Xin",
     "Lei"
    ],
    [
     "Tim",
     "Ng"
    ],
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Venkata Ramana Rao",
     "Gadde"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Yan",
     "Huang"
    ],
    [
     "Manhung",
     "Siu"
    ]
   ],
   "title": "Progress on Mandarin Conversational Telephone Speech Recognition",
   "original": "098",
   "page_count": 4,
   "order": 6,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "Over the past decade, there has been good progress on English conversational telephone speech (CTS) recognition, built on the Switchboard and Fisher corpora. In this paper, we present our efforts on extending language-independent technologies into Mandarin CTS, as well as addressing language-dependent issues such as tone. We will show the impact of each of the following factors: (a) simplified Mandarin phone set, (b) pitch features, (c) auto-retrieved web texts for augmenting ngram training, (d) speaker adaptive training, (e) maximum mutual information estimation, (f) decision-tree-based parameter sharing, (g) cross-word co-articulation modeling, and (h) combining MFCC and PLP decoding outputs using confusion networks. We have reduced the Chinese character error rate (CER) of the BBN-2003 development test set from 53.8% to 46.8% after (a)+(b)+(c)+(f)+(g) are combined. Further reduction in CER is anticipated after integrating all improvements.\n"
   ]
  },
  "pan04_iscslp": {
   "authors": [
    [
     "YiCheng",
     "Pan"
    ],
    [
     "ChiaHsing",
     "Yu"
    ],
    [
     "LinShan",
     "Lee"
    ]
   ],
   "title": "Large Vocabulary Continuous Mandarin Speech Recognition using Finite State Machine",
   "original": "092",
   "page_count": 4,
   "order": 7,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "Finite state transducer (FST), popularly used in natural language processing (NLP) area to represent the grammar rules and the characteristics of a language, has been extensively used as the core in large vocabulary continuous speech recognition (LVCSR) in recent years. By means of FST, we can effectively compose the acoustic model, pronunciation lexicon, and language model to form a compact search space. In this paper, we present our approaches of developing a LVCSR decoder using FST as the core. In addition, the traditional one-pass tree-copy search algorithm is also described for comparison in terms of speed, memory requirements and achieved character accuracy. 1 INTRODUCTION Automata theory has been developed with a long history, and relevant research is still ongoing due to its elegant framework and high efficiency. It has been widely used in natural language processing (NLP) area to model the grammar rules and characteristics of the language. The application of finite state automata on large vocabulary continuous speech recognition (LVCSR) was first introduced by M.Mohri[1], and a new concept of weighted-finite-state machine was introduced, including approaches of transforming the popularly used models such as  HMM models and N-gram language models to finite state machine[1][2]. By means of the weighting scheme introduced, we can effectively integrate several probability likelihood functions in a finite state machine in a unified approach. We can then incorporate different sources of knowledge easier and reduce the complexity of the search space. Finite-state-transducer (FST) is an extension of finite state automata. A finite state machine can accept specific input strings (the set of strings accepted by a finite-state-machine is referred to as a “language”) and a FST further has a string as output when it accepts strings. In the LVCSR system using FST as the core, we first compile three basic knowledge sources (HMM acoustic models, pronunciation lexicon, n-gram language model) into three respective FSTs. By means of the composition algorithm, we can further integrate the three FSTs into a vast search network. A Viterbi search is then performed on this network and a best-matched path along with the recognized word sequence will be returned. In the rest of this paper, we first review traditional one-pass tree-copy search algorithm in Section 2. We then introduce our decoder based on FST in Section 3. The experiments and comparison between the two different decoders in terms of speed, memory requirement and character accuracy are presented in Section 4. Finally, in Section 5 some discussions are given.   2 ONE-PASS TREE-COPY SEARCH In this section, we briefly review the traditional one-pass tree copy search algorithm. In this algorithm, the search is implemented in a left-toright, frame-synchronous fashion. We first compile a lexicon tree as shown in Fig.1, in which each arc represents a subsyllabic HMM model and each path from the root to a leaf represents one or several word(s) with the same pronunciation. The arcs visited by each path are the respective HMM models, of which each word is composed. In the process of searching, each active node may have several copies, where each copy represents a different language model history. The path from the root to an active node forms a partial path. If the active node is a leaf node, a new language model history is generated, all the nodes having the same history are recombined, and only the node with the highest score will survive while others pruned. Then from the new node having survived, we further generate a new tree copy or replace the respective existing tree root node, if the tree with specific history is already generated and the score of the new node having survived is higher than the existing one. It should be noted that we don’t need to actually make several tree-copies at run-time, and the tree is just the data structure taken as a reference. In practice, only the active nodes need to be kept in the memory, where each active node represents a partial path. It should be also noted that the number of the nodes activated at each frame may increase rapidly, thus the total number of active nodes may increase exponentially. With limited memory and computing power, we therefore need to further prune those active nodes with lowest scores. During pruning, we need to take the language model scores into account. At each node, there are one or several paths to go to different leaf node(s). We pick up the leaf among them which has the highest uni-gram language model score, and this score is the look-ahead language model score for the node. Each active node is then judged by the sum of their decoding score and the look-ahead language model score during pruning.               3 FST-BASED SEARCH ALGORITHM 3.1 FST and WFST An FST A can be represented as a six-tuple: > ∆ ∑ < δ, , , , , F n Q . Q is the set of all states in A, while n ∊ Q is the initial state and F⊆Q is the set of all final states.∑ is the alphabet set of input strings, while ∆ is the alphabet set of outt_u uo b_u f_u u u l_i g_ee ei iau uei d_u uei 對 腿 脫不了 託付給 j_u ueng 對中 uo sh_u u b_a a 巴 uan b_a ai 團拜 ji_i i 跡 多數 Fig.1 An example of a tree lexicon 0-7803-8678-7/04/$20.00 ©2004 IEEE 5 ISCSLP 2004\n"
   ]
  },
  "guo04_iscslp": {
   "authors": [
    [
     "Gang",
     "Guo"
    ],
    [
     "Chao",
     "Huang"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "A Comparative Study on Various Confidence Measures in Large Vocabulary Speech Recognition",
   "original": "102",
   "page_count": 4,
   "order": 8,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "In this paper, we have conducted a comparative study on several confidence measures (CMs) for large vocabulary speech recognition. Firstly, we propose a novel high-level CM that is based on the inter-word mutual information (MI). Secondly, we experimentally investigate several popular low-level CMs, such as word posterior probabilities, N-best counting, Likelihood Ratio Testing (LRT), etc. Finally, we have studied a simple linear interpolation strategy to combine the best low-level CMs with the best high-level CMs. All of these CMs are examined in two large vocabulary ASR tasks, namely the Switchboard task and a mandarin dictation task, to verify the recognition errors in baseline recognition systems. Experimental results show: 1) the proposed MI-based CMs greatly surpass another existing high-level CMs which are based on the LSA technique; 2) Among all lowlevel CMs, word posteriori probabilities give the best verification performance; 3) When combining the word posteriori probabilities with the MI-based CMs, the equal error rate is reduced from 24.4% to 23.9% in the Switchboard task and from 17.5% to 16.2% in the mandarin dictation task.\n"
   ]
  },
  "lo04_iscslp": {
   "authors": [
    [
     "Wai Kit",
     "Lo"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Generalized Posterior Probability for Minimizing Verification Errors at Subword, Word and Sentence Levels",
   "original": "066",
   "page_count": 4,
   "order": 9,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "Generalized posterior probability, a statistical confidence measure, is tested in this study for verifying optimally the recognized units at the subword, word and sentence levels. We developed the generalized posterior probability by analyzing the exponential weights of the acoustic and language model scores to minimize the total verification errors at different unit levels. Experimental results have demonstrated the effectiveness of this generalized confidence measure for verifying Chinese LVCSR output. The Chinese Basic Travel Expression Corpus (BTEC) is used for evaluation and the relative improvement of confidence error rate (CER) over the baseline performance is 47.76% for sentences, 27.31% for words and 4.64% for subwords.\n"
   ]
  },
  "wang04b_iscslp": {
   "authors": [
    [
     "Nick JuiChang",
     "Wang"
    ],
    [
     "ChingHo",
     "Tsai"
    ],
    [
     "Patrick",
     "Huang"
    ],
    [
     "JiaLin",
     "Shen"
    ]
   ],
   "title": "Chinese Large-Vocabulary Name Recognition System using Character Description and Syllable Spelling Recognition",
   "original": "048",
   "page_count": 4,
   "order": 10,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "The large-vocabulary name recognition technique is one of the challenging tasks in the application of Chinese speech recognition technology. It can be applied on long-list automatic attendant systems and automatic directory assistance systems. A Chinese name has usually two to three characters with each character pronounced as a single syllable. It is a high perplexity task to recognize a word from a long-list of candidates, like more than three hundred thousand unique names in our experiments, given a very short utterance like one to two seconds of speech. Two novel approaches under an interactive framework are proposed in this paper to aid the recognition of a Chinese name: Character Description Recognition (CDR) and Syllable Spelling Recognition (SSR). Together with our robust finite-state recognizer given a graph-structured syllable lexicon for the full names, we achieved a very promising name recognition success rate, 94.5%, in our system-initiative dialogue system.\n"
   ]
  },
  "zhou04_iscslp": {
   "authors": [
    [
     "Zhengyu",
     "Zhou"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Error Identification for Large Vocabulary Speech Recognition",
   "original": "099",
   "page_count": 4,
   "order": 11,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "This paper proposes two methods for identifying recognition error. The first method is a two-level schema [1]-given the recognition hypothesis of an utterance, an utterance classifier (UC) is first applied to decide if the hypothesis is error-free or erroneous; followed by a word classifier (WC) which is applied to each word hypothesis in the erroneous utterance to decide if the word hypothesis is a misrecognition. The second method is a one-level schema in which a word classifier is applied directly to all word hypotheses to detect word recognition errors. We compare the two methods at both word and utterance levels. Experimental results show that the two methods are comparable in terms of word error detection. However, the two-level schema is very effective in filtering out error-free utterance hypotheses, which offers a key advantage to economize on word error detection.\n"
   ]
  },
  "dang04_iscslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Takeharu",
     "Suzuki"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Investigation and Modeling of Coarticulation in Speech Production",
   "original": "051",
   "page_count": 4,
   "order": 12,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "Coarticulation during speech production takes place at both the physiological level concerned with articulators’ properties and the planning stage for elaborating motor commands. This study focuses on the investigation and modeling of planning aspects of coarticulation with the ultimate objective to implement human mechanism in controlling a physiological speech production model. A “carrier model” was derived from articulatory data to describe mechanisms of the coarticulation, in which the vocalic movement is considered to be the primary component as a “carrier wave” and the consonantal movement as a “modulation wave”. Interactions between the carrier and modulation waves were evaluated using phoneme sequences of VbCVcCVb (Vc: the central vowel; Vb: the bilateral vowel; C: consonants) out of articulatory data that was obtained from the electromagnetic articulographic system. The analysis of the articulatory data showed that the articulatory position of the central vowel tended to be assimilated towards that of the bilateral vowels.\n"
   ]
  },
  "tseng04_iscslp": {
   "authors": [
    [
     "ShuChuan",
     "Tseng"
    ]
   ],
   "title": "Spontaneous Mandarin Production: Results of a Corpus-Based Study",
   "original": "033",
   "page_count": 4,
   "order": 13,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "This paper presents empirical results of a corpus-based study attempting to characterize linguistic features of spontaneous Mandarin, which has been difficult to obtain before due to the lack of suitable speech material. Starting from linguistic considerations, these results of word frequency as well as syllable frequency should provide important cues to spontaneous speech production. Frequent words or syllables need special investigations into their phonetic forms in real production. Examinations of syllable structures also show that the distribution of onset consonant, nucleus and coda consonant in syllables which are often used in spontaneous Mandarin is similar across different speakers. And results of a segmental analysis also clearly indicate the likelihood of a segment being produced in spoken Mandarin.  1. INTRODUCTION  Conventionally, linguistic studies mainly rely on field works to document the use of languages often with a research focus on pronunciation, lexicon and sentence grammar. With the database construction methodology developed in corpus linguistics, new approaches to analyzing spoken language have become possible recently. It has an essential influence on spontaneous speech studies, because due to limitations of data size and database management it has been difficult to investigate and model spontaneous speech using the traditional research methods. This paper uses a corpus of spontaneous data to examine Mandarin, which is spoken in Taiwan. What we report in this paper is a new attempt to obtain linguistic characteristic of spontaneous Mandarin. The results are primitive, but with a great potential to be developed into a deep and systematic understanding of spoken language production. Frequency of actively used words and syllables in spoken language provides useful cues to a correct lexical selection, when the available acoustic information is not clear enough to select words in the lexicon. A lexicon for speech recognition systems, probably similar to the mental lexicon of a speaker, needs to store different phonetic forms of words for instance reduction, assimilation and contraction [4]. It is not realistic to consider all phonetic variations of all words listed in a standard dictionary, so frequent words are no doubt the most important ones we need to take into account first. Word and syllable frequency as well as segmental analysis can be of great use and this information can be systematically obtained by using spoken corpora. In addition, for notation used in this paper, lexical tones in Taiwan Mandarin have four marked realizations: (1) high level tone, (2) rising tone, (3) contour tone and (4) falling tone and the unmarked neutral tone (5). Different Chinese dialects have different numbers of lexical tones associated with different melodic values [2]. Throughout this paper, we use Pinyin to transcribe Mandarin words.   2. DATA AND GENERAL STATISTICS  Mandarin Conversational Dialogue Corpus (MCDC) was collected at the Institute of Linguistics, Academia Sinica from 2000 to 2001 [3]. It consists of eight transcribed conversations between strangers. The recorded speech data has a total length of approximately eight hours (the corpus will soon be released for public use). Because no blanks are available in the writing system of Mandarin to separate individual words, we have to segment the transcripts into words first. In order to ensure that the segmentation results are consistent, the automatic word segmentation and tagging system developed by the Chinese Knowledge Information Processing Group at Academia Sinica [1] is adopted to automatically segment word boundaries and syntactically tag the segmented words. General statistics are listed in Table 1. Table 1: General statistics of MCDC Speaker Sex Age Syllables Word types Word tokens Syllable/word ratio S-01 F 29 4,789 921 3,334 1.44  S-02 M 25 9,262 1,445 6,913 1.34 S-03 F 37 8,522 1,140 5,853 1.46 S-04 M 35 6,202 965 4,234 1.46 S-05 F 16 9,273 1,093 6,339 1.46 S-06 F 17 6,659 874 4,497 1.48 S-07 M 40 8,887 1,283 6,946 1.28 S-08 F 46 7,360 1,140 5,497 1.34 S-09 F 30 2,687 572 1,967 1.37 S-10 F 35 13,534 1,577 9,103 1.49 S-11 M 35 7,140 1,104 4,399\n"
   ]
  },
  "mok04_iscslp": {
   "authors": [
    [
     "Pik Ki Peggy",
     "Mok"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "Effects of Phonemic Vs Allophonic Density and Stress on Vowel-To-Vowel Coarticulation in Cantonese and Beijing Mandarin",
   "original": "039",
   "page_count": 4,
   "order": 14,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "Effects of phonemic vs. allophonic vowel distribution, stress and direction of coarticulation on V-to-V coarticulation were examined in Cantonese and Beijing Mandarin (BM). Cantonese has more vowel phonemes but BM has more allophones. Cantonese should show less Vto-V coarticulation than BM if phonemic contrast determines degree of V-to-V coarticulation. The vowels used were /i a u/ in /pVpVpV/ structures. Phonemic vowel space density did not influence V-to-V coarticulation differentially in Cantonese and BM. Effects of stress and direction were not consistent. Generally, there was more carryover coarticulation, and more coarticulation on unstressed vowels, but exceptions were common. No one factor appears to determine patterns of V-to-V coarticulation in different languages. Other potential phonological influences are discussed.\n"
   ]
  },
  "ding04_iscslp": {
   "authors": [
    [
     "Hongwei",
     "Ding"
    ],
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Ruediger",
     "Hoffmann"
    ]
   ],
   "title": "Glottalization in Inventory Construction: A Cross-Language Study",
   "original": "005",
   "page_count": 4,
   "order": 15,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "In this paper we present the study of glottalization in three languages: Mandarin Chinese, English and German. The motivation of this study originally comes from the selection of inventory speaker for Mandarin Chinese, and the design of phoneset for English and German synthesis. Because glottalization is characterized with a discontinuity in fundamental frequency, which can degrade the quality of synthesis during pitch manipulation of a concatenative synthesis, we thus investigated the common phenomenon of glottalization in these languages, with the focus on Mandarin Chinese. We illustrated the contexts where it often occurs and propose the implication to deal with glottalization in inventory building for concatenative speech synthesis. 1. INSTRUCTION 1.1. What is Glottalization Glottalization is often referred to as ”creaky voice”, ”glottal stops”, or ”laryngealization”, which is reported in many languages. We can ﬁnd the phonetic description in Ladefoged [1] ”Creaky voice is the term we will use for a mode of vibration of vocal folds in which the arytenoids cartilages are much closer together than in modal voice.” The glottalization is characterized by a signiﬁcant drop in the amplitude in the waveform, and more importantly, there is an abrupt change in the periodicity of the signal. We have observed this phenomenon both in tonal languages such as Mandarin Chinese and non-tonal languages such as English and German.\n"
   ]
  },
  "chen04_iscslp": {
   "authors": [
    [
     "Yiya",
     "Chen"
    ]
   ],
   "title": "Focus and Intonational Phrase Boundary in Standard Chinese",
   "original": "063",
   "page_count": 4,
   "order": 16,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "This paper reports results of an experiment investigating the relation of focus and prosodic boundary. We tested the hypothesis that focus in Standard Chinese introduces an intonational phrase (IP) boundary before a focused constituent by examining the durational adjustment of syllables in different prosodic positions (i.e. IP initial vs. IP medial) and focus conditions (i.e. focused vs. unfocused). Results show that under both focus conditions, IP initial onset was significantly longer than IP medial onset but little difference was observed in rhyme duration. Focus, however, tended to induce lengthening more consistently in rhyme than in onset in both prosodic positions. Furthermore, the magnitudes of lengthening on onset and rhyme tended to be comparable in terms of their percentage of lengthening. This suggests that the effect of prosodic position on segment duration is localized and restricted to onset while the effect of focus is relatively more global and spans over the whole focused constituent. We also found that an IP initial unfocused syllable differed significantly from an IP medial focused syllable in both onset and rhyme duration. We thus conclude that there is no durational evidence that focus inserts an IP boundary to the left edge of a focused constituent in Standard Chinese.\n"
   ]
  },
  "yuan04_iscslp": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ]
   ],
   "title": "Perception of Mandarin Intonation",
   "original": "042",
   "page_count": 4,
   "order": 17,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "This study investigates how tone and intonation, and how focus and intonation, interact in intonation type (statement vs. question) identification. A perception experiment was conducted on a speech corpus of 1040 utterances. Sixteen listeners participated in the experiment. The results reveal three asymmetries: statement and question intonation identification; effects of the sentence-final Tone2 and Tone4 on question intonation identification; and effects of the final focus on statement and question intonation identification. These asymmetries suggest that: 1. Statement intonation is a default or unmarked intonation type whereas question intonation is a marked intonation type. 2. Question intonation has a higher prosodic strength at the sentence final position. 3. There is a tone-dependent mechanism of question intonation at the sentence-final position.   1. INTRODUCTION  Chinese is a tonal language. There are four lexical tones in Mandarin Chinese, referred to as Tone1, Tone2, Tone3 and Tone4. The F0 contours of the tones in isolation are high level, rising, low dipping (or just low), and falling, respectively. Both tone and intonation use F0 as a primary cue. How, then, do tone and intonation interact with each other? For example, if a sentence ends with a falling tone, how can question intonation, which normally has a rising end in English and other non-tonal languages, be realized? Is it difficult for question intonation to be realized on such a sentence? On the other hand, for question intonation that has a rising tone at the end, how does a listener know that the utterance is a question? Or how does a listener tease apart the tone and intonation information from the surface rising end? By investigating the degrees of perception confusion when intonation types are realized on different tonal sequences in both focused and unfocused environments, we may find some clues as to how tone and intonation interact and how intonation is realized. This study follows this strategy.  A perception experiment was conducted on a speech corpus of 1040 utterances. Section 2 and Section 3 describe the corpus and the methods of the perception experiment respectively. The results are presented in Section 4 in two categories: intonation type identification and effects of focus on intonation type identification. Implications of the results are discussed in Section 5.  2. CORPUS  A corpus of 130 sentences was designed. The sentences, all of which contain eight syllables, are minimal sets contrasting Intonation type (Statement or Question), Presence of a focus (yes or no), Focus position (beginning, middle or end of a sentence), Tone of the focused syllable (Tone1, Tone2, Tone3 or Tone4) and Tone of the last syllable (Tone1, Tone2, Tone3 or Tone4). For example1:\n"
   ]
  },
  "lim04_iscslp": {
   "authors": [
    [
     "Boon Pang",
     "Lim"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Yu",
     "Chen"
    ]
   ],
   "title": "Language Identification Through Large Vocabulary Continuous Speech Recognition",
   "original": "082",
   "page_count": 4,
   "order": 18,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "In recent years, automatic language identiﬁcation has become an increasingly important component in practical spoken language systems, and much attention has been devoted to various competing approaches. In this paper, we are concerned with the automatic identiﬁcation of languages that may be highly similar in nature, such as the various dialects of Chinese. Our approach differs from many recent successful systems by exploiting a fusion of feature scores readily available from a large vocabulary speech recognition system. We show that such features are able to distinguish among the similar sounding dialects of Chinese, and experiments on a nine language corpus show promising performance on a three way identiﬁcation task.\n"
   ]
  },
  "wang04c_iscslp": {
   "authors": [
    [
     "Shizhen",
     "Wang"
    ],
    [
     "Jia",
     "Liu"
    ],
    [
     "Runsheng",
     "Liu"
    ]
   ],
   "title": "Language Identification using Discriminative Weighted Language Models",
   "original": "034",
   "page_count": 4,
   "order": 19,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "In this paper, discriminative weighted language models are proposed to better distinguish between similar languages. Through Parallel Phone Recognizers followed by Language Modeling (PPRLM) system in the first stage, two best candidates are hypothesized and then processed using discriminative language models. Experimental results show that, compared with the traditional one-pass language identification (LID) systems, the proposed two-pass method can greatly improve the performance without considerably increasing the computational costs. Tested on the evaluation set of CallFriend corpus, the final system achieved an error rate of 14.90% on the 30s 12-way close-set task.\n"
   ]
  },
  "ou04_iscslp": {
   "authors": [
    [
     "Guiwen",
     "Ou"
    ],
    [
     "Dengfeng",
     "Ke"
    ]
   ],
   "title": "Text-Independent Speaker Verification based on Relation of MFCC Components",
   "original": "055",
   "page_count": 4,
   "order": 20,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "TEXT-INDEPENDENT SPEAKER VERIFICATION BASED ON RELATION OF MFCC COMPONENTS  Guiwen Ou, Dengfeng Ke lnsogw@zsu.edu.cn is99kdf@tom.com  Department of Computer Science Zhongshan University, Guangzhou, 510275  ABSTPRACT  GMM is prevalent for speaker verification. It performs very well but needs a background model to give a reference value, which would greatly influence the error rate. In order to get better result of generalization, a large database with lots of people is needed to train the background model. In this paper, a new method without background model is proposed, which will be called Correlation and Kernel function method (CK method) later. In CK method, the correlation and un-correlation of MFCC are used to identify individuals, and a kernel function is used to work out the likelihood of two models. It works more than 30 times as fast as GMM method does, but requires fewer data to train and fewer space to store the model. But its performance is nearly identical to that of GMM. So it is suitable for real-time computation.  1. INTRODUCTION  Speaker verification and identification have several applications in security guard and e-Business. Speaker identification means to recognize a person out of a given group of people, while speaker verification means to verify whether the speech is spoken through the given person or not. The later one can also be separated into text-dependent one and text-independent one. In this paper, a method for text-independent speaker verification is proposed. GMM [1,2] has been being the most classical method for text-independent speaker verification. Reynolds etc. introduced GMM to speaker verification in [1,2], which needs an extra model called background GMM. The background GMM is trained from a large database of different people. The speeches of this database should be carefully selected from different people in order to get better results. In practice, the selection of speeches for training the background model becomes a hard problem. So Hsu etc. [3] proposed a method to keep from such a pet hate. But their method is too demanding to ask for about 3.5 minutes’ speech each to train 20 GMMs for every speaker and then get the thresholds for every one. Zilca [9] said that GMM mainly uses two statistics － mean and covariance. Through some particular processing, mean would be zero, and then GMM can be simplified to covariance only. So F. Bimbot [4] and R.D.Zilca [5,9] proposed a method for speaker verification called Covariance Modeling, which ignores mean statistics to simplify GMM. This method needs a background model as well. Speaker verification system with a background model has several inconveniences. In order to add a given person to the database, not only the model of the given person should be added but also the background model should be retrained accordingly. Text-independent speaker verification based on GMM supposed that LPCC or MFCC satisfy Gaussian distribution and that LPCC or MFCC of each frame contain the characteristic of the speaker, So GMM works out probability densities for every frame and then sums up the likelihood to give determination. Our point of view is that LPCC or MFCC of a single frame contains little characteristic of a person. It is the relation between frequency bands that contains the chief characteristic of a person. So in this paper, correlation matrix is used to represent the correlation and kurtosis vector is used to represent the un-correlation of frequency bands.  In fact, Gaussian distribution is only an approximate description of speech feature. There are several assumptions for speech feature. For example, Davenport [6] assumed that the speech feature is of Laplace distribution, which is given as follows: x x x x e x f δ δ | |2 * 2 1 ) ( − =        (1) Paez and Glisson [7] said that gamma distribution is better, which is shown bellow: 0-7803-8678-7/04/$20.00 ©2004 IEEE 57 ISCSLP 2004\n"
   ]
  },
  "leung04_iscslp": {
   "authors": [
    [
     "KaYee",
     "Leung"
    ],
    [
     "ManWai",
     "Mak"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "SunYuan",
     "Kung"
    ]
   ],
   "title": "Adaptive Conditional Pronunciation Modeling using Articulatory Features for Speaker Verification",
   "original": "074",
   "page_count": 4,
   "order": 21,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "This paper proposes an articulatory feature-based conditional pronunciation modeling (AFCPM) technique for speaker veriﬁcation. The technique models the pronunciation behaviors of speakers by creating a link between the actual phones produced by the speakers and the state of articulations during speech production. Speaker models consisting of conditional probabilities of two articulatory classes are adapted from a set of universal background models (UBMs) using MAP adaptation technique. This adaptation approach aims to prevent over-ﬁtting the speaker models when the amount of speaker data is insufﬁcient for a direct estimation. Experimental results show that the adaptation technique can enhance the discriminating power of speaker models by establishing a tighter coupling between speaker models and the UBMs. Results also show that fusing the scores derived from an AFCPM-based system and a conventional spectral-based system achieves a signiﬁcantly lower error rate than that of the individual systems. This suggests that AFCPM and spectral features are complementary to each other.\n"
   ]
  },
  "yang04_iscslp": {
   "authors": [
    [
     "JyhHer",
     "Yang"
    ],
    [
     "YuanFu",
     "Liao"
    ]
   ],
   "title": "Unseen Handset Mismatch Compensation based on Feature/Model-Speech A Priori Knowledge Interpolation for Robust Speaker Recognition",
   "original": "096",
   "page_count": 4,
   "order": 22,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "Unseen but mismatch handset is the major source of performance degradation for speaker recognition in telecommunication environment. In this paper, an unseen handset characteristics estimation method based on a priori knowledge interpolation (AKI) is proposed. AKI could be applied in both the feature and model space to interpolate the feature and model transformation functions measured using stochastic matching (SM) and maximum likelihood linear regression (MLLR), respectively. Cross-validation experimental results on HTIMIT database showed that the average speaker recognition rate could be improved from 59.6%/57.8% to 73.8%/66.8% for seen/unseen handsets. It is therefore a promising method for robust speaker recognition.\n"
   ]
  },
  "bai04_iscslp": {
   "authors": [
    [
     "Junmei",
     "Bai"
    ],
    [
     "Rong",
     "Zheng"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Shuwu",
     "Zhang"
    ]
   ],
   "title": "Robust Speaker Recognition Integrating Pitch and Wiener Filter",
   "original": "037",
   "page_count": 4,
   "order": 23,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "Speaker recognition (SR) has got excellent result in clean speech. But the noises or channel mismatch will cause significant performance degradation in practical appliance. The paper focuses on resolving those problems about robust and efficient speaker identification (SI) in noise environment. And it mainly contributes in two areas: signal processing based on Wiener filtering and speaker features integration of pitch and MFCC. It shows in the experimental results on YOHO corpus that Wiener filter is an efficient front-end processing technique and pitch is a robust feature for SR in noise environments.\n"
   ]
  },
  "ling04_iscslp": {
   "authors": [
    [
     "Zhenhua",
     "Ling"
    ],
    [
     "Yuping",
     "Wang"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "Modeling Glottal Effect on the Spectral Envelop of STRAIGHT using Mixture of Gaussians",
   "original": "044",
   "page_count": 4,
   "order": 24,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "This paper presents a method to model the influence of glottal excitation on STRAIGHT spectrum by fitting the spectral envelop with mixture of Gaussians (MOG). The first Gaussian component is used as estimation to glottal formant in STRAIGHT spectrum because analysis result shows that it has an obviously stronger correlation with fundamental frequency than other spectral components and has similar characteristics with glottal formant. Then  linear regression is carried out to measure the relationship between F0 and the parameters of the first Gaussian component. This model is applied to STRAIGHT synthesis process and proved to be effective in compensating the voice quality variation caused by pitch modification.  1. INTRODUCTION  STRAIGHT (Speech Transformation and Representation using Adaptive Interpolation of weiGHTed spectrum) [1],  as a high-quality VOCODER-type analysis-synthesis method, has been presented in recent years. This analysis process is based on fundamental frequency extraction using TEMPO (Time-Domain Excitation extractor using Minimum Perturbation Operator)[2] and pitch adaptive spectral smoothing in both time and frequency domains. The synthesis process is implemented by passing a serial of impulse excitations with pitch period intervals through a time varying filter which is calculated from the smoothed spectral envelop. By manipulating the pulse positions in excitation, flexible prosody modification is realized [1].  In the framework of STRAIGHT analysis and synthesis, the excitation consists of only pitch information. Therefore the smoothed spectral envelop is the integration of both spectral presentation of glottal waveform and vocal tract transfer function according to general speech production hypothesis. Here a method for measuring the effect of glottal excitation on STRAIGHT spectral envelop is presented for the following two purposes, while the work introduced in this paper focuses on the first one: 1. It has been proved that some spectral characteristics of glottal waveform are dependent on not only the parameters that define phonation type or voice quality, such as OQ, SQ, but also the fundamental frequency of glottal source [3]. By modeling the effect of F0 on STRAIGHT spectrum, the pitch modification during STRAIGHT synthesis can be improved. 2. By decomposing STRAIGHT spectrum into sourcedependent components and source-independent components, we can provide an alternative for voice quality modification under STRAIGHT framework. The mixture of Gaussians (MOG) model [4][5] is a speech spectral modeling method. Compared with linear predictive or cepstral coefficients, its parameters have more obvious physics meanings in fitting spectral peaks and are more independent from each other. So it is introduced here to estimate the glottal formant in STRAIGHT spectrum. In the following part of this paper, an introduction to the method is presented in section 2. Section 3 gives experiment results and related analysis. Section 4 and 5 are discussions and conclusions.  2. METHOD  2.1. The spectral representation of glottal waveform  The spectral characteristics of glottal waveform are studied based on LF model [6], which describes the shape of the differentiated glottal airflow using the following five parameters: T0, EE, RA, RG, RK. The open quotient of glottal source is related to both RG and RK: OQ = (1+RK)/(2RG). As mentioned in [3], the spectrum of LF model has two main characteristics:\n"
   ]
  },
  "qi04_iscslp": {
   "authors": [
    [
     "Fengyan",
     "Qi"
    ],
    [
     "Changchun",
     "Bao"
    ],
    [
     "Yan",
     "Liu"
    ]
   ],
   "title": "A Novel Two-Step SVM Classifier for Voiced/Unvoiced/Silence Classification of Speech",
   "original": "013",
   "page_count": 4,
   "order": 25,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In this paper•, a novel method to voiced/unvoiced/silence of speech classification using Support Vector Machine (SVM) is proposed. This classifier can correctly classify speech frames into voiced frame, unvoiced frame and silence frame. The comparison of experiment result shows that the proposed method outperforms other traditional methods. The performance of SVM for different kernel functions in the experiment was analyzed and discussed as well.\n"
   ]
  },
  "gu04_iscslp": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis of Shanghainese F0 Contours based on the Command-Response Model",
   "original": "105",
   "page_count": 4,
   "order": 26,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "As one of the major Chinese dialects, Shanghainese is well known for its complex tone sandhi system. This paper applies the command-response model to represent F0 contours of Shanghainese speech. Analysis-by-Synthesis is conducted both on carrier sentences with monosyllabic target words and on isolated polysyllabic words, from which a set of appropriate tone command patterns is derived for words of different lengths and different initial citation tones. By incorporating the effects of tone coarticulation, word accentuation and phrase intonation, the model gives high accuracy of approximations to F0 contours of Shanghainese utterances, and hence provides a more efficient means to quantitatively represent F0 contours and to describe the tone sandhi system of Shanghainese than the traditional 5-level tone code system.\n"
   ]
  },
  "zhu04_iscslp": {
   "authors": [
    [
     "Weibin",
     "Zhu"
    ],
    [
     "Wei",
     "Zhang"
    ],
    [
     "Qin",
     "Shi"
    ],
    [
     "Xijun",
     "Ma"
    ],
    [
     "Liqin",
     "Shen"
    ]
   ],
   "title": "Automatic Detection of Chinese Accent-Index based on Approximation-Ratio",
   "original": "036",
   "page_count": 4,
   "order": 27,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "\u0001\u0002\u0003\u0004\u0005\u0006\u0003\u0007\b\t \u000b\u0003\u000b\b\u0003\u0007\u0004\f\t\u0004\n\t\u000e\u000f\u0007\f\u000b\u0010\u000b\t\u0001\b\b\u000b\f\u0003\u0011\u0012\f\u0013\u000b\u0014\t\u0015\u0006\u0010\u000b\u0013\t\u0004\f\t \u0001\u0016\u0016\u0017\u0004\u0014\u0007\u0005\u0006\u0003\u0007\u0004\f\u0011\u0018\u0006\u0003\u0007\u0004\t \u0001\u0002\u0003\u0004\u0003\u0005\u0006\u0007\b\t \u0006\u0001\u0002\u0003\u0006\u0007\b\u000b\u0005\f \u0006\n\u0003\u0005\u0006\u000e\b\u0003 \u0006\u000f\u0003\u0010\t\u0005\u0006\u0011\u000b \u0006\u000b\u0005\u0012\u0006\u0013\u0003\u0014\u0003\u0005\u0006\u000e\b\u0002\u0005\u0006\u0006 \u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\t\u0004 \u000b\f\u000b\t\n\u000e\u0006\u0004\u000f\t\u0010\u0011\u0004\u0002\u000b\u0007\u0012\u0007\b\u0013\u0004\u0014\u0015\u0015\u0015\u0016\u0017\u0011\u0004\u0005\u0006\u0007\b\t\u0001\u0001 \u0002\u0003\u0004\u0005\u0006\u0007\b\t \u0001\u0003\u0004\u000b\f\n\u0003\u0006 \u0001\u000e\u0004\b\u000f\b\f \u0001\u0010\u000b\u0011\b\u0012\u0005\f \u0001\u000e\u0004\u0007\f\u0013\u000f\u0014\u0015\u0016\f\u0017\b\t\u0010\u0017\u0016\u0018\u0010\u0001 \u0001 \u0001\u0019\u0010\u0003\u0017\u0006\b\u0003\t \u0018\u0019\n\u0004\t\u0004\u001a\u001a\u001b\u0004\f\u001c\f\u001d\u000b\u001e\u0011\u0004\u001d\u0019\u0004\f\u001c\b\u001d\u0006\u000b\f\u0007\u001f\u000b\u0004\f \u000b\u000b\u000e\u0006\u0004!\u0007\u001d\u0006\u0004\u0010\u000b\u001d\u001d\u000b\n\u0004 \n\u0019\f\u0019\"\u001c\u0011\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \u0007\b#\u0019\n\u001e\t\u001d\u0007\u0019\b\u0004 \u0007\f\u0004 \u000b$ \u000b\u000e\u001d\u000b\"\u0004 \u001d\u0019\u0004 \u0010\u000b\u0004 \u0007\b%\u0019&%\u000b\"\u0004 \u0010\u000b\f\u0007\"\u000b\f\u0004 \u0019\u001d\u0006\u000b\n'\u0004 \u001a\u0006\u000b\n\u000b#\u0019\n\u000b\u0011\u0004!\u000b\u0004\"\u000b#\u0007\b\u000b\"\u0004\t\u0004\f\u000b\u001d\u0004\u0019#\u0004(\u000e\u000e\u000b\b\u001d\u0004\u0001\b\"\u000b$\u0004)(\u0001*\u0004\u001d\u0019\u0004\n",
    "\u000b\f\u000b\b\u001d\u0004 \u001d\u0006\u000b\u0004 %\t\n\u0007\t\b\u000e\u000b\f\u0004 \u0019#\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \u0007\b\u0004 \u0005\u0006\u0007\b\u000b\f\u000b\u0004 \f \u000b\u000b\u000e\u0006\u0011\u0004 \t\b\"\u0004 \n\u0019 \u0019\f\u000b\"\u0004 \t\u0004 \b\u0019%\u000b&\u0004\u001e\u000b\u001d\u0006\u0019\"\u0004\u001d\u0019\u0004\t+\u001d\u0019\u001e\t\u001d\u0007\u000e\t&&\u001c\u0004\t\b\b\u0019\u001d\t\u001d\u000b\u0004\u0005\u0006\u0007\b\u000b\f\u000b\u0004\f \u000b\u000b\u000e\u0006\u0004 !\u0007\u001d\u0006\u0004 (\u0001'\u0004\u0001\b\u0004\u001d\u0006\u000b\u0004\u001e\u000b\u001d\u0006\u0019\"\u0011\u0004\t\u0004 \t\n",
    "\u0011\u0004\b\t\u001e\u000b\"\u0004( \n\u0019$\u0007\u001e\t\u001d\u0007\u0019\b, \t\u001d\u0007\u0019\u0011\u0004 !\t\f\u0004+\f\u000b\"\u0004\u001d\u0019\u0004\b+\u001e\u000b\n\u0007\u000e\t&&\u001c\u0004\u0007\b\"\u0007\u000e\t\u001d\u000b\u0004\u001d\u0006\u000b\u0004\t\u000e\u000e\u000b\b\u001d\u0004\u0019#\u0004 \n\u0019\f\u0019\"\u0007\u000e\u0004+\b\u0007\u001d'\u0004 (\b\"\u0004\u001d\u0006\u000b\u0004%\t&+\u000b\u0004\u0019#\u0004(\u0001\u0004!\t\f\u0004\u001d\u0006\u000b\u0004\"\u0007\f\u000e\n\u000b\u001d\u0007\u001f\t\u001d\u0007\u0019\b\u0004\u0019#\u0004( \n\u0019$\u0007\u001e\t\u001d\u0007\u0019\b, \t\u001d\u0007\u0019'\u0004-\b\u000b\u0004\u000e\u0019\n +\f\u0004!\t\f\u0004\t\b\b\u0019\u001d\t\u001d\u000b\"\u0004!\u0007\u001d\u0006\u0004(\u0001\u0004\u0010\u001c\u0004\u001d\u0006\u000b\u0004\u001e\u000b\u001d\u0006\u0019\"'\u0004(\b\"\u0004 !\u0007\u001d\u0006\u0004\u001d\u0006\u000b\u0004\u000e\u0019\n +\f\u0011\u0004\t\u0004\n\u000b#\u0007\b\u000b\"\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\u0004 \n\u000b\"\u0007\u000e\u001d\u0007\u0019\b\u0004\u001e\u0019\"\u000b&\u0004 !\t\f\u0004 \u0010+\u0007&\u001d'\u0004 \u001a\u0006\u000b\u0004 \u000b$ \u000b\n\u0007\u001e\u000b\b\u001d\u0004 \n\u000b\f+&\u001d\f\u0004 \f\u0006\u0019!\u000b\"\u0004 \u001d\u0006\t\u001d\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\f\u0004 \n\u000b\"\u0007\u000e\u001d\u000b\"\u0004\u0010\u001c\u0004\u001d\u0006\u000b\u0004\n\u000b#\u0007\b\u000b\"\u0004\u001e\u0019\"\u000b&\u0004!\u000b\n\u000b\u0004\u001e\u0019\n\u000b\u0004\u000e&\u0019\f\u000b\u0004\u001d\u0019\u0004 \u0019\b\u000b\f\u0004 \u0019#\u0004 \n\u000b\t&\u0004 \f \u000b\u000b\u000e\u0006\u0004 \u001d\u0006\t\b\u0004 \u0010\u001c\u0004 \u001d\u0006\u000b\u0004 #\u0019\n",
    "\u0004 \u001e\u0019\"\u000b&\u0004 !\u0007\u001d\u0006\u0019+\u001d\u0004 (\u0001'\u0004 \u0018+\n\u001d\u0006\u000b\n\u0011\u0004 \t\u0004 \u000b\n\u000e\u000b \u001d+\t&\u0004 \u000b%\t&+\t\u001d\u0007\u0019\b\u0004 \f\u0006\u0019!\u000b\"\u0004 \u001d\u0006\t\u001d\u0004 \u001d\u0006\u000b\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \u001e\t\b\u0007#\u000b\f\u001d\t\u001d\u0007\u0019\b\u0004 \u0013\u000b\b\u000b\n\t\u001d\u000b\"\u0004 \u0010\u001c\u0004 (\u0001,\n\u000b\t\"\u001c\u0004 \f\u001c\b\u001d\u0006\u000b\f\u0007\u001f\u000b\n\u0004 !\t\f\u0004 \"\u0007\f\u001d\u0007\b\u0013+\u0007\f\u0006\t\u0010&\u000b\u0004\t\b\"\u0004\t\u000e\u000e\u000b \u001d\t\u0010&\u000b'\u0004 \u001a\u001b\u0001 \u0012\f\u0003\u0017\u0004\u0013\u0002\b\u0003\u0007\u0004\f\t \u001a\u0006\u000b\n\u000b.\n\u000b\u0004 \u001d!\u0019\u0004 \u001e\t\u0007\b\u0004 \u000e\u0019\u001e \u0019\b\u000b\b\u001d\f\u0004 \u0007\b\u0004 \t\u0004 \u001d\u001c \u0007\u000e\t&\u0004 \u001d\n\t\u0007\b\t\u0010&\u000b\u0004 \u001a\u001a\u001b\u0004 \f\u001c\f\u001d\u000b\u001e'\u0004 \u0018\n\u0019\b\u001d,\u000b\b\"\u0004 /\u00140\u0004 \u0007\f\u0011\u0004 \u000e\u0019\n",
    "\u000b\f \u0019\b\"\u0007\b\u0013\u0004 \u001d\u0019\u0004 \u001d\u0006\u000b\u0004 \u0007\b +\u001d\u0004 \u001d\u000b$\u001d\u0011\u0004 \u001d\u0019\u0004 \u0013\u000b\b\u000b\n\t\u001d\u000b\u0004\u001d\u0006\u000b\u0004\f\u001c\u001e\u0010\u0019&\u0007\u000e\u0004\"\u000b\f\u000e\n\u0007 \u001d\u0007\u0019\b\u0004\t\u0010\u0019+\u001d\u0004 \f \u000b\u000b\u000e\u0006\u0011\u0004 \u0007'\u000b'\u0011\u0004!\u0006\t\u001d\u0004 \u001d\u0019\u0004 \u0010\u000b\u0004+\u001d\u001d\u000b\n\u000b\"'\u0004\u0002\t\u000e1,\u000b\b\"\u0004/20\u0004\u0007\f\u0011\u0004\u0010\t\f\u000b\"\u0004\u0019\b\u0004\u001d\u0006\u000b\u0004\f\u001c\u001e\u0010\u0019&\u0007\u000e\u0004\"\u000b\f\u000e\n\u0007 \u001d\u0007\u0019\b\u0011\u0004 \u001d\u0019\u0004 \u0013\u000b\b\u000b\n\t\u001d\u000b\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004 \u001d\t\n\u0013\u000b\u001d\f\u0011\u0004 \u0007'\u000b'\u0011\u0004 \u0006\u0019!\u0004 \u001d\u0019\u0004 \u0010\u000b\u0004 +\u001d\u001d\u000b\n\u000b\"'\u0004 \u0002\u000b\u0007\b\u0013\u0004 \u001e\u000b\t\f+\n\u000b\"\u0004\u0010\u001c\u0004 \n\u000b\"\u0007\u000e\u001d\u000b\"\u0004\u001d\t\n\u0013\u000b\u001d\f\u0011\u0004\u001d\u0006\u000b\u0004\u0010\u000b\f\u001d\u0004\f\u000b3+\u000b\b\u000e\u000b\u0004\u0019#\u0004+\b\u0007\u001d\f\u0004\u000e\u0019+&\"\u0004 \u0010\u000b\u0004\f\u000b&\u000b\u000e\u001d\u000b\"\u0004#\n\u0019\u001e\u0004\t\u0004&\t\n\u0013\u000b\u0004\f \u000b\u000b\u000e\u0006\u0004\"\t\u001d\t\u0010\t\f\u000b\u0004/40\u0011\u0004\t\b\"\u0004\u001d\u0006\u0019\f\u000b\u0004+\b\u0007\u001d\f\u0004 \t\n\u000b\u0004\u000e\u0019\b\u000e\t\u001d\u000b\b\t\u001d\u000b\"\u0004\u001d\u0019\u0004#\u0019\n\u001e\u0004\u001d\u0006\u000b\u0004\t \n\u0019 \n\u0007\t\u001d\u000b\u0004\f \u000b\u000b\u000e\u0006'\u00045\u0006\t\u001d\u0004\u0010\u000b\u0007\b\u0013\u0004 \n\u000b&\t\u001d\u000b\"\u0004!\u0007\u001d\u0006\u0004 \n\u0019\f\u0019\"\u001c\u0004\u0007\b\u0004\u0010\u0019\u001d\u0006\u0004\u000e\u0019\u001e \u0019\b\u000b\b\u001d\f\u0004\t\n\u000b\u0011\u0004\u0007\b\u0004#\n\u0019\b\u001d,\u000b\b\"\u0011\u0004\u001d\u0019\u0004 \n\u000b\"\u0007\u000e\u001d\u0004\u001d\u0006\u000b\u0004 \n\u0019\f\u0019\"\u0007\u000e\u0004\u000b%\u000b\b\u001d\f\u0004\u0010\t\f\u000b\"\u0004\u0019\b\u0004\u001d\u0006\u000b\u0004\u001d\u000b$\u001d\u0004\t\b\t&\u001c\f\u0007\f\u0011\u0004!\u0006\u0007&\u000b\u0004\u0007\b\u0004 \u0010\t\u000e1,\u000b\b\"\u0011\u0004\u001d\u0019\u0004\u0013\u000b\b\u000b\n\t\u001d\u000b\u0004\u001d\u0006\u000b\u0004 \n\u0019\f\u0019\"\u0007\u000e\u0004\t\u000e\u0019+\f\u001d\u0007\u000e\f\u0004 \t\n",
    "\f\u0004\t\f\u0004\u001d\u0006\u000b\u0004 \u001d\t\n\u0013\u000b\u001d\f\u0004#\u0019\n\u0004+\b\u0007\u001d,\f\u000b&\u000b\u000e\u001d\u0007\b\u0013'\u0004 \u0001\b\u0004 \f+\u000e\u0006\u0004 \t\u0004 \f\u001c\f\u001d\u000b\u001e\u0011\u0004 \u0010\u0019\u001d\u0006\u0004 \n\u0019\f\u0019\"\u001c\u0004 \u000b%\u000b\b\u001d\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\u0004 \t\b\"\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\u0004\t\n\u000b\u0004\u001d\n\t\u0007\b\u000b\"\u0004\u0010\u001c\u0004\u000e\u0019\n \u0019\n\t\u0004\t\b\b\u0019\u001d\t\u001d\u000b\"\u0004 !\u0007\u001d\u0006\u0004 \n\u0019\f\u0019\"\u001c\u0004 \"\u000b\f\u000e\n\u0007 \u001d\u0007\u0019\b\u0004 \f\u001c\u001e\u0010\u0019&\f'\u0004 \u001a\u0006\u0019\f\u000b\u0004 \t\b\b\u0019\u001d\t\u001d\u0007\u0019\b\f\u0004 \u001e\t\u001c\u0010\u000b\u0004 \u0007\b\u000e&+\"\u000b\u0004 \n\u0019\f\u0019\"\u0007\u000e\u0004 \f\u001d\n+\u000e\u001d+\n\u000b\u0011\u0004 \u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\t&\u0004 \u001e\u0019\"\t&\u0007\u001d\u001c\u0011\u0004 \t\b\"\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \t\b\"6\u0019\n\u0004 \f\u001d\n\u000b\f\f'\u0004 5\u0007\u001d\u0006\u0019+\u001d\u0004 \u001d\u0006\u000b\u0004 \u000e\u0019\b\f\u001d\n\t\u0007\b\u001d\u0004 \u0019#\u0004 \u001d\u0006\u000b\u0004 \t\u000e\u000e\u000b\b\u001d\u0011\u0004 \u0019\b&\u001c\u0004 \u001e\u0019\b\u0019\u001d\u0019\b\u0007\u000e\u0004 \u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\u0004 \u000e\u0019+&\"\u0004 \u0010\u000b\u0004 \u0013\u000b\b\u000b\n\t\u001d\u000b\"\u0004 \u0010\u001c\u0004 \f+\u000e\u0006\u0004 \t\u0004 \f\u001c\f\u001d\u000b\u001e\u0004 \u000e\u0019\b\f\u000b3+\u000b\b\u001d&\u001c\u0004/20'\u0004\u001a\u0019\u0004\f\u001c\b\u001d\u0006\u000b\f\u0007\u001f\u000b\u0004\f \u000b\u000b\u000e\u0006\u0004!\u0007\u001d\u0006\u0004\u001e\u0019\n\u000b\u0004\b\t\u001d+\n\t&\u0004\t\b\"\u0004 \u000b$ \n\u000b\f\f\u0007%\u000b\u0004 \u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\u0011\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \u0007\b#\u0019\n\u001e\t\u001d\u0007\u0019\b\u0004 \f\u0006\u0019+&\"\u0004 \u0010\u000b\u0004 \t\"\u0019 \u001d\u000b\"\u0004 \u0007\b\u001d\u0019\u0004\t\u0010\u0019%\u000b\u0004\u001d!\u0019\u0004 \n\u0019\f\u0019\"\u001c\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\f'\u0004\u001a\u0006\u000b\n\u000b#\u0019\n\u000b\u0011\u0004\u000e\u0019\n \u0019\n\t\u0004&\t\u0010\u000b&\u000b\"\u0004 !\u0007\u001d\u0006\u0004(\u0001\u0004\t\n\u000b\u0004\b\u000b\u000b\"\u000b\"'\u0004 \u0001\b\u0004 \n\u0007\b\u000e\u0007 &\u000b\u0011\u0004\t\u000e\u000e\u000b\b\u001d\u0004\t\b\b\u0019\u001d\t\u001d\u0007\u0019\b\u0004\u000e\u0019+&\"\u0004\u0010\u000b\u0004\u0007\u001e &\u000b\u001e\u000b\b\u001d\u000b\"\u0004\u0010\u001c\u0004 \f+\u0010\u0012\u000b\u000e\u001d\f'\u0004 \u0002+\u001d\u0004 \u001d\u0019\u0004 \n\u0019\u000e\u000b\f\f\u0004 \t\u0004 &\t\n\u0013\u000b\u0004 \f\u0007\u001f\u000b\u0004 \u0019#\u0004 \u000e\u0019\n +\f\u0004 \u001e\t\b+\t&&\u001c\u0011\u0004 \u0006+\u001e\t\b,\u000e\u0019\b\f+\u001e\u0007\b\u0013\u0004 \u0007\f\u0004 \t\u0004 \u000e\n\u0007\u001d\u0007\u000e\t&\u0004 \n\u0019\u0010&\u000b\u001e'\u0004 -+\n\u0004 \u000b$ \u000b\n\u0007\u000b\b\u000e\u000b\u0004 \u0007\b\u0004 \u001e\t\b+\t&\u0004 \t\b\b\u0019\u001d\t\u001d\u0007\u0019\b\u0004 \f\u0006\u0019!\f\u0004 \u001d\u0006\t\u001d\u0004 \u001d\u0006\u000b\u0004 \u001d\u0007\u001e\u000b,\u000e\u0019\b\f+\u001e\u0007\b\u0013\u0004 \u0019#\u0004 \u0002\u0001\u0004 )\u0002\n\u000b\t1,\u0001\b\"\u000b$\u0011\u0004\u0007'\u000b'\u0011\u0004 \n\u0019\f\u0019\"\u001c\u0004\f\u001d\n+\u000e\u001d+\n\u000b*\u0004&\t\u0010\u000b&\u0007\b\u0013\u0004\u0007\f\u0004\t\u0010\u0019+\u001d\u0004\u0014\u001572\u0015\u0004 \u001d\u0007\u001e\u000b\f\u0004\u0019#\u0004\f \u000b\u000b\u000e\u0006\u0004+\u001d\u001d\u000b\n\u0007\b\u0013\u0011\u0004\t\b\"\u0004\u000e\u0019\b\u001d\n\t\f\u001d\u0007\b\u0013&\u001c\u0004\u001d\u0006\u000b\u0004\u001d\u0007\u001e\u000b,\u000e\u0019\b\f+\u001e\u0007\b\u0013\u0004 \u0019#\u0004 (\u0001\u0004 &\t\u0010\u000b&\u0007\b\u0013\u0004 \u0007\f\u0004 \t\u0010\u0019+\u001d\u0004 8\u001579\u0015\u0004 \u001d\u0007\u001e\u000b\f'\u0004 \u0018\u0019\n\u0004 \u001e\t\b+\t&\u0004 &\t\u0010\u000b&\u0007\b\u0013\u0011\u0004 \t\b\u0019\u001d\u0006\u000b\n\u0004 \n\u0019\u0010&\u000b\u001e\u0004\u0007\f\u0004\u0006\u0019!\u0004\u001d\u0019\u00041\u000b\u000b \u0004\u001d\u0006\u000b\u0004\u000e\u0019\b\f\u0007\f\u001d\u000b\b\u000e\u000b\u0004\t\u001e\u0019\b\u0013\u0004\u001d\u0006\u0019\f\u000b\u0004 \f+\u0010\u0012\u000b\u000e\u001d\f\u0011\u0004 \u0007#\u0004 \u001e\u0019\n\u000b\u0004 \u000b\u0019 &\u000b\u0004 \u0006\t%\u000b\u0004 \u001d\u0019\u0004 \u0010\u000b\u0004 \u0007\b%\u0019&%\u000b\"\u0004 \u001d\u0019\u0004 &\t\u0010\u000b&\u0004 &\t\n\u0013\u000b\u0004 \u000e\u0019\n +\f\u0004\t\b\b\u0019\u001d\t\u001d\u0007\b\u0013'\u0004\u001b+\u0010\f\u001d\u0007\u001d+\u001d\u0007%\u000b&\u001c\u0011\u0004!\u000b\u0004\u0006\t%\u000b\u0004\u001d\u0019\u0004#\u0007\b\"\u0004\t\u0004\u001e\u000b\u001d\u0006\u0019\"\u0004\u001d\u0019\u0004 \t+\u001d\u0019\u001e\t\u001d\u0007\u000e\t&&\u001c\u0004\t\b\b\u0019\u001d\t\u001d\u000b\u0004\u001d\u0006\u000b\u0004\t\u000e\u000e\u000b\b\u001d'\u0004 :\t\n\u0007\u0019+\f&\u001c\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004 \u001e\t\b\u0007#\u000b\f\u001d\t\u001d\u0007\u0019\b\u0004 \u0019#\u0004 \u0005\u0006\u0007\b\u000b\f\u000b\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 \u0006\t\f\u0004 \u0010\u000b\u000b\b\u0004\u0007\b%\u000b\f\u001d\u0007\u0013\t\u001d\u000b\"\u0004/80\u0004/\u00170'\u0004-+\n\u0004\u000b##\u0019\n\u001d\u0004#\u0019\u000e+\f\u000b\f\u0004\u0019\b\u0004\u001d\u0006\u000b\u0004&\u000b%\u000b\n\t\u0013\u000b\u0004\u0019#\u0004 \u001d\u0006\u0019\f\u000b\u0004\f+\n#\t\u000e\u000b\u0004#\u000b\t\u001d+\n\u000b\f\u0004&\u001c\u0007\b\u0013\u0004\u0007\b\u0004\f \u000b\u000b\u000e\u0006\u0004\f\u0007\u0013\b\t&'\u0004\u0001\u001d\u0004\u0007\f\u0004\u0010\t\f\u000b\"\u0004\u0019\b\u0004\u001d\u0006\u000b\u0004 #\u0019&&\u0019!\u0007\b\u0013\u0004\t\f\f+\u001e \u001d\u0007\u0019\b\f;\u0004\t*\u0011\u0004\u001d\u0006\u0019\f\u000b\u0004 \n\u0019\f\u0019\"\u0007\u000e\t&&\u001c\u0004\t\u000e\u0019+\f\u001d\u0007\u000e\u0004#\u000b\t\u001d+\n\u000b\f\u0004 \t\"\u000b3+\t\u001d\u000b&\u001c\u0004 \n\u000b#&\u000b\u000e\u001d\u0004 %\t\n\u0007\u0019+\f\u0004 \n\u0019\f\u0019\"\u0007\u000e\u0004 \u000b%\u000b\b\u001d\f\u0004 \u0007\b\u000e&+\"\u0007\b\u0013\u0004 \n\u0019\f\u0019\"\u001c\u0004 \f\u001d\n+\u000e\u001d+\n\u000b\u0011\u0004 \u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\t&\u0004 \u001e\u0019\"\t&\u0007\u001d\u001c\u0011\u0004 \t\b\"\u0004 \t\u000e\u000e\u000b\b\u001d'\u0004 \u0010*\u0011\u0004 \u0007#\u0004 \u0007\u001d\u0004 \u0007\f\u0004 \u0019\b&\u001c\u0004 \u000e\u0019\b\f\u001d\n\t\u0007\b\u000b\"\u0004!\u0007\u001d\u0006\u0004 \n\u0019\f\u0019\"\u001c\u0004\f\u001d\n+\u000e\u001d+\n\u000b\u0004\t\b\"\u0004\u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\t&\u0004\u001e\u0019\"\t&\u0007\u001d\u001c\u0011\u0004 \t\b\"\u0004\u000b\t\u000e\u0006\u0004+\b\u0007\u001d\u0004\u0007\f\u0004!\u0007\u001d\u0006\u0004\u001d\u0006\u000b\u0004\f\t\u001e\u000b\u0004(\u0001\u0011\u0004\u001d\u0006\u000b\u0004+\u001d\u001d\u000b\n\t\b\u000e\u000b\u0004\u000e\u0019+&\"\u0004\u0019\b&\u001c\u0004\u0010\u000b\u0004 \t\u0004\b\u000b+\u001d\n\t&\u0004 \u001d\u001c \u000b\u0004 \u0019#\u0004 \f \u000b\u000b\u000e\u0006\u0011\u0004 \u0007'\u000b'\u0011\u0004 !\u0007\u001d\u0006\u0004 \b\u000b+\u001d\n\t&\u0004 \n\u0019\f\u0019\"\u001c'\u0004 \u000e*\u0011\u0004#\u0019\n\u0004\t\u0004 \f\t\u001e\u000b\u0004 +\u001d\u001d\u000b\n\t\b\u000e\u000b.\f\u0004 \f\u000e\n\u0007 \u001d\u0011\u0004 \u001d\u0006\u0019\f\u000b\u0004 \f+\n#\t\u000e\u000b\u0004 #\u000b\t\u001d+\n\u000b\f.\u0004 \"\u0007##\u000b\n\u000b\b\u000e\u000b\u0004 \u0010\u000b\u001d!\u000b\u000b\b\u0004\u001d\u0006\u000b\u0004%\u0007%\u0007\"\u0004\f \u000b\u000b\u000e\u0006\u0004\t\b\"\u0004\u001d\u0006\u000b\u0004\b\u000b+\u001d\n\t&\u0004\u0019\b\u000b\u0004\f\u0006\u0019+&\"\u0004\u0010\u000b\u0004\u001e\t\u0007\b&\u001c\u0004 \u000e\t+\f\u000b\"\u0004 \u0010\u001c\u0004 \t\u000e\u000e\u000b\b\u001d\u0004 %\t\n\u001c\u0007\b\u0013\u0011\u0004 \u0019\n\u0004 \t\n\u001d&\u001c\u0004 \t\u001d\u0004 &\u000b\t\f\u001d'\u0004 \u0002\t\f\u000b\"\u0004 \u0019\b\u0004 \u001d\u0006\u0019\f\u000b\u0004 \u001d\u0006\n\u000b\u000b\u0004 \t\f\f+\u001e \u001d\u0007\u0019\b\f\u0011\u0004 \u001d\u0006\u000b\u0004 \u001e\t\u0007\b\u0004 \u0007\"\u000b\t\u0004 \u0019#\u0004 \u0019+\n\u0004 \t \n\u0019\t\u000e\u0006\u0004 \u0007\f\u0011\u0004 \t*\u0011\u0004 \u001d\u0006\u000b\u0004 \b\u000b+\u001d\n\t&\u0004 \u0019\b\u000b\u0004 \u000e\u0019+&\"\u0004 \u0010\u000b\u0004 \n\u000b\"\u0007\u000e\u001d\u000b\"\u0004 \u0010\u001c\u0004 \t\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\u0004!\u0006\u0007\u000e\u0006\u0004\u0006\t\f\u0004\u0010\u000b\u000b\b\u0004\u001d\n\t\u0007\b\u000b\"\u0004!\u0007\u001d\u0006\u0004\t\u0004\u000e\u0019\n +\f\u0004\t\b\b\u0019\u001d\t\u001d\u000b\"\u0004\u0019\b&\u001c\u0004 !\u0007\u001d\u0006\u0004 \u001d\u0006\u000b\u0004 \u0007\b#\u0019\n\u001e\t\u001d\u0007\u0019\b\u0004 \u0019#\u0004 \n\u0019\f\u0019\"\u001c\u0004 \f\u001d\n+\u000e\u001d+\n\u000b\u0004 \t\b\"\u0004 \u0007\b\u001d\u0019\b\t\u001d\u0007\u0019\b\t&\u0004 \u001e\u0019\"\t&\u0007\u001d\u001c'\u0004 \u0010*\u0011\u0004 (\u0001\u0004 \u000e\u0019+&\"\u0004 \u0010\u000b\u0004 \n",
    "\u0007\u000b%\u000b\"\u0004 #\n\u0019\u001e\u0004 \u001d\u0006\u000b\u0004 \"\u0007##\u000b\n\u000b\b\u000e\u000b\u0004 \u0010\u000b\u001d!\u000b\u000b\b\u0004\u001d\u0006\u000b\u0004 \n\u000b\"\u0007\u000e\u001d\u000b\"\u0004\u0019\b\u000b\u0004\t\b\"\u0004\u001d\u0006\u000b\u0004\n\u000b\t&\u0004\u0019\b\u000b\u0004\n\u000b\t\f\u0019\b\t\u0010&\u001c'\u0004 \u0001\u0002\u0003\u0004\u0003\t\b\"\t\n\u0007\b\u0004\u001a\u001a\u001b\u0004\u0005\u0019\n +\f\u0004/40\u0004!\t\f\u0004+\f\u000b\"\u0004\u001d\u0019\u0004\u000b$\t\u001e\u0007\b\u000b\u0004\u001d\u0006\u000b\u0004 \n\u0019 \u0019\f\u000b\"\u0004 \u001e\u000b\u001d\u0006\u0019\"'\u0004 \u0002\t\f\u000b\"\u0004 \u0019\b\u0004 \u001d\u0006\u000b\u0004 \u000e\u0019\n +\f\u0011\u0004 \t\u0004 \b\u000b+\u001d\n\t&\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\u0004 !\t\f\u0004 \u0010+\u0007&\u001d\u0004 \t\u001d\u0004 #\u0007\n\f\u001d'\u0004 (\b\"\u0004 \u001d\u0006\u000b\b\u0011\u0004 \t\b\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004 \u001e\u000b\t\f+\n\u000b\u0011\u0004 ( \n\u0019$\u0007\u001e\t\u001d\u0007\u0019\b, \t\u001d\u0007\u0019\u0004 !\t\f\u0004 \u000e\t&\u000e+&\t\u001d\u000b\"\u0011\u0004 \u0010\t\f\u000b\"\u0004 \u0019\b\u0004 \u001d\u0006\u000b\u0004 \"\u0007##\u000b\n\u000b\b\u001d\u0007\t&\f\u0004\u0010\u000b\u001d!\u000b\u000b\b\u0004\n\u000b\t&\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\f\u0004\t\b\"\u0004\b\u000b+\u001d\n\t&\u0004)\u0007'\u000b'\u0004 \n\u000b\"\u0007\u000e\u001d\u000b\"*\u0004 \u0019\b\u000b\f'\u0004 \u001a\u0006\u000b\u0004 \u000e\u0019\n +\f\u0004 !\t\f\u0004 \t\b\b\u0019\u001d\t\u001d\u000b\"\u0004 !\u0007\u001d\u0006\u0004 (\u0001\u0011\u0004 \u0007'\u000b'\u0011\u0004\u001d\u0006\u000b\u0004 \"\u0007\f\u000e\n\u000b\u001d\u000b\u0004 %\t&+\u000b\u0004 \u0019#\u0004 ( \n\u0019$\u0007\u001e\t\u001d\u0007\u0019\b, \t\u001d\u0007\u0019'\u0004 <%\u000b\b\u001d+\t&&\u001c\u0011\u0004 \t\b\u0004 (\u0001\u0004 \f+ \u0019\n\u001d\u000b\"\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\u0004 \n\u000b\"\u0007\u000e\u001d\u0019\n\u0004 !\t\f\u0004 \u0010+\u0007&\u001d\u0011\u0004 \t\b\"\u0004 \u001d\u0006\u000b\u0004 \n\u000b\t\f\u0019\b\t\u0010&\u000b\b\u000b\f\f\u0004 \u0019#\u0004 \f+\u000e\u0006\u0004 (\u0001\u0004 \"\u000b\u001d\u000b\u000e\u001d\u0019\n\u0004 !\t\f\u0004 \u000b$\t\u001e\u0007\b\u000b\"\u0004 \u0010\u001c\u0004 \"\u0007##\u000b\n\u000b\b\u000e\u000b\u0004\t\b\t&\u001c\f\u0007\f\u0004\t\b\"\u0004\t\u0004 \u000b\n\u000e\u000b \u001d+\t&\u0004\u000b%\t&+\t\u001d\u0007\u0019\b'\u0004\u0004 \u001a\u0006\u000b\u0004 \t \u000b\n\u0004\u0007\f\u0004\u0019\n\u0013\t\b\u0007\u001f\u000b\"\u0004\t\f\u0004#\u0019&&\u0019!\u0007\b\u0013;\u0004\u001b\u000b\u000e\u001d\u0007\u0019\b\u00042\u0004\u0007\b\u001d\n\u0019\"+\u000e\u000b\f\u0004 \u001d\u0006\u000b\u0004 \"\u000b#\u0007\b\u0007\u001d\u0007\u0019\b\u0004 \u0019#\u0004 (\u0001'\u0004 \u001b\u000b\u000e\u001d\u0007\u0019\b\u0004 4\u0004 \f\u0006\u0019!\f\u0004 \u001d\u0006\u000b\u0004 \n\u0019\u000e\u000b\"+\n\u000b\u0004 \u0019#\u0004 \t+\u001d\u0019\u001e\t\u001d\u0007\u000e\u0004 \"\u000b\u001d\u000b\u000e\u001d\u0007\u0019\b\u0004 \u0019#\u0004 (\u0001'\u0004 (\b\"\u0004 \u001b\u000b\u000e\u001d\u0007\u0019\b\u0004 8\u0004 \n\u000b\f\u000b\b\u001d\f\u0004 \u001d\u0006\u000b\u0004 \u000b$\t\u001e\u0007\b\u0007\b\u0013\u0004\u001d\u000b\f\u001d\f'\u0004\u0005\u0019\b\u000e&+\f\u0007\u0019\b\f\u0004\t\b\"\u0004\"\u0007\f\u000e+\f\f\u0007\u0019\b\f\u0004\t\n\u000b\u0004 \n\u000b\f\u000b\b\u001d\u000b\"\u0004\u0007\b\u0004 \u001b\u000b\u000e\u001d\u0007\u0019\b\u0004\u0017'\u0004 \u001c\u001b\u0001 \u000e\u000f\u0007\f\u000b\u0010\u000b\t\u0001\u0012\u001d\t\u0007\u0003\u0010\t \u000b\n\u0007\f\u0007\u0003\u0007\u0004\f\t\u0006\f\u0013\t\u001e\u0006\f\u0007\n\u000b\u0010\u0003\u0006\u0003\u0007\u0004\f\t \u001c\u001b\u001a\u001b\u0001 \u000b\n\u0007\f\u0007\u0003\u0007\u0004\f\t \u001a\u0006\u000b\u0004 \u001d\u000b\n\u001e\u0004 (\u0001\u0004 \n\u000b#\u000b\n\f\u0004 \u001d\u0019\u0004 \u001d\u0006\u000b\u0004 %\t\n\u001c\u0007\b\u0013\u0004 \"\u000b\u0013\n\u000b\u000b\f\u0004 \u0019#\u0004 \u001d\u0006\u0019\f\u000b\u0004 \n\u0019\f\u0019\"\u001c\u0004 #\u000b\t\u001d+\n\u000b\f\u0004!\u0006\u0007\u000e\u0006\u0004\t\n\u000b\u0004 \u000b\n\u000e\u000b\u0007%\u000b\"\u0004\t\f\u0004\t\u000e\u000e\u000b\b\u001d\u000b\"\u0004\u0019\n\u0004+\b\t\u000e\u000e\u000b\b\u001d\u000b\"'\u0004\u0018\n\u0019\u001e\u0004 \u001d\u0006\u000b\u0004%\u0007\u000b!\u0004\u0019#\u0004&\u0007\b\u0013+\u0007\f\u001d\u0007\u000e\u0004\t\f \u000b\u000e\u001d\u0011\u0004(\u0001\u0004\u0007\f\u0004\n\u000b&\u000b%\t\b\u001d\u0004\u001d\u0019\u0004\u001d\u0006\u000b\u0004\"\u0007\f\u001d\n\u0007\u0010+\u001d\u0007\u0019\b\u0004 \u0019#\u0004\f\u000b\u001e\t\b\u001d\u0007\u000e\u0004\u000b\u001e \u0006\t\f\u0007\f\u0004\u0019\n\u0004#\u0019\u000e+\f\u0004\u0007\b\u0004\t\u0004\f\u000b\b\u001d\u000b\b\u000e\u000b'\u0004\u0018\n\u0019\u001e\u0004\u001d\u0006\u000b\u0004%\u0007\u000b!\u0004\u0019#\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004 \t\f \u000b\u000e\u001d\u0011\u0004 \u0007\u001d\u0004 \u0007\f\u0004 \n\u000b\t&\u0007\u001f\u000b\"\u0004 \t\f\u0004 %\t\n\u001c\u0007\b\u0013\u0004 \n\u0019\u001e\u0007\b\u000b\b\u000e\u000b\f\u0004 \u0019#\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004 \n\u0019\f\u0019\"\u001c\u0004 \t\n",
    "\f'\u0004 \u0001\b\u0004 \u001d\u0006\u000b\u0004 \f \t\u000e\u000b\u0004 \u0019#\u0004 \u0007\u001d\f\u0004 \n\u000b\t&\u0007\u001f\u000b\"\u0004 \t\u000e\u0019+\f\u001d\u0007\u000e\u0004#\u000b\t\u001d+\n\u000b\f\u0011\u0004\t\u000e\u000e\u000b\b\u001d\u0004\f\u0006\u0019+&\"\u0004\u0010\u000b\u0004%\t\n\u001c\u0007\b\u0013\u0004\u000e\u0019\b\u001d\u0007\b+\u0019+\f&\u001c'\u0004\u0002+\u001d\u0004 \t\f\u0004\t\u0004\f\u000b\u001d\u0004\u0019#\u0004\f\u001c\u001e\u0010\u0019&\u0007\u000e\u0004\u001d\n\t\b\f\u000e\n\u0007 \u001d\u0007\u0019\b\u0011\u0004\t\u000e\u000e\u000b\b\u001d\u0004\u001e+\f\u001d\u0004\u0010\u000b\u0004\f\u000e\t&\u000b\"\u0004\u0007\b\u001d\u0019\u0004 \f\u000b%\u000b\n\t&\u0004 \"\u0007\f\u000e\n\u000b\u001d\u000b\u0004 &\u000b%\u000b&\f'\u0004 (\u000e\u000e\u0019\n\"\u0007\b\u0013\u0004 \u001d\u0019\u0004 \u001d\u0006\u000b\u0004 \u000b$ \u000b\n\u0007\u000b\b\u000e\u000b\u0004 \u0019#\u0004 \u0002\u0001\u0004 \"\u000b#\u0007\b\u0007\u001d\u0007\u0019\b\u0004/40\u0011\u0004\u001d\u0006\u000b\u0004\f\u000b\u001d\u0004\u0019#\u0004(\u0001\u0004!\t\f\u0004\"\u000b#\u0007\b\u000b\"\u0004\t\b\"\u0004\u000e\u0019\n",
    "\u000b\f \u0019\b\"\u0007\b\u0013\u0004\u001d\u0019\u0004 \t\u0004\f\u000b\u001d\u0004\u0019#\u0004\u000b$ &\u0007\u000e\u0007\u001d\u0004 \n\u0019\f\u0019\"\u001c\u0004\u001e\u000b\t\b\u0007\b\u0013\f'\u0004\u0004 0-7803-8678-7/04/$20.00 ©2004 IEEE 85 ISCSLP 2004\n"
   ]
  },
  "tian04_iscslp": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Jani",
     "Nurminen"
    ]
   ],
   "title": "On Analysis of Eigenpitch in Mandarin Chinese",
   "original": "045",
   "page_count": 4,
   "order": 28,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "Prosody is an inherent supra-segmental feature of human’s speech that is being employed to express e.g. attitude, emotion, intent and attention. Pitch is the most important feature among the prosodic information. For Mandarin Chinese speech, the pitch information is even more crucial because Mandarin is a tonal language in which the tone of each syllable is described by its pitch contour. In this paper, the concept of syllablebased eigenpitch is introduced and investigated using principal component analysis (PCA). The eigenpitch and the related eigen features are analyzed, and it is shown that the tonal patterns are preserved in the eigenpitch representation. Furthermore, we show that the dimension of pitch in the eigen space can be reduced while minimizing the energy loss of the original pitch contour. Finally, we briefly discuss the quantization properties of the eigenpitch representation. We also present experimental results obtained using a Mandarin speech database. They are in line with the theoretical reasoning and further prove the usefulness of the proposed pitch modeling technique. 1 Introduction The term prosody refers to certain properties of a speech signal that are related to audible changes in pitch, loudness and duration. Among these features, pitch usually plays the most important role. Physically, the pitch of an utterance depends on the rate of vibration of the vocal cords; the higher the rate of vibration, the higher the resulting pitch becomes. Another concept closely related to pitch is tone that is used to describe pitch variations inside short stretches of syllables. In tonal languages, these relative pitch differences are used either to differentiate between word meanings or to convey grammatical distinctions. Many of the languages of SouthEast Asia and Africa are tonal languages. Mandarin Chinese is probably the most widely studied tonal language in which each stressed syllable has a significant contrastive pitch that is an integral part of the syllable. It has four basic tones: high level, high rising, dipping/falling and high falling. They are used to distinguish otherwise homophonous words as shown in Table 1.  Word Intonation Meaning ma [--] mother ma [/] numbness ma [\\/] horse ma [\\] curse Table 1. Examples of different tones in Mandarin Chinese. The most commonly used representation of tonal pitch contours as numbers is shown in Table 2. It consists of five pitch levels, rather like the use of staves in music scores. They are labeled from the bottom upwards from 1 to 5. The tonal patterns are captured using the reference pitch numbers by observing the start, the middle and the end points of the pitch contour [7].  Contour Type Pattern Feature 5   4  1 4 3  2 2  3 1   Tone 1 Tone 2 Tone 3 Tone 4   5-5 3-5 2-1-4 5-1 H-H (High) L-H (Rising) L-L (Low) H-L (Falling) Table 2. Tonal patterns and phonological notations of four citation tones in Mandarin Chinese. Obviously pitch information plays a crucial role in speech synthesis systems, especially for tonal languages [3][8]. Since the pitch contour conveys information about word meaning distinction, prosodic phrase and word boundaries, it has been found in [5] that human beings use the pitch contour information to enhance the speech recognition performance. Various techniques have also been proposed to improve the noise robustness of speech recognition systems by using the pitch information [5]. Due to all of these reasons, pitch modeling is one of the key issues that must be addressed when dealing with tonal languages. The most popular pitch modeling approaches are mainly using the concept of separating the pitch contour into a global trend and local variation. Two examples following this approach are the superpositional modeling technique [2] and the two-stage modeling technique [1]. In [6], the mean and the shape of the syllable pitch contours are taken as two basic modeling units by using a 3rd order orthogonal polynomial expansion. Since the syllable pitch contour patterns vary dramatically from their canonical form, a reasonable assumption is that some datadriven approach could preserve more precise and more relevant information compared to pure artificial fitting. In this paper, we propose a data-driven pitch modeling approach based on the concept of eigenpitch and study its properties to verify the above assumption. In addition, we provide results related to tonal classification and pitch compression using the proposed modeling approach. The remainder of the paper is organized as follows. We first describe the process of eigenpitch extraction and some of the basic properties of the eigenpitch representation. Then, the performance of the proposed modeling approach in the tonal 0-7803-8678-7/04/$20.00 ©2004 IEEE 89 ISCSLP 2004\n"
   ]
  },
  "kong04_iscslp": {
   "authors": [
    [
     "Jiangping",
     "Kong"
    ]
   ],
   "title": "Acoustical Study on Sub-Harmonic of Glottal Source in Mandarin Tones",
   "original": "085",
   "page_count": 4,
   "order": 29,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "This paper is concerned with the acoustical analysis on sub-harmonic of glottal source in Mandarin tones. The methods used in this research are: 1) extracting glottal source of tones by inverse filtering; 2) analyzing subharmonic and spectrum tilt by FTT; 3) simulating the double peak pulse by 4 functions and describing the natures of them in both time and frequency domains. There are 3 conclusions: 1) the double peak pulse produce sub-harmonic in glottal source of Mandarin tones; 2) subharmonic influences the spectrum tilt; 3) the double peak pulse can be simulated and modeled mathematically.\n"
   ]
  },
  "zhu04b_iscslp": {
   "authors": [
    [
     "Donglai",
     "Zhu"
    ],
    [
     "Qiang",
     "Huo"
    ],
    [
     "Jian",
     "Wu"
    ]
   ],
   "title": "A Study of Switching State Segmentation in Segmental Switching Linear Gaussian Hidden Markov Models for Robust Speech Recognition",
   "original": "073",
   "page_count": 4,
   "order": 30,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "In our previous works, a Switching Linear Gaussian Hidden Markov Model (SLGHMM) and its segmental derivative, SSLGHMM, were proposed to cast the problem of modeling a noisy speech utterance in robust automatic speech recognition by a well-designed dynamic Bayesian network. An important issue of SSLGHMM is how to specify a switching state value for each frame of feature vector in a given speech utterance. In this paper, we propose several approaches for addressing this issue and compare their performance on Aurora3 connected digit recognition tasks.\n"
   ]
  },
  "chen04b_iscslp": {
   "authors": [
    [
     "Yi",
     "Chen"
    ],
    [
     "LinShan",
     "Lee"
    ]
   ],
   "title": "Robust Features for Speech Recognition using Minimum Variance Distortionless Response (MVDR) Spectrum Estimation and Feature Normalization Techniques",
   "original": "093",
   "page_count": 4,
   "order": 31,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "In this paper, feature extraction methods based on frequencywarped Minimum Variance Distortionless Response (MVDR) spectrum estimation are analyzed and tested. The effectiveness of the conventional FFT-based Mel-Frequency Cepstrum Coefficients (MFCCs) and the MVDR-based features are carefully compared. Two normalization techniques are further applied to improve the robustness of the features: the widely used cepstral normalization (CN), and newly proposed progressive histogram equalization (PHEQ). Extensive experiments with respect to the AURORA2 database were performed. The results indicated that both the MVDR-based features and the normalization processes are very helpful.\n"
   ]
  },
  "huang04b_iscslp": {
   "authors": [
    [
     "YungSheng",
     "Huang"
    ],
    [
     "Jeihweih",
     "Hung"
    ]
   ],
   "title": "Data-Driven Temporal Filters based on Maximum Mutual Information for Robust Features in Speech Recognition",
   "original": "047",
   "page_count": 4,
   "order": 32,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA) and Minimum Classification Error (MCE) have been used to derive data-driven temporal filters in order to improve the robustness of speech features for speech recognition. In this paper, the criterion of Maximum Mutual Information (MMI) is proposed for constructing the temporal filters, and detailed comparative analysis among these various approaches are presented and discussed. Experimental results show that the MMI-derived temporal filters significantly improve the recognition performance of the original MFCC features as LDA/PCA/MCE-derived filters do. Also, while the MMI-derived filters are combined with the conventional temporal filters, Cepstral Mean and Variance Normalization (CMVN), the recognition performance can be further improved.\n"
   ]
  },
  "huang04c_iscslp": {
   "authors": [
    [
     "ChihHsien",
     "Huang"
    ],
    [
     "JenTzung",
     "Chien"
    ],
    [
     "Hsinmin",
     "Wang"
    ]
   ],
   "title": "A New Eigenvoice Approach to Speaker Adaptation",
   "original": "070",
   "page_count": 4,
   "order": 33,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "In this paper, we present two approaches to improve the eigenvoice-based speaker adaptation. First, we present the maximum a posteriori eigen-decomposition (MAPED), where the linear combination coefficients for eigenvector decomposition are estimated according to the MAP criterion. By incorporating the prior decomposition knowledge, here we use a Gaussian distribution, the MAPED is established accordingly. MAPED is able to achieve better performance than maximum likelihood eigen-decomposition (MLED) with few adaptation data. On the other hand, we exploit the adaptation of covariance matrices of the hidden Markov model (HMM) in the eigenvoice framework. Our method is to use the principal component analysis (PCA) to project the speaker-specific HMM parameters onto a smaller orthogonal feature space. Then, we reliably calculate the HMM covariance matrices using the observations in the reduced feature space. The adapted HMM covariance matrices are estimated by transforming the covariance matrices in the reduced feature space to that in the original feature space. The experimental results show that the eigenvoice speaker adaptation using MAPED and incorporating covariance adaptation can improve the performance of the original eigenvoice adaptation in Mandarin speech recognition.\n"
   ]
  },
  "li04_iscslp": {
   "authors": [
    [
     "XiaoBing",
     "Li"
    ],
    [
     "LiRong",
     "Dai"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "Mce-Based Training of Subspace Distribution Clustering HMM",
   "original": "077",
   "page_count": 4,
   "order": 34,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "For resource-limited platforms, Subspace Distribution Clustering Hidden Markov Model (SDCHMM) is better than Continuous Density Hidden Markov Model (CDHMM) for its smaller storage and lower computations while maintaining a decent recognition performance. But the normal SDCHMM obtaining method doesn’t ensure the optimality in classifier design. In order to obtain an optimal classifier, a new SDCHMM training algorithm that adjusts the parameters of SDCHMM according to Minimum Classification Error (MCE) criterion is proposed in this paper. Our experimental results on TiDigits and RM tasks show the MCE-based SDCHMM training algorithm provides 15-80% Word Error Rate Reduction (WERR) compared with the normal SDCHMM that is converted from CDHMM.\n"
   ]
  },
  "tang04_iscslp": {
   "authors": [
    [
     "Yun",
     "Tang"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Yiyan",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A Framework for Fast Segment Model by Avoidance of Redundant Computation on Segment",
   "original": "018",
   "page_count": 4,
   "order": 35,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "Segment model (SM) is a family of methods by using segmental distribution rather than frame-based features (e.g. HMM) to represent the underlying characteristics of observation sequence. It has been proved to be more precise than that of HMM. However, the high complexity prevents these models from practical system. In this paper we present a framework to reduce the computational complexity of segment model by fixing the number of the basic unit in the segment to share the intermediate computation results. Our work is twofold. First, we compared the complexity of SM with HMM and proposed a fast SM framework based on the comparison. Second we use two examples to illustrate this framework. The fast SMs have better performance than the system based on HMM, and at the mean time, we successfully keep the computation complexity of SM at the same level of HMM..\n"
   ]
  },
  "hoshino04_iscslp": {
   "authors": [
    [
     "Akemi",
     "Hoshino"
    ]
   ],
   "title": "Dependence of Correct Pronunciation of Chinese Aspirated Sounds on Power During Voice Onset Time",
   "original": "001",
   "page_count": 4,
   "order": 36,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "The length of voice onset time (VOT) in uttering Chinese aspirated sounds, which are difficult for Japanese to pronounce, is an important factor in evaluating the quality of pronunciation. In this paper, both the length of the VOT and the power used during the VOT for 21 single-vowel syllables of six different Chinese aspirates were measured for 40 Japanese students and nine native speakers of Chinese. The quality of the students’ pronunciation was evaluated using a hearing test judged by eight native Chinese. The results indicated that the correlation between the quality of the students’ pronunciation and the power used in uttering a sound was greater than to the VOT within a certain range of VOT which varied for different syllables. Thus, we conclude that power is also an important factor in evaluating the quality of pronunciation.  1. INTRODUCTION  The number of students learning Chinese in Japan has increased in recent years along with the development of the Chinese economy. There are many different sounds in Chinese pronunciation. As most of them are quite different from Japanese sounds, many Japanese students have difficulty in pronouncing them, especially aspirated sounds. Uttering aspirated syllables requires the speaker to exhale. As Japanese has no aspirated sounds, Japanese students attempt to imitate the sounds made by their native Chinese teachers, but many of them are unable to pronounce them correctly. Recognizing aspirated sounds is also difficult for them. There are 21 aspirated Chinese sounds of single vowel syllables of six different aspiration sounds: bilabial (p[p‘]), alveolar (t[t‘]), velar (k[k‘]), palatal (q[ʨ‘]), retroflex (ch[tʂ‘]) and dental (c[ʦ‘])[1], as shown in table 1. We analyzed the VOT and the power used during the VOT for four bilabial syllables in our previous study[2][3] and showed that the quality of pronunciation depended not only on the VOT but also on the power used during the VOT. In this study, we analyzed another 17 single-vowel syllables of five different aspirated sounds pronounced by nine native Chinese speakers and 40 Japanese students, who had studied Chinese for 3 hours per week for one year, and again found that the quality of pronunciation depended not only on the VOT but also on the power used during the VOT as reported for bilabial sounds in the previous papers.   2. DIFFERENCES BETWEEN ASPIRATED AND UNASPIRATED SOUNDS              Figure 1: Spectrograms of unaspirated syllable ga[ka] (left), and aspirated syllable ka[k'a] (right) pronounced by Chinese speakers.  Figure 1 shows the air vibrations of uttered sounds (lower) and spectrograms of the unaspirated syllable, ga[ka], (left) and the aspirated syllable, ka[k'a], (right). The darker the horizontal bands, the higher the power of the frequency components. The aspirate appears in a brief interval in the right spectrogram, indicated by vertical stripes, between the stop burst and the onset of vocal fold vibrations followed by a vowel. This time interval is called the voice onset time (VOT)[4]. The onset of the vocal fold vibration is so close to the burst in the left spectrogram that no aspiration interval appears. These data were acquired and analyzed using a tool of Multi-Speech (Model 3700, Kay Elemetrics Corp., USA).   3. METHOD USED TO EVALUATE PRONUNCIATION Aspirate syllable ka[k‘a] Frequency (kHz) 5 4 3 2 1 Unaspirate syllable ga[ka] 0.5\n"
   ]
  },
  "hoshino04b_iscslp": {
   "authors": [
    [
     "Akemi",
     "Hoshino"
    ],
    [
     "Akio",
     "Yassuda"
    ]
   ],
   "title": "Effect of Japanese Articulation of Stops on Pronunciation of Chinese Aspirated Sounds by Japanese Students",
   "original": "040",
   "page_count": 4,
   "order": 37,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "It is relatively easy for Japanese students studying Chinese to learn the pronunciation of the bilabial, alveolar, and velar groups of Chinese aspirated sounds compared to other sounds. We examined the voicing features for voice onset time and power during the utterance of Japanese stop consonants in bilabial, alveolar, and velar sounds, which are articulated in the same way as in Chinese. The sounds were uttered by 10 native Japanese who had not studied Chinese before. We then compared these features to the features of Chinese aspirated sounds uttered by 35 Japanese students, who had studied Chinese for one year, and eight native Chinese. The results showed that the features of the Japanese stops uttered by Japanese were similar to those for Chinese aspirated sounds. The features of the students’ utterances were also similar to those of the Chinese speakers. This similarity appeared to be one of the main factors in students achieving good grades for their pronunciation of these Chinese sounds compared to other sounds.\n"
   ]
  },
  "hsu04_iscslp": {
   "authors": [
    [
     "Huiju",
     "Hsu"
    ]
   ],
   "title": "Taiwan Mandarin -- Does It Remain Homogeneous?",
   "original": "083",
   "page_count": 4,
   "order": 38,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Previous studies have shown discrepancies in tonal realizations between Guoyu and Putonghua. Early studies suggests Guoyu T3 is predominantly a falling tone and recent studies show Guoyu T2 is predominantly a dipping tone, contrastive to the long-considered default dipping and rising tone respectively. This study further explores the existence of regional varieties of Guoyu. Data are collected from Taipei and Taichung. Speakers read target sentences with 19 minimal pairs of final T2/T3 syllable being placed sentence final. Results indicate regional differences of T2/T3 patterns. The result of Taipei speakers indicates a clear distinction of T2/T3 contour in that T2 is realized as a mid-dipping contour and T3 either a mid-dipping or a mid-falling contour, with the latter as the majority. However, in the Taichung dialect, this distinction disappeared. It is shown that Taichung T2 contour has changed from mid-dipping to mid-falling, merging with T3. This merger is statistically significant.\n"
   ]
  },
  "luo04_iscslp": {
   "authors": [
    [
     "Xin",
     "Luo"
    ],
    [
     "QianJie",
     "Fu"
    ]
   ],
   "title": "Contributions of Periodicity Fluctuation Cues in Individual Frequency Channels to Chinese Speech Recognition",
   "original": "072",
   "page_count": 4,
   "order": 39,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Studies have revealed near perfect speech recognition with primarily temporal envelope cues and severely degradedspectral cues. Among different types of temporal envelope cues, periodicity ﬂuctuation cues have been found to signiﬁcantly improve Chinese tone and sentence recognition, while the contributions of periodicity ﬂuctuation cues in individual frequency channels to Chinese speech recognition have not been clearly stated. In order to make periodicity ﬂuctuation cues available in different frequency regions, the present study employed different low-pass cutoff frequencies for the temporal envelope detectors in different channels of a fourchannel noise-band cochlear implant simulation. Chinese tone and vowel recognition scores were measured for six native Chinese normal hearing listeners under six low-pass cutoff frequency combinations: all 50 Hz in four channels (all-50 Hz), all 500 Hz in four channels (all-500 Hz), and 500 Hz in one of the four channels while 50 Hz in the other three channels (ch1-500 Hz, ch2-500 Hz, et al.). The results showed that the ch4-500 Hz condition produced the highest Chinese tone recognition among the four single-channel500 Hz conditions, and was the only condition whose tone recognition was similar to that of the all-500 Hz condition and was signiﬁcantly higher than that of the all-50 Hz condition. Chinese vowel recognition was not signiﬁcantly affected by different cutoff frequency combinations. These results suggest that delivering periodicity ﬂuctuation cues in higher frequency channels might be more important and efﬁcient in enhancing Chinese tone recognition for cochlear implant patients.\n"
   ]
  },
  "dong04_iscslp": {
   "authors": [
    [
     "Bin",
     "Dong"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Jianping",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Automatic Assessment of Pronunciation Quality",
   "original": "054",
   "page_count": 4,
   "order": 40,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "Learning to speak a foreign language is not an easy task for many people. This paper describes approaches to automatic objective assessment of pronunciation quality. The approaches described here can be classified into two categories, text-dependent and text-independent, according to whether a teacher’s voice presents. In the text-independent one, algorithms based on energy and pitch contour are introduced. Also, the average rate of variation in energy and pitch frequency, mean subtracted energy and pitch frequency are used as main features. Compared to previously reported approach using average phone segment posterior probabilities, the new approach achieves favorable performance on the same test set.\n"
   ]
  },
  "li04b_iscslp": {
   "authors": [
    [
     "Jing",
     "Li"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Quantization of Sew and Rew Magnitude for 2 Kb/S Waveform Interpolation Speech Coding",
   "original": "012",
   "page_count": 4,
   "order": 41,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "This paper presents the quantization schemes for the magnitude spectra of the slowly evolving waveform (SEW) and rapidly evolving waveform (REW) components in a 2 kb/s waveform interpolation (WI) coder. The SEW magnitude spectrum is quantized using a DCT-based predictive vector quantization approach. The REW magnitude spectrum is quantized using a matrix quantizer based on the combined dimension conversion method. The objective measures and subjective results indicate that the proposed quantization schemes are effective in achieving good quantization accuracy.\n"
   ]
  },
  "wang04d_iscslp": {
   "authors": [
    [
     "Guiping",
     "Wang"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Low Complexity Decomposition for the Characteristic Waveform of Speech Signal",
   "original": "014",
   "page_count": 4,
   "order": 42,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "For efficient coding of speech, it is desirable to separate the slowly and rapidly evolving spectral components to take advantage of their different perceptual qualities. Existing decomposition methods are too inflexible to model transient changes in the speech signals, require high delay or produce a large parameter set that is not scalable to low rates. In this paper•, we present a low complexity decomposition method, based on SVD, applied to Waveform Interpolation (WI) coding. This scheme reduced the computational complexity of common SVD method in WI by exploiting the properties of human auditory perception to lower the dimensions of decomposition matrix. This method requires only a single frame of speech and overcomes the substantial delay problems. The quantization solution involves the use of vector quantization on separately decomposed the singular matrix U、V and the diagonal matrix of singular values S. The quality of reconstruction speech can be varied according to the scalable decomposition and the bit rate available.\n"
   ]
  },
  "bao04_iscslp": {
   "authors": [
    [
     "Changchun",
     "Bao"
    ],
    [
     "Jason",
     "Lukasiak"
    ],
    [
     "Christian",
     "Ritz"
    ]
   ],
   "title": "High Quality Harmonic Excitation Linear Predictive Speech Coding at 2 Kb/S",
   "original": "015",
   "page_count": 4,
   "order": 43,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "This paper• presents a high quality harmonic excited linear predictive (HE-LPC) speech coder operating at 2 kb/s based on a harmonic excitation model with two bands. The system incorporates novel features such as: combined pitch detection, residual harmonic matching voicing determination, extraction and interpolation of residual harmonic magnitudes. Subjective listening tests indicate that this coder has same quality as that of Federal Standard MELP coder at 2.4 kb/s whatever training database is from Chinese or English.\n"
   ]
  },
  "bai04b_iscslp": {
   "authors": [
    [
     "Yanning",
     "Bai"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "An Improved 4 Kbit/S Celp Speech Coding Algorithm",
   "original": "016",
   "page_count": 4,
   "order": 44,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "This paper• presents a 4 kbit/s CELP speech coder that utilizes the nonuniform and part-searching-area algebraic codebook technologies to overcome the insufficient number of signed pulses in fixed codebook (FCB). The nonuniform algebraic codebook is based on the nonuniform statistical properties of the FCB. The partsearching-area utilizes the periodicity of the FCB excitation signal at low bit rate. The latter is only employed when pitch delay is small enough. We also find that preserving the continuity of pitch is very important for voiced segment if these two technologies are used. So different pitch-detect methods are employed for voiced/unvoiced frame. Subjective and objective test results indicate that the qualities of reconstructed speech are improved, especially for the female speakers.\n"
   ]
  },
  "guilin04_iscslp": {
   "authors": [
    [
     "",
     "GuiLin"
    ]
   ],
   "title": "An Embedded English Synthesis Approach based on Speech Concatenation and Smoothing",
   "original": "059",
   "page_count": 4,
   "order": 45,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "An embedded English synthesis approach based on speech concatenation and smoothing is described. This approach adopts phonetic sub-words as carrier of variable-length units. We define 5-class units to cover all English phonetic phenomena. The corresponding cost function and search procedure based on dynamic programming are addressed in the unit-selection stage. Vocal tract response, pitch value and phase are interpolated and merged at concatenating points for smoothing speech in the synthesis stage. The preliminary test shows that this approach can reach a good balance of  naturalness, intelligibility and data footprint.\n"
   ]
  },
  "hu04_iscslp": {
   "authors": [
    [
     "GuoPing",
     "Hu"
    ],
    [
     "QingFeng",
     "Liu"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "Hearer Model based Stress Prediction for Chinese TTS System",
   "original": "027",
   "page_count": 4,
   "order": 46,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "People often feel tired if he/she listens synthesized speech for a long time. This is mainly because synthesized speech is too flat and never stresses the focus. Different to traditional TTS research approach of simulating speaker, this paper does the stress prediction research from the point of the hearer. An ideal hearer model is first proposed to predict the stress distribution based on the hypothesis: people speak with limited stress effort and distribute the limited effort to ensure that the hearer can understand the speaker easily. Then according to the limited research resource, this paper modifies the ideal hearer model and presents a practical model. Experiments show that the stress prediction achieves an acceptable rate of 87.36%. Keywords: Hearer model, Stress prediction, Speech synthesis\n"
   ]
  },
  "dong04b_iscslp": {
   "authors": [
    [
     "Honghui",
     "Dong"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Grapheme-To-Phoneme Conversion in Chinese TTS System",
   "original": "030",
   "page_count": 4,
   "order": 47,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "Phonetization is an important component in Chinese TTS system. However the polyphonic characters make this problem more complex. This paper reports the study on the relation between the Chinese characters and their pronunciation, proposes the solution to the disambiguation of polyphonic characters, dictionary-based method, and rules-based method. In the rule-based method, we used the statistic decision list method. The phonetization plan is proved effective in the experiment. Most of the improvements on the accuracy of polyphone phonetization are beyond 10%.\n"
   ]
  },
  "pin04_iscslp": {
   "authors": [
    [
     "ShaoHuang",
     "Pin"
    ],
    [
     "Yongcheng",
     "Chen"
    ],
    [
     "Hsinmin",
     "Wang"
    ],
    [
     "Chiuyu",
     "Tseng"
    ]
   ],
   "title": "A Mandarin TTS System with an Integrated Prosodic Model",
   "original": "061",
   "page_count": 4,
   "order": 48,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "Phrase grouping is essential to characterize the prosody for Mandarin fluent speech. Evidence of prosodic phrase grouping has been found both in adjustments of F0 contours and temporal allocations within and across phrases. In this paper, we discuss the development of a Mandarin TTS system that integrates the prosody processing modules, such as duration modeling, F0 modeling, and break predictions. The database consists of 1292*3 syllable-tokens chopped off specially designed threephrase carrier sentences. Since temporal allocations and rhythmic structure in speech flow are carefully dealt with, the TTS system is capable of converting long paragraph text input into natural synthesized speech output.\n"
   ]
  },
  "peng04_iscslp": {
   "authors": [
    [
     "HuaJui",
     "Peng"
    ],
    [
     "Chiching",
     "Chen"
    ],
    [
     "Chiuyu",
     "Tseng"
    ],
    [
     "Kehjiann",
     "Chen"
    ]
   ],
   "title": "Predicting Prosodic Words from Lexical Words--A First Step Towards Predicting Prosody from Text",
   "original": "081",
   "page_count": 4,
   "order": 49,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "Much remains unsolved in how to predict prosody from text for unlimited Mandarin Chinese TTS. The interactions and the governments between syntactic structure and prosodic structure were still unresolved challenges. By using Part-of-Speech tagging (hence POS), lexical information of text was required, we aimed to find significant patterns of word grouping from analyzing real speech data and such lexical information. This paper reported discrepancies found between lexical words (hence LW) parsed from text and prosodic words (hence PW) annotated from speech data, and proposed a statistical model to predict PWs from LWs. In statistical model, both length of the word and the tagging from POS are two essential features to predict PWs, and the results showed approximately 90% of prediction for PWs, however, it did leave more room for extension. We believe that evidence from PW predictions is a first step towards building prosody models from text.   1. INTRODUCTION  Much remains unsolved in how to predict prosody from text for unlimited Mandarin Chinese TTS. Linguistic analyses of text have been insufficient to provide specifications required for speech prosody, both in terms of prosodic units and boundaries, and in intonation contours for connected fluent speech. Though syntactic analyses provide possible boundaries and intonation specification for phrases, location of boundaries and breaks in connected speech require more specification, and prosody of fluent speech goes beyond concatenating simple-sentence intonations into strings. Aiming to build a prosody model for connected fluent speech from the bottom upward, our first step was to set up models that could sufficiently predict PW from LW, and to serve as a base for building speech prosody. In hierarchical rhythmic structures [1], PW is fundamental prosodic unit, while LW is basic syntactic unit in syntactic structure. However gaps and discrepancies were in each layer of syntactic and prosodic structures. Only 67.5% of PWs and LWs were coincident in our prosodic structure tagged corpora (in section 2.3). In this paper we proposed a statistical model for predicting PWs by grouping lexical words. The issues of grouping words to form PWs have been studied in [2, 3], a good word grouping strategy helped construct the temporal organization of speech and rendered spoken utterances natural and fluent. In the following sections, we focused on finding an optimal word grouping strategy by combining lexical information\n"
   ]
  },
  "chen04c_iscslp": {
   "authors": [
    [
     "GaoPeng",
     "Chen"
    ],
    [
     "Gerard",
     "Bailly"
    ],
    [
     "QingFeng",
     "Liu"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "A Superposed Prosodic Model for Chinese Text-To-Speech Synthesis",
   "original": "064",
   "page_count": 4,
   "order": 50,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "The paper presents the application of the trainable SFC superpositional prosodic model to Chinese. Within the SFC model, prosodic parameters (F0, syllabic lengthening) are interpreted as the superposition of overlapping multiparametric contours. These contours are associated with high-level prosodic features operating at different scopes, such as tones, stress, prosodic boundary, part of speech of words, etc. Each feature label corresponds to a metalinguistic function (morphological, lexical, syntactic, attitudinal…) which is represented by a neural network. The observed contour is the sum of the outputs of the corresponding neural networks. An analysis-by-synthesis scheme is implemented for automatically learning. This model works well in the concatenation of neighbored units. The RMSE of F0 prediction is 2.34st (referenced to 200Hz), correlation is 0.86. Perceptual experiments show that the predicted prosody is quite appropriate and fluent. 1 INTRODUCTION The fundamental problem for intonation analysis and synthesis is that prosody is the acoustic encoding of a large number of linguistic and paralinguistic features. Two major classes of intonation models have evolved in the past two decades. Superpositional models interpret prosody as complex patterns resulting from the superposition of more simple components. Fujisaki model [5] is the typical model in this class, which decomposes F0 into phrase component and accent component. The parameters are associated with the mechanism of pronouncing, which is quite relevant to the macro-prosodic features. It has been tried on many languages including Chinese [4, 9]. Due to the different characteristics between tonal and non-tonal languages, it is difficult to simulate tone events by accent components. Besides, the automatic extraction of the phrase and accent commands from observed F0 is not a solved problem. Other proposals [1, 6, 11] face also the problem of the ill-posed problem of analysis, i.e. decomposing an observed contours into elementary contributions. The SFC [2, 7] implements a prosodic model initially proposed for French [1] which introduces a new model-constrained, data-driven method to generate prosody contours with very few prototypical movements. The SFC introduces an original training paradigm using an analysis-by-synthesis framework that iteratively decomposes prosodic contours and builds the prosodic model in the same time (see §2). On the other hand, there are models that claim that F0 contours are generated from a sequence of phonologically distinctive tones or categorically different pitch accents, which are determined locally. The typical ones are the Tilt model [10] in English, PENTA [12, 13] in Chinese. These models focus on local events, but they ignore the trait of prosody on a big unit, such as on phrase or clause. Chinese is a tone language with high-level, lowrising, low-falling, high-falling and neutral tones. The tone events are very important to the prosody of an utterance. Each syllable that is the carrier of a tone and a basic meaningful phonetic unit normally is an individual target of prediction. However, sentence declination and phrasing are important as well. In this paper a superposed model is proposed to model Chinese prosodic contours, and the sequences of tones, phrases and clauses are all considered. 2 DESCRIPTION OF THE MODEL Principles. SFC considers that the prosodic contour is the contribution of few basic metalinguistic functions (phonetic such as tonal distinctions, segmentation, salience, hierarchy…) acting on different units at various scopes. We suppose that (1) each function affect prosody by means of a function-specific multiparametric contour called functional contour (FC); (2) An FC is co-extensive to the units concerned by the function it implements this extend is called the domain or the scope of the FC and is independent from the other units or functions implied in the discourse structure; (3) the shape of a FC is only a function of its scope (and of course of the metalinguistic function it implements); (4) the predicted/target contour is the superposition of corresponding FC using an appropriate scale (logarithmic for both F0 and syllabic lengthening). Functional contour generators. All FC implementing a given prosodic function are generated by a unique functional contour generator (FCG). FCGs are now  0-7803-8678-7/04/$20.00 ©2004 IEEE 177 ISCSLP 2004\n"
   ]
  },
  "zuo04_iscslp": {
   "authors": [
    [
     "Guoyu",
     "Zuo"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Xiaogang",
     "Ruan"
    ]
   ],
   "title": "Improving the Performance of MGM-Based Voice Conversion by Preparing Training Data Method",
   "original": "088",
   "page_count": 4,
   "order": 51,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "This paper proposes an approach to improve both the target speaker’s individuality and the quality of the converted speech by preparing the training data. In mixture Gaussian spectral mapping (MGM) based voice conversion, spectral features representations are analyzed to obtain the right feature associations between the source and target characteristics. A voiced and unvoiced (V/UV) decision scheme for time-alignment is provided to obtain the right data for training mixture Gaussian spectral mapping function while removing the misaligned data. Experiments are conducted in terms of the applications of spectral representation methods and V/UV decisions strategies to the MGM functions. When linear predictive cepstral coefficients (LPCC) are used for time-alignment and the V/UV decisions are adopted for removing bad data, results show that the conversion function can get a better accuracy and the proposed method can effectively improve the overall performance of voice conversion.\n"
   ]
  },
  "gu04b_iscslp": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis and Synthesis of Cantonese F0 Contours based on the Command-Response Model",
   "original": "089",
   "page_count": 4,
   "order": 52,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "Cantonese is a well-known Chinese dialect with a quite complex tone system. We have applied the command-response model to represent F0 contours of Cantonese speech by defining a set of appropriate tone command patterns. In this paper, the analysis is extended to Cantonese utterances at three different speech rates. By incorporating the effects of tone coarticulation, word accentuation and phrase intonation, the model gives high accuracy of approximations to F0 contours of Cantonese speech, and hence provides a much better means to quantitatively describe the F0 contours than the traditional 5-level tone code system. The distributions of timing and amplitudes of commands are investigated, based on which a set of rules is used for synthesis of Cantonese F0 contours. The validity of the current approach is confirmed by perceptual evaluation of synthetic speech of Cantonese.\n"
   ]
  },
  "liu04_iscslp": {
   "authors": [
    [
     "Bei",
     "Liu"
    ],
    [
     "Limin",
     "Du"
    ]
   ],
   "title": "The Disambiguation Strategies of Semantic Analysis in Chinese Spoken Dialogue System",
   "original": "071",
   "page_count": 4,
   "order": 53,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "Semantic frame analysis is one of the most commonly used semantic analysis methods in Chinese Spoken Dialogue System research. And the two typical ambiguous structures commonly encountered in semantic analysis are relationambiguity and structural–ambiguity. According to the features of these two ambiguous structures, this paper puts forth the Semantic PCFG model based disambiguation strategy to solve structural-ambiguity and the Expectation Model (EM) based disambiguation strategy to solve relation-ambiguity. Efficient algorithms of the two methods are also provided. The experimental results show that applying these two disambiguation strategies can most greatly improve the performance of the language understanding in base-line system. Especially, Sentence Accuracy is improved from 75.7% to 91.5%, and the three targets of Semantic Unit Understanding Rate--Correction, Recall, and Precision are also improved 10% averagely.\n"
   ]
  },
  "yip04_iscslp": {
   "authors": [
    [
     "Wing Lin",
     "Yip"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Bilingual Response Generation using Semi-Automatically-Induced Templates for a Mixed-Initiative Dialog System",
   "original": "050",
   "page_count": 4,
   "order": 54,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "We have previously developed a framework for natural language response generation for mixed-initiative dialogs in the CUHK Restaurants domain [1]. This paper investigates the use of semi-automatic technique for response templates generation.  We adopt a semi-automatic approach for grammar induction [2] to capture the language structures of responses from unannotated corpora. We wish to use this approach to induce a set of grammars from our response data. The induced grammars are coupled with a parser to produce response templates in a semi-automatic way. Our response data consists of 2349 waiter responses. It is used as the training corpus for grammar induction. Unsupervised grammar induction is first performed, followed by using the learned grammars as prior knowledge for seeding the clustering process. Results show that the semi-automatically-induced response templates cover more than 50% of the hand-designed templates in templates coverage and provide more realization options. Performance evaluation indicates that the task completion rate has at least 90%, and most of the Grice’s maxims as well as the overall user satisfaction scored at 3.5 points or above.\n"
   ]
  },
  "liu04b_iscslp": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Shudong",
     "Huang"
    ],
    [
     "Chris",
     "Cieri"
    ],
    [
     "Lufeng",
     "Zhai"
    ],
    [
     "Benfeng",
     "Chen"
    ]
   ],
   "title": "Development of A Chinese Telephony Conversational Corpus for Speech Processing",
   "original": "095",
   "page_count": 4,
   "order": 55,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "This paper describes the development of the EARS (Effective, Affordable, Reusable Speech-to-text) Chinese corpus, a telephony conversational speech database for speech processing. The EARS database is the first of its kind collected for Mandarin Chinese telephony spontaneous speech. The purpose of developing this EARS Chinese corpus is to collect Mandarin conversations between either strangers or friends, which cover a wide range of topics, over landline and cellular channels. All the speech data are annotated with standard Chinese character transcription as well as specific mark-ups for spontaneous speech. This corpus will be used for conversational and spontaneous Mandarin speech recognition tasks, under the DAPRA EARS framework. This paper introduces the design, development, structure, and initial phonetic analysis of the first 50-hour collection of this corpus. Additional 300 to 500 hours of data will be collected and transcribed between 2004 and 2005.\n"
   ]
  },
  "wu04_iscslp": {
   "authors": [
    [
     "ChungHsien",
     "Wu"
    ],
    [
     "ChiChun",
     "Hsia"
    ],
    [
     "JiunFu",
     "Chen"
    ],
    [
     "TeHsien",
     "Liu"
    ]
   ],
   "title": "Variable-Length Unit Selection using Lsa-Based Syntactic Structure Cost",
   "original": "017",
   "page_count": 4,
   "order": 56,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "This paper introduces a variable-length unit selection method based on LSA-based syntactic structure for concatenative speech synthesis. First, a probabilistic context free grammar (PCFG) based parser is used to construct the syntactic structure of the input text sentence. Second, the synthesizer selects the candidate units for each node of the syntactic structure. Latent Semantic Analysis (LSA) is then adopted to estimate the syntactic cost between the target unit and the candidate units in the database. Finally, the concatenation of units with minimum cost is selected using dynamic programming algorithm. Experimental results show that variable-length unit selection based on syntactic structure outperforms the synthesizer without considering syntactic structure. Also, the LSA-based syntactic cost provides better estimation of substitution cost than that calculated only from acoustic features.\n"
   ]
  },
  "gu04c_iscslp": {
   "authors": [
    [
     "HungYan",
     "Gu"
    ],
    [
     "KuoHsian",
     "Wang"
    ]
   ],
   "title": "An Acoustic and Articulatory Knowledge Integrated Method for Improving Synthetic Mandarin Speech's Fluency",
   "original": "046",
   "page_count": 4,
   "order": 57,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "In synthetic Mandarin speech, discontinuity of formant traces at syllable boundaries is a key factor that lowers fluency level. Therefore, we study an acoustic and articulatory knowledge integrated method to solve this discontinuity problem. First, representative trisyllable contexts are selected and their signals are recorded. The middle syllable’s signal of each trisyllable pronunciation is then extracted to make a synthesis unit. To select a synthesis unit among multiple candidates, a distance function is defined to measure the spectral similarity between two synthesis units to be concatenated. In addition, several linking-restriction rules are derived, according to articulatory knowledge, to prevent some synthesis units being linked into a sequence. Then, a globally best synthesis-unit sequence is searched by using a dynamic programming based algorithm. When the method above is applied, the formant traces at syllable boundaries will become smoother. Also, subject evaluation shows that the fluency level of synthetic Mandarin speech can indeed be improved a lot.\n"
   ]
  },
  "fung04_iscslp": {
   "authors": [
    [
     "Tien Ying",
     "Fung"
    ],
    [
     "Yuk Chi",
     "Li"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "P.C.",
     "Ching"
    ]
   ],
   "title": "Prosody and Style Controls in CU VOCAL using SSML and SAPI XML Tags",
   "original": "091",
   "page_count": 4,
   "order": 58,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "CU VOCAL is a Cantonese text-to-speech (TTS) engine. We use a syllable-based concatenative synthesis approach to generate intelligible and natural synthetic speech in Cantonese.  This paper reports on our recent enhancements in CU VOCAL to support user adjustments in prosody and style with the use of the Speech Synthesis Markup Language (SSML) [1, 2] in the input text. CU VOCAL was previously developed as a SAPI-compliant engine to enable easy integration with other applications. This paper also reports on our enhancements in the CU VOCAL SAPI engine to support the SAPI 5 XML  tags [3].\n"
   ]
  },
  "li04c_iscslp": {
   "authors": [
    [
     "JianFeng",
     "Li"
    ],
    [
     "GuoPing",
     "Hu"
    ],
    [
     "Ming",
     "Fan"
    ],
    [
     "LiRong",
     "Dai"
    ]
   ],
   "title": "Apply Length Distribution Model to Intonational Phrase Prediction",
   "original": "025",
   "page_count": 4,
   "order": 59,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "In this paper, a length distribution model for intonational phrase prediction is proposed. This model presents the probabilities that a certain length sentence is divided into some certain length intonational phrases. We will discuss how to estimate the probabilities in the model from training corpus, and how to apply it to intonational phrase prediction. We combine this model with a maximum entropy model which implements local context information. Experiment results show that length distribution is valuable information for intonational phrase prediction, and that it is able to make significant extra contribution over the maximum entropy model in terms of average score and unacceptable rate.\n"
   ]
  },
  "tseng04b_iscslp": {
   "authors": [
    [
     "Chiuyu",
     "Tseng"
    ],
    [
     "Yehlin",
     "Lee"
    ]
   ],
   "title": "Intensity in Relation to Prosody Organization",
   "original": "101",
   "page_count": 4,
   "order": 60,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "Mandarin fluent speech prosody is most significantly characterized by phrase grouping. A hierarchical prosody framework of phrase grouping was proposed, where corresponding evidences of governing effects from the prosody organization were found in two acoustic correlates already, namely, overall F0 contours and temporal allocations [1]. In this paper, we present results of investigating through corpus analyses the third acoustic correlates, i.e., intensity, to look for corresponding evidence in relation to prosody organization as well. The specific questions raised are (1) how intensity pattern could be explained by the governing effect of prosody organization, (2) whether the governing effect could be used in predicting intensity distribution. We argue that the acoustic roles of speech rhythm and intensity are as much integrated part of speech prosody as F0 contour patterns. Therefore, we conclude that in order to construct a working prosody model, all three acoustic correlates should be considered in relation to prosody organization. The conclusion is also directly applicable to TTS to improve output naturalness. [2]\n"
   ]
  },
  "tao04_iscslp": {
   "authors": [
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Rhythm Correlation of Speech Synthesis System",
   "original": "104",
   "page_count": 4,
   "order": 61,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "There has been a rapid progress of speech synthesis, however it is still hard to make good objective evaluation of the speech intonation while training the speech synthesis system. Unlike the traditional method, Standard deviation of intonation, which normally makes the speech synthesis system sounds smooth and flat but with less expressiveness, the paper integrates the rhythm correlation in the evaluation based on the tangential intonation. Furthermore, the paper makes the comparing among three typical evaluation methods, Listening Test, Standard Deviation of Intonation, Standard Deviation of Intonation & Tangential Intonation. It proves the method introduced in the paper could generate better synthesis results than others with even less training corpus.\n"
   ]
  },
  "tang04b_iscslp": {
   "authors": [
    [
     "Yun",
     "Tang"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Trigram Duration Modeling in Speech Recognition",
   "original": "019",
   "page_count": 4,
   "order": 62,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "Rate of speech (ROS) is a very important factor in speech recognition. In this paper we present a new speech rate measure method which first normalizes the duration of different acoustic units to standard duration and then builds a trigram duration model to measure the speech rate of sentence. We propose two methods based on the standard duration to compensate the influence introduced by speech rate variation in data corpus and get 11% error rate reduction in mandarin digit string recognition..\n"
   ]
  },
  "xu04_iscslp": {
   "authors": [
    [
     "Chao",
     "Xu"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Yongsheng",
     "Yang"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "A System for Mandarin Short Phrase Recognition on Portable Devices",
   "original": "021",
   "page_count": 4,
   "order": 63,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "With the proliferation of portable devices, speech recognition, especially name, address and command recognition on these devices is a topic of growing relevance. A mandarin short phrase recognition system is introduced in consideration of the limited resources and calculation ability of portable devices. A fixedpoint front-end is developed, discrete hidden Markov model is employed for acoustic modeling, and a SNR based likelihood weighting method is proposed to improve the noise robustness of the system. The memory size of the model set is 269kB, the decoding time is 0.89 times of the speech duration, and the method for robustness gives a relative 15.2% word error rate reduction in a complex practical environment with both channel distortion and non-stationary noise presence.\n"
   ]
  },
  "peng04b_iscslp": {
   "authors": [
    [
     "Gang",
     "Peng"
    ],
    [
     "Hongying",
     "Zheng"
    ],
    [
     "William S.Y.",
     "Wang"
    ]
   ],
   "title": "Tone Recognition for Chinese Speech: A Comparative Study of Mandarin and Cantonese",
   "original": "022",
   "page_count": 4,
   "order": 64,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "This paper presents a comparative study on automatic continuous tone recognition for Mandarin and Cantonese. Compared with Mandarin, Cantonese has a much more complex tone system. The effects of �� normalization on tone recognition of Mandarin and Cantonese will be studied. Furthermore, the two tone systems will be compared from an engineering point of view. Tone recognition accuracies of 71.50% and 83.06% have been obtained for Cantonese and Mandarin respectively. These results compare favorably with results reported for other tone recognition experiments on the same (for Cantonese) and similar databases (for Mandarin).\n"
   ]
  },
  "you04_iscslp": {
   "authors": [
    [
     "ShanRuei",
     "You"
    ],
    [
     "ShihChieh",
     "Chien"
    ],
    [
     "ChihHsing",
     "Hsu"
    ],
    [
     "KeShiu",
     "Chen"
    ],
    [
     "JiaJang",
     "Tu"
    ],
    [
     "Jeng Shien",
     "Lin"
    ],
    [
     "SenChia",
     "Chang"
    ]
   ],
   "title": "Chinese-English Mixed-Lingual Keyword Spotting",
   "original": "031",
   "page_count": 4,
   "order": 65,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "Base on our former experience in the “ITRI 104 Auto Attendant System”[1] of using keyword spotting for Mandarin speech recognition, a Chinese-English mixedlingual keyword spotting for catering the speaking style of Taiwanese is present. Detailed descriptions and discussions for developing the mixed-lingual autoattendant system are included in this paper, especially for solving different scoring scales in the decoding phase and the re-scoring phase for these two languages. In the decoding phase, we propose a bias-compensation method to make up the score-gap in the likelihood calculation of using Chinese and English acoustic models. To select the most probable result from the recognized hypotheses, the method for normalizing the combination scores of using different scoring mechanisms in the re-scoring phase is also presented.\n"
   ]
  },
  "yang04b_iscslp": {
   "authors": [
    [
     "Jian",
     "Yang"
    ],
    [
     "Yuanyuan",
     "Pu"
    ],
    [
     "Hong",
     "Wei"
    ]
   ],
   "title": "An Acoustic-Phonetic Analysis of Large Vocabulary Continuous Mandarin Speech Recognition for Non-Native Speakers",
   "original": "032",
   "page_count": 4,
   "order": 66,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "This paper addresses non-native accent issues in large vocabulary continuous speech recognition. We propose to analyze the transformation rules of non-native Mandarin speech spoken by native speakers of Naxi and Dai in Yunnan at the level of initials and finals. Firstly, baseline HMM models are trained using the project 863’ standard Mandarin corpus to test their performance on non-native speech recognition. Secondly, the non-native speech data is transcribed based on the baseline HMM models. In more detail, we analyze the error recognition rates of all initials and all finals, and their typical substitute error. The results obtained from our experiments might be useful for adapting a native speaker ASR system to model nonnative accented data.\n"
   ]
  },
  "tang04c_iscslp": {
   "authors": [
    [
     "Yuezhong",
     "Tang"
    ],
    [
     "Xia",
     "Wang"
    ],
    [
     "Yang",
     "Cao"
    ],
    [
     "Feng",
     "Ding"
    ]
   ],
   "title": "Feature Masking in an Embedded Mandarin Speech Recognition System",
   "original": "035",
   "page_count": 4,
   "order": 67,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "In this paper, we explored a feature component masking scheme for an embedded tonal language recognition systems, in order to reduce the computational complexity with least degradation of recognition accuracy. We made a lot of experiments on a Mandarin isolated word recognition task with a tone-confusable vocabulary. Taking consideration of both clean and noisy conditions, we were able to find a masking scheme that filtered out 31 of 54 components and still outperformed the baseline with 54 components in feature set, with dramatically less computational and memory complexity. The results showed that feature masking was a promising approach for complexity reduction in embedded tonal language recognition systems. The results also verified the effectiveness of higher order cepstral coefficients for tonal language recognition because most of them were preserved during the feature masking experiments.\n"
   ]
  },
  "hwang04b_iscslp": {
   "authors": [
    [
     "TaiHwei",
     "Hwang"
    ],
    [
     "SenChia",
     "Chang"
    ]
   ],
   "title": "Energy Contour Enhancement for Noisy Speech Recognition",
   "original": "038",
   "page_count": 4,
   "order": 68,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "The environmental noise, known as an additive noise, not only corrupts the spectra of speech signal but also blurs the shape of energy contour. The corruption of energy contour can distort the energy derived feature and degrade the performance of pattern classification of noisy speech. To reduce the distortion of the energy feature, the energy bias in the energy contour has to be removed before the feature extraction. For the purpose, we propose two methods to estimate the noise energy; one is obtained from the speech inactive period, and one is from the noisy speech itself. The methods are evaluated by the connected digit recognition of TIDigits, in which the test speech is corrupted with the white noise, babble, factory noise, and in-car noises. As shown in the experiments, the energy enhancement can provide an additional improvement when it is jointly applied with a spectral subtraction.\n"
   ]
  },
  "liu04c_iscslp": {
   "authors": [
    [
     "Bo",
     "Liu"
    ],
    [
     "LiRong",
     "Dai"
    ],
    [
     "JinYu",
     "Li"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "Double Gaussian based Feature Normalization for Robust Speech Recognition",
   "original": "053",
   "page_count": 4,
   "order": 69,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "In this paper, a new feature normalization approach based on Cumulative Density Function (CDF) matching principle is proposed. Since speech features in noisy environments usually follow bimodal distributions, we fully utilize this characteristic by representing the CDF of the features with a double Gaussian model. Feature normalization process is performed according to the estimated CDF. The experimental results on Aurora2 database show that the performance of our method is much better than that of the conventional Mean and Variance Normalization (MVN) method, and comparable to that of the method combining the spectral subtraction and histogram equalization (HE). Moreover, further improvement has been gained by combining our method with a simple temporal feature smoothing process. This result suggests that our new method has the potential to be integrated with other techniques to provide even better performance.\n"
   ]
  },
  "chen04d_iscslp": {
   "authors": [
    [
     "C.L.",
     "Chen"
    ],
    [
     "Y.R.",
     "Wang"
    ],
    [
     "S.H.",
     "Chen"
    ]
   ],
   "title": "A Study on Mandarin Broadcast News Speech Recognition",
   "original": "057",
   "page_count": 4,
   "order": 70,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In this paper, a basic Mandarin broadcast news speech recognition system is constructed using the MATBN database. It considers the acoustic modeling for Mandarin base-syllables, particles, and paralinguistic phenomena. It also considers environment-dependent acoustic modeling for three recording environments: studio anchors, outdoor reporters, and outdoor interviewee. Moreover, it incorporates a bigram language model with adaptation using data in MATBN. Syllable recognition rates of 89.64, 84.42and 61.62% were achieved for the three environments of anchors, reporters and interviewees, respectively.\n"
   ]
  },
  "ding04b_iscslp": {
   "authors": [
    [
     "GuoHong",
     "Ding"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Xia",
     "Wang"
    ],
    [
     "Yang",
     "Cao"
    ],
    [
     "Feng",
     "Ding"
    ],
    [
     "Yuezhong",
     "Tang"
    ]
   ],
   "title": "Task-Specific Adaptation in Chinese Name Recognition",
   "original": "078",
   "page_count": 4,
   "order": 71,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "In this paper, task-specific adaptation is proposed to improve Chinese name recognition performance. Since acoustic models are usually trained using large vocabulary continuous speech corpora, there exists distortion between modeling and decoding in name recognition. To compensate the mismatch, task-specific adaptation, which is performed in the MLLR framework with multi-regression classes, is proposed. Experimental results show that task-specific adaptation is very effective in Chinese name recognition to compensate the mismatch.\n"
   ]
  },
  "luo04b_iscslp": {
   "authors": [
    [
     "Dongsheng",
     "Luo"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Integrating Tonal Information into Mandarin Name Recognition with Different Strategies",
   "original": "084",
   "page_count": 4,
   "order": 72,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Name recognition is a practical application of speech recognition technology. As Chinese is well known to be a tonal language, tonal information has important influence on this task. In this paper we integrate tonal information into a speaker-independent Mandarin name recognizer, and two combination strategies: feature combination and posterior combination are investigated firstly. The recognizer is evaluated on an extremely challenging Mandarin name corpus, which includes 100 tonally confusing pairs. Although a significant improvement in the recognition accuracy can be achieved with either strategy, the system has a poor flexibility. Based on the analysis of the experiment results we propose a two-step process to improve the system performance further. It is shown that a maximal improvement of 29.96% in word accuracy can be achieved. At the same time the system has a good flexibility with tonal information being integrated dynamically.\n"
   ]
  },
  "guo04b_iscslp": {
   "authors": [
    [
     "Gang",
     "Guo"
    ],
    [
     "RenHua",
     "Wang"
    ]
   ],
   "title": "Discriminative Transform for Confidence Estimation in Mandarin Speech Recognition",
   "original": "090",
   "page_count": 4,
   "order": 73,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "In automatic speech recognition (ASR) application, log likelihood ratio testing (LRT) is one of the most popular techniques to obtain confidence measure (CM). Unlike traditional (log likelihood ratio) LLR related method, we apply non-linear transformations towards LLRs before computing string-level CMs. Different phonemes may have different transformation functions. Through suitable LLR transformations, the verification performances of those string-level CMs may increase. Transformation functions are implemented by Multi Layer Perceptron (MLP). Two algorithms are used to optimize the parameters of MLPs: One is the Minimum Verification Error (MVE) algorithm [2]; another is the Figure-of-Merit (FOM) training algorithm [3]. In our mandarin command recognition system, the two methods remarkably improve the performances of confidence measures for out-ofvocabulary words rejection compared with the performances of standard LRT related CMs, and we obtain a best 45.5% relative reduction in equal error rate (EER). In addition, in our mandarin command recognition experiments, the FOM training algorithm outperforms the MVE algorithm even they share an approximately same best performance, while due to limited experimental setups in our experiments, which algorithm is the better still needs to be explored.\n"
   ]
  },
  "zhang04_iscslp": {
   "authors": [
    [
     "Michael",
     "Zhang"
    ],
    [
     "Jun",
     "Xu"
    ]
   ],
   "title": "An Investigation into Subspace Rapid Speaker Adaptaion",
   "original": "100",
   "page_count": 4,
   "order": 74,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "Speaker adaptation is an essential part of any state-of-the-art Automatic Speech Recognizer (ASR). Recently, more and more application requirements appear for embedded ASR. For these cases, a more compact speech model, Subspace Distribution Clustering Hidden Markov Model (SDCHMM) is used instead of Continuous Density Hidden Markov Model (CDHMM). In previous studies on SDCHMM adaptation, the subspace Gaussian pools of SDCHMM are the parameters to be adjusted for speaker variations. Alternatively, we try to employ the link table parameters of SDCHMM, which defines the tying structure in subspaces, to model the inter-speaker mismatch, with the Gaussian parameters maintained. Since the variation range for the parameters is highly limited, this method is potentially faster than conventional Gaussian pools adaptation. Comparative study on Continuous Digital Dialing (CDD) task shows that when data is seriously insufficient, link table adaptation is more effective than conventional methods, with 17% relative improvement in utterance accuracy rate, compared to 14% improvement by previous Gaussian adaptation. However, further improvement with more data is limited. When data size doubled, this method gave 21% improvement, compared to 30% improvement by conventional method.\n"
   ]
  },
  "yang04c_iscslp": {
   "authors": [
    [
     "Chen",
     "Yang"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "On Noise Robustness of Dynamic and Static Features for Continuous Cantonese Digit Recognition",
   "original": "103",
   "page_count": 4,
   "order": 75,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "It has been shown previously that augmented spectral features (static and dynamic cepstra) are effective for improving ASR performance in a clean environment. In this paper we investigate the noise robustness of static and dynamic cepstral features, in a speaker independent, continuous recognition task by using a noise-added, Cantonese digit database (CUDigit). We found that the dynamic cepstrum is more robust to additive, background noise than its static counterpart. The results are consistent across different types of noise and under various SNRs. Exponential weights which can exploit the unequal robustness of two features are optimally trained in a development set. A relative word error rate reduction of 41.9%, mainly on a significant reduction of insertions, is obtained on the test data under various noise and SNR conditions.\n"
   ]
  },
  "zheng04_iscslp": {
   "authors": [
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Jing",
     "Li"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Mingxing",
     "Xu"
    ]
   ],
   "title": "A Two-Step Keyword Spotting Method based on Context-Dependent A Posteriori Probability",
   "original": "049",
   "page_count": 4,
   "order": 76,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "Keyword weighting plays an important role in traditional keyword spotting (KWS) systems: it helps detect keyword candidates in an utterance so that they will not be missed. However, if the keywords are overweighted, there will be a high number of false alarms, which will slow down the system and might introduce rejection errors; on the other hand, if the keywords are inefficiently weighted, the detection rate is not guaranteed. It is difficult to make a compromise with regard to keyword weighting. A two-step KWS method based on context-dependent a posteriori probability (CDAPP) is proposed in this paper as a way to solve this problem. The first step adopts a continuous speech recognition method, to generate a sequence of acoustic symbols for the second step, which performs a fuzzy keyword search. Preliminary experiments show that the proposed strategy is a promising one that needs additional investigation.\n"
   ]
  },
  "cheng04_iscslp": {
   "authors": [
    [
     "JyhMin",
     "Cheng"
    ],
    [
     "HsiaoChuan",
     "Wang"
    ]
   ],
   "title": "A Method of Estimating the Equal Error Rate for Automatic Speaker Verification",
   "original": "041",
   "page_count": 4,
   "order": 77,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "In an automatic speaker verification (ASV) system, the equal error rate (EER) is a measure to evaluate the system performance. Usually it needs a large number of testing samples to calculate the EER. In order to estimate the EER without running the experiments using testing samples, a method of model-based EER estimation which computes likelihood scores directly from client speaker models and imposter models is proposed. However, the distribution of the computed likelihood scores is significantly biased against the distribution of likelihood scores obtained from testing samples. Here we propose a novel idea to manipulate the speaker models of the client speakers and the imposters so that the distribution of the computed likelihood scores is closer to the distribution of likelihood scores obtained from testing samples. Then a more reliable EER can be calculated by the speaker models. The experimental results show that the proposed method can properly estimate the EER.\n"
   ]
  },
  "zheng04b_iscslp": {
   "authors": [
    [
     "Rong",
     "Zheng"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Text-Independent Speaker Identification using GMM-UBM and Frame Level Likelihood Normalization",
   "original": "043",
   "page_count": 4,
   "order": 78,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "In this paper, we describe a Gaussian Mixture ModelUniversal Background Model (GMM-UBM) speaker identification system. In this GMM-UBM system, we derive the hypothesized speaker model by adapting the parameters of UBM using the speaker’s training speech and a form of Bayesian adaptation. The UBM technique is incorporated into the GMM speaker identification system to reduce the time requirement for recognition significantly. The paper also presents a new frame level likelihood score normalization for adjusting different scores of speaker models to get more robust scores in final decision. Experiments on the 2000 NIST Speaker Recognition Evaluation corpus show that GMM-UBM and frame level likelihood score normalization yield better performance. Compared to the baseline system, around 31.2% relative error reduction is obtained from the combination of both techniques.\n"
   ]
  },
  "chan04_iscslp": {
   "authors": [
    [
     "Joyce Y.C.",
     "Chan"
    ],
    [
     "P.C.",
     "Ching"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Detection of Language Boundary in Code-Switching Utterances by Bi-Phone Probabilities",
   "original": "056",
   "page_count": 4,
   "order": 79,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "In this paper, we present an effective method to detect the language boundary (LB) in code-switching utterances. The utterances are mainly produced in Cantonese, a commonly used Chinese dialect, whilst occasionally English words are inserted between Cantonese words. Bi-phone probabilities are calculated to measure the confidence that the recognized phones are in Cantonese. Two sets of context-independent mono-phone models are trained by monolingual Cantonese and monolingual English data separately. Both knowledge-based and data-driven model selection approaches are studied in order to retain the language-dependent characteristics and to merge duplicated phone sets between the two languages. The LB detection accuracy is 75.12% for utterances that contain one single codeswitching word or phrase.\n"
   ]
  },
  "qin04_iscslp": {
   "authors": [
    [
     "Chao",
     "Qin"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Cantonese Verbal Information Verification System using GMM-Based Anti-Model",
   "original": "075",
   "page_count": 4,
   "order": 80,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "Verbal information verification (VIV) is one of the approaches for speaker authentication [1]. It is a process in which the spoken utterance of a claimed speaker is verified against the key information in speaker’s registered profile. VIV in English has been extensively studied and there has also been some work on Mandarin VIV. In the paper, we study the VIV for users who speak Cantonese, the most commonly used dialect in Southern China and Hong Kong. We propose a new technique for anti-modeling. It uses contextindependent Gaussian Mixture Model (GMM) instead of the conventional Hidden Markov Model (HMM). Experiments on 50 Cantonese native speakers show that the proposed method provides better separation of verification scores of claimant utterances from that of impostor utterances than the HMM based method. An equal error rate of 0.00% is attained with robust interval up to 15%, which manifests an excellent performance.\n"
   ]
  },
  "pao04_iscslp": {
   "authors": [
    [
     "TsangLong",
     "Pao"
    ],
    [
     "YuTe",
     "Chen"
    ],
    [
     "JunHeng",
     "Yeh"
    ]
   ],
   "title": "Emotion Recognition from Mandarin Speech Signals",
   "original": "079",
   "page_count": 4,
   "order": 81,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "In this paper, a Mandarin speech based emotion classification method is presented. Five primary human emotions including anger, boredom, happiness, neutral and sadness are investigated. In emotion classification of speech signals, conventional features are statistics of fundamental frequency, loudness, duration and voice quality. However, the recognition accuracy of systems employing these features degrades substantially when more than two valence emotion categories are invoked. For speech emotion recognition, we select 16 LPC coefficients, 12 LPCC components, 16 LFPC components, 16 PLP coefficients, 20 MFCC components and jitter as the basic features to form the feature vector. A Mandarin corpus recorded by 12 non-professional speakers is employed. The recognizer presented in this paper is based on three recognition techniques: LDA, K-NN, and HMMs. Experimental results show that the selected features are robust and effective for the emotion recognition not only in the arousal dimension but also in the valence dimension.\n"
   ]
  },
  "wang04e_iscslp": {
   "authors": [
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Shaomin",
     "Wang"
    ],
    [
     "Russell",
     "Greiner"
    ],
    [
     "Dale",
     "Schuurmans"
    ],
    [
     "Li",
     "Cheng"
    ]
   ],
   "title": "Exploiting Syntactic, Semantic and Lexical Regularities in Language Modeling Via Directed Markov Random Fields",
   "original": "006",
   "page_count": 4,
   "order": 82,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "We present a directed Markov random ﬁeld (MRF) model that combines � -gram models, probabilistic context free grammars (PC FGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. The composite directed MRF model has potentially exponential number of loops and becomes context sensitive grammar, nevertheless we are able to estimate its parameters in cubic time using an efﬁcient modiﬁed EM method, the generalized inside-outside algorithm, which extends inside-outside algorithm to incorporate the effects of the � -gram and PLSA language models.\n"
   ]
  },
  "chueh04_iscslp": {
   "authors": [
    [
     "ChuangHua",
     "Chueh"
    ],
    [
     "JenTzung",
     "Chien"
    ],
    [
     "Hsinmin",
     "Wang"
    ]
   ],
   "title": "A Maximum Entropy Approach for Integrating Semantic Information in Statistical Language Models",
   "original": "069",
   "page_count": 4,
   "order": 83,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "In this paper, we propose an adaptive statistical language model, which successfully incorporates the semantic information into an n-gram model. Traditional n-gram models exploit only the immediate context of history. We first introduce the semantic topic as a new source to extract the long distance information for language modeling, and then adopt the maximum entropy (ME) approach instead of the conventional linear interpolation method to integrate the semantic information with the n-gram model. Using the ME approach, each information source gives rise to a set of constraints, which should be satisfied to achieve the hybrid model. In the experiments, the ME language models trained using the China Times newswire corpus achieved 40% perplexity reduction over the baseline bigram model.\n"
   ]
  },
  "chen04e_iscslp": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "WenHung",
     "Tsai"
    ],
    [
     "JenWei",
     "Kuo"
    ]
   ],
   "title": "Statistical Language Model Adaptation for Mandarin Broadcast News Transcription",
   "original": "020",
   "page_count": 4,
   "order": 84,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "This paper investigates statistical language model adaptation for Mandarin broadcast news transcription. A topical mixture model was proposed to explore the long-span latent topical information for dynamic language model adaptation. The underlying characteristics and various kinds of model complexities were extensively investigated, while their performance was verified by comparison with the conventional MAP-based adaptation approaches, which are devoted to extracting the short-span n-gram information. The speech recognition experiments were conducted on the broadcast news collected in Taiwan. Very promising results in both perplexity and word error rate reductions were initially obtained.\n"
   ]
  },
  "liu04d_iscslp": {
   "authors": [
    [
     "FuHua",
     "Liu"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Use of Direct Modeling in Natural Language Generation for Chinese and English Translation",
   "original": "010",
   "page_count": 4,
   "order": 85,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "This paper proposes a new direct-modeling-based approach to improve a maximum entropy based natural language generation (NLG) in the IBM MASTOR system, an interlingua-based speech translation system. Due to the intrinsic disparity between Chinese and English sentences, the previous method employed only linguistic constituents from output language sentences to train the NLG model. The new algorithm exploits a directmodeling scheme to admit linguistic constituent information from both source and target languages into the training process seamlessly when incorporating a concept padding scheme. When concept sequences from the top level of semantic parse trees are considered, the concept error rate (CER) is significantly reduced to 14.3%, compared to 23.9% in the baseline NLG. Similarly, when concept sequences from all levels of semantic parse trees are tested, the direct-modeling scheme yields a CER of 10.8% compared to 17.8% in the baseline. A sensible improvement on the overall translation is made when the direct-modeling scheme improves the BLEU score from 0.252 to 0.294.\n"
   ]
  },
  "wang04f_iscslp": {
   "authors": [
    [
     "JhingFa",
     "Wang"
    ],
    [
     "ShunChieh",
     "Lin"
    ],
    [
     "HsuehWei",
     "Yang"
    ]
   ],
   "title": "A New Two-Layer Approach for Spoken Language Translation",
   "original": "011",
   "page_count": 4,
   "order": 86,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "This study proposes a new two-layer approach for spoken language translation. First, we develop translated examples and transform them into speech signals. Second, to properly retrieve a translated example by analyzing speech signals, we expand the translated example into two layers: an intention layer and an object layer. The intention layer is used to examine intention similarity between the speech input and the translated example. The object layer is used to identify the objective components of the examined intention. Experiments were conducted with the languages of Chinese and English. The results revealed that our proposed approach achieves about 86% and 76% understandable translation rate for Chinese-toEnglish and English-to-Chinese translations, respectively.\n"
   ]
  },
  "zhang04b_iscslp": {
   "authors": [
    [
     "Yan",
     "Zhang"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Analysis of Paraphrased Corpus and Lexical-Based Approach to Chinese Paraphrasing",
   "original": "065",
   "page_count": 4,
   "order": 87,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "In this paper, we firstly analyze the language phenomena and distribution characteristics of Chinese spontaneous utterances already paraphrased by other approaches. Based on the information obtained from a corpus, our lexical-based approach is proposed to paraphrase Chinese spoken language. Our purpose is to transform various expressions into simplified expressions with the same meanings. Chinese verbs are the main constituents in sentences, and with their flexibility they play an important role in expressing structures, especially for transitive verbs. Furthermore, negative verb expressions also appear frequently to express enquiries in question utterances. Therefore, we design four types of paraphrasing templates based on lexical information and the characteristics of the corpus: (1) synonym replacement, (2) Chinese transitive verbs, (3) verbs with two objects, and (4) the transformation of negative expressions. Our experiment found that the lexical-based approach is effective for Chinese paraphrasing.\n"
   ]
  },
  "lee04_iscslp": {
   "authors": [
    [
     "LinShan",
     "Lee"
    ],
    [
     "ShunChuan",
     "Chen"
    ],
    [
     "Yuan",
     "Ho"
    ],
    [
     "JiaFu",
     "Chen"
    ],
    [
     "MingHan",
     "Li"
    ],
    [
     "Tehsuan",
     "Li"
    ]
   ],
   "title": "An Initial Prototype System for Chinese Spoken Document Understanding and Organization for Indexing/Browsing and Retrieval Applications",
   "original": "094",
   "page_count": 4,
   "order": 88,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "In the future, the network content will include all knowledge, information, services relevant to our daily life. The most attractive form of future network content will be multi-media, which usually includes voice information. As long as the voice information is included, it usually carries the core concepts for the content. As a result, the spoken documents associated with the multi-media content very possibly can serve as the key for indexing/browsing and retrieval. However, unlike the written documents, the multi-media or voice information are very often just audio/video signals. They are very difficult to index, browse or retrieve, since the users can't go through each of them from the beginning to the end during browsing. A possible approach then may be to segment the audio/video signals automatically into short paragraphs, each with a central concept or topic, and then automatically generate a title and/or a summary for each of these short paragraphs, in either speech or text form. The topics and central concepts described in the segmented short paragraphs are then further analyzed and organized into some graphic structures describing the relationships among these topics and central concepts. In this way, the multi-media content can be much more efficiently indexed automatically and browsed and retrieved by the user based on the title, summary and the graphic structure. This is referred to as the understanding and organization of spoken documents here. In this paper, an initial prototype system for such functions with broadcast news taken as the example multi-media content was presented. The graphic structure used to describe the relationships among the topics and central concepts are 2-dimensional tree structures developed based on the probabilistic latent semantic analysis.\n"
   ]
  },
  "hsieh04_iscslp": {
   "authors": [
    [
     "ChiaHsin",
     "Hsieh"
    ],
    [
     "ChienLin",
     "Huang"
    ],
    [
     "ChungHsien",
     "Wu"
    ]
   ],
   "title": "Spoken Document Summarization using Topic-Related Corpus and Semantic Dependency Grammar",
   "original": "009",
   "page_count": 4,
   "order": 89,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "This study presents a spoken document summarization scheme using a topic-related corpus and semantic dependency grammars. The summarization score considers speech recognition confidence, word significance, word trigram, semantic dependency grammar (SDG) and probabilistic context free grammar (PCFG). In addition, a topic-related corpus consisting of keywords as well as article is used to estimate the word significance score using latent semantic indexing (LSI). Semantic relations between words are determined by SDG using HowNet and Sinica Treebank. The dynamic programming algorithm is applied to decide the summarization ratio and look for the best summarization result according to summarization scores. Experimental results indicate that the proposed approach effectively extracts important words with semantic dependency and gives a promising speech summary.\n"
   ]
  },
  "chen04f_iscslp": {
   "authors": [
    [
     "JiangChun",
     "Chen"
    ],
    [
     "JuiLin",
     "Lo"
    ],
    [
     "JyhShing Roger",
     "Jang"
    ]
   ],
   "title": "Computer Assisted Spoken English Learning for Chinese in Taiwan",
   "original": "097",
   "page_count": 4,
   "order": 90,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "This paper proposes an approach to computer assisted spoken English learning for Mandarin Chinese speaking people in Taiwan. Various studies have suggested the importance of acoustic models for pronunciation assessment. For English and Chinese people, their mother tongues are different; therefore the corresponding spoken English, as well as their acoustic models, are also different due to subtle difference in pronunciation. The aim of this work is to have robust acoustic models and better phoneme segmentation in the recognition phase of the assessment. The proposed approach improves the speech recognition rate, leading to better reliability of HMM log-probability and the higher accuracy of phoneme segmentation. These two factors, in turn, contribute to the success of our pronunciation assessment system, as demonstrated in the experimental results.  1. INTRODUCTION  With the fast-growing computing power of personal computers and the advances in speech processing and recognition technologies, computer assisted language learning (CALL) has now become a useful tool to automatically assess a person’s pronunciation via computer software, especially for the second language (L2) learning. With the integration of automatic speech recognition (ASR) technology, a computerassisted pronunciation training (CAPT) system can even provide the feedback to the student and successful applications have been reported [6]. In general, a CAPT system requires the computer to evaluate the pronunciation quality using various speech features and derives a scoring function imitating human experts.  For text-dependent pronunciation assessment, we use four speech features, including magnitude, pitch contour, rhythm, and log-probability of Hidden Markov Model (HMM) [9]. A nonlinear regression method is also applied on the speech features to derive a parametric scoring function [4]. In particular, for the difference of pronunciation for native speaker and L2 learner, the design of acoustic model and the phoneme segmentation approach has been investigated and satisfied performance is achieved. In this work, we divide the pronunciation assessment into three parts:\n"
   ]
  },
  "seneff04_iscslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Mitchell",
     "Peabody"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Second Language Acquisition Through Human Computer Dialogue",
   "original": "068",
   "page_count": 4,
   "order": 91,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "This paper describes our recent research in developing tools for second language acquisition based on spoken dialogue interaction with a computer. We argue that language proﬁciency can best be achieved through active communication, and that the computer is very patient and provides a non-threatening environment in which to practice. We have adapted our pre-existing multilingual dialogue systems for this application, focusing in our initial prototype on an English-speaking student learning Mandarin within the weather domain. Two signiﬁcant new contributions are a Webbased interface for practice exercises to gain proﬁciency in carrying out a live conversation and a high-quality narrow-domain speech translation capability. In an evaluation on 695 spoken utterances drawn from a corpus of English weather data, our translation system produces an incorrect result less than 2% of the time, with a rejection rate of 8%.\n"
   ]
  },
  "li04d_iscslp": {
   "authors": [
    [
     "Haiping",
     "Li"
    ],
    [
     "Haixin",
     "Chai"
    ]
   ],
   "title": "An Information Gain and Grammar Complexity based Approach to Attribute Selection in Speech Enabled Information Retrieval Dialogs",
   "original": "004",
   "page_count": 4,
   "order": 92,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Effective dialog driven method is required for today’s speech enabled information retrieval systems, such as name dialer. Similar to dynamic sales dialog for electronic commerce scenarios, the information gain measure based approaches are widely used for attribute selection and dialog length reduction. However for speech enabled information retrieval systems, another important factor influencing attribute selection is speech recognition accuracy. Too low accuracy will result in a failed dialog. Recognition accuracy varies with many issues including acoustic model performance, grammar’s complexity. Acoustic model is fixed for a whole dialog, while grammar is different for each interaction round, thereby grammar complexity will influence the attribute selected for next question. In this paper, an approach combining both information gain measurement and grammar complexity is present for dynamic dialog driven. Offline evaluations show that this approach can give a trade-off of faster discriminating the candidates for retrieval target and higher recognition accuracy.\n"
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "huang04_iscslp",
    "juang04_iscslp",
    "wang04_iscslp"
   ]
  },
  {
   "title": "Tutorials",
   "papers": [
    "chou04_iscslp",
    "kuo04_iscslp"
   ]
  },
  {
   "title": "Speech Recognition (I)",
   "papers": [
    "hwang04_iscslp",
    "pan04_iscslp",
    "guo04_iscslp",
    "lo04_iscslp",
    "wang04b_iscslp",
    "zhou04_iscslp"
   ]
  },
  {
   "title": "Topics In Speech Science",
   "papers": [
    "dang04_iscslp",
    "tseng04_iscslp",
    "mok04_iscslp",
    "ding04_iscslp",
    "chen04_iscslp",
    "yuan04_iscslp"
   ]
  },
  {
   "title": "Speaker And Language Recognition",
   "papers": [
    "lim04_iscslp",
    "wang04c_iscslp",
    "ou04_iscslp",
    "leung04_iscslp",
    "yang04_iscslp",
    "bai04_iscslp"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "ling04_iscslp",
    "qi04_iscslp",
    "gu04_iscslp",
    "zhu04_iscslp",
    "tian04_iscslp",
    "kong04_iscslp"
   ]
  },
  {
   "title": "Speech Recognition (II)",
   "papers": [
    "zhu04b_iscslp",
    "chen04b_iscslp",
    "huang04b_iscslp",
    "huang04c_iscslp",
    "li04_iscslp",
    "tang04_iscslp"
   ]
  },
  {
   "title": "Topics In Spoken Language Processing",
   "papers": [
    "hoshino04_iscslp",
    "hoshino04b_iscslp",
    "hsu04_iscslp",
    "luo04_iscslp",
    "dong04_iscslp",
    "li04b_iscslp",
    "wang04d_iscslp",
    "bao04_iscslp",
    "bai04b_iscslp",
    "guilin04_iscslp",
    "hu04_iscslp",
    "dong04b_iscslp",
    "pin04_iscslp",
    "peng04_iscslp",
    "chen04c_iscslp",
    "zuo04_iscslp",
    "gu04b_iscslp",
    "liu04_iscslp",
    "yip04_iscslp",
    "liu04b_iscslp"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "wu04_iscslp",
    "gu04c_iscslp",
    "fung04_iscslp",
    "li04c_iscslp",
    "tseng04b_iscslp",
    "tao04_iscslp"
   ]
  },
  {
   "title": "Recognition Of Speech, Speaker and Language",
   "papers": [
    "tang04b_iscslp",
    "xu04_iscslp",
    "peng04b_iscslp",
    "you04_iscslp",
    "yang04b_iscslp",
    "tang04c_iscslp",
    "hwang04b_iscslp",
    "liu04c_iscslp",
    "chen04d_iscslp",
    "ding04b_iscslp",
    "luo04b_iscslp",
    "guo04b_iscslp",
    "zhang04_iscslp",
    "yang04c_iscslp",
    "zheng04_iscslp",
    "cheng04_iscslp",
    "zheng04b_iscslp",
    "chan04_iscslp",
    "qin04_iscslp",
    "pao04_iscslp"
   ]
  },
  {
   "title": "Language Modeling And Spoken Language Translation",
   "papers": [
    "wang04e_iscslp",
    "chueh04_iscslp",
    "chen04e_iscslp",
    "liu04d_iscslp",
    "wang04f_iscslp",
    "zhang04b_iscslp"
   ]
  },
  {
   "title": "Applications Of Spoken Language Processing Technology",
   "papers": [
    "lee04_iscslp",
    "hsieh04_iscslp",
    "chen04f_iscslp",
    "seneff04_iscslp",
    "li04d_iscslp"
   ]
  }
 ]
}