{
 "title": "2nd European Conference on Speech Communication and Technology (Eurospeech 1991)",
 "location": "Genova, Italy",
 "startDate": "24/9/1991",
 "endDate": "26/9/1991",
 "conf": "Eurospeech",
 "year": "1991",
 "name": "eurospeech_1991",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "2nd European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1991)",
 "date": "24-26 September 1991",
 "papers": {
  "furui91_eurospeech": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Recent advances in speech recognition",
   "original": "e91_0003",
   "page_count": 8,
   "order": 1,
   "p1": "3",
   "pn": "12",
   "abstract": [
    "This paper introduces recent research activities on speech recognition, ranging from acoustic processing to linguistic processing, at NTT (Nippon Telegraph and Telephone Corporation) Laboratories. These include the proposal of hierarchical cepstral parameters and ALSP parameters, a new method of utilizing pitch information, automatic speaker adaptation techniques, robust HMM phoneme models, new training algorithms for neural networks, linguistic processing using syntactic and semantic knowledge, implementation of prototype continuous speech recognition systems, and an efficient text-independent speaker recognition algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-1"
  },
  "fallside91_eurospeech": {
   "authors": [
    [
     "Frank",
     "Fallside"
    ]
   ],
   "title": "On the acquisition of speech by machines, ASM",
   "original": "e91_0013",
   "page_count": 1,
   "order": 2,
   "p1": "13",
   "pn": "14",
   "abstract": [
    "Most studies of speech processing by machine are concerned separately with the recognition and understanding of speech or with the synthesis of speech. The technical difficulties involved have resulted in the two fields being studied separately and, while well known common knowledge in phonetics and linguistics is drawn on by both, present day implementations of speech recognition and of speech synthesis are markedly different.\n",
    ""
   ]
  },
  "ramesh91_eurospeech": {
   "authors": [
    [
     "P.",
     "Ramesh"
    ],
    [
     "Jay G.",
     "Wilpon"
    ],
    [
     "M. A.",
     "McGee"
    ],
    [
     "David B.",
     "Roe"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ]
   ],
   "title": "Speaker independent recognition of spontaneously spoken connected digits",
   "original": "e91_0017",
   "page_count": 4,
   "order": 3,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "An important area of speech recognition is automatic recognition of connected digit strings (i. e. , sequences composed of the digits zero through nine, and oh). Applications of this technology include credit card authorization, catalog ordering, dialing of telephone numbers, and data entry. For the past two years AT&T has experimented with a system for automatic recognition of 10 digit merchant identification codes, and 15 digit customer credit card numbers, for the purpose of authorizing purchases charged to a credit card. Our evaluation used data collected from about 1,000 customers who provided 2,000 connected digit strings over 800-based dialed up telephone connections. The recognizer correctly recognized 97% of the strings with no rejections using constraints on the validity of both merchant identifications and credit card numbers. Several schemes for applying these task constraints in a practical implementation are discussed in this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-2"
  },
  "gopalakrishnan91_eurospeech": {
   "authors": [
    [
     "P. S.",
     "Gopalakrishnan"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "Immediate recognition of embedded command words",
   "original": "e91_0021",
   "page_count": 4,
   "order": 4,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "In this paper we describe a. method for recognizing command words in a real-time speaker dependent isolated word speech recognition system such as the one developed at IBM. In such a system a few words are designated as commands and these may occur interspersed with other words in dictation. Given a set of command word? and a few training tokens of each of the command words we construct special models for each of the command words and use a key-word spotting algorithm to identify occurrences of any of the command words very accurately. We are able to achieve near 98% detection rate with negligible false alarm rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-3"
  },
  "wilcox91_eurospeech": {
   "authors": [
    [
     "Lynn D.",
     "Wilcox"
    ],
    [
     "Marcia A.",
     "Bush"
    ]
   ],
   "title": "HMM-based wordspotting for voice editing and indexing",
   "original": "e91_0025",
   "page_count": 4,
   "order": 5,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "This paper describes a technique for speaker-dependent wordspotting based on hidden Markov models (HMM's). The technique allows a speaker to specify keywords dynamically and to train the associated HMM's via a single repetition of a keyword. Non-keyword speech is modeled using an HMM trained from a prerecorded sample of continuous speech. The wordspotter is intended for interactive applications, such as the editing of voice mail or mixed-media documents, and for keyword indexing in single-speaker audio or video recordings. The performance of the system was tested on the Resource Management Database using 25 ship names as keywords. The probability of correct keyword detection was . 88 when the probability of a false alarm occurring in a sentence was . 03.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-4"
  },
  "baker91_eurospeech": {
   "authors": [
    [
     "Janet M.",
     "Baker"
    ]
   ],
   "title": "Large vocabulary speaker-adaptive continuous speech recognition research overview at dragon systems",
   "original": "e91_0029",
   "page_count": 4,
   "order": 6,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "This paper presents a status report on Dragon Systems' large vocabulary, speaker-adaptive, continuous speech recognition research. Major system components and their algorithmic approaches will be described, as well as application tasks and performance on these. At present, vocabularies of greater than 1000 words up to 5000 words simultaneously active are running non-realtime on a 33 MHz 486 personal computer. Natural language tasks used range from highly specialized domains, such as mammography transcription (perplexity about 60), and general radiology (perplexity about 140), to broad encyclopedia dictation (perplexity about 430). Specialized tools created for development and graphical error analysis will also be described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-5"
  },
  "sgardoni91_eurospeech": {
   "authors": [
    [
     "Victoria",
     "Sgardoni"
    ],
    [
     "Dimitrios A.",
     "Gaganelis"
    ],
    [
     "Eleftherios D.",
     "Frangoulis"
    ]
   ],
   "title": "Continuous density HMM context dependent phones for speech recognition over the telephone",
   "original": "e91_0033",
   "page_count": 4,
   "order": 7,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "In this paper we report on a novel approach for obtaining context dependent phone models for isolated word recognition over the telephone. A simple and efficient adaptation technique based on statistical classification is used to adapt a set of context independent phone models to a new custom vocabulary. Initial results indicated that the proposed methodology can achieve a high recognition performance using a small number of examples from the new vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-6"
  },
  "shirai91_eurospeech": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "K.",
     "Hashimoto"
    ],
    [
     "T.",
     "Kobayashi"
    ]
   ],
   "title": "Text-to-speech synthesizer using superposition of sinusoidal waves generated by synchronized oscillators",
   "original": "e91_0039",
   "page_count": 4,
   "order": 8,
   "p1": "39",
   "pn": "42",
   "abstract": [
    "A new speech synthesis method utilizing mutually synchronized oscillators is proposed and the possibilities of its application to text-to-speech systems are discussed. The voiced speech has the line spectrum structure and can be represented by the superposition of sinusoidal waves. In our system, these sinusoidal waves are generated by a group of mutually synchronized oscillators which are realized by numerical solutions of non-linear differential equations. This method has some characteristics as follows. (1) Voiced and voiceless sounds can be generated in a same framework to operate sinusoidal oscillators in parallel. (2) Since the phase and the power information of each sinusoidal wave can be easily controlled, if necessary, periodic waveforms in the voiced sounds can be precisely reproduced in the time domain. (3) The pitch frequency and phoneme duration can be easily changed without degradation of original sound quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-7"
  },
  "guerti91_eurospeech": {
   "authors": [
    [
     "M.",
     "Guerti"
    ],
    [
     "G.",
     "Bailly"
    ]
   ],
   "title": "Synthesis-by-rule using compost: modelling resonance trajectories",
   "original": "e91_0043",
   "page_count": 4,
   "order": 9,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "The particularity of this study resides in the spectral representation space. Resonances are defined as the formant frequencies but their order is modified so that spectral continuities are respected: the affiliation of these resonances to certain cavities may produce formant crossings for some sound pairs. Analysis of natural vowel-vowel transitions uttered by two male French speakers was performed, using cepstral and pitch synchronous LPC at the closed phase of the glottis. Resonance trajectories were obtained by the interpolation of target resonances with emergence functions obtained by the Temporal Decomposition Technique. Vocalic resonance targets were extrapolated using a continuity criterion and a one-sound one-target rule. Based on this representation, rules will be given, for describing coarticulation of [b], [d] and [g] in VCV context. These rules have been implemented and tested by a rule-compiler: COMPOST. The synthesizer is an extended version of Klatt's parallel configuration: this includes a model of Fant's voice source.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-8"
  },
  "ishikawa91_eurospeech": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Neural network based spectral interpolation method for speech synthesis by rule",
   "original": "e91_0047",
   "page_count": 4,
   "order": 10,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "In this paper, we describe a neural network based spectral interpolation method for speech synthesis by rule. In this method, two types of artificial neural networks are used. One is neural network for phoneme recognition, and another is for spectral synthesis. A recognition network performs mapping a spectrum onto a vector of which elements represent similarities to each phoneme ( phonemic vector ). A spectral synthesis network performs inverse transformation of a recognition network, maps phonemic vector onto a spectrum. At boundary of synthesis units, two phonemic vectors are obtained by a recognition network, and interpolation between these vectors are performed, then the spectra of interpolated segment are generated by a spectral synthesis network. We compare the spectral feature of our method and linear interpolation. Spectral distortion by proposed method is greater than linear interpolation, but very natural formant transition is obtained by our method. And synthetic speech is natural. Also, we show that the proposed method is able to generate various types of coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-9"
  },
  "garnierrizet91_eurospeech": {
   "authors": [
    [
     "Marine",
     "Garnier-Rizet"
    ]
   ],
   "title": "A rule-based segmental synthesis module for French",
   "original": "e91_0051",
   "page_count": 4,
   "order": 11,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "The present paper describes a module for rule-based segmental synthesis of French, i. e the module which converts an allophonic string into an acoustic parameter file to be sent to the synthesizer. This study used tools and procedures originally developed for a Dutch text-to-speech system and was conducted in the multilingual synthesis framework of the POLYGLOT project. (ESPRIT project 2104).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-10"
  },
  "fraser91_eurospeech": {
   "authors": [
    [
     "Norman M.",
     "Fraser"
    ],
    [
     "G. Nigel",
     "Gilbert"
    ]
   ],
   "title": "Effects of system voice quality on user utterances in speech dialogue systems",
   "original": "e91_0057",
   "page_count": 4,
   "order": 12,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "This paper reports a Wizard of Oz simulation of a spoken dialogue system. 100 human-human flight inquiry dialogues were analysed and used as the basis of simulation scenarios. Subjects were given the scenarios and instructed to find the specified information. They were not told anything about the nature of the service but they concluded that they were talking to a computer. 100 simulation dialogues were collected and analyzed. Significant differences were found between the corpora. In general, phenomena in the simulation corpus were more amenable to analysis by computer. Since the only significant difference between the two conditions was the voice quality of the information provider, this suggests that 'natural' quality speech should be avoided in favour of noticeably synthetic speech in dialogue systems. Keywords: dialogue understanding; Wizard of Oz simulation; speech synthesis; Sundial project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-11"
  },
  "day91_eurospeech": {
   "authors": [
    [
     "P.",
     "Day"
    ],
    [
     "A.",
     "Grünupp"
    ],
    [
     "K.-P.",
     "Muthig"
    ]
   ],
   "title": "A human factors study of speech-to-text technology: consequences of discrete speech",
   "original": "e91_0061",
   "page_count": 4,
   "order": 13,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "Three experiments are reported which aimed at analyzing the learnability of discrete speech as well as the joint influences of discrete speech, modality and location of input commands and speech velocity on performance (composing standardized business letters; reading short texts). Results reveal that discrete speech can be learned quickly, and that a considerable amount of additional tasks can be managed while speaking discretely. Some consequences of these results are discussed which may be relevant for the design of interfaces to speech-to-text systems. Keywords: Discrete speech; Human Factors; interface design; speech recognition; speech-to-text technology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-12"
  },
  "murray91_eurospeech": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "John L.",
     "Arnott"
    ],
    [
     "Alan E.",
     "Newell"
    ]
   ],
   "title": "A comparison of document composition using a listening typewriter and conventional office systems",
   "original": "e91_0065",
   "page_count": 4,
   "order": 14,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "An experiment which compared and contrasted different means of document composition is described. The systems compared were a fully speech-driven word processor (accepting only spoken text input and editing commands), a simple keyboard-driven word processor and a dictating process. Rates of composition were investigated for the different composition methods. Keywords: word processing; document composition; speech recognition; dictation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-13"
  },
  "vossen91_eurospeech": {
   "authors": [
    [
     "Paulus H.",
     "Vossen"
    ]
   ],
   "title": "Evaluating speech input and output in a CAD-system using the hidden-operator method",
   "original": "e91_0069",
   "page_count": 4,
   "order": 15,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "This paper describes the use of the hidden-operator technique, also known as the Wizard-of-Oz technique, in the context of development of speech input for an existing software tool, normally operated by function keys over a keyboard. The program allows the conversion of CAD drawings in a form suitable for CNC machines. Part of the conversion can be done automatically, but other aspects of the conversion require professional knowledge. The results so far confirm our belief, that the hidden-operator technique is an effective method for detecting in an early phase of design user problems and user reactions on simulated perfect speech recognition. Keywords: speech assessment, formative evaluation, hidden-operator technique, Wizard of Oz, audio-visual protocolling, multimodal interaction, CAD\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-14"
  },
  "zajicek91_eurospeech": {
   "authors": [
    [
     "M.",
     "Zajicek"
    ],
    [
     "J.",
     "Hewitt"
    ]
   ],
   "title": "Mixed mode input for a standard wordprocessor. investigating links between input mode, speech and keyboard, and specific task areas",
   "original": "e91_0073",
   "page_count": 4,
   "order": 16,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "The aim of recent work by the authors has been to increase the usability of a speech driven interface. This paper addresses an area that could contribute significantly to the usability of the interface, that of mixed mode input, Using the standard word processor WordPerfect, this paper investigates how speech can be used in conjunction with keyboard for the three task categories, text creation/editing, wordprocessor commands, and navigation. We felt that for certain task categories the best use of speech was not to map keystrokes, but to apply it to higher level task models. The experiments described compare the use of speech for navigation and commands only, in conjunction with total speech input, and total keyboard. Keywords: Speech, Word Processing, Usability, Task Models\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-15"
  },
  "lockwood91_eurospeech": {
   "authors": [
    [
     "P.",
     "Lockwood"
    ],
    [
     "J.",
     "Boudy"
    ]
   ],
   "title": "Experiments with a non-linear spectral subtractor (NSS), hidden Markov models and the projection, for robust speech recognition in cars",
   "original": "e91_0079",
   "page_count": 4,
   "order": 17,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "Achieving reliable performance in speech recognition in the car is an important challenge especially in the context of mobile telephony applications where the user can access the telephone functions by voice. The break through such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a full handfree mode. This paper adresses the problem of speaker-dependent discrete utterance recognition in the car-noise environment and mismatch context (training phase made in silence, recognition in noise). Experimental results are reported. The performance of a HMM-based recogniser rise from 31% (no compensation) to 98% after speech enhancement. More than 2000 utterances recorded in various conditions of noise have been used to test the system. This is achieved by the use of robust training/recognition schemes and by preprocessing the noisy speech by a novel non-linear spectral subtraction technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-16"
  },
  "lockwood91b_eurospeech": {
   "authors": [
    [
     "P.",
     "Lockwood"
    ],
    [
     "C.",
     "Baillargeat"
    ],
    [
     "J. M.",
     "Gillot"
    ],
    [
     "J.",
     "Boudy"
    ],
    [
     "G.",
     "Faucon"
    ]
   ],
   "title": "Noise reduction for speech enhancement in cars: non-linear spectral subtraction / kalman filtering",
   "original": "e91_0083",
   "page_count": 4,
   "order": 18,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "Achieving a good quality of speech transmission and reliable performance in speech recognition in the car is an important challenge especially in the context of mobile telephony applications where the user can access the telephone functions by voice. The break through such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a full handfree mode. This paper compares approaches enhancing noisy speech with applications both for speech transmission and speech recognition, two approaches have been retained: a temporal approach based on Kalman filtering and a frequential approach based on spectral subtraction. These methods have been extended further yielding new algorithms for speech processing in noise. We report comparative experiments made with various implementations of the speech enhancers. These include a non-linear spectral subtracter [12] and several implementations of a Kalman filter, including new noise whitening schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-17"
  },
  "fellbaum91_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Fellbaum"
    ],
    [
     "Dieter",
     "Becker"
    ]
   ],
   "title": "Isolated word recognition with integrated noise reduction",
   "original": "e91_0087",
   "page_count": 4,
   "order": 19,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "In this paper the design of 'robust' speech recognizers are discussed. Basic principles of noise reduction in connection with speech recognizers are presented. For the noise reduction a one-channel system with spectral subtraction, a two-channel system with adaptive filtering and a four-channel system with adaptive filtering were used. Each of the algorithms are optimized for different SNR's and application cases. The paper describes the noise reduction systems, their integration into the recognizer, different strategies to optimize the filter parameters of the noise reduction filter and it summarizes some preliminary recognition results. Keywords: Speech Recognition, Speech Enhancement, Noise Compensation, Spectral Subtraction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-18"
  },
  "hernando91_eurospeech": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "A comparative study of parameters and distances for noisy speech recognition",
   "original": "e91_0091",
   "page_count": 4,
   "order": 20,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "Speech recognition in noisy environments remains an unsolved problem even in the case of isolated word recognition with small vocabularies. Recently, several techniques have been proposed to alleviate this problem. Concretely, the Short-Time Modified Coherence (SMC) parameterization and the Cepstral Projection Distortion (CPD) measure have shown excellent results when tested in a speech recognition system based on Dynamic Time Warping (DTW) and using speech contaminated by additive white noise. In this paper, a new technique based on the AR modeling of the one-sided autocorrelation sequence (OS ALPC) is presented and, from a comparative study of these LPC-based techniques in the Hidden Markov Model (HMM) approach, two main conclusions are attained: 1) the slope cepstral window and a relatively high model order are preferable, and 2) the cepstral representation based on the autocorrelation (rather on the signal) modeling achieves excellent results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-19"
  },
  "laaksonen91_eurospeech": {
   "authors": [
    [
     "Jorma T.",
     "Laaksonen"
    ]
   ],
   "title": "A new reliability-based phoneme segmentation method for the \"neural\" phonetic typewriter",
   "original": "e91_0097",
   "page_count": 4,
   "order": 21,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "This paper describes a new segmentation algorithm for the \"Neural\" Phonetic Typewriter developed at Helsinki University of Technology. The algorithm is based on use of homogeneity and length of speech segments as their reliability parameters. The maximum reliability is sought b}r merging short segments, which increases individual segment lengths and possibly decreases their homogeneity. The merging leads to formation of tree structure. The final stage of the algorithm is the search for an optimal path through it. Before segmentation, vector-quantized representation of speech is produced with a codebook created by Learning Vector Quantization. Each codebook vector is associated with another vector describing probabilities of the phonemic classes. The sum of these phoneme probability vectors within a segment determines its classification and homogeneity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-20"
  },
  "apolloni91_eurospeech": {
   "authors": [
    [
     "Bruno",
     "Apolloni"
    ],
    [
     "Francesco",
     "Pazienti"
    ],
    [
     "Vincenzo",
     "Trotta"
    ]
   ],
   "title": "Isolated word adaptive recognizer based on neural networks",
   "original": "e91_0101",
   "page_count": 4,
   "order": 22,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "This paper describes a general approach for implementing an isolated word recognizer based on neural networks. The target is the realization of a machine able to expand its knowledge with semi-automatic procedures, following the requirements of the environment. The vocabulary of the recognizer could be initially limited to few words selected by the constructor, then new samples can be added at run time. The growth of the vocabulary can be due to unaspected utterances of the same word, within the purpose of a speaker independent small vocabulary, or to new words, within the purpose of a speaker dependent large vabulary. Whenever the recognition is dubious the machine answers \"I don't know\", and starts a sequence of procedures intended to modify the lay out of the system, in order to allocate a larger vocabulary by a small interaction with the user. Keywords: speach recognition; voice signal compression; neural networks; back-propagation; learning algorithms; Kohonen maps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-21"
  },
  "hataoka91_eurospeech": {
   "authors": [
    [
     "Nobuo",
     "Hataoka"
    ],
    [
     "Alex H.",
     "Waibel"
    ]
   ],
   "title": "Evaluation of speaker-independent phoneme recognition on TIMIT database using TDNNs",
   "original": "e91_0105",
   "page_count": 4,
   "order": 23,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "This paper describes evaluation results and a new structure of Time-Delay Neural Networks (TDNN) for speaker-independent and context-independent phoneme recognition. The proposed new structure is based on the integration of TDNNs which have several TDNNs separated according to the duration of phonemes, so that it deals with phonemes of varying duration more effectively. In the experimental evaluation of the proposed new structure, 16-English vowel recognition was performed using 5268 vowel tokens picked from 480 sentences spoken by 140 speakers (98 males and 42 females) on the TIMIT (TI-MIT) database. A 60. 5% recognition rate, which was improved from 56% in the single TDNN structure, and stability improvement of recognition rate showed the effectiveness of the proposed integrated TDNNs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-22"
  },
  "morgan91_eurospeech": {
   "authors": [
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "C.",
     "Wooters"
    ],
    [
     "Phil",
     "Kohn"
    ],
    [
     "M.",
     "Cohen"
    ]
   ],
   "title": "Phonetic context in hybrid HMM/MLP continuous speech recognition",
   "original": "e91_0109",
   "page_count": 4,
   "order": 24,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "Earlier work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for a Hidden Markov Model (HMM) [1][2][3]. In these reports, we have shown that these estimates have led to improved performance over counting estimation techniques in the case where a fairly simple HMM was used. However, current state-of-the-art continuous speech recognizers require HMMs with greater complexity, e. g. multiple densities per phone and/or context-dependent phone models. Brute-force application of our earlier techniques to triphones (the standard approach to context-dependent HMMs) would result in an output layer with many thousands of units, and many millions of connections to train. In this report we describe another approach to the application of MLPs to context-dependent probability density estimation, as well as some practical aspects of efficient implementation of the method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-23"
  },
  "andrews91_eurospeech": {
   "authors": [
    [
     "E. C.",
     "Andrews"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "Neural network classification of complex-valued speech features",
   "original": "e91_0113",
   "page_count": 4,
   "order": 25,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "Most speech features are inherently complex but usually only their magnitude is considered in terms of spectral distortion measures. DFT and cepstral spectra are typical examples of this, where the phase information is usually thought to be of little value and is therefore discarded. This paper describes a new form of neural network that is inherently complex. We propose its use in applications where the input to a pattern recognition task contains complex information, and choose the task of speaker verification. The complex feature we consider here is the DFT of cepstral time series spanning a single utterance. In generating such features we show the effects of sampling rate and aliasing on the 2D mel-cepstra. The role of the non-linearity in the complex network is of paramount importance. We propose functions suitable for this case, since the standard sigmoid is inappropriate. To evaluate this new structure the task of speaker verification is chosen. Preliminary results are promising, supporting the case for the complex net.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-24"
  },
  "norris91_eurospeech": {
   "authors": [
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Rewiring lexical networks on the fly",
   "original": "e91_0117",
   "page_count": 4,
   "order": 26,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "A back-propagation network with recurrent connections can successfully model many aspects of human spoken word recognition [1] [2]. However, the network is unable to revise its decisions in the light of subsequent context. TRACE [3] , on the other hand, manages to deal appropriately with following context but only by using a highly implausible architecture that fails to account for some important experimental results. A new model is presented which combines the more desirable properties of these two models. In contrast to TRACE the model is entirely bottom-up and can readily perform simulations with vocabularies of tens of thousands of words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-25"
  },
  "elenius91_eurospeech": {
   "authors": [
    [
     "K.",
     "Elenius"
    ],
    [
     "G.",
     "Takacs"
    ]
   ],
   "title": "Phoneme recognition with an artificial neural network",
   "original": "e91_0121",
   "page_count": 4,
   "order": 27,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "An artificial neural network has been trained to recognize phonemes using the error back-propagation technique. First a coarse feature network is trained to extract seven quasi-phonetic features from the spectral frames of a Bark-scaled filter bank. The outputs of this net and the spectral outputs of the filter bank were input to a phoneme recognition net. The coarse features were recognized with 80% - 93% accuracy. Using manual segmentation the phone recognition rate was 64% and in 82% of the cases, the correct phone was among the best three candidates. Keywords: speech recognition; phoneme recognition; backword propagation; artificial neural networks\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-26"
  },
  "jianxin91_eurospeech": {
   "authors": [
    [
     "Jiang",
     "Jianxin"
    ],
    [
     "Yi",
     "Kechu"
    ],
    [
     "Hu",
     "Zheng"
    ]
   ],
   "title": "A new self-organization algorithm of forming a phoneme map",
   "original": "e91_0125",
   "page_count": 4,
   "order": 28,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "In this paper,based on Kohonen's SOFM model, a new learning algorithm is given, in which the lateral feedback effects are strengthened. Experements have shown that this algorithm speeds up the processing of producing a topographically ordered feature map and improves the clustering effect. Keywords: Neural Networks; Self-Organization; Lateral Feedback; Differential Competitive Learning; Phoneme Map\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-27"
  },
  "ran91_eurospeech": {
   "authors": [
    [
     "Shuping",
     "Ran"
    ],
    [
     "J. Bruce",
     "Millar"
    ]
   ],
   "title": "Phoneme classification using neural networks based on acoustic-phonetic structure",
   "original": "e91_0129",
   "page_count": 4,
   "order": 29,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "Experiments in phoneme recognition using a system of hierarchically organised connectionist networks are reported. The hierarchy of the system is based on the acoustic-phonetic structure of speech. The architecture of each level was designed following the principle that the complexity of the decision boundaries achieved by a multi-layer perceptron classifier depends on the complexity of its architecture. Low complexity classifiers are used to provide two levels of binary feature classification providing four classes of speech signal segments. Two of these classes are then used to provide input into more complex classification procedures for vowels and consonants. The performance of the system in classifying the consonants and vowels in a restricted set of monosyllables is described. Keywords: Phonetic Structure; Speech Analysis; Neural Networks; Speech Recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-28"
  },
  "dodd91_eurospeech": {
   "authors": [
    [
     "Nigel",
     "Dodd"
    ],
    [
     "Donald",
     "Macfarlane"
    ],
    [
     "Chris",
     "Marland"
    ]
   ],
   "title": "Networks for speech recognition structurally optimised by genetic techniques implemented on parallel hardware",
   "original": "e91_0133",
   "page_count": 3,
   "order": 30,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "Artificial Genetic algorithms have been shown to be effective in searching the space of networks. This paper describes the implementation of a family of such algorithms on various parallel computers consisting of Transputers. The benchmark problem chosen was the, so-called, ee-set taken from British Telecom's Connex database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-29"
  },
  "pittam91_eurospeech": {
   "authors": [
    [
     "J.",
     "Pittam"
    ],
    [
     "J.",
     "Ingram"
    ]
   ],
   "title": "Influence of vietnamese tone and prosody on the acquisition of English stress patterns",
   "original": "e91_0139",
   "page_count": 4,
   "order": 31,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "This paper is taken from a longitudinal study of accent change in Vietnamese refugees learning Australian English. The speech of four subjects was examined across a two year period starting from shortly after arrival in Australia. Samples of spontaneous conversations with the investigators were analysed prosodically using a custom-designed acoustic editor and accompanying data base and annotation programs. The subjects' L2 prosody was examined acoustically and auditorally to find evidence of interference from the Vietnamese tonal and stress systems. Results are discussed in two ways: (a) possible influences on both L2 word and sentence stress from the Vietnamese prosodic systems, and in particular, (b) the influence of the 'sac' tone. In both cases, the changes that were observed in subjects' speech across the two year period, and the particular environments affected are also examined. Keywords: Vietnamese-Australians; Prosody; Acquisition of English; Tonal Influence; Stress Patterns; Syllable Structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-30"
  },
  "sendlmeier91_eurospeech": {
   "authors": [
    [
     "Walter F.",
     "Sendlmeier"
    ]
   ],
   "title": "The voiced/unvoiced distinction of initial stops by normal and hearing impaired listeners",
   "original": "e91_0143",
   "page_count": 4,
   "order": 32,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "The question is raised in how far hearing impaired subjects process the relatively robust 'voicing* feature differently from normal listeners. Starting from the German words \"Deich\", \"Teich\" and \"Eich\", twelve different stimuli were produced by using a splicing technique in which the cues 'burst', 'aspiration', 'VOT' and 'vowel-onset' were interchanged, deleted or added. For both groups of listeners, the VOT only played a minor role in the discrimination of the voicing feature. A difference between the two groups was found when the burst of the /d/ was substituted by the burst of the /t/. In this case, a clear majority of the normal listeners heard a /t/, while the hard of hearing still recognized a /d/. Starting from the stimulus \"Teich\", both the burst and the aspiration were sufficient to evoke the judgement 'voiceless stop'. But again the burst was less important for the hearing impaired subjects. Altogether, the aspiration seemed to play the dominant role in all listeners'judgements. Keywords: speech perception, recognition of stops, comparison of normal and hearing impaired listeners, auditory feature analysis, acoustic analysis of speech signals, auditory training, perception strategies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-31"
  },
  "nathan91_eurospeech": {
   "authors": [
    [
     "Krishna S.",
     "Nathan"
    ]
   ],
   "title": "Comparison of formant transition based stop classifiers: time-varying and time-invariant signal models",
   "original": "e91_0147",
   "page_count": 4,
   "order": 33,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "A feature set that captures the dynamics of formant transitions is utilized to classify the unvoiced stop consonants. The second formant and its slope are used to characterize the transition between the vowel and closure in a VCV environment. The performance of a feature set obtained by means of a time-varying, data-selective model for the signal is compared with that of a standard time-invariant (LPC) and data-selective, time-invariant models. The different feature sets are evaluated on a database consisting of 10 talkers. A two-fold reduction in the error rate is obtained by means of the time-varying model. The performance of three different classifiers is presented. A novel adaptive algorithm, termed Learning Vector Classifier, is compared with standard K-means and LVQ2 classifiers. Speaker independent error rates of 11% are obtained for the 3-way classification task. Further improvements, expected when an expanded time-varying feature set is coupled with information from the burst, could lead to very high quality stop classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-32"
  },
  "benoit91_eurospeech": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "L. J.",
     "Roe"
    ]
   ],
   "title": "The effect of context on labiality in French",
   "original": "e91_0151",
   "page_count": 6,
   "order": 34,
   "p1": "151",
   "pn": "156",
   "abstract": [
    "The lip-jaw space of a French speaker was studied in various contextual conditions for facial animation purposes within the scope of tcxt-to-visual-spccch synthesis. The corpus included the 14 French vowels and six major consonants (/b/, /v/, /z/, /3/, /I/, /R/) in all possible combinations of preceding and following vocalic contexts with /i/, /a/, and /y/. These sounds were realized within a quotation-like carrier sentence. The speech material was simultaneously recorded on an audio and a video support. Both front and profile views of the speaker's face were archived and each phoneme was labelled on the center of its acoustic realization. Nine repetitions of 86 differing allophones were selected and their corresponding video frames were then automatically analysed so that 14 geometric or anatomic parameters were measured on each frame. The analysis of results here presented led us to a structural description of the French labial space that shows the importance of the context on the visual production of allophones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-33"
  },
  "datta91_eurospeech": {
   "authors": [
    [
     "A. K.",
     "Datta"
    ],
    [
     "N. R.",
     "Ganguli"
    ],
    [
     "B.",
     "Mukherjee"
    ]
   ],
   "title": "Nasalisation in bengali speech sounds acoustic-phonetic study",
   "original": "e91_0157",
   "page_count": 4,
   "order": 35,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "The paper presents a spectrographic study of the acoustic phonetics aspect of Bengali nasal speech sounds in three different cases namely, nasal murmurs , nasalisation of vowels due to contextual effects of nasal murmurs and purely nasal vowels. Data base consists of 100 and 14 Bengali words pronounced by 3 male and 1 female informants. The analysis of formant structure of nasal vowels and nasal murmurs are presented A formant frequency shift is noted due to nasalisation in pure nasal vowels. Additional nasal formants are also found to occur in few cases. Specific formant frequency region of nasal murmur exhibits some discrimination with respect to the place of articulatioa Keywords: Formant; Antiformant; Nasal murmur; Nasal vowel; Spectrographic analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-34"
  },
  "ganguli91_eurospeech": {
   "authors": [
    [
     "N. R.",
     "Ganguli"
    ]
   ],
   "title": "Vowel formant frequency distribution of a major indian language",
   "original": "e91_0161",
   "page_count": 4,
   "order": 36,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "The distribution of formant frequencies of vowels in one of the major Indian languages (Telugu) are investigated. Spectrographic analysis of 600 multi-syllabic words spoken by three male informants provide the basic data for this study. The results confirms the existence of eleven Telugu vowels. A comparative study of Telugu vowels with cardinal vowels and Bengali vowels are also presented Again, comparative degree of variation for different pairwise articulatory distinctive features are discussed Keywords: Formant frequencies; Spectrographic analysis; Cardinal vowel; Articulatory features; Vowel diagram.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-35"
  },
  "harmegnies91_eurospeech": {
   "authors": [
    [
     "Bernard",
     "Harmegnies"
    ],
    [
     "M.",
     "Bruyninckx"
    ],
    [
     "Joaquim",
     "Llisterri"
    ],
    [
     "Dolors",
     "Poch"
    ]
   ],
   "title": "Effects of language change on voice quality in bilingual speakers, corpus content effect",
   "original": "e91_0165",
   "page_count": 4,
   "order": 37,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "The paper investigates the reasons for LTAS sensitivity to language change. Six bilingual Catalan/Castilian Spanish have produced 5 utterances of 8 corpuses with controlled phonetic content. Experiment 1 shows that the LTAS interlanguage effect does not vary when the interlanguage corpus similarity increases. Experiment 2 shows that corpuses with invariant between- or within languages dissimilarities exhibit variable LTAS dissimilarities, i. e. more LTAS dissimilarity in interlanguage comparisons. It is therefore concluded that the corpus content variations due to the languages own characteristics are no cause of the LTAS interlanguage variability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-36"
  },
  "shevchenko91_eurospeech": {
   "authors": [
    [
     "T. I.",
     "Shevchenko"
    ],
    [
     "T. S.",
     "Skopintseva"
    ]
   ],
   "title": "Effects of social and regional backgrounds on LTAS in british English",
   "original": "e91_0169",
   "page_count": 4,
   "order": 38,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "To investigate the influence of regional and social backgrounds on voice quality of speakers, LTAS (long-term average spectra; drawn from 21 readings of an invariant text and 25 interviews were compared by means of SDDD dissimilarity index The results tentatively suggest correlation with major geographical divisions on the British Isles, More impressive are SDDD values drawn from comparisons of middle class, urban working class LTAS data. In search of more informative frequency bands data for regional and social groups identification. the ratio between two major areas under the curve was introduced, and the results were assessed with regards to socially acceptable cultivated voice quality. The communicative value of situational LTAS variability was tested In experiments with two speakers performing a number of social roles.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-37"
  },
  "heuvel91_eurospeech": {
   "authors": [
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Toni",
     "Rietveld"
    ]
   ],
   "title": "Speaker related variability in the durations of dutch speech segments",
   "original": "e91_0251",
   "page_count": 4,
   "order": 39,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "Lists of 24 CVCa nonsense words read by 10 speakers with 10 replications were used to determine speaker specificities in the realization of segment durations. Results indicate that long speech segments (long vowels) display more speaker variability than short segments and that sonorant consonants display less speaker variability than other consonants. Also individual speaking rate proved to be an important factor in this study. Keywords: segmental durations; automatic speaker recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-38"
  },
  "liljencrants91_eurospeech": {
   "authors": [
    [
     "Johan",
     "Liljencrants"
    ]
   ],
   "title": "Numerical simulations of glottal flow",
   "original": "e91_0255",
   "page_count": 4,
   "order": 40,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "Features of glottal flow are visualized by numerical simulation, using a Navier-Stokes solver for two-dimensional, incompressible flow operating with the MAC method. Results include pressure distributions leading to the forces acting on the vocal folds. Using a vocal fold model with translational and rotational mechanical resonators the boundaries corresponding to the vocal folds are allowed to move over the computation mesh to simulate dynamic behaviour.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-39"
  },
  "jansen91_eurospeech": {
   "authors": [
    [
     "Joop",
     "Jansen"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Modelling of source characteristics of speech sounds by means of the LF-model",
   "original": "e91_0259",
   "page_count": 4,
   "order": 41,
   "p1": "259",
   "pn": "262",
   "abstract": [
    "Automatic parameterization of the voice source is dependent on the availability of a suitable parametric model of the glottal flow pulses. Models like the Liljencrants/Fant (LF) model may not be rich enough to accurately match all those characteristics found in the results of inverse filtering. This may result in noisy estimates of the model parameters if flow waveforms are approximated that have features not accounted for by the model. In this paper an overview is given of the different types of source waveform that were obtained through automatic inverse filtering. We point out a number of features that we think are relevant and that are difficult to describe with the LF model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-40"
  },
  "herzel91_eurospeech": {
   "authors": [
    [
     "H.",
     "Herzel"
    ],
    [
     "J.",
     "Wendler"
    ]
   ],
   "title": "Evidence of chaos in phonatory samples",
   "original": "e91_0263",
   "page_count": 4,
   "order": 42,
   "p1": "263",
   "pn": "266",
   "abstract": [
    "Acoustic waveforms from pathological voices are analyzed by methods from nonlinear dynamics. It is emphasized that many observations of rough and creaky voices, of frequency jumps, and diplophonia can be interpreted as manifestations of low-dimensional dynamics in an appropriate phase space. Attractors are characterized by means of Poincare sections, fractal dimensions and Lyapunov exponents. Keywords: voice pathologies, deterministic chaos, phase space, attractor, bifurcation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-41"
  },
  "van91_eurospeech": {
   "authors": [
    [
     "L. Trinh",
     "Van"
    ],
    [
     "Bernard",
     "Guérin"
    ],
    [
     "E.",
     "Castelli"
    ]
   ],
   "title": "Source-tract coupling and the subglottal system in an articulatory synthesizer",
   "original": "e91_0267",
   "page_count": 4,
   "order": 43,
   "p1": "267",
   "pn": "270",
   "abstract": [
    "We describe a synthesizer in the time-domain and in particular a method of source-tract coupling. In our synthesizer, a two-mass model represents vocal folds. The vocal tract simulation is based on the Kelly-Lochbaum model. To solve the problem of source-tract coupling, vocal folds are considered as a special tube of the vocal tract. A subglottal system has also been studied within the context of a model of speech aerodynamics. Keywords: articulatory synthesizer, source-tract coupling, subglottal system, aerodynamic processes of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-42"
  },
  "bamberg91_eurospeech": {
   "authors": [
    [
     "Paul",
     "Bamberg"
    ],
    [
     "Anne",
     "Demedts"
    ],
    [
     "John",
     "Elder"
    ],
    [
     "Caroline",
     "Huang"
    ],
    [
     "Charles",
     "Ingold"
    ],
    [
     "Mark",
     "Mandel"
    ],
    [
     "Linda",
     "Manganaro"
    ],
    [
     "Stijn van",
     "Even"
    ]
   ],
   "title": "Phoneme-based training for large-vocabulary recognition in six european languages",
   "original": "e91_0175",
   "page_count": 7,
   "order": 44,
   "p1": "175",
   "pn": "182",
   "abstract": [
    "Dragon Systems and Lernout & Hauspie Speechproducts are jointly developing large-vocabulary speaker-dependent discrete speech-recognition systems in German, Spanish, French, Italian, and Dutch. These systems use the same strategy for training as does the DragonDictate-30K English-language product. Models are created for all the phonemes in the language, each in a wide variety of contexts. The resulting phoneme-in-context models are converted to hidden Markov models whose parameters can be re-estimated on the basis of a modest amount of adaptation data. Prototype systems have now been developed for the five languages. These systems all have vocabularies of a few thousand words and operate on an 80386-based personal computer. The performance of the systems is generally dose to that achieved in English about 85% of the words are recognized correctly, and the majority of errors can be corrected with a single keystroke.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-43"
  },
  "cerfdanon91_eurospeech": {
   "authors": [
    [
     "Helene",
     "Cerf-Danon"
    ],
    [
     "Steven",
     "DeGennaro"
    ],
    [
     "Marco",
     "Ferretti"
    ],
    [
     "Jorge",
     "Gonzalez"
    ],
    [
     "Eric",
     "Keppel"
    ]
   ],
   "title": "1. 0 TANGORA - a large vocabulary speech recognition system for five languages",
   "original": "e91_0183",
   "page_count": 10,
   "order": 45,
   "p1": "183",
   "pn": "192",
   "abstract": [
    "TANGORA is the prototype of a large vocabulary isolated word recognizer for English developed by IBM Research, USA. IBM research groups in Europe completed experimental Tangora systems for Italian, French, German, and Spanish. Starting from these prototypes, the groups are developing jointly a multi-lingual Tangora integrated in a completely re-designed architecture. It aims at providing a flexible and versatile speech recognizer allowing for running real-life dictation experiments. This paper presents details of the Tangora architecture, problems of the language adaptations and describes the specific issues that have been solved in order to cope with the peculiarities of each language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-44"
  },
  "ney91_eurospeech": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ],
    [
     "Roberto",
     "Billi"
    ]
   ],
   "title": "Prototype systems for large-vocabulary speech recognition: polyglot and spicos",
   "original": "e91_0193",
   "page_count": 8,
   "order": 46,
   "p1": "193",
   "pn": "200",
   "abstract": [
    "This paper gives an overview of two prototype systems for phoneme-based, large-vocabulary speech recognition. During the last decade, the performance of automatic systems for continuous speech recognition has been drastically improved. This progress has been mainly achieved by sophisticated statistical techniques. The prototype systems to be described are being developed within European research projects: in the ESPRIT project POLYGLOT, an isolated word recognition system is being developed for several European languages; in the SPICOS project, a continuous speech recognition system has been built that is being tested for several tasks, such as spoken database queries and the DARPA RM task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-45"
  },
  "wright91_eurospeech": {
   "authors": [
    [
     "J. H.",
     "Wright"
    ]
   ],
   "title": "Adaptation of grammar-based language models for continuous speech recognition",
   "original": "e91_0203",
   "page_count": 4,
   "order": 47,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "Probabilistic grammars are less adaptive to the user than are bigrams and trigrams. User-adaptation of grammar-rules is difficult, but an adaptable probabilistic dependence can be built into the way sentences are constructed within a given framework of rules. An optimising criterion using mutual information serves to link pairs of symbols within a derivation tree. The approach is data-driven, based on joint distributions of pairs of symbols obtained from training data. A simulation shows that the approach can provide an effective approximation to the true sentence probability. With adequate training this should lead to improved recognition performance. The dependence model can be built into a probabilistic parser with little loss of efficiency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-46"
  },
  "su91_eurospeech": {
   "authors": [
    [
     "Keh-Yih",
     "Su"
    ],
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "A robustness and discrimination oriented score function for integrating speech and language processing",
   "original": "e91_0207",
   "page_count": 4,
   "order": 48,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "In this paper, a robustness and discrimination oriented score function for integrating speech and language information is proposed. This unified approach uses probabilistic score functions to characterize different levels of knowledge in a uniform way. By jointly considering the knowledge from different levels, ranging from acoustics to syntax, the processing capability of speech and language are both enhanced with the information provided by the other module. To enhance the performance, the score function was modified to directly pursue correct rank ordering in the testing set, instead of pursuing maximal likelihood in the training set. This formulation has been applied to a Chinese phonetic typewriter task, and have considerably improved the performance of our system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-47"
  },
  "baggia91_eurospeech": {
   "authors": [
    [
     "Paolo",
     "Baggia"
    ],
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "E.",
     "Gerbino"
    ],
    [
     "Egidio P.",
     "Giachin"
    ],
    [
     "C.",
     "Rullent"
    ]
   ],
   "title": "Improving speech understanding performance through feedback verification",
   "original": "e91_0211",
   "page_count": 4,
   "order": 49,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "A parser for continuous speech has to deal with lattices where the word hypotheses of the correct sentence are not usually perfectly aligned and short function words can be missing. To cope with these problems, a two-way interaction between the recognition module and the parser, called feedback verification procedure (FVP), has been investigated. The parser generates many solutions, that are fed back to the front-end processor (FEP) which realigns them against the acoustical data, finds the missing function words among the given candidates and attributes them a new score. The best scored solution is finally selected by the parser.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-48"
  },
  "corazza91_eurospeech": {
   "authors": [
    [
     "A.",
     "Corazza"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "R.",
     "Gretter"
    ],
    [
     "G.",
     "Satta"
    ]
   ],
   "title": "Computation of upper-bounds for island-driven stochastic parsers",
   "original": "e91_0215",
   "page_count": 4,
   "order": 50,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "Automatic speech understanding is the process of deriving a complete sentence interpretation of an acoustic signal. Stochastic language models can be of considerable help for the solution of this problem. In this paper we present a new method to apply stochastic context-free grammar models to the search of the most likely syntactic interpretation of the signal. The problem is discussed both theoretically and computationally. The analysis is also extended to cases in which the underlying parsing process is carried out in a bidirectional way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-49"
  },
  "andry91_eurospeech": {
   "authors": [
    [
     "Francois",
     "Andry"
    ],
    [
     "Simon",
     "Thornton"
    ]
   ],
   "title": "A parser for speech lattices using a UCG grammar",
   "original": "e91_0219",
   "page_count": 4,
   "order": 51,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "In this paper we present a lattice parser that uses a variant of a Unification Categoriai Grammar. We explain how this formalism, which offers interesting properties for natural language processing, can be implemented for speech lattices in an effective way without losing of its expressiveness and flexibility. In the solution we have adopted, the parsing process has been split in two phases: an acceptance phase based on bit string unification which includes score combination functions, followed by a full semantic interpretation phase. The parser operates on two linguistic knowledge bases built from the flight enquiries and reservations domains for English and French. Keywords: lattice parsing; constraining features; Unification Categoriai Grammar; bit encoding; score combination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-50"
  },
  "young91_eurospeech": {
   "authors": [
    [
     "Sheryl",
     "Young"
    ],
    [
     "Michael",
     "Matessa"
    ]
   ],
   "title": "Using pragmatic and semantic knowledge to correct parsing of spoken language utterances",
   "original": "e91_0223",
   "page_count": 5,
   "order": 52,
   "p1": "223",
   "pn": "227",
   "abstract": [
    "This paper describes the structure and operation of Soul, or Semantically-Oriented Understanding of Language. Soul is a knowledge intensive reasoning system which is opportunistically used to provide a more thorough, fine grained analysis of an input utterance following its processing by a case-frame speech parser. The Soul postprocessor relies upon extensive semantic and pragmatic knowledge to correct, reject and/or clarify the outputs of the CMU Phoenix case-frame parser for speech and speech transcripts. We describe briefly both some of the linguistic phenomena which Soul addresses and how Soul works to correct inaccurate interpretations produced by the Phoenix parser. Finally, we present the results six non-overlapping test sets, each evaluated for both speech and speech transcription processing. These test sets evaluate the systems ability to enhance performance in both highly restricted and completely unrestricted input data. Further, some test sets capitalize upon the unique linguistic features of spontaneous speech. These evaluations illustrate that the decrease in incorrect interpretations and total error rate resulting from Soul's postprocessing are most pronounced in unrestricted transcript data and all forms of speech data, as opposed to carefully constrained test sets For example, in processing transcription data incorrect interpretations are reduced by 53% in constrained sets, as opposed to 81% in unrestricted sets. In other words, the more difficult the processing and/or interpretation, the more there was to be gained by using extensive reasoning abilities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-51"
  },
  "abrantes91_eurospeech": {
   "authors": [
    [
     "A. J.",
     "Abrantes"
    ],
    [
     "J. S.",
     "Marques"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Hybrid sinusoidal modeling of speech without voicing decision",
   "original": "e91_0231",
   "page_count": 4,
   "order": 53,
   "p1": "231",
   "pn": "234",
   "abstract": [
    "This paper presents a new approach to sinusoidal modeling of speech which avoids the use of a voicing detector. The proposed model represents the speech signal as a sum of sinusoids and bandpass random signals. The use of two different sets of basis functions increases the robustness of the model since there is no need to switch between techniques tailored to particular classes of sounds. The sinusoidal basis functions with harmonically related frequencies allow an accurate representation of the quasi-periodic structure of the speech signal. The bandpass random functions, on the other hand, are better suited for high quality representation of unvoiced speech sounds, since their bandwidth is larger than the bandwidth of an harmonic. The amplitudes of all the two sets of basis functions are simultaneously estimated by a least squares algorithm and the output speech signal is synthesized in the time domain by the superposition of all basis functions multiplied by their amplitudes. Preliminary tests confirm the better performance of the hybrid model for operation with noise-corrupted input speech, relative to classical sinusoidal models with a strong dependency on voiced/unvoiced decisions. Keywords: Speech Modeling, Sinusoidal Modeling, Coding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-52"
  },
  "marques91_eurospeech": {
   "authors": [
    [
     "J. S.",
     "Marques"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "A. J.",
     "Abrantes"
    ]
   ],
   "title": "Harmonic coding of speech: an experimental study",
   "original": "e91_0235",
   "page_count": 4,
   "order": 54,
   "p1": "235",
   "pn": "238",
   "abstract": [
    "Harmonic coding has reached a state of maturity where real-time implementation can already be envisaged. However, there is still a long way before one can say that harmonic coding concepts are fully explored. This paper attempts to pinpoint the weakest features of harmonic coding and improving them. One of the main criticisms pointed to harmonic coders is the heavy dependence of the output quality on the performance of the pitch estimation and voicing decision algorithms. Pitch and voicing estimation, therefore, was one of the first problems addressed in this study. Another one is phase encoding, which we have previously reported as a major source of degradation at 4. 8 Kb/s. Finally, since the coder is sensitive to pitch and voicing decision, an important question concerns its performance with noisy speech input. All these problems are addressed on the present study. Keywords: sinusoidal / harmonic coding; speech coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-53"
  },
  "rowe91_eurospeech": {
   "authors": [
    [
     "David",
     "Rowe"
    ],
    [
     "William",
     "Cowley"
    ],
    [
     "Andrew",
     "Perkis"
    ]
   ],
   "title": "A multiband excitation linear predictive speech coder",
   "original": "e91_0239",
   "page_count": 4,
   "order": 55,
   "p1": "239",
   "pn": "242",
   "abstract": [
    "A new low rate speech coder based on the Multi-Band Excitation (MBE) model is presented that incorporates features of existing Code Excited Linear Prediction (CELP) [12] and Multi-Band Excitation (MBE) [1] coders. Unlike existing MBE coders, the spectral envelope information is represented using Linear Prediction Coefficients (LPCs). The coder provides communications quality speech at 3kbit/s and offers significant gains in robustness over other MBE coders. The complexity is low enough for implementation on commercial DSP devices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-54"
  },
  "leung91_eurospeech": {
   "authors": [
    [
     "S. H.",
     "Leung"
    ],
    [
     "K. L.",
     "Lai"
    ],
    [
     "O. Y.",
     "Wong"
    ],
    [
     "A.",
     "Luk"
    ]
   ],
   "title": "A new coded excitation model using multifrequency decomposition",
   "original": "e91_0245",
   "page_count": 4,
   "order": 56,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "This paper is to present a new high quality code-excited LPC (linear predictive coding) system. In this coder, the excitation comprises a set of compound pulses, and each compound pulse consists of a group of element pulses spaced uniformly at a lower rate. The compound pulse can be viewed as a subband of excitations sampled at a lower rate. Thus the excitation can be interpreted as the combination of a set of generalized subband signals. In the coder, the locations of the compound pulses and amplitudes of the element pulses are vector-quantized. We use two seperate codebooks: one for the locations of the compound pulses and the other for the amplitudes of the element pulses. Since the range of the locations and the number of the compound pulses are reduced according to pulse size, the location codebook is substantially reduced. After obtaining the pulse locations, the amplitudes are searched from the amplitude codebook. The coder is shown to have very good performance for the bit rate below 8kbps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-55"
  },
  "sereno91_eurospeech": {
   "authors": [
    [
     "Daniele",
     "Sereno"
    ]
   ],
   "title": "Frame substitution and adaptive post-filtering in speech coding",
   "original": "e91_0595",
   "page_count": 4,
   "order": 57,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "In this paper we focus on the problem of improving the performance of a speech and channel coder under very bad channel conditions. The study has been tailored to the half-rate sub-system of the pan-European Digital Mobile Radio system. Two major aspects are investigated: the first relates to the reliability of the most important bits in the received bit-stream and the second concerns the use of adaptive post-filtering to cope with errors over those bits related to the excitation parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-56"
  },
  "atungsiri91_eurospeech": {
   "authors": [
    [
     "S. A.",
     "Atungsiri"
    ],
    [
     "R.",
     "Soheili"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "Effective lost speech frame reconstruction for CELP coders",
   "original": "e91_0599",
   "page_count": 4,
   "order": 58,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "Low bit rate speech coders in mobile communications systems suffer from the effects of bursty errors on the transmission channels. By using forward error control (FEC) techniques, some of these errors can be corrected. However, the channel distortions occasionally become so severe that these FEC techniques fail and so the transmitted parameters to the speech synthesizer are effectively lost. The only way to restore the lost speech then is the application of a lost frame reconstruction or replacement strategy. In this paper, we report on a novel frame reconstruction strategy for a 6. 8 Kb/s CELP-like coder. By breaking the transmitted parameters into two separate classes, we adopted different strategies for recovering from the loss of each class and so cut down on the more severe full frame reconstruction. In informal listening tests, speech intelligibility was maintained at up to 20% random frame loss of either parameter category and two consecutive full frame losses (~ 40ms of speech).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-57"
  },
  "nagabuchi91_eurospeech": {
   "authors": [
    [
     "Hiromi",
     "Nagabuchi"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ]
   ],
   "title": "Evaluation and improvement of coded speech quality degraded by cell loss in ATM networks",
   "original": "e91_0603",
   "page_count": 4,
   "order": 59,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "This paper investigates the subjective effects of speech quality impairments resulting from cell loss and coding distortion, which are likely to occur in communications systems using ATM (Asynchronous Transfer Mode). A method is proposed for improving speech quality degraded by cell loss. This method uses an LPC (Linear Predictive Coding) spectrum-shaping filter. The performance of the proposed method is compared with that of a speech signal waveform substitution method. Listening tests were used to assess both quality (defined using a rating scale) and intelligibility. Speech quality evaluation results are presented for various speech coding methods, methods for dividing speech signals into cells, unit cell sizes, and missing-cell-rates. Experimental results show the robustness of embedded coding against cell loss and the effectiveness of the proposed quality-improvement method. Keywords: Speech quality evaluation; ATM; Cell loss\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-58"
  },
  "vigier91_eurospeech": {
   "authors": [
    [
     "Alain J.",
     "Vigier"
    ]
   ],
   "title": "Combined source-channel coding for a very noisy channed",
   "original": "e91_0607",
   "page_count": 4,
   "order": 60,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "This paper reports the study of codecs at global bit rates (source and channel coding) of 8, 12 and 16 kb/s. We examine combined source-channel coding algorithms for very noisy channels. We first describe our application: transmission over VHF channel, using frequency hopping (FH). We hypothesize an hostile environment, therefore the bit error rate can reach a non-uniform rate of 25%! Next, we describe the 3 speech coders used in this study and analyse their sensitivity to bit errors. Efficient channel coding and error recovery procedures are proposed, such as using some matrix (cascaded) codes and repeating or smoothing parameters assumed in error. These combined source-channel coding procedures yield to much better results than if both coding procedures were carried out separately or if we used a speech coder at the total transmission rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-59"
  },
  "rosina91_eurospeech": {
   "authors": [
    [
     "G.",
     "Rosina"
    ],
    [
     "M. Sant'",
     "Agostino"
    ],
    [
     "E.",
     "Turco"
    ],
    [
     "L.",
     "Vetrano"
    ]
   ],
   "title": "Testing and quality enhancement of the GSM full rate voice channel",
   "original": "e91_0611",
   "page_count": 4,
   "order": 61,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "Digital Mobile Radio Systems require low bit rate speech codecs maintaining a high quality In spite of bad transmission conditions. In Europe, work Is under way for the standardization of a digital mobile radio system at 900 MHz (GSM). Until now, a 13 Kbit/s speech codec based on a Regular-Pulse Excitation Long-Term Prediction (RPE-LTP) [1] algorithm and a protection scheme for voice traffic channels at gross bit rate of 22. 8 Kbit/s have been standardized by ETSI/GSM. Recommendations leave room for Improvements at various levels of the digital signal processing. The optimization of the subjective speech quality required the set-up of a test bed allowing a continuous check and evaluation of the adopted algorithms. This paper focus on the test arrangements at Italtel Labs for voice quality assessment and describes our approach In the optimization of the GSM receiver. Keywords: Digital Mobile Radio System; GSM; Italtel Labs; Full Rate system; Half Rate system; Test bed; Enhanced BFI\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-60"
  },
  "kipper91_eurospeech": {
   "authors": [
    [
     "U.",
     "Kipper"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Dietrich",
     "Wolf"
    ]
   ],
   "title": "Low bit rate speech coding using CELP with adaptive excitation codebook",
   "original": "e91_0893",
   "page_count": 4,
   "order": 62,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "In this paper we propose a method denoted as ACELP for improving the speech quality of a Code-Excited-Linear-Predictive (CELP) codec at low bit rates by adaptation of the excitation codebook. The adaptive codebook consists of a set of basic excitation vectors which are adapted to an analysis speech frame by calculating optimum amplitude values. The amplitudes are quantized and encoded at a very low bit rate using a gain shape vector quantizer. The results of simulation experiments show that the mean squared error of CELP with an adaptive codebook is substantially lower than that of CELP with a fixed stochastic codebook. In informal listening tests the quality of the speech processed by the adaptive CELP scheme offered a reduced roughness and was less speaker dependent as with a conventionel CELP scheme.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-61"
  },
  "fuldseth91_eurospeech": {
   "authors": [
    [
     "A.",
     "Fuldseth"
    ],
    [
     "E.",
     "Harborg"
    ],
    [
     "F. T.",
     "Johansen"
    ],
    [
     "J. E.",
     "Knudsen"
    ]
   ],
   "title": "A real-time implementable 7 khz speech coder at 16 kbit/s",
   "original": "e91_0897",
   "page_count": 4,
   "order": 63,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "This paper presents results on wideband 7 kHz speech coding at 16 kbit/s where the proposed CELP algorithm is implementable on a single floating point DSP. As a basic coder structure, the long-term predictor is implemented as an adaptive codebook, while a sparse Gaussian codebook with non-overlapping vectors is used for the stochastic excitation. In order to meet the complexity requirements, several methods for efficient codebook search\" are adopted. With these methods, it is shown that the computational effort for the basic coder structure can be reduced to 12. 4 MIPS with a 7 bit stochastic codebook. A two-stage hierarchical search through the adaptive codebook is investigated. This search method reduces the computational effort further although at the cost of a small degradation in coder performance. The coder is evaluated in an absolute category rating (MOS) test using both a hi-fi handset and a loudspeaker, and compared to the CCITT standard coder G. 722 at 48-64 kbit/s. The speech quality with the basic CELP structure is judged to be comparable to the G. 722 coder at 48 kbit/s. Keywords: Wideband Speech Coding, CELP.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-62"
  },
  "zarkadis91_eurospeech": {
   "authors": [
    [
     "D. J.",
     "Zarkadis"
    ]
   ],
   "title": "Adaptive spectral weighting for vector predictive coding of the LPC-spectra",
   "original": "e91_0901",
   "page_count": 4,
   "order": 64,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "We propose Adaptive Spectral Weighting (ASW) for improving the low-rate coding of the LPC spectral information which is represented in the form of spectral envelope vectors. This method employs intraframe weighted-error criteria applied to split-vector quantization of the residual LPC spectral components which in effect minimize a weighted distance between the input and the reconstructed LPC spectra. Spectral weighting is introduced by means of a weighting operator corresponding to an equalized version of the spectral envelope vector which is obtained by hierarchical processing. The weighting operator is adaptive and it is characterized by gain and shape. The ASW scheme utilizes residual codebooks which have been populated by embedding weighted-distance criteria to the clustering and centroid estimation rules of the design algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-63"
  },
  "saoudi91_eurospeech": {
   "authors": [
    [
     "Samir",
     "Saoudi"
    ],
    [
     "J. Marc",
     "Boucher"
    ],
    [
     "Alain Le",
     "Guyader"
    ]
   ],
   "title": "Medium band speech coding using optimal scalar quantization of LSP",
   "original": "e91_0905",
   "page_count": 4,
   "order": 65,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "The most popular LPC parameters used for speech Linear Predictive Coding at medium or low bit rate are the reflection coefficients and the Line Spectrum Pairs. In this paper, comparisons of the quantization performance of these two types of coefficients will be given. Several algorithms are studied for the quantization of the LSP parameters. The performance of the quantization methods of the LSP parameters is studied for a different total number of bits per frame. The Fox-Makhoul procedure is used to derive the optimal bit allocation to achieve the best quantization performance for both LPC parameters (Line Spectrum Pairs (LSP) and Log Area Ratio (LAR)). The quantization tables of the LSP and the LAR parameters are introduced in a 8 kbits/s CELP coder for comparison.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-64"
  },
  "seeker91_eurospeech": {
   "authors": [
    [
     "Philip",
     "Seeker"
    ],
    [
     "Andrew",
     "Perkis"
    ]
   ],
   "title": "Joint source and channel coding of line spectrum pairs",
   "original": "e91_0909",
   "page_count": 4,
   "order": 66,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "An innovative method of coding Line Spectrum Pair (LSP) parameters for transmission over noisy channels is presented. The scheme uses a joint source and channel code applied to a concatenated trellis structure. The Viterbi algorithm is used to minimize a cost function that accounts for the expected distortion over the channel. By combining source and channel coding into one operation, the typically large bandwidth increase introduced from explicit channel coding is reduced and overall complexity is decreased. It is noted that the encoding scheme performs well over a wide range of channel Bit Error Rates (BERs) and compares favourably with a standard scalar LSP quantization scheme in terms of bit rate and channel noise immunity. Keywords: Line Spectrum Pair, Joint Source and Channel Coding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-65"
  },
  "chan91_eurospeech": {
   "authors": [
    [
     "C. F.",
     "Chan"
    ],
    [
     "K. W.",
     "Law"
    ]
   ],
   "title": "An algorithm for computing LSP frequencies directly from the reflection coefficients",
   "original": "e91_0913",
   "page_count": 4,
   "order": 67,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "An algorithm for computing the LSP frequencies will be given in this paper. The algorithm employs a recursive technique to generate the line spectrum from the reflection coefficients. In each step of a recursion, a scaling and rotation on the complex frequency plane are required. The even and odd line spectral frequencies are found to be the zero cross-points on the real and imaginary axis respectively. The algorithm does not require to calculate the LPC coefficients and it can be implemented using fixed-point arithmetic with low complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-66"
  },
  "meyer91_eurospeech": {
   "authors": [
    [
     "Peter",
     "Meyer"
    ],
    [
     "W.",
     "Peters"
    ],
    [
     "J.",
     "Paulus"
    ]
   ],
   "title": "Variable rate speech coding using perceptive thresholds and adaptive VUS detection",
   "original": "e91_0809",
   "page_count": 4,
   "order": 68,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "This paper describes a 8. 4 kBit/s RPE-coder which is modified to form a variable bit rate coder with an average bit rate of 5 kBit/s. To realize such variable bit rates, the coding scheme must depend on the type of the speech segment to be encoded. Reduction has been achieved for segments that are spectrally stationary (repetition of an LPC parameter set), non-periodic (omission of an LTP parameter set), unvoiced (modeling of the residual as a noise source) or contain background noise (simple coding of station- ary background noise). To classify these states we used distance measures, perceptive thresholds and a robust adaptive Voiced-Unvoiced-Silence (VUS) detector.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-67"
  },
  "suddle91_eurospeech": {
   "authors": [
    [
     "M. R.",
     "Suddle"
    ],
    [
     "S. A.",
     "Atungsiri"
    ],
    [
     "A. M.",
     "Kondoz"
    ],
    [
     "B. G.",
     "Evans"
    ]
   ],
   "title": "A secure and robust CELP coder for land and satellite mobile systems",
   "original": "e91_0813",
   "page_count": 4,
   "order": 69,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "During the last decade CELP coding [1] of speech has been the most popular and promising scheme for producing high quality speech at medium to low bit-rates. Computational complexity of CELP however, has been its main disadvantage. For mobile systems where power and transmission bandwidth are at premium and channel conditions are severe, computational complexity of the speech coding algorithm, its robustness to channel errors, power consumption and the cost of implementation are the main design considerations. In this paper we present a CELP coder implemented using a single AT&T DSP32C. In addition to encoding and decoding in one DSP, we have implemented a forward error control scheme to increase its robustness to channel errors. The coder operates at gross bit rates of 6. 4 and 9. 6 kb/s and produces very high quality speech. A security algorithm to protect against the hardware/software piracy has also been included.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-68"
  },
  "ribeiro91_eurospeech": {
   "authors": [
    [
     "C. M.",
     "Ribeiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "A 4. 8 kbps celp coder with post-processing",
   "original": "e91_0817",
   "page_count": 4,
   "order": 70,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "This paper presents a systematic study of CELP coders at 4. 8 Kbps, where some alternatives and trade-offs between perceptual quality, overall delay, complexity and memory requirements are considered. This study has used the version adopted as a standard by the U. S. D. o. D. (United States Department of Defense) as a reference coder, and compares the results obtained with this version with the ones obtained by using different approaches, namely in terms of overall delay and codebook search procedures. Two post-processing techniques are also studied: envelope post-processing and harmonic post-processing. An adaptive pitch pre-filter is also implemented, with the same rule of the harmonic post-processing, that is to improve the harmonic structure of voiced regions. The large number of options considered in this work reflects the existence of more than one \"optimum\" solution. The choice of one of them must be dictated by the delay and quality requirements and hardware resources. Keywords: CELP Coding, Post-processing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-69"
  },
  "law91_eurospeech": {
   "authors": [
    [
     "K. W.",
     "Law"
    ],
    [
     "O. Y.",
     "Wong"
    ],
    [
     "C. F.",
     "Chan"
    ]
   ],
   "title": "A real-time high quality joint-excitation linear predictive coder at 8 kbps",
   "original": "e91_0821",
   "page_count": 4,
   "order": 71,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "A real time 8 kbps full-duplex vocoder using Joint-Excitation Linear Predictive Coding (JELPC) scheme is presented. JELPC utilizes a self excitation sequence for long term prediction in addition to two codebooks populated with stochastic distributions (one with multi-pulse entries and another with full entries) to model the excitation signal. With these special codebook structures and an efficient codebook searching algorithm, the coder complexity is reduced significantly and well suited for real time applications. The algorithm was implemented using a single TMS320C25 based DSP board, and has been shown to generate high quality speech. The details of the hardware architecture, algorithm implementation and system performance are also addressed clearly in this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-70"
  },
  "deiacovo91_eurospeech": {
   "authors": [
    [
     "Rosario Drogo",
     "Deiacovo"
    ],
    [
     "Roberto",
     "Montagna"
    ]
   ],
   "title": "Some experiments in perceptual masking of quantizing noise in analysis-by-synthesis speech coders",
   "original": "e91_0825",
   "page_count": 4,
   "order": 72,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "Analysis-by-synthesis speech coders employ suitable weighting filters in order to perform noise shaping of the quantizing error in the output speech signal The subjective effect of quantizing noise can be reduced by modeling more precisely the \"masking\" phenomenon in the human auditory system. Starting from a simplified model of speech perception, we evaluated the \"masking threshold\", such that the noise frequency components below that threshold cannot be perceived by the human ear. As a second step, we substituted the weighting filter included in the excitation quantization loop of an analysis-by-synthesis scheme, with a filter having as frequency response the reciprocal of the masking threshold and a suitable phase. Informal listening tests demonstrated the ability of this model to substantially reduce the noise in the output speech, in spite of its simplicity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-71"
  },
  "yang91_eurospeech": {
   "authors": [
    [
     "Gao",
     "Yang"
    ],
    [
     "Kenri",
     "Leich"
    ],
    [
     "René",
     "Boite"
    ]
   ],
   "title": "A very high-quality CELP coder at the rate of 2400 bps",
   "original": "e91_0829",
   "page_count": 4,
   "order": 73,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "This paper proposes a special CELP vocoder which is able to bring the bit rate down to 2400 BPS. The auditory quality of the synthetic speech was demonstrated to be much better than the conventional coder at the bit rate of 2. 4 kbps/sec and to be equivalant to the original CELP coder at fee rate of 4. 2 kbps/sec. The simplification of the algorithm allows a real time implementation on a single chip of the TMS320C25 family while maintaining the same high quality. Keywords: Speech Coding, CELP Vocoder, Low bit-rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-72"
  },
  "liu91_eurospeech": {
   "authors": [
    [
     "Z. Yong",
     "Liu"
    ]
   ],
   "title": "An effective pulse adaptive code-excited linear predictive coder at 4kb/S",
   "original": "e91_0835",
   "page_count": 4,
   "order": 74,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "Code-excitation is an attractive way of modeling the input to the LPC filter at low bit rates. The excitation consists of a codebook, populated from Gaussian random number. This leads to two disadvantages: 1) highly computational load and storage capacity; 2) crucial role of long term prediction. The paper proposes an effective Pulse Adaptive Code-Excited Linear Predictive (PACELP) speech coding algorithm. This scheme allows a much less computational power to determine the optimal excitation sequence when pulse adaptive search procedure is introduced and much fewer storage capacities to put the codebook and its related parameters. The codebook of PACELP is obtained from Regular-Pulse Excitation sequences using varying threshold clustering algorithm. Besides, this scheme omits the long term prediction because pitch information can be easily expressed when the initial phase is introduced to the codebook. The computer simulation results show that at 4kb/s, PACELP can produce a clear and natural-sounding synthetic speech and its realtime implementation on TMS320C25 is being under way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-73"
  },
  "chan91b_eurospeech": {
   "authors": [
    [
     "C. F.",
     "Chan"
    ],
    [
     "S. H.",
     "Leung"
    ]
   ],
   "title": "A vocoder using high-order LPC filter with very few non-zero coefficients",
   "original": "e91_0839",
   "page_count": 4,
   "order": 75,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "This paper introduces a new linear predictive modelling technique based on using a high-order LPC filter having only few non-zero coefficients. Efficient algorithms were developed to find the non-zero coefficients for the cases of transversal filter and lattice filter implementations. Coupling with multi-pulse analysis procedure for vocoding, the new modelling technique can produce good-quality synthetic speech at low data rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-74"
  },
  "rossi91_eurospeech": {
   "authors": [
    [
     "Mario",
     "Rossi"
    ],
    [
     "Robert",
     "Espesser"
    ],
    [
     "Chaslav",
     "Pavlovic"
    ]
   ],
   "title": "The effects of in internal reference system and cross-modality matching on the subjective rating of speech synthesisers",
   "original": "e91_0273",
   "page_count": 4,
   "order": 76,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "In earlier reports we have concluded that contextual invariance of categorical and magnitude estimates of speech quality could be improved by introducing a reference system and by normalizing the results with respect to it. In this study we investigate the possibility of substituting an actual reference signal with an \"internal\" reference. It is also studied whether in magnitude estimations a cross-modality matching using lines on a computer screen could be employed. Keywords: Speech quality, Speech synthesis, Magnitude estimations, Categorical estimations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-75"
  },
  "sydeserff91_eurospeech": {
   "authors": [
    [
     "H. A.",
     "Sydeserff"
    ],
    [
     "R. J.",
     "Caley"
    ],
    [
     "Stephen D.",
     "Isard"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "Alex I. C.",
     "Monaghan"
    ],
    [
     "J.",
     "Verhoeven"
    ]
   ],
   "title": "Evaluation of speech synthesis techniques in a comprehension task",
   "original": "e91_0277",
   "page_count": 4,
   "order": 77,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "Six types of speech synthesis were evaluated for comprehensibility: Standard Linear Predictive Coding Analysis/Resynthesis; Pitch Synchronous Analysis/Resynthesis; Pitch Synchronous Multi-Pulse Analysis/Resynthesis; and three PSOLA (Pitch Synchronous Overlap-and-Add) techniques. The relative comprehensibility of the synthesis types was tested by using the synthesised speech to convey information that subjects needed in order to perform a diagram-based multiple-choice task. Keywords: Speech Synthesis; Evaluation; Multi-Pulse Linear Predictive Coding; PSOLA\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-76"
  },
  "howardjones91_eurospeech": {
   "authors": [
    [
     "P. A.",
     "Howard-Jones"
    ]
   ],
   "title": "'SOAP' - a speech output assessment package for controlled multilingual evaluation of synthetic speech",
   "original": "e91_0281",
   "page_count": 3,
   "order": 78,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "The ESPRIT project SAM (Speech Assessment Methods) is primarily concerned with establishing and using, within the European Community, standards for multilingual speech input/output assessment and methodology. Most test procedures for assessing speech synthesis, whether in terms of intelligibility, naturalness, prosodic quality, etc. , are based upon presenting a set of examples to a human subject and eliciting an appropriate response which needs to be recorded, interpreted and scored. As part of the SAM project, a system has been designed to implement and control the procedure with maximum flexibility with regard to methodology. SOAP, which has been developed for a PC workstation, was initially designed for segmental testing, but its versatility allows other test methodologies, and other tests, to be automatically controlled. Keywords: Assessment, synthesis, multilingual.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-77"
  },
  "houtgast91_eurospeech": {
   "authors": [
    [
     "Tammo",
     "Houtgast"
    ],
    [
     "Jan A.",
     "Verhave"
    ]
   ],
   "title": "A physical approach to speech quality assessment: correlation patterns in the speech spectrogram",
   "original": "e91_0285",
   "page_count": 4,
   "order": 79,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "A bank of filters has been implemented digitally to obtain, with running speech as input, energy values within well defined, Gaussian-shaped, frequency-time windows. The analysis concentrates on the correlation between the dB-outputs of pairs of different windows, with the frequency-spacing and/or the time-spacing between two such windows as parameters. The resulting correlation patterns reflect, in a global way, the statistics of the dynamic characteristics of running speech in both the frequency and the time domain. Various aspects of such correlation patterns will be considered briefly, illustrating interesting relations with some basic features in hearing and speech intelligibility. The main issue concerns the possible usefulness of this global measure for speech quality assessment. It is found that these correlation patters derived from natural speech have a typical structure, providing a basis for judging the degree of \"naturalness\" of a token of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-78"
  },
  "miyata91_eurospeech": {
   "authors": [
    [
     "H.",
     "Miyata"
    ],
    [
     "Tammo",
     "Houtgast"
    ]
   ],
   "title": "Weighted MTF for predicting speech intelligibility in reverberant sound fields",
   "original": "e91_0289",
   "page_count": 4,
   "order": 80,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "This paper describes a method for predicting speech intelligibility in reverberant sound fields. The RASTI method, which utilizes the MTF (Modulation Transfer Function) as the basic parameter, is commonly used for predicting speech intelligibility in sound fields. This paper proposes a weighted MTF, obtained by weighting the echograms with an exponential time window before calculating the MTF. Intelligibility tests using a variety of computer-generated echograms confirm that the proposed method is better for predicting speech intelligibility in reverberant sound fields than the ordinary MTF (RASTI) method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-79"
  },
  "jekosch91_eurospeech": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ]
   ],
   "title": "Speech intelligibility studies for the european hermes spaceplane",
   "original": "e91_0293",
   "page_count": 4,
   "order": 81,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "At the end of the 90s the European spaceplane programme HERMES is planned to be carried out. Presently, a telecommunication subsystem is under development. This development is directed towards video and audio communication. The definition of the audio communication system includes the system design as well as the development of different speech coders that are geared to the specific conditions of the HERMES mission. For being able to choose the best system, each coder must be validated. A reliable quality judgement requires a test where the functional aspect of the object is paid attention to. In order to open up the possibility to collect data for this specific application, new intelligibility tests have been designed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-80"
  },
  "wei91_eurospeech": {
   "authors": [
    [
     "Jianing",
     "Wei"
    ],
    [
     "Andrew",
     "Faulkner"
    ],
    [
     "Adrian",
     "Fourcin"
    ]
   ],
   "title": "An application of speech processing and encoding scheme for Chinese lexical tone and consonant perception by hearing impaired listeners",
   "original": "e91_0299",
   "page_count": 4,
   "order": 82,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "This paper investigates the effectiveness of using simplified speech patterns based on voice fundamental frequency, speech amplitude envelope, and the presence of voiceless excitation for Chinese lexical tone and consonant perception by hearing impaired Chinese listeners. The test results indicated that for one listener with profound hearing loss (>100 dB) at high frequencies (1 KHz and above), the perception performances from the presentation of voice fundamental frequency information were distinctively higher compared with speech presentation. Listeners with less severe hearing loss were able to make good use of both amplified speech and simplified speech pattern information. Keywords: speech pattern perception, hearing aid, lipreading\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-81"
  },
  "kanevsky91_eurospeech": {
   "authors": [
    [
     "D.",
     "Kanevsky"
    ],
    [
     "P.",
     "Gopalakrishan"
    ],
    [
     "C.",
     "Danis"
    ],
    [
     "G.",
     "Daggett"
    ],
    [
     "E.",
     "Epstein"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "On the development of a phone communication aid for the hearing impaired",
   "original": "e91_0303",
   "page_count": 4,
   "order": 83,
   "p1": "303",
   "pn": "306",
   "abstract": [
    "We describe the use of Tangora - a large vocabulary, isolated word, speaker dependent automatic speech recognizer (ASR) - as a communication device that would allow a hearing impaired person to communicate over the telephone with hearing individuals. The usability of this system could be limited by the degradation in the Tangora's recognition accuracy if it is used over public telephone lines or if it is used as a speaker independent system. In this paper we discuss some issues that could arise as a result of this. These include: How well can the hearing impaired cope with relatively low recognizer accuracy in phone conversation? How does the topic of conversation affect understanding of the meaning of spoken sentences? In particular, we have found that users can correctly understand the topic of ASR decoded phrases even when the recognizer accuracy is below 80%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-82"
  },
  "anglade91_eurospeech": {
   "authors": [
    [
     "Yolande",
     "Anglade"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "A spoken language interface for a telephone switchboard operator center",
   "original": "e91_0307",
   "page_count": 4,
   "order": 84,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "This paper describes an experimental conversational system integrated in a telephone switchboard operator center. It is aimed to help a partially sighted telephone switchboard operator working in a large company. Using an intelligent speech interface and a database containing information about all the people and departments in the company, it provides the telephone switchboard operator with a telephone number or any information in response to the request of an external or internal caller. A feature of this interface is to allow the access to various information on the basis of the name of a person in the company. As the number of people in this company is very large (about 5000), the operator's request is formulated by spelling the particular name to avoid doing a large vocabulary recognition directly. To help the system find the name uttered, a phonetic lattice is created. This phonetic step provides some tolerance for orthographical mistakes, differences in pronunciations, and possible confusions made by the speech recognizer. As output, a speech synthesizer gives the requested information. The current implementation is taking place on a PC 486, equipped with two specialized boards and connected to the switchboard. Keywords: Conversational system, telephone switchboard, dialogue, real-time, robustness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-83"
  },
  "murray91b_eurospeech": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "John L.",
     "Arnott"
    ],
    [
     "Norman",
     "Alm"
    ],
    [
     "Alan F.",
     "Newell"
    ]
   ],
   "title": "A communication system for the disabled with emotional synthetic speech produced by rule",
   "original": "e91_0311",
   "page_count": 4,
   "order": 85,
   "p1": "311",
   "pn": "314",
   "abstract": [
    "A system for producing synthetic speech which incorporates vocal emotion effects has been developed. This has been incorporated into a communication prosthesis for nonvocal people to give them more freedom of expression via a synthetic voice. A range of common emotions can be simulated by the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-84"
  },
  "portele91_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Birgit",
     "Steffan"
    ],
    [
     "Rainer",
     "Preuß"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "German speech synthesis by concatenation of non-parametric units",
   "original": "e91_0317",
   "page_count": 4,
   "order": 86,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "This paper describes the advantages and difficulties of a German Speech Synthesis system that concatenates non- parametric time domain units similar to demisyllables. Problems arise when transitions not contained in the inventory must be generated by manipulation of existing units. A proposal for a unit inventory suitable for German is made.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-85"
  },
  "abbattista91_eurospeech": {
   "authors": [
    [
     "Giuseppe",
     "Abbattista"
    ],
    [
     "Antonello",
     "Riccio"
    ],
    [
     "Enzo",
     "Mumolo"
    ]
   ],
   "title": "Automatic document reader with speech output capabilities",
   "original": "e91_0321",
   "page_count": 4,
   "order": 87,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "The paper describes a reliable system capable to read typewritten documents and to convert them into a speech output. The basic architecture of the system is built around a commercial optical scanner connected to a Personal Computer provided with an add-on DSP card; this card represents the heart of the speech capability of the entire system; the board, entirely developed at the research centre of Alcatel FACE, allows several speech processing features, as connected words speech recognition, speech compression and Italian text to speech conversion. The text to speech conversion, also developed at our laboratory, is based on the segment concatenation approach where the basic segments are diphones and triphones; the current size of the segment database is in the range of 400; both male and female voices are available by means of two separate set of segments and if required the type of voice can be changed in real time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-86"
  },
  "king91_eurospeech": {
   "authors": [
    [
     "R. W.",
     "King"
    ]
   ],
   "title": "Tools and processes for developing low-cost and high-quality text-to-speech synthesis for communication aids",
   "original": "e91_0325",
   "page_count": 4,
   "order": 88,
   "p1": "325",
   "pn": "329",
   "abstract": [
    "This paper describes some aspects of the development of speech synthesis systems as communication aids. The work aims towards aids in which a range of accents and male and female voices can be provided. In order to minimize hardware, and also provide the user with a small number of recorded system messages, the proposed system exploits a single formant synthesizer device both for 'copy synthesis', and for the output of the text-to-speech software processes. The paper describes in some detail a software editor tool for defining formant and excitation data for the chosen phoneme set, and for examination of phoneme interpolation processes. This editor has been developed for the Philips PCF8200 single chip serial formant synthesizer device, though its functional principles may be applied to other devices and modules. The use of published and recorded data as sources of material for specification of target phoneme data for different voices is discussed. Keywords: speech synthesis, text to speech, communication aids.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-87"
  },
  "hermansky91_eurospeech": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Louis",
     "Anthony Cox Jr."
    ]
   ],
   "title": "Perceptual linear predictive (PLP) analysis-resynthesis technique",
   "original": "e91_0329",
   "page_count": 4,
   "order": 89,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "A technique for re-synthesis of speech from its low-dimensional PLP representation is presented. The technique is based on a speaker-dependent linear mapping from the vector of cepstral coefficients of the low-order PLP model to a vector of resonance peaks of the synthesis model. The linear mapping can be constructed from a relatively small amount of training data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-88"
  },
  "greisbach91_eurospeech": {
   "authors": [
    [
     "Reinhold",
     "Greisbach"
    ],
    [
     "Bernd J.",
     "Kröger"
    ],
    [
     "O.",
     "Esser"
    ],
    [
     "G.",
     "Plaßmann"
    ]
   ],
   "title": "A display technique for measurements of natural and synthetic articulatory dynamics",
   "original": "e91_0333",
   "page_count": 4,
   "order": 90,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "A method for the visual representation of two-dimensional time-varying articulatory data (\"polargram\") is described which allows direct comparison with the corresponding acoustic signal. It also helps to evaluate simulation of the articulatory dynamics of an articulation based model of speech production in relation to natural data measurements. Keywords: Polargram, articulatory dynamics, articulation based model of speech production\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-89"
  },
  "chang91_eurospeech": {
   "authors": [
    [
     "Yueh-Chin",
     "Chang"
    ],
    [
     "Yi-Fan",
     "Lee"
    ],
    [
     "Bang-Er",
     "Shia"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Statistical models for the Chinese text-to-speech system",
   "original": "e91_0337",
   "page_count": 4,
   "order": 91,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "This paper presents an approach using statistical models for the implementation of Chinese text-to-speech system. Since Chinese is a syllablic and tonal language, the synthesis of Chinese speech can be accomplished by connecting the syllables in sequence and imbedding the prosodic rules. In this study, the database of text-reading speech provided by a male college student is analyzed to obtain the statistical models of prosodic features, such as the pitch patterns of the syllables, the durations of the syllables, and the declination of the pitch level in a sentence. During the synthesis procedure, the optimal pitch pattern sequence and duration sequence are determined for a given sentence by using Viterbi algorithm. The intonation of a sentence is implemented by shifting the pitch level. The silence pauses are imposed according to several simple pause rules. An experimental system is implemented on an IBM-PC compatible computer equipped with a TMS-32010-based DSP board. A perceptural test is conducted to evaluate the intelligibility and natualness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-90"
  },
  "taylor91_eurospeech": {
   "authors": [
    [
     "P. A.",
     "Taylor"
    ],
    [
     "I. A.",
     "Nairn"
    ],
    [
     "A. M.",
     "Sutherland"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "A realtime speech synthesis system",
   "original": "e91_0341",
   "page_count": 4,
   "order": 92,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "A real time speech synthesis system for a personal computer is described. Details are given of real time considerations and an explanation is given as to how the system was implemented. The main modules of the linguistic and synthesis component are described. The flexible configuration of the system is demonstrated and two example applications are examined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-91"
  },
  "valbret91_eurospeech": {
   "authors": [
    [
     "H.",
     "Valbret"
    ],
    [
     "E.",
     "Moulines"
    ],
    [
     "Jean-Pierre",
     "Tubach"
    ]
   ],
   "title": "Voice tranformation using PSOLA technique",
   "original": "e91_0345",
   "page_count": 4,
   "order": 93,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Whereas speaker normalization and adaptation has received a lot of attention for speech recognition, few studies have been devoted to voice transformation for speech synthesis despite the potential interestof such techniques. Converting voice individuality needs spectrum, glottal excitation and prosody modifications. This work focuses on spectral modifications but some easy prosodic alterations are taken into account. We combine two techniques to simulate speaker changement. The first one is the TD-PSOLA technique which is very efficient to alter prosody. The second is a classical source-filter decomposition. It extracts from the signal a spectral representation on which spectral modifications arc performed. Two approaches are suggested to transform the spectrum: the first is the well-known Linear Multivariate Regression; the second is the Dynamic Frequency Warping.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-92"
  },
  "giustiniani91_eurospeech": {
   "authors": [
    [
     "M.",
     "Giustiniani"
    ],
    [
     "Piero",
     "Pierucci"
    ]
   ],
   "title": "Phonetic ergodic HMM for speech synthesis",
   "original": "e91_0349",
   "page_count": 4,
   "order": 94,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "In the present paper a statistical approach to the representation of speech units for speech synthesis applications is proposed. The approach is based on the use of two different Ergodic IIMM, an Acoustic EHMM (AEIIMM) and a PMonctic EIIMM (PIIEIIMM). The first is representative of the alphabet of elementary sounds, the second is representative of the correspondance among phonetic units, in the linguistic sense, and elementary sounds. In this way the whole process of associating a sequence of spectral states to a sequence of phonetic units is seen as a two-level stochastic process. The paper initially describes the main characteristics of the proposed models, shows how they can be included in an I/PC based text-to-speech synthesizer, and reports some example of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-93"
  },
  "delogu91_eurospeech": {
   "authors": [
    [
     "C.",
     "Delogu"
    ],
    [
     "P.",
     "Paoloni"
    ],
    [
     "P.",
     "Pocci"
    ],
    [
     "C.",
     "Sementina"
    ]
   ],
   "title": "Quality evaluation of text-to-speech synthesizers using magnitude estimation, categorical estimation, pair comparison and reaction time methods",
   "original": "e91_0353",
   "page_count": 3,
   "order": 95,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "Recent developments in the field of speech technology leead us to assume that speech synthesis techniques could offer a quality sufficient for practical applications, such as information services or aids for handicapped people (Deliege 1989). To facilitate the diffusion of these applications it is necessary to develop reliable and valid performance measures to compare different systems for different applications. Some results on comparisons among different evaluation methodologies such as categorical estimation, magnitude estimation, paired comparison, and reaction time, are reported in the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-94"
  },
  "zingte91_eurospeech": {
   "authors": [
    [
     "H.",
     "Zingte"
    ],
    [
     "Cl.",
     "Hennebois"
    ]
   ],
   "title": "Helping young children to associate sounds and letters through speech synthesis",
   "original": "e91_0357",
   "page_count": 3,
   "order": 96,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "LUDILEC is a software intending to help young children to associate oral and orthographic expressions of language through speech synthesis. This paper describes the features of LUDILEC, its contribution to symbolic learning and the first results in pedagogical experimentation. Keywords: speech synthesis, computer aided learning\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-95"
  },
  "bourlard91_eurospeech": {
   "authors": [
    [
     "Herve",
     "Bourlard"
    ]
   ],
   "title": "Neural nets and hidden Markov models: review and generalizations",
   "original": "e91_0363",
   "page_count": 7,
   "order": 97,
   "p1": "363",
   "pn": "369",
   "abstract": [
    "Previous work has shown the ability of Artificial Neural Networks (ANN), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-96"
  },
  "jayant91_eurospeech": {
   "authors": [
    [
     "N. S.",
     "Jayant"
    ],
    [
     "J. D.",
     "Johnston"
    ],
    [
     "Y.",
     "Shoham"
    ]
   ],
   "title": "Coding of wideband speech",
   "original": "e91_0373",
   "page_count": 7,
   "order": 98,
   "p1": "373",
   "pn": "379",
   "abstract": [
    "The technologies of ISDN teleconferencing, CD-ROM multimedia services, and High Definition Television are creating new opportunities and challenges for the digital coding of wideband audio signals, wideband speech in particular. In the coding of wideband speech, an important point of reference is the CCITT standard for 7 kHz speech at a rate of 64 kbps. Results of recent research are pointing to better capabilities-higher signal bandwidth at 64 kbps, and 7 kHz bandwidth at lower bit rates such as 32 and 16 kbps. The coding of audio with a signal bandwidth of 20 kHz is receiving significant attention due to recent activity in the ISO (International Standards Organization), with a goal of storing a CD-grade monophonic audio channel at a bit rate not exceeding 128 kbps. Prospects for accomplishing this are very good. As a side result, emerging algorithms will offer very attractive options at lower rates such as 96 and 64 kbps. As we address new challenges in wideband speech technology, several strides in coding research are likely to occur. Among these are refinements of existing models for auditory noise-masking, and a unification of linear prediction and frequency-domain coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-97"
  },
  "pieraccini91_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Esther",
     "Levin"
    ]
   ],
   "title": "Stochastic representation of semantic structure for speech understanding",
   "original": "e91_0383",
   "page_count": 4,
   "order": 99,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "We propose a model for a statistical representation of the conceptual structure of a restricted subset of spoken natural language. The model is used for segmenting a sentence into phrases and labeling them with concept relations (or cases). The model is trained using a corpus of annotated transcribed sentences. An understanding system is being built around this model, allowing for unconstrained spoken input in a database retrieval task. The results on a test set of 148 sentences show that almost 97% of cases were correctly assigned.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-98"
  },
  "matheson91_eurospeech": {
   "authors": [
    [
     "Colin",
     "Matheson"
    ],
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "Incorporating probabilities into the dualgram language model",
   "original": "e91_0387",
   "page_count": 3,
   "order": 100,
   "p1": "387",
   "pn": "390",
   "abstract": [
    "It is arguable that for certain applications of speech recognition technology it is useful to employ a language model whose characteristics lie between bigrams and trigrams. While bigrams are efficient and initially effective, the perplexity grows quickly as the model is extended to a point where recognition accuracy is affected. The move to trigrams solves this problem at the expense of increasing the amount of corpus required to levels which may be uneconomic. The DUALGRAM model was proposed in Matheson et al. 1990 [1] as a possible solution to some of these problems and initial results were presented. The present paper reviews the main points of the DUALGRAM approach and reports the addition of probabilities to the model. It has been demonstrated many times that probabilistic bigrams produce better recognition results than simple follow-set models, and similar improvements are to be expected with DUAL-GRAM. Finally, a new method of combining the probabilities derived from the two halves of the model is proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-99"
  },
  "giachin91_eurospeech": {
   "authors": [
    [
     "Egidio P.",
     "Giachin"
    ]
   ],
   "title": "A dynamic programming based framework for stochastic spoken language understanding",
   "original": "e91_0391",
   "page_count": 4,
   "order": 101,
   "p1": "391",
   "pn": "394",
   "abstract": [
    "We describe a language model suitable for integration with an HMM-based, speaker-independent, 1000-word recognizer. The system accepts spoken queries and commands to an electronic mailbox. The model represents linguistic information through one single finite state network, whose size is kept small thanks to a procedure for ignoring language constructs not semantically relevant for the task. Experimental evaluations show that this approach competes with other approaches based on the use of phrase-structure grammars for parsing lattices of word hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-100"
  },
  "prieto91_eurospeech": {
   "authors": [
    [
     "Natividad",
     "Prieto"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "Learning language models through the ECGI method",
   "original": "e91_0395",
   "page_count": 4,
   "order": 102,
   "p1": "395",
   "pn": "398",
   "abstract": [
    "A new approach to adaptive Semantic-Language modelling has recently been proposed which allows automatic learning of all the acoustic and syntactic-semantic models that are required for a given Continuous Speech Recognition task. The proposed approach is based on the so called \"Error Correcting Grammatical Inference\" algorithm which supplies homogeneous finite-state structural models both at the acoustic and at the syntactic-semantic levels. Recognition or Understanding is seen as a Formal Transduction procedure that exploits the set of acoustic and linguistic constraints that have been captured in the learned models to directly input raw acoustic signals and output the semantic messages that are conveyed by these signals. In this paper the proposed approach is reviewed and new improvements are presented. Also, preliminary results with a large semantic-space continuous speech task (Spanish numbers in the one-million range) are presented showing the currently achieved capabilities of this approach. KEYWORDS: Language Modelling, Grammatical Inference, Adaptive Language Acquisition, Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-101"
  },
  "cremonini91_eurospeech": {
   "authors": [
    [
     "R.",
     "Cremonini"
    ],
    [
     "M.",
     "Ferretti"
    ],
    [
     "M. C.",
     "Galimberti"
    ],
    [
     "Giulio",
     "Maltese"
    ],
    [
     "Federico",
     "Mancini"
    ]
   ],
   "title": "Using a generative grammar to train a probabilistic language model for speaker-independent speech recognition",
   "original": "e91_0399",
   "page_count": 4,
   "order": 103,
   "p1": "399",
   "pn": "402",
   "abstract": [
    "This article describes a voice-activated flight information and reservation system. The system is based on the speaker-dependent, real-time, large vocabulary speech recognizer developed at the IBM Rome Scientific Center. To remove the speaker-dependency constraint, a speaker-independent acoustic model was built. To achieve a high recognition rate, a technique was developed to introduce grammatical constraints into the probabilistic language model of the recognizer. In particular, instead of substituting the language model for a grammar, we trained it on the \"complete\" corpus of the language to recognize. The corpus was generated using a generative grammar. No grammar was used during the recognition process. The high recognition rate attests the validity of the approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-102"
  },
  "shirai91b_eurospeech": {
   "authors": [
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "E.",
     "Kitagawa"
    ],
    [
     "T.",
     "Endo"
    ]
   ],
   "title": "Optimal construction of context sensitive quantizer for phoneme recognition in continuous speech",
   "original": "e91_0405",
   "page_count": 4,
   "order": 104,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "It is a basic problem how to construct an effective code book discriminating phoneme classes. Especially, if various acoustic. features are used, they are mutually related and also have context dependent characteristics. In this paper, an optimal code book is obtained by clustering various acoustic features by maximizing the mutual information between the acoustic and phoneme categories. The main issue of this paper is the consideration of context in the hierarchy of acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-103"
  },
  "okane91_eurospeech": {
   "authors": [
    [
     "M.",
     "O'Kane"
    ],
    [
     "P.",
     "Kenne"
    ],
    [
     "D.",
     "Landy"
    ],
    [
     "S.",
     "Atkins"
    ]
   ],
   "title": "Generalising from single-speaker recognition in a feature-based recogniser",
   "original": "e91_0409",
   "page_count": 4,
   "order": 105,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "Speaker-dependent recognition of continuous speech is generally held to be an easier task than speaker-independent continuous speech recognition. In this paper we describe experiments which attempt to quantify this assertion for a particular feature-based recogniser. The primary basis for the study is an investigation of how well a feature-based recogniser developed to work near-perfectly on one particular speaker works on continuous speech examples from a large number of other speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-104"
  },
  "hirsch91_eurospeech": {
   "authors": [
    [
     "H. G.",
     "Hirsch"
    ],
    [
     "Peter",
     "Meyer"
    ],
    [
     "Hans-Wilhelm",
     "Ruehl"
    ]
   ],
   "title": "Improved speech recognition using high-pass filtering of subband envelopes",
   "original": "e91_0413",
   "page_count": 4,
   "order": 106,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "To cope with variability of speech introduced by e. g. background noise, different conditions for training and recognition, and by changes of transmission or speaker characteristics, two series of experiments were carried out using high-pass filtering of spectral envelopes for speech recognition. In the first series of experiments, FIR high-pass filters were evaluated to reduce the influence of background noise for isolated-word recognition. Introducing high-pass filtering, S/N ratio may be increased by 7 dB while keeping recognition rate constant at 95%. In the second line of experiments, MR high-pass filtering was examined to improve speaker independency and robustness of a connected-words recogniser. For 7-digit strings, the digit error rate was reduced from 21. 8% to 2. 0%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-105"
  },
  "gong91_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Comparing two phoneme identification methods using a continuous speech recognizer",
   "original": "e91_0417",
   "page_count": 4,
   "order": 107,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "We describe two techniques for phoneme identification, which give the phoneme plausibility at a frame slot, for all phonemes and all frames. In the first one, non-linear vectorial interpolation, the correlation between vectors of the input speech as well as between components within vectors are supposed to be specific to phonetic units and are extracted using non-linear vector interpolators trained for each phoneme to minimize interpolation error. Three categories of interpolators are introduced, according to quantities to be interpolated: vector-pair, vector-center and component-pair. The second one, reference comparison, estimates the plausibility by template matching which consists in computing slot by slot the maximum of the average similarity between references and input speech vectors. We have evaluated the performance of the four models (reference comparison model and non-linear interpolator models) using a phoneme-based continuous speech recognition system. Throughout the experiments the four systems were connected successively to the same large vocabulary continuous speech recognition system as phoneme recognition input. In speaker-dependent tests using 30 sentences as training data and 12 LPCC-derived cepstral coefficients as parametric vectors, the reference comparison method yielded best, global recognition rate, while vector-pair model showed highly promising. Keywords: phoneme recognition, non-linear interpolation, artificial neural networks\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-106"
  },
  "ederveen91_eurospeech": {
   "authors": [
    [
     "D.",
     "Ederveen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Knowledge-based phoneme recognition",
   "original": "e91_0421",
   "page_count": 4,
   "order": 108,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "We are building a knowledge-based phoneme recognition system. After a short introduction to the system, and a discussion of some theoretical motivations, we present some results from the first stage: the recognition of the voiceless plosive class and the voiceless fricative class. Keywords: speech recognition, acoustic phonetic knowledge\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-107"
  },
  "kraayeveld91_eurospeech": {
   "authors": [
    [
     "J.",
     "Kraayeveld"
    ],
    [
     "A. C. M.",
     "Rietveld"
    ],
    [
     "V. J. van",
     "Heuven"
    ]
   ],
   "title": "Speaker characterization in dutch using prosodic parameters",
   "original": "e91_0427",
   "page_count": 4,
   "order": 109,
   "p1": "427",
   "pn": "430",
   "abstract": [
    "In this study the speaker characterising properties of two methods of representing pitch contours were compared. The first is Atal's (1972) approach, in which the entire intonation contour is divided up into 40 segments that form the input to data reduction and analysis techniques. The second method is a more analytical one, in which the contour is summarized by measurements that are related to 'key points' in the contour. The first approach turned out to yield superior recognition results. However, these results must probably be attributed to differences in the underlying phonological form and not to individual differences in the realisation of this representation. Keywords: speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-108"
  },
  "hunt91_eurospeech": {
   "authors": [
    [
     "Alan K.",
     "Hunt"
    ]
   ],
   "title": "New commercial applications of telephone-network-based speech recognition and speaker verification",
   "original": "e91_0431",
   "page_count": 4,
   "order": 110,
   "p1": "431",
   "pn": "434",
   "abstract": [
    "VCS has recently developed and made commercially available a system providing speaker verification and simultaneous speaker-independent speech recognition over the public switched telephone network. By combining speech recognition and speaker verification, the system (1) provides secure access to all callers, including rotary telephone users, and (2) performs verification transparently during ordinary transactions, rather than requiring entry of supplemental phrases or PINs. This dual capability was achieved through a natural extension of existing VCS telephone-channel speaker-independent speech recognition technology, in commercial use since early 1987. The new technology resides in the same compact hardware platforms as have been used by prior recognition-only systems, and incurs virtually no additional computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-109"
  },
  "bonastre91_eurospeech": {
   "authors": [
    [
     "Jean-Francois",
     "Bonastre"
    ],
    [
     "Henri",
     "Meloni"
    ],
    [
     "Philippe",
     "Langlais"
    ]
   ],
   "title": "Analytical strategy for speaker identification",
   "original": "e91_0435",
   "page_count": 4,
   "order": 111,
   "p1": "435",
   "pn": "438",
   "abstract": [
    "Within the framework of an analytical system for speaker recognition, we present a study relating to characteristic information concerning individuals, expressed in short term spectra. The reference base is constituted by spectra representing stable phases of the most discriminatory phonemes, for several dozen speakers. The references are acquired automatically from a number of sentences, in which the selected phonemes appear in low distortion contexts. The speakers are classified by providing the system with the phonemic decomposition of a particular utterance. Once the interesting zones containing useful phonemes have automatically been localized, the resemblance score is calculated using the distances between the corresponding spectra and the references. The results validate the method utilized as soon as the number of useful phonemes is sufficient. Keywords: Speaker identification, Analytic method, Short term spectra information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-110"
  },
  "xu91_eurospeech": {
   "authors": [
    [
     "L.",
     "Xu"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "Optimization of perceptually-based spectral transforms in speaker identification",
   "original": "e91_0439",
   "page_count": 4,
   "order": 112,
   "p1": "439",
   "pn": "442",
   "abstract": [
    "It is recently reported in [l][2] that perceptually-based linear prediction, PLP, features achieve significantly better speaker recognition results than when using standard LPC features. The superiority of the PLP model is attributed to a series of perceptually-based spectral transforms, applied prior to deriving feature sequences from the standard linear prediction process. This paper investigates further the use of PLP features in speaker identification, focusing on the contributions of each of the perceptual factors. PLP, as proposed originally by Hermansky [3] was optimised for speech recognition. This paper demonstrates that, not surprisingly, different optimum conditions apply for speaker recognition. In particular we show the distinct benefit of increasing the number of critical bands (from the original 17 up to 64). The increased spectral detail is clearly important in this task, and ASI experiments based on 1000 single-digit tests in digit-independent codebook scheme gives a 2. 7% error rate for the modified PLP method, compared with 4. 7% and 6. 5% when using the original PLP and standard LPC models respectively. Furthermore, it is found that all the perceptual weightings considered in the PLP model to some extent enhance the performance, and in agreement with Gu's findings in speech recognition [4], ASI performance is shown to be relatively insensitive to the precise masking pattern.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-111"
  },
  "cheveigne91_eurospeech": {
   "authors": [
    [
     "Alain de",
     "Cheveigne"
    ]
   ],
   "title": "A mixed speech F0 estimation algorithm",
   "original": "e91_0445",
   "page_count": 4,
   "order": 113,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "This mixed speech fundamental frequency (f0) estimation algorithm is an extension of the classical AMDF (Average Magnitude Difference Function) algorithm for one voice. An exhaustive search of the parameter space of two cascaded time-domain comb filters yields an estimation of the periods of the component voices. The algorithm, which is computationally expensive but easily parallelizable, was tested on a database of continuous male and female speech. Segments of voiced speech, selected according to a \"good periodicity\" criterion to ensure that the reference single-voice f0 algorithm would not fail (this criterion rejected 25% of voiced speech frames), were paired and summed to simulate mixed speech. The search range of the algorithm was limited to a 3 octave range, and search was performed frame-by-frame without continuity constraints. The resulting estimates were compared to those of the reference algorithm and found to be within 3 % of target values for 90 % of all frames. Keywords: speech, fundamental frequency, pitch extraction, mixed speech separation, noise reduction, cocktail-party effect.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-112"
  },
  "jones91_eurospeech": {
   "authors": [
    [
     "Edward",
     "Jones"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "A perceptually-based pitch extractor for band-limited speech",
   "original": "e91_0449",
   "page_count": 4,
   "order": 114,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "This paper describes a frequency-domain method of extracting the fundamental frequency of voiced speech which has been band-limited to 300 Hz to 3. 4 KHz. The method uses a linear auditory model into which non-linearity has been introduced. Two methods for introducing the non-linearity into the model are described. Harmonic product spectra are derived from the outputs of the linear and non-linear auditory models. Results show that the spectrum derived from the output of the non-linear auditory model is superior to that obtained from the output of the linear model. Keywords: auditory modelling, speech processing, pitch extraction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-113"
  },
  "gu91_eurospeech": {
   "authors": [
    [
     "Yu Hua",
     "Gu"
    ]
   ],
   "title": "A robust pseudo perceptual pitch estimator",
   "original": "e91_0453",
   "page_count": 4,
   "order": 115,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "A new pitch estimation algorithm is proposed which is robust against various distortions on speech signals such as white noise and interference speech. The algorithm exploits the coincidence appearance of pitch information contained either in signal envelopes or in signal carriers. It is found that better estimation is obtained in the extremely noisy circumstances by combining these two pieces of information. The algorithm is based on the time directional analysis of wideband filtered signals, which is globally consistent with the auditory processing without mimicking its behavior. Simulations on noisy speech signals showed evidence of its robustness. Keywords: pitch estimation, signal envelope & carrier.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-114"
  },
  "degan91_eurospeech": {
   "authors": [
    [
     "N. Dal",
     "Degan"
    ],
    [
     "M.",
     "Fratti"
    ]
   ],
   "title": "Pitch estimation based on a \"narrowed\" autocorrelation function",
   "original": "e91_0457",
   "page_count": 4,
   "order": 116,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "This paper presents experiments and results relative to a pitch determination of speech signals based on the \"Narrowed\" Autocorrelation Function (NACF), recently introduced by Brown and Puckette [1], It is shown that a reliable pitch estimation can be obtained even in a very noisy environment. An advantage of the NACF is that no preprocessing is needed before performing the pitch detection task. Also, a simple post-processing technique is sufficient to recover from the rare, isolated gross pitch errors that may occur. Finally, the paper proposes a short-cut in the computation of the NACF and it is shown that the number of multiplications can be drastically reduced. Keywords: Pitch, Narrowed autocorrelation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-115"
  },
  "nakagawa91_eurospeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Yoshimitsu",
     "Hirata"
    ],
    [
     "Isao",
     "Murase"
    ]
   ],
   "title": "The syntax-oriented spoken Japanese understanding system SPOJOS-SYNO II",
   "original": "e91_0463",
   "page_count": 4,
   "order": 117,
   "p1": "463",
   "pn": "466",
   "abstract": [
    "This paper describes a syntax oriented spoken Japanese understanding system named \"SPOJUS-SYNO. At first this system makes word based Hidden-Markov-Models (HMM) automatically by concatenating syllable-based (trained) HMMs. Then a word lattice is hypothesized by using a word spotting algorithm and word-based HMMs for an input utterance. In SPOJUS-SYNO, the time-synchronous left-to-right parsing algorithm is executed to find the best word sequence from the word lattice according to syntactic & semantic knowledge represented by a context free semantic grammar. This system was implemented in the \"UNIX-QA\" task with the vocabulary size of 521 words. Experimental result shows that the sentence recognition / understanding rate was about 80 / 87% for six male speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-116"
  },
  "bergmann91_eurospeech": {
   "authors": [
    [
     "H.",
     "Bergmann"
    ],
    [
     "H.-H.",
     "Hamer"
    ],
    [
     "A.",
     "Noll"
    ],
    [
     "A.",
     "Paeseler"
    ],
    [
     "H.",
     "Tomaschewski"
    ]
   ],
   "title": "An adaptable man-machine interface using connected-word recognition",
   "original": "e91_0467",
   "page_count": 4,
   "order": 118,
   "p1": "467",
   "pn": "470",
   "abstract": [
    "The architecture of the aspect speech recognition system, a speaker-dependent connected-word recognizer is described. As an integral part the system contains an adaptable interface to application programs, based on partitioned finite-state semantic grammars and a message passing scheme to track the state transitions of the application. The interface design is demonstrated to be effective, both in reducing perplexity and in the context-dependent interpretation of recognized word sequences. Finally, a PC-based real-time implementation is presented, which consists of two separate processes: the connected-word recognizer and a window-oriented sample application written in an object-oriented LISP extension.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-117"
  },
  "poza91_eurospeech": {
   "authors": [
    [
     "M. J.",
     "Poza"
    ],
    [
     "C. de la",
     "Torre"
    ],
    [
     "D.",
     "Tapias"
    ],
    [
     "L.",
     "Villarrubia"
    ]
   ],
   "title": "An approach to automatic recognition of keywords in unconstrained speech using parametric models",
   "original": "e91_0471",
   "page_count": 4,
   "order": 119,
   "p1": "471",
   "pn": "474",
   "abstract": [
    "This paper describes the field trial of a speech system with an isolated word recognizer (HMM based) that offers a 'News Service' to the general public over the Spanish telephone network. The results of this trial show that real recognition rates are substantially lower that laboratory recognition rates, due mainly to human factor aspects, e. g. users do always not follow system instructions, speaking fluently instead of uttering isolated words when the system prompts them. To cope with this practical situation, a connected word recognition algorithm has been implemented, and some parametric models have been computed to perform keyword spotting. The algorithm and the way this models have been trained will be presented, and the results of the new system will be exposed. Keywords: Hidden Markov Models, Word Spotting, Telephone line, Speech Machine\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-118"
  },
  "hetherington91_eurospeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ],
    [
     "Hong C.",
     "Leung"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Toward vocabulary-independent recognition of telephone speech",
   "original": "e91_0475",
   "page_count": 4,
   "order": 120,
   "p1": "475",
   "pn": "478",
   "abstract": [
    "This paper describes a set of experiments using a speaker-independent, isolated-word recognition system over the telephone network. We are concerned with the sensitivity of recognition accuracy to a set of external factors. Specifically, we want to know how much performance degradation can be expected when no vocabulary-specific data are available for training. We also want to determine the relative importance of several factors in choosing a vocabulary-independent training set including: data collection paradigm, speaking mode, and recording environment. Our results indicate that recognition accuracy decreases appreciably in the absence of vocabulary-specific training data. If vocabulary-specific training data are not available, reasonable initial recognition performance can be achieved by using a phonetically balanced corpus for training, preferably consisting of isolated words recorded over the telephone network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-119"
  },
  "cole91_eurospeech": {
   "authors": [
    [
     "Ronald",
     "Cole"
    ],
    [
     "Krist",
     "Roginski"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "English alphabet recognition with telephone speech",
   "original": "e91_0479",
   "page_count": 4,
   "order": 121,
   "p1": "479",
   "pn": "482",
   "abstract": [
    "We describe database and system development for speaker-independent recognition of telephone speech. The telephone speech database contains about 4,000 callers from the USA and Canada each of whom provided several utterances, including city names, first and last names, spelled names, and answers to yes/no questions. About 1,000 of the callers recited the English alphabet with pauses between letters. A portion of the database has been verified and phonetically labeled, and this portion was used to develop a baseline system that recognizes names spelled with pauses between letters. The system uses a neural network to segment speech into a sequence of 24 phonetic categories. The phonetic categories are used to hypothesize a sequence of letters which are then reclassified using a second neural network. First choice letter recognition accuracy was 87. 6% in the best condition. First choice name retrieval was S5. 5% for 200 spelled names retrieved from a database of 50,000 common last names.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-120"
  },
  "fiset91_eurospeech": {
   "authors": [
    [
     "J.-Y.",
     "Fiset"
    ],
    [
     "J.-M.",
     "Robert"
    ],
    [
     "Raymond",
     "Descout"
    ]
   ],
   "title": "Evolutionary language models in air traffic control training",
   "original": "e91_0483",
   "page_count": 4,
   "order": 122,
   "p1": "483",
   "pn": "486",
   "abstract": [
    "A research has been conducted to adapt a frame-based language model to a voice-driven pseudo-pilot system. A pseudo-pilot is used in the training of air traffic controllers. The model was derived in an evolutionary fashion: a first frame-based model was obtained for simple exercises. Then, a more complex type of exercises was analyzed and extensions to the simple model were identified to adapt it to the more complex type of exercises. Dialogue error handling mechanisms were also identified and modeled. KEYWORDS: dialogue, air traffic control, language model, error handling, frame\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-121"
  },
  "jones91b_eurospeech": {
   "authors": [
    [
     "G. J. F.",
     "Jones"
    ],
    [
     "J. H.",
     "Wright"
    ],
    [
     "E. N.",
     "Wrigley"
    ],
    [
     "M. J.",
     "Carey"
    ],
    [
     "Eluned S.",
     "Parris"
    ]
   ],
   "title": "Isolated-word sentence recognition using probabilistic context-free grammar",
   "original": "e91_0487",
   "page_count": 3,
   "order": 123,
   "p1": "487",
   "pn": "489",
   "abstract": [
    "A hybrid speech recognition system is described, in which both a grammar model and a bigram model are used to aid sentence recognition. Model selection is automatic for each utterance, being based on the highest probability. The system has been tested with several speakers and for both grammatical and ungrammatical sentences. Although recognition performance is best for grammatical sentences, the bigram model is successfully invoked to pick up the others. Keywords: speech recognition, probabilistic context-free grammar, bigram models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-122"
  },
  "hood91_eurospeech": {
   "authors": [
    [
     "Mitchell",
     "Hood"
    ]
   ],
   "title": "Lexical access in a speech understanding and dialogue system",
   "original": "e91_0490",
   "page_count": 4,
   "order": 124,
   "p1": "490",
   "pn": "493",
   "abstract": [
    "This paper describes the lexical access component which forms part of a complete speech understanding system being developed within the ESPRIT Sundial project (P2218). The objective function of a lexical access component within a dialogue system is discussed. The paper presents results from application-specific tests. Recognition results are given for tests where the lexical access component is used alone and where it used in conjunction with a parser. A method for using lexical access to directly assess the performance of phoneme models is also presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-123"
  },
  "haebumbach91_eurospeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "A look-ahead search technique for large vocabulary continuous speech recognition",
   "original": "e91_0495",
   "page_count": 4,
   "order": 125,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "In a large vocabulary continuous speech recognition task the search for the \"best\" (in the maximum-a-posteriori sense) word sequence is the most (computing) time consuming part of the system. End-of-word hypotheses are created almost every time frame. With a stochastic language model every lexicon entry is an admissible successor candidate. By using a \"fast match\" module which scores the word candidates according to their acoustic feasibility ahead of the current time frame, the search cost can be considerably reduced. Only the fraction of the words with favourable fast match scores will be further processed in the detailed match, where the likelihood of a segment of acoustics given the word model is computed. We derive a novel word selection strategy which is \"consistent\" in the sense that it introduces no additional decoding errors and which still reduces the search space by a factor of 2 - 3 compared to standard Viterbi beam search. Giving up the consistency requirement, pruning strategies can be deduced which further reduce the search effort significantly: the size of the word startup list is reduced to 2% - 4% of its original size with a modest increase in error rate by l%-2%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-124"
  },
  "teixeira91_eurospeech": {
   "authors": [
    [
     "Carlos J.",
     "Teixeira"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "Spectral subtraction for front-end noise reduction in a speech recognizer",
   "original": "e91_0499",
   "page_count": 4,
   "order": 126,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "The goal of this work is the design of a front-end processor to be used with state-of-the-art word recognizers, capable of reducing stationary noise from a speech signal transmitted over the public telephone network, to be implemented on a single DSP32C board, integrating an echo canceller module as well. The environment in which the speech signal is produced can vary from the typical office environment (with noise emerging from keyboards, air conditioning, etc. ) to public buildings, public phone booths and even homes. Besides this environmental noise, the speech signal is also corrupted by telephone line noise and its original amplitude and phase characteristics are also subject to channel distortion. The selected noise reduction method is the classic spectral subtraction algorithm. This selection was primarily motivated by its relative low complexity, in view of the real-time operation requirements. The tests performed so far with real environmental and telephone line noise signals confirm the effectiveness of integrating this module as a preprocessor for speech recognition in a public telephone network environment. Keywords: noise reduction, spectral subtraction, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-125"
  },
  "larnel91_eurospeech": {
   "authors": [
    [
     "Lori F.",
     "Larnel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "BREF, a large vocabulary spoken corpus for French",
   "original": "e91_0505",
   "page_count": 4,
   "order": 127,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "This paper presents some of the design considerations of BREF, a large read-speech corpus for French. BREF was designed to provide continuous speech data for the development of dictation machines, for the evaluation of continuous speech recognition systems (both speaker-dependent and speaker-independent), and for the study of phonological variations. The texts to be read were selected from 5 million words of the French newspaper, Le Monde. In total, 11,000 texts were selected, with selection criteria that emphasisized maximizing the number of distinct triphones. Separate text materials were selected for training and test corpora. Ninety speakers have been recorded, each providing between 5,000 and 10,000 words (approximately 40-70 min. ) of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-126"
  },
  "mathan91_eurospeech": {
   "authors": [
    [
     "Luc",
     "Mathan"
    ],
    [
     "Dominique",
     "Morin"
    ]
   ],
   "title": "Speech field databases: development and analysis",
   "original": "e91_0509",
   "page_count": 3,
   "order": 128,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "In automatic speech recognition applications, recognition rates are usually estimated using clean laboratory speech. This kind of data is of course needed to train the system before it is put in service, but it does not reflect the true behavior of users. As a result, the actual recognition rates are much lower. In addition, a good part of the input data is not even valid, not being among the vocabulary words. We present a short description of two field databases containing close to 26,000 speech tokens from real users. The field databases were extracted from two voice response systems equipped with isolated word recognition, operating on the French telephone network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-127"
  },
  "itahashi91_eurospeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Large scale Japanese dialect speech corpora",
   "original": "e91_0513",
   "page_count": 4,
   "order": 129,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "This paper describes Japanese efforts to collect speech samples of various Japanese dialects and to create speech databases. A Grant-in-Aid for Scientific Research for Priority Area project, entitled Trosodic Features of Spoken Japanese', started in 1989, is an attempt to collect various dialect speech samples from all over Japan and create speech databases for the purpose of linguistic, phonetic, acoustic and physiological analyses. Plans exist for collecting common speech items from 100 districts all over Japan. These comprise about 1450 common items, including words and sentences. All recordings are to be performed using DAT. Some of these are to be edited and converted into compact discs and CD-ROMs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-128"
  },
  "vossen91b_eurospeech": {
   "authors": [
    [
     "Paulus H.",
     "Vossen"
    ]
   ],
   "title": "Outline of a design-oriented evaluation framework for speech-driven applications",
   "original": "e91_0517",
   "page_count": 4,
   "order": 130,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "The basic technologies for speech input and output are ready for large-scale implementation, though improvements in several areas will certainly be made. What is still missing is an integrated framework for evaluation and design, in which the existing know-how about speech input and output ergonomics can be embedded. In this paper we describe the components of such a framework and a strategy for design-oriented system evaluation. We will focus on formative evaluation, i. e. a comprehensive scheme of assessment which can be used during speech system design. The proposed approach has been accepted within the ESPRIT project SUNSTAR as the common framework for assessment. Keywords: speech input and output assessment, dialogue evaluation, application evaluation, evaluation strategy, evaluation layers, evaluation criteria, evaluation methods\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-129"
  },
  "winski91_eurospeech": {
   "authors": [
    [
     "Richard",
     "Winski"
    ],
    [
     "Kamran",
     "Kordi"
    ]
   ],
   "title": "Assessment of continuous speech recognisers using recogniser sensitivity analysis",
   "original": "e91_0521",
   "page_count": 4,
   "order": 131,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "This paper describes the use of Recogniser Sensitivity Analysis (RSA) for performance assessment of a connected word, speaker independent speech recogniser. A number of speech-production factors were considered and an analysis of variance indicated that relative energy, word duration and speaker consistency were found to account for over 86% of the variability in recognition performance for this data set. Second-order interaction effects between these were not significant indicating that the parameters can be considered independent. This permits the performance to be predicted as a function of each parameter independently. The results confirm the value of RSA both for obtaining diagnostic information about recognition performance, and as a method of predicting performance. Keywords: performance assessment; continuous speech recognition; recogniser sensitivity analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-130"
  },
  "bourjot91_eurospeech": {
   "authors": [
    [
     "C.",
     "Bourjot"
    ],
    [
     "A.",
     "Boyer"
    ],
    [
     "D.",
     "Fohr"
    ]
   ],
   "title": "A tool for assessment of acoustic phonetic lattices",
   "original": "e91_0525",
   "page_count": 4,
   "order": 132,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "This paper proposes a methodology to compare the behaviour of acoustic phonetic decoders. We define a norm which allows an overall comparison of decoders providing lattices of very different size. For a more precised analysis, we display curves which draw the evolution of the number of errors with the number of selected pathes in the lattice. The classes of errors are parameters of the software. KEYWORDS lattice, acoustic phonetic decoder, assessment\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-131"
  },
  "steeneken91_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "Jeroen G. van",
     "Velden"
    ]
   ],
   "title": "Ramos - recognizer assessment by means of manipulation of speech applied to connected speech recognition",
   "original": "e91_0529",
   "page_count": 4,
   "order": 133,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "The performance of a recognizer or recognition algorithm is studied as a function of the variation of specific speech production and speech transmission parameters (production and post-production factors). Production factors include all speaker related production parameters. Post-production factors include speech transmission parameters. The method uses a data-base with minimal difference word sets based on CVC-words. The CVC-words are embedded in a carrier phrase in order to obtain connected word conditions. Three groups of test-words are used, related to: initial consonants, final consonants and vowels. By means of an analysis-resynthesis technique the words are physically manipulated in such a way that the effect of the manipulations corresponds to a range of conditions for a given parameter representing variations within and between male and female voices. This method was presented earlier by Steeneken and v. Velden (1989). The extension to connected word recognition is described in this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-132"
  },
  "alphen91_eurospeech": {
   "authors": [
    [
     "Paul van",
     "Alphen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Comparing various feature vectors in automatic speech recognition",
   "original": "e91_0533",
   "page_count": 4,
   "order": 134,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "In speech recognition suitable feature extraction is an important issue. In our research we tested the performance of several individual feature vectors, as well as various combinations of these vectors. As a basis for the feature vectors we use two types of analysis: a 15-band filterbank and an LPC-cepstral analysis. From these base-feature vectors several other feature vectors are derived. In the recognition part of our system (REXY) we used HMM's to model phone-like-units (PLU's). The system is trained on continuous speech of one male speaker. We tested the different feature vectors separately, and some of their combinations (combining feature vectors is done at a probabilistic level). The overall performance of the filterbank analysis turns out to be better than the LPC-cepstral analysis. keywords; speech recognition, HMM, feature vectors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-133"
  },
  "zue91_eurospeech": {
   "authors": [
    [
     "Victor W.",
     "Zue"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "David",
     "Goodine"
    ],
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "Hong C.",
     "Leung"
    ],
    [
     "Michael",
     "Phillips"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "The MIT ATIS system; preliminary development, spontaneous speech data collection, and performance evaluation",
   "original": "e91_0537",
   "page_count": 4,
   "order": 135,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "This paper describes the MIT ATIS system, with particular emphasis on the discourse and dialogue models and the consequences for data collection. The ATIS system provides air travel information and can simulate the booking of a flight, using a mixed-initiative dialogue framework. An intermediate semantic frame representation serves as the focal point for all back-end operations, and the discourse model includes the resolution of explicit anaphoric references and indirect and direct references to information mentioned earlier in the conversation. We have collected over 4500 utterances from subjects using the system to solve simulated booking scenarios. We have studied these dialogues, and have used them productively to guide the development of better discourse and dialogue models. We have also tabulated some interesting differences between our data and those collected at Texas Instruments using a very different paradigm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-134"
  },
  "benaouicha91_eurospeech": {
   "authors": [
    [
     "S.",
     "Benaouicha"
    ],
    [
     "A.",
     "Rajouani"
    ],
    [
     "M.",
     "Zyoute"
    ]
   ],
   "title": "Construction of an Arabic speech data base - duration model of Arabic vowels",
   "original": "e91_0541",
   "page_count": 4,
   "order": 136,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "Two text-to-speech systems for Arabic language have been realized in LEESA. Intelligibility of synthetic speech is satisfactory, however prosodic informations (intonation and rhythm) necessary for improving the naturalness remain rudimentary. In this paper, we will report on the adopted approach in modeling the variations of the vowels durations in Arabic speech. For each phonetic segment, a special coding includes lexical, phonetic information and their durations. Interrogation of database is realized by a procedure of the type \"searching by key-words\" implying a combination of the functions \"And, Or, But, Neither\". The interrogation is done in a sequential way and allows an automatic and interactive processing in getting statistics. Using the statistically significant factors, a duration model of Arabic vowels is proposed. The duration prediction using this model shows that the root mean square error between prediction duration and duration found in the data-base is equal to 14.74 ms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-135"
  },
  "denbigh91_eurospeech": {
   "authors": [
    [
     "P. N.",
     "Denbigh"
    ],
    [
     "J.",
     "Zhao"
    ]
   ],
   "title": "Pitch extraction and separation of overlapping speech",
   "original": "e91_0545",
   "page_count": 4,
   "order": 137,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "This paper describes two algorithms for separating two overlapping speech signals. Both rely heavily on accurate measurements of the pitch of the target voice. The first uses a single microphone, and an important feature is the exploitation of the onset of a voiced sound as an aid to the extraction of its pitch in the presence of interference. The second uses two microphones, and an important feature is the utilisation of the direction of the target voice.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-136"
  },
  "bengio91_eurospeech": {
   "authors": [
    [
     "Yoshua",
     "Bengio"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Half",
     "Kompe"
    ]
   ],
   "title": "Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks",
   "original": "e91_0551",
   "page_count": 4,
   "order": 138,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "In the framework of an ANN/HMM hybrid system for phone recognition three specialized ANNs were designed and evaluated. One of these ANNs detects the manner of articulation. The other two ANNs describe the speech signal in terms of place of articulation. One of these is used for plosive and nasal classification, and the other one is used for fricative classification. The design of these networks was inspired by acoustic-phonetic knowledge. Input parameters, ANN topology, and desired output representation have been optimized for the specific task of the network. A main advantage of ANNs over statistical classifiers like HMMs is seen in the possibility to use a large unconstrained feature set which can be setup in order to contain all necessary information rather than to fulfill statistical constraints. Experiments are reported for the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-137"
  },
  "carey91_eurospeech": {
   "authors": [
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Eluned S.",
     "Parris"
    ]
   ],
   "title": "Adapting input transformations using alpha-nets for whole word speech recognition",
   "original": "e91_0555",
   "page_count": 4,
   "order": 139,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "Alpha-nets interpret a set of Hidden Markov Models (HMMs) as a connectionist network enabling back propagation to be used to adapt the parameters of the HMMs, thus providing a powerful technique for improving the discriminatory ability of the HMMs. Previous work has focussed on the adaption of the state parameters of models in which the emission probability is described by a single mode Gaussian distribution. The parameters adapted can be the mixture coefficients themselves in addition to the multivariate means and variances or the elements of the input transformation. The input transformation may be linear or non-linear. We provide experimental results which show that, in a speaker independent isolated digit recognition task using telephone quality speech, adapting the parameters of the input transformation can reduce the error rate observed on the training set, while testing with unseen material produces less improvement in the system performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-138"
  },
  "niles91_eurospeech": {
   "authors": [
    [
     "Les T.",
     "Niles"
    ]
   ],
   "title": "TIMIT phoneme recognition using an HMM-derived recurrent neural network",
   "original": "e91_0559",
   "page_count": 4,
   "order": 140,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "Hidden Markov model (HMM) and recurrent neural net recognizers were used to classify phonetic segments from the TIMIT database. The neural net was derived from the HMM by removing the probability constraints on the parameters and using an error-correcting, minimum-mean-squared-error (MMSE) training criterion. The neural net was 0. 3% to nearly 5% better than the HMM (corresponding to 2% to 15% reductions in error rate), depending on the phonetic class. The improvement was due to both the error-correcting training and the removal of the parameter constraints. Some comparisons were also done with a maximum mutual information (MMI) training criterion; MMI yielded only slight improvements over ML, and was more difficult to train to than was MMSE.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-139"
  },
  "husoy91_eurospeech": {
   "authors": [
    [
     "P. O.",
     "Husoy"
    ],
    [
     "T.",
     "Svendsen"
    ]
   ],
   "title": "ANN-based speech recognition using a preprocessor for non-linear time compression",
   "original": "e91_0563",
   "page_count": 4,
   "order": 141,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "Artificial Neural Networks have generated widespread research activity also within the speech recognition community. Although ANN has a great potential, traditional methods have so far outperformed ANN-based solutions for speech recognition in most cases. In this study, we propose a method for non-linearly compressing the data presented to the classifier. The non-linear compression reduces the amount of input data, thereby simplifying the task of the network which in turn leads to improved performance. In a isolated word recognition task, using an alpha-digit vocabulary, the proposed system obtained a recognition score of 99. 1%, better than any of the \"traditional\" speech recognizers tested on the same task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-140"
  },
  "sorensen91_eurospeech": {
   "authors": [
    [
     "Helge B. D.",
     "Sorensen"
    ],
    [
     "Uwe",
     "Hartmann"
    ]
   ],
   "title": "A self-structuring neural noise reduction model",
   "original": "e91_0567",
   "page_count": 4,
   "order": 142,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "This paper describes how speech recognition in the presence of F-16 jet cockpit noise can be performed using a sequence of three units - an auditory model and two neural models. A method for noise reduction in the cepstral domian based on a self-structuring universal approximates is proposed and tested on a large database of isolated words contaminated with jet noise. This approach is a potential alternative to traditional recognition methods for noisy speech and makes noise reduction possible in all three models as in the system in [1]. The first model performs a spectral analysis of the input speech signal. The second model is a Self-structuring Neural Noise Reduction (SNNR) model, which is an alternative to the noise reduction model [1] presented at ICASSP91. The noise reduced output from the SNNR network is propagated through the speech recognizer consisting of a set of Hidden Control Neural Networks (HCNN).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-141"
  },
  "petek91_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Petek"
    ],
    [
     "Alex H.",
     "Waibel"
    ],
    [
     "Joseph M.",
     "Tebelskis"
    ]
   ],
   "title": "Integrated phoneme-function word architecture of hidden control neural networks for continuous speech recognition",
   "original": "e91_1407",
   "page_count": 4,
   "order": 143,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "We present a context-dependent, phoneme and function word based, Hidden Control Neural Network (HCNN) architecture for continuous speech recognition. The system can be seen as a large vocabulary extension of the word-based HCNN system proposed by Levin [Levin90]. Initially, we analysed the context-independent HCNN modeling principle in the framework of the Linked Predictive Neural Network speech recognition system [Tebelskis91] and found that it results in a 6% increase of the word recognition accuracy at perplexity 402. Significant savings in the resource requirements and computational load for the HCNN implementation can be achieved. In speaker-dependent recognition experiments with perplexity 111, the current versions of the LPNN and HCNN systems achieve 60% and 75% word recognition accuracy, respectively. Keywords: Automatic Speech Recognition, Hid- den Control Neural Network, Large vocabulary recognition, Context-dependent modeling, Function-word modeling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-142"
  },
  "zhang91_eurospeech": {
   "authors": [
    [
     "X.",
     "Zhang"
    ],
    [
     "J. S.",
     "Mason"
    ],
    [
     "E. C.",
     "Andrews"
    ]
   ],
   "title": "Multiple dynamic features to enhance neural net based speaker verification",
   "original": "e91_1411",
   "page_count": 4,
   "order": 144,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "This paper examines the performance of dynamic features in automatic speaker verification. In particular we consider feature combinations at the vector level, combining static with velocity and acceleration representations derived from regression analysis. Previous work has shown that these sets can be usefully combined at the mode/level, with a separate model for each feature type. However, in the case of a multi-layer perceptron (MLP) classifier, the vector level of combination might be more appropriate. We consider both word-serial and word-parallel inputs to the MLP, showing that the former is generally better. While the MLP is shown to give slightly better results than a vector quantization codebook approach, the inclusion of the dynamic features does not result in a significant improvement in performance. This is attributed to limitations in the net training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-143"
  },
  "haffner91_eurospeech": {
   "authors": [
    [
     "Patrick",
     "Haffner"
    ],
    [
     "Alex H.",
     "Waibel"
    ]
   ],
   "title": "Time-delay neural networks embedding time alignment: a performance analysis",
   "original": "e91_1415",
   "page_count": 4,
   "order": 145,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "Multi-State Time Delay Neural Networks (MS-TDNNs), using a new connectionist architecture with embedded time alignement, have been successfully applied to speaker-dependent continuous spoken letter recognition[lj. This shows the value of extending the classification capabilities of connectionist networks up to the word level in recognizing confusable vocabularies. This paper describes the application of MS-TDNNs to a very different task; speaker independent telephone-quality isolated digit recognition. The resulting 1. 6% error rate demonstrates the value of embedded time alignement, since multi-feature TDNNs, which do not implement time alignement, have a 6. 5% error rate on the same task. Comparisons with HMMs are also provided.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-144"
  },
  "fukuda91_eurospeech": {
   "authors": [
    [
     "Yohji",
     "Fukuda"
    ],
    [
     "Haruya",
     "Matsumoto"
    ]
   ],
   "title": "Phoneme recognition using recurrent neural networks",
   "original": "e91_1419",
   "page_count": 4,
   "order": 146,
   "p1": "1419",
   "pn": "1423",
   "abstract": [
    "In this paper, we describe the result of the introduction of a prediction layer and a similarity index in the phoneme recognition experiments based on a recurrent neural network. The proposed network has the prediction layer and the recognition layer in the output layer. The prediction layer predicts a next input vector from the present input vectors, and the recognition layer classifies them. The purpose of the prediction layer is to transfer a contextual information to the network. The activation of recognition layer is multiplied by a cosine value of angle made between the predicted vector and the actual input vector every time. We call this cosine value the similarity index. When the predicted vector is different from the actual input vector, the output of recognition layer becomes smaller, because of the multiplication of the similarity index, so that we avoid an incorrect classification of the recognition layer. Keywords: phoneme recognition, recurrent neural network, prediction layer, similarity index, contextual information\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-145"
  },
  "komori91_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Kaichiro",
     "Hatazaki"
    ]
   ],
   "title": "An integration of knowledge and neural networks toward a phoneme typewriter without a language model",
   "original": "e91_1423",
   "page_count": 4,
   "order": 147,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "This paper proposes a phoneme recognizer without any language model. The system is realized as an integration of spectrogram reading knowledge and Time-Delay Neural Networks. The system mainly consists of two parts: a consonant recognition part and a vowel recognition part, in which a sophisticated integration of knowledge and TDNN, is proposed. The knowledge part is mainly used for verification of ciitegories and boundaries. An experiment of speaker-dependent phoneme recognition without any language model, using 2,620 words, showed a 91. 4% recognition rate, a 3. 6% deletion error rate, a 5. 0% substitution error rate and a 20. 7% insertion error rate, for all Japanese phonemes. Keywords: Speech recognition; Spectrogram reading knowledge; Time-Delay Neural Networks; Expert system\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-146"
  },
  "hosaka91_eurospeech": {
   "authors": [
    [
     "Junko",
     "Hosaka"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Terumasa",
     "Ehara"
    ]
   ],
   "title": "Utilizing empirical data for postposition classification toward spoken Japanese speech recognition",
   "original": "e91_0573",
   "page_count": 4,
   "order": 148,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "This paper proposes a classification of postpositions for spoken Japanese speech recognition based on empirical data. The speaker's attitude toward his utterance is best expressed in the sentence final phrase in Japanese. Therefore, we first retrieve our empirical database to investigate the syntactic varieties in the sentence final phrase, especially focusing on the conjunctive postpositions. Second, we investigate how they behave in speech recognition by applying loosely constrained syntactic rules. Further, we refine the rules, based on the study. Finally, we establish the validity of their improvement by both speech recognition rate and acceptance rate. Keywords: Spoken Japanese, Postposition, Empirical data, Speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-147"
  },
  "phillips91_eurospeech": {
   "authors": [
    [
     "Michael",
     "Phillips"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Automatic learning of lexical representations for sub-word unit based speech recognition systems",
   "original": "e91_0577",
   "page_count": 4,
   "order": 149,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "In 1989, we first reported on the development of summit, a segment-based speaker-independent continuous-speech recognition system [12]. The initial version of summit made use of a small set of context-independent models for the lexical labels. In this paper, we describe our recent attempts to develop a framework that can produce an arbitrarily complex lexical representation. The procedure should permit us to achieve simultaneously the goals of determining a set of context-dependent labels and a lexical network representing alternate pronunciations of the words in our lexicon. Our experiments thus far have been conducted independently on two separate recognition tasks. In both cases, a significant reduction in recognition error rate has been realized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-148"
  },
  "lacouture91_eurospeech": {
   "authors": [
    [
     "Roxane",
     "Lacouture"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Lexical tree compression",
   "original": "e91_0581",
   "page_count": 4,
   "order": 150,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "One of the problems of Automatic Speech Recognition (ASR) systems dealing with large lexica (more than 10,000 words) is the time needed to extract candidate words from a given input signal. Ideally, one should execute a Viterbi-like algorithm for each word model and pick the best word(s) or, in case of continuous speech, the best sequence(s) of words. However, the practical time complexity of such an operation is too high for large lexica. To overcome this problem, we propose to compress the lexicon into a graph by merging common suffixes and prefixes. For French and English lexica respectively, this yields a 90% and 70% reduction in memory space. A reduction in time complexity was also achieved which allowed us to execute in real time an exact A* search algorithm on the whole graph for isolated words as well as for whole sentences with no need for any fastmatch or other heuristics. Keywords: Large lexicon access, Graph, Lexicon Compression, Lexicon Representation, A* search\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-149"
  },
  "riley91_eurospeech": {
   "authors": [
    [
     "Michael D.",
     "Riley"
    ],
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Lexical access with a statistically-derived phonetic network",
   "original": "e91_0585",
   "page_count": 4,
   "order": 151,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "A probabilistic approach to lexical access from a recognized phone lattice is presented. Lexical access is seen as finding the overall likelihood of a sequence of phones and durations for given words. Finding the word sequence that maximizes this likelihood combined with priors obtained from a language model comprises the overall recognition strategy. The likelihood computed in lexical access is a combination of the acoustic likelihoods obtained from a phone recognizer and lexical likelihoods, which represent phone realization and duration likelihoods for given word sequences. Classification trees are used to estimate the phone realiziation distributions and regression trees are used to estimate the phone duration distributions. We find they can capture effectively allophonic variation, alternative pronunciation, word coarticulation and segmental durations. We describe a simpified, but efficient implementation of these models to lexical access in the DARPA resource management recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-150"
  },
  "antoniol91_eurospeech": {
   "authors": [
    [
     "G.",
     "Antoniol"
    ],
    [
     "F.",
     "Brugnara"
    ],
    [
     "D.",
     "Giuliani"
    ]
   ],
   "title": "Admissible strategies for acoustic matching with a large vocabulary",
   "original": "e91_0589",
   "page_count": 4,
   "order": 152,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "In a large vocabulary isolated word automatic speech recognition system, using Hidden Markov Models(HMM), it is time consuming to perform a detailed likelihood computation of an input utterance for all the words of the dictionary. A tree based A* search algorithm is presented for finding the N best word hypotheses, using a new likelihood estimator to drive time asyncronously the search. The proposed likelihood estimator has been tested with two different tasks: a large vocabulary (5000 words) multi-speaker radiological reports recognizer, and a 94 word speaker-independent command language recognizer. In both cases, the speed performance was improved by a factor of about 6, without losing admissibility. Using discrete models, the mean word hypothesis evaluation time is 8 ms on Sun4/330 workstation, the recognition rates vary from 94% to 98% depending on the speaker in the 5000 word task with a bigram language model, and 91% in the speaker-independent task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-151"
  },
  "macarron91_eurospeech": {
   "authors": [
    [
     "Alejandro",
     "Macarron"
    ],
    [
     "Gregorio",
     "Escalada"
    ],
    [
     "Miguel Angel",
     "Rodriguez"
    ]
   ],
   "title": "Generation of duration rules for a Spanish text-to-speech synthesizer",
   "original": "e91_0617",
   "page_count": 4,
   "order": 153,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "In this paper we describe the implementation of the duration rules for a Spanish text-to-speech synthesizer called AMIGO. Durations of the phonemes are generated with a simple multiplicative model, in which a base duration value specific for each phoneme is modified by a series of multiplicative coefficients that depend on the context. In some cases, instead of the value predicted by this model, a minimum duration is applied. For the construction of the rules, the factors that are relevant in duration patterns (stress, left and right context, position within the phrase, etc) and their weights were determined first, through the study of an acoustic database containing more than 10,000 labelled spoken phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-152"
  },
  "mortamet91_eurospeech": {
   "authors": [
    [
     "L.",
     "Mortamet"
    ]
   ],
   "title": "Implementing duration expert rules into a text-to-speech synthesis system",
   "original": "e91_0621",
   "page_count": 4,
   "order": 154,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "We have used a natural speech database available at CNET to validate a theoretical duration model defined by a linguistic expert. This model has a hierarchical tree structure, each node corresponds to a particular syntactic-prosodic configuration to which an absolute duration value is assigned. This value is calculated by averaging the natural duration values for database entries, matching the considered configuration. The validation of this model had two goals: the first was to evaluate the validity and predictive power of the rules (i. e. to ensure that they define a minimal and complete rule set); the second was to validate the rules (i. e. to perceptually evaluate the quality of speech produced using them). A systematic perceptual test was done to compare natural phoneme durations and rule-calculated durations. The preliminary results seem globally correct but indicate the need for some local alterations, such as semi-vowel processing. These changes will allow us to refine the initial model using symbolic learning techniques. Keywords: speech synthesis, prosody, duration rules, natural speech database\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-153"
  },
  "kaiki91_eurospeech": {
   "authors": [
    [
     "Nobuyoshi",
     "Kaiki"
    ],
    [
     "Katsuhiko",
     "Mimura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Statistical modeling of segmental duration and power control for Japanese",
   "original": "e91_0625",
   "page_count": 4,
   "order": 155,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "Segmental duration and segmental power control factors were statistically analyzed for Japanese speech synthesis using a large sentence speech database. Through these analyses, prosodic characteristics of segmental duration control and segmental power control were compared. Large differences were found in factors such as the neighboring phoneme, the intensity of fundamental frequency and the range of utterance group final positions. It has also been confirmed that segmental duration and segmental power were accurately predicted by the linear model used in our statistical analysis. Keywords: Speech synthesis, Segmental duration, Power, Synthesis by rule, Prosody control\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-154"
  },
  "campbell91_eurospeech": {
   "authors": [
    [
     "W. Nick",
     "Campbell"
    ]
   ],
   "title": "Phrase-level factors affecting timing in speech",
   "original": "e91_0629",
   "page_count": 4,
   "order": 156,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "This paper reports findings related to local changes in speech rate at the level of the phrase. It examines the tendency for local resets in speaking rate to align with boundaries delimiting groups of related words, and presents evidence for an overall slowing down of speaking rate throughout these phrase-sized chunks of speech. The durations of all segments in a phonetically-balanced two-hundred sentence database were normalised to facilitate comparison of the lengthening or shortening undergone by each, regardless of any differences in actual duration that may be attributable to differences in manner or place of articulation. The normalised data was fitted by a regression line having a positive slope that was steepest for shorter sentences. When longer sentences were divided into their component phrases, the slope of the regression lines increased. This indicates that segment durations tend to increase as a function of time within a phrase and are reset at phrase boundaries within the larger prosodic unit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-155"
  },
  "karjalainen91_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Phoneme duration rules for speech synthesis by neural networks",
   "original": "e91_0633",
   "page_count": 4,
   "order": 157,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "It has been proposed and dernostrated recently that neural networks, instead of conventional discrete rules, can be applied to speech synthesis from text. This paper concentrates primarily on a subtask of text-to-speech speech synthesis, namely the computation of phoneme durations by taking into account the complex contextual information of a phoneme. Possible network-based formulations and data representations are discussed in general and some experimental results are shown to demonstrate the feasibility of the approach. The results are promising and duration computation in the Finnish language according to preliminary experiments performs well compared to rule sets used so far. The more general problem of computing other control parameters of speech synthesis by neural networks is also dicussed shortly. Keywords: Speech synthesis by rule, Neural networks, Prosodic features of speech\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-156"
  },
  "mcinnes91_eurospeech": {
   "authors": [
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "Context-sensitive phoneme lattice generation using interpolated demi-diphone and triphone models",
   "original": "e91_0639",
   "page_count": 4,
   "order": 158,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "Automatic speech recognition systems based on subword units (such as phonemes) can be enhanced by the use of context-specific modelling. This has been applied successfully in top-down recognition systems, in which strong lexical and syntactic constraints limit the number of context-specific units to be modelled. This paper describes a method for applying context-specific modelling in a modular system in which the acoustic-phonetic front end operates independently of vocabulary and syntax. Such a modular system has certain advantages as a research tool, particularly when combined with an entropy measure for evaluation of phoneme lattices. A technique for robust modelling of context-specific units, by interpolation of general and specific probability estimates, is also described. Comparative results are presented which show the improvements due to the context-specific modelling. Keywords: continuous speech recognition, hidden Markov models, triphone modelling, entropy estimation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-157"
  },
  "song91_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Song"
    ],
    [
     "T.",
     "Thomas"
    ],
    [
     "M.",
     "Patel"
    ]
   ],
   "title": "Experiments of 991-word speaker independent continuous speech recognition on DARPA RM task",
   "original": "e91_0643",
   "page_count": 4,
   "order": 159,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "This paper describes the implementation of a large vocabulary speaker independent continuous speech recognition system for the DARPA RM task. The continuous HMM approach is used with mel-cepstra plus time differenced mel-cepstra and a new subword HMM topology. A set of within-word context dependent phone units are generated from a simple context merging scheme by counting and trained by both Baum-Welch and Segmental K-means algorithms. Only 6 continuous mixture diagonal Gaussian PDFs are used in each model. The recognition algorithm is based on a frame synchronous Viterbi beam search, which produces a lattice of words during forward searching, and N-best sentence candidates can be extracted in the reverse tracking, subject to the different levels of knowledge constraints. On the 991-word DARPA RM task, we present the preliminary results on the Feb-89 test set and the original SPHINX test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-158"
  },
  "meloni91_eurospeech": {
   "authors": [
    [
     "Henri",
     "Meloni"
    ],
    [
     "F.",
     "Bechet"
    ],
    [
     "P.",
     "Gilles"
    ]
   ],
   "title": "Bottom-up acoustic-phonetic decoding for the selection of word cohorts from a large vocabulary",
   "original": "e91_0647",
   "page_count": 4,
   "order": 160,
   "p1": "647",
   "pn": "650",
   "abstract": [
    "Within the framework of the analytical recognition of the words of a large vocabulary, we propose a system permitting the selection of limited cohorts of lexical items from a lattice of valued phonetic units. Before the system can work, it has to learn all the phonetic units of a given speaker by storing the spectra of each phoneme. During the construction of the phonetic lattice, the units are simultaneously localized and identified by means of various types of spectral distances adjusted according to the phonemes, the context and certain characteristics of the speaker's voice. A few factors dependent on the phonemes as well as various types of spectral stability make it possible to determine the kind of sound and the areas in which the scores will be calculated. In the first stage the system interprets the information available in the lattice and then it accesses the lexis through a series of filters thus reducing the number of possible words. The solutions are given in the form of a cohort of words that are assigned a value according to the scores and the rate of temporal covering-up of the phonemes identified.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-159"
  },
  "peinado91_eurospeech": {
   "authors": [
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Ramon",
     "Roman"
    ],
    [
     "Jose C.",
     "Segura"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Pedro",
     "Garcia"
    ],
    [
     "Jesus E.",
     "Diaz"
    ]
   ],
   "title": "Entropic training for HMM speech recognition",
   "original": "e91_0651",
   "page_count": 4,
   "order": 161,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "The segmentation of the training data has become the most used initialization method for HMM training. The Viterbi reestimation has been widely applied for that purpose. We introduce a new segmentation method, based on the maximization of the Entropic Cohesion Measure (ECM) between segments and observations, which is equivalent to minimize the entropy model. This maximization is carried out by looking for the optimal boundaries between segments of the training utterances. Thus, we obtain an optimal segmentation (in the ECM sense) that achieves similar performance to the Viterbi reestimation with a considerable computational saving.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-160"
  },
  "kenny91_eurospeech": {
   "authors": [
    [
     "P.",
     "Kenny"
    ],
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "V. N.",
     "Gupta"
    ],
    [
     "Matthew",
     "Lennig"
    ],
    [
     "Paul",
     "Mermelstein"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Energy, duration and Markov models",
   "original": "e91_0655",
   "page_count": 4,
   "order": 162,
   "p1": "655",
   "pn": "658",
   "abstract": [
    "We present a new stochastic model for the energy and duration of phone segments ivhich takes account of the speech rate, the loudness of the signal and the effects of stress and pre-pausal lengthening and we show how the block Viterbi decoding algorithm can be used to integrate it with phone-based HMM speech recognizers. The model has been implemented on an isolated-word data-base and a preliminary experiment gives a modest improvement in word recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-161"
  },
  "nijtmans91_eurospeech": {
   "authors": [
    [
     "J. J.",
     "Nijtmans"
    ]
   ],
   "title": "A new recursive Markov model with a new state pruning approach for large vocabulary continuous speech recognition",
   "original": "e91_0659",
   "page_count": 4,
   "order": 163,
   "p1": "659",
   "pn": "663",
   "abstract": [
    "A Recursive form of the Markov Model (RMM) is defined in which every state itself can be modeled by a Markov Model. It is a hierarchic model that can be used to describe analytically the structure of human speech or all possible levels: syntactic- semantic, lexical phonetic, sub-phonetic. This model is a generalization of the Hidden Markov Model. A Recursive form of the Forward-Backward [3] algorithm is derived which can be used to do training with the new model. A new scaling approach is introduced to make state pruning possible, and to prevent numerical underflow problems without using logarithms (as usual in HMM). The same improvement is possible with the Viterbi algorithm [6] for recognition. To show the relation with a conventional Hidden Markov Model, an example RMM is given that is shown to be identical with an HMM. Also simulation results an given according to a practical useful example. keywords: Speech Recognition, Hidden Markov Model, Forward-Backward algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-162"
  },
  "mcinnes91b_eurospeech": {
   "authors": [
    [
     "Fergus R.",
     "McInnes"
    ]
   ],
   "title": "Context-sensitive phoneme lattice generation using interpolated demi-diphone and triphone models",
   "original": "e91_0663",
   "page_count": 4,
   "order": 164,
   "p1": "663",
   "pn": "666",
   "abstract": [
    "Automatic speech recognition systems based on subword units (such as phonemes) can be enhanced by the use of context-specific modelling. This has been applied successfully in top-down recognition systems, in which strong lexical and syntactic constraints limit the number of context-specific units to be modelled. This paper describes a method for applying context-specific modelling in a modular system in which the acoustic-phonetic front end operates independently of vocabulary and syntax. Such a modular system has certain advantages as a research tool, particularly when combined with an entropy measure for evaluation of phoneme lattices. A technique for robust modelling of context-specific units, by interpolation of general and specific probability estimates, is also described. Comparative results are presented which show the improvements due to the context-specific modelling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-163"
  },
  "nowell91_eurospeech": {
   "authors": [
    [
     "Peter",
     "Nowell"
    ],
    [
     "Henry S.",
     "Thompson"
    ]
   ],
   "title": "An efficient implementation of the n-best algorithm for lexical access",
   "original": "e91_0667",
   "page_count": 4,
   "order": 165,
   "p1": "667",
   "pn": "670",
   "abstract": [
    "This paper describes an efficient implementation of the N-Best algorithm for lexical access. An extension has been made to the original algorithm which reduces the search space by 80% whilst retaining over 99% of the top ten optimal paths produced by the standard N-Best algorithm. We have also found that pruning can reduce the search space further by a further 50% without a significant loss of optimal paths. Keywords: Lexical Access, Dynamic Programming, N-Best Algorithm, Path Re-generation, Beam-Width Pruning\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-164"
  },
  "falaschi91_eurospeech": {
   "authors": [
    [
     "Alessandro",
     "Falaschi"
    ],
    [
     "Massimo",
     "Pucci"
    ]
   ],
   "title": "Automatic derivation of HMM alternative pronunciation network topologies",
   "original": "e91_0671",
   "page_count": 4,
   "order": 166,
   "p1": "671",
   "pn": "674",
   "abstract": [
    "A new method for the automatic derivation of HMM topologies is presented. At first, the speech signal is segmented into acoustical units by using an Ergodic HMM. Then, a lattice structure is built from all the observed pronunciations. The lattice is thus pruned according to an information-theoretic criterion, aiming to preserve only the more characteristic event sequences. A circuit-free HMM topology is finally built after a proper state number assignment. The method naturally permits the sharing of phonetically-motivated observation densities within different HMM and states. Results for a speaker-independent recognition experiment are given. Keywords: Hidden Markov Models, Structure Inference, Information Measures, Alternative Pronunciations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-165"
  },
  "galiano91_eurospeech": {
   "authors": [
    [
     "Isabel",
     "Galiano"
    ],
    [
     "Francisco",
     "Casacuberta"
    ],
    [
     "Emilio",
     "Sanchis"
    ]
   ],
   "title": "On the structure of subword units for a speaker independent continuous speech task",
   "original": "e91_0675",
   "page_count": 4,
   "order": 167,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "The subword units in a continuous speech recognition system can be represented under different formalisms, but stochastic finite state networks are the most popular (i. e. hidden Markov models). However, the design of the structural component of these models requires a heuristic tailoring based on some a priori knowledge and/or by experimentation. Recently, two grammatical inference methods have been proposed to automatically obtain the structural component of the subword units from training speech data. These methods are based on the Error Correcting Grammatical Inference algorithm and the Morphic Generator Grammatical Inference methodology. In this paper we present speaker-independent experiments of acoustic-phonetic decoding in a continuous Spanish speech corpus. The models used are context-independent and context-dependent. They were obtained by using both grammatical inference algorithms. KEYWORDS: Acoustic-Phonetic Decoding, Grammatical Inference, Subword Modelling, Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-166"
  },
  "zhao91_eurospeech": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ],
    [
     "Hisashi",
     "Wakita"
    ],
    [
     "Xinhua",
     "Zhuang"
    ]
   ],
   "title": "Generate word transcription dictionary from sentence utterances and evaluate its effect on speaker-independent continuous speech recognition",
   "original": "e91_0679",
   "page_count": 4,
   "order": 168,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "In this paper a method of preparing a word transcription dictionary from utterances of training sentences is described. The word transcriptions are first extracted from the acoustic-phonetic labels of the training sentences, and then compressed using a cost measure of the joint word likelihood. The decoding performance from using this dictionary in a newly developed speaker-independent continuous speech recognition system is compared with the performance obtained by using a standard dictionary. The experimental results indicate that dictionaries prepared from sentence utterances can reflect the reduced pronunciation and contextual effect in continuous speech, and yield better decoding accuracy than word transcriptions in the forms of isolated utterances. On a task of an 853 word vocabulary and a test set grammar perplexity of 104, the system achieved a word accuracy of 85. 3% which represents an error reduction of 23% compared with the decoding results from using a standard dictionary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-167"
  },
  "varga91_eurospeech": {
   "authors": [
    [
     "A. P.",
     "Varga"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Simultaneous recognition of concurrent speech signals using hidden Markov model decomposition",
   "original": "e91_1175",
   "page_count": 4,
   "order": 169,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "This paper addresses the problem of automatic recognition of two simultaneous speech signals, that is the recognition of speech in the presence of speech from interfering talkers. The approach used is that of hidden Markov model based signal decomposition in which two or more concurrent signals are recognised simultaneously. In order to accommodate concurrent dynamically varying processes the decomposition technique uses conventional hidden Markov modelling of speech, but a generalisation of the conventional recognition algorithm. The performance of a decomposition based recogniser is compared with that of a conventional recogniser; the results demonstrate the ability of the new technique to decompose and recognise interfering signals as structurally complex as speech. Keywords: Speech recognition, hidden Markov model based decomposition, recognition of simultaneous speech signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-168"
  },
  "ballantyne91_eurospeech": {
   "authors": [
    [
     "I. A.",
     "Ballantyne"
    ],
    [
     "A. M.",
     "Sutherland"
    ],
    [
     "J. M.",
     "Hannah"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "A large vocabulary parallel processing continuous speech recognition system",
   "original": "e91_1179",
   "page_count": 4,
   "order": 170,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "Parallel processing is now an accepted method of providing the computational power and flexibility required for large vocabulary continuous speech recognition. The Centre for Speech Technology Research has developed a large vocabulary real time continuous speech recogniser which addresses the issues of the efficient parallelisation of the hidden Markov model (HMM) Viterbi speech recognition algorithm. This system, runs on a flexible distributed memory parallel processing platform capable of sustaining increased vocabularies and improved recognition algorithms. The purpose of this paper is to describe the efficient parallelisation of the recognition algorithm and the underlying architecture and associated developments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-169"
  },
  "rose91_eurospeech": {
   "authors": [
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Edward M.",
     "Hofstetter"
    ]
   ],
   "title": "Techniques for robust word spotting in continuous speech messages",
   "original": "e91_1183",
   "page_count": 4,
   "order": 171,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "The Lincoln hidden Markov model (HMM) based word spotting system has demonstrated good performance in spotting keywords in completely unconstrained continuous speech utterances [8]. The word spotter has been evaluated under a number of scenarios, and has been integrated into a system that performs the higher level task of classifying conversational speech messages according to topic [7]. In all of these scenarios, anywhere from 25 to 78 exemplars per keyword have been used to train the subword acoustic HMM's that are used in the word spotter. In most word spotting applications it is simply not possible to collect such a large number of spoken utterances for all the keywords in the vocabulary every time the system is to be reconfigured for a given task. Therefore, it is essential that techniques be developed to reduce the amount of \"task specific\" speech data required for training HMM based word spotters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-170"
  },
  "falaschi91b_eurospeech": {
   "authors": [
    [
     "Alessandro",
     "Falaschi"
    ],
    [
     "Alfredo",
     "Micozzi"
    ]
   ],
   "title": "Word spotting by CSR through vector quantized background models",
   "original": "e91_1187",
   "page_count": 4,
   "order": 172,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "A simple solution to the word spotting problem is experimented. Continuous Speech Recognition (CSR) techniques are adopted, with the background speech represented by a filler templates set defined through vector quantization algorithms. Quantitative results about the performed experiments are given. Keywords: Word Spotting, Continuous Speech Recognition, Vector Quantization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-171"
  },
  "junqua91_eurospeech": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Hisashi",
     "Wakita"
    ]
   ],
   "title": "Towards an artificial laboratory for the design and simulation of cooperative speech processing algorithms",
   "original": "e91_1191",
   "page_count": 4,
   "order": 173,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "To help the modelling and simulation of hybrid speech algorithms, and the fast prototyping of applications using speech input or output, the new concept of artificial laboratory for speech processing is proposed. Such a system provides a large range of computing facilities integrated in the same framework. After an overview of the system and the presentation of the different levels of reusable components, two applications recently developed with the system are outlined as examples. Both applications use speech input and benefit from the artificial laboratory. Because of the fast prototyping capabilities provided by the artificial laboratory more time can be spent on the testing and the use of the applications. Consequently, new insight on the use of speech input for building applications can be obtained. Keywords: Artificial laboratory, reusable components, integration, cooperative algorithms, prototyping\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-172"
  },
  "edwards91_eurospeech": {
   "authors": [
    [
     "K.",
     "Edwards"
    ],
    [
     "Fergus R.",
     "McInnes"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "Accent specific modifications for continuous speech recognition based on a sub-word lattice approach",
   "original": "e91_1195",
   "page_count": 4,
   "order": 174,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "Speech research has to date been dominated by either General American or RP British English accents. This limits the applicability of continuous speech recognition systems based on subword modelling to a small percentage of the English speaking population and makes true speaker independence more elusive. The modular speech recognition system discussed here allows experimentation at different stages in the recognition process, and is well suited to the implementation of different accents and languages. The 'front end' processing which uses Hidden Markov Models allows information about the phonetic and phonological details of an accent or language to be incorporated with the minimum of adjustment to the whole system. The 'back end' uses a lexicon which contains word transcriptions in terms of the units modelled by the HMMs. This paper describes the adaptation involved in enrolling speakers of a Scottish accent into the system, which was previously based on the assumption of RP accent users.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-173"
  },
  "lleida91_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Jose B.",
     "Marino"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Albert",
     "Oliveras"
    ]
   ],
   "title": "Two level continuous speech recognition using demisyllable-based HMM word spotting",
   "original": "e91_1199",
   "page_count": 4,
   "order": 175,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "This paper describes a two level Spanish Continuous Speech Recognition System based on Demisyllable HMM modelling, word-spotting and finite-state lexical and syntactic knowledge. The first level, the word level, is based on a spotting algorithm which takes as input the unknown utterance, the HMM of the reference demisyllable and the lexical knowledge in terms of a finite-state network. The output of the word level is a lattice of word hypothesis [1]. The second level, the phrase level, searches in a time-synchronous procedure the best sentence that end at each time instant. It takes as input the word lattice and the syntactic knowledge in terms of a finite-state network, giving as output the best legal sentence. The proposal two-level system was tested recognizing the integers from 0 to 1000 in a speaker independent approach. We get a word accuracy of 93,2% with a sentence accuracy of 84. 5%. Keywords: Speech Recognition, Hidden Markov Model, Fuzzy Training, Demisyllable, Word-spotting, Multiple Hypothesis, Finite State Networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-174"
  },
  "applebaum91_eurospeech": {
   "authors": [
    [
     "Ted H.",
     "Applebaum"
    ],
    [
     "Brian A.",
     "Hanson"
    ]
   ],
   "title": "Tradeoffs in the design of regression features for word recognition",
   "original": "e91_1203",
   "page_count": 4,
   "order": 176,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "This paper discusses issues affecting the design of regression features for use in word recognition. Particular attention is paid to interactions with environmental noise effects, including additive noise and the Lombard reflex. We discuss separate control of the time length and number of frames used to calculate the regression features, selection of regression window parameter values, and the selection of regression feature order (up to third order). Recognition results are shown on a vocabulary of confusable English alpha-digits and function words, and on the JEIDA Japanese Cityname vocabulary. KEYWORDS: speech recognition, regression feature, delta cepstrum, Lombard, noise\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-175"
  },
  "bahl91_eurospeech": {
   "authors": [
    [
     "Lalit R.",
     "Bahl"
    ],
    [
     "Peter F.",
     "Brown"
    ],
    [
     "Peter V. de",
     "Souza"
    ],
    [
     "Robert L.",
     "Mercer"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "A fast algorithm for deleted interpolation",
   "original": "e91_1209",
   "page_count": 4,
   "order": 177,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "Deleted interpolation is a widely used technique for smoothing probability distributions associated with Markov models in speech recognition. In recent years it has been used by the IBM Tangora system, the CMU Sphinx system, BBN Byblos system, etc. Jelinek and Mercer showed that the forward-back ward algorithm can be used to obtain linear weighting coefficients for smoothing probability distributions. In this paper we extend some of their results. First we prove a general convexity theorem about linear combinations oF probability distributions. Then we show how this theorem can be used to derive a fast algorithm for deleted interpolation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-176"
  },
  "franzini91_eurospeech": {
   "authors": [
    [
     "Michael A.",
     "Franzini"
    ],
    [
     "Alex H.",
     "Waibel"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Recent work in continuous speech recognition using the connectionist viterbi training procedure",
   "original": "e91_1213",
   "page_count": 4,
   "order": 178,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "Hybrid methods which combine hidden Markov models (HMMs) and connectionist techniques take advantage of what are believed to be the strong points of each of the two approaches: the powerful discrimination-based learning of connectionist networks and the time-alignment capability of HMMs. Connectionist Viterbi Training (CVT) is a simple variation of Viterbi training which uses a back-propagation network to represent the output distributions associated with the transitions in the HMM. The work reported here represents the culmination of three years of investigation of various means by which HMMs and neural networks (NNs) can be combined for continuous speech recognition. This paper describes the CVT procedure, discusses the factors most important to its design and reports its recognition performance. Several changes made to the system over the past year are reported here, including: (1) the change from recurrent to non-recurrent NNs, (2) the change from SPHlNX-style phone-based HMMs to word-based HMMS, (3) the addition of a corrective training procedure, and (3) the addition of an alternate model for every word. The CVT system, incorporating these changes, achieves 99. 1% word accuracy and 98. 0% string accuracy on the TI/NBS Connected Digits task (\"TI Digits\"). Keywords: hybrid systems, neural networks, back propagation, TI Digits, viterbi training, Connectionist Viterbi Training, CVT\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-177"
  },
  "steinbiss91_eurospeech": {
   "authors": [
    [
     "Volker",
     "Steinbiss"
    ]
   ],
   "title": "A search organization for large-vocabulary recognition based on n-best decoding",
   "original": "e91_1217",
   "page_count": 4,
   "order": 179,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "This paper presents a time-synchronous search concept in which (a) the language model contexts are represented in local hypotheses lists, as in N-best decoding, and where (b) these hypotheses lists are rescored using a language model in a time-delayed manner. The concept is shown to work well on a real 12000-word continuous-speech recognition task with a phonetic-tree based search. A faster though approximative search algorithm is presented which compares favorably well with the exact ones. - The discussion starts with some new aspects on word-dependent N-best decoding. Keywords: continuous speech recognition, search, N-best decoding, top N.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-178"
  },
  "gong91b_eurospeech": {
   "authors": [
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "VINICS: a continuous speech recognizer based on a new robust formulation",
   "original": "e91_1221",
   "page_count": 4,
   "order": 180,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "Given a time function of plausibilities of observing each phoneme symbol, continuous speech recognition can be formulated as finding the sentence which maximizes the sum of plausibilities of individual symbols of which the sentence is made up. In making use of peaks of plausibility time functions, the maximization can be solved by embedded search processes, via dynamic programmings: finding the best path connecting peaks in the- plausibility functions of two successive symbols, and finding the best time transition slot index for two given peaks. Not searching slot by slot, the resulting algorithm is highly efficient. Based on this principle, the Vinics continuous speech recognition system (1200 words, 1500 grammar rules) has achieved a 95% word recognition rate on speaker-dependent test, using 17 machine labeled training utterances for each speaker and a non-linear vectorial interpolation technique for phoneme plausibility estimation. The search algorithms, system configuration and experimental results are described. Keywords: continuous speech recognition, phoneme plausibility, search algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-179"
  },
  "sagayama91_eurospeech": {
   "authors": [
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "A matrix representation of HMM-based speech recognition algorithms",
   "original": "e91_1225",
   "page_count": 4,
   "order": 181,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "This paper describes a matrix representation from which we can derive a new formulation of HMM-based speech recognition algorithms. This idea provides not only an alternative mathematical formulation equivalent to conventional trellis and Viterbi algorithms but also better understanding of HMM algorithms under grammatical constraints as well as more efficient computational possibilities. In this formulation, a likelihood matrix is defined by an (N + 1) x (N + 1) dimensional upper triangular matrix whose (t,s) component is the observation likelihood of the given signal in a time span between t + 1 and s. First, it is shown that the likelihood matrix for a pair of serially connected signal sources is the product of matrices (P = P1P2) and the parallel connection is represented by the sum (P = Pi + P2) From these basic properties, matrix-based HMM computation al- gorithms are derived. Explicit duration control at all levels, such as state, phoneme, syllable, and word, can be easily done. Grammatical rewriting rules are directly interpreted as matrix operations. A matrix parser is suggested for generalization of a CYK parser. This algorithm is particularly effective in large vocabulary systems where same phone units (phonemes) appear in many syntactic paths.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-180"
  },
  "dalsgaard91_eurospeech": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ],
    [
     "William",
     "Barry"
    ]
   ],
   "title": "Multi-lingual acoustic-phonetic features for a number of european languages",
   "original": "e91_0685",
   "page_count": 4,
   "order": 182,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "To examine the robustness of Acoustic-Feature definitions, broad-phonetic label alignment was carried out on Danish, English and Italian continuous speech recordings after training a Neural-Net-based system on speech material taken from more than one language. Shared feature specifications for phonemes adjudged to be sufficiently similar across languages increased the training base for those sounds. Other phonemes with no equivalents across the languages were trained only on language specific material. In some cases labelling accuracy was better than earlier experiments in which only language specific training was carried out. However, results indicate that the amount of training material alone does not increase accuracy. Contextual variation in the segment transitions makes some sounds inherently more difficult to label automatically.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-181"
  },
  "kabre91_eurospeech": {
   "authors": [
    [
     "H.",
     "Kabre"
    ],
    [
     "Guy",
     "Pérénnou"
    ],
    [
     "Nadine",
     "Vigouroux"
    ]
   ],
   "title": "A non-linear filtering method applied to automatic segmentation of multilingual speech corpora",
   "original": "e91_0689",
   "page_count": 4,
   "order": 183,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "In this paper, we give the general principles behind an automatic system, developed at IRIT and capable of labelling speech signal for phonetic events. First, we present a non-linear smoothing method that was run on both amplitude and ZCR. Next, we give results obtained on EUROM-0 English, French and Swedish corpora. The results demonstrate that the labelling operation becomes completely independent from either language, corpus or speaker. Moreover, this operation requires no manual adaptation or training whatsoever. Keywords: Automatic Segmentation, Multilingual Speech Analysis, Phonetic Events, Non-Linear Filtering\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-182"
  },
  "cosi91_eurospeech": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "A preliminary statistical evaluation of manual and automatic segmentation discrepancies",
   "original": "e91_0693",
   "page_count": 4,
   "order": 184,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "The accuracy of automatic alignment systems will always be checked using references manually segmented by phonetic or speech communication experts. Consequently, the statistical characterisation of manual segmentation discrepancies becomes quite an important issue. To achieve the goal of finding a better statistical description of these discrepancies, two different experiments were carried out. The first pilot experiment, which was focused on the segmentation of a small subset of an Italian isolated-word speech data-base, was carried out only to better define the second experiment regarding the segmentation of few continuous Italian sentences. This work has been developed in part under the ESPRIT project \"Speech Assessment Methodology\" (SAM).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-183"
  },
  "mcqueen91_eurospeech": {
   "authors": [
    [
     "J. M.",
     "McQueen"
    ],
    [
     "E. J.",
     "Briscoe"
    ]
   ],
   "title": "A computational tool for examining lexical segmentation in continuous speech",
   "original": "e91_0697",
   "page_count": 4,
   "order": 185,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "A major problem of lexical access is that of lexical segmentation: how is fluent, continuous speech, which has few reliable cues to word boundaries, mapped onto a lexicon consisting of discrete entries? A plausible segmentation process must be efficient, generating small sets of word candidates, and reliable over different types of input: citation-form speech; speech in noise; and natural fluent speech, where phonological processes have operated. A computational tool for assessing the performance of various segmentation strategies is described. Input to a machine-readable dictionary is based on a featural representation, allowing flexibility in the degree of specification of the input string. The segmentation strategies can therefore be tested on input which represents the variability inherent in continuous natural speech. The tool can also be used to generate psycholinguistic predictions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-184"
  },
  "schmidt91_eurospeech": {
   "authors": [
    [
     "M. S.",
     "Schmidt"
    ],
    [
     "G. S.",
     "Watson"
    ]
   ],
   "title": "The evaluation and optimization of automatic speech segmentation",
   "original": "e91_0701",
   "page_count": 4,
   "order": 186,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "This paper demonstrates a method of evaluating the output from automatic speech segmentation systems. The methodology is exemplified by experiments performed on the output of a Hidden Markov model based automatic segmentation system developed at the Centre for Speech Technology Research. Subject to comparison were the differences in allocated phoneme boundaries between hand-segmented and automatically segmented speech of the same utterances. The results of this large-scale evaluation on almost 6000 phonemes facilitate a qualitative analysis of each HMM and allow the identification of bad automatic segmentations with respect to particular contexts. This type of analysis provides the platform for an iterative optimization and fine tuning of the models, in that HMM-training for subsequent automatic segmentations can be tailored to contain more examples of a segment (perhaps in a specific context), for which the performance of the system was unsatisfactory. Furthermore, the use of standardized methods of evaluation allows for objective cross-comparison between different automatic segmentation systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-185"
  },
  "feng91_eurospeech": {
   "authors": [
    [
     "G.",
     "Feng"
    ],
    [
     "N.",
     "Achab"
    ],
    [
     "R.",
     "Combescure"
    ]
   ],
   "title": "On-line speech segmentation using adaptive models: application to variable rate speech coding",
   "original": "e91_0705",
   "page_count": 4,
   "order": 187,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper presents a new solution to the asymmetry problem of the test statistic used in speech segmentation. It is shown that the asymmetric behavior can be greatly influenced by the model identification procedures. The commonly used growing memory Burg procedure and the autocorrelation method can intensify the asymmetric behavior. Our proposed method consists of using adaptive models to accomodate to the nonstationary speech environment. These models can effectively reduce the asymmetric behavior of the test, and allow a simple on-line segmentation algorithm to be constructed. This algorithm is quite suitable for variable rate speech coding systems, and one is now being developed. This method can also be adapted to other applications, such as speech labeling and speech recognition. KEY-WORDS: speech segmentation, test statistic, asymmetric behavior, adaptive model, variable rate speech coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-186"
  },
  "taylor91b_eurospeech": {
   "authors": [
    [
     "P. A.",
     "Taylor"
    ],
    [
     "Stephen D.",
     "Isard"
    ]
   ],
   "title": "Automatic diphone segmentation",
   "original": "e91_0709",
   "page_count": 3,
   "order": 188,
   "p1": "709",
   "pn": "711",
   "abstract": [
    "A method is described for automatically segmenting a database of diphones for speech synthesis purposes. This involves using hidden Markov models to find phoneme boundaries and then a spectral mismatch minimisation algorithm to choose the precise diphone boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-187"
  },
  "ottesen91_eurospeech": {
   "authors": [
    [
     "Georg E.",
     "Ottesen"
    ]
   ],
   "title": "An automatic diphone segmentation system",
   "original": "e91_0713",
   "page_count": 4,
   "order": 189,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "This paper discusses the requirements for an automatic diphone recording and segmentation system, and presents a PC-based system. The level and speech rate are controlled for each test word at recording time. A set of Norwegian test words is segmented by two different methods: 1) A speaker indepedant Hidden Markov Model (HMM), and 2) A Dynamic Time Warping (DTW) procedure adapted to one speaker. Norwegian diphones are then extracted. The best performance is obtained with the DTW procedure, giving a satisfactory segmentation for about 99 percent of the diphones. Keywords: - Automatic segmentation - Diphone synthesis - PSOLA synthesis - Dynamic time warping - Hidden Markov Model\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-188"
  },
  "brierton91_eurospeech": {
   "authors": [
    [
     "R. A.",
     "Brierton"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ]
   ],
   "title": "An evaluation oof spectral transitivity functions for speech segmentation in variable frame-rate speech vocoding",
   "original": "e91_0717",
   "page_count": 4,
   "order": 190,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "This paper is concerned speech segmentation for a variable frame-rate (VFR) vocoder. The segmentation is done on the basis of transitivity functions which measure the rate of change of spectral or excitation parameters derived from speech. It has been found that significant data reductions can be achieved without loss of synthetic speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-189"
  },
  "compernolle91_eurospeech": {
   "authors": [
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "J.",
     "Smolders"
    ],
    [
     "P.",
     "Jaspers"
    ],
    [
     "T.",
     "Hellemans"
    ]
   ],
   "title": "Speaker clustering for dialectic robustness in speaker independent recognition",
   "original": "e91_0723",
   "page_count": 4,
   "order": 191,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "In this paper methods for creating multiple baseforms, in an HMM speaker independent speech recognition system are compared and analyzed. The multiple baseforms are used to better model different speakers characteristics, such as sex or regional accent. The required speaker classes are obtained either from known categorical differences (sex, address) or in an adaptive clustering procedure. Both methods are compared in a Dutch/Flemish digit recognizer. Telephone recordings from 600 Dutch and 600 Flemish speakers were used. A 2 baseform system based on regional subdivision leads to a 3% improvement in recognition performance, and yields results comparable to within class performances using a single model. Furthermore division on the basis of accent is significantly more advantageous than a division based on sex. Iterative clustering procedures do in general not work well as the different models tend to overlap more with every iteration step. Ultimately it was found that subdivision of speakers in classes only helps if the number of speakers per class remains truly large (typically > 200).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-190"
  },
  "yashchin91_eurospeech": {
   "authors": [
    [
     "Dina",
     "Yashchin"
    ],
    [
     "William C. G.",
     "Ortel"
    ]
   ],
   "title": "Experience with speech recognition in automating telephone operator functions",
   "original": "e91_0727",
   "page_count": 4,
   "order": 192,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "A NYNEX network service, VOIS, automates the processing of intercept calls. VOIS employs speech recognition when customers are asked to say the dialed number. Some responses have been recorded in an application-specific database, VOIS-2, which contains information on recognizer and system performance and customer behavior. VOIS-2 has 1636 utterances of which 54% are continuously spoken digit strings with no extraneous speech, 14% have extraneous speech, and 32% have no speech at all. For the 881 in-vocabulary utterances, the string accuracy is 49% and the single-digit accuracy 85%. The VOIS application accepts only seven-digit strings, with a recognition confidence level above a predetermined threshold, and with a valid initial three-digit end-office code. For the 282 utterances that meet these stringent criteria, the string accuracy is 91. 5% and the single-digit accuracy 98. 5%. Keywords: Automatic Speech Recognition, Digit Recognition, NYNEX Science and Technology, Operator Number Identification, Operator Services, VOIS\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-191"
  },
  "canavesio91_eurospeech": {
   "authors": [
    [
     "F.",
     "Canavesio"
    ],
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "M.",
     "Oreglia"
    ],
    [
     "P.",
     "Ruscitti"
    ]
   ],
   "title": "HMM modeling in the public telephone network environment: experiments and results",
   "original": "e91_0731",
   "page_count": 4,
   "order": 193,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "A voice-activated inquire system to be used over the telephone network imposes severe constraints upon the speech recognition component: it must be able to accept input speech from untrained speakers, it must be robust against spurious words and extra-linguistic phenomena (environmental noises and telephone line noises) and it must deal with reduced bandwidth. This paper approaches these aspects in the HMM framework.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-192"
  },
  "morin91_eurospeech": {
   "authors": [
    [
     "Dominique",
     "Morin"
    ]
   ],
   "title": "Influence of field data in HMM training for a vocal server",
   "original": "e91_0735",
   "page_count": 4,
   "order": 194,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "Our task is to improve speech recognition in large scale, general public applications over the telephone network, known as Voice Response Systems (vrs). It has been observed that the performance of speech recognition systems decreases as they are put in service, compared to the rates obtained with laboratory data. This paper studies the contribution of field data (i. e. data extracted from the VRS in operation) in the training of a speaker-independent isolated-word speech recognition system. The experiments are conducted with a database of about 20,000 tokens, half field and half laboratory data, a vocabulary of 21 words, and HMM modelling with 3 types of modelling units (fixed or variable length word, and allophones). The results show that models trained on a mixture of field and laboratory data perform 30% better than models trained exclusively on laboratory data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-193"
  },
  "ciaramella91_eurospeech": {
   "authors": [
    [
     "A.",
     "Ciaramella"
    ],
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "A.",
     "Pacchiotti"
    ],
    [
     "R.",
     "Pacifici"
    ]
   ],
   "title": "An isolated word speech recognizer prototype for mobile-radio applications",
   "original": "e91_0739",
   "page_count": 4,
   "order": 195,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "This paper describes a real-time recognizer, suitable for the car environment, which is able to recognize isolated words for a voice dialing application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-194"
  },
  "monaghan91_eurospeech": {
   "authors": [
    [
     "James",
     "Monaghan"
    ],
    [
     "Christine",
     "Cheepen"
    ]
   ],
   "title": "Linguistic modelling for a speech interface in the office context",
   "original": "e91_0745",
   "page_count": 4,
   "order": 196,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "This paper describes some of the linguistic research underlying the development of a speech-driven interface, and outlines the basic steps in data capture and analysis which are required to produce the model of linguistic activity in the office context needed to ensure full functionality. Particular attention is paid to the results of some of our linguistic modelling from dictation data. Keywords: speech recognition and understanding, natural language interfaces, office automation, spoken dialogue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-195"
  },
  "carlo91_eurospeech": {
   "authors": [
    [
     "Andrea Di",
     "Carlo"
    ],
    [
     "Rino",
     "Falcone"
    ]
   ],
   "title": "Ill-formedness problem in the spoken language processing",
   "original": "e91_0749",
   "page_count": 4,
   "order": 197,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "A very important issue for the Natural Language Processing (NLP) in the Speech Understanding perspective is the processing of input utterances that deviate from their correct expectations. It is possible to find the Ill-Formedness problem at every level of the Speech Processing: in the phonetic transcription of the signal, in the lexical interpretation, in the syntactical and semantical parsing. The main way to represent the Ill-Formedness is creating sets of hypotheses which give temporal and grammatical interpretations of the input: segment lattice, word lattice, syntactical chart. At the properly syntactical level, three main kinds of action to process the uncertainty and the incompleteness are possible: hypothesis scoring, island driven chart parsing, top-down prediction. In this paper, we present simulation test results in terms of word accuracy - 85% of word accuracy if the simulated phonemic transcription contains 10% of substitutions, deletions and insertions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-196"
  },
  "maltese91_eurospeech": {
   "authors": [
    [
     "Giulio",
     "Maltese"
    ],
    [
     "Federico",
     "Mancini"
    ]
   ],
   "title": "A technique to automatically assign parts-of-speech to words taking into account word-ending information through a probabilistic model",
   "original": "e91_0753",
   "page_count": 4,
   "order": 198,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "A system to automatically tag arbitrary text with the part-of-speech of each word is described. The system is based on a probabilistic model where we assume that words in a given sequence are the output symbols of a Hidden Markov Model, the states of which are represented by pairs of parts-of-speech. Using a 17 tag set the rate of correctly tagged words ranged from 96. 2% to 97. 2% on various texts. The system proved to be quite effective even using a small set of initial statistics. As to words never occurred in training data, we employed a statistical technique based on word-endings frequencies. This technique resulted in a 22% decrease in tagging error rate using a 260,000-word reference vocabulary and in a 49% decrease making use of a 20,000-word vocabulary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-197"
  },
  "pelillo91_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Pelillo"
    ],
    [
     "Mario",
     "Refice"
    ]
   ],
   "title": "Syntactic category disambiguation through relaxation processes",
   "original": "e91_0757",
   "page_count": 4,
   "order": 199,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "Tagging words according to their syntactic categories represents an important problem in natural language processing with several applications to speech synthesis, speech recognition, character recognition, automatic translation, etc. Due to the presence of homographs any word-labeling algorithm must incorporate some mechanism for coping with ambiguity. A number of methods have been proposed for the disambiguation task, including connectionist models, dynamic programming techniques, Markov models. In this paper we introduce a novel approach based on probabilistic relaxation labeling, an iterative method quite popular in the area of image processing and pattern recognition. Experiments are described and preliminary results are given for the Italian language. Keywords: natural language processing, probabilistic relaxation, syntactic category disambiguation, speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-198"
  },
  "wrigley91_eurospeech": {
   "authors": [
    [
     "E. N.",
     "Wrigley"
    ],
    [
     "J. H.",
     "Wright"
    ]
   ],
   "title": "Computational requirements of probabilistic LR parsing for speech recognition using a natural language grammar",
   "original": "e91_0761",
   "page_count": 4,
   "order": 200,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "Language modelling is an area of speech recognition research which has received considerable attention in recent years. Probabilistic context-free grammars provide a relatively powerful model, but the computational requirements of the application are stringent. This paper examines the computational requirements of a probabilistic generalised LR parser, both in terms of the size of the tables required to drive the parsing algorithm and the run time data structures involved. An efficient general method for generating the n best parses of uncertain input is also described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-199"
  },
  "tiboni91_eurospeech": {
   "authors": [
    [
     "J.",
     "Tiboni"
    ],
    [
     "G.",
     "Perennon"
    ]
   ],
   "title": "Phonotypical transcription through the GEPH expert system",
   "original": "e91_0767",
   "page_count": 4,
   "order": 201,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "We describe an automatic system for generating phonotypical prosodic items from orthographic texts and their syntactic-prosodic underlying structures. The method we use involves a morphophonological lexicon and a phonological component that relies on the notion of multifold pronunciation group. This allows us to account for pronunciation variants by means of one single phonotypical transcription of a speech item. Corpora of such items can be used as input of an automatic alignment system for the purpose of automatic labelling of recorded speech corpora. The method can also be applied for the purpose of phonological knowledge representation of automatic speech recognition systems. Keywords: Speech Recognition, Automatic alignment, Prosody, Phonology, Phonetic\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-200"
  },
  "williams91_eurospeech": {
   "authors": [
    [
     "Briony",
     "Williams"
    ],
    [
     "Franziska",
     "Maier"
    ]
   ],
   "title": "A spelling corrector for use in text-to-speech synthesis for English",
   "original": "e91_0771",
   "page_count": 4,
   "order": 202,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "Current spelling correction methods for phonetically-based errors make use of a whole-word dictionary. An algorithm has been implemented which uses a fully morphological lexicon, and so can handle many newly-coined words. The lexical entries of phonetically misspelled words are located, and the correct orthographic form is thus obtained. Keywords: Lexicon, interpreting written language, text-to-speech systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-201"
  },
  "russi91_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Russi"
    ]
   ],
   "title": "Robust and efficient parsing for applications such as text-to-speech conversion",
   "original": "e91_0775",
   "page_count": 4,
   "order": 203,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "In automatic synthesis of speech from text, both morphological and syntactic knowledge is necessary for correct grapheme-to-phoneme conversion, accent assignment and prosodic phrasing. In this paper, a language independent framework for syntactic and morphological analysis is presented. It relies on an extended chart parsing algorithm and handles unrestricted text in an efficient and robust manner. This approach has been implemented in the software package Syma and has been used successfully as a text analysis module in a text-to-speech system for German. It is currently being integrated into an English text-to-speech system developed at Edinburgh University.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-202"
  },
  "luk91_eurospeech": {
   "authors": [
    [
     "Robert W. P.",
     "Luk"
    ],
    [
     "Robert I.",
     "Damper"
    ]
   ],
   "title": "Stochastic transduction for English text-to-phoneme conversion",
   "original": "e91_0779",
   "page_count": 4,
   "order": 204,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "This paper describes the theory of stochastic transduction and our current implementation for English text-to-phoneme conversion. A transduction grammar is defined which generates orthographic-phonemic word pairs; here the sentential derivation is modelled as a Markov process. We envisage that our grammar, which is regular, will ultimately operate at three levels: morphemic, syllabic and phonographic. Thus far, we have only implemented the phonographic grammar. The most likely translation is obtained by an extended form of the Viterbi algorithm with probabilities inferred by the Viterbi update procedure. Testing with 4676 words yields a word translation accuracy of around 82% but generalisation to unseen words remains to be tested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-203"
  },
  "hwang91_eurospeech": {
   "authors": [
    [
     "M. Y.",
     "Hwang"
    ],
    [
     "X. D.",
     "Huang"
    ]
   ],
   "title": "Acoustic distribution clustering in phonetic hidden Markov models",
   "original": "e91_0785",
   "page_count": 4,
   "order": 205,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "Output distributions in hidden Markov models describe essential acoustic characteristics. Triphone generalization may force two models to be merged together when only parts of the model output distributions are similar, while the rest of the output distributions are different. This problem can be avoided if clustering is carried out at the distribution level. In this paper, a shared-distribution model is proposed to replace generalized triphone models for speaker-independent continuous speech recognition. Here, output distributions in the hidden Markov model are shared with each other if they exhibit acoustic similarity. In addition to detailed representation, it also gives us the freedom to use a large number of states for each phonetic model. Although an increase in the number of states will increase the total number of free parameters, with distribution sharing we can essentially eliminate those redundant states and have the luxury to maintain necessary ones. By using the shared-distribution model, the error rate on the DARPA Resource Management task has been reduced by 20% in comparison with the baseline SPHINX system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-204"
  },
  "blomberg91_eurospeech": {
   "authors": [
    [
     "M.",
     "Blomberg"
    ]
   ],
   "title": "Modelling articulatory inter-timing variation in a speech recognition system based on synthetic references",
   "original": "e91_0789",
   "page_count": 4,
   "order": 206,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "Variation in the synchrony between two or more simultaneous articulatory gestures in speech may cause large variability in the acoustic signal and lower the accuracy and robustness of recognition systems. In this report, a technique is described that accounts for this effect by predicting alternative ways of pronunciation of an utterance. A formant based speech production system is used for generating the reference templates to be used for recognition. The delay between voicing transition and formant movements has been systematically varied, by the production system, forming different paths through a transition network at phoneme boundaries. In a pilot experiment, the recogniser behaviour was examined for utterances having different time position of the devoicing of phrase-final vowels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-205"
  },
  "torkkola91_eurospeech": {
   "authors": [
    [
     "Kari",
     "Torkkola"
    ],
    [
     "Mikko",
     "Kokkonen"
    ],
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Pekka",
     "Utela"
    ]
   ],
   "title": "Improving short-time speech frame recognition results by using context",
   "original": "e91_0793",
   "page_count": 4,
   "order": 207,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "This paper focuses on comparing three approaches to improve the accuracy of classifying short-time speech frames into phoneme classes by taking into account the classifications of nearby frames, also individually classified. We investigate whether this improvement has an effect to the accuracy of transcribing speech into phoneme sequences using two different decoding schemes, one based on simple durational rules, and the other on hidden Markov models (HMMs). The experiments indicate that recognition accuracies can indeed be improved significantly by taking the local context into account.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-206"
  },
  "rentzepopoulos91_eurospeech": {
   "authors": [
    [
     "P. A.",
     "Rentzepopoulos"
    ],
    [
     "George K.",
     "Kokkinakis"
    ]
   ],
   "title": "Phoneme to grapheme conversion using HMM",
   "original": "e91_0797",
   "page_count": 4,
   "order": 208,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "Grapheme to phoneme conversion has been successfully implemented in the majority of the european languages mostly by rules. Conversion by rules however has given poor results when used in the reverse process, i. e. phoneme to grapheme conversion (PTGC). In this paper a statistical method is presented, which serves both as a PTGC technique and as a linguistic model. This method eliminates the need for search in a dictionary, which always limits the vocabulary of the system and the need of using a linguistic model in order to select the right word among possible candidates. The natural language is modelled as a Hidden Markov Model (HMM) and the conversion is performed using the Viterbi algorithm. The system can be trained using a medium size text corpus (e. g. 30-40 kwords) and there is almost no need for a linguist expert for the training process. Experimental results are promising since in the word level the system produced an average score of 70% correctly transcribed words for unknown and 78% for known test corpus, while in the phoneme level the score was 94. 6% and 95. 5% respectively. Keywords: Large Vocabulary Speech Recognition, Phoneme to Grapheme Conversion, HMM, Viterbi Algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-207"
  },
  "parfitt91_eurospeech": {
   "authors": [
    [
     "S. H.",
     "Parfitt"
    ],
    [
     "R. A.",
     "Sharman"
    ]
   ],
   "title": "A bi-directional model of English pronunciation",
   "original": "e91_0801",
   "page_count": 4,
   "order": 209,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "The two tasks of finding the pronunciation of a word from its spelling, and the spelling from its pronunciation, are basic problems in speech synthesis and recognition respectively. A related problem is how best to align the phonemic and orthographic representations of a given word to show the correspondence between each of the letters in the word and the sounds to which they belong. The finding of one form of a word when observing only the other form, is likened to decoding an encrypted message to find a hidden meaning. If a Hidden Markov Model (HMM) is assumed to generate the observed form of the word from its hidden form, then a method exists to solve the alignment problem, provided that the parameters of the model are known. Since they are in general not accurately known, a training algorithm, such as the Forward-Backward (maximum-likelihood) method can be used to determine a good estimate for them. A simple HMM for solving the two decoding tasks is suggested, and the results of training it on real data are discussed. The use of a single methodology to solve two different but related tasks is offered as an example for other language tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-208"
  },
  "goodine91_eurospeech": {
   "authors": [
    [
     "David",
     "Goodine"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "Michael",
     "Phillips"
    ]
   ],
   "title": "Full integration of speech and language understanding in the MIT spoken language system",
   "original": "e91_0845",
   "page_count": 4,
   "order": 210,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "This paper describes research on the integration of the MIT SUMMIT speech recognition system [4] with the TINA language understanding system [3]. Our goal is the creation of a spoken language system whose input consists of spontaneous speaker-independent spoken queries and whose output consists of cooperative responses to those queries [5]. We describe a series of experiments to test the hypothesis that a combination of linguistic and acoustic information can improve system performance over the use of acoustic information alone. We use several configurations, moving from a loosely coupled interface between recognizer and language understanding system to a tightly coupled system where the language understanding component predicts next possible words for the recognizer. We achieved improvement in two areas. First, for the set of sentences that had an answer for a perfect transcription, we improved the percent of sentences correctly understood from 23. 4% using no linguistic information to 67. 6% in the tightly coupled system where sentence hypotheses are sorted based on a linear combination of acoustic and linguistic score. Second, we improved overall system score (defined as percent correct minus percent incorrect) from 12. 5% with no linguistic information to 29. 4% in the tightly coupled system. This was done by incorporating rejection criteria based on linguistic score and measures of work.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-209"
  },
  "yamaoka91_eurospeech": {
   "authors": [
    [
     "Takayuki",
     "Yamaoka"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "Dialogue interpretation model and its application to next utterance prediction for spoken language processing",
   "original": "e91_0849",
   "page_count": 4,
   "order": 211,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "We propose a dialogue interpreting model for telephone inquiry dialogues and an application method of predicting the next utterance using the model for a speech-language interface. This method can provide context-sensitive information about the appropriate next utterance on an abstract level. Using this type of information and linguistic and pragmatic knowledge to connect the information to the corresponding surface linguistic expressions, selecting the correct candidate from speech recognition output in a context-sensitive way can be realized. Keywords: spoken dialogue, speech recognition and understanding\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-210"
  },
  "boogers91_eurospeech": {
   "authors": [
    [
     "W.",
     "Boogers"
    ]
   ],
   "title": "Dialogue construction by compilation",
   "original": "e91_0853",
   "page_count": 4,
   "order": 212,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "The focus of this paper is on the development and prototyping of speech based applications. We discuss the motivation behind our approach, the speech platform used, the compiler used for developing applications and the formalism used to describe applications. Keywords: dialogue, dialogue system, speech applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-211"
  },
  "nogaito91_eurospeech": {
   "authors": [
    [
     "Izuru",
     "Nogaito"
    ],
    [
     "Masahiko",
     "Takahashi"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Fumihiro",
     "Yato"
    ]
   ],
   "title": "Dialogue management in an extension number guidance system",
   "original": "e91_0857",
   "page_count": 4,
   "order": 213,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "This paper describes a dialogue management module for an extension telephone number guidance system. The system is composed of three modules: speech recognition, dialogue management, and speech synthesis. The dialogue management module selects the combination of phrases which best satisfies the dialogue grammar. The module understands dialogue context, generates sentences for speech synthesis and predicts words in the next utterance. Keywords: Dialogue Understanding; Plan Recognition; Word Prediction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-212"
  },
  "segarra91_eurospeech": {
   "authors": [
    [
     "Encarna",
     "Segarra"
    ],
    [
     "Pedro",
     "Garcia"
    ]
   ],
   "title": "Automatic learning of acoustic and syntactic-semantic levels in continuous speech understanding",
   "original": "e91_0861",
   "page_count": 4,
   "order": 214,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "We present a system for Continuous Speech Understanding tasks, which studies the learning of transducers for these tasks within the framework of Language Models; that is, it modelizes certain linguistic units and constrains the possible concatenation of such units. Both the learning of the acoustic models corresponding to the linguistic units, and the unit concatenation rules are inductively obtained through Grammatical Inference techniques. Some results obtained for the task of understanding Spanish numbers are reported. KEYWORDS: Continuous Speech Understanding, Language Models,\" Transduction, Grammatical Inference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-213"
  },
  "baggia91b_eurospeech": {
   "authors": [
    [
     "Paolo",
     "Baggia"
    ],
    [
     "A.",
     "Ciaramella"
    ],
    [
     "D.",
     "Clementino"
    ],
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "E.",
     "Gerbino"
    ],
    [
     "Egidio P.",
     "Giachin"
    ],
    [
     "G.",
     "Micca"
    ],
    [
     "L.",
     "Nebbia"
    ],
    [
     "R.",
     "Pacifici"
    ],
    [
     "G.",
     "Pirani"
    ],
    [
     "C.",
     "Rullent"
    ]
   ],
   "title": "A man-machine dialogue system for speech access to e-mail information using the telephone: implementation and first results",
   "original": "e91_0865",
   "page_count": 4,
   "order": 215,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "In the framework of the SUNDIAL Esprit System we are developing a man-machine dialogue system for interactive speech access to a remote data base. We describe the major system blocks, i. e. the acoustical front-end, the linguistic processor, the dialogue management and the message generation components, as well as their interplay. Finally we give an overview of the first speed and accuracy performance of the real time demonstrator.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-214"
  },
  "bezooijen91_eurospeech": {
   "authors": [
    [
     "Renee van",
     "Bezooijen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Performance of text-to-speech conversion for dutch: a comparative evaluation of allophone and diphone based synthesis at the level of the segment, the word, and the paragraph",
   "original": "e91_0871",
   "page_count": 4,
   "order": 216,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "During the years 1985-1990 an extensive research program was carried out in the Netherlands to improve the quality of text-to-speech conversion for Dutch, both allophone and diphone based. In this paper the (developmental) quality of the two types of synthesis will be compared at the level of the segment, the word, and the paragraph. Keywords: evaluation, synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-215"
  },
  "benoit91b_eurospeech": {
   "authors": [
    [
     "Christian",
     "Benoît"
    ],
    [
     "Francoise",
     "Emerard"
    ],
    [
     "Betina",
     "Schnabel"
    ],
    [
     "A.",
     "Tseva"
    ]
   ],
   "title": "Quality comparisons of prosodic and of acoustic components of various synthesisers",
   "original": "e91_0875",
   "page_count": 4,
   "order": 217,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "Text-to-speech synthesisers were assessed in Italian and German using the paired comparison method. Stimuli were obtained by cross-transfering the prosodic model from one system to the other so that various combinations of prosodic component and acoustic decoder could be compared to one another. It was thus possible to compare the overall quality of the same prosodic model applied to two different acoustic decoders, the overall quality of the same acoustic decoder excited by two different prosodic models, and other combinations. 10 sentences were synthesized by each system. We here present and discuss the results that were obtained in Italian and German with 16 normal hearing subjects who participated in each experiment. First, the proportion of equivalent responses from the two tests show that listeners are much more confident in their judgment when comparing acoustic differences than when comparing prosodic differences. Second, it seems that the differences between two prosodic models that are compared caeteris paribus depend highly on the acoustic decoder used, and that the differences between two acoustic decoders depend somewhat upon the prosodic model used. To conclude, these observations lead us to recommend care when comparing text-to-speech synthesisers that differ in both components, as the effects of the components on listeners' judgments are not independent and cannot be separately assessed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-216"
  },
  "griee91_eurospeech": {
   "authors": [
    [
     "Martine",
     "Griee"
    ],
    [
     "Kiki",
     "Vagges"
    ],
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "Assessment of intonation in text-to-speech synthesis systems - a pilot test in English and Italian",
   "original": "e91_0879",
   "page_count": 4,
   "order": 218,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "Tests of prosodic form are seen as the first stage of a programme to evaluate the quality of intonation algorithms in text-to-speech synthesis. Before we can assess the functional appropriateness of an intonation contour , we must first assess whether the synthesiser's realisation of this contour is adequate. A pilot test to evaluate the realisation of falls and rises over stretches of differing segmental complexity was carried out in English and Italian. For each segmental context, scores for these two contours were calculated relative to the monotone realisation. Keywords: Text-to-Speech Synthesis, Speech Output, Assessment, Intonation, Prosody\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-217"
  },
  "monaghan91b_eurospeech": {
   "authors": [
    [
     "Alex I. C.",
     "Monaghan"
    ]
   ],
   "title": "Evaluation of the naturalness of prosody generated by the CSTR TTS system",
   "original": "e91_0883",
   "page_count": 4,
   "order": 219,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "This paper reports an experiment to assess the naturalness of the output of the prosodic rules in the CSTR ITS system. Both the duration rules and the intonation rules were evaluated. This work was carried out as part of the final evaluation of the text-to-speech software developed during the Alvey Integrated Speech Technology Demonstrator project: the other section of this evaluation is reported in Sydeserff et al. (1991). The experiment involved 165 subjects judging pairs of stimuli and simply indicating which was the more natural in a forced-choice design. The results clearly indicate that subjects could not reliably distinguish natural from automatic prosody, and that in the worst case our prosodic output is distinguishable from natural prosody in only 17% of cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-218"
  },
  "halka91_eurospeech": {
   "authors": [
    [
     "Ulrich",
     "Halka"
    ]
   ],
   "title": "Speech-model processes for objective quality measurements of speech-coding systems",
   "original": "e91_0887",
   "page_count": 4,
   "order": 220,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "Up to now, the general problem of speech quality assessment, especially of coded speech signals has been accomplished by using highly standardized human listening tests. Because of several shortcomings of these subjective procedures many objective approaches have been proposed in the past. Each of the objective measures tries to avoid most of these drawbacks. However, one of the major problems is still left out of account by all of them: The test results depend on the underlying set of speech-data (e. g. speaker dependence!), since they use natural speech as the test signal. Of course, this problem is unavoidable in the case of subjective procedures, but there is no reason to accept it in the case of objective measurements. So, in this paper we propose a test signal, which is a special random process. It is shown that the important characteristics of this process agree with those of natural speech. Furthermore, the results of a study of objective quality-measures involving a speech-model process as the test signal are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-219"
  },
  "euler91_eurospeech": {
   "authors": [
    [
     "S.",
     "Euler"
    ]
   ],
   "title": "Adaptation techniques in tied density hidden Markov models",
   "original": "e91_0919",
   "page_count": 4,
   "order": 221,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "In this paper we discuss the extension of an isolated word recognition system based on tied density hidden Markov models towards a digit string recognition system. Starting from the models for isolated digits we compare different schemes developed in order to adapt the models. In particular, the modular concept of the recognition system based on tied density hidden Markov models allows to modify either the parameters of the underlying densities or only the weights of the given densities in the model states.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-220"
  },
  "jouvet91_eurospeech": {
   "authors": [
    [
     "D.",
     "Jouvet"
    ],
    [
     "K.",
     "Bartkova"
    ],
    [
     "Jean",
     "Monné"
    ]
   ],
   "title": "On the modelization of allophones in an HMM based speech recognition system",
   "original": "e91_0923",
   "page_count": 4,
   "order": 222,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "This paper describes a new approach for modelling allophones in a speech recognition system based on hidden Markov models. This approach allows to model in detail, and with a limited amount of parameters, the different acoustical realizations of the sounds by integrating left and right context-dependent transitions as well as acoustical targets in the basic units. Phonetic knowledge is used to define the structure of the models, and a standard training procedure determines the optimum value of the parameters. The efficiency of the approach is demonstrated both in a multispeaker mode, with a 500-word vocabulary, and in a speaker-independent mode with several other data bases recorded through the telephone network by more than 500 speakers. The improvement resulting from the use of temporal derivatives are also compared for several kinds of basic units. Keywords: Speech recognition, Markov modelling, Modelization of allophones, Basic units, Temporal derivatives.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-221"
  },
  "jouvet91b_eurospeech": {
   "authors": [
    [
     "D.",
     "Jouvet"
    ],
    [
     "L.",
     "Mauuary"
    ],
    [
     "Jean",
     "Monné"
    ]
   ],
   "title": "Automatic adjustments of the structure of Markov models for speech recognition applications",
   "original": "e91_0927",
   "page_count": 4,
   "order": 223,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "This paper investigates some automatic adjustments of the structure of Markov models: the reduction of the model complexity which is achieved by merging similar gaussian functions, and the improvement of the acoustical modelization which relies on the splitting of some gaussian functions and the discarding of unreliable parameters. These modifications are tested on isolated word vocabularies recorded by more than 500 speakers through the telephone network. On a 36-word vocabulary, and with regular word models, a 40 % reduction of the number of gaussians functions is achieved while keeping a similar recognition performance. Furthermore, a detailed analysis of the training phase shows that the merging operator is useful for discarding unreliable parameters. Several dynamic expansion procedures are also described, which lead to a 30 % error rate reduction for two types of basic units. Keywords: Speech recognition, Markov modelling, Merging gaussian functions, Splitting gaussian functions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-222"
  },
  "leung91b_eurospeech": {
   "authors": [
    [
     "Hong C.",
     "Leung"
    ],
    [
     "I. Lee",
     "Hetherington"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Speech recognition using stochastic explicit-segment modeling",
   "original": "e91_0931",
   "page_count": 4,
   "order": 224,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "In this paper, we present a stochastic explicit-segment modeling (SESM) approach to speech recognition. This approach can be characterized by three major problems: (1) estimation of the probability of a segmentation is formulated as a boundary classification problem, (2) estimation of the probability of a phonetic unit in a segment is treated as a phonetic classification problem, and (3) boundaries and segments used for problems (1) and (2) are proposed by stochastic segmentation. In our current implementation, artificial neural networks are used to deal with the first two problems. We have experimented with SESM on a task of recognizing 25 words (city names) recorded from actual customers over the telephone network. Performance evaluation shows that our approach achieves a recognition accuracy over 93%, or about 99% at a rejection rate of 20%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-223"
  },
  "dubois91_eurospeech": {
   "authors": [
    [
     "D.",
     "Dubois"
    ]
   ],
   "title": "Comparison of time-dependent acoustic features for a speaker-independent speech recognition system",
   "original": "e91_0935",
   "page_count": 4,
   "order": 225,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "Improvement of speech recognition performance was the goal to achieve with the experiments carried out in our laboratories. In this paper we shall attempt to show how we set out to maximize performance once the temporal variations of the coefficients were combined with the instantaneous vector as input to the HMM. Tests were conducted on several databases (digits, isolated commands of a vocal server, and 2-digit numbers), recorded over the telephone. Following which, the concatenation of several frames, linear data reduction analysis techniques and linear regression data were tested. Considerable improvement of the recognition performance was obtained by combining the first and second derivatives with the current frame over five adjacent frames. A recognition error rate of 0. 7% was obtained. Normally, a 2. 7%. error rate is observed using exclusively cepstrum coefficients. This resulted in a 70% error rate reduction. Keywords: HMM input coefficients, temporal variations, principal component analysis, discriminant analysis, linear regression.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-224"
  },
  "gauvain91_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Bayesian learning for hidden Markov model with Gaussian mixture state observation densities",
   "original": "e91_0939",
   "page_count": 4,
   "order": 226,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried but. In a continuous density hidden Markov model (CDHMM) framework, Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering and corrective training. The goal is to enhance model robustness in a CDHMM-based speech recognition system so as to improve performance. Our approach is to use Bayesian learning to incorporate prior knowledge into the training process in the form of prior densities of the HMM parameters. The theoretical basis for this procedure is presented and results applying it to HMM parameter smoothing, speaker adaptation, speaker clustering, and corrective training are given. The following word error reductions were observed on the DARPA RM task: 10% with HMM parameter smoothing, 31% for speaker adaptation with 2 minutes of speaker specific training data, and 15% with sex-dependent modeling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-225"
  },
  "ruehl91_eurospeech": {
   "authors": [
    [
     "Hans-Wilhelm",
     "Ruehl"
    ]
   ],
   "title": "Voice controlled mail ordering via telephone using SPREIN",
   "original": "e91_0945",
   "page_count": 4,
   "order": 227,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "For a mail order application, the SPREIN voice control system has been developed and tested in a field trial. It consists of a dialogue and communications controller and several telephone conversation units (TCU). Each TCU contains a speaker independent isolated word recognizer, an unlimited vocabulary speech synthesizer and a tele- phone line interface. Ordering is done fully automatically using synthetic speech for user guidance and feedback. The device has been developed in 1986 to 1988, and has been evaluated by the German Telekom in 1988 to 1990. In off-line tests, the recognizer's average error rates improved from 2. 7% in 1988 to 1. 8% in 1990. A field trial with voluntary users revealed that the main weakness of the system Is its slow speed of data entry compared to keyboard entry or human order acceptance via telephone. In spite of its drawbacks, users accepted the system and wanted it to to stay on-line after the end of the field trial.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-226"
  },
  "dobler91_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Dobler"
    ],
    [
     "Werner",
     "Armbruester"
    ],
    [
     "Peter",
     "Meyer"
    ],
    [
     "Hans-Wilhelm",
     "Ruehl"
    ]
   ],
   "title": "A voice dialling device for mobile radio",
   "original": "e91_0949",
   "page_count": 4,
   "order": 228,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "Road traffic safety is an increasing problem for users of mobile telephones. In this paper a system is presented, which is designed to assist the driver to use his telephone while driving. For this reason, hands-free telephony, speech recognition and speech coding are used to guide users by voice, let them dial by saying names of the people they want to talk to, and provide them with echo cancellation for hands-free telephone talks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-227"
  },
  "smaili91_eurospeech": {
   "authors": [
    [
     "K.",
     "Smaili"
    ],
    [
     "F.",
     "Charpillet"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "A continuous speech recognition approach for the design of a dictation machine",
   "original": "e91_0953",
   "page_count": 4,
   "order": 229,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "The oral entry of texts (dictation machine) remains an important potential field of application for automatic speech recognition. The RFIA group of CRIN / INRIA has been investigating this research area for the French language during the past ten years. We propose in this paper a general presentation of the present state of our MAUD system which is based upon four major interacting components: an acoustic phonetic decoder, a lexical component, a linguistic model and a user interface. Keywords: dictation machine, linguistic model, blackboard\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-228"
  },
  "thomson91_eurospeech": {
   "authors": [
    [
     "David L.",
     "Thomson"
    ],
    [
     "Jay G.",
     "Wilpon"
    ],
    [
     "Rafid A.",
     "Sukkar"
    ],
    [
     "Dimitrios P.",
     "Prezas"
    ]
   ],
   "title": "Automatic speech recognition in the Spanish telephone network",
   "original": "e91_0957",
   "page_count": 4,
   "order": 230,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "As automatic speech recognition (ASR) moves from the laboratory to the marketplace, a new set of challenges emerge. These difficulties relate to the human interface, environmental factors, and adaptation of recognition technology to specific services. Such issues have prompted new research, resulting in algorithms with word spotting, early decision, and other capabilities. With recent applications such as the AT&T Intelligent Network (IN) in Spain, ASR technology has seen significant improvements in response to field issues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-229"
  },
  "billi91_eurospeech": {
   "authors": [
    [
     "Roberto",
     "Billi"
    ],
    [
     "P.",
     "Buttafava"
    ],
    [
     "P. De",
     "Stefani"
    ],
    [
     "M.",
     "Gamba"
    ],
    [
     "D.",
     "Voltolini"
    ]
   ],
   "title": "Computer-aided, voice-based, medical report preparation: an application to radiology",
   "original": "e91_0961",
   "page_count": 4,
   "order": 231,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "This paper describes an application of speech recognition and synthesis aimed at simplifying the procedure of creating medical reports. The hardware configuration consists of a PC equipped with 2 boards, devoted respectively to speech synthesis and recognition. The system, which has been specialized for radiology, permits to dictate a report in isolated word mode, using a vocabulary which contains about 97% of the words found in a large corpus of radiological reports. To allow fast creation of reports for normal cases or for frequent pathologies, the system permits to use coded sentences which can be directly called or, alternatively, identified by means of a simple question/answering procedure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-230"
  },
  "carlos91_eurospeech": {
   "authors": [
    [
     "Filipe N.",
     "Carlos"
    ],
    [
     "Jose P.",
     "Carmona"
    ],
    [
     "Pedro M.",
     "Chagas"
    ],
    [
     "Luis C.",
     "Oliveira"
    ],
    [
     "Antonio J.",
     "Serralheiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "A recognition / synthesis system applied to database access through the telephone network",
   "original": "e91_0965",
   "page_count": 4,
   "order": 232,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "This paper describes a prototype of a recognition / synthesis system in the Portuguese language. The system was designed to demonstrate an application of the voice-driven database type, consisting of queries to a database installed in a remote computer accessed by the public telephone network. The selected application is a telephone directory service, in which the user requests the telephone number of a subscriber by indicating his first and last names, and the system asks for confirmation of the recognized names before returning the desired information. If requested, a telephone call is made to that particular telephone number. The recognition of the requested names uses a classical DTW technique, and the confirmation software synthesizes the recognized names on the basis of their pre-stored phonetic description, using phoneme concatenation. Preliminary tests with the system show the large potential of this type of applications. Keywords: recognition, DTW, text-to-speech synthesis, phoneme concatenation, voice-driven database\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-231"
  },
  "helle91_eurospeech": {
   "authors": [
    [
     "Seppo",
     "Helle"
    ]
   ],
   "title": "An experiment in using a hypertext system in phonetics and speech processing education",
   "original": "e91_0969",
   "page_count": 4,
   "order": 233,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "The number of educational programs in limited subject areas like phonetics is still quite small, although personal computers are very common equipment. This may be because these programs are not considered commercially productive and because programming is relatively expensive. Different methods of creating educational programs are described briefly in this paper. Besides the traditional programming environments there exist different authoring systems that are easier to learn, yet powerful enough for creating useful applications. They reduce the amount of programming effort in a project, giving also non-programmers more possibilities to develop specialised applications. One particular experiment programmed in a hypertext environment is discussed. The topics of the program are phonetics, speech synthesis and speech recognition. It includes text, pictures and sound samples in a package that is easy enough to use for a novice in computers. Keywords: hypertext, computer aided education, phonetics, speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-232"
  },
  "antoniol91b_eurospeech": {
   "authors": [
    [
     "G.",
     "Antoniol"
    ],
    [
     "F.",
     "Brugnara"
    ],
    [
     "F. Dalla",
     "Palma"
    ],
    [
     "G.",
     "Lazzari"
    ],
    [
     "E.",
     "Moser"
    ]
   ],
   "title": "A. RE. s. : an interface for automatic reporting by speech",
   "original": "e91_0973",
   "page_count": 4,
   "order": 234,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "The project and the first prototype of an interface for dictating, recording and printing radiological reports is presented. The most important feature of this interface is multimodality. The radiologist may choose among speech, keyboard and mouse to generate a report. The choice of the input modality depends on the working conditions of the radiologist and the ease and quickness of communication. The motivation for such a project, and the study of the impact of this system on the organisation of the radiologic department in terms of possible improvements in the reporting service, are also presented. The constraints on radiologist-computer speech communication are analyzed. The speech recognizer has been trained on utterances obtained by processing voice reports relative to chest and Ultra Sound (US) examinations. Four users pronounced 100 chest and 20 US reports. This speech material was used as test-data for the automatic speech recognizer. The recognition rate of the speech recognizer, on the two dictionaries, was 98% in the best case and 93% in the worst case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-233"
  },
  "schulthei91_eurospeech": {
   "authors": [
    [
     "U.",
     "Schultheiß"
    ],
    [
     "B.",
     "Lochschmidt"
    ]
   ],
   "title": "COGNITO - an experimental voice-controlled telecommunication system",
   "original": "e91_0977",
   "page_count": 4,
   "order": 235,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "The available technology may enable many handicapped people to make use of telecommunication equipment and services. In this field, modern speech processing technology plays an important part. As a first step, it is necessary, in this context, to develop appropriate concepts for equipment and services which fulfill the needs of a maximum number of handicapped persons and, at the same time, promise economic success. For these purposes, a PC-based experimental system named COGNITO has been developed. Seen from a functional viewpoint, the present COGNITO version constitutes a telephone with advanced functions all of which are controllable by the human voice.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-234"
  },
  "bernstein91_eurospeech": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Dimitry",
     "Rtischev"
    ]
   ],
   "title": "A voice interactive language instruction system",
   "original": "e91_0981",
   "page_count": 4,
   "order": 236,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "SRI International has developed a spoken-language instruction system that uses audio input and output in foreign language teaching. The system presents information to a learner in either graphical or audio form, and the learner responds by speaking. The learner's speech to the system is recognized and evaluated by a modified version of SRI's DECIPHER HMM-based speech recognition system. The system has been implemented on commercially available microcomputer hardware and responds in real time to the learner's speech. The recognition system was modified for non-native speech. The system environment supports several kinds of instruction in spoken language, including spoken responses to multiple choice questions as well as practice in the production of syntactic patterns and conversational formulas. Several example lessons have been developed for this voice-input environment. The Voice Interactive Language Instruction (VDLJ TM) system itself is demonstrated as part of the presentation. Keywords: language, instruction, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-235"
  },
  "rooney91_eurospeech": {
   "authors": [
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Steven",
     "Hiller"
    ],
    [
     "John",
     "Laver"
    ],
    [
     "Maria-Gabriella Di",
     "Benedetto"
    ]
   ],
   "title": "Macro and micro features for automated pronunciation improvement in the spell system",
   "original": "e91_0985",
   "page_count": 4,
   "order": 237,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "The analysis of macro (prosodic) and micro (segmental) features is described for a workstation designed to improve the pronunciation of English, French and Italian by non-native speakers. The SPELL workstation is intended to be a teaching device aimed at intermediate ability foreign language learners. Audio and visual aids will be used to help students improve their general intelligibility within a basic teaching paradigm called DELTA (Demonstrate, Evaluate Listening, Teach and Assess). Prosodic analysis will apply to the features of intonation, stress and rhythm. A phonological approach is used for intonation which provides a well-structured system of contrasting units that correlate with discrete linguistic functions. A more limited approach to the prosodic phonology of stress and rhythm will be taught in the SPELL system by manipulating vowel quality and segmental duration. The micro feature analysis will focus on vowel contrasts, using a distinctive feature approach to characterize non-native vowel pronunciation. Acoustic properties are sought which will be speaker-independent. Keywords: Computer-aided Language Learning; Pronunciation, Prosody, Articulatory Phonetics, ESPRIT\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-236"
  },
  "devillers91_eurospeech": {
   "authors": [
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Christian",
     "Dugast"
    ]
   ],
   "title": "Comparison of continuous mixture densities and TDNN in a viterbi-framework: experiments on speaker dependent DARPA RM1+",
   "original": "e91_0991",
   "page_count": 4,
   "order": 238,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "We present a comparison of a continuous hidden Markov model (CHMM) with a hybrid system using a Time Delay Neural Network (TDNN) and a HMM for speaker-dependent continuous speech recognition. The network pre-processes the speech signal for a discrete HMM system. Several hybrid systems combining Neural Networks and HMMs have been compared with basic discrete HMMs using one codebook and no contextual phone models, in which case they give better results. The TDNN has a powerful ability to extract discriminant features but this possibility decreases when the difficulty of the task increases. Instead of learning a unique large neural net to capture all the regions in the feature space, we structured the net in sub-networks discriminating macro-classes of phonemes, each macro-class being itself a subnetwork discriminating phonemes in its own class. Each of these sub-networks has high performances. Two different integrations of these sub-networks in the HMM formalism are tested: combining all the sub-networks in a global net including a new learning phase, and integrating the hierarchically structured sub-networks directly in the HMM structure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-237"
  },
  "thurston91_eurospeech": {
   "authors": [
    [
     "Peter",
     "Thurston"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "A comparison of two compression functions used for noisy vowel detection with back-propagation networks",
   "original": "e91_0995",
   "page_count": 4,
   "order": 239,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "This paper compares the merits of two alternative front-ends for a connectionist vowel recogniser. One, a psychologically motivated auditory hair-cell model, and the other, a conventional FFT as commonly employed in speech recognisers. Our results demonstrate that the choice of front-end has a marked impact on the recognition of vowels, particularly when the networks are trained and tested using speech in the presence of noise. The two front-ends differ in a number of respects that might account for their differing performance. One notable difference is that whereas the FFT based front-end uses an output compression function that is cubic, the psychological model has an intrinsic compression function that is logarithmic. By rescalling the output values for the two front-ends, we demonstrate that the difference in performance is determined by the speech compression function and not by the choice of front-end per se. These results serve two roles. Firstly, they suggest that in comparing alternative front-ends for connectionist recognisers the outputs of the frontends should be transformed to ensure that they are all subject to the same effective compression function. Secondly, they have been used by us to aid in making fine adjustments to the given auditory model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-238"
  },
  "ferreiros91_eurospeech": {
   "authors": [
    [
     "J.",
     "Ferreiros"
    ],
    [
     "A.",
     "Castro"
    ],
    [
     "J. M.",
     "Pardo"
    ]
   ],
   "title": "Comparison between two different approaches in speaker - independent isolated digit recognition",
   "original": "e91_0999",
   "page_count": 4,
   "order": 240,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "The object of this paper is to present and compare two different systems for isolated-digit speaker-independent speech recognition, one based in Continuous-densities HMM and the other in a Time-Delay Neural Network. The HMM system passed several optimization phases from a system with 10 Mel frequency cepstrum coefficients (MFCC) plus energy, to a system with 10 MFCC plus their 10 incremental parameters, plus energy and delta-energy. Optimization of the TDNN system has been made based on shifting some input frames for each activation of the neurons in the first layer instead of one as used in other experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-239"
  },
  "poirier91_eurospeech": {
   "authors": [
    [
     "Franck",
     "Poirier"
    ]
   ],
   "title": "DVQ: dynamic vector quantization application to speech processing",
   "original": "e91_1003",
   "page_count": 4,
   "order": 241,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "In this paper, we present a modified version of the Learning Vector Quantization (LVQ) called Dynamic Vector Quantization (DVQ). We compare the performances of both classifiers based on competitive learning on speech classification tasks. All the experiments clearly highlight the ability of generalization of the DVQ algorithm, it gives best result than LVQ2 on the test set. Moreover, DVQ always provides substancial gain in memory size and consequently is less time- consuming. Keywords: classification, vector quantization, evaluation, artificial neural network, acoustic-phonetic decoding, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-240"
  },
  "bengio91b_eurospeech": {
   "authors": [
    [
     "Yoshua",
     "Bengio"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "Ralf",
     "Kompe"
    ]
   ],
   "title": "A comparative study on hybrid acoustic phonetic decoders based on artificial neural networks",
   "original": "e91_1007",
   "page_count": 4,
   "order": 242,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "In this paper we compare two hybrid acoustic-phonetic decoders based on Artificial Neural Networks (ANN). We evaluate them on the task of recognizing stop phones in continuous speech independently from the speaker. ANNs are well suited to perform detailed phonetic distinctions. In general, techniques based on Dynamic Programming (DP), in particular Hidden Markov Models (HMMs), have proven to be successful at modeling the temporal structure of the speech signal. In the approach described here, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters of the ANN/HMM decoder. Comparative experiments using this ANN/HMM hybrid decoder and another ANN-DP hybrid are reported for the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-241"
  },
  "sawai91_eurospeech": {
   "authors": [
    [
     "Hidefumi",
     "Sawai"
    ],
    [
     "Satoru",
     "Nakamura"
    ]
   ],
   "title": "Time-delay neural network architectures for high-performance speaker-independent recognition",
   "original": "e91_1011",
   "page_count": 4,
   "order": 243,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "Several Time-Delay Neural Network(TDNN) architectures applied to speaker-dependent and multi-speaker's phoneme recognition are compared with respect to their capabilities on a speaker-independent phoneme recognition problem. Phoneme experiments for recognizing voiced stops /b, d, g/ using six and twelve training speakers showed high average recognition rates of 91. 3% and 93. 6%, respectively for eight test speakers. In addition, constructing networks by speakers' modules is effective in terms of saving training time, and leads to higher recognition performance than a single structure of TDNN with comparable network capacity. Furthermore, we propose an extended architecture for recognizing all phonemes based on the achievements in this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-242"
  },
  "wittenburg91_eurospeech": {
   "authors": [
    [
     "P.",
     "Wittenburg"
    ],
    [
     "R.",
     "Couwenberg"
    ]
   ],
   "title": "Recurrent neural nets as building blocks for human word recognition",
   "original": "e91_1015",
   "page_count": 4,
   "order": 244,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "Until now we concentrated our work on methodological questions. Here we report on experimental results which show that recurrent neural networks being trained to behave as phoneme spotters can be used to transform complex speech patterns into simpler and more unique sequences of events. They are able to deal with the fairly different time structure of the different phoneme classes. The representations stored of at least plosives become more robust, if the training covers several speakers, since the networks are forced to extract only the relevant information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-243"
  },
  "mekuria91_eurospeech": {
   "authors": [
    [
     "Fisseha",
     "Mekuria"
    ],
    [
     "Tore",
     "FjÖllbrant"
    ]
   ],
   "title": "A neural net model for vector quantization",
   "original": "e91_1019",
   "page_count": 4,
   "order": 245,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "In this paper a neural net model for the tree searched vector quantizer is described. The design of the codebook containing the optimal number of representative vectors, incorporates a tree structure with a modified perceptron neural at each node. The proposed algorithm gives reduction in computation requirement compared to the usual full search and tree-search algorithms and requires less memory than the tree search algorithm. Keywords: Vector Quantization, Percetron neural-net, tree search, codebook\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-244"
  },
  "russell91_eurospeech": {
   "authors": [
    [
     "N. H.",
     "Russell"
    ],
    [
     "Frank",
     "Fallside"
    ],
    [
     "A. J.",
     "Robinson"
    ],
    [
     "R. W.",
     "Prager"
    ]
   ],
   "title": "Lexical access using a recurrent error propagation network",
   "original": "e91_1023",
   "page_count": 4,
   "order": 246,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "The paper explores the use of a recurrent neural net to perform lexical access. It is trained to effect a mapping from the phonemic output of a neural net front end (reported in [5]), to lexical items. A cascaded multi-level approach is used which allows common phonetic variation in words to be captured by the front end. In order to keep the context duration manageable, non-linear time compression is performed on the input data to the lexical access network, such that transitional segments are retained, while steady state probability segments are much shortened, but duration information is not completely obscured. A post-processor, based on a simple finite state automaton, is used to decode the raw NN output, which takes the form of a word pseudo-probability vector (and is synchronous with the input) to produce a symbolic (i. e. orthographic) interpretation of the input speech. Encouraging results using digit strings are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-245"
  },
  "brauer91_eurospeech": {
   "authors": [
    [
     "Peter",
     "Brauer"
    ],
    [
     "Per",
     "Hedelin"
    ],
    [
     "Dieter",
     "Huber"
    ],
    [
     "Petter",
     "Knagenhjelm"
    ],
    [
     "Johan",
     "Molno"
    ]
   ],
   "title": "Model or non-model based classifiers",
   "original": "e91_1027",
   "page_count": 4,
   "order": 247,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "This paper presents a comparative study of model versus non-model based classifiers, which are systematically evaluated on two standard tasks in speech processing. Model based approaches are represented by a Gaussian classifier, non-model based approaches by a multi-layer perceptron. The performance of both classifiers is evaluated with respect to (1) a acoustics-to-phoneme class recognition task, and (2) a limited grapheme-to-allophone conversion task. The Gaussian classifier introduced in this study differs from conventional designs in that the density parameters, i. e. the mean and covariance values, are established by way of optimization. An iterative, gradient-based, algorithm is employed to determine the parameter values that will result in minimal mis-classification. The results of the evaluations presented in this study indicate that the Gaussian classifier performs at least as well as the MLP-based classifier, with respect to both discrimination tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-246"
  },
  "aliosaar91_eurospeech": {
   "authors": [
    [
     "Toomas",
     "Aliosaar"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "Event-based recognition and analysis of speech by neural networks",
   "original": "e91_1031",
   "page_count": 4,
   "order": 248,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "Neural networks have been shown to be a powerful approach to speech recognition. Especially the class of time-delay neural networks (TDNN) exhibiting a time invariance feature has been a popular method. In this paper we emphasize another approach to speech recognition and analysis where neural nets are used differently from principles like TDNN in the way in which time is treated. We call this approach time-event neural networks (TENN) because it is based on the detection of events as time moments or intervals of interest as the first step and recognition (classification) of the events as the second step. These steps may also be integrated together. The advantages are explicit temporal information gained from events and reduced computation due to focused classification. The paper describes primarily the general principles of the approach. Experiments and preliminary results are shown from diphone-based speech recognition. Keywords: Speech recognition; Neural networks; Event-Based Speech Analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-247"
  },
  "jelinek91_eurospeech": {
   "authors": [
    [
     "Frederick",
     "Jelinek"
    ]
   ],
   "title": "Up from trigrams! - the struggle for improved language models",
   "original": "e91_1037",
   "page_count": 4,
   "order": 249,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "The first experimental results in \"large\" vocabulary speech recognition were obtained in 1976 [ Bahl 78 ]. They involved continuous speech reading of. the so called Laser Patent Text. The test set was limited to sentences that were entirely composed of words belonging to a vocabulary of the 1000 most frequent words found in the training text.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-248"
  },
  "carlson91_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ]
   ],
   "title": "Synthesis: modelling variability and constraints",
   "original": "e91_1043",
   "page_count": 6,
   "order": 250,
   "p1": "1043",
   "pn": "1048",
   "abstract": [
    "This paper discusses some important issues in current speech synthesis research. Modelling of speaker characteristics and emotions are used as a examples of new trends in the area. The relation to speech recognition research is also emphasized. New methods such as automatic learning and the use of new analysis techniques are also referred to.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-249"
  },
  "guyomard91_eurospeech": {
   "authors": [
    [
     "M.",
     "Guyomard"
    ],
    [
     "J.",
     "Siroux"
    ],
    [
     "A.",
     "Cozannet"
    ]
   ],
   "title": "The role of dialogue in speech recognition the case of the yellow",
   "original": "e91_1051",
   "page_count": 4,
   "order": 251,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "The predictable nature of certain categories of dialogues can be used both to constrain and to improve the linguistic processing part in speech recognition and understanding. Having looked at several kinds of prediction capabilities as they appear in certain oral dialogue systems, we shall develop the implementation of the predictions in the Yellow Pages (PJ) system. Eventually we shall investigate the influence of dialogue strategies on the linguistic behaviour of the user. We particularly focus on some of the benefits one can gain from using certain forms of co-operation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-250"
  },
  "gerbino91_eurospeech": {
   "authors": [
    [
     "E.",
     "Gerbino"
    ],
    [
     "Paolo",
     "Baggia"
    ]
   ],
   "title": "Interpretation of context-dependent utterances in man-machine dialogue",
   "original": "e91_1055",
   "page_count": 4,
   "order": 252,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "This paper describes a strategy for maintaining the information necessary to interpret Context-dependent utterances. The interpretation context of an utterance can be defined as the information that are necessary to interpret it in a plausible way. Context-dependent utterances usually present linguistic phenomena like: Elliptical Reference or Anaphora. The paper, starting from utterance examples pertaining to the application domain, will describe the use of the interpretation context structures and the resulting interpretation process of context-dependent utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-251"
  },
  "eggins91_eurospeech": {
   "authors": [
    [
     "S.",
     "Eggins"
    ],
    [
     "Julie P.",
     "Vonwiller"
    ],
    [
     "C. M. I.",
     "Matthiessen"
    ],
    [
     "P.",
     "Sefton"
    ]
   ],
   "title": "The description of minor clauses in information-seeking telephone dialogues",
   "original": "e91_1059",
   "page_count": 4,
   "order": 253,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "This paper reports on work which is part of a project on the development of a dialogue manager for a speech recognition system. The analysis incorporates systemic-functional discourse and lexico-grammatical analysis, Hallidayian intonational analysis, and human factors design. This paper reports primarily on the linguistic analysis. Within linguistic models which seek to relate discourse function to lexico-grammatical realisations, research has concentrated on grammatically well formed sentence-type moves, realised as major clauses. However, because of the high frequency of minor clauses in our dialogue corpus, we found it necessary to extend the description to this aspect of the MOOD system of English grammar. The analysis involved isolating all minor clauses, which we examined for pitch contour, speech function and position in generic and exchange structure. Our paper reports on how we have integrated the description of minor clauses within a functional semantic model of dialogue, associating intonational patterns with the speech functions of these clauses through the systems of MOOD and KEY. Keywords: minor clause, systemic grammar, intonation, discourse analysis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-252"
  },
  "roe91_eurospeech": {
   "authors": [
    [
     "David B.",
     "Roe"
    ],
    [
     "Fernando",
     "Pereira"
    ],
    [
     "Richard W.",
     "Sproat"
    ],
    [
     "Michael D.",
     "Riley"
    ],
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Alejandro",
     "Macarron"
    ]
   ],
   "title": "Toward a spoken language translator for restricted-domain context-free languages",
   "original": "e91_1063",
   "page_count": 4,
   "order": 254,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "An effort is underway at AT&T Beil Laboratories and Telefonica Investigacion y Desarrollo to build a restricted domain spoken language translation system, which we call VEST (Voice English/Spanish Translator). The eventual goal is a voice input, voice output translator which is speaker-independent, and has a vocabulary of several thousand words covering a specific application. This paper is a progress report on the first step of our research, a system which recognizes several speakers and is limited to a few hundred words. The key new idea is that the speech recognition and the language analysis are tightly coupled by using the same language model, an augmented phrase structure grammar, for both.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-253"
  },
  "subramaniam91_eurospeech": {
   "authors": [
    [
     "N. Venkata",
     "Subramaniam"
    ],
    [
     "N.",
     "Alwar"
    ],
    [
     "G.",
     "Mallikarjuna"
    ],
    [
     "P. Prabhakar",
     "Rao"
    ],
    [
     "S.",
     "Raman"
    ]
   ],
   "title": "Bidirectional machine translation in indian languages",
   "original": "e91_1067",
   "page_count": 4,
   "order": 255,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "This paper, discusses the approach adopted in the development of a bidirectional Machine Translation system for Indian languages. The approach makes use of the characteristics of the languages in simplifying the process of translation. The verb- final sentence structure and the case-inflected nature of Indian language sentences have led us to adopt a verb-centered approach. The analysis is carried out at phrase level, with a phrase being demarked by a verb. A frame-based representation scheme is used to map the results of the analyzer. Due to the free word order nature of Indian languages, the word order of the source languages is retained in the target language. The system translates the typical conversation at railway counters between Telugu and Hindi. Keywords: verb-centered approach, case inflections, frames, free word order\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-254"
  },
  "papaodysseus91_eurospeech": {
   "authors": [
    [
     "C.",
     "Papaodysseus"
    ],
    [
     "E.",
     "Koukoutsis"
    ],
    [
     "C.",
     "Triantafyllou"
    ],
    [
     "C.",
     "Vasilatos"
    ]
   ],
   "title": "Exact monitoring of the numerical error in various speech algorithms",
   "original": "e91_1073",
   "page_count": 4,
   "order": 256,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "In this paper, it is proved that there are two types of numerical error, due to finite precision in the Levinson-Durbin algorithm: an erratic and a systematic one. The erratic one depends on the value the input autocorrelation accidentally takes at an iteration, and, essentially, it affects only the results obtained at this particular recursion. On the contrary, the systematic numerical error increases with the information the system carries and propagates essentially throughout the algorithm. It is shown that, for both types of error, as well as the overall one, there are specific intermediate quantities, calculated in the evolution of the algorithm, which may serve as precise indicators of the exact number of erroneous digits with which the various quantities are computed including the PARCORs and the filter coefficients. Therefore, the generated numerical error can be accurately traced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-255"
  },
  "koreman91_eurospeech": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Automatic computation and comparison of dynamically varying voice source parameters",
   "original": "e91_1077",
   "page_count": 4,
   "order": 257,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "In this paper we describe a (semi-automatic method for the characterization of the dynamics of the (flow) voice source in terms of variations in a set of descriptive parameters. The method uses the electroglottogram and oral flow as input signals. We use Hidden Markov Modelling (HMM) to search for typical patterns in the parameter set under different linguistic conditions. In this paper we focus on a discussion of the usefulness of HMM for modelling the dynamics of the source characteristics during the production of vowels and voiceless and voiced obstruents.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-256"
  },
  "alku91_eurospeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Glottal wave analysis with pitch synchronous iterative adaptive inverse filtering",
   "original": "e91_1081",
   "page_count": 4,
   "order": 258,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "A new glottal wave analysis method, Pitch Synchronous Iterative Adaptive Inverse Filtering (PSIAIF), is presented. The algorithm takes advantage of a previously developed glottal wave analysis technique, Iterative Adaptive Inverse Filtering (IAIF). The basic idea in the IAIF-algorithm is that the average effect of the glottal excitation to the speech spectrum is first estimated using an adaptive low-order all-pole filter. The estimate for the vocal tract is then computed by applying LPC-analysis to the signal from which the estimated glottal contribution was eliminated. The glottal excitation is obtained by eliminating the effects of the vocal tract and lip radiation by inverse filtering. The new PSIAIF-method applies first the IAIF-analysis described above pitch asynchronously to a speech signal. The resulting glottal wave estimate is used in order to determine positions of frames for pitch synchronous IAIF-analysis. The final estimate for the glottal pulseform is obtained by analysing the original speech signal by the IAIF-algorithm using frames that span speech samples between consecutive maximal glottal openings. Preliminary results show that the PSIAIF-technique is able to give a fairly reliable estimate for the glottal excitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-257"
  },
  "galas91_eurospeech": {
   "authors": [
    [
     "Thieny",
     "Galas"
    ],
    [
     "Xavier",
     "Rodet"
    ]
   ],
   "title": "Generalized functional approximation for source-filter system modeling",
   "original": "e91_1085",
   "page_count": 4,
   "order": 259,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "Most of speech signals exhibit a discrete power spectrum: i. e. a limited number of values only are of importance, which generally correspond to harmonic partials of voiced sounds. It is well known that source-filter modeling of such signals leads to difficulties with usual methods such AR modeling. This article deals with a new method for modeling this class of signals which extend and unify preceeding works.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-258"
  },
  "bimbot91_eurospeech": {
   "authors": [
    [
     "Frederic",
     "Bimbot"
    ],
    [
     "Bishnu S.",
     "Atal"
    ]
   ],
   "title": "An evaluation of temporal decomposition",
   "original": "e91_1089",
   "page_count": 4,
   "order": 260,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "Temporal decomposition is a technique for modeling speech spectral evolution. Under this approach, a speech segment is described as a linear combination of a small number of spectral targets. The contributions of the targets are expressed by non-uniformly spaced interpolation functions that are constrained to be of limited duration. This model provides a mathematical representation of speech acoustic structure, that allows reconstruction by simple linear combinations. It thus could be applied for speech synthesis as well as for speech recognition. The aim of the work presented here has been to evaluate the temporal decomposition technique in providing consistent target vectors (that contain the spectral information) and interpolation functions (that give a time-segmentation). A protocol of experiments was therefore designed and carried out. Performances in segmentation were evaluated automatically using a manual phonemic labeling as a reference. Approximately 85 % phonemes in the normative transcription are directly retrieved by temporal decomposition, jointly with 35 % insertions. Dictionaries of spectral targets were constituted and compared with other dictionaries, built from original spectral parameters, in basic pattern recognition experiments. In most cases, target vectors and original parameters performed equally well. That tends to show that no significant spectral information is lost through temporal decomposition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-259"
  },
  "zunkler91_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Zünkler"
    ]
   ],
   "title": "A discriminative recognizer for isolated and continuous speech using statistical separability measures",
   "original": "e91_1095",
   "page_count": 4,
   "order": 261,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "This paper describes a new approach to im- prove the discriminative capabilities of poorly trained Hidden Markov Models. A discriminative post-classifier is developed for the recognition procedure, which reduces the classification to pairwise phoneme comparisons by weighting the components of the feature vectors according to their ability to separate the phoneme pairs. In this paper the application to single word and continuous speech recognition is presented. The discriminative approach has much effect on models, trained with a small database. In experiments the speaker independent recognition rate of German digits could be increased from 90 % to 96 %. With a larger training database and well trained models, the recognition rate of 99. 5 % can not be further improved. Keywords: Hidden Markov Model, discriminative recognition, transinformation, Kolmogorov distance\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-260"
  },
  "schmidbauer91_eurospeech": {
   "authors": [
    [
     "O.",
     "Schmidbauer"
    ],
    [
     "H.",
     "Höge"
    ]
   ],
   "title": "Speaker adaptation based on articulatory features",
   "original": "e91_1099",
   "page_count": 4,
   "order": 262,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "We present an approach for rapid speaker-adaptation which both reduces inter-speaker variability on the acoustic level and permits dynamic adaptation of the system's reference model. In contrast to other methods we are using a two level approach, we (1) dynamicly normalize speech parameters (formants) to speaker specific means and variances, and (2) we are using an articulatory based representation which is situated between the acoustic and phonemic level. Performance was evaluated on a vocabulary independent continuous speech task with perplexity 120. We achieved 9. 2% word error using only 10 short sentences for adaptation to a new speaker; the error rates for the speaker-dependent and cross-speaker mode are 8. 5% and 24. 7% respectively. The results show that the articulatory representation is relatively speaker-invariant and can be \"tuned\" to a new speaker with only a small amount of training samples. Keywords: two-step speaker-adaptation method, normalized formant features, articulatory-feature vector (AFV), Hidden Markov Models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-261"
  },
  "brugnara91_eurospeech": {
   "authors": [
    [
     "F.",
     "Brugnara"
    ],
    [
     "Renato De",
     "Mori"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "M.",
     "Omologo"
    ]
   ],
   "title": "A parallel HMM approach to speech recognition",
   "original": "e91_1103",
   "page_count": 4,
   "order": 263,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "Stochastic signal models represent a powerful way to approach the problem of speech recognition. A particular stochastic modeling, the first order Hidden Markov Model (HMM), has become increasingly popular, because it has a solid theoretical basis and offers practical advantages. In this paper we will extend the standard HMM theory to Parallel Hidden Markov Model (PHMM). The parallel model consists of two statistically related HMMs. This configuration permits a more complete and accurate characterization of the speech signal. In this framework, an observation consists of a couple of acoustic parameter vectors, one for a standard HMM and the other for an HMM whose parameters are probabilistic functions of the state of the first model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-262"
  },
  "nitta91_eurospeech": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Jun'ichi",
     "Iwasaki"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ]
   ],
   "title": "Speaker independent word recognition using HMMs with an orthogonalized phonetic segment codebook",
   "original": "e91_1107",
   "page_count": 4,
   "order": 264,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "The large matrix quantization (MQ) distortion becomes a problem as a spectrum-time pattern in MQ have many dimensions and wide variation. In this paper, we introduce a multiple phonological unit called the phonetic segment for a unit of MQ and apply a statistical matrix quantization (SMQ). The SMQ effectively incorporates pattern variations of each phonetic segment into an orthogonalized phonetic segment codebook. We also propose a simple SMQ-HMM training algorithm called an Equally Counted K-best Learning in which each phonetic event observed within the best K is equally counted in a model and output probabilities are smoothed without fuzzy rule. The proposed method has been tested on a 100-word vocabulary data set uttered by 10 unknown speakers, using a real time recognition system, and has achieved the high performance of 96. 0%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-263"
  },
  "fung91_eurospeech": {
   "authors": [
    [
     "Pascale",
     "Fung"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Unsupervised speaker normalization by speaker Markov model converter for speaker-independent speech recognition",
   "original": "e91_1111",
   "page_count": 4,
   "order": 265,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "We present a new speaker normalization method by which new speaker (NS) data are converted into data similar to the reference speaker (RS) utterance. The Speaker Markov Model Converter (SMMC) converts input NS spectrum data into RS label sequence, which is passed directly to a Hidden Markov Model recognition system. The Converter parameters are estimated from NS spectrum DP-aligned with RS spectrum and RS label stream. The training of the Converter is done using NS input test data and the original RS training data, by this we achieve an unsupervised normalization process. Converter training which includes parameter estimation and improvement is in parallel with the recognition process. Iterations are performed to improve the Converter. HMM score thresholding, template matching and DP thresholding techniques are applied to select suitable data for unsupervised mapping of NS and RS data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-264"
  },
  "son91_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The influence of formant track shape on the perception of synthetic vowels",
   "original": "e91_1117",
   "page_count": 4,
   "order": 266,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "The influence of vowel duration and vowel formant track shape on vowel recognition were tested with a listening experiment. Listeners heard synthetic vowel stimuli in isolation. The duration of the stimuli varied between 6ms and 150ms. Formant tracks were either constant or were shaped according to a parabolic function. The excursion size of the formant tracks (i. e. maximum - minimum) varied between 0 and 375 Hz. Except for long-short vowel oppositions, the responses of the listeners showed no influence of stimulus duration. The effects of formant track shape on vowel identification can best be explained by assuming perceptual undershoot (i. e. averaging over formant tracks). Keywords: vowel perception, duration, formant tracks\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-265"
  },
  "howardjones91b_eurospeech": {
   "authors": [
    [
     "P. A.",
     "Howard-Jones"
    ]
   ],
   "title": "Fluctuation of noise background: measurement and significance in relation to speech masking",
   "original": "e91_1121",
   "page_count": 4,
   "order": 267,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "The formulation of appropriate methods to measure the fluctuation of a noise environment may prove helpful in studies concerning both human and automatic speech recognition. This paper first discusses what is meant by the term 'fluctuation' in the context of speech masking. It then describes two recent experiments which investigated the effects of masker fluctuation on human speech perception and proposes an approach to measuring the fluctuation of a speech masker. Keywords: noise, speech perception, fluctuation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-266"
  },
  "ma91_eurospeech": {
   "authors": [
    [
     "C.",
     "Ma"
    ],
    [
     "L. F.",
     "Willems"
    ]
   ],
   "title": "The audibility of narrow band noise in fiat spectral complex sounds",
   "original": "e91_1125",
   "page_count": 4,
   "order": 268,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "Masking of narrow band noise by periodic pulse trains of different repetition rates and by synthetic vowels was investigated in this paper. Results indicate that the masking process is controlled by limited spectral resolution in the low-frequency part of the masker and by limited temporal resolution in the high-frequency part. The results are also discussed in connection with auditory correlates in speech signal processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-267"
  },
  "laan91_eurospeech": {
   "authors": [
    [
     "Gitta P. M.",
     "Laan"
    ],
    [
     "Dick R. van",
     "Bergem"
    ],
    [
     "Fiorien J. Koopmans-van",
     "Beinum"
    ]
   ],
   "title": "The importance of spectral quality of vowels for the intelligibility of sentences",
   "original": "e91_1129",
   "page_count": 4,
   "order": 269,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "The intelligibility of Dutch sentences in which specific vowels were systematically replaced by the Dutch vowel /oe/ was tested. The prosodic features of the vowels were copied to their substitute by using the TD-PSOLA technique. Plausibility of the sentences appeared to be the predominant factor in this experiment. Word frequency had only a small effect. For function words the number of correct responses was higher than for content words. A word stress effect could not be demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-268"
  },
  "steeneken91b_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ],
    [
     "Tammo",
     "Houtgast"
    ]
   ],
   "title": "On the mutual dependency of octave-band-specific contributions to speech intelligibility",
   "original": "e91_1133",
   "page_count": 4,
   "order": 270,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "Currently objective me*asures for predicting the intelligibility of speech are modelled as a linear summation of the contribution of individual frequency bands. The Articulation Index (AI, French and Steinberg, 1947) and Speech Transmission Index (STI, Steeneken and Houtgast, 1980) are based on such a model. There is evidence that this concept is not correct for conditions with gaps or selective masking present in the frequency domain. We designed an experiment where the contribution of individual frequency bands is studied. For this purpose the speech signal is subdivided into seven octave bands ranging from 125 Hz to 8 kHz. For 26 different combinations of minimal three octave bands the CVC-word score (Consonant-Vowel-Consonant, nonsense words) was obtained at three signal-to-noise ratios. A revised model, which accounts for mutual dependency between adjacent octave bands by the introduction of a so-called redundancy factor will be proposed. Consequences for the existing objective measures are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-269"
  },
  "ooyen91_eurospeech": {
   "authors": [
    [
     "Brit van",
     "Ooyen"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Detection times for vowels versus consonants",
   "original": "e91_1451",
   "page_count": 4,
   "order": 271,
   "p1": "1451",
   "pn": "1454",
   "abstract": [
    "This paper reports two experiments with vowels and consonants as phoneme detection targets in real words. In the first experiment, two relatively distinct vowels were compared with two confusible stop consonants. Response times to the vowels were longer than to the consonants. Response times correlated negatively with target phoneme length. In the second, two relatively distinct vowels were compared with their corresponding semivowels. This time, the vowels were detected faster than the semivowels. We conclude that response time differences between vowels and stop consonants in this task may reflect differences between phoneme categories in the variability of tokens, both in the acoustic realisation of targets and in the representation of targets by subjects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-270"
  },
  "bergem91_eurospeech": {
   "authors": [
    [
     "Dick R. van",
     "Bergem"
    ]
   ],
   "title": "The influence of sentence accent, word stress, and word class on the quality of vowels",
   "original": "e91_1455",
   "page_count": 4,
   "order": 272,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "The effect of sentence accent, word stress, and word class (function words vs. content words) on the pronunciation of Dutch vowels was investigated. It was found that these linguistic factors have a clear influence both on the duration and on the position of ('steady-state') formant frequencies of vowels. Furthermore it was found that the shift of formant frequencies due to these factors is not necessarily towards the centre of the vowel triangle because of an interaction between spectral reduction and coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-271"
  },
  "beinum91_eurospeech": {
   "authors": [
    [
     "Florien J. Koopmans-van",
     "Beinum"
    ]
   ],
   "title": "A peak-and-level model for focus words in read and spontaneous natural speech and in synthetic speech",
   "original": "e91_1459",
   "page_count": 4,
   "order": 273,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "In order to account»for the role of focus words, i. e. words bearing the highest load of semantic information in speech, we introduced the so-called 'peak-and-level' model. By means of this model we studied acoustic-phonetic characteristics of focus words in spontaneous speech as well as in the same texts, read aloud after orthographic transcription. The results of our measurements in both natural speech conditions were compared with the data when the same texts are synthesized by Dutch diphone synthesis. After manipulation of some spectro-temporal aspects in the speech synthesis, listeners were asked to judge the naturalness and intelligibility of the speech, as compared to the original spontaneous and read versions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-272"
  },
  "ingram91_eurospeech": {
   "authors": [
    [
     "J.",
     "Ingram"
    ],
    [
     "J.",
     "Pittam"
    ]
   ],
   "title": "Connected speech processes in second language learning",
   "original": "e91_1463",
   "page_count": 4,
   "order": 274,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "This paper presents longitudinal data on the acquisition of connected speech processes (CSPs) and the preservation of phonological transfer effects and developmental processes in four Vietnamese speakers acquiring Australian English (Aust. Eng. ). Samples of spontaneous speech ('mini-discourses'), obtained from home interviews over a 2 year period following initial entry to Australia, were transcribed and coded for the presence of phonetic processes, using a custom-designed acoustic editor and annotation program. Keywords: Vietnamese-English; Connected Speech Processes; second language learning\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-273"
  },
  "potapova91_eurospeech": {
   "authors": [
    [
     "Rodmonga K.",
     "Potapova"
    ]
   ],
   "title": "Modification of acoustic features in Russian connected speech",
   "original": "e91_1141",
   "page_count": 3,
   "order": 275,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "This paper describes some of differences between the prosodic characteristics such as the fundamental frequency (F0), intensity (I), duration (t) of isolated words vs. the same words in connected speech (phrases, sentences, texts). Ten native speakers of Russia of Moscow pronouncing orthoepic norm contributed to this study. It was of great interest to study the correlation between the acoustic characteristics of isolated and non-isolated words. The solution of this problem will make it easier to obtain necessary information about the acoustic features of speech flow and to formulate the rules of expert-systems on the domain of automatic speech recognition and speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-274"
  },
  "strik91_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "On the relation between voice source characteristics and prosody",
   "original": "e91_1145",
   "page_count": 4,
   "order": 276,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "The behaviour of the voice source characteristics in connected speech was studied. Voice source parameters were obtained by automatic inverse filtering, followed by automatic fitting of the LF-model to the data. Consistent relations between voice source parameters and prosody were observed. Keywords: inverse filtering; LF-model; voice source\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-275"
  },
  "stensby91_eurospeech": {
   "authors": [
    [
     "Sverre",
     "Stensby"
    ]
   ],
   "title": "Prosody in a rule-based norwegian text-to-speech system",
   "original": "e91_1149",
   "page_count": 4,
   "order": 277,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper describes an algorithm for rule-based prosody in Norwegian. Authentic prosodic patterns are essential in establishing natural sounding synthetic speech. Norwegian is predominantly an intonation language. There is a limited use of tones though. Most Norwegian dialects have two distinguishing tones for words. Many of those words are homonyms. Revealing and realizing of the correct tone and pronunciation among homonyms are especially addressed. The algorithm for rule-based prosody is based on a word lexicon, a grammatical and prosodic parser, and authentic prosodic patterns. The prosodic model is implemented as a preprocessor to a real-time synthesizer on a PC. Keywords: Speech synthesis, Intonation, Word tone, Prosody, Parsing\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-276"
  },
  "madhukumar91_eurospeech": {
   "authors": [
    [
     "A. S.",
     "Madhukumar"
    ],
    [
     "S.",
     "Rajendran"
    ],
    [
     "C. Chandra",
     "Sekhar"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Synthesizing intonation for speech in hindi",
   "original": "e91_1153",
   "page_count": 4,
   "order": 278,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "The present work lays emphasis on acquisition and incorporation of intonational rules in a text-to-speech system for Hindi, an Indian language. The phonetic characteristics of Indian languages are exploited in building the basic system. Intonation refers to the stream of fundamental frequency values which models the timing of glottal pulse source of voiced speech sounds. Properties of intonational patterns such as declination tendency, local fall-rise pattern, resetting of pitch contour and inherent fundamental frequency are discussed. The rules governing these properties are coded into production system format. The synthesized speech becomes more intelligible and natural when these are incorporated into the system. Keywords: text-to-speech system, fundamental frequency (F0), declination tendency, fall-rise pattern, word order, resetting, production system\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-277"
  },
  "hieronymus91_eurospeech": {
   "authors": [
    [
     "James L.",
     "Hieronymus"
    ],
    [
     "Briony J.",
     "Williams"
    ]
   ],
   "title": "An investigation of the relation between perceived pitch accent and automatically-located accent in british English",
   "original": "e91_1157",
   "page_count": 4,
   "order": 279,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "Previous work on the prosodic analysis of British English has used a purely perceptual analysis of the data, the results being stated in terms of structural features of the prosodic unit (eg. length of tone-unit, type of nucleus or head pattern). However, this type of analysis makes no reference to the acoustic exponents of prosodic features, and so does not make it possible to study the physical parameters that are giving rise to a given perceptual category. This means in turn that it is not possible to extract reliable intonation generation and detection information automatically. The work to be reported here begins to remedy the situation by studying the performance of an automatic intonation detection algorithm when compared against perceptually labelled intonation for the same speech data. The acoustic correlates of perceived stress are compared with rather more rigid acoustic measurements used by the algorithm. KEYWORDS: Intonation, Prosody, Sentence Level Stress, Automatic intonation detection\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-278"
  },
  "quazza91_eurospeech": {
   "authors": [
    [
     "S.",
     "Quazza"
    ]
   ],
   "title": "Modelling Italian intonation in a text-to-speech system",
   "original": "e91_1161",
   "page_count": 4,
   "order": 280,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "The paper describes the design of a text-to-speech oriented model of Italian intonation, based on analyses of natural speech and implemented as the pitch assignment module of an actual synthesis system for the Italian language. The model was not intended to be an exhaustive description of natural intonation, its aim being the generation of plausible melodies, appropriate to neutral reading style. The experimental investigations, concerning fundamental frequency and its relations with stress and syntax, were carried out on a subset of a speech database explicitely designed for prosodic analyses. Sentence pitch profiles were 'stylized', following the guidelines of the perceptual approach developed at the Institute of Perception Research of Eindhoven. The standardized pitch movements and their recurring configurations were organized in a set of rules associating f0 variations to text structures. The sketched model was implemented in the Olivetti text-to-speech system, replacing a previous pitch algorithm, and was augmented and refined by direct tuning. The naturalness of synthetic speech resulted considerably improved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-279"
  },
  "omalley91_eurospeech": {
   "authors": [
    [
     "Michael H.",
     "O'Malley"
    ],
    [
     "Howard",
     "Resnick"
    ],
    [
     "Michelle",
     "Caisse"
    ]
   ],
   "title": "An analysis of strategies for finding prosodic clues in text",
   "original": "e91_1165",
   "page_count": 4,
   "order": 281,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "The purpose of this paper is to evaluate the potential benefits of associating certain text phenomena with certain prosodic effects in a text-to-speech (TTS) system. Samples of text from two common TTS applications - interactive electronic messages and expository news articles - were collected. The frequency of these phenomena was measured for each of the two styles of text and a judgement was made as to the appropriateness of the associated prosodic effect. In electronic mail, 7 of the 11 phenomena studied would have produced the appropriate prosody more often than about once per 1000 words of text. In news text, none of the phenomena occurred that often. The effect of implementing the reliable prosody assignment rules would be to improve prosody an average of about once every 100 words for e-mail and once every 880 words for news text. A simple nuclear accent placement rule was also evaluated. Nuclear accent was located incorrectly on 22% of the intonational phrases. Most of these errors were due to compounds rather than to more complex discourse phenomena. Accurate compound and two-word verb detection could improve prosody an average of once every 60 words for both styles of text.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-280"
  },
  "balestri91_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Balestri"
    ]
   ],
   "title": "A coded dictionary for stress assignment rules in Italian",
   "original": "e91_1169",
   "page_count": 3,
   "order": 282,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "This paper describes a method for the automatic location of the lexical stress in an Italian text. The method is based on the observation that in Italian many words ending with the same letters show the same stress position; for example, in all words ending in \"-grafia\" like \"fotografia\" (photograph) the stress falls on the penultimate vowel. We can formulate a rule encompassing the majority of words showing the same stress pattern; words which do not correspond to the pattern being dealt with by an appropriate exception mechanism.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-281"
  },
  "lindert91_eurospeech": {
   "authors": [
    [
     "Enrico te",
     "Lindert"
    ],
    [
     "Hugo C. van",
     "Leeuwen"
    ]
   ],
   "title": "Speech maker: text-to-speech conversion based on a multi-level, synchronized data structure",
   "original": "e91_1231",
   "page_count": 4,
   "order": 283,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "This paper describes the implementation of a text-to-speech system for the Dutch language. Important features by which the system distinguishes itself from most other text-to-speech systems are (a) the central position of a database, (b) the multi-level design of this data structure, and (c) the special form of the rule formalism designed for manipulating the database's contents. The current linguistic analysis modules, and the way they communicate with the database, are described. Examples of the usage of the rule formalism are presented. Finally, the system's interactive usage is briefly touched upon. Keywords: Dutch, grid, rule formalism, Speech Maker, text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-282"
  },
  "lewis91_eurospeech": {
   "authors": [
    [
     "E.",
     "Lewis"
    ],
    [
     "Mark A. A.",
     "Tatham"
    ]
   ],
   "title": "A new text-to-speech synthesis system",
   "original": "e91_1235",
   "page_count": 4,
   "order": 284,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "This paper describes the new SPRUCE (SPeech Response from Unconstrained English) text-to-speech synthesis system being developed at the Universities of Bristol and Essex in the UKThe system exhibits a marked improvement in output naturalness over current systems by incorporating recent fundamental changes in the underlying theory of speech production. Based on a large scale dictionary the system performs a grammatical parse on the input text to produce improved prosodies. The final output is based on stored normalised parametric analyses of natural syllables to produce improved segmental rendering. To date we have been able to demonstrate an early version of the system which shows considerable promise. Keywords: text-to-speech synthesis - naturalness in synthesis - syllables\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-283"
  },
  "oliveira91_eurospeech": {
   "authors": [
    [
     "Luis C.",
     "Oliveira"
    ],
    [
     "M. Ceu",
     "Viana"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ]
   ],
   "title": "DIXI - portuguese text-to-speech system",
   "original": "e91_1239",
   "page_count": 4,
   "order": 285,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "This paper describes the software architecture of the Portuguese text-to-speech system DIXI1. The system has three major modules. The first one contains the text normalizer and searches each word in the lexicon. The second one is a multi-level rule based module for lexical stress assignment, orthographic to phonetic transcription, metrically based prosodic patterning and for generating the evolution of the synthesizer parameters. The final module is the Klatt 80 formant synthesizer. The paper describes each of these main modules, emphasizing the particularities of text-to-speech synthesis in the Portuguese language. Keywords: Speech Synthesis; Text-to-speech Systems; Portuguese Language; Synthesis-by-rule.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-284"
  },
  "hansen91_eurospeech": {
   "authors": [
    [
     "P. Molbaek",
     "Hansen"
    ],
    [
     "N. Reinholt",
     "Petersen"
    ],
    [
     "J.",
     "Rischel"
    ],
    [
     "C.",
     "Henriksen"
    ]
   ],
   "title": "Higher-level linguistic information in a text-to-speech system for danish",
   "original": "e91_1243",
   "page_count": 4,
   "order": 286,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "The main components of the Danish text-to-speech system are presented in outline. Then follows the main theme of our paper, viz. some illustrations of the importance of syntactic information for arriving at an adequate phonological representation of the input, and a description of the most recent version of the formalism we use to express syntactic structuring of Danish sentences in order for a parser to assign suitable morpho-phonemic representations to input sentences. Some recent improvements on the rules for the phonetic behaviour of certain Danish segments and the inclusion of a new type of voice source in the synthesizer program are reported, and finally, some views on the state-of-the-art of text-to-speech research in general are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-285"
  },
  "olaszy91_eurospeech": {
   "authors": [
    [
     "Gabor",
     "Olaszy"
    ]
   ],
   "title": "Adaptation of the multivox text-to-speech system to Italian",
   "original": "e91_1247",
   "page_count": 4,
   "order": 287,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "The MULTIVOX multilingual text-to-speech system was adapted to Italian in 1990-91. The steps and the special features of this adaptation process will be described. In the adaptation the target language (Italian) was developed from the Esperanto version as a source language. Esperanto was chosen to this purpose, because its sound set and the word stress was very close to Italian. Key words: text-to-speech, grapheme-phoneme conversion, acoustic building unit (ABU), phonetic rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-286"
  },
  "niyogi91_eurospeech": {
   "authors": [
    [
     "Partha",
     "Niyogi"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Correlation analysis of vowels and their application to speech recognition",
   "original": "e91_1253",
   "page_count": 4,
   "order": 288,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "This paper describes an approach to exploiting within speaker correlations among different speech sounds for phonetic recognition. By incorporating speaker-specific models and speaker-specific constraints to varying degrees, four different paradigms were suggested. These paradigms were empirically evaluated on the task of classifying eight vowels in American English, using nearly 20,000 vowel tokens excised from the TIMIT corpus: Two speaker classes, representing the male and female speakers, were used. The results suggest that by incorporating gender-specific constraints, one can improve on the performance based on gender-specific models alone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-287"
  },
  "holmes91_eurospeech": {
   "authors": [
    [
     "John N.",
     "Holmes"
    ]
   ],
   "title": "Use of phonetic knowledge when designing and training stochastic models for speech recognition",
   "original": "e91_1257",
   "page_count": 4,
   "order": 289,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "For speech recognition systems that use stochastic models it is usual to choose the model topology independently of the phonetic structure of the recognition units. Such an arrangement can cause difficulties for modelling linguistic units that clearly have alternative forms of phonetic realization. Choosing a general topology which allows for alternatives gives undesirable tolerance for those units where such variations are not expected. This paper presents a design and training technique which selects the topology for each unit to suit its expected phonetic structure. A recognizer using these techniques is described. Keywords: Speech recognition, stochastic models, phonetic knowledge, forward-backward algorithm\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-288"
  },
  "kaspar91_eurospeech": {
   "authors": [
    [
     "B.",
     "Kaspar"
    ],
    [
     "K.",
     "Schuhmacher"
    ]
   ],
   "title": "Modelling phones by microsegments in a phonetically oriented recognition system",
   "original": "e91_1261",
   "page_count": 4,
   "order": 290,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "A tentative system of subphone units, called 'microsegments', is presented. Microsegments are defined by articulatory features and are used to model the temporal structure of phones by simple pronunciation networks. The inventories of both phones and of microsegments are of moderate size but allow the description of common variants of pronunciation as well as the handling of coarticulation effects. The process of labelling speech according to these models requires more effort than labelling phonemes only, but remains reliable. Keywords: Phonetically oriented speech recognition, subword units, articulatory features, temporal modelling\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-289"
  },
  "kim91_eurospeech": {
   "authors": [
    [
     "Il K.",
     "Kim"
    ],
    [
     "H. S.",
     "Lee"
    ]
   ],
   "title": "An extended LVQ2 algorithm and its application to phoneme classification",
   "original": "e91_1265",
   "page_count": 4,
   "order": 291,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "In this paper, we propose a new extension of LVQ2 based on the Kohonen's feature map algorithm. The neurons of the neural network trained by the Kohonen's feature map algorithm are labeled by using the newly proposed selective learning (SL) algorithm which is the first stage of extended LVQ2. Next, LVQ2 is applied to the neural network labeled by the SL algorithm. And then for further training, the weight vectors of the network are perturbed and finally the LVQ2 algorithm is applied to the network again to complete the extended LVQ2 algorithm. As an application of this extension of LVQ2 algorithm, we construct phoneme classifiers using LVQ2 and extended LVQ2 to compare the performances of the two algorithms. From the phoneme classification tests, we obtain the recognition rates of 60. 4% and 65. 4% for the LVQ2-based and extended LVQ2-based systems, respectively. Keywords: Neural Network, Kohonen's Feature Map, LVQ2, Selective Learning, Extended LVQ2, Phoneme Classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-290"
  },
  "dix91_eurospeech": {
   "authors": [
    [
     "P. J.",
     "Dix"
    ],
    [
     "G. J.",
     "Vernooij"
    ],
    [
     "G.",
     "Bloothooft"
    ]
   ],
   "title": "A hierarchical broad phonetic classification scheme",
   "original": "e91_1269",
   "page_count": 4,
   "order": 292,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "In broad phonetic classification of phonemes, a group of phonemes is put together and associated with one single broad phonetic class. As a result, the identity of the phonemes which make up such a class is lost, but in general the capacity to classify such a class will improve. Several simulation studies have been reported in the literature, showing the potential of broad phonetic classification for reduction of possible word candidates. These studies used efficiency of lexical access as the main criterion and paid little attention to the interaction with acoustic information. In this paper we will describe a broad classification experiment based on acoustic information rather than lexical information, and describe a classification hierarchy based on classification scores of broad phonetic classes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-291"
  },
  "hirschberg91_eurospeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Using text analysis to predict intonational boundaries",
   "original": "e91_1275",
   "page_count": 4,
   "order": 293,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "Relating the intonational characteristics of an utterance to features inferable from its orthographic transcription is important both for speech recognition and for speech synthesis. Results are presented for predicting the location of intonational phrase boundaries in a corpus of spontaneous (elicited) speech from syntactic, temporal and other features inferred from simple text analysis of its transcription. Classification and Regression Tree (CART) techniques are employed to model the relationship between hand-labeled boundary phenomena and textual features. Results from an additional experiment using these prediction trees to distinguish correct strings from those incorrectly recognized by a speech recognizer are also reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-292"
  },
  "horne91_eurospeech": {
   "authors": [
    [
     "Merle",
     "Horne"
    ]
   ],
   "title": "Why do speakers accent 'given' information ?",
   "original": "e91_1279",
   "page_count": 4,
   "order": 294,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "The accenting of contextually 'given' information constitutes a problem for analyses that regard accents as correlating only with 'new' information. It will be shown that the accenting of 'given' information is explainable as resulting from general metrical well-formedness conditions on prosodic constituents. Units higher than the word are seen to obey the same metrical constraints that are present at the word level. Key Words: 'new information', 'given information', accentual phrase, intermediate phrase, tone-unit, prosodic structure, metrical structure, deaccenting, rhythm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-293"
  },
  "vonwiller91_eurospeech": {
   "authors": [
    [
     "Julie P.",
     "Vonwiller"
    ],
    [
     "R. W.",
     "King"
    ],
    [
     "R. W. T.",
     "Lloyd"
    ]
   ],
   "title": "Automatic prosody assignment for interactive synthesized dialogue systems",
   "original": "e91_1283",
   "page_count": 4,
   "order": 295,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "Correct assignment of prosodic data in synthesized speech has considerable importance to its intelligibility and acceptance. Accurate prosody is particularly significant for interactive speech response systems, where the synthesized speech responses form part of the inquirer-system dialogue, and it is known that specific prosodic patterns carry interactional meanings. These patterns have been described for natural speech in the intonation component of Halliday's systemic functional grammar model of language. This paper describes an initial investigation into the application of Halliday's model to compute the tone group pitch contour component within a text-to-speech synthesis system from a minimal specification. Pitch contours for fifteen interactional meanings are described, and examples of their automated computation are given. The paper discusses how the system might be incorporated within an automated speech response system. Keywords: speech synthesis, prosody, intonation, speech response systems\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-294"
  },
  "nickyoud91_eurospeech": {
   "authors": [
    [
     "NickYoud",
     "NickYoud"
    ],
    [
     "Jill",
     "House"
    ]
   ],
   "title": "Generating intonation in a voice dialogue system",
   "original": "e91_1287",
   "page_count": 4,
   "order": 296,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "This paper describes the voice output subsystem of a speech dialogue system, as this relates to prosody generation, and in particular, prosodic marking of focus. We demonstrate how focal domain values are assigned within the text generation component at the intentional, attentional and surface-linguistic levels, and how realisation of the pitch contour is achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-295"
  },
  "delmonte91_eurospeech": {
   "authors": [
    [
     "Rodolfo",
     "Delmonte"
    ],
    [
     "Roberto",
     "Dolci"
    ]
   ],
   "title": "Computing linguistic knowledge for text-to-speech systems with PROSO",
   "original": "e91_1291",
   "page_count": 4,
   "order": 297,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "In this paper we present a computer program PROSO a linguistic rule compiler which applies a number of prosodic rules inserting appropriate markers to any text inputted, on the basis of phonological rules and of syntactic structure, this one computed separately by an RTN parser(see Delmonte & Dolci, 1989) and made visible to PROSO only at a certain structural level, though. The output of this program can then be passed on to a system like VOXPC, the text-to-speech module commercialized by Olivetti, which allows the user to provide explicit phonetic and prosodic information in order to modify the internal coding procedures. VOXPC is a system completely lacking in linguistic knowledge apart from the well-known distinction between content and function words: the quality of the system is very poor both at word level and at sentence and text level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-296"
  },
  "acker91_eurospeech": {
   "authors": [
    [
     "C.",
     "Acker"
    ],
    [
     "Peter",
     "Vary"
    ],
    [
     "H.",
     "Ostendarp"
    ]
   ],
   "title": "Acoustic echo cancellation using prediction residual signals",
   "original": "e91_1297",
   "page_count": 4,
   "order": 298,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "Adaptive echo cancellers are currently being studied for application to hands-free telephone sets with high speech quality. The major problem is the acoustic echo control from the loudspeaker to the microphone. This proposal presents a new concept by using linear prediction techniques in combination with a conventional adaptive echo canceller. Computer simulations show that the introduced algorithm improves significantly the performance compared to conventional methods. In this paper the new structure is described. A comparison of the results with the state of the art indicates that the complexity can be reduced by one third.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-297"
  },
  "dabis91_eurospeech": {
   "authors": [
    [
     "H. S.",
     "Dabis"
    ],
    [
     "Alan A.",
     "Wrench"
    ]
   ],
   "title": "An evaluation of adaptive noise cancelling for speech recognition",
   "original": "e91_1301",
   "page_count": 4,
   "order": 299,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "Speech recognition systems suffer greatly from the presence of additive noise. In this paper a two microphone approach is used as a front end processor to enhance speech corrupted by acoustically added noise signals. Extensive testing of the effect of the algorithm on a continuous speech recognition system was made and the results presented. The results show a reasonable SNR improvement of around 5 dB and an improvement in the recognition rate of the enhanced signal although the performance is not as good as that of the original clean signal. Keywords: Adaptive noise cancelling, continuous speech recognition, Adaptive filtering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-298"
  },
  "mumolo91_eurospeech": {
   "authors": [
    [
     "Enzo",
     "Mumolo"
    ],
    [
     "Antonello",
     "Riccio"
    ],
    [
     "Giuseppe",
     "Abbattista"
    ]
   ],
   "title": "An efficient algorithm for real-time voiced/unvoiced decision",
   "original": "e91_1305",
   "page_count": 4,
   "order": 300,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "An efficient and reliable algorithm for voiced/unvoiced discrimination is presented in this paper. Voicing determination is of great importance in many fields of speech processing, ranging from coding to recognition. In this work, the log value of the ratio between low and high frequency energies has been taken as basic parameter for the discrimination. Moreover differential parameters, computed as the difference of the log-energy ratios of adjacent frames, have been used. The algorithm is quite simple because it requires only two filters, two integrators and some logic for obtaining the final decision. The algorithm has been implemented in real-time on a TMS320C25 and it is shown that it requires only the 11% of the available time. Some classification examples are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-299"
  },
  "aarset91_eurospeech": {
   "authors": [
    [
     "Tim",
     "Aarset"
    ],
    [
     "Ben",
     "Gold"
    ]
   ],
   "title": "Models of pitch perception",
   "original": "e91_1309",
   "page_count": 4,
   "order": 301,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "Two pitch perception modelling algorithms are described. The first algorithm models \"periodicity\" pitch perception and the second algorithm models \"place\" pitch perception. The two models are now applied to various psychoacoustic stimuli. Both \"periodicity\" and \"place\" models yield results that are in general agreement with psychoacoustic measurements for the missing fundamental and for inharmonic stimuli. The \"place\" algorithm proved to be a better approximation than \"periodicity\" for processing comb-filtered noise. \"Periodicity\" was more successful for periodic pulse train stimuli.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-300"
  },
  "corney91_eurospeech": {
   "authors": [
    [
     "P.",
     "Corney"
    ],
    [
     "J. S.",
     "Mason"
    ]
   ],
   "title": "A new perspective on LPC excitation using singular value decomposition",
   "original": "e91_1315",
   "page_count": 4,
   "order": 302,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "The lack of an efficient technique to encode the LPC excitation functioii impedes the development of high quality speech codecs at very low bit rates, below 4 kbits. In this work we examine the use of singular value decomposition (SVD) to define an orthogonal data-dependent transformation domain into which the associated time domain LPC residual signal is mapped. We seek to understand and evaluate this approach. The main features of the method, detailed by Atal [1], concern the representation of the LPC excitation function as a linear combination of an orthogonal set of signals, which are the eigenvectors of the LPC filter impulse response matrix. We consider SVD-based CELP coders and the suitability of the basefunctions to describe both the filter and the excitation signal information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-301"
  },
  "verhelst91_eurospeech": {
   "authors": [
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Marcel",
     "Borger"
    ]
   ],
   "title": "Intra-speaker transplantation of speech characteristics an application of waveform vocoding techniques and DTW",
   "original": "e91_1319",
   "page_count": 4,
   "order": 303,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "In this paper, we present a transplantation system that can be used to interchange selected prosodic features among repetitions of the same utterance by the same speaker. The system is based on a 'waveform vocoder' that allows high quality prosodic modifications of speech, and uses DTW for proper time alignment of transplanted features. The quality of the transplantation system is evaluated in relation to the underlying waveform vocoding model. Applications to the study of a number of important problems in text-to-speech synthesis and speech perception are suggested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-302"
  },
  "leung91c_eurospeech": {
   "authors": [
    [
     "S. H.",
     "Leung"
    ],
    [
     "O. Y.",
     "Wong"
    ],
    [
     "K. L.",
     "Lai"
    ]
   ],
   "title": "Decomposition of the LPC excitation using wavelet functions",
   "original": "e91_1327",
   "page_count": 4,
   "order": 304,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "This paper proposes a new linear predictive coding (LPC) based on the wavelet functions. It has been shown in the literature that the wavelet functions form a complete set of orthonormal basis to decompose the time signals. One of the main advantages of using the wavelet functions is their optimal time-frequency distribution which provides good resolution in analyzing the excitations. In the excitation model, the excitation signal comprises of a set of compound pulses. Each compound pulse contains L equally-spaced pulses, in which the location of the compound pulse and the amplitudes of the pulse elements are determined by analysis-by-synthesis technique to minimize the error between the original speech and synthetic speech. A real speech is used to evaluate the performance of the new model. The parameters of the model are chosen to limit the coding rate to 8 kbps. The new scheme is proved to provide a class of excitations having better performance than multipulse LPC model presented in [1].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-303"
  },
  "ambikairajah91_eurospeech": {
   "authors": [
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Liam",
     "Kilmartin"
    ]
   ],
   "title": "An adaptive cochlear model for speech recognition",
   "original": "e91_1331",
   "page_count": 4,
   "order": 305,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper describes an adaptive cochlear model, in which the basilar membrane is modelled as a cascade of 128 digital filters, covering the frequency band from 70 Hz to 3. 4 kHz. The output of the inner hair cell of each filter is used to vary the coefficients of that filter so that its Q-factor is modified. Thus, for a low-amplitude stimulus, the Q-factor is increased, while it is decreased for a high-amplitude stimulus. This modification takes place at a rate which simulates the continuous adaptation of the basilar membrane. Results are presented for sinusoidal stimuli of different amplitudes, and also for speech input signals. Keywords: auditory modelling, speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-304"
  },
  "jacovitti91_eurospeech": {
   "authors": [
    [
     "Gianni",
     "Jacovitti"
    ],
    [
     "Piero",
     "Pierucci"
    ],
    [
     "Alessandro",
     "Falaschi"
    ]
   ],
   "title": "Speech segmentation and classification using higher order moments",
   "original": "e91_1335",
   "page_count": 4,
   "order": 306,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "A new criterion for speech segmentation based on higher order moments is presented. It is shown that simple estimates of third and fourth order cumulants can provide useful cues for speech segmentation and classification. In fact, the analysis of the cumulants behaviour seems well suited for discriminating voiced, unvoiced and transient sounds. Moreover, the estimated quantities give some indications about the information content of segments of speech. This permit the experimentation of a coding scheme for the speech signal, which has yet been experimented in image coding, founded on a perceptually - based conjecture. Keywords: Higher Order Moments; Speech Segmentation; Speech Classification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-305"
  },
  "ciaramella91b_eurospeech": {
   "authors": [
    [
     "A.",
     "Ciaramella"
    ],
    [
     "D.",
     "Clementino"
    ],
    [
     "R.",
     "Pacifici"
    ]
   ],
   "title": "A PC-housed speaker independent large vocabulary continuous telephonic speech recognizer",
   "original": "e91_1341",
   "page_count": 4,
   "order": 307,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "We implemented a PC housed continuous speech acoustical front-end (AFE) for a vocabulary of a thousand words, incorporated into an overall man-machine dialogue system for data base access using telephonic speech. The AFE can be instructed to perform both isolated word and continuous speech recognition; in the latter case the recognition is done in two stages, the first being an evaluation of a lattice of words and of the best scored word sequence, and the second a verification of the most acoustically likely sentences chosen from amongst those which are syntactically and semantically correct. For real-time interactive behaviour, the PC is equipped with 1 DSP board for features extraction and 3 DSP boards with the corresponding memory extension for recognition. Appropriate task synchronization and partition between these boards and the PC program provide real-time performance and a satisfactory degree of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-306"
  },
  "aktas91_eurospeech": {
   "authors": [
    [
     "Abdulmesih",
     "Aktas"
    ],
    [
     "Klaus",
     "Zünkler"
    ]
   ],
   "title": "Speaker independent continuous HMM-based recognition of isolated words on a real-time multi-DSP system",
   "original": "e91_1345",
   "page_count": 4,
   "order": 308,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper describes a speaker independent continuous Hidden Markov Model recognizer implemented on a real-time multi-DSP system. Training and recognition are based on continuous mixture density HMMs for phonemes. Context dependent triphone models are used and the Viterbi algorithm is applied for both training and recognition. The system is implemented on a workstation with an integrated multi-DSP based acoustic front-end employing three Texas Instruments TMS320C25 signal processors and a Siemens ASIC for vector quantization. In spite of the simplifications made in order to reduce the high computational requirements for the continuous mixture densities, the system has a recognition rate of 99. 5% for the speaker independent German digit task with telephone quality. Keywords: Speech Recognition, Hidden Markov Model, mixture density, speaker independent, real-time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-307"
  },
  "tsopanoglou91_eurospeech": {
   "authors": [
    [
     "A.",
     "Tsopanoglou"
    ],
    [
     "E. D.",
     "Kyriakis-Bitzaros"
    ],
    [
     "J.",
     "Mourjopoulos"
    ],
    [
     "George K.",
     "Kokkinakis"
    ]
   ],
   "title": "A real time speech decoder using instantaneous frequency and energy",
   "original": "e91_1349",
   "page_count": 4,
   "order": 309,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "A real-time speech Acoustic-to-Phonetic Decoder (APD) is presented, suitable for efficient segmentation of speech into phonemes and classification of these phonemes into broad phonetic categories. The system can be implemented in DSP modules (e. g. TMS32025), mainly because the analysis parameters are estimated in the time domain. These parameters are the Envelope Function and the Instantaneous Frequency of the signal evaluated efficiently via digital filters. The system can achieve satisfactory performance, which is both speaker and language independent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-308"
  },
  "schulthei91b_eurospeech": {
   "authors": [
    [
     "M.",
     "Schultheiß"
    ],
    [
     "A.",
     "Lacroix"
    ]
   ],
   "title": "Fast hardware for efficient parallel processing of speech signals",
   "original": "e91_1353",
   "page_count": 4,
   "order": 310,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "A multiprocessor architecture for high complexity speech processing is described that uses groups of 4 DSP chips each in a hierarchical, reconfigurable structure, yielding some 100 MFlops per board. This architecture combines low amount of hardware with high efficiency for many speech applications. The software environment is based on available development tools and can easily be adapted to future DSP components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-309"
  },
  "sedivy91_eurospeech": {
   "authors": [
    [
     "Jan",
     "Sedivy"
    ],
    [
     "Jiff",
     "Filcev"
    ],
    [
     "Jan",
     "Uhlir"
    ],
    [
     "Tomas",
     "Vanek"
    ],
    [
     "Vaclav",
     "Hanzl"
    ],
    [
     "Zdenek",
     "Oliva"
    ],
    [
     "Petr",
     "Kotek"
    ]
   ],
   "title": "The one chip speech recognition system",
   "original": "e91_1357",
   "page_count": 5,
   "order": 311,
   "p1": "1357",
   "pn": "1631",
   "abstract": [
    "This paper summarises the work which was carried out in the department of circuit theory in the Czech Technical University, Prague. We describe a project of an one chip automatic speech recognition system built around the TMS320E17. Our aim is to design a simple and cheap speech recognition system and acquire necessary knowledge for more sophisticated systems. We discuss the algorithm used and hardware implementation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-310"
  },
  "villarrubia91_eurospeech": {
   "authors": [
    [
     "L.",
     "Villarrubia"
    ],
    [
     "M. J.",
     "Poza"
    ],
    [
     "C.",
     "Crespo"
    ]
   ],
   "title": "Influence of the telephone line on automatic speech recognition",
   "original": "e91_1363",
   "page_count": 4,
   "order": 312,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "As it is well known, the successful deployment of large scale speaker independent word recognition systems for use over the telephone network can generate great cost savings, besides new revenue sources. Telefonica I+D is very interested in the development of new services that use vocal access and response. This is the reason why we have been involved in an extensive study of different structures of isolated word recognizers working with 'clean' and 'telephone quality speech'. To carry out these studies we have collected two databases: a clean database ,using a microphone and a telephone database; both databases consist of the same utterances and speakers, but recorded by two different methods. The results we are going to point out have been extracted in several tests using these two collections of files and some 'quality modified'speech files. Keywords: Hidden Markov Models, 'telephone-quality database','clean database'.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-311"
  },
  "hermansky91b_eurospeech": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Aruna",
     "Bayya"
    ],
    [
     "Phil",
     "Kohn"
    ]
   ],
   "title": "Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)",
   "original": "e91_1367",
   "page_count": 4,
   "order": 313,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "Human perception is generally insensitive to steady-state stimuli, responding largely to relative values of the input stimulus. In spite of that, most speech parameter estimation techniques use absolute spectral values. Consequently, they fail when the spectral values are modified by the frequency response of the communication channel. In response to this challenge, we have developed an analysis technique that is more robust to steady-state factors in speech. The approach appears to work well with realistic speech and channel deformations, and is conceptually simple and computationally efficient. We have conducted an experiment on recognition of telephone-quality digits with a distorted channel. The new method is described, and experimental results for realistic channel distortion are reported, showing order-of-magnitude improvements in error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-312"
  },
  "junqua91b_eurospeech": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Ben",
     "Reaves"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "A study of endpoint detection algorithms in adverse conditions: incidence on a DTW and HMM recognizer",
   "original": "e91_1371",
   "page_count": 4,
   "order": 314,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "In this paper the performances of three recently developed endpoint algorithms are evaluated and compared to the Lamel and Rosenberg's algorithm [1] based on energy levels and timing, which is enhanced by automatic threshold setting. Their performances are reported when integrated with two commonly used speech recognizers (discrete density vector quantization-based hidden Markov model (VQ-based HMM) and dynamic time warping (DTW)) in various types of noisy conditions. Accuracy was judged by agreement with hand-labeled endpoints, and by recognition rates. Results show that 1) a new noise adaptive algorithm using rms energy, zero-crossing rate, and a set of heuristics gives generally the best results at high or medium signal-to-noise ratio (>15 dB). 2) The HMM recognizer when used with this algorithm performs as well as if the endpoints were hand-labeled for clean Lombard speech; for noisy Lombard speech, depending on the type of noise used and the SNR, there is a degradation from 1% to 43% in recognition accuracy compared to manual labeling. 3) At low SNR, the algorithm based on Lamel and Rosenberg's method [1] and enhanced by automatic threshold setting gives generally better performance than the other algorithms. Keywords: Endpoint detection, voice activation, adaptive algorithm, Lombard-noisy speech, robustness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-313"
  },
  "dvorak91_eurospeech": {
   "authors": [
    [
     "Susanne",
     "Dvorak"
    ],
    [
     "Thomas",
     "Hormann"
    ]
   ],
   "title": "High-performance speech recognition in noise by continuously updated reference templates",
   "original": "e91_1375",
   "page_count": 4,
   "order": 315,
   "p1": "1375",
   "pn": "1378",
   "abstract": [
    "Although reaching results of almost 100% recognition rate in quiet surrounding, the performance of speech recognizers decreases very fast in a noisy environment. Various algorithms are available for noise reduction and they are working well in reducing moderate and stationary ambient noise. The idea presented here is to continuously adapt templates to different situations and noise scenarios by updating original reference patterns after every successful recognition. A new reference template is calculated from the weighted sum of the recognized reference template and the actual test pattern. In this way, it is possible to continuously incorporate information about different variations of noise and Lombard speech in the reference templates, thus improving recognition performance in changing situations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-314"
  },
  "vicsi91_eurospeech": {
   "authors": [
    [
     "Klara",
     "Vicsi"
    ]
   ],
   "title": "Speech enhancement in the case of speech recognizers",
   "original": "e91_1379",
   "page_count": 3,
   "order": 316,
   "p1": "1379",
   "pn": "1381",
   "abstract": [
    "The efficiency of speech recognizers depends in a great extent en the background noise. In this paper 3 very simple, but useful methods are presented for decreasing the effects of the background noise: 1 - correlated averaging technique using 2 or more pressure microphones 2 - speaker adaptiv filtering, using one preasure gradient Microphone, 3 - speech signal separation from the disturbing impulse-noise, using one pressure and one pressure-gradient microphones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-315"
  },
  "gomezmena91_eurospeech": {
   "authors": [
    [
     "Juan",
     "Gomez-Mena"
    ],
    [
     "J.",
     "Santos-Suarez"
    ],
    [
     "Ramón",
     "Garcia-Gomez"
    ]
   ],
   "title": "A robust feature extraction method for automatic speech recognition in noisy environments",
   "original": "e91_1383",
   "page_count": 4,
   "order": 317,
   "p1": "1383",
   "pn": "1386",
   "abstract": [
    "We are going to refer to the problem of speech recognition under noisy conditions, with medium size vocabularies of isolated words. We describe a method for feature extraction and the scores obtained with a HMM based recognizer. This method uses a combination of others two methods: the Short-time Modified Coherence and the spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-316"
  },
  "fissore91_eurospeech": {
   "authors": [
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "Egidio P.",
     "Giachin"
    ],
    [
     "P.",
     "Laface"
    ],
    [
     "G.",
     "Micca"
    ]
   ],
   "title": "Selection of speech units for a speaker-independent CSR task",
   "original": "e91_1389",
   "page_count": 4,
   "order": 318,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "This paper focuses on the problem of finding a set of Hidden Markov Models that can be trained to model context dependencies with good statistical accuracy, given the constraint of a fixed amount of training data. Two aspects have been investigated in this work: clustering of intra-word context-dependent units with similar contexts on the basis of different similarity measures, and definition of inter-word coarticulation units. A Dynamic Programming procedure is presented that allows a large set of context-dependent units to be clustered into a given number of units while optimizing a global cost measure. Inter-word units were found to provide better phonetic representations of word junctures and to increase recognition accuracy, though less than it has been reported for the English language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-317"
  },
  "giachin91b_eurospeech": {
   "authors": [
    [
     "Egidio P.",
     "Giachin"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ],
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Word juncture modeling using inter-word context-dependent phone-like units",
   "original": "e91_1393",
   "page_count": 4,
   "order": 319,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "We report on a speech recognition system that uses inter-word context dependent units (CDP) to model pronunciation variations at word boundaries. Word juncture coarticulation phenomena are a major source of acoustic variability for the initial and final parts of a word when spoken in fluent speech. In some cases, the alteration of a phone due to neighboring phones is comparatively small and the actual realization is perceived as a variation of the original phone. To cope with this type of variations, we design a set of inter-word CDP and modify the pronunciation of a word by replacing the word beginning and word ending phones with all possible inter-word units. By properly connecting all possible word beginnings and word endings, word boundaries are thereby better represented. Taking into account word juncture pronunciation changes, better models can be obtained in training. Such models have achieve better results in recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-318"
  },
  "nagai91_eurospeech": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Kenji",
     "Kita"
    ]
   ],
   "title": "Phoneme-context-dependent LR parsing algorithms for HMM-based continuous speech recognition",
   "original": "e91_1397",
   "page_count": 4,
   "order": 320,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "This paper discusses two approaches for combining an efficient LR parser and phoneme-context-dependent HMM phone models and compares them through continuous speech recognition experiments. LR parsing is one of the most efficient parsing algorithms for speech recognition under grammatical constraints based on a context free grammar (CFG). To recognize continuous speech, it is advantageous to use allophone models for improving speech recognition accuracy, because they precisely represent allophonic variation caused by phoneme context. This paper aims to combine accurate allophonic models with the efficient LR parser to construct a powerful scheme for continuous speech recognition. In this paper, two phoneme-context-dependent LR parsing algorithms are proposed, which make it possible to drive allophonic HMMs. The algorithms are outlined as follows: (1) Algorithm for generating a phoneme-context-dependent LR parsing table using a CFG. (2) Algorithm for predicting the phoneme context dynamically in the LR parser by using a phoneme-context-independent LR table. This paper also includes discussion of the results of recognition experiments, and a comparison of performance and efficiency between these two algorithms. Keywords: continuous speech recognition, LR parser, phoneme environment, allophone, Hidden Markov Models, HMM-LR\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-319"
  },
  "drexler91_eurospeech": {
   "authors": [
    [
     "H.",
     "Drexler"
    ],
    [
     "R.",
     "Roddeman"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Optimizing lexical fast search in a large vocabulary isolated word speech recognition system",
   "original": "e91_1401",
   "page_count": 4,
   "order": 321,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "This paper describes methods developed to improve the performance of a preselection algorithm in a large vocabulary isolated word recognizer. First we investigate ways for optimizing the phonetic representations of the words in the lexicon (the base forms of which are obtained from a grapheme-to-phoneme converter) in such a way that the inherent limitations of the front-end are taken into account and where possible are remedied. Then an attempt is made to improve the quality of the symbol strings produced by the acoustic front-end. keywords: Isolated-Word-Recognition preselection lexicon\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-320"
  },
  "fjallbrant91_eurospeech": {
   "authors": [
    [
     "Tore",
     "Fjällbrant"
    ],
    [
     "Fisseha",
     "Mekuria"
    ]
   ],
   "title": "Signal processing using an auditory filter bank with side-lobes and phase-jumps",
   "original": "e91_1429",
   "page_count": 3,
   "order": 322,
   "p1": "1429",
   "pn": "1431",
   "abstract": [
    "An auditory model is described which makes use of a filter bank with filters having a deep notch on the high frequency side with an associated 180 degree jump in phase, analogous to those previously observed in the responses of single hair cells at lower frequencies. The model is being simulated on a DSP32C processor equipped workstation using DSPlay flowdiagram system design software. It is shown how robust frequency analysis can be carried out with this model, which is also being used for feature extraction experiments. Keywords: Auditory model, Filter bank, Frequency analysis, feature extraction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-321"
  },
  "dijk91_eurospeech": {
   "authors": [
    [
     "J. S. C. van",
     "Dijk"
    ]
   ],
   "title": "Notes on auditive coding of sophisticated signals",
   "original": "e91_1433",
   "page_count": 4,
   "order": 323,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "It is well-known that in sophisticated signals the nature of perceptually relevant features changes considerably. For certain parts of the signal, it is useful to characterize features in terms of spectral properties. For other parts a spectral description is rather poorly , just because the leading cue is a specific temporal structure. If the signal is of long duration, just as happens in speech and music, it is self-evident to use a filter bank and to code the output conform perceptually relevant cues. However, the human auditory system includes a highly specialized filter bank. For, the basilar membrane actually works as a system of coupled filters. Consequently, it is natural to apply this kind of systems in the science of speech and music. In this work we describe a filter bank which simulates basilar membrane behaviour. Because the system is a real time realization of what happens at the level of the basilar membrane, both spectral and temporal properties are found. Some preliminary results are given. Keywords: Filter bank; Dynamical system; Spectro-temporal codes\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-322"
  },
  "beham91_eurospeech": {
   "authors": [
    [
     "Manfred",
     "Beham"
    ]
   ],
   "title": "An auditorily based spectral transformation of speech signals",
   "original": "e91_1437",
   "page_count": 4,
   "order": 324,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "This paper describes a special perceptually based spectral transformation which takes into account basic properties of the auditory system. The main feature of this spectral transformation which has been especially designed as preprocessing stage for an automatic speech recognition system is the possibility of simple recursive calculation. Aurally adequate spectrum analysis is performed by a modified Fourier-t-Transformation which allows the adjustment of time and frequency resolution to the frequency dependency of the human ear. Psychoacoustic data is the basis for these resolution capabilities. Keywords: Speech Signal Preprocessing, Auditorily Based Spectral Transformation, Fourier-t-Transformation, auditory model\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-323"
  },
  "morris91_eurospeech": {
   "authors": [
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "Pierre",
     "Escudier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "On and off units detect information bottle-necks for speech recognition",
   "original": "e91_1441",
   "page_count": 4,
   "order": 325,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "We show how the objective measure of Mutual Information (MI) can be used to confirm that information for identifying Place of Articulation (PoA) for plosives in vowel-plosive-vowel (VPV) context is concentrated at both ON (burst onset) and OFF (voicing termination) events in the acoustic spectrogram and is predominantly dynamic rather than static. We then run recognition tests to show that single-speaker plosive PoA in VPV context can be reliably identified from just one pair of short-term spectra centred at either ON or OFF position. Keywords: mutual information; phoneme recognition; data compression\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-324"
  },
  "pozasalvarez91_eurospeech": {
   "authors": [
    [
     "Jose A.",
     "Pozas-Alvarez"
    ]
   ],
   "title": "A new logic operator-based auditory system model",
   "original": "e91_1445",
   "page_count": 4,
   "order": 326,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "This paper relates to the establishment of a model of the Auditory System (AS). The present model is based on a Logic Cancellation and Equalization Process, applied to the firing rate of Auditory Nerve Endings. The output of the process is a set of Frequency Selective Neurons, which support the coding of the sound signal. The model establishes the coding of loudness, tonality and delay of sound signals in the AS. The paper analyzes the frequency and time responses and linearity of the process. The paper also justifies the role of the transduction process in the coding process and masking in the AS. Keywords: Logic Cancellation and Equalization Process; Frequency Selective Neurons; Auditory Nerve Ending; Auditory System.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-325"
  },
  "peckham91_eurospeech": {
   "authors": [
    [
     "Jeremy",
     "Peckham"
    ]
   ],
   "title": "Speech understanding and dialogue over the telephone: an overview of progress in the sundial project",
   "original": "e91_1469",
   "page_count": 4,
   "order": 327,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "This paper describes an early prototype of a telephone based oral dialogue system developed within the ESPRIT SUNDIAL project. Prototypes have been implemented in four languages for two major application areas, flight reservations and enquiries and train timetable enquiries. The paper discusses the overall system architecture adopted and the approaches taken within each of the major modules of the system. Some preliminary results are shown for parts of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-326"
  },
  "tubach91_eurospeech": {
   "authors": [
    [
     "Jean-Pierre",
     "Tubach"
    ],
    [
     "P.",
     "Doignon"
    ]
   ],
   "title": "A system for natural spoken language queries design, implementation and assessment",
   "original": "e91_1473",
   "page_count": 4,
   "order": 328,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "This paper describes a dialog system allowing a \"naive\" user to retrieve information from a knowledge base about historical facts, using natural, spontaneous questions in French.\n",
    "Keyboard input, continuous speech input (through connected word recognition) and isolated word input are provided, as three separate modes. The system is implemented on a VECSYS Datavox speech recognizer, and a PC, programmed in Prolog and C.\n",
    "Careful assessment sessions were conducted, mainly for continuous speech input. Observations of speakers behaviour when recognition errors occur provide guidelines for implementing better dialog systems in the future.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-327"
  },
  "deville91_eurospeech": {
   "authors": [
    [
     "G.",
     "Deville"
    ],
    [
     "P.",
     "Mousel"
    ]
   ],
   "title": "Operational validation of syntactic-semantic models in a spoken man-machine dialogue system",
   "original": "e91_1477",
   "page_count": 4,
   "order": 329,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "This paper presents an initial evaluation of the syntactic-semantic component within a natural language spoken manmachine dialogue system, the SYNSEM component. After giving a brief account of the DIAL dialogue system, we present the validation methodology that aims at refining SYNSEM's underlying natural language models, and tuning the component's heuristic search strategies. Next we discuss the results of SYNSEM's validation obtained through several parses of a representative sample of utterances using various search strategies. Keywords: Natural Language Processing; Speech Processing; Syntactic Parsing; Assessment and Validation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-328"
  },
  "gaiffe91_eurospeech": {
   "authors": [
    [
     "B.",
     "Gaiffe"
    ],
    [
     "L.",
     "Romary"
    ],
    [
     "Jean-Marie",
     "Pierrel"
    ]
   ],
   "title": "References in a multimodal dialogue: towards a unified processing",
   "original": "e91_1481",
   "page_count": 5,
   "order": 330,
   "p1": "1481",
   "pn": "1485",
   "abstract": [
    "In this paper we tackle the problem of understanding references expressed through several different modes in a multimodal. dialogue system. We focus our attention on the two Natural Language and designation (by means of a mouse), for which we show that there exists two main strands of answers to this problem: the referential and the syntactical strand. We illustrate the first one from our experience within the Multiworks project and we show that both present some major drawbacks that they alone cannot solve. Moreover, since multimodal systems are usually based on NL, we show that the designation mode can also present some interesting properties which give it an important power of expression. This leads us to the conclusion that a multimodal dialogue system should be based on a unified architecture hinging on a multifold focus mechanism. Keywords: Multimodal dialogue, Natural Language, Focus, Reference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-329"
  },
  "lefebvre91_eurospeech": {
   "authors": [
    [
     "P.",
     "Lefebvre"
    ],
    [
     "G.",
     "Duncan"
    ],
    [
     "F.",
     "Poirier"
    ]
   ],
   "title": "The user-unix dialogue: a novel integrated approach to enhancing the operating system interface",
   "original": "e91_1487",
   "page_count": 4,
   "order": 331,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "Man-machine dialogue with the Unix operating system presents difficulties for occasional, non-expert system users. This paper presents a novel approach to enhancing the User-Unix interface through multi-modal dialogue techniques. In particular, this paper explores the benefits, nature and treatment of system commands which integrate both graphics- and voice-based information pathways. Keywords: Spoken Dialogue, Natural Language Processing, Man-machine Interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-330"
  },
  "arndt91_eurospeech": {
   "authors": [
    [
     "Bodo",
     "Arndt"
    ]
   ],
   "title": "Adoption op verbal and visual dialogue behaviour in document handling systems",
   "original": "e91_1491",
   "page_count": 4,
   "order": 332,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "The domain of this presentation is the dialogue behaviour of a multiprocess graphical system extented by speech input and output facilities. This work is done within the ESPRIT II project 2094 (SUNSTAR). The processes regarded are the services (applications) of a document handling system which produce spoken messages by external and internal events and can react on spoken commands. Keywords: Dialogue behaviour, timers, integrated graphical-speech interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-331"
  },
  "smeele91_eurospeech": {
   "authors": [
    [
     "P. M. T.",
     "Smeele"
    ],
    [
     "A. C.",
     "Sittig"
    ]
   ],
   "title": "The contribution of vision to speech perception",
   "original": "e91_1495",
   "page_count": 3,
   "order": 333,
   "p1": "1495",
   "pn": "1497",
   "abstract": [
    "Earlier research has shown that vision can improve the perception of spoken language. The experiment described in this paper was conducted to investigate the nature of information transmitted by eye and ear. We analyzed the data in terms of phonetic distinctive features and the role of the different modalities. A computation method is used to determine relative information transmission values for each phonetic feature. For consonants it appears that vision plays the dominant role in transmitting place of articulation, whereas affrication, nasality, stop and voicing can be best transmitted by ear. For vowels visual dominance is not clear, the auditory channel seems to transmit each feature in a better way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-332"
  },
  "lickley91_eurospeech": {
   "authors": [
    [
     "R. J.",
     "Lickley"
    ],
    [
     "R. C.",
     "Shillcock"
    ],
    [
     "E. G.",
     "Bard"
    ]
   ],
   "title": "Processing disfluent speech: how and when are disfluencies found?",
   "original": "e91_1499",
   "page_count": 4,
   "order": 334,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "Disfluency in spontaneous speech presents problems for both psycholinguistic and computational models of speech understanding. However, it is not clear that the human speech processing mechanism is greatly disrupted by the presence of disfluency. This paper presents the results of three experiments on the perception of spontaneous speech: two gating experiments show that disfluency can usually be recognised by the end of the word following a disfluent interruption, while listeners' ability to recognize words is not greatly affected by the presence of disfluency; the third experiment, using low-pass filtered speech, suggests that prosodic information may have a key role in aiding the processing of disfluent speech. Keywords: speech perception; word recognition; spontaneous speech; disfluency; gating experiments; low-pass filtered speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-333"
  },
  "chointere91_eurospeech": {
   "authors": [
    [
     "A.",
     "Chointere"
    ],
    [
     "J.-M.",
     "Robert"
    ],
    [
     "Raymond",
     "Descout"
    ]
   ],
   "title": "Building a user interface for a speech recognition-based telephone application system",
   "original": "e91_1503",
   "page_count": 4,
   "order": 335,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "This research aims at providing designers of speech recognition telephone applications with a user interface evaluation and design-aid method, as well as design guidelines. To achieve this, we first elaborated an evaluation methodology suitable for any telephone-based system using speech recognition. This method was then used to evaluate a speech-activated voice information system. This evaluation allowed us to validate the method, and improve the application's design. An automated telephone-receptionist application was then developed to integrate the lessons learned previously, and to elaborate new recognition and dialogue strategies. From these two studies, many results have been generalized in terms of recommendations and guidelines for designers of speech-activated user interfaces. Keywords: user interface design; evaluation methodology; speech recognition; telephone application; design guidelines.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-334"
  },
  "murray91c_eurospeech": {
   "authors": [
    [
     "A. C.",
     "Murray"
    ],
    [
     "C. R.",
     "Frankish"
    ],
    [
     "D. M.",
     "Jones"
    ]
   ],
   "title": "System design and human factors in auditory interfaces",
   "original": "e91_1507",
   "page_count": 4,
   "order": 336,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "Human factors and dialogue design issues are described with reference to computer interfaces which accept only speech input and provide only auditory feedback. The use of a syntax to switch between vocabulary subsets in the course of speech recognition is discussed. Recommendations for design of auditory interfaces are made, with particular reference to dialogue design and to error prevention and correction. Keywords: Automatic speech recognition (ASR), Auditory interface, Dialogue design\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1991-335"
  }
 },
 "sessions": [
  {
   "title": "Plenary",
   "papers": [
    "furui91_eurospeech",
    "fallside91_eurospeech"
   ]
  },
  {
   "title": "Continuous Speech Recognition",
   "papers": [
    "ramesh91_eurospeech",
    "gopalakrishnan91_eurospeech",
    "wilcox91_eurospeech",
    "baker91_eurospeech",
    "sgardoni91_eurospeech"
   ]
  },
  {
   "title": "Segmental Speech Synthesis",
   "papers": [
    "shirai91_eurospeech",
    "guerti91_eurospeech",
    "ishikawa91_eurospeech",
    "garnierrizet91_eurospeech"
   ]
  },
  {
   "title": "Human Factors",
   "papers": [
    "fraser91_eurospeech",
    "day91_eurospeech",
    "murray91_eurospeech",
    "vossen91_eurospeech",
    "zajicek91_eurospeech"
   ]
  },
  {
   "title": "Robust Isolated Word Recognition",
   "papers": [
    "lockwood91_eurospeech",
    "lockwood91b_eurospeech",
    "fellbaum91_eurospeech",
    "hernando91_eurospeech"
   ]
  },
  {
   "title": "Neural Nets: Phonetic Features, Phoneme Recognition, and Time Alignment",
   "papers": [
    "laaksonen91_eurospeech",
    "apolloni91_eurospeech",
    "hataoka91_eurospeech",
    "morgan91_eurospeech",
    "andrews91_eurospeech",
    "norris91_eurospeech",
    "elenius91_eurospeech",
    "jianxin91_eurospeech",
    "ran91_eurospeech",
    "dodd91_eurospeech"
   ]
  },
  {
   "title": "Phonetics I, II",
   "papers": [
    "pittam91_eurospeech",
    "sendlmeier91_eurospeech",
    "nathan91_eurospeech",
    "benoit91_eurospeech",
    "datta91_eurospeech",
    "ganguli91_eurospeech",
    "harmegnies91_eurospeech",
    "shevchenko91_eurospeech",
    "heuvel91_eurospeech",
    "liljencrants91_eurospeech",
    "jansen91_eurospeech",
    "herzel91_eurospeech",
    "van91_eurospeech"
   ]
  },
  {
   "title": "Multilingual Speech Recognition Systems (Special Session)",
   "papers": [
    "bamberg91_eurospeech",
    "cerfdanon91_eurospeech",
    "ney91_eurospeech"
   ]
  },
  {
   "title": "Spoken Language Parsing",
   "papers": [
    "wright91_eurospeech",
    "su91_eurospeech",
    "baggia91_eurospeech",
    "corazza91_eurospeech",
    "andry91_eurospeech",
    "young91_eurospeech"
   ]
  },
  {
   "title": "Speech Coding I-IV",
   "papers": [
    "abrantes91_eurospeech",
    "marques91_eurospeech",
    "rowe91_eurospeech",
    "leung91_eurospeech",
    "sereno91_eurospeech",
    "atungsiri91_eurospeech",
    "nagabuchi91_eurospeech",
    "vigier91_eurospeech",
    "rosina91_eurospeech",
    "kipper91_eurospeech",
    "fuldseth91_eurospeech",
    "zarkadis91_eurospeech",
    "saoudi91_eurospeech",
    "seeker91_eurospeech",
    "chan91_eurospeech",
    "meyer91_eurospeech",
    "suddle91_eurospeech",
    "ribeiro91_eurospeech",
    "law91_eurospeech",
    "deiacovo91_eurospeech",
    "yang91_eurospeech",
    "liu91_eurospeech",
    "chan91b_eurospeech"
   ]
  },
  {
   "title": "Assessment, Intelligibility and Aids for Disabled",
   "papers": [
    "rossi91_eurospeech",
    "sydeserff91_eurospeech",
    "howardjones91_eurospeech",
    "houtgast91_eurospeech",
    "miyata91_eurospeech",
    "jekosch91_eurospeech",
    "wei91_eurospeech",
    "kanevsky91_eurospeech",
    "anglade91_eurospeech",
    "murray91b_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Techniques and Applications",
   "papers": [
    "portele91_eurospeech",
    "abbattista91_eurospeech",
    "king91_eurospeech",
    "hermansky91_eurospeech",
    "greisbach91_eurospeech",
    "chang91_eurospeech",
    "taylor91_eurospeech",
    "valbret91_eurospeech",
    "giustiniani91_eurospeech",
    "delogu91_eurospeech",
    "zingte91_eurospeech",
    "bourlard91_eurospeech",
    "jayant91_eurospeech"
   ]
  },
  {
   "title": "Probabilistic Language Models for Speech Recognition",
   "papers": [
    "pieraccini91_eurospeech",
    "matheson91_eurospeech",
    "giachin91_eurospeech",
    "prieto91_eurospeech",
    "cremonini91_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Phonetic Modelling",
   "papers": [
    "shirai91b_eurospeech",
    "okane91_eurospeech",
    "hirsch91_eurospeech",
    "gong91_eurospeech",
    "ederveen91_eurospeech"
   ]
  },
  {
   "title": "Speaker Identification and Verification",
   "papers": [
    "kraayeveld91_eurospeech",
    "hunt91_eurospeech",
    "bonastre91_eurospeech",
    "xu91_eurospeech"
   ]
  },
  {
   "title": "Pitch Determination and Voice Separation",
   "papers": [
    "cheveigne91_eurospeech",
    "jones91_eurospeech",
    "gu91_eurospeech",
    "degan91_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Understanding Systems",
   "papers": [
    "nakagawa91_eurospeech",
    "bergmann91_eurospeech",
    "poza91_eurospeech",
    "hetherington91_eurospeech",
    "cole91_eurospeech",
    "fiset91_eurospeech",
    "jones91b_eurospeech",
    "hood91_eurospeech",
    "haebumbach91_eurospeech",
    "teixeira91_eurospeech"
   ]
  },
  {
   "title": "Speech Databases, Analysis And Assessment",
   "papers": [
    "larnel91_eurospeech",
    "mathan91_eurospeech",
    "itahashi91_eurospeech",
    "vossen91b_eurospeech",
    "winski91_eurospeech",
    "bourjot91_eurospeech",
    "steeneken91_eurospeech",
    "alphen91_eurospeech",
    "zue91_eurospeech",
    "benaouicha91_eurospeech",
    "denbigh91_eurospeech"
   ]
  },
  {
   "title": "Neural Nets I, II",
   "papers": [
    "bengio91_eurospeech",
    "carey91_eurospeech",
    "niles91_eurospeech",
    "husoy91_eurospeech",
    "sorensen91_eurospeech",
    "petek91_eurospeech",
    "zhang91_eurospeech",
    "haffner91_eurospeech",
    "fukuda91_eurospeech",
    "komori91_eurospeech"
   ]
  },
  {
   "title": "Parsing and Lexical Access",
   "papers": [
    "hosaka91_eurospeech",
    "phillips91_eurospeech",
    "lacouture91_eurospeech",
    "riley91_eurospeech",
    "antoniol91_eurospeech"
   ]
  },
  {
   "title": "Modelling Duration in Speech",
   "papers": [
    "macarron91_eurospeech",
    "mortamet91_eurospeech",
    "kaiki91_eurospeech",
    "campbell91_eurospeech",
    "karjalainen91_eurospeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Algorithms I-III",
   "papers": [
    "mcinnes91_eurospeech",
    "song91_eurospeech",
    "meloni91_eurospeech",
    "peinado91_eurospeech",
    "kenny91_eurospeech",
    "nijtmans91_eurospeech",
    "mcinnes91b_eurospeech",
    "nowell91_eurospeech",
    "falaschi91_eurospeech",
    "galiano91_eurospeech",
    "zhao91_eurospeech",
    "varga91_eurospeech",
    "ballantyne91_eurospeech",
    "rose91_eurospeech",
    "falaschi91b_eurospeech",
    "junqua91_eurospeech",
    "edwards91_eurospeech",
    "lleida91_eurospeech",
    "applebaum91_eurospeech",
    "bahl91_eurospeech",
    "franzini91_eurospeech",
    "steinbiss91_eurospeech",
    "gong91b_eurospeech",
    "sagayama91_eurospeech"
   ]
  },
  {
   "title": "Segmentation",
   "papers": [
    "dalsgaard91_eurospeech",
    "kabre91_eurospeech",
    "cosi91_eurospeech",
    "mcqueen91_eurospeech",
    "schmidt91_eurospeech",
    "feng91_eurospeech",
    "taylor91b_eurospeech",
    "ottesen91_eurospeech",
    "brierton91_eurospeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Applications",
   "papers": [
    "compernolle91_eurospeech",
    "yashchin91_eurospeech",
    "canavesio91_eurospeech",
    "morin91_eurospeech",
    "ciaramella91_eurospeech"
   ]
  },
  {
   "title": "Natural Language Processing",
   "papers": [
    "monaghan91_eurospeech",
    "carlo91_eurospeech",
    "maltese91_eurospeech",
    "pelillo91_eurospeech",
    "wrigley91_eurospeech"
   ]
  },
  {
   "title": "Symbolic Processing in Speech Synthesis",
   "papers": [
    "tiboni91_eurospeech",
    "williams91_eurospeech",
    "russi91_eurospeech",
    "luk91_eurospeech"
   ]
  },
  {
   "title": "Sub-Lexical Unit Modelling",
   "papers": [
    "hwang91_eurospeech",
    "blomberg91_eurospeech",
    "torkkola91_eurospeech",
    "rentzepopoulos91_eurospeech",
    "parfitt91_eurospeech"
   ]
  },
  {
   "title": "Speech Understanding and Dialogue",
   "papers": [
    "goodine91_eurospeech",
    "yamaoka91_eurospeech",
    "boogers91_eurospeech",
    "nogaito91_eurospeech",
    "segarra91_eurospeech",
    "baggia91b_eurospeech"
   ]
  },
  {
   "title": "Assessment",
   "papers": [
    "bezooijen91_eurospeech",
    "benoit91b_eurospeech",
    "griee91_eurospeech",
    "monaghan91b_eurospeech",
    "halka91_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition: Stochastic Modelling",
   "papers": [
    "euler91_eurospeech",
    "jouvet91_eurospeech",
    "jouvet91b_eurospeech",
    "leung91b_eurospeech",
    "dubois91_eurospeech",
    "gauvain91_eurospeech"
   ]
  },
  {
   "title": "Speech Interfaces: Systems and Applications",
   "papers": [
    "ruehl91_eurospeech",
    "dobler91_eurospeech",
    "smaili91_eurospeech",
    "thomson91_eurospeech",
    "billi91_eurospeech",
    "carlos91_eurospeech",
    "helle91_eurospeech",
    "antoniol91b_eurospeech",
    "schulthei91_eurospeech",
    "bernstein91_eurospeech",
    "rooney91_eurospeech"
   ]
  },
  {
   "title": "Neural Nets: Comparative Studies, Lexical Recognition",
   "papers": [
    "devillers91_eurospeech",
    "thurston91_eurospeech",
    "ferreiros91_eurospeech",
    "poirier91_eurospeech",
    "bengio91b_eurospeech",
    "sawai91_eurospeech",
    "wittenburg91_eurospeech",
    "mekuria91_eurospeech",
    "russell91_eurospeech",
    "brauer91_eurospeech",
    "aliosaar91_eurospeech",
    "jelinek91_eurospeech",
    "carlson91_eurospeech"
   ]
  },
  {
   "title": "Dialogue and Translation",
   "papers": [
    "guyomard91_eurospeech",
    "gerbino91_eurospeech",
    "eggins91_eurospeech",
    "roe91_eurospeech",
    "subramaniam91_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis and Signal Representation",
   "papers": [
    "papaodysseus91_eurospeech",
    "koreman91_eurospeech",
    "alku91_eurospeech",
    "galas91_eurospeech",
    "bimbot91_eurospeech"
   ]
  },
  {
   "title": "Discriminant Training and Speaker Adaptation",
   "papers": [
    "zunkler91_eurospeech",
    "schmidbauer91_eurospeech",
    "brugnara91_eurospeech",
    "nitta91_eurospeech",
    "fung91_eurospeech"
   ]
  },
  {
   "title": "Perception I",
   "papers": [
    "son91_eurospeech",
    "howardjones91b_eurospeech",
    "ma91_eurospeech",
    "laan91_eurospeech",
    "steeneken91b_eurospeech",
    "ooyen91_eurospeech",
    "bergem91_eurospeech",
    "beinum91_eurospeech",
    "ingram91_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis and Prosody",
   "papers": [
    "potapova91_eurospeech",
    "strik91_eurospeech",
    "stensby91_eurospeech",
    "madhukumar91_eurospeech",
    "hieronymus91_eurospeech",
    "quazza91_eurospeech",
    "omalley91_eurospeech",
    "balestri91_eurospeech"
   ]
  },
  {
   "title": "Text-to-Speech Synthesis Systems",
   "papers": [
    "lindert91_eurospeech",
    "lewis91_eurospeech",
    "oliveira91_eurospeech",
    "hansen91_eurospeech",
    "olaszy91_eurospeech"
   ]
  },
  {
   "title": "Phonetic Modelling",
   "papers": [
    "niyogi91_eurospeech",
    "holmes91_eurospeech",
    "kaspar91_eurospeech",
    "kim91_eurospeech",
    "dix91_eurospeech"
   ]
  },
  {
   "title": "Generation of Prosody",
   "papers": [
    "hirschberg91_eurospeech",
    "horne91_eurospeech",
    "vonwiller91_eurospeech",
    "nickyoud91_eurospeech",
    "delmonte91_eurospeech"
   ]
  },
  {
   "title": "Speech Processing and Analysis",
   "papers": [
    "acker91_eurospeech",
    "dabis91_eurospeech",
    "mumolo91_eurospeech",
    "aarset91_eurospeech",
    "corney91_eurospeech",
    "verhelst91_eurospeech",
    "leung91c_eurospeech",
    "ambikairajah91_eurospeech",
    "jacovitti91_eurospeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Hardware and Noise Reduction",
   "papers": [
    "ciaramella91b_eurospeech",
    "aktas91_eurospeech",
    "tsopanoglou91_eurospeech",
    "schulthei91b_eurospeech",
    "sedivy91_eurospeech",
    "villarrubia91_eurospeech",
    "hermansky91b_eurospeech",
    "junqua91b_eurospeech",
    "dvorak91_eurospeech",
    "vicsi91_eurospeech",
    "gomezmena91_eurospeech"
   ]
  },
  {
   "title": "Sub-Word Units for Automatic Speech Recognition",
   "papers": [
    "fissore91_eurospeech",
    "giachin91b_eurospeech",
    "nagai91_eurospeech",
    "drexler91_eurospeech"
   ]
  },
  {
   "title": "Auditory Modelling",
   "papers": [
    "fjallbrant91_eurospeech",
    "dijk91_eurospeech",
    "beham91_eurospeech",
    "morris91_eurospeech",
    "pozasalvarez91_eurospeech"
   ]
  },
  {
   "title": "Speech Interfaces: Dialogue and Human Factors",
   "papers": [
    "peckham91_eurospeech",
    "tubach91_eurospeech",
    "deville91_eurospeech",
    "gaiffe91_eurospeech",
    "lefebvre91_eurospeech",
    "arndt91_eurospeech",
    "smeele91_eurospeech",
    "lickley91_eurospeech",
    "chointere91_eurospeech",
    "murray91c_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1991"
}