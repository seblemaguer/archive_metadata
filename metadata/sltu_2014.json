{
 "title": "4th Workshop on Spoken Language Technologies for Under-Resourced Languages  (SLTU 2014)",
 "location": "St. Petersburg, Russia",
 "startDate": "14/5/2014",
 "endDate": "16/5/2014",
 "conf": "SLTU",
 "year": "2014",
 "name": "sltu_2014",
 "series": "SLTU",
 "SIG": "SIGUL",
 "title1": "4th Workshop on Spoken Language Technologies for Under-Resourced Languages",
 "title2": "(SLTU 2014)",
 "date": "14-16 May 2014",
 "booklet": "sltu_2014.pdf",
 "papers": {
  "nakamura14_sltu": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Towards real-time multilingual multimodal speech-to-speech translation",
   "original": "sl14_013",
   "page_count": 4,
   "order": 1,
   "p1": "13",
   "pn": "15",
   "abstract": [
    "Speech-to-speech translation technology enables natural oral communication between different language speaking people. Many research projects have addressed speech-to-speech translation (S2ST) technology, such as ATR, VERBMOBIL, C-STAR, NESPOLE!, BABYLON, GALE, and EU-bridge. The speechto- speech translation system is normally composed of automatic speech recognition (ASR), machine translation (MT), and speech synthesis (TTS). All of the modules are corpus-based and statistical model-based systems. In this talk, new challenges toward a real-time multimodal speechto- speech translation will be introduced.\n",
    "Index Terms: Speech-to-speech translation, S2ST, multimodal processing, multilingual systems\n",
    ""
   ]
  },
  "gales14_sltu": {
   "authors": [
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Kate M.",
     "Knill"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Shakti P.",
     "Rath"
    ]
   ],
   "title": "Speech recognition and keyword spotting for low-resource languages: Babel project research at CUED",
   "original": "sl14_016",
   "page_count": 9,
   "order": 2,
   "p1": "16",
   "pn": "23",
   "abstract": [
    "Recently there has been increased interest in Automatic Speech Recognition (ASR) and Key Word Spotting (KWS) systems for low resource languages. One of the driving forces for this research direction is the IARPA Babel project. This paper describes some of the research funded by this project at Cambridge University, as part of the Lorelei team co-ordinated by IBM. A range of topics are discussed including: deep neural network based acoustic models; data augmentation; and zero acoustic model resource systems. Performance for all approaches is evaluated using the Limited (approximately 10 hours) and/or Full (approximately 80 hours) language packs distributed by IARPA. Both KWS and ASR performance figures are given. Though absolute performance varies from language to language, and keyword list, the approaches described show consistent trends over the languages investigated to date. Using comparable systems over the five Option Period 1 languages indicates a strong correlation between ASR performance and KWS performance.\n",
    "Index Terms: keyword spotting, deep neural network, low-resource languages, multi-lingual systems.\n",
    ""
   ]
  },
  "anguera14_sltu": {
   "authors": [
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Luis J.",
     "Rodriguez-Fuentes"
    ],
    [
     "Igor",
     "Szöke"
    ],
    [
     "Andi",
     "Buzo"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Mikel",
     "Penagarikano"
    ]
   ],
   "title": "Query-by-example spoken term detection evaluation on low-resource languages",
   "original": "sl14_024",
   "page_count": 8,
   "order": 3,
   "p1": "24",
   "pn": "31",
   "abstract": [
    "As part of the MediaEval 2013 benchmark evaluation campaign, the objective of the Spoken Web Search (SWS) task was to perform Query-by-Example Spoken Term Detection (QbE-STD), using spoken queries to retrieve matching segments in a set of audio files. As in previous editions, the SWS 2013 evaluation focused on the development of technology specifically designed to perform speech search in a low-resource setting. In this paper, we first describe the main features of past SWS evaluations and then focus on the 2013 SWS task, in which a special effort was made to prepare a challenging database, including speech in 9 different languages with diverse environment and channel conditions. The main novelties of the submitted systems are reviewed and performance figures are then presented and discussed, demonstrating the feasibility of the proposed task, even under such challenging conditions. Finally, the fusion of the 10 top-performing systems is analyzed. The best fusion provides a 30% relative improvement over the best single system in the evaluation, which proves that a variety of approaches can be effectively combined to bring complementary information in the search for queries.\n",
    "Index Terms: benchmark evaluation, low-resource languages, query-by-example spoken term detection\n",
    ""
   ]
  },
  "adel14_sltu": {
   "authors": [
    [
     "Heike",
     "Adel"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Dominic",
     "Telaar"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ],
    [
     "Tim",
     "Schlippe"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Features for factored language models for code-Switching speech",
   "original": "sl14_032",
   "page_count": 7,
   "order": 4,
   "p1": "32",
   "pn": "38",
   "abstract": [
    "This paper presents investigations of features which can be used to predict Code-Switching speech. For this task, factored language models are applied and implemented into a state-of-the-art decoder. Different possible factors, such as words, part-of-speech tags, Brown word clusters, open class words and open class word clusters are explored. We find that Brown word clusters, part-of-speech tags and open-class words are most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In decoding experiments, the model containing Brown word clusters and part-of-speech tags and the model also including open class word clusters yield the best mixed error rate results. In summary, the factored language models can reduce the perplexity on the SEAME evaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative.\n",
    "Index Terms: language modeling, factored language models, Code-Switching speech\n",
    ""
   ]
  },
  "grezl14_sltu": {
   "authors": [
    [
     "Frantisek",
     "Grézl"
    ],
    [
     "Martin",
     "Karafiát"
    ]
   ],
   "title": "Adapting multilingual neural network hierarchy to a new language",
   "original": "sl14_039",
   "page_count": 7,
   "order": 5,
   "p1": "39",
   "pn": "45",
   "abstract": [
    "The neural network based features became an inseparable part of state-of-the-art LVCSR systems. With the increasing accent on fast development of ASR system on limited resources, there is an effort to alleviate the need of large amount of transcribed in-domain data. One successful way is to use data from other languages. We present extensive evaluation of several strategies to adapt hierarchical neural network in search for the most effective one. To avoid the bias towards one target language, our strategies were evaluated on five languages. Also, several multilingual neural network hierarchies were trained on two sets of languages. Thus the results provide solid insight into the problem of adapting hierarchical system.\n",
    "Index Terms: feature extraction, Bottle-Neck features, neural network adaptation, multilingual neural networks, Stacked Bottle- Neck structure\n",
    ""
   ]
  },
  "sakti14_sltu": {
   "authors": [
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Recent progress in developing grapheme-based speech recognition for Indonesian ethnic languages: Javanese, Sundanese, Balinese and Bataks",
   "original": "sl14_046",
   "page_count": 7,
   "order": 6,
   "p1": "46",
   "pn": "52",
   "abstract": [
    "With the advent of globalization, multilingualism in Indonesia gradually faces a state of catastrophe. Currently among 726 ethnic languages spoken in Indonesian archipelago, 146 are endangered. Several projects have been initiated for cultural preservation which can prevent the endangered language from being lost. Nevertheless, the available technology that could support communication within indigenous communities, as well as with people outside the community, is still very rare in Indonesia. Speech translation technology is one of the technologies that may help indigenous communities in Indonesia to overcome language barrier and cross cultural gap as well as to face globalization. Our long-term goal is to establish an infrastructure of speech translation system from ethnic languages to English/Indonesian, and this paper presents recent progress of data resources collection and speech recognition system development for four Indonesian major ethnic languages: Javanese, Sundanese, Balinese and Bataks.\n",
    "Index Terms: Language preservation, Indonesian ethnic languages, speech data collection, speech recognition system.\n",
    ""
   ]
  },
  "addadecker14_sltu": {
   "authors": [
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ]
   ],
   "title": "Speech alignment and recognition experiments for Luxembourgish",
   "original": "sl14_053",
   "page_count": 8,
   "order": 7,
   "p1": "53",
   "pn": "60",
   "abstract": [
    "Luxembourgish, embedded in a multilingual context on the divide between Romance and Germanic cultures, remains one of Europe’s under-described languages. In this paper, we propose to study acoustic similarities between Luxembourgish and major contact languages (German, French, English) with the help of automatic speech alignment and recognition systems. Experiments were run using monolingual acoustic models trained on German, French and English together with (i) “multilingual” models trained on pooled speech data from these three languages, or with (ii) native Luxembourgish acoustic models from 1200 hours of untranscribed Luxembourgish audio data using unsupervised methods. We investigated whether Luxembourgish was globally better represented by one of the individual languages, by the multilingual model or by the native (unsupervised) model. While German provides globally the best acoustic match for native Luxembourgish, detailed analyses reveal language-specific preferences, in particular English and Luxembourgish models are preferred on diphthongs. The first ASR results illustrate the accuracy of the various sets of supervised monolingual and multilingual models versus unsupervised Luxembourgish acoustic models. The ASR word error rate is progressively reduced from 60 to 25% on the development data set by unsupervised training of larger context-dependent models on increasing anounts of audio data.\n",
    "Index Terms: under-resourced languages, languages in contact, Luxembourgish, language similarity, acoustic modeling, multilingual models, large vocabulary speech recognition, forced alignment, unsupervised training.\n",
    ""
   ]
  },
  "sahraeian14_sltu": {
   "authors": [
    [
     "Reza",
     "Sahraeian"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Febe de",
     "Wet"
    ]
   ],
   "title": "On using intrinsic spectral analysis for low-resource languages",
   "original": "sl14_061",
   "page_count": 5,
   "order": 8,
   "p1": "61",
   "pn": "65",
   "abstract": [
    "This paper demonstrates the application of Intrinsic Spectral Analysis (ISA) for low-resource Automatic Speech Recognition (ASR). State-of-the-art speech recognition systems that require large amounts of task specific training data fail to reliably model feature distributions in resource impoverished settings. We address this issue by approaching the problem in the front-end, where we can learn an intrinsic subspace that can replace the traditional feature space like mel frequency cepstral coefficients (MFCC).We use ISA features for underresourced settings to model the acoustic feature distribution with less complexity. We also propose to combine intrinsic features with extrinsic ones to take advantage of both subspaces. Experimental results for a phone recognition task on the Afrikaans language show that a combination of the intrinsic subspace and extrinsic subspaces provides us with improved performance compared to conventional features.\n",
    "Index Terms: low-resource speech recognition, manifold learning, intrinsic spectral analysis\n",
    ""
   ]
  },
  "samsonjuan14_sltu": {
   "authors": [
    [
     "Sarah",
     "Samson Juan"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Solange",
     "Rossato"
    ]
   ],
   "title": "Semi-supervised G2p bootstrapping and its application to ASR for a very under-resourced language: Iban",
   "original": "sl14_066",
   "page_count": 7,
   "order": 9,
   "p1": "66",
   "pn": "72",
   "abstract": [
    "This paper describes our experiments and results on using a local dominant language in Malaysia (Malay), to bootstrap automatic speech recognition (ASR) for a very under-resourced language: Iban (also spoken in Malaysia on the Borneo Island part). Resources in Iban for building a speech recognition were nonexistent. For this, we tried to take advantage of a language from the same family with several similarities. First, to deal with the pronunciation dictionary, we proposed a bootstrapping strategy to develop an Iban pronunciation lexicon from a Malay one. A hybrid version, mix of Malay and Iban pronunciations, was also built and evaluated. Following this, we experimented with three Iban ASRs; each depended on either one of the three different pronunciation dictionaries: Malay, Iban or hybrid. Our best results (WER) for Iban ASR (with different lexicon) were as follows: 20.82% (Malay G2P), 21.90% (Iban G2P) and 20.60% (Hybrid G2P). Apart from that, we applied system combination using all of the systems and obtained an improved accuracy of 19.22%.\n",
    "Index Terms: under-resourced language, speech recognition, Iban language, Malay language, bootstrapping, Kaldi, grapheme-to-phoneme\n",
    ""
   ]
  },
  "stahlberg14_sltu": {
   "authors": [
    [
     "Felix",
     "Stahlberg"
    ],
    [
     "Tim",
     "Schlippe"
    ],
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Towards automatic speech recognition without pronunciation dictionary, transcribed speech and text resources in the target language using cross-lingual word-to-phoneme alignment",
   "original": "sl14_073",
   "page_count": 8,
   "order": 10,
   "p1": "73",
   "pn": "80",
   "abstract": [
    "In this paper we tackle the task of bootstrapping an Automatic Speech Recognition system without an a priori given language model, a pronunciation dictionary, or transcribed speech data for the target language Slovene – only untranscribed speech and translations to other resource-rich source languages of what was said are available. Therefore, our approach is highly relevant for under-resourced and non-written languages. First, we borrow acoustic models from a strongly related language (Croatian) and apply a Croatian phoneme recognizer to the Slovene speech. Second, we segment the recognized phoneme strings into word units using cross-lingual word-tophoneme alignment. Third, we compensate for phoneme recognition and alignment errors in the segmented phoneme sequences and aggregate the resulting phoneme sequence segments in a pronunciation dictionary for Slovene. Orthographic representations are generated using a Croatian phoneme-to-grapheme model. Finally, we use the resulting dictionary and the Croatian acoustic models to recognize Slovene. Our best recognizer achieves a Character Error Rate of 52% on the BMED corpus.\n",
    "Index Terms: pronunciation dictionary, non-written languages, word-to-phoneme alignment, language discovery, zeroresource\n",
    ""
   ]
  },
  "kipyatkova14_sltu": {
   "authors": [
    [
     "Irina",
     "Kipyatkova"
    ],
    [
     "Vasilisa",
     "Verkhodanova"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "Rescoring n-best lists for Russian speech recognition using factored language models",
   "original": "sl14_081",
   "page_count": 6,
   "order": 11,
   "p1": "81",
   "pn": "86",
   "abstract": [
    "In this paper, we present a research of factored language model (FLM) for rescoring N-best lists for Russian speech recognition task. As a baseline language model we used a 3- gram language model. Both baseline and factored language models were trained on a text corpus collected from recent news texts on Internet sites of online newspapers; total size of the corpus is about 350 million words (2.4 GB data). For FLMs creation, we used five factors: word, its lemma, stem, part-of-speech, and morphological tag. We investigate the influence of factor set on language model perplexity and word error rate (WER). Experiments on large vocabulary continuous Russian speech recognition showed that FLM can reduce WER.\n",
    "Index Terms: factored language model (FLM), automatic speech recognition (ASR), N-best lists, Russian language processing\n",
    ""
   ]
  },
  "ferreira14_sltu": {
   "authors": [
    [
     "José Pedro",
     "Ferreira"
    ],
    [
     "Cristiano",
     "Chesi"
    ],
    [
     "Hyongsil",
     "Cho"
    ],
    [
     "Daan",
     "Baldewijns"
    ],
    [
     "Daniela",
     "Braga"
    ],
    [
     "Miguel",
     "Dias"
    ]
   ],
   "title": "On Mirandese language resources for text-to-speech",
   "original": "sl14_087",
   "page_count": 5,
   "order": 12,
   "p1": "87",
   "pn": "91",
   "abstract": [
    "This paper aims at describing the major components of the first Text-to-Speech (TTS) system ever built for Mirandese, [1] a minority language spoken in the Northeast of Portugal. Both language resources development (corpus, textnormalization rules, annotated lexicon, phone sets and recordings) and the TTS (Statistical Parameter Synthesis) system are documented here.\n",
    "",
    "",
    "J. P. Ferreira, C. Chesi, D. Baldewijns, D. Braga, M.S. Dias 2012. The First Mirandese Text-to-Speech System. Proceedings of ELE2013 Conference\n",
    "",
    "",
    "Index Terms: Mirandese, text-to-speech, language resources\n",
    ""
   ]
  },
  "ahmed14_sltu": {
   "authors": [
    [
     "Zeeshan",
     "Ahmed"
    ],
    [
     "Joao P.",
     "Cabral"
    ]
   ],
   "title": "HMM-based speech synthesiser for the Urdu language",
   "original": "sl14_092",
   "page_count": 6,
   "order": 13,
   "p1": "92",
   "pn": "97",
   "abstract": [
    "This work presents Hidden Markov Model (HMM) based speeeh synthesis for the Urdu language. This is a widely spoken language across different regions in Asia. For example, Urdu is the official language of Pakistan and one of the national languages of India. Unfortunately, there is no corpus of Urdu currently publicly available that to our knowledge is appropriate for HMM-based speech synthesis purpose. We overcame this problem by recording an Urdu speech database with word and phone labels obtained using manual and semi-automatic annotation approaches. In summary, the objective of this work is to develop an HMM- based Urdu speech synthesiser from scratch by trying to use publicly available text processing tools for this language and by developing the necessary processing components.\n",
    "Index Terms: HMM-based speech synthesis, Urdu speech synthesiser, Urdu speech corpus\n",
    ""
   ]
  },
  "nguyen14_sltu": {
   "authors": [
    [
     "Thi Thu Trang",
     "Nguyen"
    ],
    [
     "Do Dat",
     "Tran"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Christophe",
     "d’Alessandro"
    ],
    [
     "Thi Ngoc Yen",
     "Pham"
    ]
   ],
   "title": "Intonation issues in HMM-based speech synthesis for Vietnamese",
   "original": "sl14_098",
   "page_count": 7,
   "order": 14,
   "p1": "98",
   "pn": "104",
   "abstract": [
    "In an HMM-based Text-To-Speech system, contextual features, including phonetic and prosodic factors have a significant influence to the spectrum, F0 and duration of the synthetic voice. This paper proposes prosodic features aiming at improving the naturalness of an HMM-based TTS system (VTed) for a tonal language, Vietnamese. The ToBI (Tones and Break Indices) features are used to learn two crucial prosodic cues i.e. intonation (boundary tones) and pause (break indices), concurrently with another set of features. The result of MOS test showed that the general quality of synthetic voice is rather good, 1.21 point lower than the natural voice. About 55% of the voice trained with ToBI boundary tone feature are perceived as similar to the voice trained without this feature, while a 10% difference in favour of the voice trained without this ToBI feature is observed. This may be linked with F0 contour lowering or raising regardless of lexical tones. This brought two main problems in the synthetic voice: discontinuity in spectrum and F0 or unexpected voice quality. This paper then concluded the need of much more work on intonation modeling that should take into account the Vietnamese tones. A new prosody model can be designed, which may consider the ToBI model, with respect to lexical tones and the syntactic structure of Vietnamese.\n",
    "Index Terms: Text-to-speech (TTS), speech synthesis, tonal language, Vietnamese, HMM-based speech synthesis, intonation, ToBI\n",
    ""
   ]
  },
  "chistikov14_sltu": {
   "authors": [
    [
     "Pavel",
     "Chistikov"
    ],
    [
     "Andrey",
     "Talanov"
    ]
   ],
   "title": "High quality speech synthesis using a small speech dataset",
   "original": "sl14_105",
   "page_count": 7,
   "order": 15,
   "p1": "105",
   "pn": "111",
   "abstract": [
    "We propose an approach to synthesizing high-quality speech under the conditions of a small dataset. A robust method for solving this problem is vital for voice restoration (recreation of lost fragments of records based on available speech material of a well-known person, e.g. an actor). The proposed TTS system is a hybrid system which includes the advantages of both HMM- and Unit Selection-based TTS systems. The approach described in the paper is based on statistical models of intonation parameters and special algorithms of speech element concatenation and modification. Listening tests show that it is possible to synthesize high-quality speech even with a small speech database (approximately one hour of speech).\n",
    "Index Terms: speech synthesis, voice restoration, hidden Markov models, Unit Selection, speech modification.\n",
    ""
   ]
  },
  "hartmann14_sltu": {
   "authors": [
    [
     "William",
     "Hartmann"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Cross-word sub-word units for low-resource keyword spotting",
   "original": "sl14_112",
   "page_count": 6,
   "order": 16,
   "p1": "112",
   "pn": "117",
   "abstract": [
    "We investigate the use of sub-word lexical units for the detection of out-of-vocabulary (OOV) keywords in the keyword spotting task. Sub-word units based on morphological decomposition and character ngrams are compared. In particular, we examine the benefit of sub-word units that cross word boundaries. Experiments are performed on the IARPA Babel Turkish dataset. Our results demonstrate that cross-word subword units achieve similar performance on OOV keywords as other types of sub-word units, but can be combined to produce further gains. We also show that sub-word units can be used to improve detection of in-vocabulary keywords. System combination provides a 18% relative gain in ATWV with the best two systems, and 25% with the best three systems.\n",
    "Index Terms: keyword search, spoken term detection, OOV, sub-word lexical units, low resource LVCSR\n",
    ""
   ]
  },
  "alumae14_sltu": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ]
   ],
   "title": "Recent improvements in Estonian LVCSR",
   "original": "sl14_118",
   "page_count": 6,
   "order": 17,
   "p1": "118",
   "pn": "123",
   "abstract": [
    "This paper describes our current automatic transcription system for Estonian semi-spontaneous speech that we are developing within the Estonian language technology national program. A three pass decoding strategy is employed, with speaker-independent GMM acoustic models used in the first pass and speaker-adapted DNN-HMM models in the last pass. A neural network based phone duration model is used to rescore recognition lattices after the final pass and is found to give a surprisingly large gain in recognition accuracy. Compound words are split before building a statistical language model, and reconstructed from recognized hypotheses using an n-gram model. The word error rate of our system is 17.9% on broadcast conversations and 26.3% on conference speeches. This is around 8% absolute (24-30% relative) improvement compared to a GMM-based system of 2012.\n",
    "Index Terms: Speech recognition, LVCSR, DNN, duration model, Estonian\n",
    ""
   ]
  },
  "cucu14_sltu": {
   "authors": [
    [
     "Horia",
     "Cucu"
    ],
    [
     "Andi",
     "Buzo"
    ],
    [
     "Corneliu",
     "Burileanu"
    ]
   ],
   "title": "Unsupervised acoustic model training using multiple seed ASR systems",
   "original": "sl14_124",
   "page_count": 7,
   "order": 18,
   "p1": "124",
   "pn": "130",
   "abstract": [
    "Unsupervised acoustic modeling can offer a cost and time effective way of creating a solid acoustic model for any under-resourced language. This paper explores the novel idea of using two independent ASR systems to transcribe new speech data, align and filter the ASR hypotheses and use the presumably correct transcriptions to iteratively improve the two seed ASR systems. In parallel, the newly transcribed speech is used to retrain the mainstream ASR system. The methodology leads to WER relative improvements of 5.5% after the first iteration. The experiments are made with data in the Romanian language.\n",
    "Index Terms: unsupervised acoustic modeling, speech recognition, unsupervised training, under-resourced languages\n",
    ""
   ]
  },
  "tarjan14_sltu": {
   "authors": [
    [
     "Balázs",
     "Tarján"
    ],
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Péter",
     "Mihajlik"
    ]
   ],
   "title": "A bilingual study on the prediction of morph-based improvement",
   "original": "sl14_131",
   "page_count": 8,
   "order": 19,
   "p1": "131",
   "pn": "138",
   "abstract": [
    "Morph-based language modeling has been efficiently applied in improving the accuracy of Large-Vocabulary Continuous Speech Recognition (LVCSR) systems - especially in morphologically rich languages. However, the rate of improvements varies greatly and the underlying principles have been only superficially studied. Having a method that can predict the expected improvement prior to experimentations would be largely useful. In this paper, we introduce language-independent factors affecting morphbased improvement and show how they can be utilized in estimating the effectiveness of statistical morph-based language modeling. The task was broadcast news transcription in two less investigated languages, Hungarian and Romanian. It was found that in case of under-resourced conditions morph-based models can bring significant improvement - even for a morphologically less rich language like Romanian. In addition, it was shown that noninitial morph tagging can constantly outperform explicit modeling of word-boundaries both in terms of letter and word accuracies.\n",
    ""
   ]
  },
  "schlippe14_sltu": {
   "authors": [
    [
     "Tim",
     "Schlippe"
    ],
    [
     "Wolf",
     "Quaschningk"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Combining grapheme-to-phoneme converter outputs for enhanced pronunciation generation in low-resource scenarios",
   "original": "sl14_139",
   "page_count": 7,
   "order": 20,
   "p1": "139",
   "pn": "145",
   "abstract": [
    "For pronunciation dictionary creation, we propose the combination of grapheme-to-phoneme (G2P) converter outputs where low resources are available to train the single converters. Our experiments with German, English, French, and Spanish show that in most cases the phoneme-level combination approaches validated reference pronunciations more than the single converters. In case of only little training data, the impact of the fusion is high which shows their great importance for under-resourced languages. We detected that the output of G2P converters built with web-derived wordpronunciation pairs can further improve pronunciation quality. With 23.1% relative in terms of phoneme error rate to the reference dictionary, we report the largest improvement for the scenario where only 200 French word-pronunciation pairs and web data are given as training data. In additional automatic speech recognition experiments we show that the resulting dictionaries can lead to performance improvements.\n",
    "Index Terms: pronunciation dictionary, pronunciation modeling, low-resource scenarios, multilingual speech recognition, rapid language adaptation\n",
    ""
   ]
  },
  "laurent14_sltu": {
   "authors": [
    [
     "Antoine",
     "Laurent"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Development of a Korean speech recognition system with little annotated data",
   "original": "sl14_146",
   "page_count": 7,
   "order": 21,
   "p1": "146",
   "pn": "152",
   "abstract": [
    "This paper investigates the development of a speech-totext transcription system for the Korean language in the context of the DGA RAPID Rapmat project. Korean is an alphasyllabary language spoken by about 78 million people worldwide. As only a small amount of manually transcribed audio data were available, the acoustic models were trained on audio data downloaded from several Korean websites in an unsupervised manner, and the language models were trained on web texts. The reported word and character error rates are estimates, as development corpus used in these experiments was also constructed from the untranscribed audio data, the web texts and automatic transcriptions. Several variants for unsupervised acoustic model training were compared to assess the influence of the vocabulary size (200k vs 2M), the type of language model (words vs characters), the acoustic unit (phonemes vs half-syllables), as well as incremental batch vs iterative decoding of the untranscribed audio corpus.\n",
    "Index Terms: Speech recognition system, unsupervised acoustic training, korean, approximative transcripts\n",
    ""
   ]
  },
  "do14_sltu": {
   "authors": [
    [
     "Thi-Ngoc-Diep",
     "Do"
    ],
    [
     "Alexis",
     "Michaud"
    ],
    [
     "Eric",
     "Castelli"
    ]
   ],
   "title": "Towards the automatic processing of Yongning Na (sino-tibetan): developing a ‘light’ acoustic model of the target language and testing ‘heavyweight’ models from five national languages",
   "original": "sl14_153",
   "page_count": 8,
   "order": 22,
   "p1": "153",
   "pn": "",
   "abstract": [
    "Automatic speech processing technologies hold great potential to facilitate the urgent task of documenting the world’s languages. The present research aims to explore the application of speech recognition tools to a littledocumented language, with a view to facilitating processes of annotation, transcription and linguistic analysis. The target language is Yongning Na (a.k.a. Mosuo), an unwritten Sino-Tibetan language with less than 50,000 speakers. An acoustic model of Na was built using CMU Sphinx. In addition to this ‘light’ model, trained on a small data set (only 4 hours of speech from 1 speaker), ‘heavyweight’ models from five national languages (English, French, Chinese, Vietnamese and Khmer) were also applied to the same data. Preliminary results are reported, and perspectives for the long road ahead are outlined.\n",
    "Index Terms: Acoustic models, automatic speech recognition (ASR), multilingual modelling, under-resourced languages, endangered languages, Yongning Na, Naish languages, language portability, statistical language modeling, crosslingual acoustic modelling and adaptation\n",
    ""
   ]
  },
  "vasilescu14_sltu": {
   "authors": [
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Bianca",
     "Vieru"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Exploring pronunciation variants for Romanian speech-to-text transcription",
   "original": "sl14_161",
   "page_count": 8,
   "order": 23,
   "p1": "161",
   "pn": "168",
   "abstract": [
    "Speech processing tools were applied to investigate morpho-phonetic trends in contemporary spoken Romanian, with the objective of improving the pronunciation dictionary and more generally, the acoustic models of a speech recognition system. As no manually transcribed audio data were available for training, language models were estimated on a large text corpus and used to provide indirect supervision to train acoustic models in a semi-supervised manner. Automatic transcription errors were analyzed in order to gain insights into language specific features for both improving the current performance of the system and to explore linguistic issues. Two aspects of the Romanian morpho-phonology were investigated based on this analysis: the deletion of the masculine definite article -l and the secondary palatalization of plural nouns and adjectives and of 2nd person indicative of verbs.\n",
    "Index Terms: ASR, Romanian, speech transcription errors, pronunciation variants, definite article, palatalization\n",
    ""
   ]
  },
  "vakil14_sltu": {
   "authors": [
    [
     "Anjana",
     "Vakil"
    ],
    [
     "Alexis",
     "Palmer"
    ]
   ],
   "title": "Cross-language mapping for small-vocabulary ASR in under-resourced languages: investigating the impact of source language choice",
   "original": "sl14_169",
   "page_count": 7,
   "order": 24,
   "p1": "169",
   "pn": "175",
   "abstract": [
    "For small-vocabulary applications, a mapped pronunciation lexicon can enable speech recognition in a target underresourced language using an out-of-the-box recognition engine for a high-resource source language. Existing algorithms for cross-language phoneme mapping enable the fully automatic creation of such lexicons using just a few minutes of audio, making speech-driven applications in any language feasible. What such methods have not considered is whether careful selection of the source language based on the linguistic properties of the target language can improve recognition accuracy; this paper reports on a preliminary exploration of this question. Results from a first case study seem to indicate that phonetic similarity between target and source language does not significantly impact accuracy, underscoring the languageindependence of such techniques.\n",
    "Index Terms: under-resourced languages, speech recognition, lexicon building, phoneme mapping\n",
    ""
   ]
  },
  "do14b_sltu": {
   "authors": [
    [
     "Cong-Thanh",
     "Do"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Speech-to-text development for Slovak, a low-resourced language",
   "original": "sl14_176",
   "page_count": 7,
   "order": 25,
   "p1": "176",
   "pn": "182",
   "abstract": [
    "Development of an automatic speech recognition (ASR) system for low-resourced languages is an important research topic in ASR. This paper reports on the development of a speech-to-text (STT) system targeting broadcast news and broadcast conversation transcription for the low-resourced Slovak language. Context-dependent acoustic models are trained without any manually transcribed audio data via cross-language transfer and unsupervised training. In addition, a pronunciation dictionary for Slovak language is created using efficient rule-based pronunciation modeling. For language modeling, large N-gram language models were estimated on 63M words of texts downloaded from the Internet. The system uses MLP (multilayer perceptron) features imported from English which are concatenated with cepstral PLP (perceptual linear prediction) and F0 (pitch) features. These techniques were applied to develop a Slovak STT system with performance similar to that obtained by state-of-the-art systems for other languages. Furthermore, we propose to reduce the dimension of the MLP+PLP+F0 features from 81 to 50, using principal component analysis (PCA), in order to reduce the redundancy between the MLP and the PLP+F0 features. This feature reduction makes it possible to reduce the word error rate (WER) and the recognition time while reducing the CMLLR adaptation time by a factor of 3.\n",
    "Index Terms: Slovak speech-to-text, ASR for low-resourced languages, Multi-layer perceptron, Unsupervised acoustic model training, Principal component analysis\n",
    ""
   ]
  },
  "vazhenina14_sltu": {
   "authors": [
    [
     "Daria",
     "Vazhenina"
    ],
    [
     "Konstantin",
     "Markov"
    ]
   ],
   "title": "Sequence memoizer based language model for Russian speech recognition",
   "original": "sl14_183",
   "page_count": 5,
   "order": 26,
   "p1": "183",
   "pn": "187",
   "abstract": [
    "In this paper, we propose a novel language model for Russian large vocabulary speech recognition based on sequence memoizer modeling technique. Sequence memoizer is a long span text dependency model and was initially proposed for character language modeling. Here, we use it to build word level language model (LM) in ASR. We compare its performance with recurrent neural network (RNN) LM, which also models long span word dependencies. A number of experiments were carried out using various amounts of train data and different text data arrangements. According to our experimental results, the sequence memoizer LM outperforms recurrent neural network and standard 3-gram LMs in terms of perplexity, while RNN LM achieves better word error rate. The lowest word error rate is achieved by combining all three language models together using linear interpolation.\n",
    "Index Terms: sequence memoizer, advanced language modeling, inflective languages\n",
    ""
   ]
  },
  "lyudovyk14_sltu": {
   "authors": [
    [
     "Tetyana",
     "Lyudovyk"
    ],
    [
     "Valeriy",
     "Pylypenko"
    ]
   ],
   "title": "Code-Switching speech recognition for closely related languages",
   "original": "sl14_188",
   "page_count": 6,
   "order": 27,
   "p1": "188",
   "pn": "193",
   "abstract": [
    "This work presents an approach to recognition of multispeaker conversational speech with code-switching between Ukrainian and Russian languages. Both inter-sentential and intra-sentential code-switching is handled. The approach takes into account peculiarities of phonetic systems of the closely related Russian and Ukrainian languages. A crosslingual LVCSR system is developed. The acoustic model and pronunciation lexicon are based on Ukrainian phone set. Modeling of pronunciation variation in lexicons helps to cope not only with code-switching speech but also with accented speech. Results of code-switching speech recognition are presented. The approach is suitable especially in cases of intra-sentential code-switching where language identification is problematic.\n",
    "Index Terms: mixed speech, bilingual speech, codeswitching, Ukrainian, Russian\n",
    ""
   ]
  },
  "barnard14_sltu": {
   "authors": [
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Marelie H.",
     "Davel"
    ],
    [
     "Charl van",
     "Heerden"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Jaco",
     "Badenhorst"
    ]
   ],
   "title": "The NCHLT speech corpus of the South African languages",
   "original": "sl14_194",
   "page_count": 7,
   "order": 28,
   "p1": "194",
   "pn": "200",
   "abstract": [
    "The NCHLT speech corpus contains wide-band speech from approximately 200 speakers per language, in each of the eleven official languages of South Africa. We describe the design and development processes that were undertaken in order to develop the corpus, and report on associated materials such as orthographic transcriptions and pronunciation dictionaries that were released as part of the corpus. In order to benchmark speechrecognition performance on the corpus, we have also developed both phone-recognition and word-recognition systems for all eleven languages; we find that high accuracies can be achieved for these speaker-independent but vocabulary-dependent recognition tasks in all languages.\n",
    ""
   ]
  },
  "jokinen14_sltu": {
   "authors": [
    [
     "Kristiina",
     "Jokinen"
    ],
    [
     "Graham",
     "Wilcock"
    ]
   ],
   "title": "Community-based resource building and data collection",
   "original": "sl14_201",
   "page_count": 6,
   "order": 29,
   "p1": "201",
   "pn": "206",
   "abstract": [
    "The paper describes our work on participatory and community-based resource collection for the Sami language. This includes community events where participants wrote new Sami Wikipedia articles and took part in speech data collection by reading aloud Sami Wikipedia articles and discussing freely in group conversations. The aim was to increase the number of Sami Wikipedia articles and thereby strengthen Wikipedia as a digital resource for the Sami language and to collect speech data to be used in developing Sami speech components. Such components are intended to be combined with the Sami Wikipedia in order to build a spoken interactive knowledge access system.\n",
    "Index Terms: language resources development, Wikipedia, Sami language, community-based participatory data collection\n",
    ""
   ]
  },
  "leidig14_sltu": {
   "authors": [
    [
     "Sebastian",
     "Leidig"
    ],
    [
     "Tim",
     "Schlippe"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Automatic detection of anglicisms for the pronunciation dictionary generation: a case study on our German IT corpus",
   "original": "sl14_207",
   "page_count": 8,
   "order": 30,
   "p1": "207",
   "pn": "214",
   "abstract": [
    "With the globalization more and more words from other languages come into a language without assimilation to the phonetic system of the new language. To economically build up lexical resources with automatic or semi-automatic methods, it is important to detect and treat them separately. Due to the strong increase of Anglicisms, especially from the IT domain, we developed features for their automatic detection and collected and annotated a German IT corpus to evaluate them. Furthermore we applied our methods to Afrikaans words from the NCHLT corpus and German words from the news domain. Combining features based on grapheme perplexity, grapheme-to-phoneme confidence, Google hits count as well as spell-checker dictionary and Wiktionary lookup reaches 75.44% fscore. Producing pronunciations for the words in our German IT corpus based on our methods resulted in 1.6% phoneme error rate to reference pronunciations, while applying exclusively German grapheme-to-phoneme rules for all words achieved 5.0%.\n",
    "Index Terms: Foreign entity detection, lexical resources, pronunciation modeling, Anglicisms\n",
    ""
   ]
  },
  "petrica14_sltu": {
   "authors": [
    [
     "Lucian",
     "Petrică"
    ],
    [
     "Horia",
     "Cucu"
    ],
    [
     "Andi",
     "Buzo"
    ],
    [
     "Corneliu",
     "Burileanu"
    ]
   ],
   "title": "A robust diacritics restoration system using unreliable raw text data",
   "original": "sl14_215",
   "page_count": 6,
   "order": 31,
   "p1": "215",
   "pn": "220",
   "abstract": [
    "Statistical language models are utilized in many speech processing algorithms, e.g., automatic speech recognition (ASR). Such a model is created from a text corpus, but many of the text corpora for Romanian are unreliable with respect to the use of diacritic marks, i.e., diacritics are either partially or completely missing, resulting in low quality language models. We present a methodology for restoring diacritic marks to an unreliable text corpus, which requires no text resources apart from the corpus itself. The proposed methodology (i) identifies sections of the input corpus which are correct with respect to the use of diacritics, (ii) utilizes these sections to train a diacritics restoration system (DRS), and (iii) utilizes the DRS to correct the remaining sections of the corpus. We compare the DRS trained at (ii) with state-of-the-art systems, and observe up to 12% improvement with regard to the correctness of diacritic restoration. Furthermore, we utilize our methodology to create improved language models for the ASR system developed by the SpeeD laboratory, and demonstrate a decrease of 14% in perplexity and a 20% reduction of the out-of-vocabulary rate as a result.\n",
    "Index Terms: Diacritics, speech recognition\n",
    ""
   ]
  },
  "simonchik14_sltu": {
   "authors": [
    [
     "Konstantin",
     "Simonchik"
    ],
    [
     "Vadim",
     "Shchemelinin"
    ]
   ],
   "title": "“STC spoofing” database for text-dependent speaker recognition evaluation",
   "original": "sl14_221",
   "page_count": 4,
   "order": 32,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "The paper describes the “STC Spoofing” database, which consists of a set of recordings of “live” speech by several speakers, as well as synthesized speech fragments obtained using a TTS engine based on these speakers’ voices. The database can be used for testing the robustness of textdependent speaker verification systems against spoofing attacks, as well as for research and development of methods for fighting break-ins into biometric systems that are performed using synthesized speech. .\n",
    "Index Terms: Database, spoofing, anti-spoofing\n",
    ""
   ]
  },
  "mabokela14_sltu": {
   "authors": [
    [
     "Koena Ronny",
     "Mabokela"
    ],
    [
     "Madimetja Jonas",
     "Manamela"
    ],
    [
     "Mabu",
     "Manaileng"
    ]
   ],
   "title": "Modeling code-Switching speech on under-resourced languages for language identification",
   "original": "sl14_225",
   "page_count": 6,
   "order": 33,
   "p1": "225",
   "pn": "230",
   "abstract": [
    "This paper presents an integration of phonotactic information to perform language identification (LID) in a mixed-language speech. A single-pass front-end recognition system is employed to convert the spoken utterances into a statistical occurrence of phone sequences. To process such phone sequences, a hidden Markov model (HMM) is utilized to build robust acoustic models that can handle multiple languages within an utterance. A supervised Support Vector Machine (SVM) learns the language transition of the phonotactic information given the recognized phone sequences. The back-end SVM-based decision classifies language identity given the likelihood scores phone occurrences. The experiments are conducted on commonly mixed-language Northern Sotho and English speech utterances. We evaluate the system measuring the performance of the phone recognition and LID portions separately. We obtained a phone error rate of 15.7% when a data-driven phoneme mapping approach is modeled with 16 Gaussian mixtures per state. However, the proposed integrated LID system has achieved a considerable performance with an acceptable LID accuracy of 85.0% and average of 81% on code-switched speech and monolingual speech segments respectively.\n",
    "Index Terms: Code-switching speech, under-resourced languages, phonotactic information, acoustic models, language model\n",
    ""
   ]
  },
  "ludusan14_sltu": {
   "authors": [
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ]
   ],
   "title": "Towards low-resource prosodic boundary detection",
   "original": "sl14_231",
   "page_count": 7,
   "order": 34,
   "p1": "231",
   "pn": "237",
   "abstract": [
    "In (his study we propose a method of prosodic boundary detection based only on acoustic cues which are easily extractable from the speech signal and without any supervision. Drawing a parallel between the process of language acquisition in babies and the speech processing techniques for under-resourced languages, we take advantage of the findings of several psycholinguistic studies relative to the cues used by babies for the identification of prosodic boundaries. Several durational and pitch cues were investigated, by themselves or in a combination, and relatively good performances were achieved. The best result obtained, a combination of all the cues, compares well against a previously proposed approach, without relying on any learning method or any lexical or syntactic cues.\n",
    "Index Terms: Prosodic boundaries, acoustic cues, prosody recognition\n",
    ""
   ]
  },
  "molapo14_sltu": {
   "authors": [
    [
     "Raymond",
     "Molapo"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Febe de",
     "Wet"
    ]
   ],
   "title": "Speech data collection in an under-resourced language within a multilingual context",
   "original": "sl14_238",
   "page_count": 5,
   "order": 35,
   "p1": "238",
   "pn": "242",
   "abstract": [
    "In this paper, we present an end-to-end solution to the development of an automatic speech recognition (ASR) system in typical under-resourced languages, where the target language is likely to be influenced by one more embedded foreign languages. We first describe the collection and processing of the text corpus crawled from the World Wide Web using the Rapid Language Adaptation Toolkit. In particular, we highlight the challenges faced when foreign languages are embedded within the matrix language. Thereafter, we discuss our speech data collection efforts in under-resourced environments. We finally report on a strategy called transliteration that aids to improve recognition results of our grapheme-based automatic speech recognition system in the presence of embeddedlanguage words.\n",
    "Index Terms: under-resourced languages, matrix language, transliteration, grapheme-based ASR\n",
    ""
   ]
  },
  "skrelin14_sltu": {
   "authors": [
    [
     "Pavel",
     "Skrelin"
    ],
    [
     "Nina",
     "Volskaya"
    ],
    [
     "Karina",
     "Evgrafova"
    ],
    [
     "Riikka",
     "Ullakonoja"
    ]
   ],
   "title": "The development of new corpora for under-resourced languages using data available for well-resourced ones",
   "original": "sl14_243",
   "page_count": 4,
   "order": 36,
   "p1": "243",
   "pn": "246",
   "abstract": [
    "In the paper we propose to exploit existing corpora of wellresourced languages as a basis for developing similar corpora of under-resourced ones. The construction of this type of corpora will allow finding common patterns of acoustic manifestation of similar functional states regardless of the language. The analysis of these corpora will also allow investigating universal and language-specific features reflected in speech. Two pilot experiments which may contribute to the proposed strategy are presented.\n",
    "Index Terms: under-resourced languages, parallel speech corpora, acoustics, intonation\n",
    ""
   ]
  },
  "dmitriev14_sltu": {
   "authors": [
    [
     "Dmitri",
     "Dmitriev"
    ]
   ],
   "title": "Web lexicography for and by non-tech people",
   "original": "sl14_247",
   "page_count": 5,
   "order": 37,
   "p1": "247",
   "pn": "251",
   "abstract": [
    "Globbie Neologia is a set of open source software tools, GUI and methodologies for building a collaborative network of lexicographical sites. The ultimate goal is to make the process of lexicon compilation less complicated and more accessible to nontech people. The crowdsourced contributions are evaluated on-the-fly by the NLP service to conform to adopted formalism. The resulting linguistic descriptions are ready for use in digital applications, including crosslanguage search engines, machine translation, language learning etc.\n",
    "Index Terms: web lexicography, crowdsourcing, NLP, SVG based GUI\n",
    ""
   ]
  },
  "masmoudi14_sltu": {
   "authors": [
    [
     "Abir",
     "Masmoudi"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Mariem Ellouze",
     "Khmekhem"
    ],
    [
     "Fethi",
     "Bougares"
    ],
    [
     "Lamia Hadrich",
     "Belguith"
    ]
   ],
   "title": "Phonetic tool for the Tunisian Arabic",
   "original": "sl14_253",
   "page_count": 5,
   "order": 38,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "A phonetic dictionary is an essential component of a speech recognition system or a speech synthesis system. Our work targets the generation of an automatic pronunciation dictionary for the Tunisian Arabic, in particular in the field of rail transport. To do this, we created two tools of phonetic vowelized and unvowelized words in the Tunisian Arabic. The proposed method to automatically generate phonetic dictionaries is based on rules and is presented in this article. This paper outlines the steps to create our own study corpus TARIC: Tunisian Arabic Railway Interaction Corpus[l]. Then it details the phonetic and phonological exceptions of the Tunisian Arabic and illustrates some rules used for the construction of phonetic dictionary.\n",
    "s\n",
    "A. Masmoudi. M. Ellouze Khmekhem, Y. Estève, L. Hadrich Belguith, and N. Ilabash, \"A corpus and a phonetic dictionary for Tunisian Arabic speech recognition\". In 19th edition of the Language Resources and Evaluation Conference (LREC), Reykjavik, Iceland, 2014.\n",
    "",
    "",
    "Index Terms: Tunisian Arabic, Phonologic, Phonetic Dictionary, vowelized and unvowelized words.\n",
    ""
   ]
  },
  "harrat14_sltu": {
   "authors": [
    [
     "S.",
     "Harrat"
    ],
    [
     "Karima",
     "Meftouh"
    ],
    [
     "M.",
     "Abbas"
    ],
    [
     "K.",
     "Smaili"
    ]
   ],
   "title": "Grapheme to phoneme conversion: an Arabic dialect case",
   "original": "sl14_257",
   "page_count": 6,
   "order": 39,
   "p1": "257",
   "pn": "262",
   "abstract": [
    "We aim to develop a Speech-to-Speech translation system between Modern Standard Arabic and Algiers dialect. Such a system must include a Text-to-Speech module which itself must include a Grapheme-to-Phoneme converter. Algiers dialect is an Arabic dialect concerned by the most problems of Modern Standard Arabic in NLP area. Furthermore, it could be considered as an under-resourced language because it is a vernacular language for which no substantial corpus exists. In this paper we present a grapheme-to-phoneme converter for this language. We used a rule based approach and a statistical approach, we got an accuracy of 92% VS 85% despite the lack of resource for this language.\n",
    "Index Terms: Modern Standard Arabic (MSA), Algiers Dialect, Grapheme-to-phoneme conversion, Statistical Machine Translation.\n",
    ""
   ]
  },
  "goudi14_sltu": {
   "authors": [
    [
     "Maria",
     "Goudi"
    ],
    [
     "Pascal",
     "Nocera"
    ]
   ],
   "title": "Sounds and symbols: an overview of different types of methods dealing with letters-to-sounds relationships in a wide range of languages in automatic speech recognition",
   "original": "sl14_263",
   "page_count": 5,
   "order": 40,
   "p1": "263",
   "pn": "267",
   "abstract": [
    "Mapping a language's graphemes lo a sequence of symbols, which represent ils corresponding phonemes, is of great importance for the recognition accuracy of an automatic speech recognition system. Phoneme-based and grapheme-based approaches are mainly employed for the creation of a pronunciation dictionary. In this paper, we present the application of these approaches on a variety of languages with different types of writing systems and various degrees of complexity between graphemes and phonemes.\n",
    "Index Terms: Writing systems, automatic speech recognition, pronunciation dictionary, acoustic model, speech recognition, phoneme-based, grapheme-based, rule-based.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "nakamura14_sltu",
    "gales14_sltu"
   ]
  },
  {
   "title": "Contributed Papers",
   "papers": [
    "anguera14_sltu",
    "adel14_sltu",
    "grezl14_sltu",
    "sakti14_sltu",
    "addadecker14_sltu",
    "sahraeian14_sltu",
    "samsonjuan14_sltu",
    "stahlberg14_sltu",
    "kipyatkova14_sltu",
    "ferreira14_sltu",
    "ahmed14_sltu",
    "nguyen14_sltu",
    "chistikov14_sltu",
    "hartmann14_sltu",
    "alumae14_sltu",
    "cucu14_sltu",
    "tarjan14_sltu",
    "schlippe14_sltu",
    "laurent14_sltu",
    "do14_sltu",
    "vasilescu14_sltu",
    "vakil14_sltu",
    "do14b_sltu",
    "vazhenina14_sltu",
    "lyudovyk14_sltu",
    "barnard14_sltu",
    "jokinen14_sltu",
    "leidig14_sltu",
    "petrica14_sltu",
    "simonchik14_sltu",
    "mabokela14_sltu",
    "ludusan14_sltu",
    "molapo14_sltu",
    "skrelin14_sltu",
    "dmitriev14_sltu",
    "masmoudi14_sltu",
    "harrat14_sltu",
    "goudi14_sltu"
   ]
  }
 ]
}