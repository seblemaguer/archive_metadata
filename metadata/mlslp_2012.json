{
 "title": "Machine Learning in Speech and Language Processing (MLSLP 2012)",
 "location": "Portland, Oregon, USA",
 "startDate": "14/9/2012",
 "endDate": "14/9/2012",
 "conf": "MLSLP",
 "year": "2012",
 "name": "mlslp_2012",
 "series": "MLSLP",
 "SIG": "",
 "title1": "Machine Learning in Speech and Language Processing",
 "title2": "(MLSLP 2012)",
 "date": "14 September 2012",
 "booklet": "mlslp_2012.pdf",
 "papers": {
  "gales12_mlslp": {
   "authors": [
    [
     "Mark",
     "Gales"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Austin",
     "Zhang"
    ],
    [
     "Rogier van",
     "Dalen"
    ]
   ],
   "title": "Structured discriminative models for speech recognition",
   "original": "ml12_101",
   "page_count": 0,
   "order": 1,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Generative models, normally in the form of hidden Markov models, have been the dominant form of acoustic model for automatic speech recognition for more than two decades. In recent years there has been interest in applying structured discriminative models to this task. This talk discusses one particular form of discriminative model, log-linear models, and how they may be applied to continuous speech recognition tasks. Two important issues will be discussed in detail: the appropriate form of features for this model; and the training criterion to be used. Generative models are proposed to extract the features for the discriminative log-linear model. This combination of generative and discriminative models enables state-of-the-art adaptation and noise robustness approaches to be used to handle mismatches between the training and test conditions. An interesting aspect of these features is that the conditional independence assumptions of the underlying generative models are not necessarily reflected in the features that are derived from the models. Various forms of training criteria, including minimum Bayes' risk and large margin approaches, are discussed. The relationship between large-margin training of log-linear models and structured support vector machines is described. Results are presented on two noise-robustness tasks: AURORA-2 and AURORA-4.\n",
    ""
   ]
  },
  "bendavid12_mlslp": {
   "authors": [
    [
     "Shai",
     "Ben-David"
    ]
   ],
   "title": "Theoretical analysis of domain adaptation - current state of the art",
   "original": "ml12_102",
   "page_count": 0,
   "order": 2,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Domain adaptation (DA) learning occurs often in practice. It refers to situations in which the training data available to the learner is (generated by a distribution that is) different than (the distribution generating) the target task that the learnt predictor will be evaluated on. Theoretical work on domain adaptation aims to characterize situations in which such learning is possible and to propose appropriate learning paradigms and algorithms. In this talk I will survey the current theoretical understanding of DA learning and list some directions in which further research may require collaboration with people that encounter such learning issues in practice.\n",
    ""
   ]
  },
  "roark12_mlslp": {
   "authors": [
    [
     "Brian",
     "Roark"
    ],
    [
     "Arda",
     "Celebi"
    ],
    [
     "Erinc",
     "Dikici"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Maider",
     "Lehr"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ],
    [
     "Kenji",
     "Sagae"
    ],
    [
     "Murat",
     "Saraclar"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Puyang",
     "Xu"
    ]
   ],
   "title": "Hallucinating system outputs for discriminative language modeling",
   "original": "ml12_103",
   "page_count": 0,
   "order": 3,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Discriminative language modeling methods learn language model parameterizations from system outputs when applied to training data, by optimizing objective functions close to actual system objectives via algorithms such as the perceptron. Thus, for speech recognition, training data utterances are recognized with a baseline recognizer, and the output lattices or n-best lists are evaluated with respect to the reference transcript to determine parameter updates. Relying on supervised training data limits the amount of available data for such methods, even in the most data-rich scenarios. In this talk, I will review some recent work on discriminative language modeling for speech recognition that aims to augment existing supervised training data by \"hallucinating\" system outputs in cases where no system inputs exist, e.g., from text corpora. We present several approaches for hallucinating confusion sets for a given reference string, and compare results achieved with these methods on large vocabulary continuous speech recognition tasks. We demonstrate real system improvements using such methods, and discuss potential future directions for the work.\n",
    ""
   ]
  },
  "compernolle12_mlslp": {
   "authors": [
    [
     "Dirk van",
     "Compernolle"
    ]
   ],
   "title": "Large vocabulary template based speech recognition",
   "original": "ml12_104",
   "page_count": 0,
   "order": 4,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Acoustic Modeling, the core component of any speech recognition system, has been dominated by the HMM/GMM modeling paradigm for the past decades. This approach is very powerful thanks to the very powerful machine learning algorithms that allow for learning and optimizing the parameters in the system based on thousands of hours of speech data. The weakness is that it requires a crude approximation of reality, in which short term speech frames are modeled as being independent observations generated by a first order Markov process. In recent years a variety of examplar based approaches have been developed as an alternative to the HMM paradigm. The main promise of any examplar based approach is that it can form a new local reference model on demand that is more relevant for the test sample than a global HMM ever can be and can tell us more about the test sample and hence should result in better recognition.   The system we have developed at KULeuven-ESAT is at its basis very similar to the small vocabulary Dynamic Time Warping systems used in the early days of isolated word recognition. We will focus on those novel components that were essential to make DTW work for a large vocabulary continuous speech recognition task. Unit (class) definition and local distance metric (especially within class, thus very local) needed a critical review. Data sharpening - a technique commonly used in non-parametric classifiers to deal with outliers and mislabeled data - proved to be an essential complement for any local distance metric. We will review both single best and k-NN decoding strategies that each hold different options to deal with long span continuity constraints in the decoded match. We will then dig deeper into how we can query the most similar examplars for properties beyond classical spectral envelope features. These meta- features may have been derived from the signal observed over a much wider time window, any available annotation or both: e.g. gender, speaking style, phonetic context, speaking rate, background ... Given the variable nature of such meta-features merging them into a global score is accomplished SCRFs (Segmental Conditional Random Fields).   Doing time warping at the template level makes our system computationally intensive. We will highlight some issues and adopted solutions. Finally we will make a comparison of our approach with some of the other examplar based systems.\n",
    ""
   ]
  },
  "mcdermott12_mlslp": {
   "authors": [
    [
     "Erik",
     "McDermott"
    ]
   ],
   "title": "An integrated framework for \"margin\" based sequential discriminative training over lattices based on differenced maximum mutual information (dMMI)",
   "original": "ml12_105",
   "page_count": 0,
   "order": 5,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Using the central observation that margin-based weighted classification error (modeled using Minimum Phone Error (MPE)) corresponds to the derivative with respect to the margin term of margin-based hinge loss (modeled using ``boosted'' Maximum Mutual Information (bMMI)), the differenced Maximum Mutual Information (dMMI) approach subsumes and extends margin-based MPE and bMMI within a broader framework in which the objective function is an integral of MPE loss over a range of margin values. Applying the Fundamental Theorem of Calculus, this integral is easily evaluated using finite differences of bMMI functionals. Practical lattice-based training using the new criterion can then be carried out using differences of bMMI gradients. Experimental results for training of Gaussian Mixture Model (GMM) based hidden Markov models (HMMs) using dMMI on Large Vocabulary Continuous Speech Recognition tasks show that dMMI with the right margin interval can recover the expected MPE or bMMI performance [McDermott et al. ICASSP 2010]; results for dMMI-trained feature transformations suggest that suitably chosen margin intervals can improve over the corresponding MPE- or bMMI- trained transformations [Delcroix et al. ICASSP 2012]. One consequence of these findings is that dMMI can be used to implement a close approximation of MPE without recourse to the modified Baum-Welch algorithm [Povey 2002], using a simple difference of bMMI functionals instead; conversely dMMI can be used to verify that a given MPE implementation is correct, as it must match dMMI results for a narrow margin interval centered on the origin. Finally, dMMI can be used as the basis for a Bayesian framework where the margin-modified cost function is integrated over a general margin prior, approximated as a sum of dMMI functionals [McDermott, Acoustical Society of Japan, Spring 2010]. The Error-indexed Forward-Backward algorithm [McDermott et al. Interspeech 2008], which aggregates occupancies of lattice word strings by equal error count, can be used to visualize the way in which MPE and bMMI are special cases of the more general dMMI.\n",
    ""
   ]
  },
  "dhillon12_mlslp": {
   "authors": [
    [
     "Inderjit S.",
     "Dhillon"
    ],
    [
     "Cho-Jui",
     "Hsieh"
    ],
    [
     "Matyas",
     "Sustik"
    ],
    [
     "Pradeep",
     "Ravikumar"
    ]
   ],
   "title": "Sparse inverse covariance matrix estimation using quadratic approximation",
   "original": "ml12_106",
   "page_count": 0,
   "order": 6,
   "p1": "0",
   "pn": "",
   "abstract": [
    "The L1-regularized Gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a new algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods.\n",
    ""
   ]
  },
  "poon12_mlslp": {
   "authors": [
    [
     "Hoifung",
     "Poon"
    ]
   ],
   "title": "Unsupervised semantic parsing",
   "original": "ml12_107",
   "page_count": 0,
   "order": 7,
   "p1": "0",
   "pn": "",
   "abstract": [
    "We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.\n",
    ""
   ]
  },
  "heigold12_mlslp": {
   "authors": [
    [
     "Georg",
     "Heigold"
    ]
   ],
   "title": "Exemplar-based speech recognition in a rescoring approach",
   "original": "ml12_108",
   "page_count": 0,
   "order": 8,
   "p1": "0",
   "pn": "",
   "abstract": [
    "The acoustic models in state-of-the-art speech recognition systems are based on phones in context that are represented by hidden Markov models. This modeling approach may be insufficient to model long-span acoustic context. Exemplar-based approaches may be an attractive alternative to this conventional approach. In this talk, we will discuss a general rescoring framework based on conditional random fields and show how to do exemplar-based speech recognition within this framework, with the focus on massive, noisy data. Experimental results for the Voice Search and the YouTube tasks are presented and analyzed.\n",
    ""
   ]
  },
  "daume12_mlslp": {
   "authors": [
    [
     "Hal",
     "Daumé"
    ]
   ],
   "title": "Transfer learning in language",
   "original": "ml12_109",
   "page_count": 0,
   "order": 9,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Human language is messy, and machine learning has done a lot to tame this messiness. There are many facets to language processing, and while the common approach is to run a bunch of component systems in a pipeline, there is mounting evidence that this is a bad idea. Enter transfer learning and multitask learning. Unfortunately, there are many open-ended problems in transfer learning for language due to the sorts of data and annotations that we have easy access to in the language domain. This talk will highlight some successful attempts to use transfer learning (generative and otherwise) in language, but will also talk a good deal about what is unsolved, and point to some interesting current avenues of research.\n",
    ""
   ]
  },
  "deng12_mlslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Learning deep architectures using kernel modules",
   "original": "ml12_110",
   "page_count": 0,
   "order": 10,
   "p1": "0",
   "pn": "",
   "abstract": [
    "A review on the key concepts and techniques in kernel-based learning will be provided that are relevant to extending and advancing deep learning architectures and algorithms. I will then survey and analyze the counterparts in deep learning relevant to extending kernel methods. Insight into how the generally finite-dimensional (hidden) features in deep networks can be transformed into infinite-dimensional features via the kernel trick without incurring computational and regularization difficulty will be elaborated. Insight is also offered into how the use of deep architectures can overcome the potential limitation of linear pattern functions associated with the kernel feature space. These insights lead to integrated kernel and deep learning architectures, with interesting new regularization properties and hyper-parameter sets that are distinct from those associated with separate kernel and deep learning methods. Relevance of such new sets of machine learning methods to speech and language processing will be discussed.\n",
    ""
   ]
  },
  "demarco12_mlslp": {
   "authors": [
    [
     "Andrea",
     "DeMarco"
    ],
    [
     "Stephen J.",
     "Cox"
    ]
   ],
   "title": "Iterative classification of regional British accents in i-vector space",
   "original": "ml12_001",
   "page_count": 4,
   "order": 11,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "Joint-Factor Analysis (JFA) and I-vectors have been shown to be effective for speaker verification and language identification. Channel factor adaptation has also been used for language and accent identification. In this paper, we show how these techniques can be used successfully in the task of accent classification, and we achieve good accuracy on a 14 accent problem using a novel iterative classification framework based on an iterative linear/quadratic classifier. These results compare favourably with recent results obtained using other non-fused acoustic techniques.\n",
    "Index Terms: Ivector, accent classification, discriminant analysis, confidence measure\n",
    ""
   ]
  },
  "nishikawa12_mlslp": {
   "authors": [
    [
     "Hitoshi",
     "Nishikawa"
    ],
    [
     "Toshiro",
     "Makino"
    ],
    [
     "Yoshihiro",
     "Matsuo"
    ]
   ],
   "title": "Domain adaptation with augmented space method for multi-domain contact center dialogue summarization",
   "original": "ml12_005",
   "page_count": 4,
   "order": 12,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "In this paper we propose a method to improve the quality of extractive summarization for contact center dialogues in various domains by making use of training samples whose domains are different from that of the test samples. Since preparing sufficient numbers of training samples for each domain is too expensive, we leverage references from many different domains and employ the Augmented Space Method to implement domain adaptation. As the target of summarization, we take up contact center dialogues in six domains and summarize their transcripts. Our experiment shows that the proposed method achieves better results than the usual supervised learning approach.\n",
    "Index Terms: speech summarization, domain adaptation\n",
    ""
   ]
  },
  "delcroix12_mlslp": {
   "authors": [
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Dynamic variance adaptation using differenced maximum mutual information",
   "original": "ml12_009",
   "page_count": 4,
   "order": 13,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "A conventional approach for noise robust automatic speech recognition consists of using a speech enhancement before recognition. However, speech enhancement cannot completely remove noise, thus a mismatch between the enhanced speech and the acoustic model inevitably remains. Uncertainty decoding approaches have been used to mitigate such a mismatch by accounting for the feature uncertainty during decoding. We have proposed dynamic variance adaptation to estimate the feature uncertainty given adaptation data by maximization of likelihood or discriminative criterion such as MMI. For unsupervised adaptation, the transcriptions are obtained from a first recognition pass and thus contain errors. Such errors are fatal when using a discriminative criterion. In this paper, we investigate the recently proposed differenced MMI discriminative criterion for unsupervised dynamic variance adaptation, because it inherently includes a mechanism to mitigate the influence of errors in the transcriptions.\n",
    "Index Terms: Robust speech recognition, dynamic variance adaptation, unsupervised adaptation, discriminative training, dMMI\n",
    ""
   ]
  },
  "nallasamy12_mlslp": {
   "authors": [
    [
     "Udhyakumar",
     "Nallasamy"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Semi-supervised learning for speech recognition in the context of accent adaptation",
   "original": "ml12_013",
   "page_count": 5,
   "order": 14,
   "p1": "13",
   "pn": "17",
   "abstract": [
    "Accented speech that is under-represented in the training data still suffers highWord Error Rate (WER) with state-of-the-art Automatic Speech Recognition (ASR) systems. Careful collection and transcription of training data for different accents can address this issue, but it is both time consuming and expensive. However, for many tasks such as broadcast news or voice search, it is easy to obtain large amounts of audio data from target users with representative accents, albeit without accent labels or even transcriptions. Semi-supervised training have been explored for ASR in the past to leverage such data, but many of these techniques assume homogeneous training and test conditions. In this paper, we experiment with cross-entropy based speaker selection to adapt a source recognizer to a target accent in a semi-supervised manner, using additional data with no accent labels. We compare our technique to self-training based only on confidence scores and show that we obtain significant improvements over the baseline by leveraging additional unlabeled data on two different tasks in Arabic and English.\n",
    "Index Terms: Semi-supervised learning, Automatic speech recognition, Accent adaptation\n",
    ""
   ]
  },
  "roy12_mlslp": {
   "authors": [
    [
     "Anindya",
     "Roy"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Sébastien",
     "Marcel"
    ]
   ],
   "title": "Boosting localized binary features for speech recognition",
   "original": "ml12_018",
   "page_count": 4,
   "order": 15,
   "p1": "18",
   "pn": "21",
   "abstract": [
    "In a recent work, the framework of Boosted Binary Features (BBF) was proposed for ASR. In this framework, a small set of localized binary-valued features are selected using the Discrete Adaboost algorithm. These features are then integrated into a standard HMM-based system using either single layer perceptrons (SLP) or multilayer perceptrons (MLP). The features were found to perform significantly better (when coupled with SLP) and equally well (when coupled with MLP) compared to MFCC features on the TIMIT phoneme recognition task. The current work presents an overview of the idea and extends it in two directions: 1) fusion of BBF withMFCC and an analysis of their complementarity, 2) scalability of the proposed features from phoneme recognition to the continuous speech recognition task and reusability on unseen data.\n",
    "Index Terms: Boosting, localized features, spectrotemporal features, speech recognition, feature fusion.\n",
    ""
   ]
  },
  "prabhavalkar12_mlslp": {
   "authors": [
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Karen",
     "Livescu"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Discriminative spoken term detection with limited data",
   "original": "ml12_022",
   "page_count": 4,
   "order": 16,
   "p1": "22",
   "pn": "25",
   "abstract": [
    "We study spoken term detection:the task of determining whether and where a given word or phrase appears in a given segment of speech:in the setting of limited training data. This setting is becoming increasingly important as interest grows in porting spoken term detection to multiple lowresource languages and acoustic environments. We propose a discriminative algorithm that aims at maximizing the area under the receiver operating characteristic curve, often used to evaluate the performance of spoken term detection systems. We implement the approach using a set of feature functions based on multilayer perceptron classifiers of phones and articulatory features, and experiment on data drawn from the Switchboard database of conversational telephone speech. Our approach outperforms a baseline HMM-based system by a large margin across a number of training set sizes.\n",
    "Index Terms: spoken term detection, discriminative training, AUC, structural SVM\n",
    ""
   ]
  },
  "zhang12_mlslp": {
   "authors": [
    [
     "Bin",
     "Zhang"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Semi-supervised learning for text classification using feature affinity regularization",
   "original": "ml12_026",
   "page_count": 4,
   "order": 17,
   "p1": "26",
   "pn": "29",
   "abstract": [
    "Most conventional semi-supervised learning methods attempt to directly include unlabeled data into training objectives. This paper presents an alternative approach that learns feature affinity information from unlabeled data, which is incorporated into the training objective as regularization of a maximum entropy model. The regularization favors models for which correlated features have similar weights. The method is evaluated in text classification, where feature affinity can be computed from feature co-occurrences in unlabeled data. Experimental results show that this method consistently outperforms baseline methods.\n",
    "Index Terms: semi-supervised learning, text classification, maximum entropy, feature affinity matrix, regularization\n",
    ""
   ]
  },
  "ylmaz12_mlslp": {
   "authors": [
    [
     "Emre",
     "Yılmaz"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Combining exemplar-based matching and exemplar-based sparse representations of speech",
   "original": "ml12_030",
   "page_count": 4,
   "order": 18,
   "p1": "30",
   "pn": "33",
   "abstract": [
    "In this paper, we compare two different frameworks for exemplarbased speech recognition and propose a combined system that approximates the input speech as a linear combination of exemplars of variable length. This approach allows us not only to use multiple length long exemplars, each representing a certain speech unit, but also to jointly approximate input speech segments using several exemplars. While such an approach is able to model noisy speech, it also enforces a feature representation in which additivity of the effect of signal sources holds. This is observed to limit the recognition accuracy compared to e.g. discriminatively trained representations. We investigate the system performance starting from a baseline single-neighbor exemplar matching system using discriminative features to the proposed combined system to identify the main reasons of recognition errors. Even though the proposed approach has a lower recognition accuracy than the baseline, it significantly outperforms the intermediate systems using comparable features.\n",
    "Index Terms: speech recognition, exemplar-based, template matching, sparse representations\n",
    ""
   ]
  },
  "arora12_mlslp": {
   "authors": [
    [
     "Raman",
     "Arora"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements",
   "original": "ml12_034",
   "page_count": 4,
   "order": 19,
   "p1": "34",
   "pn": "37",
   "abstract": [
    "We consider the problem of learning transformations of acoustic feature vectors for phonetic frame classification, in a multi-view setting where articulatory measurements are available at training time but not at test time. Canonical correlation analysis (CCA) has previously been used to learn linear transformations of the acoustic features that are maximally correlated with articulatory measurements. Here, we learn nonlinear transformations of the acoustics using kernel canonical correlation analysis (KCCA). We present an incremental SVD approach that makes the KCCA computations feasible for typical speech data set sizes. In phonetic frame classification experiments on data drawn from the University of Wisconsin X-ray Microbeam Database, we find that KCCA provides consistent improvements over linear CCA, as well as over single-view unsupervised dimensionality reduction.\n",
    "Index Terms: multi-view learning, kernel canonical correlation analysis, XRMB, articulatory measurements\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Oral and Invited Talks (abstracts and presentations; no full papers)",
   "papers": [
    "gales12_mlslp",
    "bendavid12_mlslp",
    "roark12_mlslp",
    "compernolle12_mlslp",
    "mcdermott12_mlslp",
    "dhillon12_mlslp",
    "poon12_mlslp",
    "heigold12_mlslp",
    "daume12_mlslp",
    "deng12_mlslp"
   ]
  },
  {
   "title": "Poster Session (full papers)",
   "papers": [
    "demarco12_mlslp",
    "nishikawa12_mlslp",
    "delcroix12_mlslp",
    "nallasamy12_mlslp",
    "roy12_mlslp",
    "prabhavalkar12_mlslp",
    "zhang12_mlslp",
    "ylmaz12_mlslp",
    "arora12_mlslp"
   ]
  }
 ]
}